{"title": "The Cornell program synthesizer: a syntax-directed programming environment\n", "abstract": " Programs are not text; they are hierarchical compositions of computational structures and should be edited, executed, and debugged in an environment that consistently acknowledges and reinforces this viewpoint. The Cornell Program Synthesizer demands a structural perspective at all stages of program development. Its separate features are unified by a common foundation: a grammar for the programming language. Its full-screen derivation-tree editor and syntax-directed diagnostic interpreter combine to make the Synthesizer a powerful and responsive interactive programming tool.", "num_citations": "942\n", "authors": ["1739"]}
{"title": "Integrating noninterfering versions of programs\n", "abstract": " The need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to integrate programs by hand. To date, the only available tools for assisting with program integration are variants of text-based differential file comparators; these are of limited utility because one has no guarantees about how the program that is the product of an integration behaves compared to the programs that were integrated. This paper concerns the design of a semantics-based tool for automatically integrating program versions. The main contribution of the paper is an algorithm that takes as input three programs A, B, and Base, where A and B are two variants of Base. Whenever the changes made to Base to create A and B do not \u201cinterfere\u201d (in a sense defined in the paper), the algorithm produces a program M that integrates A and B. The algorithm is predicated on the\u00a0\u2026", "num_citations": "614\n", "authors": ["1739"]}
{"title": "Analyzing memory accesses in x86 executables\n", "abstract": " This paper concerns static-analysis algorithms for analyzing x86 executables. The aim of the work is to recover intermediate representations that are similar to those that can be created for a program written in a high-level language. Our goal is to perform this task for programs such as plugins, mobile code, worms, and virus-infected code. For such programs, symbol-table and debugging information is either entirely absent, or cannot be relied upon if present; hence, the technique described in the paper makes no use of symbol-table/debugging information. Instead, an analysis is carried out to recover information about the contents of memory locations and how they are manipulated by the executable.", "num_citations": "508\n", "authors": ["1739"]}
{"title": "An incremental algorithm for a generalization of the shortest-path problem\n", "abstract": " Thegrammar problem, a generalization of the single-source shortest-path problem introduced by D. E. Knuth (Inform. Process. Lett.6(1) (1977), 1\u20135) is to compute the minimum-cost derivation of a terminal string from each nonterminal of a given context-free grammar, with the cost of a derivation being suitably defined. This problem also subsumes the problem of finding optimal hyperpaths in directed hypergraphs (under varying optimization criteria) that has received attention recently. In this paper we present an incremental algorithm for a version of the grammar problem. As a special case of this algorithm we obtain an efficient incremental algorithm for the single-source shortest-path problem with positive edge lengths. The aspect of our work that distinguishes it from other work on the dynamic shortest-path problem is its ability to handle \u201cmultiple heterogeneous modifications\u201d: between updates, the input graph is\u00a0\u2026", "num_citations": "493\n", "authors": ["1739"]}
{"title": "The synthesizer generator\n", "abstract": " Programs are hierarchical compositions of formulae satisfying structural and extra-structural relationships. A program editor can use knowledge of such relationships to detect and provide immediate feedback about violations of them. The Synthesizer Generator is a tool for creating such editors from language descriptions. An editor designer specifies the desired relationships and the feedback to be given when they are violated, as well as a user interface; from the specification, the Synthesizer Generator creates a full-screen editor for manipulating programs in the language.", "num_citations": "457\n", "authors": ["1739"]}
{"title": "Program analysis via graph reachability\n", "abstract": " This paper describes how a number of program-analysis problems can be solved by transforming them to graph-reachability problems. Some of the program-analysis problems that are amenable to this treatment include program slicing, certain dataflow-analysis problems, one version of the problem of approximating the possible \u201cshapes\u201d that heap-allocated structures in a program can take on, and flow-insensitive points-to analysis. Relationships between graph reachability and other approaches to program analysis are described. Some techniques that go beyond pure graph reachability are also discussed.", "num_citations": "438\n", "authors": ["1739"]}
{"title": "The use of program dependence graphs in software engineering\n", "abstract": " This paper describes a language-independent program representation-the program dependence graph\u2014and &mCusses how program dependence graphs, together with operations such as program slicing, can provide the basis for powerfid programming tools that address important software-engineering problems, such as understanding what an existing program does and how it works, understanding the differences between several versions of a program, and creating new programs by combining pieces of old programs.", "num_citations": "406\n", "authors": ["1739"]}
{"title": "Generating language-based environments\n", "abstract": " This thesis concerns the design of interactive, language-based programming  environments that use knowledge of a programming language to provide functions  based on the structure and meaning of programs. The goal of the research is a  system-constructor to enable editors for different languages to be created  easily. The most challenging aspect of such a system is the design of the semantic  component, because a language-based editor performs static semantic analysis  when a program is altered in order to detect erroneous constructions or to  prevent illegal modifications. For efficiency, this should be performed  incrementally, re-using as much old information as possible; therefore, a  major focus of my research concerns a model of editing for which it is  possible to perform incremental semantic analysis efficiently. In this model, a program is represented as an attributed tree in which all  attributes have consistent values; programs are modified by tree operations  such as pruning, grafting, and deriving. After each modification, some of the  attributes require new values; incremental semantic analysis is performed by  updating attribute values to again make them all consistent. The thesis  presents several algorithms for this process that are asymptotically optimal  in time. The chief disadvantage of attribute grammars is that they use large amounts of  storage. The thesis discusses three aspects of utilizing storage efficiently  in such systems. One way to reduce the amount of storage used is to reduce the  number of atttribute values retained at any stage of attribute evaluation.  The thesis establishes two results concerning this idea: it presents one\u00a0\u2026", "num_citations": "398\n", "authors": ["1739"]}
{"title": "Incremental context-dependent analysis for language-based editors\n", "abstract": " Knowledge of a programming language's grammar allows language-based editors to enforce syntactic correctness at all times during development by restricting editing operations to legitimate modifications ot~ the program's context-free derivation tree; however, not all language constraints can be enforced in this way because not all features can be described by the context-free formalism. Attribute grammars permit context-dependent language features to be expressed in a modular, declarative fashion and thus are a good basis for specifying language-based editors. Such editors represent programs as attributed trees, Which are modified by operations such as subtree pruning and grafting. Incremental analysis is performed by updating attribute values after every modification. This paper discusses how updating can be carried out and presents several algorithms for the task, including one that is asymptotically\u00a0\u2026", "num_citations": "384\n", "authors": ["1739"]}
{"title": "Identifying modules via concept analysis\n", "abstract": " Describes a general technique for identifying modules in legacy code. The method is based on concept analysis - a branch of lattice theory that can be used to identify similarities among a set of objects based on their attributes. We discuss how concept analysis can identify potential modules using both \"positive\" and \"negative\" information. We present an algorithmic framework to construct a lattice of concepts from a program, where each concept represents a potential module. We define the notion of a concept partition, present an algorithm for discovering all concept partitions of a given concept lattice, and prove the algorithm to be correct.", "num_citations": "383\n", "authors": ["1739"]}
{"title": "Dependence analysis for pointer variables\n", "abstract": " Our concern is how to determine data dependencies between program constructs in programming languages with pointer variables. We are particularly interested in computing data dependencies for languages that manipulate heap-allocated storage, such as Lisp and Pascal. We have defined a family of algorithms that compute safe approximations to the flow, output, and anti-dependencies of a program written in such a language. Our algorithms account for destructive updates to fields of a structure and thus are not limited to the cases where all structures are trees or acyclic graphs; they are applicable to programs that build cyclic structures.", "num_citations": "336\n", "authors": ["1739"]}
{"title": "WYSINWYX: What You See Is Not What You eXecute\n", "abstract": " Over the last seven years, we have developed static-analysis methods to recover a good approximation to the variables and dynamically allocated memory objects of a stripped executable, and to track the flow of values through them. The article presents the algorithms that we developed, explains how they are used to recover Intermediate Representations (IRs) from executables that are similar to the IRs that would be available if one started from source code, and describes their application in the context of program understanding and automated bug hunting. Unlike algorithms for analyzing executables that existed prior to our work, the ones presented in this article provide useful information about memory accesses, even in the absence of debugging information. The ideas described in the article are incorporated in a tool for analyzing Intel x86 executables, called CodeSurfer/x86. CodeSurfer/x86 builds a system\u00a0\u2026", "num_citations": "313\n", "authors": ["1739"]}
{"title": "On the computational complexity of dynamic graph problems\n", "abstract": " A common way to evaluate the time complexity of an algorithm is to use asymptotic worst-case analysis and to express the cost of the computation as a function of the size of the input. However, for an incremental algorithm this kind of analysis is sometimes not very informative. (By an \u201cincremental algorithm\u201d, we mean an algorithm for a dynamic problem.) When the cost of the computation is expressed as a function of the size of the (current) input, several incremental algorithms that have been proposed run in time asymptotically no better, in the worst-case, than the time required to perform the computation from scratch. Unfortunately, this kind of information is not very helpful if one wishes to compare different incremental algorithms for a given problem.This paper explores a different way to analyze incremental algorithms. Rather than express the cost of an incremental computation as a function of the size of the\u00a0\u2026", "num_citations": "296\n", "authors": ["1739"]}
{"title": "Reducing concurrent analysis under a context bound to sequential analysis\n", "abstract": " This paper addresses the analysis of concurrent programs with shared memory. Such an analysis is undecidable in the presence of multiple procedures. One approach used in recent work obtains decidability by providing only a partial guarantee of correctness: the approach bounds the number of context switches allowed in the concurrent program, and aims to prove safety, or find bugs, under the given bound. In this paper, we show how to obtain simple and efficient algorithms for the analysis of concurrent programs with a context bound. We give a general reduction from a concurrent program P, and a given context bound K, to a sequential program P                                                                 s                                                           K                                     such that the analysis of P                                                                 s                                                           K                                     can be used to prove properties about P. The\u00a0\u2026", "num_citations": "238\n", "authors": ["1739"]}
{"title": "WYSINWYX: What You See Is Not What You eXecute\n", "abstract": " What You See Is Not What You eXecute: computers do not execute source-code programs; they execute machine-code programs that are generated from source code. Not only can the WYSINWYX phenomenon create a mismatch between what a programmer intends and what is actually executed by the processor, it can cause analyses that are performed on source code to fail to detect certain bugs and vulnerabilities. This issue arises regardless of whether one\u2019s favorite approach to assuring that programs behave as desired is based on theorem proving, model checking, or abstract interpretation.", "num_citations": "222\n", "authors": ["1739"]}
{"title": "Incremental evaluation for attribute grammars with application to syntax-directed editors\n", "abstract": " A syntax-directed editor is a tool for structured program development. Such an editor can enforce syntactic correctness incrementally by restricting editing operations to legitimate modifications of the program's context-free derivation tree. However, not all language features can be described by the context-free formalism. To build editors that enforce non-context-free correctness, a more powerful specification technique is needed. In this paper we discuss the advantages of attribute grammars as a specification technique for a syntax-directed editing system. We also present an efficient algorithm for incrementally evaluating attributes as a program tree is derived.", "num_citations": "222\n", "authors": ["1739"]}
{"title": "On the adequacy of program dependence graphs for representing programs\n", "abstract": " Program dependence graphs were introduced by Kuck as an intermediate program representation well suited for performing optimizations, vectorization, and parallelization. There are also additional applications for them as an internal program representation in program development environments.", "num_citations": "205\n", "authors": ["1739"]}
{"title": "A categorized bibliography on incremental computation\n", "abstract": " In many kinds of emnputatiomd contexts, modifications of the input data are to be processed at once so as to have immediate effect on the output. Because small changes in the input to a computation often cause only small changes in the outpu~ the challenge is to compute the new output incrementally by updating parts of the old outpu~ rather than by recomputing the entire output from scratch (as a \u201cbatch computation\u201d). Put another way, the goal is to make use of the solution to one problem instance to find the solution to a \u201cnearby\u201d problem irtstanee. The abstract~ oblem of incremental computation can be phrased as follows: The goal is to compute a function~ on the user\u2019s \u201cinput\u201d data x\u2014where x is often some data structure, such as a tree, graph, or matrix-and to keep the output~(x) updated as the input undergoes changes. An incremental algorithm for computing~ takes as input the \u201cbatch input\u201d x, the \u201cbatch\u00a0\u2026", "num_citations": "197\n", "authors": ["1739"]}
{"title": "Codesurfer/x86\u2014a platform for analyzing x86 executables\n", "abstract": " CodeSurfer/x86 is a prototype system for analyzing x86 executables. It uses a static-analysis algorithm called value-set analysis (VSA) to recover intermediate representations that are similar to those that a compiler creates for a program written in a high-level language. A major challenge in building an analysis tool for executables is in providing useful information about operations involving memory. This is difficult when symbol-table and debugging information is absent or untrusted. CodeSurfer/x86 overcomes these challenges to provide an analyst with a powerful and flexible platform for investigating the properties and behaviors of potentially malicious code (such as COTS components, plugins, mobile code, worms, Trojans, and virus-infected code) using (i) CodeSurfer/x86\u2019s GUI, (ii) CodeSurfer/x86\u2019s scripting language, which provides access to all of the intermediate representations that CodeSurfer/x86\u00a0\u2026", "num_citations": "193\n", "authors": ["1739"]}
{"title": "Precise interprocedural chopping\n", "abstract": " The notion of a program slice, originally introduced by Mark Weiser, is a fundamental operation for addressing many software-engineering problems, including program understanding, debugging, maintenance, testing, and merging. A slice determines either all program elements that might affect a given element(\u201cbackward slicing\u201d) or all elements that could be affected by a given element(\u201cforward slicing\u201d).Jackson and Rollins introduced a related operation, called program chopping, which is a kind of \u201cfiltered\u201d slice: Chopping answers questions of the form\u201cWhat are all the program elements v that serve to transmit effects from a given source elements to a given target element t?\u201d However, Jackson and Rollins define only a limited form of chopping: Among other restrictions, they impose the restriction thats and tbe in the same procedure. This paper solves the unrestricted interprocedural chopping problem, as well as\u00a0\u2026", "num_citations": "168\n", "authors": ["1739"]}
{"title": "Interconvertibility of a class of set constraints and context-free-language reachability\n", "abstract": " We show the interconvertibility of context-free-language reachability problems and a class of set-constraint problems: given a context-free-language reachability problem, we show how to construct a set-constraint problem whose answer gives a solution to the reachability problem; given a set-constraint problem, we show how to construct a context-free-language reachability problem whose answer gives a solution to the set-constraint problem. The interconvertibility of these two formalisms offers a conceptual advantage akin to the advantage gained from the interconvertibility of finite-state automata and regular expressions in formal language theory, namely, a problem can be formulated in whichever formalism is most natural. It also offers some insight into the \u201cO (n 3) bottleneck\u201d for different types of program-analysis problems and allows results previously obtained for context-free-language reachability problems to\u00a0\u2026", "num_citations": "163\n", "authors": ["1739"]}
{"title": "Divine: Discovering variables in executables\n", "abstract": " This paper addresses the problem of recovering variable-like entities when analyzing executables in the absence of debugging information. We show that variable-like entities can be recovered by iterating Value-Set Analysis (VSA), a combined numeric-analysis and pointer-analysis algorithm, and Aggregate Structure Identification, an algorithm to identify the structure of aggregates. Our initial experiments show that the technique is successful in correctly identifying 88% of the local variables and 89% of the fields of heap-allocated objects. Previous techniques recovered 83% of the local variables, but 0% of the fields of heap-allocated objects. Moreover, the values computed by VSA using the variables recovered by our algorithm would allow any subsequent analysis to do a better job of interpreting instructions that use indirect addressing to access arrays and heap-allocated data objects: indirect operands\u00a0\u2026", "num_citations": "156\n", "authors": ["1739"]}
{"title": "Optimal-time incremental semantic analysis for syntax-directed editors\n", "abstract": " Attribute grammars permit the specification of static semantics in an applicative and modular fashion, and thus are a good basis for syntax-directed editors. Such editors represent programs as attributed trees, which are modified by operations such as subtree pruning and grafting. After each modification, a subset of attributes, AFFECTED, requires new values. Membership in AFFECTED is not known a priori; this paper presents an algorithm that identifies attributes in AFFECTED and computes their new values. The algorithm is time-optimal, its cost is proportional to the size of AFFECTED.", "num_citations": "142\n", "authors": ["1739"]}
{"title": "Program specialization via program slicing\n", "abstract": " This paper concerns the use of program slicing to perform a certain kind of program-specialization operation. We show that the specialization operation that slicing performs is different from the specialization operations performed by algorithms for partial evaluation, supercompilation, bifurcation, and deforestation. To study the relationship between slicing and these operations in a simplified setting, we consider the problem of slicing functional programs. We identify two different goals for what we mean by \u201cslicing a functional program\u201d and give algorithms that correspond to each of them.", "num_citations": "139\n", "authors": ["1739"]}
{"title": "A program integration algorithm that accommodates semantics-preserving transformations\n", "abstract": " Given a program Base and two variants, A and B, each created by modifying separate copies of Base, the goal of program integration is to determine whether the modifications interfere, and if they do not, to create an integrated program that includes both sets of changes as well as the portions of Base preserved in both variants. Text-based integration techniques, such as the one used by the Unix diff3 utility, are obviously unsatisfactory because one has no guarantees about how the execution behavior of the integrated program relates to the behaviors of Base, A, and B. The first program-integration algorithm to   provide such guarantees was developed by Horwitz et al.[13]. However, a limitation of that algorithm is that it incorporates no notion of semantics-preserving transformations. This limitation causes the algorithm to be overly conservative in its definition of interference. For example, if one variant changes the\u00a0\u2026", "num_citations": "135\n", "authors": ["1739"]}
{"title": "Pointer analysis for programs with structures and casting\n", "abstract": " Type casting allows a program to access an object as if it had a type different from its declared type. This complicates the design of a pointer-analysis algorithm that treats structure fields as separate objects; therefore, some previous pointer-analysis algorithms \"collapse\" a structure into a single variable. The disadvantage of this approach is that it can lead to very imprecise points-to information. Other algorithms treat each field as a separate object based on its offset and size. While this approach leads to more precise results, the results are not portable because the memory layout of structures is implementation dependent.This paper first describes the complications introduced by type casting, then presents a tunable pointer-analysis framework for handling structures in the presence of casting. Different instances of this framework produce algorithms with different levels of precision, portability, and efficiency\u00a0\u2026", "num_citations": "134\n", "authors": ["1739"]}
{"title": "Recency-abstraction for heap-allocated storage\n", "abstract": " In this paper, we present an abstraction for heap-allocated storage, called the recency-abstraction, that allows abstract-interpretation algorithms to recover some non-trivial information for heap-allocated data objects. As an application of the recency-abstraction, we show how it can resolve virtual-function calls in stripped executables (i.e., executables from which debugging information has been removed). This approach succeeded in resolving 55% of virtual-function call-sites, whereas previous tools for analyzing executables fail to resolve any of the virtual-function call-sites.", "num_citations": "131\n", "authors": ["1739"]}
{"title": "Shape analysis\n", "abstract": " Program Analysis via Graph Reachability Page 1 Shape Analysis Mooly Sagiv Page 2 \u2022 Tel-Aviv University \u2013 D. Amit \u2013 I. Bogudlov \u2013 G. Arnold \u2013 G. Erez \u2013 N. Dor \u2013 T. Lev-Ami \u2013 R. Manevich \u2013 R. Shaham \u2013 A. Rabinovich \u2013 N. Rinetzky \u2013 G. Yorsh \u2013 A. Warshavsky \u2022 Universit\u00e4t des Saarlandes \u2013 J. Bauer \u2013 R. Biber \u2013 R. Wilhelm . . . and also \u2022 University of Wisconsin \u2013 F. DiMaio \u2013 D. Gopan \u2013 A. Loginov \u2013 T. Reps \u2022 IBM Research \u2013 J. Field \u2013 H. Kolodner \u2013 M. Rodeh \u2013 E. Yahav \u2022 Microsoft Research \u2013 J. Berdine \u2013 B. Cook \u2013 G. Ramalingam \u2022 University of Massachusetts \u2013 N. Immerman \u2013 B. Hesse \u2022 Inria \u2013 B. Jeannet Page 3 Shape Analysis [Jones and Muchnick 1981] \u2022 Determine the possible shapes of a dynamically allocated data structure at a given program point Page 4 Programs and Properties \u2022 Dynamically allocated memory \u2022 Recursive data structures \u2022 Recursive procedures \u2022 Concurrency \u2022 Memory safety \u2022 \u2026", "num_citations": "128\n", "authors": ["1739"]}
{"title": "Demand interprocedural program analysis using logic databases\n", "abstract": " This paper describes how algorithms for demand versions of interprocedural program-analysis problems can be obtained from their exhaustive counterparts essentially for free, by applying the so-called magic-sets transformation that was developed in the logic-programming and deductive-database communities. Applications to interprocedural dataflow analysis and interprocedural program slicing are described.1", "num_citations": "128\n", "authors": ["1739"]}
{"title": "Design and implementation of a fine-grained software inspection tool\n", "abstract": " Although software inspection has led to improvements in software quality, many software systems continue to be deployed with unacceptable numbers of errors, even when software inspection is part of the development process. The difficulty of manually verifying that the software under inspection conforms to the rules is partly to blame. We describe the design and implementation of a tool designed to help alleviate this problem. The tool provides mechanisms for fine-grained inspection of software by exposing the results of sophisticated whole-program static analysis to the inspector. The tool computes many static-semantic representations of the program, including an accurate call graph and dependence graph. A whole-program pointer analysis is used to make sure that the representation is precise with respect to aliases induced by pointer usage. Views on the dependence graph and related representations are\u00a0\u2026", "num_citations": "125\n", "authors": ["1739"]}
{"title": "Undecidability of context-sensitive data-dependence analysis\n", "abstract": " A number of program-analysis problems can be tackled by transforming them into certain kinds of graph-reachability problems in labeled directed graphs. The edge labels can be used to filter out paths that are not interest: a path P from vertex s to vertex t only counts as a\u201cvalid connection\u201d between s and t if the word spelled out by P is in a certain language. Often the languages used for such filtering purposes are languages of matching parantheses. In some cases, the matched-parenthesis condition is used to filter out paths with mismatched calls and returns. This leads to so-called \u201ccontext-sensitive\u201d program analyses, such as context-sensitive interprocedural slicing and context-sensitive interprocedural dataflow analysis. In other cases, the matched-parenthesis condition is used to capture a graph-theoretic analog of McCarthy's rules: \u201ccar (cons(x,y)) = x\u201d and \u201ccdr(cons(x,y)) =y\u201d. That is, in the code fragment    c\u00a0\u2026", "num_citations": "106\n", "authors": ["1739"]}
{"title": "Lookahead widening\n", "abstract": " We present lookahead widening, a novel technique for using existing widening and narrowing operators to improve the precision of static analysis. This technique is both self-contained and fully-automatic in the sense that it does not rely on separate analyzes or human involvement. We show how to integrate lookahead widening into existing analyzers with minimal effort. Experimental results indicate that the technique is able to achieve sizable precision improvements at reasonable costs.", "num_citations": "98\n", "authors": ["1739"]}
{"title": "The semantics of program slicing\n", "abstract": " This paper concerns the relationship between the execution behavior of a program and the execution behavior of its slices. Our main results are those stated as the Slicing Theorem and the Termination Theorem. The proof of the Slicing Theorem demonstrates that a slice captures a portion of a program's behavior in the sense that, for any initial state on which the program halts, the program and the slice compute the same sequence of values for each element of the slice. The proof of the Termination Theorem demonstrates that if a program is decomposed into (two or more) slices, the program halts on any state for which all the slices halt. These results are used to provide semantic justification for a program-integration algorithm of Horwitz, Prins, and Reps.", "num_citations": "98\n", "authors": ["1739"]}
{"title": "Debugging via run-time type checking\n", "abstract": " This paper describes the design and implementation of a tool for C programs that provides run-time checks based on type information. The tool instruments a program to monitor the type stored in each memory location. Whenever a value is written into a location, the location\u2019s run-time type tag is updated to match the type of the value. Also, the location\u2019s static type is compared with the value\u2019s type; if there is a mismatch, a warning message is issued. Whenever the value in a location is used, its run-time type tag is checked, and if the type is inappropriate in the context in which the value is being used, an error message is issued. The tool has been used to pinpoint the cause of bugs in several Solaris utilities and Olden benchmarks, usually providing information that is succinct and precise.", "num_citations": "95\n", "authors": ["1739"]}
{"title": "Improved memory-access analysis for x86 executables\n", "abstract": " Over the last seven years, we have developed static-analysis methods to recover a good approximation to the variables and dynamically allocated memory objects of a stripped executable, and to track the flow of values through them. It is relatively easy to track the effects of an instruction operand that refers to a global address (i.e., an access to a global variable) or that uses a stack-frame offset (i.e., an access to a local scalar variable via the frame pointer or stack pointer). In our work, our algorithms are able to provide useful information for close to 100% of such \u201cdirect\u201d uses and defs.               It is much harder for a static-analysis algorithm to track the effects of an instruction operand that uses a non-stack-frame register. These \u201cindirect\u201d uses and defs correspond to accesses to an array or a dynamically allocated memory object. In one study, our approach recovered useful information for only 29% of indirect\u00a0\u2026", "num_citations": "94\n", "authors": ["1739"]}
{"title": "Intermediate-representation recovery from low-level code\n", "abstract": " The goal of our work is to create tools that an analyst can use to understand the workings of COTS components, plugins, mobile code, and DLLs, as well as memory snapshots of worms and virus-infected code. This paper describes how static analysis provides techniques that can be used to recover intermediate representations that are similar to those that can be created for a program written in a high-level language.", "num_citations": "92\n", "authors": ["1739"]}
{"title": "Solving demand versions of interprocedural analysis problems\n", "abstract": " This paper concerns the solution of demand versions of interprocedural analysis problems. In a demand version of a program-analysis problem, some piece of summary information (e.g., the dataflow facts holding at a given point) is to be reported only for a single program element of interest (or a small number of elements of interest). Because the summary information at one program point typically depends on summary information from other points, an important issue is to minimize the number of other points for which (transient) summary information is computed and/or the amount of information computed at those points. The paper describes how algorithms for demand versions of program-analysis problems can be obtained from their exhaustive counterparts essentially for free, by applying the so-called \u201cmagic-sets\u201d transformation that was developed in the logic-programming and deductive-database\u00a0\u2026", "num_citations": "91\n", "authors": ["1739"]}
{"title": "Guided static analysis\n", "abstract": " In static analysis, the semantics of the program is expressed as a set of equations. The equations are solved iteratively over some abstract domain. If the abstract domain is distributive and satisfies the ascending-chain condition, an iterative technique yields the most precise solution for the equations. However, if the above properties are not satisfied, the solution obtained is typically imprecise. Moreover, due to the properties of widening operators, the precision loss is sensitive to the order in which the state-space is explored.               In this paper, we introduce guided static analysis, a framework for controlling the exploration of the state-space of a program. The framework guides the state-space exploration by applying standard static-analysis techniques to a sequence of modified versions of the analyzed program. As such, the framework does not require any modifications to existing analysis techniques, and\u00a0\u2026", "num_citations": "87\n", "authors": ["1739"]}
{"title": "Interprocedural path profiling\n", "abstract": " In path profiling, a program is instrumented with code that counts the number of times particular path fragments of the program are executed. This paper extends the intraprocedural path-profiling technique of Ball and Larus to collect information about interprocedural paths (i.e., paths that may cross procedure boundaries).", "num_citations": "87\n", "authors": ["1739"]}
{"title": "Remote attribute updating for language-based editors\n", "abstract": " A major drawback to the use of attribute grammars in language-based editors has been that attributes can only depend on neighboring attributes in a program's syntax tree. This paper concerns new attribute-grammar-based methods that, for a suitable class of grammars, overcome this fundamental limitation. The techniques presented allow the updating algorithm to skip over arbitrarily large sections of the tree that more straightforward updating methods visit node by node. These techniques are then extended to deal with aggregate values, so that the attribute updating procedure need only follow dependencies due to a changed component of an aggregate value. Although our methods work only for a restricted class of attribute grammars, satisfying the necessary restrictions should not place an undue burden on the writer of the grammar.", "num_citations": "87\n", "authors": ["1739"]}
{"title": "Extended weighted pushdown systems\n", "abstract": " Recent work on weighted-pushdown systems shows how to generalize interprocedural-dataflow analysis to answer \u201cstack-qualified queries\u201d, which answer the question \u201cwhat dataflow values hold at a program node for a particular set of calling contexts?\u201d The generalization, however, does not account for precise handling of local variables. Extended-weighted-pushdown systems address this issue, and provide answers to stack-qualified queries in the presence of local variables as well.", "num_citations": "86\n", "authors": ["1739"]}
{"title": "The why and wherefore of the Cornell Program Synthesizer\n", "abstract": " The Cornell Program Synthesizer is a syntax-directed programming environment that has been used in introductory programming courses since June, 1979. We present our experience with the Synthesizer by introducing its main features, by presenting our basic principles of design, and by discussing important design decisions.", "num_citations": "86\n", "authors": ["1739"]}
{"title": "The semantics of program slicing and program integration\n", "abstract": " A slice of a program with respect to a program point p and variable x consists of all statements of the program that might affect the value of x at point p. Slices can be extracted particularly easily from a program representation called a program dependence graph, originally introduced as an intermediate program representation for performing optimizing, vectorizing, and parallelizing transformations. Such slices are of a slightly restricted form: rather than permitting a program to be sliced with respect to program point p and an arbitrary variable, a slice must be taken with respect to a variable that is defined at or used at p.             This paper concerns the relationship between the execution behavior of a program and the execution behavior of its slices. Our main results about slicing are those stated as the Slicing Theorem and the Termination Theorem. The Slicing Theorem demonstrates that a slice captures a\u00a0\u2026", "num_citations": "85\n", "authors": ["1739"]}
{"title": "Directed Proof Generation for Machine Code\n", "abstract": " We present the algorithms used in McVeto (Machine-Code VErification TOol), a tool to check whether a stripped machine-code program satisfies a safety property. The verification problem that McVeto addresses is challenging because it cannot assume that it has access to (i) certain structures commonly relied on by source-code verification tools, such as control-flow graphs and call-graphs, and (ii) meta-data, such as information about variables, types, and aliasing. It cannot even rely on out-of-scope local variables and return addresses being protected from the program\u2019s actions. What distinguishes McVeto from other work on software model checking is that it shows how verification of machine-code can be performed, while avoiding conventional techniques that would be unsound if applied at the machine-code level.", "num_citations": "81\n", "authors": ["1739"]}
{"title": "Extracting output formats from executables\n", "abstract": " We describe the design and implementation of FFE/x86 (File-Format Extractor for x86), an analysis tool that works on stripped executables (i.e., neither source code nor debugging information need be available) and extracts output data formats, such as file formats and network packet formats. We first construct a hierarchical finite state machine (HFSM) that over-approximates the output data format. An HFSM defines a language over the operations used to generate output data. We use value-set analysis (VSA) and aggregate structure identification (ASI) to annotate HFSMs with information that partially characterizes some of the output data values. VSA determines an over-approximation of the set of addresses and integer values that each data object can hold at each program point, and ASI analyzes memory accesses in the program to recover information about the structure of aggregates. A series of filtering\u00a0\u2026", "num_citations": "81\n", "authors": ["1739"]}
{"title": "Tool support for fine-grained software inspection\n", "abstract": " Software inspection is a proven technique to improve quality and reduce costs. Detailed source code inspections are an important part of the formal inspection process, but they require a significant time investment. Research advances in static program analysis can reduce the inspection time required. By calculating answers to standard inspection questions, CodeSurfer frees experts to focus their efforts on more challenging inspection issues. CodeSurfer provides access to and answers queries about the system-dependence graph representation of programs, and can be integrated with other tools.", "num_citations": "76\n", "authors": ["1739"]}
{"title": "Shape analysis as a generalized path problem\n", "abstract": " This paper concerns a method for approximating the possible \u201cshapes\u201d that heap-allocated structures in a program can take on. We present a new approach to finding solutions to shape-analysis problems that involves formulating them as generalized graph-reachability problems. The reachability problem that arises is not an ordinary reachability problem (eg, transitive closure), but one in which a path is considered to connect two vertices only if the concatenation of the labels on the edges of the path is a word in a certain context-free language. This graph-reachability approach allows us to give polynomial bounds on the running time of an algorithm for shape analysis. It also permits us to obtain a demand algorithm for shape analysis.(In a demand algorithm, the goal is to determine shape information selectively\u2014ie, for particular variables at particular points in the program, rather than for every variable at every\u00a0\u2026", "num_citations": "68\n", "authors": ["1739"]}
{"title": "Algebraic properties of program integration\n", "abstract": " The need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to merge programs by hand. The program-integration algorithm proposed by Horwitz, Prins, and Reps provides a way to create a semantics-based tool for integrating a base program with two or more variants. The integration algorithm is based on the assumption that any change in the behavior, rather than the text, of a program variant is significant and must be incorporated in the merged program. An integration system based on this algorithm will determine whether the variants incorporate interfering changes, and, if they do not, create an integrated program that includes all changes as well as all features of the base program that are preserved in all variants. To determine this information, the algorithm employs a program representation that is similar to the program dependence\u00a0\u2026", "num_citations": "68\n", "authors": ["1739"]}
{"title": "Low-level library analysis and summarization\n", "abstract": " Programs typically make extensive use of libraries, including dynamically linked libraries, which are often not available in source-code form, and hence not analyzable by tools that work at source level (i.e., that analyze intermediate representations created from source code). A common approach is to write library models by hand. A library model is a collection of function stubs and variable declarations that capture some aspect of the library code\u2019s behavior. Because these are hand-crafted, they are likely to contain errors, which may cause an analysis to return incorrect results.               This paper presents a method to construct summary information for a library function automatically by analyzing its low-level implementation (i.e., the library\u2019s binary).", "num_citations": "64\n", "authors": ["1739"]}
{"title": "Non-linear reasoning for invariant synthesis\n", "abstract": " Automatic generation of non-linear loop invariants is a long-standing challenge in program analysis, with many applications. For instance, reasoning about exponentials provides a way to find invariants of digital-filter programs, and reasoning about polynomials and/or logarithms is needed for establishing invariants that describe the time or memory usage of many well-known algorithms. An appealing approach to this challenge is to exploit the powerful recurrence-solving techniques that have been developed in the field of computer algebra, which can compute exact characterizations of non-linear repetitive behavior. However, there is a gap between the capabilities of recurrence solvers and the needs of program analysis: (1) loop bodies are not merely systems of recurrence relations---they may contain conditional branches, nested loops, non-deterministic assignments, etc., and (2) a client program analyzer must\u00a0\u2026", "num_citations": "63\n", "authors": ["1739"]}
{"title": "A method for symbolic computation of abstract operations\n", "abstract": " This paper helps to bridge the gap between (i) the use of logic for specifying program semantics and performing program analysis, and (ii) abstract interpretation. Many operations needed by an abstract interpreter can be reduced to the problem of symbolic abstraction: the symbolic abstraction of a formula \u03d5 in logic, denoted by, is the most-precise value in abstract domain that over-approximates the meaning of \u03d5. We present a parametric framework that, given and, implements. The algorithm computes successively better over-approximations of. Because it approaches from \u201cabove\u201d, if it is taking too much time, a safe answer can be returned at any stage. Moreover, the framework is \u201cdual-use\u201d: in addition to its applications in abstract interpretation, it provides a new way for an SMT (Satisfiability Modulo Theories) solver to perform unsatisfiability checking: given, the condition implies that \u03d5 is unsatisfiable.", "num_citations": "63\n", "authors": ["1739"]}
{"title": "Detecting program components with equivalent behaviors\n", "abstract": " The execution behavior of a program component is defined as the sequence of values produced at the component during program execution.  This paper presents an efficient algorithm for detecting program components ? in one or more program ? that exhibit identical execution behaviors.  The algorithm operates on a new graph representation  for programs that combines features of static-single- assignment forms and program dependence graphs.  The result provides insight into the relationship between execution behaviors and (control and flow) dependence in the program.  The algorithm, called the Sequence-Congruence Algorithm, is applicable to programs written in a language that includes scalar variables and constants, assignment statements, conditional statements, and while-loops.  The Sequence-Congruence Algorithm can be used as the basis for an algorithm for integrating program variants.", "num_citations": "62\n", "authors": ["1739"]}
{"title": "Model checking x86 executables with CodeSurfer/x86 and WPDS++\n", "abstract": " This paper presents a toolset for model checking x86 executables. The members of the toolset are CodeSurfer/x86, WPDS++, and the Path Inspector. CodeSurfer/x86 is used to extract a model from an executable in the form of a weighted pushdown system. WPDS++ is a library for answering generalized reachability queries on weighted pushdown systems. The Path Inspector is a software model checker built on top of CodeSurfer and WPDS++ that supports safety queries about the program\u2019s possible control configurations.", "num_citations": "60\n", "authors": ["1739"]}
{"title": "Code vectors: Understanding programs through embedded abstracted symbolic traces\n", "abstract": " With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied.", "num_citations": "57\n", "authors": ["1739"]}
{"title": "An incremental algorithm for maintaining the dominator tree of a reducible flowgraph\n", "abstract": " We present a new incremental algorithm for the problem of maintaining the dominator tree of a reducible flowgraph as the flowgraph undergoes changes such as the insertion and deletion of edges. Such an algorithm has applications in incremental dataflow analysis and incremental compilation.", "num_citations": "56\n", "authors": ["1739"]}
{"title": "A system for generating static analyzers for machine instructions\n", "abstract": " This paper describes the design and implementation of a language for specifying the semantics of an instruction set, along with a run-time system to support the static analysis of executables written in that instruction set. The work advances the state of the art by creating multiple analysis phases from a specification of the concrete operational semantics of the language to be analyzed.", "num_citations": "55\n", "authors": ["1739"]}
{"title": "Program generalization for software reuse: From C to C++\n", "abstract": " We consider the problem of software generalization: Given a program component C, create a parameterized program component C\u2032 such that C\u2032 is usable in a wider variety of syntactic contexts than C. Furthermore, C\u2032 should be a semantically meaningful generalization of C; namely, there must exist an instantiation of C\u2032 that is equivalent in functionality to C.In this paper, we present an algorithm that generalizes C functions via type inference. The original functions operate on specific data types; the result of generalization is a collection of C++ function templates that operate on parameterized types. This version of the generalization problem is useful in the context of converting existing C programs to C++.", "num_citations": "54\n", "authors": ["1739"]}
{"title": "Language processing in program editors\n", "abstract": "", "num_citations": "54\n", "authors": ["1739"]}
{"title": "Compositional recurrence analysis revisited\n", "abstract": " Compositional recurrence analysis (CRA) is a static-analysis method based on a combination of symbolic analysis and abstract interpretation. This paper addresses the problem of creating a context-sensitive interprocedural version of CRA that handles recursive procedures. The problem is non-trivial because there is an \"impedance mismatch\" between CRA, which relies on analysis techniques based on regular languages (i.e., Tarjan's path-expression method), and the context-free-language underpinnings of context-sensitive analysis.   We show how to address this impedance mismatch by augmenting the CRA abstract domain with additional operations. We call the resulting algorithm Interprocedural CRA (ICRA). Our experiments with ICRA show that it has broad overall strength compared with several state-of-the-art software model checkers.", "num_citations": "53\n", "authors": ["1739"]}
{"title": "Efficient comparison of program slices\n", "abstract": " Theslice of a program with respect to a componentc is a projection of the program that includes all components that might affect (either directly or transitively) the values of the variables used atc. Slices can be extracted particularly easily from a program representation called a program dependence graph, originally introduced as an intermediate program representation for performing optimizing, vectorizing, and parallelizing transformations. This paper presents a linear-time algorithm for determining whether two slices of a program dependence graph are isomorphic.", "num_citations": "53\n", "authors": ["1739"]}
{"title": "Improving Pushdown System Model Checking\n", "abstract": " In this paper, we reduce pushdown system (PDS) model checking to a graph-theoretic problem, and apply a fast graph algorithm to improve the running time for model checking. Several other PDS questions and techniques can be carried out in the new setting, including witness tracing and incremental analysis, each of which benefits from the fast graph-based algorithm.", "num_citations": "50\n", "authors": ["1739"]}
{"title": "On the computational complexity of incremental algorithms\n", "abstract": " Our results, together with some previously known ones, shed light on the organization of the complexity hierarchy that exists when incremental-computation problems are classified according to their incremental complexity with respect to locally persistent algorithms. In particular, these results separate the classes of P-time incremental problems, inherently Exp-time incremental problems, and non-incremental problems.", "num_citations": "50\n", "authors": ["1739"]}
{"title": "TSL: A system for generating abstract interpreters and its application to machine-code analysis\n", "abstract": " This article describes the design and implementation of a system, called Tsl (for Transformer Specification Language), that provides a systematic solution to the problem of creating retargetable tools for analyzing machine code. Tsl is a tool generator---that is, a metatool---that automatically creates different abstract interpreters for machine-code instruction sets. The most challenging technical issue that we faced in designing Tsl was how to automate the generation of the set of abstract transformers for a given abstract interpretation of a given instruction set. From a description of the concrete operational semantics of an instruction set, together with the datatypes and operations that define an abstract domain, Tsl automatically creates the set of abstract transformers for the instructions of the instruction set. Tsl advances the state-of-the-art in program analysis because it provides two dimensions of parameterizability:(i) a\u00a0\u2026", "num_citations": "46\n", "authors": ["1739"]}
{"title": "A next-generation platform for analyzing executables\n", "abstract": " In recent years, there has been a growing need for tools that an analyst can use to understand the workings of COTS components, plugins, mobile code, and DLLs, as well as memory snapshots of worms and virus-infected code. Static analysis provides techniques that can help with such problems; however, there are several obstacles that must be overcome:               \u2013 For many kinds of potentially malicious programs, symbol-table and debugging information is entirely absent. Even if it is present, it cannot be relied upon.               \u2013 To understand memory-access operations, it is necessary to determine the set of addresses accessed by each operation. This is difficult because                                                                                While some memory operations use explicit memory addresses in the instruction (easy), others use indirect addressing via address expressions (difficult\u00a0\u2026", "num_citations": "46\n", "authors": ["1739"]}
{"title": "Abstract domains of affine relations\n", "abstract": " This article considers some known abstract domains for affine-relation analysis (ARA), along with several variants, and studies how they relate to each other. The various domains represent sets of points that satisfy affine relations over variables that hold machine integers and are based on an extension of linear algebra to modules over a ring (in particular, arithmetic performed modulo 2w, for some machine-integer width w). We show that the abstract domains of M\u00fcller-Olm/Seidl (MOS) and King/S\u00f8ndergaard (KS) are, in general, incomparable. However, we give sound interconversion methods. In other words, we give an algorithm to convert a KS element vKS to an overapproximating MOS element vMOS\u2014that is, \u03b3 (vKS) \u2286 \u03b3 (vMOS\u2014as well as an algorithm to convert an MOS element wMOS to an overapproximating KS element wKS\u2014that is, \u03b3 (wMOS) \u2286 \u03b3 (wKS). The article provides insight on the range of\u00a0\u2026", "num_citations": "44\n", "authors": ["1739"]}
{"title": "Analyzing stripped device-driver executables\n", "abstract": " This paper sketches the design and implementation of Device-Driver Analyzer for x86 (DDA/x86), a prototype analysis tool for finding bugs in stripped Windows device-driver executables (i.e., when neither source code nor symbol-table/debugging information is available), and presents a case study. DDA/x86 was able to find known bugs (previously discovered by source-code-based analysis tools) along with useful error traces, while having a reasonably low false-positive rate.               This work represents the first known application of automatic program verification/analysis to stripped industrial executables, and allows one to check that an executable does not violate known API usage rules (rather than simply trusting that the implementation is correct).", "num_citations": "44\n", "authors": ["1739"]}
{"title": "The care and feeding of wild-caught mutants\n", "abstract": " Mutation testing of a test suite and a program provides a way to measure the quality of the test suite. In essence, mutation testing is a form of sensitivity testing: by running mutated versions of the program against the test suite, mutation testing measures the suite's sensitivity for detecting bugs that a programmer might introduce into the program. This paper introduces a technique to improve mutation testing that we call wild-caught mutants; it provides a method for creating potential faults that are more closely coupled with changes made by actual programmers. This technique allows the mutation tester to have more certainty that the test suite is sensitive to the kind of changes that have been observed to have been made by programmers in real-world cases.", "num_citations": "43\n", "authors": ["1739"]}
{"title": "Program analysis using weighted pushdown systems\n", "abstract": " Pushdown systems (PDSs) are an automata-theoretic formalism for specifying a class of infinite-state transition systems. Infiniteness comes from the fact that each configuration    in the state space consists of a (formal) \u201ccontrol location\u201d p coupled with a stack S of unbounded size. PDSs can model program paths that have matching calls and returns, and automaton-based representations allow analysis algorithms to account for the infinite control state space of recursive programs.               Weighted pushdown systems (WPDSs) are a generalization of PDSs that add a general \u201cblack-box\u201d abstraction for program data (through weights). WPDSs also generalize other frameworks for interprocedural analysis, such as the Sharir-Pnueli functional approach.               This paper surveys recent work in this area, and establishes a few new connections with existing work.", "num_citations": "43\n", "authors": ["1739"]}
{"title": "Illustrating interference in interfering versions of programs\n", "abstract": " The need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to merge programs by hand. The program-integration algorithm recently proposed by S. Horwitz, J. Prins, and T. Reps provides a way to create a semantics-based tool for program integration. The integration algorithm is based on the assumption that any change in the behavior, rather than the text, of a program variant is significant and must be preserved in the merged program. An integration system based on this algorithm will determine whether the programs incorporate interfering changes, and, if they do not, will automatically create an integrated program that includes all changes as well as all features that are preserved in all variants.", "num_citations": "42\n", "authors": ["1739"]}
{"title": "Automated resource analysis with Coq proof objects\n", "abstract": " This paper addresses the problem of automatically performing resource-bound analysis, which can help programmers understand the performance characteristics of their programs. We introduce a method for resource-bound inference that (i) is compositional, (ii) produces machine-checkable certificates of the resource bounds obtained, and (iii) features a sound mechanism for user interaction if the inference fails. The technique handles recursive procedures and has the ability to exploit any known program invariants. An experimental evaluation with an implementation in the tool Pastis shows that the new analysis is competitive with state-of-the-art resource-bound tools while also creating Coq certificates.", "num_citations": "41\n", "authors": ["1739"]}
{"title": "There\u2019s plenty of room at the bottom: Analyzing and verifying machine code\n", "abstract": " This paper discusses the obstacles that stand in the way of doing a good job of machine-code analysis. Compared with analysis of source code, the challenge is to drop all assumptions about having certain kinds of information available (variables, control-flow graph, call-graph, etc.) and also to address new kinds of behaviors (arithmetic on addresses, jumps to \u201chidden\u201d instructions starting at positions that are out of registration with the instruction boundaries of a given reading of an instruction stream, self-modifying code, etc.).               The paper describes some of the challenges that arise when analyzing machine code, and what can be done about them. It also provides a rationale for some of the design decisions made in the machine-code-analysis tools that we have built over the past few years.", "num_citations": "41\n", "authors": ["1739"]}
{"title": "On the sequential nature of interprocedural program-analysis problems\n", "abstract": " In this paper, we study two interprocedural program-analysis problems\u2014interprocedural slicing and interprocedural dataflow analysis\u2014 and present the following results:                                        \u2022 Interprocedural slicing is log-space complete forP.                                                           \u2022 The problem of obtaining \u201cmeet-over-all-valid-paths\u201d solutions to interprocedural versions of distributive dataflow-analysis problems isP-hard.                                                           \u2022 Obtaining \u201cmeet-over-all-valid-paths\u201d solutions to interprocedural versions of distributive dataflow-analysis problems that involve finite sets of dataflow facts (such as the classical \u201cgen/kill\u201d problems) is log-space complete forP.                                    These results provide evidence that there do not exist fast (N\u2110-class) parallel algorithms for interprocedural slicing and precise interprocedural dataflow analysis (unlessP =N\u2110). That is, it is unlikely that there are\u00a0\u2026", "num_citations": "38\n", "authors": ["1739"]}
{"title": "Interactive proof checking\n", "abstract": " Knowledge of logical inference rules allows a specialized proof editor to provide a user with feedback about errors in a proof under development. Providing such feedback involves checking a collection of constraints on the strings of the proof language. Because attribute grammars allow such constraints to be expressed in a modular, declarative fashion, they are a suitable underlying formalism for a proof-checking editor. This paper discusses how an attribute grammar can be used in an editor for partial-correctness program proofs in Hoare-style logic, where verification conditions are proved using the sequent calculus.", "num_citations": "35\n", "authors": ["1739"]}
{"title": "PMAF: an algebraic framework for static analysis of probabilistic programs\n", "abstract": " Automatically establishing that a probabilistic program satisfies some property \u03d5 is a challenging problem. While a sampling-based approach\u2014which involves running the program repeatedly\u2014can suggest that \u03d5 holds, to establish that the program satisfies \u03d5, analysis techniques must be used. Despite recent successes, probabilistic static analyses are still more difficult to design and implement than their deterministic counterparts. This paper presents a framework, called PMAF, for designing, implementing, and proving the correctness of static analyses of probabilistic programs with challenging features such as recursion, unstructured control-flow, divergence, nondeterminism, and continuous distributions. PMAF introduces pre-Markov algebras to factor out common parts of different analyses. To perform interprocedural analysis and to create procedure summaries, PMAF extends ideas from non-probabilistic\u00a0\u2026", "num_citations": "34\n", "authors": ["1739"]}
{"title": "Bilateral algorithms for symbolic abstraction\n", "abstract": " Given a concrete domain , a concrete operation , and an abstract domain , a fundamental problem in abstract interpretation is to find the best abstract transformer                  that over-approximates \u03c4. This problem, as well as several other operations needed by an abstract interpreter, can be reduced to the problem of symbolic abstraction: the symbolic abstraction of a formula \u03d5 in logic , denoted by , is the best value in  that over-approximates the meaning of \u03d5. When the concrete semantics of \u03c4 is defined in  using a formula \u03d5                                    \u03c4                  that specifies the relation between input and output states, the best abstract transformer \u03c4                 # can be computed as .               In this paper, we present a new framework for performing symbolic abstraction, discuss its properties, and present several instantiations for various logics and abstract domains. The key\u00a0\u2026", "num_citations": "34\n", "authors": ["1739"]}
{"title": "Synthesis of machine code from semantics\n", "abstract": " In this paper, we present a technique to synthesize machine-code instructions from a semantic specification, given as a Quantifier-Free Bit-Vector (QFBV) logic formula. Our technique uses an instantiation of the Counter-Example Guided Inductive Synthesis (CEGIS) framework, in combination with search-space pruning heuristics to synthesize instruction-sequences. To counter the exponential cost inherent in enumerative synthesis, our technique uses a divide-and-conquer strategy to break the input QFBV formula into independent sub-formulas, and synthesize instructions for the sub-formulas. Synthesizers created by our technique could be used to create semantics-based binary rewriting tools such as optimizers, partial evaluators, program obfuscators/de-obfuscators, etc. Our experiments for Intel's IA-32 instruction set show that, in comparison to our baseline algorithm, our search-space pruning heuristics reduce\u00a0\u2026", "num_citations": "32\n", "authors": ["1739"]}
{"title": "PostHat and all that: Attaining most-precise inductive invariants\n", "abstract": " In abstract interpretation, the choice of an abstract domain fixes a limit on the precision of the inductive invariants that one can express; however, for a given abstract domain A, there is a most-precise (\u201cstrongest\u201d,\u201cbest\u201d) inductive A-invariant for each program. Many techniques have been developed in abstract interpretation for finding overapproximate solutions, but only a few algorithms have been given that can achieve the fundamental limits that abstract-interpretation theory establishes. In this paper, we present an algorithm that solves the following problem:", "num_citations": "32\n", "authors": ["1739"]}
{"title": "A Generalization of Staalmarck's Method\n", "abstract": " This paper gives an account of St\u00e5lmarck\u2019s method for validity checking of propositional-logic formulas, and explains each of the key components in terms of concepts from the field of abstract interpretation. We then use these insights to present a framework for propositional-logic validity-checking algorithms that is parametrized by an abstract domain and operations on that domain. St\u00e5lmarck\u2019s method is one instantiation of the framework; other instantiations lead to new decision procedures for propositional logic.", "num_citations": "32\n", "authors": ["1739"]}
{"title": "Semantics of program representation graphs\n", "abstract": " Program representation graphs (PRGs) are an intermediate representation for programs.(They are closely related to program dependence graphs.) In this paper, we develop a mathematical semantics for PRGs that, inspired by Kahn\u2019s semantics for a parallel programming language, interprets PRGs as data\ufb02ow graphs. We also study the relationship between this semantics and the standard operational semantics of programs. We show that the semantics of PRGs is more de\ufb01ned than the standard operational semantics, and (ii) for states on which a program terminates normally, the PRG semantics is identical to the standard operational semantics.", "num_citations": "29\n", "authors": ["1739"]}
{"title": "Symbolic analysis via semantic reinterpretation\n", "abstract": " The paper presents a novel technique to create implementations of the basic primitives used in symbolic program analysis: forward symbolic evaluation, weakest liberal precondition, and symbolic composition. We used the technique to create a system in which, for the cost of writing just one specification\u2014an interpreter for the programming language of interest\u2014one obtains automatically-generated, mutually-consistent implementations of all three symbolic-analysis primitives. This can be carried out even for languages with pointers and address arithmetic. Our implementation has been used to generate symbolic-analysis primitives for x86 and PowerPC.", "num_citations": "28\n", "authors": ["1739"]}
{"title": "On competitive on-line algorithms for the dynamic priority-ordering problem\n", "abstract": " The vertices of a directed acyclic graph (DAG) are correctly prioritized if every vertex v in the graph is assigned a priority, denoted by priority(v), such that if there is an edge in the DAG from vertex v to vertex w then priority(v)<priority(w). The dynamic priority-ordering problem is to maintain a correct prioritization of the graph as the DAG is modified. We show that the Alpern et al. algorithm for this problem does not have a constant competitive ratio, where the cost of the algorithm is measured in terms of the number of primitive priority-manipulation operations. The proof shows that there exists no algorithm for the problem that has a constant competitive ratio, as long as the allowed primitive priority-manipulation operations satisfy a simple property. The proof also shows that there exists no algorithm for the problem of maintaining a topological-sort ordering that has a constant competitive ratio.", "num_citations": "27\n", "authors": ["1739"]}
{"title": "Closed forms for numerical loops\n", "abstract": " This paper investigates the problem of reasoning about non-linear behavior of simple numerical loops. Our approach builds on classical techniques for analyzing the behavior of linear dynamical systems. It is well-known that a closed-form representation of the behavior of a linear dynamical system can always be expressed using algebraic numbers, but this approach can create formulas that present an obstacle for automated-reasoning tools. This paper characterizes when linear loops have closed forms in simpler theories that are more amenable to automated reasoning. The algorithms for computing closed forms described in the paper avoid the use of algebraic numbers, and produce closed forms expressed using polynomials and exponentials over rational numbers. We show that the logic for expressing closed forms is decidable, yielding decision procedures for verifying safety and termination of a class of\u00a0\u2026", "num_citations": "25\n", "authors": ["1739"]}
{"title": "Method for representing information in a highly compressed fashion\n", "abstract": " CFLOBDDs are a new compressed representation of functions over Boolean-valued arguments. They provide an alternative to the now-standard representation provided by Ordered Binary Decision Diagrams (OBDDs) and Multi-Terminal Binary Decision Diagrams (MTBDDs)(also known as Algebraic Decision Diagrams (ADDs)). CFLOBDDs share many of the good properties of OBDDs and MTBDDs, but can lead to data structures of drastically smaller size\u2014exponentially smaller than OBDDs and MTBDDs, in fact. That is, OBDDs and MTBDDs are data structures that\u2014in the best case\u2014yield an exponential reduction in the size of the representation of a function (ie, compared with the size of the decision tree for the function). In contrast, a CFLOBDD\u2014again, in the best case\u2014yields a doubly exponential reduction in the size of the representation of a function. Obviously, not every function has such a highly\u00a0\u2026", "num_citations": "25\n", "authors": ["1739"]}
{"title": "Support for integrating program variants in an environment for programming in the large\n", "abstract": " CiNii \u8ad6\u6587 - Support for Integrating Program Variants in an Environment for Programming in the Large CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092 \u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Support for Integrating Program Variants in an Environment for Programming in the Large REPS T. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 REPS T. \u53ce\u9332 \u520a\u884c\u7269 Proceeding of the International Workshop on Software Version and Configuration Control Proceeding of the International Workshop on Software Version and Configuration Control, 197-216, 1988 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30de\u30cd\u30b8\u30e1\u30f3\u30c8\uff1a\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2 \u69cb\u6210\u7ba1\u7406\u3068\u4fdd\u5b88\u7ba1\u7406 \u677e\u5c3e\u8c37 \u5fb9 \u60c5\u5831\u51e6\u7406 33(8), 945-953, 1992-08-15 \u53c2\u8003\u6587\u732e31\u4ef6 \u88ab\u5f15\u7528\u2026", "num_citations": "24\n", "authors": ["1739"]}
{"title": "A relational abstraction for functions\n", "abstract": " This paper concerns the abstraction of sets of functions for use in abstract interpretation. The paper gives an overview of existing methods, which are illustrated with applications to shape analysis, and formalizes a new family of relational abstract domains that allows sets of functions to be abstracted more precisely than with known approaches, while being still machine-representable.", "num_citations": "23\n", "authors": ["1739"]}
{"title": "\u201cMaximal-munch\u201d tokenization in linear time\n", "abstract": " The lexical-analysis (or scanning) phase of a compiler attempts to partition an input string into a sequence of tokens. The convention in most languages is that the input is scanned left to right, and each token identified is a \u201cmaximal munch\u201d of the remaining input\u2014the longest prefix of the remaining input that is a token of the language. Although most of the standard compiler textbooks present a way to perform maximal-munch tokenization, the algorithm they describe is one that, for certain sets of token definitions, can cause the scanner to exhibit quadratic behavior in the worst case. In the article, we show that maximal-munch tokenization can always be performed in time linear in the size of the input.", "num_citations": "23\n", "authors": ["1739"]}
{"title": "An improved algorithm for slicing machine code\n", "abstract": " Machine-code slicing is an important primitive for building binary analysis and rewriting tools, such as taint trackers, fault localizers, and partial evaluators. However, it is not easy to create a machine-code slicer that exhibits a high level of precision. Moreover, the problem of creating such a tool is compounded by the fact that a small amount of local imprecision can be amplified via cascade effects.   Most instructions in instruction sets such as Intel's IA-32 and ARM are multi-assignments: they have several inputs and several outputs (registers, flags, and memory locations). This aspect of the instruction set introduces a granularity issue during slicing: there are often instructions at which we would like the slice to include only a subset of the instruction's semantics, whereas the slice is forced to include the entire instruction. Consequently, the slice computed by state-of-the-art tools is very imprecise, often including\u00a0\u2026", "num_citations": "22\n", "authors": ["1739"]}
{"title": "Newtonian program analysis via tensor product\n", "abstract": " Recently, Esparza et al. generalized Newton's method--a numerical-analysis algorithm for finding roots of real-valued functions---to a method for finding fixed-points of systems of equations over semirings. Their method provides a new way to solve interprocedural dataflow-analysis problems. As in its real-valued counterpart, each iteration of their method solves a simpler``linearized''problem. One of the reasons this advance is exciting is that some numerical analysts have claimed that``all'effective and fast iterative [numerical] methods are forms (perhaps very disguised) of Newton's method.''However, there is an important difference between the dataflow-analysis and numerical-analysis contexts: when Newton's method is used on numerical-analysis problems, multiplicative commutativity is relied on to rearrange expressions of the form``c* X+ X* d''into``(c+ d)* X.''Such equations correspond to path problems\u00a0\u2026", "num_citations": "21\n", "authors": ["1739"]}
{"title": "Recovery of variables and heap structure in x86 executables\n", "abstract": " This paper addresses two problems that arise when analyzing executables: (1) recovering variable-like quantities in the absence of symbol-table and debugging information, and (2) recovering useful information about objects allocated in the heap.", "num_citations": "21\n", "authors": ["1739"]}
{"title": "Neural attribute machines for program generation\n", "abstract": " Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients. Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar. Neural Attribute Machines (NAMs) are equipped with a logical machine that represents the underlying grammar, which is used to teach the constraints to the neural machine by (i) augmenting the input sequence, and (ii) optimizing a custom loss function. Unlike traditional RNNs, NAMs are exposed to the grammar, as well as samples from the language of the grammar. During generation, NAMs make significantly fewer violations of the constraints of the underlying grammar than RNNs trained only on samples from the language of the grammar.", "num_citations": "20\n", "authors": ["1739"]}
{"title": "Semantic foundations of binding-time analysis for imperative programs\n", "abstract": " This paper examines the role of dependence analysis in defimng bindingtime analyses (BTAs) for imperative programs and in establishing that such BTAs are safe. In particular, we are concerned with characterizing safety conditions under which a program specialize that uses the results of a BTA is guaranteed to terminate. Our safety conditions are formalized wa semantic characterizations of the statements in a program along two dimensions: srartc versus dynamic, and finite versus injinife. This permits us to give a semantic definition of \u201cstatic-infinite computation\u201d, a concept that has not been previously formalized. To illustrate the concepts, we present three different BTAs for an imperative language, we show that two of them me safe in the absence of \u201cstatic-infinite computations\u201d. In developing these notions, we make use of program represenrarion graphs, which are a program representation similar to the\u00a0\u2026", "num_citations": "20\n", "authors": ["1739"]}
{"title": "Computational divided differencing and divided-difference arithmetics\n", "abstract": " Tools for computational differentiation transform a program that computes a numerical function F(x) into a related program that computes F\u2032(x) (the derivative of F). This paper describes how techniques similar to those used in computational-differentiation tools can be used to implement other program transformations\u2014in particular, a variety of transformations for computational divided differencing. The specific technical contributions of the paper are as follows:               \u2013 It presents a program transformation that, given a numerical function F(x) defined by a program, creates a program that computes F[x 0, x 1], the first divided difference of F(x), where $$F\\left[ {x_0 ,x_1 } \\right]\\mathop  = \\limits^{def} \\left\\{ {\\begin{array}{*{20}c}   {\\frac{{F\\left( {x_0 } \\right) - F\\left( {x_1 } \\right)}}{{x_0  - x_0 }}{\\text{               }}}  \\\\   {\\frac{d}{{dz}}F\\left( z \\right),\\operatorname{evaluatedatz}  = x_0 }  \\\\ \\end{array} } \\right.\\begin{array}{*{20}c\u00a0\u2026", "num_citations": "18\n", "authors": ["1739"]}
{"title": "Interprocedural path profiling and the interprocedural express-lane transformation\n", "abstract": " The contributions of this thesis can be broadly divided into two categories: we present novel path-profiling techniques, and we present techniques for performing the express-lane transformation, a program transformation that duplicates frequently executed paths in the hope that better data-flow facts result along those paths.", "num_citations": "18\n", "authors": ["1739"]}
{"title": "A theory of program modifications\n", "abstract": " The need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to merge programs by hand. The program-integration algorithm proposed by Horwitz, Prins, and Reps provides a way to create a semantics-based tool for integrating a base program with two or more variants. The integration algorithm is based on the assumption that any change in the behaviour, rather than the text, of a program variant is significant and must be preserved in the merged program. An integration system based on this algorithm will determine whether the variants incorporate interfering changes, and, if they do not, create an integrated program that includes all changes as well as all features of the base program that are preserved in all variants.             This paper studies the algebraic properties of the program-integration operation, such as whether there is a law of\u00a0\u2026", "num_citations": "18\n", "authors": ["1739"]}
{"title": "Learning from, understanding, and supporting DevOps artifacts for Docker\n", "abstract": " With the growing use of DevOps tools and frameworks, there is an increased need for tools and techniques that support more than code. The current state-of-the-art in static developer assistance for tools like Docker is limited to shallow syntactic validation. We identify three core challenges in the realm of learning from, understanding, and supporting developers writing DevOps artifacts: (i) nested languages in DevOps artifacts, (ii) rule mining, and (iii) the lack of semantic rule-based analysis. To address these challenges we introduce a toolset, binnacle, that enabled us to ingest 900,000 GitHub repositories. Focusing on Docker, we extracted approximately 178,000 unique Dockerfiles, and also identified a Gold Set of Dockerfiles written by Docker experts. We addressed challenge (i) by reducing the number of effectively uninterpretable nodes in our ASTs by over 80% via a technique we call phased parsing. To\u00a0\u2026", "num_citations": "17\n", "authors": ["1739"]}
{"title": "Automating abstract interpretation\n", "abstract": " Abstract interpretation has a reputation of being a kind of \u201cblack art,\u201d and consequently difficult to work with. This paper describes a twenty-year quest by the first author to address this issue by raising the level of automation in abstract interpretation. The most recent leg of this journey is the subject of the second author\u2019s 2014 Ph.D. dissertation. The paper discusses several different approaches to creating correct-by-construction analyzers. Our research has allowed us to establish connections between this problem and several other areas of computer science, including automated reasoning/decision procedures, concept learning, and constraint programming.", "num_citations": "17\n", "authors": ["1739"]}
{"title": "Recovery of class hierarchies and composition relationships from machine code\n", "abstract": " We present a reverse-engineering tool, called Lego, which recovers class hierarchies and composition relationships from stripped binaries. Lego takes a stripped binary as input, and uses information obtained from dynamic analysis to (i) group the functions in the binary into classes, and (ii) identify inheritance and composition relationships between the inferred classes. The software artifacts recovered by Lego can be subsequently used to understand the object-oriented design of software systems that lack documentation and source code, e.g., to enable interoperability. Our experiments show that the class hierarchies recovered by Lego have a high degree of agreement\u2014measured in terms of precision and recall\u2014with the hierarchy defined in the source code.", "num_citations": "17\n", "authors": ["1739"]}
{"title": "Opennwa: A nested-word automaton library\n", "abstract": " Nested-word automata (NWAs) are a language formalism that helps bridge the gap between finite-state automata and pushdown automata. NWAs can express some context-free properties, such as parenthesis matching, yet retain all the desirable closure characteristics of finite-state automata.               This paper describes OpenNWA, a C++ library for working with NWAs. The library provides the expected automata-theoretic operations, such as intersection, determinization, and complementation. It is packaged with WALi\u2014the Weighted Automaton Library\u2014and interoperates closely with the weighted pushdown system portions of WALi.", "num_citations": "17\n", "authors": ["1739"]}
{"title": "Program slicing for design automation: An automatic technique for speeding-up hardware design, simulation, testing, and verification\n", "abstract": " Program slicing is a static program analysis technique that allows an analyst to automatically extract portions of programs relevant to the aspects being analyzed. This paper discusses a tool that applies slicing to hardware description languages. The resulting tool provides assistance in various hardware design, simulation, testing, and formal veri cation tasks, as described here.", "num_citations": "17\n", "authors": ["1739"]}
{"title": "Partial evaluation using dependence graphs\n", "abstract": " This thesis describes the use of program dependence graphs, as opposed to control flow graphs, as the basis for the partial evaluation of imperative programs. Partial evaluation is a program specialization operation in which programs with multiple inputs are specialized to take into account known values for some of their inputs. Thus, the result of partially evaluating a program given a division of its inputs into known or\" static\" inputs and unknown or\" dynamic\" inputs is a specialized version of the program, in which computations that require only the static inputs are absent.", "num_citations": "17\n", "authors": ["1739"]}
{"title": "Satisfiability modulo abstraction for separation logic with linked lists\n", "abstract": " Separation logic is an expressive logic for reasoning about heap structures in programs. This paper presents a semi-decision procedure for checking unsatisfiability of formulas in a fragment of separation logic that includes points-to assertions (x|-> y), acyclic-list-segment assertions (ls (x, y)), logical-and, logical-or, separating conjunction, and septraction (the DeMorgan-dual of separating implication). The fragment that we consider allows negation at leaves, and includes formulas that lie outside other separation-logic fragments considered in the literature.", "num_citations": "16\n", "authors": ["1739"]}
{"title": "Fast graph simplification for interleaved Dyck-reachability\n", "abstract": " Many program-analysis problems can be formulated as graph-reachability problems. Interleaved Dyck language reachability. Interleaved Dyck language reachability (InterDyck-reachability) is a fundamental framework to express a wide variety of program-analysis problems over edge-labeled graphs. The InterDyck language represents an intersection of multiple matched-parenthesis languages (ie, Dyck languages). In practice, program analyses typically leverage one Dyck language to achieve context-sensitivity, and other Dyck languages to model data dependences, such as field-sensitivity and pointer references/dereferences. In the ideal case, an InterDyck-reachability framework should model multiple Dyck languages simultaneously.", "num_citations": "15\n", "authors": ["1739"]}
{"title": "Refinement of path expressions for static analysis\n", "abstract": " Algebraic program analyses compute information about a program\u2019s behavior by first (a) computing a valid path expression\u2014i.e., a regular expression that recognizes all feasible execution paths (and usually more)\u2014and then (b) interpreting the path expression in a semantic algebra that defines the analysis. There are an infinite number of different regular expressions that qualify as valid path expressions, which raises the question \u201cWhich one should we choose?\u201d While any choice yields a sound result, for many analyses the choice can have a drastic effect on the precision of the results obtained. This paper investigates the following two questions: (1) What does it mean for one valid path expression to be \u201cbetter\u201d than another? (2) Can we compute a valid path expression that is \u201cbetter,\u201d and if so, how? We show that it is not satisfactory to compare two path expressions E1 and E2 solely by means of the languages\u00a0\u2026", "num_citations": "15\n", "authors": ["1739"]}
{"title": "Partial evaluation of machine code\n", "abstract": " This paper presents an algorithm for off-line partial evaluation of machine code. The algorithm follows the classical two-phase approach of binding-time analysis (BTA) followed by specialization. However, machine-code partial evaluation presents a number of new challenges, and it was necessary to devise new techniques for use in each phase. - Our BTA algorithm makes use of an instruction-rewriting method that ``decouples'' multiple updates performed by a single instruction. This method counters the cascading imprecision that would otherwise occur with a more naive approach to BTA. - Our specializer specializes an explicit representation of the semantics of an instruction, and emits residual code via machine-code synthesis. Moreover, to create code that allows the stack and heap to be at different positions at run-time than at specialization-time, the specializer represents specialization-time addresses using\u00a0\u2026", "num_citations": "15\n", "authors": ["1739"]}
{"title": "Specialization slicing\n", "abstract": " This paper defines a new variant of program slicing, called specialization slicing, and presents an algorithm for the specialization-slicing problem that creates an optimal output slice. An algorithm for specialization slicing is polyvariant: for a given procedure \u0440, the algorithm may create multiple specialized copies of \u0440. In creating specialized procedures, the algorithm must decide for which patterns of formal parameters a given procedure should be specialized and which program elements should be included in each specialized procedure. We formalize the specialization-slicing problem as a partitioning problem on the elements of the possibly infinite unrolled program. To manipulate possibly infinite sets of program elements, the algorithm makes use of automata-theoretic techniques originally developed in the model-checking community. The algorithm returns a finite answer that is optimal (with respect to a criterion\u00a0\u2026", "num_citations": "15\n", "authors": ["1739"]}
{"title": "Incremental evaluation for attribute grammars with unrestricted movement between tree modifications\n", "abstract": " This paper concerns the design of editors that perform checks on a language's context-dependent constraints. Our particular concern is the design of an efficient, incremental analysis algorithm for systems based on the attribute-grammar model of editing. With previous incremental evaluation algorithms for arbitrary noncircular attribute grammars, the editing model required there to be a restriction on the operation that moves the editing cursor: moving the cursor was limited to just a single step in the tree \u2014 either to the parent node or to one of the child nodes of the current cursor location. This paper describes a new updating algorithm that can be used when an arbitrary movement of the cursor in the tree is permitted. After an operation that restructures the tree, the tree's attributes can be updated with a cost of 0 ((1+\u00a6AFFECTED\u00a6)\u00b7\u221am), where m is the size of the tree and AFFECTED is the subset of the tree's\u00a0\u2026", "num_citations": "15\n", "authors": ["1739"]}
{"title": "Source forager: A search engine for similar source code\n", "abstract": " Developers spend a significant amount of time searching for code: e.g., to understand how to complete, correct, or adapt their own code for a new context. Unfortunately, the state of the art in code search has not evolved much beyond text search over tokenized source. Code has much richer structure and semantics than normal text, and this property can be exploited to specialize the code-search process for better querying, searching, and ranking of code-search results. We present a new code-search engine named Source Forager. Given a query in the form of a C/C++ function, Source Forager searches a pre-populated code database for similar C/C++ functions. Source Forager preprocesses the database to extract a variety of simple code features that capture different aspects of code. A search returns the  functions in the database that are most similar to the query, based on the various extracted code features. We tested the usefulness of Source Forager using a variety of code-search queries from two domains. Our experiments show that the ranked results returned by Source Forager are accurate, and that query-relevant functions can be reliably retrieved even when searching through a large code database that contains very few query-relevant functions. We believe that Source Forager is a first step towards much-needed tools that provide a better code-search experience.", "num_citations": "14\n", "authors": ["1739"]}
{"title": "Checking conformance of a producer and a consumer\n", "abstract": " This paper addresses the problem of identifying incompatibilities between two programs that operate in a producer/consumer relationship. It describes the techniques that are incorporated in a tool called PCCA (Producer-Consumer Conformance Analyzer), which attempts to (i) determine whether the consumer is prepared to accept all messages that the producer can emit, or (ii) find a counter-example: a message that the producer can emit and the consumer considers ill-formed.", "num_citations": "14\n", "authors": ["1739"]}
{"title": "Language strength reduction\n", "abstract": " This paper concerns methods to check for atomic-set serializability violations in concurrent Java programs. The straightforward way to encode a reentrant lock is to model it with a context-free language to track the number of successive lock acquisitions. We present a construction that replaces the context-free language that describes a reentrant lock by a regular language that describes a non-reentrant lock. We call this replacement language strength reduction. Language strength reduction produces an average speedup (geometric mean) of 3.4. Moreover, for 2 programs that previously exhausted available space, the tool is now able to run to completion.", "num_citations": "14\n", "authors": ["1739"]}
{"title": "Algorithmic differencing\n", "abstract": " An algorithmic representation of a function is a step-by-step specification of its evaluation in terms of known operations and functions, such as a computer program. In addition to function values, the algorithmic representation can be used to compute related quantities such as derivatives of the function. A process similar to automatie (or algorithmic) differentiation will be applied to obtain differences and divided differences of functions. Advantages of this approach are that it often reduces the sometimes catastrophic cancellation errors in computation of differences and divided differences and provides numerical convergence of divided differences to derivatives.", "num_citations": "14\n", "authors": ["1739"]}
{"title": "Scan grammars: parallel attribute evaluation via data-parallelism\n", "abstract": " This paper concerns the problem of how to exploit parallelism during the phases of compilation involving syntaxdirected analysis and translation. In particular, we address the problem of how to exploit parallelism during the evaluation of the attributes of a derivation tree of a non-circular attribute grammar. What distinguishes the ideas presented in this paper from earlier work on parallel attribute evaluation is the use of a data-parallel model: We define a new variant of attribute grammars, called scan grammars, that incorporates a data-parallel attribution construct.", "num_citations": "13\n", "authors": ["1739"]}
{"title": "Semantics-based program integration\n", "abstract": " The need to integrate several versions of a base program into a common one arises frequently, but it is a tedious and time consuming task to integrate programs by hand. To date, the only available tools for assisting with program integration are variants of text-based differential file comparators; these are of limited utility because one has no guarantees about how the program that is the product of an integration behaves compared to the programs that were integrated.             Our recent work addresses the problem of building a semantics-based tool for program integration; this paper describes the techniques we have developed, which provide the foundation for creating such a tool. Semantics-based integration is based on the assumption that a difference in the behavior of one of the variant programs from that of the base program, rather than a difference in the text, is significant and must be preserved in the\u00a0\u2026", "num_citations": "13\n", "authors": ["1739"]}
{"title": "McDash: Refinement-based property verification for machine code\n", "abstract": " This paper presents MCDASH, a refinement-based model checker for machine code. While model checkers such as SLAM, BLAST, and DASH have each made significant contributions in the field of verification/flaw-detection, their use has been restricted to programs for which source code is available.  This paper discusses several challenges that arise when working with machine code, and explains how they are addressed in MCDASH.  Unlike previous model checkers, MCDASH does not require the usual preprocessing steps of (a) building control-flow graphs, and (b) performing points-to analysis (or alias analysis); nor does MCDASH require type information to be supplied. The paper also describes how we extended MCDASH to check properties of self-modifying code.  MCDASH is built using language-independent meta-tools that generate the implementations of the required analysis components from descriptions of an instruction set's syntax and semantics.  It has been instantiated for Intel x86 and PowerPC.", "num_citations": "12\n", "authors": ["1739"]}
{"title": "Advanced Querying for Property Checking\n", "abstract": " Extended weighted pushdown systems (EWPDSs) are an extension of pushdown systems that incorporate infinite-state data abstractions. Nested-word automata (NWAs) are able to recognize languages that exhibit context-free properties, while retaining many of the decidability properties of finite automata. We study property checking of programs where the program model is an EWPDS and the property is specified by an NWA. We show how to combine an NWA A with an EWPDS E to create an EWPDS  E' such that reachability analysis on E' checks property A on program E. This construction allows us to retain the capability of running advanced queries on programs modeled as EWPDSs, such as the ability to (i) find all program nodes that lie on an error path (via error projections); and (ii) answer context-bounded reachability queries for concurrent programs with infinite-state abstractions (via context-bounded model checking).", "num_citations": "12\n", "authors": ["1739"]}
{"title": "Abstraction refinement for 3-valued-logic analysis\n", "abstract": " This paper concerns the question of how to create abstractions that are useful for program analysis. It presents a method that refines an abstraction automatically for analysis problems in which the semantics of statements and the query of interest are expressed using logical formulas. Refinement is carried out by introducing new instrumentation relations (defined via logical formulas over core relations, which capture the basic properties of memory configurations). A tool that incorporates the algorithm has been implemented and applied to several algorithms that manipulate linked lists and binary-search trees. In all but a few cases, Ihe tool is able to demonstrate (i) the partial correctness of the algorithms, and (ii) that the algorithms possess additional properties--e.g., stability or antistability.", "num_citations": "12\n", "authors": ["1739"]}
{"title": "Speeding up machine-code synthesis\n", "abstract": " Machine-code synthesis is the problem of searching for an instruction sequence that implements a semantic specification, given as a formula in quantifier-free bit-vector logic (QFBV). Instruction sets like Intel's IA-32 have around 43,000 unique instruction schemas; this huge instruction pool, along with the exponential cost inherent in enumerative synthesis, results in an enormous search space for a machine-code synthesizer: even for relatively small specifications, the synthesizer might take several hours or days to find an implementation. In this paper, we present several improvements to the algorithms used in a state-of-the-art machine-code synthesizer McSynth. In addition to a novel pruning heuristic, our improvements incorporate a number of ideas known from the literature, which we adapt in novel ways for the purpose of speeding up machine-code synthesis. Our experiments for Intel's IA-32 instruction set show\u00a0\u2026", "num_citations": "11\n", "authors": ["1739"]}
{"title": "Templates and recurrences: Better together\n", "abstract": " This paper is the confluence of two streams of ideas in the literature on generating numerical invariants, namely:(1) template-based methods, and (2) recurrence-based methods.", "num_citations": "10\n", "authors": ["1739"]}
{"title": "Modification algebras\n", "abstract": " Modification Algebras | Proceedings of the Second International Conference on Methodology and Software Technology: Algebraic Methodology and Software Technology ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsAMAST '91Modification Algebras ARTICLE Modification Algebras Share on Authors: G. Ramalingam profile image G. Ramalingam View Profile , Thomas Reps profile image Thomas W. Reps View Profile Authors Info & Affiliations Publication: AMAST '91: Proceedings of the Second International Conference on Methodology and Software Technology: Algebraic Methodology and Software TechnologyMay \u2026", "num_citations": "9\n", "authors": ["1739"]}
{"title": "Sublinear-space evaluation algorithms for attribute grammars\n", "abstract": " A major drawback of attribute-grammar-based systems is that they are profligate consumers of storage. This paper concerns new storage-management techniques that reduce the number of attribute values retained at any stage of attribute evaluation; it presents an algorithm for evaluating an n-attribute tree that never retains more than O(log n) attribute values. This method is optimal, although it may require nonlinear time. A second algorithm, which never retains more than O(\u221an) attribute values, is also presented, both as an introduction to the O(log n) method and because it works in linear time.", "num_citations": "9\n", "authors": ["1739"]}
{"title": "Sound bit-precise numerical domains\n", "abstract": " This paper tackles the challenge of creating a numerical abstract domain that can identify affine-inequality invariants while handling overflow in arithmetic operations over bit-vector data-types. The paper describes the design and implementation of a class of new abstract domains, called the Bit-Vector-Sound, Finite-Disjunctive () domains. We introduce a framework that takes an abstract domain  that is sound with respect to mathematical integers and creates an abstract domain  whose operations and abstract transformers are sound with respect to machine integers. We also describe how to create abstract transformers for  that are sound with respect to machine arithmetic. The abstract transformers make use of an operation \u2014where  and v is a set of program variables\u2014which performs wraparound in av for the variables in v.               To reduce the loss of precision from , we use finite\u00a0\u2026", "num_citations": "7\n", "authors": ["1739"]}
{"title": "An abstract domain for bit-vector inequalities\n", "abstract": " This paper advances the state of the art in abstract interpretation of machine code. It tackles two of the biggest challenges in machine-code analysis:(1) holding onto invariants about values in memory, and (2) identifying affine-inequality invariants while handling overflow in arithmetic operations over bit-vector data-types. Most current approaches either capture relations only among registers (and ignore memory entirely), or make potentially unsound assumptions when handling memory. Furthermore, existing bit-vector domains are able to represent either relational affine equalities or non-relational inequalities (eg, intervals).The key insight to tackling both challenges is to define a new domain combinator (denoted by V), called the view-product combinator. V constructs a reduced product of two domains in which shared view-variables are used to communicate information among the domains. V applied to a non-relational memory domain and a relational bit-vector affine-equality domain constructs the Bit-Vector Memory-Equality Domain (BVME), a domain of bit-vector affine-equalities over memory and registers. V applied to the BVME domain and a bit-vector interval domain constructs the Bit-Vector Memory-Inequality Domain, a domain of relational bitvector affine-inequalities over memory and registers.", "num_citations": "7\n", "authors": ["1739"]}
{"title": "Static analysis of software executables\n", "abstract": " In recent years, there has been a growing need for tools that an analyst can use to understand the workings of COTS software as well as malicious code. Static analysis provides techniques that can help with such problems; however, there are several obstacles that must be overcome, including the absence of source code and the difficulty of analyzing machine code. We have created CodeSurfer/x86, a prototype tool for browsing, inspecting, and analyzing x86 executables. From an x86 executable, CodeSurfer/x86 recovers intermediate representations that are similar to what would be created by a compiler for a program written in a high-level language. These facilities provide a platform for the development of additional tools for analyzing the security properties of executables. CodeSurfer/x86 analyses are automatically generated from a formal specification of the x86 instruction semantics. This makes the analyses\u00a0\u2026", "num_citations": "7\n", "authors": ["1739"]}
{"title": "Refinement-based program* verification via three-valued-logic analysis\n", "abstract": " Recently, Sagiv, Reps, and Wilhelm introduced a powerful abstract-interpretation framework for program analysis based on three-valued logic [84]. Instantiations of this framework have been used to show a number of interesting properties of programs that manipulate a variety of linked data structures. However, two aspects of the framework represented significant challenges in its user-model. The work that is reported in this thesis addressed these two shortcomings, developed solutions to them, and carried out experiments to demonstrate their effectiveness.", "num_citations": "7\n", "authors": ["1739"]}
{"title": "BTA termination using CFL-reachability\n", "abstract": " In this paper, we develop a BTA algorithm that ensures termination of off-line partial evaluation for first-order functional programs, extending the work of Holst and of Glenstrup and Jones. Holst's characterization of in-situ-decreasing behaviour does not account for parameters that do not control the recursion of their functions. We extend Holst's framework to handle this phenomenon by defining the\" influential\u201d property for parameters. Glenstrup and Jones's algorithm for identifying in-situdecreasing parameters, which relies on the size markings (f, J.,=) on the edges of a dependence graph, says that a path must be free of f edges and must contain at least one J edge to be size-decreasing. We extend their language of size-decreasing paths: A size-decreasing path may contain f edges provided that every f edge is balanced by the appropriate kind of \u012e edge. We modify the size markings on the graph edges and use\u00a0\u2026", "num_citations": "7\n", "authors": ["1739"]}
{"title": "Wisconsin Program-Integration System 2.0-Reference Manual\n", "abstract": " This document describes the basic command set of the Wisconsin ProgramIntegration System. An overview of the system's theoretical underpinnings can be found in [7]. A tutorial introduction to an earlier version of the system can be found in [13]. The user interface for the integration system incorporates a languagespecific editor created using the Synthesizer Generator, a meta-system for creating interactive, language-based program-development systems [15, 16] As with all editors created with the Synthesizer Generator, the integration system's editor exhibits characteristics that are specific to the language being edited, while at the same time sharing the generic user interface described in Chapter 3 of The Synthesizer Generator Reference Manual [16]. This document primarily describes the commands that are specific to the integration system---for example, for invoking the system's operations for slicing, differencing, and integration---as opposed to commands that are part of the standard user interface of editors created with the Synthesizer Generator. A few of the commands that are part of the standard user interface of Synthesizer-Generator-created editors are also described because these commands have been altered to function in non-standard ways. 1.2 Background", "num_citations": "7\n", "authors": ["1739"]}
{"title": "A new abstraction framework for affine transformers\n", "abstract": " This paper addresses the problem of abstracting a set of affine transformers , where  and  represent the pre-state and post-state, respectively. We introduce a framework to harness any base abstract domain  in an abstract domain of affine transformations. Abstract domains are usually used to define constraints on the variables of a program. In this paper, however, abstract domain  is re-purposed to constrain the elements of C and \u2014thereby defining a set of affine transformers on program states. This framework facilitates intra- and interprocedural analyses to obtain function and loop summaries, as well as to prove program assertions.", "num_citations": "6\n", "authors": ["1739"]}
{"title": "The interprocedural express-lane transformation\n", "abstract": " The express-lane transformation isolates and duplicates frequently executed program paths, aiming for better data-flow facts along the duplicated paths. An express-lane p is a copy of a frequently executed program path such that p has only one entry point at its beginning; p may have branches back to the original code, but the original code never branches into p. Classical data-flow analysis is likely to find sharper data-flow facts along an express-lane, because there are no join points.               This paper describes several variants of interprocedural express-lane transformations; these duplicate hot interprocedural paths, i.e., paths that may cross procedure boundaries. The paper also reports results from an experimental study of the effects of the express-lane transformation on interprocedural range analysis.", "num_citations": "6\n", "authors": ["1739"]}
{"title": "The Use of Program Profiling in Software Testing\n", "abstract": " This paper describes new techniques to help with testing and debugging, using information obtained from path profiling. A path profiler instruments a program so that each run of the program generates a path spectrum for the execution\u2014a distribution of acyclic path fragments that were executed during that run. Our techniques are based on the idea of comparing path spectra from different runs of the program. When different runs produce different spectra, the spectral differences can be used to identify paths in the program along which control diverges in the two runs. By choosing input datasets to hold all factors constant except one, the divergence can be attributed to this factor. The point of divergence itself may not be the cause of the underlying problem, but provides a starting place for a programmer to begin his exploration.", "num_citations": "6\n", "authors": ["1739"]}
{"title": "Report of a workshop on future directions in programming languages and compilers\n", "abstract": " PurposeIn January, 1993, a panel of experts in the area of programming languages and compilers met in a one and a half day workshop to discuss the future of research in that area. This paper is the report of their findings. Its purposes are to explain the need for, and benefits of, research in this field--both basic and applied; to broadly survey the various parts of the field and indicate its general research directions; and to propose an initiative aimed at moving basic research results into wider use.", "num_citations": "6\n", "authors": ["1739"]}
{"title": "A denotational semantics for low-level probabilistic programs with nondeterminism\n", "abstract": " Probabilistic programming is an increasingly popular formalism for modeling randomness and uncertainty. Designing semantic models for probabilistic programs has been extensively studied, but is technically challenging. Particular complications arise when trying to account for (i) unstructured control-flow, a natural feature in low-level imperative programs; (ii) general recursion, an extensively used programming paradigm; and (iii) nondeterminism, which is often used to represent adversarial actions in probabilistic models, and to support refinement-based development. This paper presents a denotational-semantics framework that supports the three features mentioned above, while allowing nondeterminism to be handled in different ways. To support both probabilistic choice and nondeterministic choice, the semantics is given over control-flow hyper-graphs. The semantics follows an algebraic approach: it can be\u00a0\u2026", "num_citations": "5\n", "authors": ["1739"]}
{"title": "Model-assisted machine-code synthesis\n", "abstract": " Binary rewriters are tools that are used to modify the functionality of binaries lacking source code. Binary rewriters can be used to rewrite binaries for a variety of purposes including optimization, hardening, and extraction of executable components. To rewrite a binary based on semantic criteria, an essential primitive to have is a machine-code synthesizer---a tool that synthesizes an instruction sequence from a specification of the desired behavior, often given as a formula in quantifier-free bit-vector logic (QFBV). However, state-of-the-art machine-code synthesizers such as McSynth++ employ naive search strategies for synthesis: McSynth++ merely enumerates candidates of increasing length without performing any form of prioritization. This inefficient search strategy is compounded by the huge number of unique instruction schemas in instruction sets (e.g., around 43,000 in Intel's IA-32) and the exponential cost\u00a0\u2026", "num_citations": "5\n", "authors": ["1739"]}
{"title": "Newtonian program analysis via tensor product\n", "abstract": " Recently, Esparza et al. generalized Newton\u2019s method\u2014a numerical-analysis algorithm for finding roots of real-valued functions\u2014to a method for finding fixed-points of systems of equations over semirings. Their method provides a new way to solve interprocedural dataflow-analysis problems. As in its real-valued counterpart, each iteration of their method solves a simpler \u201clinearized\u201d problem. One of the reasons this advance is exciting is that some numerical analysts have claimed that \u201c\u2018all\u2019 effective and fast iterative [numerical] methods are forms (perhaps very disguised) of Newton\u2019s method.\u201d However, there is an important difference between the dataflow-analysis and numerical-analysis contexts: When Newton\u2019s method is used in numerical-analysis problems, commutativity of multiplication is relied on to rearrange an expression of the form \u201ca * X * b + c * X * d\u201d into \u201c(a*b + c*d)*X.\u201d Equations with such expressions\u00a0\u2026", "num_citations": "5\n", "authors": ["1739"]}
{"title": "Software-architecture recovery from machine code\n", "abstract": " In this paper, we present a tool, called Lego, which recovers object-oriented software architecture from stripped binaries. Lego takes a stripped binary as input, and uses information obtained from dynamic analysis to (i) group the functions in the binary into classes, and (ii) identify inheritance and composition relationships between the inferred classes. The information obtained by Lego can be used for reengineering legacy software, and for understanding the architecture of software systems that lack documentation and source code. Our experiments show that the class hierarchies recovered by Lego have a high degree of agreement\u2014measured in terms of precision and recall\u2014with the hierarchy defined in the source code.", "num_citations": "5\n", "authors": ["1739"]}
{"title": "W., Teitelbaum, T.: CodeSurfer/x86-A Platform for Analyzing x86 Executables\n", "abstract": " CodeSurfer/x86 is a prototype system for analyzing x86 executables. It uses a static-analysis algorithm called value-set analysis (VSA) to recover intermediate representations that are similar to those that a compiler creates for a program written in a high-level language. A major challenge in building an analysis tool for executables is in providing useful information about operations involving memory. This is difficult when symbol-table and debugging information is absent or untrusted. CodeSurfer/x86 overcomes these challenges to provide an analyst with a powerful and flexible platform for investigating the properties and behaviors of potentially malicious code (such as COTS components, plugins, mobile code, worms, Trojans, and virus-infected code) using (i) CodeSurfer/x86's GUI,(ii) CodeSurfer/x86's scripting language, which provides access to all of the intermediate representations that CodeSurfer/x86 builds for the executable, and (iii) GrammaTech's Path Inspector, which is a tool that uses a sophisticated pattern-matching engine to answer questions about the flow of execution in a program.", "num_citations": "5\n", "authors": ["1739"]}
{"title": "Computational divided differencing\n", "abstract": " This invention concerns the manipulation and restructuring of programs that compute numerical functions using floating-point numbers, for the purpose of controlling round-off error. In particular, it provides a variety of means for transforming a program that computes a numerical function F (x) into a related program that computes divided differences of F:", "num_citations": "5\n", "authors": ["1739"]}
{"title": "Adoption of immunization delivery services in pharmacies\n", "abstract": " Increasing immunization rates among high-risk populations is a goal of Healthy People 2010. One strategy to improve these rates is Increasing accessibility to immunization providers. Pharmacists are able to make tremendous contributions in immunization delivery services by:(a) contracting with outside providers to administer vaccines, referred to as \u201coutsourced service\u201d and (b) having staff pharmacists administer vaccines, referred to as \u201cin-house service\u201d. Goals of this study are to (a) identify the level of and trends in immunization promotion programs and delivery services and (b) better understand reasons for community pharmacies to provide outsourced and in-house immunization services. A framework integrating between Rational Choice and Institutional Theories is developed for predicting the adoption of outsourced and in-house immunization services.", "num_citations": "5\n", "authors": ["1739"]}
{"title": "Analyzing memory accesses in x86 binary executables\n", "abstract": " This paper concerns static analysis algorithms for analyzing x86 executables. The aim of the work is to recover intermediate representations that are similar to those that can be created for a program written in a high-level language. Our goal is to perform this task for programs such as plugins, mobile code, worms, and virus-infected code. For such programs, symbol-table and debugging information is either entirely absent, or cannot be relied upon if present; hence, the technique described in the paper makes no use of symbol-table/debugging information. Instead, an analysis is carried out to recover information about the contents of memory locations and how they are manipulated. The analysis, called value-set analysis, tracks address-valued and integer-valued quantities simultaneously.", "num_citations": "5\n", "authors": ["1739"]}
{"title": "Programming environments: report on an international workshop at Dagstuhl Castle\n", "abstract": " [o= >: '= - =.= [ ~. . Page 1 gg. =\" \u03c5 .= =' , \" ; :z ' o . - . - = - \" oo - - . 0 o = ,. . . o . -\u03c5= ~ =- ' - 8 ' \"= ='= \" o o o =. [o= >: '= .\u03c5 - =.= [ . \" o ' 8 TM o =o , 8-o * q \" 0 o 0 0 \"0 =o ,= ~. . 90 \" - .- 0 _ = \" w o . \" \" .... : e \" , o - = o [] - ,uu ,. 0 .,,.- \u0386 =. E- = ACM SIGPLAN Notices, Volume 27, No. 11 November 1992 Page 2 \u03c5 - ,. o . o .o o- - ' \u03c5\u03c5 .- . . \u2022 _. .- - = ..s8 -&.\u03c5,. :-., . . \u2022 -- .-- - s \u03c5 o: .. T=o \u2022\u03c5 '-= o . ,-., . .. -- =, \" - S -. , : - o .. o o g - . - . - - . . .:.o - = \". g . s R \" S\" n. . 0 S: =. , - ..o ' , - ,.= =., = .- ,. . - - . o- \" . ..o o\u03c5- . s\" . o _ . -. -. , o \u03c5 ' .- \"r. o .- . . o ... . . 0 o B . _ - \u03c5 .& o: iZ- - - = . .\u03c5 0 co . . . . - \" i \u03c5 - o . . o E . .. . , -. . \u03c5 \u03c5E. - . r -... . :&s to n'.o ' \"\" \u2022 @ - . . . -\u03c5 \u03c5.\" . - . \u03c5 . . - o p\u03c5 \u03c5. .. - i .\" . \" ', o, & 91 i \u03c5 #o . o o . , \u03c5. =. g-& \" \u03c5 \" s. - o\" oo \"E o \u03c5-z. .. Page 3 2 P. I I EE t. #. \" \"o o e 8 ' \u00a3 c. o - :i ., - . - - \" o * o , . ..= a =o -. = <' '1: oo 0, ss 2 - = s o , . . , = . =o=- 8\" w=- o. 0- = .. \" 'E \u00a3 0 0 I \u2022 . o' ...*. ca.. . ' . I.< \u03c5\" 0 - 0 o o =.e .'. \" .< o =. . \u03c5. n 8 === . - r =o & o 8\" E ,e, k ;S. s=o=\u00a3 \u03c5 &a= B -* : = , . a .=\u2026", "num_citations": "5\n", "authors": ["1739"]}
{"title": "A new program integration algorithm\n", "abstract": " Program integration attempts to construct a merged program from several related but different variants of a base program.  The merged program must include the changed computations of the variants as well as the computations of the base program that are preserved in all variants.  A fundamental problem of program integration is determining the sets of changed and preserved computations of each variant.  This paper describes a new algorithm for partitioning program components (in one or more programs) into disjoint equivalence classes so that two components are in the same class only if they have the same execution behavior.  This partitioning algorithm can be used to identify changed and preserved computations, and thus forms the basis for the new program-integration algorithm presented here.  The new program-integration algorithm is strictly better than the original algorithm of Horwitz, Prins, and Reps: integrated programs produced by the new algorithm have the same semantic properties relative to the base program and its variants as do integrated programs produced by the original algorithm, the new algorithm successfully integrates program variants whenever the original algorithm does, but here are classes of program modifications for which the new algorithm succeeds while the original algorithm reports interference.", "num_citations": "5\n", "authors": ["1739"]}
{"title": "On the complexity of bidirected interleaved Dyck-reachability\n", "abstract": " Many program analyses need to reason about pairs of matching actions, such as call/return, lock/unlock, or set-field/get-field. The family of Dyck languages {Dk}, where Dk has k kinds of parenthesis pairs, can be used to model matching actions as balanced parentheses. Consequently, many program-analysis problems can be formulated as Dyck-reachability problems on edge-labeled digraphs. Interleaved Dyck-reachability (InterDyck-reachability), denoted by Dk \u2299 Dk-reachability, is a natural extension of Dyck-reachability that allows one to formulate program-analysis problems that involve multiple kinds of matching-action pairs. Unfortunately, the general InterDyck-reachability problem is undecidable.  In this paper, we study variants of InterDyck-reachability on bidirected graphs, where for each edge \u27e8 p, q \u27e9 labeled by an open parenthesis \u201d(a\u201d, there is an edge \u27e8 q, p \u27e9 labeled by the corresponding close\u00a0\u2026", "num_citations": "4\n", "authors": ["1739"]}
{"title": "TOFU: Target-Oriented FUzzer\n", "abstract": " Program fuzzing---providing randomly constructed inputs to a computer program---has proved to be a powerful way to uncover bugs, find security vulnerabilities, and generate test inputs that increase code coverage. In many applications, however, one is interested in a target-oriented approach-one wants to find an input that causes the program to reach a specific target point in the program. We have created TOFU (for Target-Oriented FUzzer) to address the directed fuzzing problem. TOFU's search is biased according to a distance metric that scores each input according to how close the input's execution trace gets to the target locations. TOFU is also input-structure aware (i.e., the search makes use of a specification of a superset of the program's allowed inputs). Our experiments on xmllint show that TOFU is 28% faster than AFLGo, while reaching 45% more targets. Moreover, both distance-guided search and exploitation of knowledge of the input structure contribute significantly to TOFU's performance.", "num_citations": "4\n", "authors": ["1739"]}
{"title": "Recovering execution data from incomplete observations\n", "abstract": " Due to resource constraints, tracing production applications often results in incomplete data. Nevertheless, developers ideally want answers to queries about the program's execution beyond data explicitly gathered. For example, a developer may ask whether a particular program statement executed during the run corresponding to a given failure report. In this work, we investigate the problem of determining whether each statement in a program executed, did not execute, or may have executed, given a set of (possibly-incomplete) observations. Using two distinct formalisms, we propose two solutions to this problem. The first formulation represents observations as regular languages, and computes intersections over these languages using finite-state acceptors. The second formulation encodes the problem as a set of Boolean constraints, and uses answer set programming to solve the constraints.", "num_citations": "4\n", "authors": ["1739"]}
{"title": "Verifying concurrent programs via bounded context-switching and induction\n", "abstract": " This paper presents a new approach to the problem of verifying safety properties of concurrent programs with shared memory and interleaving semantics. The method described leverages recent work on context-bounded analysis (CBA) via \"sequentialization\". In such work, a concurrent program P is converted to a sequential program whose behavior is equivalent to P for up to K context switches. By itself, CBA is not a sound verification method because it bounds the number of context switches considered, and hence does not explore all of P's behaviors.  We combine CBA with K-induction to create a method that can verify properties for an unbounded number of context switches. In a K-induction argument, two \"windows\" of K steps are considered: the base case considers a prefix of up to K steps; the inductive case assumes that the property of interest is true for the previous K steps, and attempts to establish the property for one more step.   We argue that CBA has the right characteristics to complement K-induction.  In fact, our method uses CBA unchanged to discharge the base case.  It also uses CBA as a subroutine when discharging the inductive case.  The method works by analyzing two sequential programs, T1 and T2, each of which is a transformed version of P that simulates P's behavior for K context switches. T1 and T2 work slightly differently, and the analyses performed on them start from different sets of initial configurations.  If, in both T1 and T2, the analyzer shows that the error state cannot be reached in K context switches, K-induction implies that P cannot reach the error state in any number of context switches.   The account sketched\u00a0\u2026", "num_citations": "4\n", "authors": ["1739"]}
{"title": "Checking compatibility of a producer and a consumer\n", "abstract": " This paper addresses the problem of identifying incompatibilities between two programs that operate in a producer/consumer relationship. It describes the techniques that are incorporated in a tool called PCCA (Producer-Consumer Conformance Analyzer), which attempts to (i) determine whether the consumer is prepared to accept all messages that the producer can emit, or (ii) find a counterexample: a message that the producer can emit and the consumer will reject.", "num_citations": "4\n", "authors": ["1739"]}
{"title": "View-augmented abstractions\n", "abstract": " This paper introduces view-augmented abstractions, which specialize an underlying numeric domain to focus on a particular expression or set of expressions. A view-augmented abstraction adds a set of materialized views to the original domain. View augmentation can extend a domain so that it captures information unavailable in the original domain. We show how to use finite differencing to maintain a materialized view in response to a transformation of the program state. Our experiments show that view augmentation can increase precision in useful ways.", "num_citations": "4\n", "authors": ["1739"]}
{"title": "A next-generation platform for analyzing executables\n", "abstract": " In recent years, there has been a growing need for tools that an analyst can use to understand the workings of COTS components, plug-ins, mobile code, and DLLs, as well as memory snapshots of worms and virus-infected code. Static analysis provides techniques that can help with such problems; however, there are several obstacles that must be overcome:                                         For many kinds of potentially malicious programs, symbol-table and debugging information is entirely absent. Even if it is present, it cannot be relied upon.                                                           To understand memory-access operations, it is necessary to determine the set of addresses accessed by each operation. This is difficult because                                                                                             While some memory operations use explicit memory addresses in the instruction (easy), others use indirect addressing via address expressions\u00a0\u2026", "num_citations": "4\n", "authors": ["1739"]}
{"title": "A dataset of dockerfiles\n", "abstract": " Dockerfiles are one of the most prevalent kinds of DevOps artifacts used in industry. Despite their prevalence, there is a lack of sophisticated semantics-aware static analysis of Dockerfiles. In this paper, we introduce a dataset of approximately 178,000 unique Dockerfiles collected from GitHub. To enhance the usability of this data, we describe five representations we have devised for working with, mining from, and analyzing these Dockerfiles. Each Dockerfile representation builds upon the previous ones, and the final representation, created by three levels of nested parsing and abstraction, makes tasks such as mining and static checking tractable. The Dockerfiles, in each of the five representations, along with metadata and the tools used to shepard the data from one representation to the next are all available at: https://doi. org/10.5281/zenodo. 3628771.", "num_citations": "3\n", "authors": ["1739"]}
{"title": "Slicing machine code\n", "abstract": " Machine-code slicing is an important primitive for building binary analysis and rewriting tools, such as taint trackers, fault localizers, and partial evaluators. However, it is not easy to create a machine-code slicer that exhibits a high level of precision. Moreover, the problem of creating such a tool is compounded by the fact that a small amount of local imprecision can be amplified via cascade effects. Most instructions in instruction sets such as Intel's IA-32 and ARM are multi-assignments: they have several inputs and several outputs (registers, flags, and memory locations). This aspect of the instruction set introduces a granularity issue during slicing: there are often instructions at which we would like the slice to include only a subset of the instruction's multiple assignments, whereas the slice is forced to include the entire instruction. Consequently, the slice computed by state-of-the-art tools is very imprecise, often including essentially the entire program. We present an algorithm to slice machine code more accurately. To counter the granularity issue, our algorithm attempts to include in the slice only the subset of assignments in an instruction's semantics that is relevant to the slicing criterion. Our experiments on IA-32 binaries of FreeBSD utilities show that, in comparison to slices computed by a state-of-the-art tool, our algorithm reduces the number of instructions in backward slices by 36%, and in forward slices by 82%.", "num_citations": "3\n", "authors": ["1739"]}
{"title": "Central moment analysis for cost accumulators in probabilistic programs\n", "abstract": " For probabilistic programs, it is usually not possible to automatically derive exact information about their properties, such as the distribution of states at a given program point. Instead, one can attempt to derive approximations, such as upper bounds on tail probabilities. Such bounds can be obtained via concentration inequalities, which rely on the moments of a distribution, such as the expectation (the first raw moment) or the variance (the second central moment). Tail bounds obtained using central moments are often tighter than the ones obtained using raw moments, but automatically analyzing central moments is more challenging.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "Expected-Cost Analysis for Probabilistic Programs and Semantics-Level Adaption of Optional Stopping Theorems\n", "abstract": " In this article, we present a semantics-level adaption of the Optional Stopping Theorem, sketch an expected-cost analysis as its application, and survey different variants of the Optional Stopping Theorem that have been used in static analysis of probabilistic programs.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "Enabling Open-World Specification Mining via Unsupervised Learning\n", "abstract": " Many programming tasks require using both domain-specific code and well-established patterns (such as routines concerned with file IO). Together, several small patterns combine to create complex interactions. This compounding effect, mixed with domain-specific idiosyncrasies, creates a challenging environment for fully automatic specification inference. Mining specifications in this environment, without the aid of rule templates, user-directed feedback, or predefined API surfaces, is a major challenge. We call this challenge Open-World Specification Mining. In this paper, we present a framework for mining specifications and usage patterns in an Open-World setting. We design this framework to be miner-agnostic and instead focus on disentangling complex and noisy API interactions. To evaluate our framework, we introduce a benchmark of 71 clusters extracted from five open-source projects. Using this dataset, we show that interesting clusters can be recovered, in a fully automatic way, by leveraging unsupervised learning in the form of word embeddings. Once clusters have been recovered, the challenge of Open-World Specification Mining is simplified and any trace-based mining technique can be applied. In addition, we provide a comprehensive evaluation of three word-vector learners to showcase the value of sub-word information for embeddings learned in the software-engineering domain.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "Dissolve: A Distributed SAT Solver Based on Stalmarck's Method\n", "abstract": " Creating an effective parallel SAT solver is known to be a challenging task. At present, the most efficient implementations of parallel SAT solvers are portfolio solvers with some heuristics to share learnt clauses. In this paper, we propose a novel approach for solving SAT problems in parallel based on the combination of traditional CDCL with St\u00e5lmarck\u2019s method. In particular, we use a variant of St\u00e5lmarck\u2019s Dilemma Rule to partition the search space between solvers and merge their results. The paper describes the design of a new distributed SAT solver, called Dissolve, and presents experiments that demonstrate the value of the Dilemma-rule-based approach. The experiments showed that running times decreased on average (geometric mean) by 25% and 17% for SAT and UNSAT examples, respectively.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "Software-architecture recovery from machine code\n", "abstract": " In this paper, we present a tool, called Lego, which recovers object-oriented software architecture from stripped binaries. Lego takes a stripped binary as input, and uses information obtained from dynamic analysis to (i) group the functions in the binary into classes, and (ii) identify inheritance and composition relationships between the inferred classes. The information obtained by Lego can be used for reengineering legacy software, and for understanding the architecture of software systems that lack documentation and source code. Our experiments show that the class hierarchies recovered by Lego have a high degree of agreement---measured in terms of precision and recall---with the hierarchy defined in the source code.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "WALi: Nested-word automata\n", "abstract": " WALi-NWA is a C++ library for constructing, querying, and operating on nested-word automata. It is a portion of the WALi library, which provides types and operations for weighted automata. While the NWA portions of WALi are mostly logically separate from the rest of WALi, it does use facilities provided by WALi and inter-operates with WALi's weighted pushdown system (WPDS) code.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "Program comprehension and software reengineering\n", "abstract": " Analyzing old software systems has become an important topic in software technology. There are billions of lines of legacy code which constitute substantial corporate assets. Legacy systems have been subject to countless modifications and enhancements and, hence, software entropy has typically increased steadily over the years. If these systems are not refurbished they might die of old age\u2014and the knowledge embodied in these systems will be lost forever.As a first step in \u201csoftware geriatrics\u201d one usually tries to understand the old system using program understanding or program comprehension techniques. In a second step, one reconstructs abstract concepts (eg, the system architecture, business rules) from the source code, the documentation, and corporate knowledge; this is called software reverse engineering. Given an abstract representation of the system, one can then re-implement the system. This forward engineering step ranges from fully automatic approaches to manual reimplementations including restructuring techniques, formal transformations, injecting component technologies, replacing old user interface or database technology. The process of moving from an old legacy system to a new implementation is called software reengineering.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "Optimal-time incremental semantic analysis for syntax-directed editors\n", "abstract": " Attribute grammars permit the specification of static semantics in an  applicative and modular fashion, and thus are a good basis for syntax-directed  editors. Such editors represent programs as as attributed trees, which are  modified by operations such as subtree pruning and grafting. After each  modification, a subset of attributes, AFFECTED, requires new values.  Membership in AFFECTED is not known a priori; this paper presents an algorithm  that identifies attributes in AFFECTED and recomputes their values. The  algorithm is time-optimal, its cost is proportional to the size of AFFECTED.", "num_citations": "2\n", "authors": ["1739"]}
{"title": "Sound probabilistic inference via guide types\n", "abstract": " Probabilistic programming languages aim to describe and automate Bayesian modeling and inference. Modern languages support programmable inference, which allows users to customize inference algorithms by incorporating guide programs to improve inference performance. For Bayesian inference to be sound, guide programs must be compatible with model programs. One pervasive but challenging condition for model-guide compatibility is absolute continuity, which requires that the model and guide programs define probability distributions with the same support.", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Shipwright: A Human-in-the-Loop System for Dockerfile Repair\n", "abstract": " Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and\u2014to our great surprise\u2014 found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a \u201ctime-travel\u201d analysis of broken Dockerfiles that were later fixed, we\u00a0\u2026", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Interprocedural Context-Unbounded Program Analysis Using Observation Sequences\n", "abstract": " A classical result by Ramalingam about synchronization-sensitive interprocedural program analysis implies that reachability for concurrent threads running recursive procedures is undecidable. A technique proposed by Qadeer and Rehof, to bound the number of context switches allowed between the threads, leads to an incomplete solution that is, however, believed to catch \u201cmost bugs\u201d in practice, as errors tend to occur within few contexts. The question of whether the technique can also prove the absence of bugs at least in some cases has remained largely open. Toward closing this gap, we introduce in this article the generic verification paradigm of observation sequences for resource-parameterized programs. Such a sequence observes how increasing the resource parameter affects the reachability of states satisfying a given property. The goal is to show that increases beyond some \u201ccutoff\u201d parameter value\u00a0\u2026", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Tail Bound Analysis for Probabilistic Programs via Central Moments\n", "abstract": " For probabilistic programs, it is usually not possible to automatically derive exact information about their properties, such as the distribution of states at a given program point. Instead, one can attempt to derive approximations, such as upper bounds on tail probabilities. Such bounds can be obtained via concentration inequalities, which rely on the moments of a distribution, such as the expectation (the first raw moment) or the variance (the second central moment). Tail bounds obtained using central moments are often tighter than the ones obtained using raw moments, but automatically analyzing higher moments is more challenging. This paper presents an analysis for probabilistic programs that automatically derives symbolic over-and under-approximations for variances, as well as higher central moments. To overcome the challenges of higher-moment analysis, it generalizes analyses for expectations with an\u00a0\u2026", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Program Analyses Using Newton\u2019s Method\n", "abstract": " Esparza et al. generalized Newton\u2019s method\u2014a numerical-analysis algorithm for finding roots of real-valued functions\u2014to a method for finding fixed-points of systems of equations over semirings. Their method provides a new way to solve interprocedural dataflow-analysis problems. As in its real-valued counterpart, each iteration of their method solves a simpler \u201clinearized\u201d problem.                 Because essentially all fast iterative numerical methods are forms of Newton\u2019s method, this advance is exciting because it may provide the key to creating faster program-analysis algorithms. However, there is an important difference between the dataflow-analysis and numerical-analysis contexts: when Newton\u2019s method is used in numerical problems, commutativity of multiplication is relied on to rearrange an expression of the form \u201c\u201d into \u201c.\u201d Equations with such expressions correspond to path\u00a0\u2026", "num_citations": "1\n", "authors": ["1739"]}
{"title": "A Denotational Semantics for Nondeterminism in Probabilistic Programs\n", "abstract": " Probabilistic programming is an increasingly popular technique for modeling randomness and uncertainty. Designing semantic models for probabilistic programs is technically challenging and has been extensively studied. A particular complication is to precisely account for nondeterminism, which is often used to represent adversarial actions in probabilistic models, and to power refinementbased development. This paper studies a denotational semantics for probabilistic programs that is based on a novel treatment of the nondeterminism, which involves nondeterminacy among transformers instead of states. The studied language combines nondeterminism with interesting features such as continuous sampling, conditioning, unstructured control-flow, general recursion, and local variables. The semantics is based on a domain-theoretic characterization of sub-probability kernels, defining a notion of transition maps, as well as constructing powerdomains over transition maps to model nondeterminism. Semantic objects in the powerdomain enjoy general convexity, which is a generalization of convexity. As an application, the paper studies the semantic foundations of an algebraic framework for static analysis of probabilistic programs and demonstrates that the denotational semantics is instrumental for the effectiveness of the analysis.", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Through the lens of abstraction\n", "abstract": " Establishing that a program is correct is undecidable in general. Program-analysis and verification tools sidestep this tar-pit of undecidability by working on an abstraction of a program, which over-approximates the original program\u2019s behavior. The theory underlying this approach is called abstract interpretation [1]. Abstract interpretation provides a way to obtain information about the possible states that a program reaches during execution, but without actually running the program on specific inputs. Instead, it explores the program\u2019s behavior for all possible inputs, thereby accounting for all possible states that the program can reach.Operationally, one can think of abstract interpretation as running the program \u201cin the aggregate\u201d. That is, rather than executing the program on ordinary states, the program is executed on abstract states, which are finite-sized descriptors that represent collections of states. For example, one can use abstract states that represent only the sign of a variable\u2019s value: neg, zero, pos, or unknown. If the abstract state is [a\u21a6\u2192 neg, b\u21a6\u2192 neg]), the product \u201ca\u2217 b\u201d would be performed as \u201cneg\u2217 neg\u201d, yielding pos. This approximation discards information about the specific values of a and b:[a\u21a6\u2192 neg, b\u21a6\u2192 neg] represents all states in which a and b hold negative integers.", "num_citations": "1\n", "authors": ["1739"]}
{"title": "A method for symbolic computation of precise abstract transformers\n", "abstract": " In 1979, Cousot and Cousot gave a specification of the \u201cbest\u201d(most-precise) abstract transformer possible for a given concrete transformer and a given abstract domain. Unfortunately, their specification does not lead to an algorithm for obtaining best transformers. In fact, algorithms are known for only a few abstract domains. This paper presents a parametric framework that, for some abstract domains, is capable of obtaining best transformers in the limit. Because the method approaches best transformers from \u201cabove\u201d, if the computation takes too much time it can be stopped to yield a sound abstract transformer. Thus, the framework provides a tunable algorithm that offers a performance-versus-precision trade-off. We describe instantiations of the framework for well-known abstract domains, such as intervals, polyhedra, and Cartesian predicate abstraction. We also show that the framework applies to several new variants of predicate-abstraction domains that we define in the paper.", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Report of a Workshop on Future Directions in Programming Languages and Compilers\n", "abstract": " PurposeIn January, 1993, a panel of experts in the area of programming languages and compilers met in a one and a half day workshop to discuss the future of research in that area. This paper is the report of their ndings. Its purposes are to explain the need for, and benets of, research in this eld| both basic and applied; to broadly survey the various parts of the eld and indicate its general research directions; and to propose an initiative aimed at moving basic research results into wider use.", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Cornell \u7a0b\u5e8f\u7efc\u5408\u5668\u8bed\u6cd5\u5236\u5bfc\u7a0b\u5e8f\u8bbe\u8ba1\u73af\u5883\n", "abstract": " \u7a0b\u5e8f\u5e76\u4e0d\u662f\u6b63\u6587\u6587\u672c, \u800c\u662f\u5177\u6709\u5206\u5c42\u7684\u8ba1\u7b97\u7ed3\u6784, \u5e76\u4e14\u5e94\u8be5\u5728\u81ea\u59cb\u81f4\u7ec8\u627f\u8ba4\u548c\u652f\u6301\u8fd9\u79cd\u89c2\u70b9\u7684\u73af\u5883\u4e2d\u7f16\u8f91, \u6267\u884c\u548c\u8c03\u8bd5. Cornell \u7a0b\u5e8f\u7efc\u5408\u5668\u8981\u6c42\u5728\u7a0b\u5e8f\u5f00\u53d1\u7684\u6240\u6709\u9636\u6bb5\u90fd\u662f\u7ed3\u6784\u5316\u7684. \u7efc\u5408\u5668\u7684\u5404\u79cd\u7279\u6027\u5747\u4ee5\u7a0b\u5e8f\u8bbe\u8ba1\u8bed\u8a00\u7684\u8bed\u6cd5\u4e3a\u57fa\u7840, \u5e76\u4e14\u5c06\u5168\u5c4f\u5e55\u6d3e\u751f\u6811\u7f16\u8f91\u5668\u4e0e\u8bed\u6cd5\u5236\u5bfc\u7684\u8bca\u65ad\u89e3\u91ca\u5668\u76f8\u7ed3\u5408, \u4f7f\u5f97\u7efc\u5408\u5668\u6210\u4e3a\u4e00\u4e2a\u5f3a\u6709\u529b\u7684, \u5e94\u7b54\u5f0f\u7684\u4ea4\u4e92\u578b\u7a0b\u5e8f\u8bbe\u8ba1\u5de5\u5177.", "num_citations": "1\n", "authors": ["1739"]}
{"title": "Position Paper: Semantics-Based Program Manipulation\n", "abstract": " With tens of billions of dollars spent annually on software, and hundreds of billions of dollars already invested in existing large complex systems, one of the most important problems in software engineering is to make program development and enhancement easier, faster, and less error prone. Because of the pervasiveness of software in modern technology, even small improvements in our ability to develop and enhance software would lead to saving of tens of millions of dollars per year. In seeking leverage to apply to this problem, it is natural to ask questions such asIs it possible to apply computers to aid in this task? Is it possible to create a new class of tools that will aid programmers in creating new code and in understanding, enhancing, debugging, testing, and reusing existing code? Is it possible to create tools that change the character of the programming task in fundamental ways?", "num_citations": "1\n", "authors": ["1739"]}