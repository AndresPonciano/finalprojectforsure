{"title": "Adaptive random test case prioritization\n", "abstract": " Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the \"additional\" techniques) and yet involves much less time cost.", "num_citations": "270\n", "authors": ["138"]}
{"title": "Coincidental Correctness: Refine Code Coverage with Context Pattern to Improve Fault Localization\n", "abstract": " Coincidental Correctness: Refine Code Coverage with Context Pattern to Improve Fault Localization - HKUST SPD | The Institutional Repository Skip to content Search Publications Advanced Search Profiles Coincidental Correctness: Refi... Please use this identifier to cite or link to this item: http://hdl.handle.net/1783.1/8828 Coincidental Correctness: Refine Code Coverage with Context Pattern to Improve Fault Localization Author Wang, Xinming Cheung, Shing Chi View this author's profile Chan, WK Zhang, Zhenyu Issue Date 2009 Source 31st International Conference on Software Engineering (ICSE 2009), Vancouver, Canada, May 2009, pp. 45-55 Language English Genre Conference paper Usage Metrics 40 Page views Similar Items Taming coincidental correctness: Coverage refinement with context patterns to improve fault localization Author(s): Wang, Xinming; Cheung, Shing Chi; Chan, WK... 2009 Automatic \u2026", "num_citations": "186\n", "authors": ["138"]}
{"title": "Capturing propagation of infected program states\n", "abstract": " Coverage-based fault-localization techniques find the fault-related positions in programs by comparing the execution statistics of passed executions and failed executions. They assess the fault suspiciousness of individual program entities and rank the statements in descending order of their suspiciousness scores to help identify faults in programs. However, many such techniques focus on assessing the suspiciousness of individual program entities but ignore the propagation of infected program states among them. In this paper, we use edge profiles to represent passed executions and failed executions, contrast them to model how each basic block contributes to failures by abstractly propagating infected program states to its adjacent basic blocks through control flow edges. We assess the suspiciousness of the infected program states propagated through each edge, associate basic blocks with edges via such\u00a0\u2026", "num_citations": "140\n", "authors": ["138"]}
{"title": "Test case prioritization for regression testing of service-oriented business applications\n", "abstract": " Regression testing assures the quality of modified service-oriented business applications against unintended changes. However, a typical regression test suite is large in size. Earlier execution of those test cases that may detect failures is attractive. Many existing prioritization techniques order test cases according to their respective coverage of program statements in a previous version of the application. On the other hand, industrial service-oriented business applications are typically written in orchestration languages such as WS-BPEL and integrated with workflow steps and web services via XPath and WSDL. Faults in these artifacts may cause the application to extract wrong data from messages, leading to failures in service compositions. Surprisingly, current regression testing research hardly considers these artifacts. We propose a multilevel coverage model to capture the business process, XPath, and WSDL\u00a0\u2026", "num_citations": "105\n", "authors": ["138"]}
{"title": "Modeling and testing of cloud applications\n", "abstract": " What is a cloud application precisely? In this paper, we formulate a computing cloud as a kind of graph, a computing resource such as services or intellectual property access rights as an attribute of a graph node, and the use of a resource as a predicate on an edge of the graph. We also propose to model cloud computation semantically as a set of paths in a subgraph of the cloud such that every edge contains a predicate that is evaluated to be true. Finally, we present algorithms to compose cloud computations and a family of model-based testing criteria to support the testing of cloud applications.", "num_citations": "100\n", "authors": ["138"]}
{"title": "Non-parametric statistical fault localization\n", "abstract": " Fault localization is a major activity in program debugging. To automate this time-consuming task, many existing fault-localization techniques compare passed executions and failed executions, and suggest suspicious program elements, such as predicates or statements, to facilitate the identification of faults. To do that, these techniques propose statistical models and use hypothesis testing methods to test the similarity or dissimilarity of proposed program features between passed and failed executions. Furthermore, when applying their models, these techniques presume that the feature spectra come from populations with specific distributions. The accuracy of using a model to describe feature spectra is related to and may be affected by the underlying distribution of the feature spectra, and the use of a (sound) model on inapplicable circumstances to describe real-life feature spectra may lower the effectiveness of\u00a0\u2026", "num_citations": "59\n", "authors": ["138"]}
{"title": "Fault localization through evaluation sequences\n", "abstract": " Predicate-based statistical fault-localization techniques find fault-relevant predicates in a program by contrasting the statistics of the evaluation results of individual predicates between failed runs and successful runs. While short-circuit evaluations may occur in program executions, treating predicates as atomic units ignores this fact, masking out various types of useful statistics on dynamic program behavior. In this paper, we differentiate the short-circuit evaluations of individual predicates on individual program statements, producing one set of evaluation sequences per predicate. We then investigate experimentally the effectiveness of using these sequences to locate faults by comparing existing predicate-based techniques with and without such differentiation. We use both the Siemens program suite and four real-life UNIX utility programs as our subjects. The experimental results show that the proposed use of short\u00a0\u2026", "num_citations": "52\n", "authors": ["138"]}
{"title": "An empirical comparison between direct and indirect test result checking approaches\n", "abstract": " An oracle on software testing is a mechanism for checking whether the system under test has behaved correctly for any executions. In some situations, oracles are unavailable or too expensive to apply. This is known as the oracle problem. It is crucial to develop techniques to address it, and metamorphic testing (MT) was one of such proposals. This paper conducts a controlled experiment to investigate the cost effectiveness of using MT by 38 testers on three open-source programs. The fault detection capability and time cost of MT are compared with the popular assertion checking method. Our results show that MT is cost-efficient and has potentials for detecting more faults than the assertion checking method.", "num_citations": "49\n", "authors": ["138"]}
{"title": "Are slice-based cohesion metrics actually useful in effort-aware post-release fault-proneness prediction? An empirical study\n", "abstract": " Background. Slice-based cohesion metrics leverage program slices with respect to the output variables of a module to quantify the strength of functional relatedness of the elements within the module. Although slice-based cohesion metrics have been proposed for many years, few empirical studies have been conducted to examine their actual usefulness in predicting fault-proneness. Objective. We aim to provide an in-depth understanding of the ability of slice-based cohesion metrics in effort-aware post-release fault-proneness prediction, i.e. their effectiveness in helping practitioners find post-release faults when taking into account the effort needed to test or inspect the code. Method. We use the most commonly used code and process metrics, including size, structural complexity, Halstead's software science, and code churn metrics, as the baseline metrics. First, we employ principal component analysis to analyze\u00a0\u2026", "num_citations": "46\n", "authors": ["138"]}
{"title": "Fault localization based only on failed runs\n", "abstract": " Fault localization commonly relies on both passed and failed runs, but passed runs are generally susceptible to coincidental correctness and modern software automatically produces a huge number of bug reports on failed runs. FOnly is an effective new technique that relies only on failed runs to locate faults statistically.", "num_citations": "39\n", "authors": ["138"]}
{"title": "Experimental study to compare the use of metamorphic testing and assertion checking\n", "abstract": " A test oracle in software testing is a mechanism for checking whether the program under test behaves correctly for any execution. In some practical situations, oracles can be unavailable or too expensive to apply. Metamorphic testing (MT) was proposed to alleviate this problem so that software can be delivered under the time-to-market pressure. However, the effectiveness of MT has not been studied adequately. This paper conducts a controlled experiment to investigate the cost effectiveness of using MT. The fault detection capability and time cost of MT are compared with the standard assertion checking method. Our results show that MT has potentials to detect more faults than the assertion checking method. The experimental results also show a trade-off between the two testing methods: MT can be less efficient but more effective, and can be defined at a coarser level of granularity than the assertion checking method.", "num_citations": "36\n", "authors": ["138"]}
{"title": "Is non-parametric hypothesis testing model robust for statistical fault localization?\n", "abstract": " Fault localization is one of the most difficult activities in software debugging. Many existing statistical fault-localization techniques estimate the fault positions of programs by comparing the program feature spectra between passed runs and failed runs. Some existing approaches develop estimation formulas based on mean values of the underlying program feature spectra and their distributions alike. Our previous work advocates the use of a non-parametric approach in estimation formulas to pinpoint fault-relevant positions. It is worthy of further study to resolve the two schools of thought by examining the fundamental, underlying properties of distributions related to fault localization. In particular, we ask: Can the feature spectra of program elements be safely considered as normal distributions so that parametric techniques can be soundly and powerfully applied? In this paper, we empirically investigate this question\u00a0\u2026", "num_citations": "35\n", "authors": ["138"]}
{"title": "A subsumption hierarchy of test case prioritization for composite services\n", "abstract": " Many composite workflow services utilize non-imperative XML technologies such as WSDL, XPath, XML schema, and XML messages. Regression testing should assure the services against regression faults that appear in both the workflows and these artifacts. In this paper, we propose a refinement-oriented level-exploration strategy and a multilevel coverage model that captures progressively the coverage of different types of artifacts by the test cases. We show that by using them, the test case prioritization techniques initialized on top of existing greedy-based test case prioritization strategy form a subsumption hierarchy such that a technique can produce more test suite permutations than a technique that subsumes it. Our experimental study of a model instance shows that a technique generally achieves a higher fault detection rate than a subsumed technique, which validates that the proposed hierarchy and\u00a0\u2026", "num_citations": "34\n", "authors": ["138"]}
{"title": "A general noise-reduction framework for fault localization of Java programs\n", "abstract": " ContextExisting fault-localization techniques combine various program features and similarity coefficients with the aim of precisely assessing the similarities among the dynamic spectra of these program features to predict the locations of faults. Many such techniques estimate the probability of a particular program feature causing the observed failures. They often ignore the noise introduced by other features on the same set of executions that may lead to the observed failures. It is unclear to what extent such noise can be alleviated.ObjectiveThis paper aims to develop a framework that reduces the noise in fault-failure correlation measurements.MethodWe develop a fault-localization framework that uses chains of key basic blocks as program features and a noise-reduction methodology to improve on the similarity coefficients of fault-localization techniques. We evaluate our framework on five base techniques using\u00a0\u2026", "num_citations": "29\n", "authors": ["138"]}
{"title": "Accuracy graphs of spectrum-based fault localization formulas\n", "abstract": " The effectiveness of spectrum-based fault localization techniques primarily relies on the accuracy of their fault localization formulas. Theoretical studies prove the relative accuracy orders of selected formulas under certain assumptions, forming a graph of their theoretical accuracy relations. However, it is unclear whether in such a graph the relative positions of these formulas may change when some assumptions are relaxed. On the other hand, empirical studies can measure the actual accuracy of any formula in controlled settings that more closely approximate practical scenarios but in less general contexts. In this paper, we propose an empirical framework of accuracy graphs and their construction that reveal the relative accuracy of formulas. Our work not only evaluates the association between certain assumptions and the theoretical relations among formulas, but also expands our knowledge to reveal new\u00a0\u2026", "num_citations": "23\n", "authors": ["138"]}
{"title": "Clustering-based acceleration for virtual machine image deduplication in the cloud environment\n", "abstract": " More and more virtual machine (VM) images are continuously created in datacenters. Duplicated data segments may exist in such VM images, and it leads to a waste of storage resource. As a result, VM image deduplication is a common daily activity in datacenters. Our previous work Crab is such a product and it is on duty regularly in our datacenter.The size of VM images is large and the amount of VM images is huge, and it is inefficient and impractical to load massive VM image fingerprints into memory for a fast comparison to recognize duplicated segments. To address this issue, we in this paper propose a clustering-based acceleration method. It uses an improved k-means clustering to find images having high chances to contain duplicated segments. With such a candidate selection phase, only limited VM image candidate fingerprints are loaded into memory.We empirically evaluate the effectiveness\u00a0\u2026", "num_citations": "22\n", "authors": ["138"]}
{"title": "Debugging through evaluation sequences: a controlled experimental study\n", "abstract": " Predicate-based statistical fault-localization techniques locate fault-relevant predicates in a program by contrasting the statistics of the values of individual predicates between successful and failure-causing runs. While short-circuit evaluations are common in program execution, treating predicates as atomic units ignores this fact, masking out various types of important statistics. On the contrary, are such statistics useful for debugging? In this paper, we investigate experimentally the impact of the use of short-circuit evaluation information on fault localization. The results show that, by doing so, it significantly improves predicate-based statistical fault-localization techniques.", "num_citations": "21\n", "authors": ["138"]}
{"title": "A dynamic fault localization technique with noise reduction for Java programs\n", "abstract": " Existing fault localization techniques combine various program features and similarity coefficients with the aim of precisely assessing the similarities among the dynamic spectra of these program features to predict the locations of faults. Many such techniques estimate the probability of a particular program feature causing the observed failures. They ignore the noise introduced by the other features on the same set of executions that may lead to the observed failures. In this paper, we propose both the use of chains of key basic blocks as program features and an innovative similarity coefficient that has noise reduction effect. We have implemented our proposal in a technique known as MKBC. We have empirically evaluated MKBC using three real-life medium-sized programs with real faults. The results show that MKBC outperforms Tarantula, Jaccard, SBI, and Ochiai significantly.", "num_citations": "20\n", "authors": ["138"]}
{"title": "On the adoption of MC/DC and control-flow adequacy for a tight integration of program testing and statistical fault localization\n", "abstract": " ContextTesting and debugging consume a significant portion of software development effort. Both processes are usually conducted independently despite their close relationship with each other. Test adequacy is vital for developers to assure that sufficient testing effort has been made, while finding all the faults in a program as soon as possible is equally important. A tight integration between testing and debugging activities is essential.ObjectiveThe paper aims at finding whether three factors, namely, the adequacy criterion to gauge a test suite, the size of a prioritized test suite, and the percentage of such a test suite used in fault localization, have significant impacts on integrating test case prioritization techniques with statistical fault localization techniques.MethodWe conduct a controlled experiment to investigate the effectiveness of applying adequate test suites to locate faults in a benchmark suite of seven\u00a0\u2026", "num_citations": "18\n", "authors": ["138"]}
{"title": "More tales of clouds: software engineering research issues from the cloud application perspective\n", "abstract": " Cloud computing is an emerging computing paradigm. It aims to share data, calculations, and services transparently among users of a massive grid. Although the industry has started selling cloud-computing products, the software engineering infrastructure and application model are still unclear. In this paper, we compare cloud computing with service-oriented computing and pervasive computing. Both the industry and research community have actively examined these three computing paradigms. In this paper, we draw a qualitative comparison among their application characteristics based on the classic model of computer architecture from the cloud application perspective. We contrast the difference and pinpoint areas that researchers may examine in the future.", "num_citations": "17\n", "authors": ["138"]}
{"title": "Where to adapt dynamic service compositions\n", "abstract": " Peer services depend on one another to accomplish their tasks, and their structures may evolve. A service composition may be designed to replace its member services whenever the quality of the composite service fails to meet certain quality-of-service (QoS) requirements. Finding services and service invocation endpoints having the greatest impact on the quality are important to guide subsequent service adaptations. This paper proposes a technique that samples the QoS of composite services and continually analyzes them to identify artifacts for service adaptation. The preliminary results show that our technique has the potential to effectively find such artifacts in services.", "num_citations": "13\n", "authors": ["138"]}
{"title": "Fault localization with non-parametric program behavior model\n", "abstract": " Fault localization is a major activity in software debugging. Many existing statistical fault localization techniques compare feature spectra of successful and failed runs. Some approaches, such as SOBER, test the similarity of the feature spectra through parametric self-proposed hypothesis testing models. Our finding shows, however, that the assumption on feature spectra forming known distributions is not well-supported by empirical data. Instead, having a simple, robust, and explanatory model is an essential move toward establishing a debugging theory. This paper proposes a non-parametric approach to measuring the similarity of the feature spectra of successful and failed runs, and picks a general hypothesis testing model, namely the Mann-Whitney test, as the core. The empirical results on the Siemens suite show that our technique can outperform existing predicate-based statistical fault localization techniques\u00a0\u2026", "num_citations": "12\n", "authors": ["138"]}
{"title": "BPELDebugger: An effective BPEL-specific fault localization framework\n", "abstract": " ContextBusiness Process Execution Language (BPEL) is a widely recognized executable service composition language, which is significantly different from typical programming languages in both syntax and semantics, and especially shorter in program scale. How to effectively locate faults in BPEL programs is an open and challenging problem.ObjectiveIn this paper, we propose a fault localization framework for BPEL programs.MethodBased on BPEL program characteristics, we propose two fault localization guidelines to locate the integration and interaction faults in BPEL programs. Our framework formulates the BPEL fault localization problem using the popular fault localization problem settings, and synthesizes BPEL-specific fault localization techniques by reuse of existing fault localization formulas. We use two realistic BPEL programs and three existing fault localization formulas to evaluate the feasibility and\u00a0\u2026", "num_citations": "11\n", "authors": ["138"]}
{"title": "Resource prioritization of code optimization techniques for program synthesis of wireless sensor network applications\n", "abstract": " Wireless sensor network (WSN) applications sense events in situ and compute results in-network. Their software components should run on platforms with stringent constraints on node resources. To meet these constraints, developers often design their programs by trial-and-error. Such manual process is time-consuming and error-prone.Based on an existing task view that treats a WSN application as tasks and models resources as constraints, we propose a new component view that associates components with code optimization techniques and constraints. We provide a visualization mechanism to help developers select code optimization techniques. We also develop algorithms to synthesize components running on nodes, fulfilling the constraints, and thus optimizing their quality.", "num_citations": "10\n", "authors": ["138"]}
{"title": "PAFL: Fault Localization via Noise Reduction on Coverage Vector.\n", "abstract": " Coverage-based fault localization techniques assess the extent of how much a program entity relates to faults by contrasting the execution spectra of passed executions and failed executions. However, previous studies show that different test cases may generate similar or identical coverage information in program execution, which makes the execution spectra of program entities indistinguishable to one another, thus involves noise and decreases the effectiveness of existing techniques. In this paper, we use the concept of coverage vector to model program coverage in execution, compare coverage vectors to capture the similarity among test cases, reduce noise by removing similar coverage vector to refine the execution spectra, and based on them assess the suspicious basic blocks being related to fault. We thus narrow down the search region and facilitate fault localization. The empirical evaluation using Siemens programs and realistic UNIX utilities shows that our technique effectively addresses the problem caused by similar test cases and outperforms existing representative techniques.", "num_citations": "9\n", "authors": ["138"]}
{"title": "Program structure aware fault localization\n", "abstract": " Software testing is always an effective method to show the presence of bugs in programs, while debugging is never an easy task to remove a bug from a program. To facilitate the debugging task, statistical fault localization estimates the location of faults in programs automatically by analyzing the program executions to narrow down the suspicious region. We observe that program structure has strong impacts on the assessed suspiciousness of program elements. However, existing techniques inadequately pay attention to this problem. In this paper, we emphasize the biases caused by program structure in fault localization, and propose a method to address them. Our method is dedicated to boost a fault localization technique by adapting it to various program structures, in a software development process. It collects the suspiciousness of program elements when locating historical faults, statistically captures the\u00a0\u2026", "num_citations": "7\n", "authors": ["138"]}
{"title": "Facilitating Monkey Test by Detecting Operable Regions in Rendered GUI of Mobile Game Apps\n", "abstract": " Graphical User Interface (GUI) is a component of many software applications. Many mobile game applications in particular have to provide excellent user experiences using graphical engines to render GUI screens. On a rendered GUI screen such as a treasury map, no GUI widget is embodied in it and the operable GUI regions, each of which is a region that triggers actions when certain events acting on these regions, may only be implicitly determinable. Traditional testing tools like monkey test do not effectively generate effective event sequences over such operable GUI regions. Our insight is that operable regions in a rendered GUI screen of many mobile game applications are given with visible hints to catch user attentions. In this paper, we propose Smart Monkey, which uses the fundamental features of a screen, including color, intensity, and texture, as visual signals to detect operable GUI region candidates\u00a0\u2026", "num_citations": "4\n", "authors": ["138"]}
{"title": "Precise propagation of fault-failure correlations in program flow graphs\n", "abstract": " Statistical fault localization techniques find suspicious faulty program entities in programs by comparing passed and failed executions. Existing studies show that such techniques can be promising in locating program faults. However, coincidental correctness and execution crashes may make program entities indistinguishable in the execution spectra under study, or cause inaccurate counting, thus severely affecting the precision of existing fault localization techniques. In this paper, we propose a Block Rank technique, which calculates, contrasts, and propagates the mean edge profiles between passed and failed executions to alleviate the impact of coincidental correctness. To address the issue of execution crashes, Block Rank identifies suspicious basic blocks by modeling how each basic block contributes to failures by apportioning their fault relevance to surrounding basic blocks in terms of the rate of successful\u00a0\u2026", "num_citations": "4\n", "authors": ["138"]}
{"title": "Replay Debugging of Real-Time Vxworks Applications\n", "abstract": " Debugging multi-task real-time VxWorks applications is tedious and time-consuming for developers. The non-determinism within the application execution makes the developers hard to reproduce a failure.1As a result, the developers cannot perform cyclic debugging easily on these real-time applications. Replay debugging techniques can help developers to replay the failure scenario with determinism. In this paper, we propose an approach to replaying real-time VxWorks applications. We make use of dynamic binary instrumentation to record data flow non-determinism. Furthermore, we instrument the VxWorks kernel to record control flow non-determinism. Finally, the recorded information together with a modified VxWorks kernel is used by our replayer to support effective replaying. Our evaluation results show that our approach can effectively replay real-time multi-task VxWorks applications with low overhead.", "num_citations": "3\n", "authors": ["138"]}
{"title": "Markov model-based effectiveness predicting for software fault localization\n", "abstract": " Debugging is a necessary phase in software development. Statistical fault localization techniques estimate fault locations by analyzing dynamic program spectra. They build different heuristic analytical models for different program spectra to describe the program behavior. Previous studies show that their effectiveness is related to the target faults and program types; and there is no universally effective technique. By evaluating the feasibility of predicting fault class in a unit test process, this paper employs a Markov model to select a proper such technique to apply, from a candidate set. Empirical study shows it is more effective to locate faults.", "num_citations": "3\n", "authors": ["138"]}
{"title": "Analyzing GUI running fluency for Android apps\n", "abstract": " Android as a free open platform has become increasingly popular and been widespread adopted in mobile, tablet, and other devices. However, a great number of issues, such as inadequate quality and the fragmentation phenomenon, have emerged, enhancing the difficulty of developing. Among them, the running fluency of Android apps directly affects user experience directly. As a result, it is of great significance to detect and analyze it.", "num_citations": "2\n", "authors": ["138"]}
{"title": "A fault localization framework to alleviate the impact of execution similarity\n", "abstract": " Coverage-based fault localization (CBFL) techniques contrast the execution spectra of a program entity to assess the extent of how much a program entity is being related to faults. However, different test cases may result in similar executions, which further make the execution spectra of program entities be indistinguishable among similar executions. As a consequence, most of the current CBFL techniques are impacted by the noise of indistinguishable spectra. To alleviate the impact of execution similarity and improve the effectiveness of CBFL techniques, we propose a general fault localization framework. This framework is general to current execution spectra based CBFL techniques, which could synthesize a fault localization technique based on a given base technique. To synthesize the new technique, we use the concept of coverage vector to model execution spectra and capture the execution similarity, then\u00a0\u2026", "num_citations": "2\n", "authors": ["138"]}
{"title": "Toward effectively locating integration-level faults in BPEL programs\n", "abstract": " Business Process Execution Language (BPEL) is a widely recognized executable service composition language. Since BPEL integrates services of desired functionality to compose business processes, it is significantly different from typical programming languages. How to effectively locate the integration-level faults in BPEL programs is an open issue. In this paper, we propose the BPEL fault localization guidelines based on the characteristics of BPEL programs, and adapt Tarantula, a traditional fault localization technique, to locate the integration-level faults in BPEL programs. We also conducted an empirical study to demonstrate the feasibility of our methodology.", "num_citations": "2\n", "authors": ["138"]}
{"title": "Synthesizing component-based WSN applications via automatic combination of code optimization techniques\n", "abstract": " Wireless sensor network (WSN) applications sense events in-situ and compute results in-network. Their software components should run on platforms with stringent constraints on node resources. Developers often design their programs by trial-and-error with a view to meeting these constraints. Through numerous iterations, they manually measure and estimate how far the programs cannot fulfill the requirements, and make adjustments accordingly. Such manual process is time-consuming and error-prone. Automated support is necessary. Based on an existing task view that treats a WSN application as tasks and models resources as constraints, we propose a new component view that associates components with code optimization techniques and constraints. We develop algorithms to synthesize components running on nodes, fulfilling the constraints, and thus optimizing their quality. We evaluate our proposal by a\u00a0\u2026", "num_citations": "2\n", "authors": ["138"]}
{"title": "Slope-based Sequencing Yardstick for Analyzing Unsatisfactory performance of multithreaded programs\n", "abstract": " As users are increasingly concerned about energy efficiency, they are also increasingly intolerant of performance anomalies of programs that may cause significant energy waste. Bug localization is a bottleneck in the development of multithreaded programs. Although both static and dynamic performance bug localization techniques have been proposed, they cannot handle performance anomalies with unforeseen patterns, and cannot work well if the concept of performance anomaly is fuzzy or evolves over time for the same program. We propose a novel model-based approach to performance bug localization. The approach is based on curve fitting and trend estimation over program executions with performance data. We describe our trend estimation model and illustrate it with the result of a case study on locating three real-world performance bugs in MySQL.", "num_citations": "1\n", "authors": ["138"]}
{"title": "Mining associations to improve the effectiveness of fault localization\n", "abstract": " Coverage-based fault localization(CBFL) techniques find the fault-related positions in programs by comparing the execution statistics of passed executions and failed executions have been proven to be efficient by several empirical studies. However, these techniques assess the suspiciousness of program entities individually, whereas the individual coverage information cannot reflect the complicated control- and data-dependency relationships, and thus oversimplify the execution spectra. In this paper, we first use motivating examples to show the impact of the cause-effect relationship on the effectiveness of CBFL. Second, we propose the rules of program failures and design the execution analysis model based on the coverage of different program execution spectrum. By computing the frequency items for statements with high suspiciousness, we also bring out the coverage vector to mine fault-prone statements\u00a0\u2026", "num_citations": "1\n", "authors": ["138"]}
{"title": "Wielding Statistical Fault Localization Statistically\n", "abstract": " Program debugging is a laborious but necessary phase of software development. It generally consists of fault localization, bug fix, and regression testing. Statistical software fault localization automates the manual and error-prone first task. It predicts fault locations by analyzing dynamic program spectrum captured in program runs. Previous studies mostly focused on how to provide reliable input data to such a technique and how to process the data accurately, but inadequately studied how to wield the output result of such a technique. In this work, we raise the assumption of symmetric distribution on the effectiveness of such a technique in locating faults, based on empirical results. We use maximum likelihood estimate and linear programming to develop a tuning method to enhance the result of a statistical fault localization technique. Experiments with two representative such techniques on two realistic UNIX utility\u00a0\u2026", "num_citations": "1\n", "authors": ["138"]}
{"title": "A Community-Centric Model for Service Publication, Discovery, Selection, Binding, and Maintenance\n", "abstract": " Services discovery, selection, composition, verification, and adaptation are important in service-oriented computing. Existing researches often study techniques to maximize the benefits of individual services. However, following the power laws, a small fraction of quality services offers their executions to support a significant portion of all service requests. We argue that locating and maintaining such a small and significant set of services is important to the development of service-oriented computing. In this paper, we propose the notion of adaptive service-oriented community. A community consists of peer reviewed services, and only those operations of member services that the community collectively exceeds a significance threshold are discoverable and bondable. Services also select such communities to bind to its requested operations primarily based on their significance. Our proposal essentially raises a service\u00a0\u2026", "num_citations": "1\n", "authors": ["138"]}
{"title": "Software debugging through dynamic analysis of program structures\n", "abstract": " Software debugging is difficult and time-consuming because developers do not know the locations of the faults in advance and they may require many failed executions to be produced in addition to the very first observed one. Modern software often provides error reporting for users to feedback the failure information to developers, opening a new door of using many lowcost failed executions statistically. Many existing fault-localization techniques correlate failures to fault positions. They not only ignore how infected program states propagate among statements, but also require passed executions to be available. Such unnecessary constraints limit the power of statistical approach to fault localization. Zhenyu Zhang's thesis work presents the results of my investigation to tackle these problems.Zhenyu Zhang addresses the first problem, namely strong correlation not necessarily the root cause of the observed failures\u00a0\u2026", "num_citations": "1\n", "authors": ["138"]}
{"title": "Testing in Parallel-A Need for Practical Regression Testing.\n", "abstract": " When software evolves, its functionalities are evaluated using regression testing. In a regression testing process, a test suite is augmented, reduced, prioritized, and run on a software build version. Regression testing has been used in industry for decades; while in some modern software activities, we find that regression testing is yet not practical to apply. For example, according to our realistic experiences in Sohu. com Inc., running a reduced test suite, even concurrently, may cost two hours or longer. Nevertheless, in an urgent task or a continuous integration environment, the version builds and regression testing requests may come more often. In such a case, it is not strange that a new round of test suite run needs to start before all the previous ones have terminated. As a solution, running test suites on different build versions in parallel may increase the efficiency of regression testing and facilitate evaluating the fitness of software evolutions. On the other hand, hardware and software resources limit the number of paralleled tasks. In this paper, we raise the problem of testing in parallel, give the general problem settings, and use a pipeline presentation for data visualization. Solving this problem is expected to make practical regression testing.", "num_citations": "1\n", "authors": ["138"]}