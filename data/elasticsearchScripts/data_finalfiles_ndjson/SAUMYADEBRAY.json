{"title": "Obfuscation of executable code to improve resistance to static disassembly\n", "abstract": " A great deal of software is distributed in the form of executable code. The ability to reverse engineer such executables can create opportunities for theft of intellectual property via software piracy, as well as security breaches by allowing attackers to discover vulnerabilities in an application. The process of reverse engineering an executable program typically begins with disassembly, which translates machine code to assembly code. This is then followed by various decompilation steps that aim to recover higher-level abstractions from the assembly code. Most of the work to date on code obfuscation has focused on disrupting or confusing the decompilation phase. This paper, by contrast, focuses on the initial disassembly phase. Our goal is to disrupt the static disassembly process so as to make programs harder to disassemble correctly. We describe two widely used static disassembly algorithms, and discuss\u00a0\u2026", "num_citations": "930\n", "authors": ["271"]}
{"title": "Reasoning about naming systems\n", "abstract": " This paper reasons about naming systems as specialized inference mechanisms, It describes a preference)-zierarch. v that can be used to specify the structure of a naming system\u2019s inference mechanism and defines criteria by which different naming systems can be evaluated, For example, the preference hierarchy allows one to compare naming systems based on how dkcrzmznating they are and to identify the class of names for which a given naming system is sound and complete. A study of several example naming systems demonstrates how the preference hierarchy can be used as a formal tool for designing naming systems.", "num_citations": "536\n", "authors": ["271"]}
{"title": "Compiler techniques for code compaction\n", "abstract": " In recent years there has been an increasing trend toward the incorpor ation of computers into a variety of devices where the amount of memory available is limited. This makes it desirable to try to reduce the size of applications where possible. This article explores the use of compiler techniques to accomplish code compaction to yield smaller executables. The main contribution of this article is to show that careful, aggressive, interprocedural optimization, together with procedural abstraction of repeated code fragments, can yield significantly better reductions in code size than previous approaches, which have generally focused on abstraction of repeated instruction sequences. We also show how \u201cequivalent\u201d code fragments can be detected and factored out using conventional compiler techniques, and without having to resort to purely linear treatments of code sequences as in suffix-tree-based approaches, thereby\u00a0\u2026", "num_citations": "365\n", "authors": ["271"]}
{"title": "Disassembly of executable code revisited\n", "abstract": " Machine code disassembly routines form a fundamental component of software systems that statically analyze or modify executable programs, e.g., reverse engineering systems, static binary translators, and link-time optimizers. The task of disassembly is complicated by indirect jumps and the presence of non-executable data - jump tables, alignment bytes, etc. - in the instruction stream. Existing disassembly algorithms are not always able to cope successfully with executable files containing such features, and they fail silently - i.e., produce incorrect disassemblies without any indication that the results they are producing are incorrect. In this paper we examine two commonly-used disassembly algorithms and illustrate their shortcomings. We propose a hybrid approach that performs better than these algorithms in the sense that it is able to detect situations where the disassembly may be incorrect and limit the extent of\u00a0\u2026", "num_citations": "354\n", "authors": ["271"]}
{"title": "Automatic mode inference for logic programs\n", "abstract": " In general, logic programs are undirected, i.e., there is no concept of \u201cinput\u201d and \u201coutput\u201d arguments to a procedure. An argument may be used either as an input or as an output argument, and programs may be executed either in a \u201cforward\u201d direction or in a \u201cbackward\u201d direction. However, it is often the case that in a given program, a predicate is used with some of its arguments used consistently as input arguments and others as output arguments. Such mode information can be used by a compiler to effect various optimizations. This paper considers the problem of automatically inferring the models of the predicates in a program. The dataflow analysis we use is more powerful than approaches relying on syntactic characteristics of programs. Our work differs from that of Mellish in that (1) we give a sound and efficient treatment of variable aliasing in mode inference; (2) by propagating instantiation information using\u00a0\u2026", "num_citations": "286\n", "authors": ["271"]}
{"title": "Deobfuscation: Reverse engineering obfuscated code\n", "abstract": " In recent years, code obfuscation has attracted attention as a low cost approach to improving software security by making it difficult for attackers to understand the inner workings of proprietary software systems. This paper examines techniques for automatic deobfuscation of obfuscated programs, as a step towards reverse engineering such programs. Our results indicate that much of the effects of code obfuscation, designed to increase the difficulty of static analyses, can be defeated using simple combinations of straightforward static and dynamic analyses. Our results have applications to both software engineering and software security. In the context of software engineering, we show how dynamic analyses can be used to enhance reverse engineering, even for code that has been designed to be difficult to reverse engineer. For software security, our results serve as an attack model for code obfuscators, and can\u00a0\u2026", "num_citations": "255\n", "authors": ["271"]}
{"title": "Cost analysis of logic programs\n", "abstract": " Cost analysis of programs has been studied in the context of imperative and functional programming languages. For logic programs, the problem is comphcated by the fact that programs may be nondeterministic and produce multiple solutions. A related problem is that because failure of execution is not an abnormal situation, it is possible to wr~ te programs where irnphclt failures have to be dealt with exphcitly in order to get meaningful results. This paper addresses these problems and develops a method for (semi-) automatlc analysls of the worst-case cost of a large class of logic programs. The prl mary contribution of this paper is the development of techmques to deal with nondeterminism and the generation of multiple solutions via backtracking. Apphcations include program transformation and synthesis, software engineering, and in parallelizing compilers.", "num_citations": "231\n", "authors": ["271"]}
{"title": "Dynamic path-based software watermarking\n", "abstract": " Software watermarking is a tool used to combat software piracy by embedding identifying information into a program. Most existing proposals for software watermarking have the shortcoming that the mark can be destroyed via fairly straightforward semantics-preserving code transformations. This paper introduces path-based watermarking, a new approach to software watermarking based on the dynamic branching behavior of programs. The advantage of this technique is that error-correcting and tamper-proofing techniques can be used to make path-based watermarks resilient against a wide variety of attacks. Experimental results, using both Java bytecode and IA-32 native code, indicate that even relatively large watermarks can be embedded into programs at modest cost.", "num_citations": "222\n", "authors": ["271"]}
{"title": "Static inference of modes and data dependencies in logic programs\n", "abstract": " Mode and data dependency analyses find many applications in the generation of efficient executable code for logic programs. For example, mode information can be used to generate specialized unification instructions where permissible, to detect determinacy and functionality of programs, generate index structures more intelligently, reduce the amount of runtime tests in systems that support goal suspension, and in the integration of logic and functional languages. Data dependency information can be used for various source-level optimizing transformations, to improve backtracking behavior and to parallelize logic programs. This paper describes and proves correct an algorithm for the static inference of modes and data dependencies in a program. The algorithm is shown to be quite efficient for programs commonly encountered in practice.", "num_citations": "202\n", "authors": ["271"]}
{"title": "alto: a link\u2010time optimizer for the Compaq Alpha\n", "abstract": " Traditional optimizing compilers are limited in the scope of their optimizations by the fact that only a single function, or possibly a single module, is available for analysis and optimization. In particular, this means that library routines cannot be optimized to specific calling contexts. Other optimization opportunities, exploiting information not available before link time, such as addresses of variables and the final code layout, are often ignored because linkers are traditionally unsophisticated. A possible solution is to carry out whole\u2010program optimization at link time. This paper describes alto, a link\u2010time optimizer for the Compaq Alpha architecture. It is able to realize significant performance improvements even for programs compiled with a good optimizing compiler with a high level of optimization. The resulting code is considerably faster than that obtained using the OM link\u2010time optimizer, even when the latter is used in\u00a0\u2026", "num_citations": "196\n", "authors": ["271"]}
{"title": "A Generic Approach to Automatic Deobfuscation of Executable Code\n", "abstract": " Malicious software are usually obfuscated to avoid detection and resist analysis. When new malware is encountered, such obfuscations have to be penetrated or removed (\"deobfuscated\") in order to understand the internal logic of the code and devise countermeasures. This paper discusses a generic approach for deobfuscation of obfuscated executable code. Our approach does not make any assumptions about the nature of the obfuscations used, but instead uses semantics-preserving program transformations to simplify away obfuscation code. We have applied a prototype implementation of our ideas to a variety of different kinds of obfuscation, including emulation-based obfuscation, emulation-based obfuscation with runtime code unpacking, and return-oriented programming. Our experimental results are encouraging and suggest that this approach can be effective in extracting the internal logic from code\u00a0\u2026", "num_citations": "194\n", "authors": ["271"]}
{"title": "Binary Obfuscation Using Signals.\n", "abstract": " Reverse engineering of software is the process of recovering higher-level structure and meaning from a lower-level program representation. It can be used for legitimate purposes-eg, to recover source code that has been lost-but it is often used for nefarious purposes, eg, to search for security vulnerabilities in binaries or to steal intellectual property. This paper addresses the problem of making it hard to reverse engineering binary programs by making it difficult to disassemble machine code statically. Binaries are obfuscated by changing many control transfers into signals (traps) and inserting dummy control transfers and\" junk\" instructions after the signals. The resulting code is still a correct program, but even the best current disassemblers are unable to disassemble 40%-60% of the instructions in the program. Furthermore, the disassemblers have a mistaken understanding of over half of the control flow edges. However, the obfuscated program necessarily executes more slowly than the original. Experimental results quantify the degree of obfuscation, stealth of the code, and effects on execution time and code size.", "num_citations": "184\n", "authors": ["271"]}
{"title": "Alias analysis of executable code\n", "abstract": " Recent years have seen increasing interest in systems that reason about and manipulate executable code. Such systems can generally benefit from information about aliasing. Unfortunately, most existing alias analyses are formulated in terms of high-level language features, and are unable to cope with features, such as pointer arithmetic, that pervade executable programs. This paper describes a simple algorithm that can be used to obtain aliasing information for executabie code. In order to be practical, the algorithm is carefut to keep its memory requirements low, sacrificing precision where necessary to achieve this goal. Experimental results indicate that it is nevertheless able to provide a reasonable amount of information about memory references across a variety of benchmark programs.", "num_citations": "169\n", "authors": ["271"]}
{"title": "Deobfuscation of virtualization-obfuscated software: a semantics-based approach\n", "abstract": " When new malware are discovered, it is important for researchers to analyze and understand them as quickly as possible. This task has been made more difficult in recent years as researchers have seen an increasing use of virtualization-obfuscated malware code. These programs are difficult to comprehend and reverse engineer, since they are resistant to both static and dynamic analysis techniques. Current approaches to dealing with such code first reverse-engineer the byte code interpreter, then use this to work out the logic of the byte code program. This outside-in approach produces good results when the structure of the interpreter is known, but cannot be applied to all cases. This paper proposes a different approach to the problem that focuses on identifying instructions that affect the observable behavior of the obfuscated code. This inside-out approach requires fewer assumptions, and aims to complement\u00a0\u2026", "num_citations": "164\n", "authors": ["271"]}
{"title": "Plto: A link-time optimizer for the Intel IA-32 architecture\n", "abstract": " This paper describes PLTO, a link-time instrumentation and optimization tool we have developed for the Intel IA-32 architecture. A number of characteristics of this architecture complicate the task of link-time optimization. These include a large number of op-codes and addressing modes, which increases the complexity of program analysis; variable-length instructions, which complicates disassembly of machine code; a paucity of available registers, which limits the extent of some optimizations; and a reliance on using memory locations for holding values and for parameter passing, which complicates program analysis and optimization. We describe how PLTO addresses these problems and the resulting performance improvements it is able to achieve.", "num_citations": "146\n", "authors": ["271"]}
{"title": "Denotational and operational semantics for Prolog\n", "abstract": " The semantics of PROLOG programs is usually given in terms of the model theory of first-order logic. However, this does not adequately characterizethe computational behavior of PROLOG programs. PROLOG implementations typically use a sequential evaluation strategy based on the textual order of clauses and literals in a program, as well as nonlogical features like cut. In this work we develop a denotational semantics that captures thecomputational behavior of PROLOG. We present a semantics for \u201ccut-free\u201d PROLOG, which is then extended to PROLOG with cut. For each case we develop a congruence proof that relates the semantics to a standard operational interpreter. As an application of our denotational semantics, we show the correctness of some standard \u201cfolk\u201d theorems regarding transformations on PROLOG programs.", "num_citations": "138\n", "authors": ["271"]}
{"title": "Profile-guided code compression\n", "abstract": " As computers are increasingly used in contexts where the amount of available memory is limited, it becomes important to devise techniques that reduce the memory footprint of application programs while leaving them in an executable form. This paper describes an approach to applying data compression techniques to reduce the size of infrequently executed portions of a program. The compressed code is decompressed dynamically (via software) if needed, prior to execution. The use of data compression techniques increases the amount of code size reduction that can be achieved; their application to infrequently executed code limits the runtime overhead due to dynamic decompression; and the use of software decompression renders the approach generally applicable, without requiring specialized hardware. The code size reductions obtained depend on the threshold used to determine what code is \"infrequently\u00a0\u2026", "num_citations": "130\n", "authors": ["271"]}
{"title": "Software protection through dynamic code mutation\n", "abstract": " Reverse engineering of executable programs, by disassembling them and then using program analyses to recover high level semantic information, plays an important role in attacks against software systems, and can facilitate software piracy. This paper introduces a novel technique to complicate reverse engineering. The idea is to change the program code repeatedly as it executes, thereby thwarting correct disassembly. The technique can be made as secure as the least secure component of opaque variables and pseudorandom number generators.", "num_citations": "116\n", "authors": ["271"]}
{"title": "Compositional analysis of modular logic programs\n", "abstract": " This paper describes a semantic basis for a compositional approach to the analysis of logic programs. A logic program is viewed as consisting of a set of modules, each module defining a subset of the program's predicates. Analyses are constructed by considering abstract interpretations of a compositional semantics. The abstract meaning of a module corresponds to its analysis and composition of abstract meanings corresponds to composition of analyses. Such an approach is essential for large program development so that altering one module does not require re-analysis of the entire program. We claim that for a substantial class of programs, compositional analyses which are based on a notion of abstract unfolding provide the same precision as non-compositional analysis. A compositional analysis for ground dependencies is included to illustrate the approach. To the best of our knowledge this is the first\u00a0\u2026", "num_citations": "99\n", "authors": ["271"]}
{"title": "Functional computations in logic programs\n", "abstract": " Although the ability to simulate nondeterminism and to compute multiple solutions for a single query is a powerful and attractive feature of logic programming languages, it is expensive in both time and space. Since programs in such languages are very often functional, that is, they do not produce more than one distinct solution for a single input, this overhead is especially undesirable. This paper describes how programs may be analyzed statically to determine which literals and predicates are functional, and how the program may then be optimized using this information. Our notion of \u201cfunctionality\u201d subsumes the notion of \u201cdeterminacy\u201d that has been considered by various researchers. Our algorithm is less reliant on language features such as the cut, and thus extends more easily to parallel execution strategies, than others that have been proposed.", "num_citations": "94\n", "authors": ["271"]}
{"title": "Symbolic execution of obfuscated code\n", "abstract": " Symbolic and concolic execution find important applications in a number of security-related program analyses, including analysis of malicious code. However, malicious code tend to very often be obfuscated, and current concolic analysis techniques have trouble dealing with some of these obfuscations, leading to imprecision and/or excessive resource usage. This paper discusses three such obfuscations: two of these are already found in obfuscation tools used by malware, while the third is a simple variation on an existing obfuscation technique. We show empirically that existing symbolic analyses are not robust against such obfuscations, and propose ways in which the problems can be mitigated using a combination of fine-grained bit-level taint analysis and architecture-aware constraint generations. Experimental results indicate that our approach is effective in allowing symbolic and concolic execution to handle\u00a0\u2026", "num_citations": "93\n", "authors": ["271"]}
{"title": "Code specialization based on value profiles\n", "abstract": " It is often the case at runtime that variables and registers in programs are \u201cquasi-invariant,\u201d i.e., the distribution of the values they take on is very skewed, with a small number of values occurring most of the time. Knowledge of such frequently occurring values can be exploited by a compiler to generate code that optimizes for the common cases without sacrificing the ability to handle the general case. The idea can be generalized to the notion of expression profiles, which profile the runtime values of arbitrary expressions and can permit optimizations that may not be possible using simple value profiles. Since this involves the introduction of runtime tests, a careful cost-benefit analysis is necessary to make sure that the benefits from executing the code specialized for the common values outweigh the cost of testing for these values. This paper describes a static cost-benefit analysis that allows us to discover\u00a0\u2026", "num_citations": "91\n", "authors": ["271"]}
{"title": "Detection and optimization of functional computations in Prolog\n", "abstract": " While the ability to simulate nondeterminism and return multiple outputs for a single input is a powerful and attractive feature of Prolog, it is expensive both in time and space. Since Prolog programs are very often functional, i.e. do not produce more than one distinct output for a single input, this overhead is especially undesirable. This paper describes how a program may be analyzed statically to determine which literals and predicates are functional, and how the program may then be optimized using this information. Our notion of \u201cfunctionality\u201d subsumes the notion of \u201cdeterminacy\u201d that has been considered by various researchers. Our algorithms are less reliant on features such as cut, and thus extend more easily to parallel execution strategies than others that have been proposed.", "num_citations": "81\n", "authors": ["271"]}
{"title": "Automatic static unpacking of malware binaries\n", "abstract": " Current malware is often transmitted in packed or encrypted form to prevent examination by anti-virus software. To analyze new malware, researchers typically resort to dynamic code analysis techniques to unpack the code for examination. Unfortunately, these dynamic techniques are susceptible to a variety of anti-monitoring defenses, as well as \"time bombs\" or \"logic bombs,\" and can be slow and tedious to identify and disable. This paper discusses an alternative approach that relies on static analysis techniques to automate this process. Alias analysis can be used to identify the existence of unpacking, static slicing can identify the unpacking code, and control flow analysis can be used to identify and neutralize dynamic defenses. The identified unpacking code can be instrumented and transformed, then executed to perform the unpacking.We present a working prototype that can handle a variety of malware\u00a0\u2026", "num_citations": "80\n", "authors": ["271"]}
{"title": "Generalized semantics and abstract interpretation for constraint logic programs\n", "abstract": " We present simple and powerful generalized algebraic semantics for constraint logic programs that are parameterized with respect to the underlying constraint system. The idea is to abstract away from standard semantic objects by focusing on the general properties of any\u2014possibly nonstandard\u2014semantic definition. In constraint logic programming, this corresponds to a suitable definition of the constraint system supporting the semantic definition. An algebraic structure is introduced to formalize the notion of a constraint system, thus making classical mathematical results applicable. Both top-down and bottom-up semantics are considered. Nonstandard semantics for constraint logic programs can then be formally specified using the same techniques used to define standard semantics. Different nonstandard semantics for constraint logic languages can be specified in this framework. In particular, abstract\u00a0\u2026", "num_citations": "79\n", "authors": ["271"]}
{"title": "Protecting Against Unexpected System Calls.\n", "abstract": " This paper proposes a comprehensive set of techniques which limit the scope of remote code injection attacks. These techniques prevent any injected code from making system calls and thus restrict the capabilities of an attacker. In defending against the traditional ways of harming a system these techniques significantly raise the bar for compromising the host system forcing the attack code to take extraordinary steps that may be impractical in the context of a remote code injection attack. There are two main aspects to our approach. The first is to embed semantic information into executables identifying the locations of legitimate system call instructions; system calls from other locations are treated as intrusions. The modifications we propose are transparent to user level processes that do not wish to use them (so that, for example, it is still possible to run unmodified third-party software), and add more security at minimal cost for those binaries that have the special information present. The second is to back this up using a variety of techniques, including a novel approach to encoding system call traps into the OS kernel, in order to deter mimicry attacks. Experiments indicate that our approach is effective against a wide variety of code injection attacks.", "num_citations": "77\n", "authors": ["271"]}
{"title": "A Generalized Semantics for Constraint Logic Programs.\n", "abstract": " We present a simple and powerful generalized alge-braic semantics for constraint logic programs that is parameterized with respect to the underlying con-straint system.\u201cGeneralized semantics\" abstract away from standard semantics objects, by focus-ing on the general properties of any (possibly non-standard) semantics definition. In constraint logic programming, this corresponds to a suitable defi-nition of the constraint system supporting the se-mantics definition. An algebraic structure is in-troduced to formalize the constraint system notion, thus making applicable classical mathematical re-sults and both a top-down and bottom-up seman-tics are considered. Non-standard semantics for CLP can then be formally specified by means of the same techniques used to define standard semantics. Differ-ent non-standard semantics for constraint logic languages can be specified in this framework: eg ab-stract interpretation, machine level traces and any computation based on an instance of the constraint system.", "num_citations": "74\n", "authors": ["271"]}
{"title": "jc: An E cient and Portable Sequential Implementation of Janus\n", "abstract": " Janus is a language designed for distributed constraint programming 12]. This paper describes jc, an e cient and portable sequential implementation of Janus, which compiles Janus programs down to C code. Careful attention to the C code generated, together with some simple local optimizations, allows the system to have fairly good performance despite the lack (at this time) of global ow analysis and optimization.", "num_citations": "73\n", "authors": ["271"]}
{"title": "Efficient dataflow analysis of logic programs\n", "abstract": " A framework for efficient dataflow analyses of logic programs is investigated. A number of problems arise in this context: aliasing effects can make analysis computationally expensive for sequential logic programming languages; synchronization issues can complicate the analysis of parallel logic programming languages; and finiteness restrictions to guarantee termination can limit the expressive power of such analyses. Our main result is to give a simple characterization of a family of flow analyses where these issues can be ignored without compromising soundness. This results in algorithms that are simple to verify and implement, and efficient in execution. Based on this approach, we describe an efficient algorithm for flow analysis of sequential logic programs, extend this approach to handle parallel executions, and finally describe how infinite chains in the analysis domain can be accommodated without\u00a0\u2026", "num_citations": "59\n", "authors": ["271"]}
{"title": "Unfold/fold transformations and loop optimization of logic programs\n", "abstract": " Programs typically spend much of their execution time in loops. This makes the generation of efficient code for loops essential for good performance. Loop optimization of logic programming languages is complicated by the fact that such languages lack the iterative constructs of traditional languages, and instead use recursion to express loops. In this paper, we examine the application of unfold/fold transformations to three kinds of loop optimization for logic programming languages: recursion removal, loop fusion and code motion out of loops. We describe simple unfold/fold transformation sequences for these optimizations that can be automated relatively easily. In the process, we show that the properties of unification and logical variables can sometimes be used to generalize, from traditional languages, the conditions under which these optimizations may be carried out. Our experience suggests that such source\u00a0\u2026", "num_citations": "53\n", "authors": ["271"]}
{"title": "Automatic simplification of obfuscated JavaScript code: A semantics-based approach\n", "abstract": " JavaScript is a scripting language that is commonly used to create sophisticated interactive client-side web applications. However, JavaScript code can also be used to exploit vulnerabilities in the web browser and its extensions, and in recent years it has become a major mechanism for web-based malware delivery. In order to avoid detection, attackers often take advantage of the dynamic nature of JavaScript to create highly obfuscated code. This paper describes a semantics-based approach for automatic deobfuscation of JavaScript code. Experiments using a prototype implementation indicate that our approach is able to penetrate multiple layers of complex obfuscations and extract the core logic of the computation, which makes it easier to understand the behavior of the code.", "num_citations": "50\n", "authors": ["271"]}
{"title": "On the complexity of flow-sensitive dataflow analyses\n", "abstract": " This paper attempts to address the question of why certain dataflow analysis problems can be solved efficiently, but not others. We focus on flow-sensitive analyses, and give a simple and general result that shows that analyses that require the use of relational attributes for precision must be PSPACE-hard in general. We then show that if the language constructs are slightly strengthened to allow a computation to maintain a very limited summary of what happens along an execution path, inter-procedural analyses become EXPTIME-hard. We discuss applications of our results to a variety of analyses discussed in the literature. Our work elucidates the reasons behind the complexity results given by a number of authors, improves on a number of such complexity results, and exposes conceptual commonalities underlying such results that are not readily apparent otherwise.", "num_citations": "47\n", "authors": ["271"]}
{"title": "Abstract interpretation of logic programs using magic transformations\n", "abstract": " In dataflow analysis of logic programs, information must be propagated according to the control strategy of the language under consideration. However, for languages with top-down control flow, naive top-down dataflow analyses may have problems guaranteeing completeness and/or termination. What is required in the dataflow analysis is a bottom-up fixpoint computation, guided by the (possibly top-down) control strategy of the language. This paper describes the application of the magic templates algorithm, originally devised as a technique for efficient bottom-up computation of logic programs, to dataflow analysis of logic programs. It turns out that a direct application of the magic templates algorithm can result in an undesirable loss in precision, because connections between \u201ccalling patterns\u201d and the corresponding \u201csuccess patterns\u201d may be lost. We show how the original magic templates algorithm can be\u00a0\u2026", "num_citations": "43\n", "authors": ["271"]}
{"title": "QD\u2010Janus: A sequential implementation of Janus in Prolog\n", "abstract": " Janus is a language designed for distributed constraint programming. This paper describes QD\u2010Janus, a sequential implementation of Janus in Prolog. The compiler uses a number of novel analyses and optimizations to improve the performance of the system. The choice of Prolog as the target language for a compiler, although unusual, is motivated by the following: (i) the semantic gap between Janus and Prolog is much smaller than that between Janus and, say, C or machine language\u2014this simplifies the compilation process significantly, and makes it possible to develop a system with reasonable performance fairly quickly; (ii) recent progress in Prolog implementation techniques, and the development of Prolog systems whose speeds are comparable to those of imperative languages, indicates that the translation to Prolog need not entail a significant performance loss compared to native code compilers; and (iii\u00a0\u2026", "num_citations": "43\n", "authors": ["271"]}
{"title": "A simple code improvement scheme for Prolog\n", "abstract": " The generation of efficient code for Prolog programs requires sophisticated code transformation and optimization systems. Much of the recent work in this area has focused on high level transformations, typically at the source level. Unfortunately, such high level transformations suffer from the deficiency of being unable to address low level implementation details. This paper presents a simple code improvement scheme that can be used for a variety of low level optimizations. Applications of this scheme are illustrated using low level optimizations that reduce tag manipulation, dereferencing, trail testing, envi- ronment allocation, and redundant bound checks. The transformation scheme serves as a unified framework for reasoning about a variety of low level optimizations that have, to date, been dealt with in a more or less ad hoc manner.", "num_citations": "40\n", "authors": ["271"]}
{"title": "Reverse engineering self-modifying code: Unpacker extraction\n", "abstract": " An important application of binary-level reverse engineering is in reconstructing the internal logic of computer malware. Most malware code is distributed in encrypted (or \"packed\") form, at runtime, an unpacker routine transforms this to the original executable form of the code, which is then executed. Most of the existing work on analysis of such programs focuses on detecting unpacking and extracting the unpacked code. However, this does not shed any light on the functionality of different portions of the code so obtained, and in particular does not distinguish between code that performs unpacking and code that does not, identifying such functionality can be helpful for reverse engineering the code. This paper describes a technique for identifying and extracting the unpacker code in a self-modifying program. Our algorithm uses offline analysis of a dynamic instruction trace both to identify the point(s) where\u00a0\u2026", "num_citations": "39\n", "authors": ["271"]}
{"title": "Detection and optimization of suspension-free logic programs\n", "abstract": " In recent years, language mechanisms to suspend, or delay, the execution of goals until certain variables become bound have become increasingly popular in logic programming languages. While convenient, such mechanisms can make control flow within a program difficult to predict at compile time, and therefore render many traditional compiler optimizations inapplicable. Unfortunately, this performance cost is also incurred by programs that do not use any delay primitives. In this paper, we describe a simple dataflow analysis for detecting computations where suspension effects can be ignored, and discuss several low-level optimizations that rely on this information. Our algorithm has been implemented in the jc system. Optimizations based on information it produces result in significant performance improvements, demonstrating speed comparable to or exceeding that of optimized C programs.", "num_citations": "36\n", "authors": ["271"]}
{"title": "Bit-level taint analysis\n", "abstract": " Taint analysis has a wide variety of applications in software analysis, making the precision of taint analysis an important consideration. Current taint analysis algorithms, including previous work on bit-precise taint analyses, suffer from shortcomings that can lead to significant loss of precision (under/over tainting) in some situations. This paper discusses these limitations of existing taint analysis algorithms, shows how they can lead to imprecise taint propagation, and proposes a generalization of current bit-level taint analysis techniques to address these problems and improve their precision. Experiments using a deobfuscation tool indicate that our enhanced taint analysis algorithm leads to significant improvements in the quality of deobfuscation.", "num_citations": "35\n", "authors": ["271"]}
{"title": "Modelling metamorphism by abstract interpretation\n", "abstract": " Metamorphic malware apply semantics-preserving transformations to their own code in order to foil detection systems based on signature matching. In this paper we consider the problem of automatically extract metamorphic signatures from these malware. We introduce a semantics for self-modifying code, later called phase semantics, and prove its correctness by showing that it is an abstract interpretation of the standard trace semantics. Phase semantics precisely models the metamorphic code behavior by providing a set of traces of programs which correspond to the possible evolutions of the metamorphic code during execution. We show that metamorphic signatures can be automatically extracted by abstract interpretation of the phase semantics, and that regular metamorphism can be modelled as finite state automata abstraction of the phase semantics.", "num_citations": "35\n", "authors": ["271"]}
{"title": "Making compiler design relevant for students who will (most likely) never design a compiler\n", "abstract": " Compiler Design courses are a common component of most modern Computer Science undergraduate curricula. At the same time, however, compiler design has become a highly specialized topic, and it is not clear that a significant number of Computer Science students will find themselves designing compilers professionally. This paper argues that the principles, techniques, and tools discussed in compiler design courses are nevertheless applicable to a wide variety of situations that would generally not be considered to be compiler design. Generalizing the content of compiler design courses to emphasize this broad applicability can make them more relevant to students.", "num_citations": "35\n", "authors": ["271"]}
{"title": "Combining global code and data compaction\n", "abstract": " Computers are increasingly being incorporated in devices with a limited amount of available memory. As a result research is increasingly focusing on the automated reduction of program size. Existing literature focuses on either data or code compaction or on highly language dependent techniques. This paper shows how combined code and data compaction can be achieved using a link-time code compaction system that reasons about the use of both code and data addresses. The analyses proposed rely only on fundamental properties of linked code and are therefore generally applicable. The combined code and data compaction is implemented in SQUEEZE, a link-time program compaction system, and evaluated on SPEC2000, MediaBench and C++ programs, resulting in total binary program size reductions of 23.6%-46.6%. This compaction involves no speed trade-off, as the compacted programs are on\u00a0\u2026", "num_citations": "35\n", "authors": ["271"]}
{"title": "Goal-directed value profiling\n", "abstract": " Compilers can exploit knowledge that a variable has a fixed known value at a program point for optimizations such as code specialization and constant folding. Recent work has shown that it is possible to take advantage of such optimizations, and thereby obtain significant performance improvements, even if a variable cannot be statically guaranteed to have a fixed constant value. To do this profitably, however, it is necessary to take into account information about the runtime distribution of values taken on by variables. This information can be obtained though value profiling. Unfortunately, existing approaches to value profiling incur high overheads, primarily because profiling is carried out without consideration for the way in which the resulting information will be used. In this paper, we describe an approach to reduce the cost of value profiling by making the value profiler aware of the utility of the value\u00a0\u2026", "num_citations": "32\n", "authors": ["271"]}
{"title": "Towards banishing the cut from Prolog\n", "abstract": " Logic programs can often be inefficient. The usual solution to this problem has been to return some control to the user in the form of impure language features like cut. The authors argue that it is not necessary to resort to such impure features for efficiency. This point is illustrated by considering how most of the common uses of cut can be eliminated from Prolog source programs, relying on static analysis to generate them at compile time. Three common situations where the cut is used are considered. Static analysis techniques are given to detect such situations, and applicable program transformations are described. Two language constructs, firstof and oneof, for situations involving don't-care nondeterminism, are suggested. These constructs have better declarative readings than the cut and extend better to parallel evaluation strategies. Together, these proposals result in a system where users need rely much less\u00a0\u2026", "num_citations": "30\n", "authors": ["271"]}
{"title": "Unpredication, unscheduling, unspeculation: reverse engineering Itanium executables\n", "abstract": " EPIC (explicitly parallel instruction computing) architectures, exemplified by the Intel Itanium, support a number of advanced architectural features, such as explicit instruction-level parallelism, instruction predication, and speculative loads from memory. However, compiler optimizations that take advantage of these features can profoundly restructure the program's code, making it potentially difficult to reconstruct the original program logic from an optimized Itanium executable. This paper describes techniques to undo some of the effects of such optimizations and thereby improve the quality of reverse engineering such executables.", "num_citations": "28\n", "authors": ["271"]}
{"title": "Resource-bounded partial evaluation\n", "abstract": " Most partial evaluators do not take the availability of machine-level resources, such as registers or cache, into consideration when making their specialization decisions. The resulting resource contention can lead to severe performance degradation---causing, in extreme cases, the specialized code to run slower than the unspecialized code. In this paper we consider how resource considerations can be incorporated within a partial evaluator. We develop an abstract formulation of the problem, show that optimal resource-bounded partial evaluation is NP-complete, and discuss simple heuristics that can be used to address the problem in practice.", "num_citations": "28\n", "authors": ["271"]}
{"title": "Register Allocation in a Prolog Machine.\n", "abstract": " We consider the Prolog Engine described by DHD Warren. An interesting feature of this machine is its parameter passing mechanism, which uses register k to pass the kth. parameter to a procedure. Warren distinguishes between temporary variables, which can be kept in registers in such a machine, and permanent variables, which must be allocated in memory. The issue in register allocation here is one of minimizing data movement, and is somewhat different from that of minimizing the number of loads and stores, as in a conventional machine. We describe three register allocation algorithms for such a machine. These strategies rely on a high-level analysis of the source program to compute information which is then used for register allocation during code generation. The algorithms are simple yet quite efficient, and produce code of good quality.", "num_citations": "28\n", "authors": ["271"]}
{"title": "Automatic complexity analysis of logic programs\n", "abstract": " Automatic complexity analysis of programs has been widely studied in the context of functional languages. This paper develops a method for automatic analysis of the worse-case complexity of a large class of logic programs. The primary contribution of this paper is that it shows how to deal with nondeterminism and the generation of multiple solutions via backtracking. One advantage of our method is that analyses for different complexity measures (eg time complexity, space complexity, number of solutions, etc) are performed in a unified framework that simplifies both formal reasoning about, and implementation of, the algorithms.", "num_citations": "27\n", "authors": ["271"]}
{"title": "Executing Distributed Prolog Programs on a Broadcast Network.\n", "abstract": " EXECUTING DISTRIBUTED PROLOG PROGRAMS ON A BROADCAST NETWORK. \u2014 University of Arizona Skip to main navigation Skip to search Skip to main content University of Arizona Logo Home Profiles Research Units Projects Research Output Activities Prizes Search by expertise, name or affiliation EXECUTING DISTRIBUTED PROLOG PROGRAMS ON A BROADCAST NETWORK. David S. Warren, Mustaque Ahamad, Saumya K. Debray, LV Kale Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution 6 Scopus citations Overview Fingerprint Original language English (US) Title of host publication Unknown Host Publication Title Publisher IEEE Pages 12-21 Number of pages 10 ISBN (Print) areas Engineering(all) Access to Document Link to publication in Scopus Link to citation , .\u2026", "num_citations": "27\n", "authors": ["271"]}
{"title": "On the complexity of dataflow analysis of logic programs\n", "abstract": " It is widely held that there is a correlation between complexity and precision in dataflow analysis, in the sense that the more precise an analysis algorithm, the more computationally expensive it must be. The details of this correspondence, however, appear to not have been explored extensively. This paper reports some results on this tradeoff in the context of Horn logic programs. A formal notion of the \u201cprecision\u201d of an analysis algorithm is proposed, and this is used to characterize the worst case computational complexity of a number of dataflow analysis algorithms with different degrees of precision.", "num_citations": "26\n", "authors": ["271"]}
{"title": "Call forwarding: A simple interprocedural optimization technique for dynamically typed languages\n", "abstract": " This paper discusses call forwarding, a simple interprocedural optimization technique for dynamically typed languages. The basic idea behind the optimization is straightforward: find an ordering for the \u201centry actions\u201d of a procedure, and generate multiple entry points for the procedure, so as to maximize the savings realized from different call sites bypassing different sets of entry actions. We show that the problem of computing optimal solutions to arbitrary call forwarding problems is NP-complete, and describe an efficient greedy algorithm for the problem. Experimental results indicate that (i) this algorithm is effective, in that the solutions produced are generally close to optimal; and (ii) the resulting optimization leads to significant performance improvements for a number of benchmarks tested.", "num_citations": "26\n", "authors": ["271"]}
{"title": "Flow analysis of dynamic logic programs\n", "abstract": " Research on flow analysis and optimization of logic programs typically assumes that the programs being analyzed are static, i.e. any code that can be executed at runtime is available for analysis at compile time. This assumption may not hold for \u201creal\u201d programs, which can contain dynamic goals of the form call(X), where X is a variable at compile time, or where predicates may be modified via features like assert and retract. In such contexts, a compiler must be able to take the effects of such dynamic constructs into account in order to perform nontrivial flow analyses that can be guaranteed to be sound. This paper outlines how this may be done for certain kinds of dynamic programs. Our techniques allow analysis and optimization techniques that have been developed for static programs to be extended to a large class of \u201cwell-behaved\u201d dynamic programs.", "num_citations": "25\n", "authors": ["271"]}
{"title": "Methods of micro-specialization in database management systems\n", "abstract": " Systems and methods for utilizing relation-and query-specific information to specialize DBMS code at runtime based on identifying runtime locally invariant variables. Runtime invariant is often of the form of variables in code that hold values that are constant during a portion of code execution. Micro-specialization is applied to eliminate from the original program unnecessary code such as branching statements that reference local invariant (s) in branch-condition evaluation. The resulting specialized code reduces the code complexity as well as significantly improves the runtime efficiency during code execution.", "num_citations": "24\n", "authors": ["271"]}
{"title": "Micro-specialization: dynamic code specialization of database management systems\n", "abstract": " Database management systems (DBMSes) form a cornerstone of modern IT infrastructure, and it is essential that they have excellent performance. Much of the work to date on optimizing DBMS performance has emphasized ensuring efficient data access from secondary storage. This paper shows that DBMSes can also benefit significantly from dynamic code specialization. Our approach focuses on the iterative query evaluation loops typically used by such systems. Query evaluation involves extensive references to the relational schema, predicate values, and join types, which are all invariant during query evaluation, and thus are subject to dynamic value-based code specialization.", "num_citations": "24\n", "authors": ["271"]}
{"title": "Static detection of disassembly errors\n", "abstract": " Static disassembly is a crucial first step in reverse engineering executable files, and there is a considerable body of work in reverse-engineering of binaries, as well as areas such as semantics-based security analysis, that assumes that the input executable has been correctly disassembled. However, disassembly errors, e.g., arising from binary obfuscations, can render this assumption invalid. This work describes a machine-learning-based approach, using decision trees, for statically identifying possible errors in a static disassembly; such potential errors may then be examined more closely, e.g., using dynamic analyses. Experimental results using a variety of input executables indicate that our approach performs well, correctly identifying most disassembly errors with relatively few false positives.", "num_citations": "23\n", "authors": ["271"]}
{"title": "Partial inlining\n", "abstract": " Subprogram inlining is an optimization that has been studied extensively in the compiler community 1, 2, 5, 7, 8, 10, 14, 15, 16]. By replacing a procedure call with a copy of the body of the called procedure, it o ers a number of bene ts: the overhead associated with the procedure call is eliminated; the inlined code can be optimized for the particular call site that was expanded; and the caller's code can potentially be improved because of the additional information about the called code, which is usually not available in the absence of inter-procedural program analysis, that becomes available due to inlining. Nevertheless, it has long been recognized that careless inlining can give rise to a host of problems that can lead to a loss in performance. Davidson and Holler show that inattention to register allocation considerations can cause inlining to result in a decrease in execution speed 8]. Inlining, even when it is guided by pro le information, can also cause increases in code size| in extreme cases, of over an order of magnitude 8, 11, 12]; this can increase the time and space requirements during compilation and optimization and adversely a ect the quality of the code generated. 1 These problems can, however, be addressed by a combination of attention to register allocation considerations, use of pro le information to determine which call sites to select for inline expansion, use of budgets to prevent excessive code growth, and pro le-guided code layout to mitigate any loss in locality due to inlining 2, 5, 16].Assuming that pro le information is used to guide inlining decisions, a call site will be chosen for inline expansion only if its execution count is su\u00a0\u2026", "num_citations": "23\n", "authors": ["271"]}
{"title": "Formal bases for dataflow analysis of logic programs\n", "abstract": " Formal bases for dataflow analysis of logic programs | Advances in logic programming theory ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksAdvances in logic programming theoryFormal bases for dataflow analysis of logic programs chapter Formal bases for dataflow analysis of logic programs Share on Author: Saumya Kanti Debray profile image Saumya K. Debray View Profile Authors Info & Affiliations Publication: Advances in logic programming theoryFebruary 1995 Pages 115\u2013182 3citation 0 Downloads Metrics Total Citations3 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! : \u2026", "num_citations": "23\n", "authors": ["271"]}
{"title": "Weighted Decision Trees.\n", "abstract": " While decision tree compilation is a promising way to carry out guard tests e ciently, the methods given in the literature do not take into account either the execution characteristics of the program or the machine-level tradeo s between di erent ways to implement branches. These methods therefore o er little or no guidance for the implementor with regard to how decision trees are to be realized on a particular machine. In this paper, we describe an approach that takes execution frequencies of di erent program branches, as well as the costs of alternative branch realizations, to generate decision trees. Experiments indicate that the performance of our approach is uniformly better than that of plausible alternative schemes.", "num_citations": "23\n", "authors": ["271"]}
{"title": "The SB-Prolog System, Version 2.5: A User Manual\n", "abstract": " SB-Prolog is a Prolog system for Unix1-based systems. The core of the system is an emulator, written in C for portability, of a Prolog virtual machine that is an extension of the Warren Abstract Machine. The remainder of the system, including the translator from Prolog to the virtual machine instructions, is written in Prolog. Parts of this manual, specifically the sections on Prolog syntax and descriptions of some of the builtins, are based on the C-Prolog User Manual by Fernando Pereira.", "num_citations": "23\n", "authors": ["271"]}
{"title": "Identifying and understanding self-checksumming defenses in software\n", "abstract": " Software self-checksumming is widely used as an anti-tampering mechanism for protecting intellectual property and deterring piracy. This makes it important to understand the strengths and weaknesses of various approaches to self-checksumming. This paper describes a dynamic information-flow-based attack that aims to identify and understand self-checksumming behavior in software. Our approach is applicable to a wide class of self chesumming defenses and the information obtained can be used to determine how the checksumming defenses may be bypassed. Experiments using a prototype implementation of our ideas indicate that our approach can successfully identify self-checksumming behavior in (our implementations of) proposals from the research literature.", "num_citations": "21\n", "authors": ["271"]}
{"title": "Micro-specialization in DBMSes\n", "abstract": " Relational database management systems are general in the sense that they can handle arbitrary schemas, queries, and modifications, this generality is implemented using runtime metadata lookups and tests that ensure that control is channelled to the appropriate code in all cases. Unfortunately, these lookups and tests are carried out even when information is available that renders some of these operations superfluous, leading to unnecessary runtime overheads. This paper introduces micro-specialization, an approach that uses relation- and query-specific information to specialize the DBMS code at runtime and thereby eliminate some of these overheads. We develop a taxonomy of approaches and specialization times and propose a general architecture that isolates most of the creation and execution of the specialized code sequences in a separate DBMS-independent module. Through three illustrative types\u00a0\u2026", "num_citations": "21\n", "authors": ["271"]}
{"title": "Code compaction of an operating system kernel\n", "abstract": " General-purpose operating systems, such as Linux, are increasingly being used in embedded systems. Computational resources are usually limited, and embedded processors often have a limited amount of memory. This makes code size especially important. This paper describes techniques for automatically reducing the memory footprint of general-purpose operating systems on embedded platforms. The problem is complicated by the fact that kernel code tends to be quite different from ordinary application code, including the presence of a significant amount of hand-written assembly code, multiple entry points, implicit control flow paths involving interrupt handlers, and frequent indirect control flow via function pointers. We use a novel \"approximate decompilation\" technique to apply source-level program analysis to hand-written assembly code. A prototype implementation of our ideas on an Intel x86 platform\u00a0\u2026", "num_citations": "21\n", "authors": ["271"]}
{"title": "Predicate analysis and if-conversion in an Itanium link-time optimizer\n", "abstract": " EPIC architectures, such as the Intel IA-64 (Itanium), combine explicit instruction-level parallelism with instruction predication. To generate efficient code, it is important to use predication effectively. In particular, it is important to replace conditional branches and multiple code blocks by single, branch-free code blocks when doing so would lead to faster code. This process, which is known as if-conversion, is generally carried out early in the code-generation process; hence subsequent analyses and optimizations have to deal with predicated code. This paper examines an alternative approach in which code is unpredicated during disassembly, the internal representations are virtually identical to those in a conventional architecture (specifically the IA-32 Pentium) and if-conversion is done late in the compilation process, at the same time as instruction scheduling and just before code layout. This paper also presents new algorithms for analyzing predicated code and evaluates their efficacy. We show that our approach is able to produce code that is denser (fewer nop instructions) and almost as fast as the best code produced by the Intel ecc compiler on the SPECint-2000 benchmark suite. On the same programs, our predicate analysis and ifconversion algorithms lead to an average speed improvement of a little over 4% on the best code produced by the gcc compiler.", "num_citations": "20\n", "authors": ["271"]}
{"title": "Understanding Finiteness Analysis Using Abstract Interpretation.\n", "abstract": " Finiteness analyses are compile-time techniques to determine (su cient) conditions for the niteness of relations computed during the bottom-up execution of a logic program. We examine niteness analyses from the perspective of abstract interpretation. However, problems arise when trying to use standard abstract interpretation theory for niteness analysis. They occur because niteness is not an admissible property and so naive application of abstract interpretation leads to incorrect analyses. Here we develop three simple techniques based on abstract interpretation theory which allow inadmissible properties to be handled. Existing approaches to niteness analysis may be explained and compared in terms of our extension to abstract interpretation theory, and we claim that their correctness is more easily argued in it. To support our claim we use our techniques to develop and prove correct a niteness analysis which is more accurate than any that we are aware of.", "num_citations": "20\n", "authors": ["271"]}
{"title": "Optimizing almost-tail-recursive Prolog programs\n", "abstract": " There is a wide class of problems for which the natural Prolog specification, as a top-down, recursive computation, is significantly less efficient than an iterative bottom-up version. However, the transformation from the top-down specification to the bottom-up implementation is not always obvious, principally due to problems with nondeterminism and the order in which variables get bound \u2014 problems which do not arise in comparable situations in functional languages. This paper illustrates how these problems can be handled in certain cases, and the transformation mechanized, using algebraic properties of operators such as associativity and distributivity. The resulting programs are tail-recursive, and hence significantly more efficient in space usage, with no deterioration in execution speed.", "num_citations": "20\n", "authors": ["271"]}
{"title": "Interprocedural control flow analysis of first-order programs with tail-call optimization\n", "abstract": " Knowledge of low-level control flow is essential for many compiler optimizations. In systems with tail-call optimization, the determination of interprocedural control flow is complicated by the fact that because of tail-call optimization, control flow at procedure returns is not readily evident from the call graph of the program. This article shows how interprocedural control-flow analysis of first-order programs can be carried out using well-known concepts from parsing theory. In particular, we show that context-insensitive ( or zeroth-order) control-flow analysis corresponds to the notion of FOLLOW sets in context-free grammars, while context-sensitive (or first-order) control-flow analysis corresponds to the notion of LR(1) items. The control-flow information so obtained can be used to improve the precision of interprocedural dataflow analyses as well as to extend certain low-level code optimizations across procedure boundaries.", "num_citations": "19\n", "authors": ["271"]}
{"title": "Static estimation of query sizes in horn programs\n", "abstract": " Knowledge about relation sizes for queries can be used to improve the performance of deductive database programs, e.g. to plan the order in which body goals are evaluated, or to \u201cmap\u201d predicates to processors in distributed systems. For Horn programs, the analysis is complicated by the presence of function symbols and recursion. This paper describes an approach for statically computing upper bound estimates for relation sizes in Horn programs. The techniques are applicable to a wide class of programs that use structural recursion.", "num_citations": "19\n", "authors": ["271"]}
{"title": "Unveiling metamorphism by abstract interpretation of code properties\n", "abstract": " Metamorphic code includes self-modifying semantics-preserving transformations to exploit code diversification. The impact of metamorphism is growing in security and code protection technologies, both for preventing malicious host attacks, e.g., in software diversification for IP and integrity protection, and in malicious software attacks, e.g., in metamorphic malware self-modifying their own code in order to foil detection systems based on signature matching. In this paper we consider the problem of automatically extracting metamorphic signatures from metamorphic code. We introduce a semantics for self-modifying code, later called phase semantics, and prove its correctness by showing that it is an abstract interpretation of the standard trace semantics. Phase semantics precisely models the metamorphic code behavior by providing a set of traces of programs which correspond to the possible evolutions of the\u00a0\u2026", "num_citations": "17\n", "authors": ["271"]}
{"title": "A framework for understanding dynamic anti-analysis defenses\n", "abstract": " Malicious code often use a variety of anti-analysis and anti-tampering defenses to hinder analysis. Researchers trying to understand the internal logic of the malware have to penetrate these defenses. Existing research on such anti-analysis defenses tend to study them in isolation, thereby failing to see underlying conceptual similarities between different kinds of anti-analysis defenses. This paper proposes an information-flow-based framework that encompasses a wide variety of anti-analysis defenses. We illustrate the utility of our approach using two different instances of this framework: self-checksumming-based anti-tampering defenses and timing-based emulator detection. Our approach can provide insights into the underlying structure of various anti-analysis defenses and thereby help devise techniques for neutralizing them.", "num_citations": "17\n", "authors": ["271"]}
{"title": "Visualizing the behavior of dynamically modifiable code\n", "abstract": " Recent years have seen an increased recognition of some of the advantages offered by dynamically modifiable code, i.e., code that changes during the execution of the program. In its full generality, it can be very difficult to understand the behavior of such self-modifiable code. This paper describes a system that graphically displays the execution behavior of dynamic code, focusing on code modifications and their effect on the structure of the program, i.e., the call graph and control flow graphs of functions. This can help users visualize the structure of runtime code modifications and understand the behavior of dynamically modifiable programs.", "num_citations": "17\n", "authors": ["271"]}
{"title": "Constraint-based termination analysis for cyclic active database rules\n", "abstract": " There are many situations where cyclic rule activations\u2014where some set of active database rules may be activated repeatedly until the database satisfies some condition\u2014arise naturally. However, most existing approaches to termination analysis of active rules, which typically rely on checking that the triggering graph for the rules is acyclic, cannot infer termination for such rules. We present a constraint-based approach to termination analysis that is able to handle such cyclic rule activations for a wide class of rules.", "num_citations": "17\n", "authors": ["271"]}
{"title": "On the semantics of self-unpacking malware code\n", "abstract": " The rapid increase in attacks on software systems via malware such as viruses, worms, trojans, etc., has made it imperative to develop effective techniques for detecting and analyzing malware binaries. Such binaries are usually transmitted in packed or encrypted form, with the executable payload decrypted dynamically and then executed. In order to reason formally about their execution behavior, therefore, we need semantic descriptions that can capture this self-modifying aspect of their code. However, current approaches to the semantics of programs usually assume that the program code is immutable, which makes them inapplicable to self-unpacking malware code. This paper takes a step towards addressing this problem by describing a formal semantics for self-modifying code. We use our semantics to show how the execution of self-unpacking code can be divided naturally into a sequence of phases, and uses this to show how the behavior of a program can be characterized statically in terms of a program evolution graph. We discuss several applications of our work, including static unpacking and deobfuscation of encrypted malware and static cross-phase code analysis. 1", "num_citations": "16\n", "authors": ["271"]}
{"title": "A Simple Program Transformation for Parallelism.\n", "abstract": " Most of the research, to date, on optimizing program transformations for declarative languages has focused on sequential execution strategies. In this paper, we consider a class of commonly encountered computations whose\\natural\" speci cation is essentially sequential, and show how algebraic properties of the operators involved can be used to transform them into divide-and-conquer programs that are considerably more e cient, both in theory and in practice, on parallel machines.", "num_citations": "16\n", "authors": ["271"]}
{"title": "Equational reasoning on x86 assembly code\n", "abstract": " Analysis of software is essential to addressing problems of correctness, efficiency, and security. Existing source code analysis tools are very useful for such purposes, but there are many instances where high-level source code is not available for software that needs to be analyzed. A need exists for tools that can analyze assembly code, whether from disassembled binaries or from handwritten sources. This paper describes an equational reasoning system for assembly code for the ubiquitous Intel x86 architecture, focusing on various problems that arise in low-level equational reasoning, such as register-name aliasing, memory indirection, condition-code flags, etc. Our system has successfully been applied to the problem of simplifying execution traces from obfuscated malware executables.", "num_citations": "15\n", "authors": ["271"]}
{"title": "Profiling prolog programs\n", "abstract": " Profilers play an important role in the development of efficient programs. Profiling techniques developed for traditional languages are inadequate for logic programming languages, for a number of reasons: first, the flow of control in logic programming languages, involving back\u2010tracking and failure, is significantly more complex than in traditional languages; secondly, the time taken by a unification operation, the principal primitive operation of such languages, cannot be predicted statically because it depends on the size of the input; and finally, programs may change at run\u2010time because clauses may be added or deleted using primitives such as assert and retract. This paper describes a simple profiler for Prolog. The ideas outlined here may be used either to implement a simple interactive profiler, or integrated into Prolog compilers.", "num_citations": "15\n", "authors": ["271"]}
{"title": "GLOBAL OPTIMIZATION OF LOGIC PROGRAMS (ANALYSIS, TRANSFORMATION, COMPILATION).\n", "abstract": " Degree: Ph. D.DegreeYear: 1986Institute: State University of New York at Stony BrookLogic programming languages have several very attractive features, such as separation of program logic from control, declarative reading of programs, and ease of understanding and maintenance. Despite this, however, the pragmatic problem of efficiency has hampered the widespread acceptance of such languages as general-purpose programming languages, and has led to the proliferation of\" impure\" language features whose use compromises precisely those properties of these languages that made them attractive in the first place.", "num_citations": "15\n", "authors": ["271"]}
{"title": "Application of micro-specialization to query evaluation operators\n", "abstract": " Relational database management systems support a wide variety of data types and operations. Such generality involves much branch condition checking, which introduces inefficiency within the query evaluation loop. We previously introduced micro-specialization, which improves performance by eliminating unnecessary branching statements and the actual code branches by exploiting invariants present during the query evaluation loop. In this paper, we show how to more aggressively apply micro-specialization to each individual operator within a query plan. Rather than interpreting the query plan, the DBMS dynamically rewrites its object code to produce executable code tailored to the particular query. We explore opportunities for applying micro-specialization to DBMSes, focusing on query evaluation. We show through an examination of program execution profiles that even with a simple query in which just a\u00a0\u2026", "num_citations": "14\n", "authors": ["271"]}
{"title": "Stack analysis of x86 executables\n", "abstract": " Binary rewriting is becoming increasingly popular for a variety of low-level code manipulation purposes. One of the difficulties encountered in this context is that machine-language programs typically have much less semantic information compared to source code, which makes it harder to reason about the program\u2019s runtime behavior. This problem is especially acute in the widely used Intel x86 architecture, where the paucity of registers often makes it necessary to store values on the runtime stack. The use of memory in this manner affects many analyses and optimizations because of the possibility of indirect memory references, which are difficult to reason about. This paper describes a simple analysis of some basic aspects of the way in which programs manipulate the runtime stack. The information so obtained can be very helpful in enhancing and improving a variety of other dataflow analyses that reason about and manipulate values stored on the runtime stack. Experiments indicate that the analyses are efficient and useful for improving optimizations that need to reason about the runtime stack.", "num_citations": "14\n", "authors": ["271"]}
{"title": "Binary rewriting of an operating system kernel\n", "abstract": " This paper deals with some of the issues that arise in the context of binary rewriting and instrumentation of an operating system kernel. OS kernels are very different from ordinary application code in many ways, eg, they contain a significant amount of hand-written assembly code. Binary rewriting is an attractive approach for processing OS kernel code for several reasons, eg, it provides a uniform way to handle heterogeneity in code due to a combination of source code, assembly code and legacy code such as in device drivers. However, because of the many differences between ordinary application code and OS kernel code, binary rewriting techniques that work for application code do not always carry over directly to kernel code. This paper describes some of the issues that arise in this context, and the approaches we have taken to address them. A key goal when developing our system was to deal in a systematic manner with the various peculiarities seen in low-level systems code, and reason about the safety and correctness of code transformations, without requiring significant deviations from the regular developmental path. For example, a precondition we assumed was that no compiler or linker modifications should be required to use it and the tool should be able to process kernel binaries in the same way as it does ordinary applications.We have implemented a prototype kernel binary rewriter as an extension to the PLTO binary rewriting toolkit [13]. PLTO takes as input a relocatable binary that it manipulates in various ways, eg, to insert instrumentation code or to apply various optimizing transformations using optional execution profiles for\u00a0\u2026", "num_citations": "13\n", "authors": ["271"]}
{"title": "Output value placement in moded logic programs\n", "abstract": " Most implementations of logic programming languages treat input and output arguments to procedures in a fundamentally asymmetric way: input values are passed in registers, but output values are returned in memory. In some cases, placing the outputs in memory is useful to preserve the opportunity for tail call optimization. In other cases, this asymmetry can lead to a large number of unnecessary memory references and adversely affect performance. When input/output modes for arguments are known it is often possible to avoid much of this unnecessary memory tra c via a form of interprocedural register allocation. In this paper we discuss how this problem may be addressed by returning output values in registers where it seems profitable to do so. The techniques described have been implemented in the jc system, but are also applicable to other moded logic programming languages, such as Parlog, as well as languages like Prolog when input/output modes have been determined via data ow analysis.", "num_citations": "13\n", "authors": ["271"]}
{"title": "On Copy Avoidance in Single Assignment Languages.\n", "abstract": " Copy avoidance refers to the safe replacement, at compile time, of copying operations by destructive updates in single-assignment languages. Conceptually, the problem can be divided into two components: identifying memory cells that can safely be reused at a program point via destructive updating; and deciding how to actually reuse such cells. Most of the work on this problem, to date, has focused on the first component, typically via dataflow analyses to detect when memory cells become dead and may be safely reused. In this paper, we examine the second component of the problem. We give an abstract formulation of the memory reuse problem, show that optimal reuse is NP-complete in general, and give an efficient polynomial-time approximation algorithm based on graph-matching techniques that produces optimal solutions for most commonly encountered cases of memory reuse.", "num_citations": "13\n", "authors": ["271"]}
{"title": "Generalized Horn clause programs\n", "abstract": " This paper considers, in a general setting, an axiomatic basis for Horn clause logic programming. It characterizes a variety of\\Horn-clause-like\" computations, arising in contexts such as deductive databases, various abstract interpretations, and extensions to logic programming involving E-uni cation, quantitative deduction, and inheritance, in terms of two simple operators, and discusses algebraic properties these operators must satisfy. It develops xpoint and model-theoretic semantics in this generalized setting, and shows that the xpoint semantics is well-de ned and coincides with the model-theoretic semantics. This leads to a generalized notion of a Horn clause logic program that captures a variety of xpoint computations proposed in different guises, and allows concise expression in the logic programming idiom of several programs that involve aggregate operations.", "num_citations": "12\n", "authors": ["271"]}
{"title": "The revenge of the overlay: automatic compaction of OS kernel code via on-demand code loading\n", "abstract": " There is increasing interest in using general-purpose operating systems, such as Linux, on embedded platforms. It is especially important in embedded systems to use memory efficiently because embedded processors often have limited physical memory. This paper describes an automatic technique for reducing the memory footprint of general-purpose operating systems on embedded platforms by keeping infrequently executed code on secondary storage and loading such code only if it is needed at run time. Our technique is based on an old idea-memory overlays-and it does not require hardware or operating system support for virtual memory. A prototype of the technique has been implemented for the Linux kernel. We evaluate our approach with two benchmark suites: MiBench and MediaBench, and a Web server application. The experimental results show that our approach reduces memory requirements for\u00a0\u2026", "num_citations": "11\n", "authors": ["271"]}
{"title": "A simple approach to supporting untagged objects in dynamically typed languages\n", "abstract": " In many modern high-level programming languages, the exact low-level representation of data objects cannot always be predicted at compile-time. Implementations usually get around this problem using descriptors (\u201ctags\u201d) and/or indirect (\u201cboxed\u201d) representations. However, the flexibility so gained can come at the cost of significant performance overheads. The problem is especially acute in dynamically typed languages, where both tagging and boxing are necessary in general. This paper discusses a straightforward approach to using untagged and unboxed values in dynamically typed languages. An implementation of our algorithms allows a dynamically typed language to attain performance close to that of highly optimized C code on a variety of benchmarks (including many floating-point intensive computations) and dramatically reduces heap usage.", "num_citations": "11\n", "authors": ["271"]}
{"title": "Implementing logic programming systems: The quiche-eating approach\n", "abstract": " In recent years, it seems to have become somewhat unfashionable to build implementations of logic programming languages by translating them to Prolog. Instead, implementors appear, more and more, to be designing their own abstract instruction sets and indulging in a great deal of slow and painful low-level hacking. This paper argues that in many cases, it is preferable to build systems by translating programs to Prolog, using a good underlying Prolog system, and using dataflow analysis and high-level optimizations to reduce performance overheads. In support of our arguments, we compare two sequential implementations of committed choice languages: QD-Janus, which translates to Prolog, and FCP(:), which compiles to a low-level byte-coded instruction set. Even though QD-Janus took significantly less time to implement, its performance is significantly better than that of FCP(:).", "num_citations": "11\n", "authors": ["271"]}
{"title": "Joining abstract and concrete computations in constraint logic programming\n", "abstract": " The use of standard instances of the CLP framework (e.g. CLP(Bool) and CLP (R)) for non-standard (possibly abstract) interpretations, weakens the distinction between concrete and abstract computations in semantics and analysis. We formalize this idea by applying the well known approximation techniques (e.g. the standard theory of closure operators) in conjunction with a generalized notion of constraint system, supporting any program evaluation. The \u201cgeneralized semantics\u201d resulting from this process, abstracts away from standard semantic objects, by focusing on the general properties of any (possibly non-standard) semantic definition. In constraint logic programming, this corresponds to a suitable definition of the constraint system supporting the semantic definition. Both top-down and a bottom-up semantics are considered.", "num_citations": "11\n", "authors": ["271"]}
{"title": "Probabilistic obfuscation through covert channels\n", "abstract": " This paper presents a program obfuscation framework that uses covert channels through the program's execution environment to obfuscate information flow through the program. Unlike prior works on obfuscation, the use of covert channels removes visible information flows from the computation of the program and reroutes them through the program's runtime system and/or the operating system. This renders these information flows, and the corresponding control and data dependencies, invisible to program analysis tools such as symbolic execution engines. Additionally, we present the idea of probabilistic obfuscation which uses imperfect covert channels to leak information with some probabilistic guarantees. Experimental evaluation of our approach against state of the art detection and analysis techniques show the engines are not well-equipped to handle these obfuscations, particularly those of the probabilistic\u00a0\u2026", "num_citations": "10\n", "authors": ["271"]}
{"title": "Analysis of exception-based control transfers\n", "abstract": " Dynamic taint analysis and symbolic execution find many important applications in security-related program analyses. However, current techniques for such analyses do not take proper account of control transfers due to exceptions. As a result, they can fail to account for implicit flows arising from exception-based control transfers, leading to loss of precision and potential false negatives in analysis results. While the idea of using exceptions for obfuscating (unconditional) control transfers is well known, we are not aware of any prior work discussing the use of exceptions to implement conditional control transfers and implicit information flows. This paper demonstrates the problems that can arise in existing dynamic taint analysis and symbolic execution systems due to exception-based implicit information flows and proposes a generic architecture-agnostic solution for reasoning about the behavior of code using user\u00a0\u2026", "num_citations": "10\n", "authors": ["271"]}
{"title": "Weaknesses in defenses against web-borne malware\n", "abstract": " Web-based mechanisms, often mediated by malicious JavaScript code, play an important role in malware delivery today, making defenses against web-borne malware crucial for system security. This paper explores weaknesses in existing approaches to the detection of malicious JavaScript code. These approaches generally fall into two categories: lightweight techniques focusing on syntactic features such as string obfuscation and dynamic code generation; and heavier-weight approaches that look for deeper semantic characteristics such as the presence of shellcode-like strings or execution of exploit code. We show that each of these approaches has its weaknesses, and that state-of-the-art detectors using these techniques can be defeated using cloaking techniques that combine emulation with dynamic anti-analysis checks. Our goal is to promote a discussion in the research community focusing on\u00a0\u2026", "num_citations": "10\n", "authors": ["271"]}
{"title": "Enhancing software tamper-resistance via stealthy address computations\n", "abstract": " A great deal of software is distributed in the form of executable code. The ability to reverse engineer such executables can create opportunities for theft of intellectual property via software piracy, as well as security breaches by allowing attackers to discover vulnerabilities in an application [9]. Techniques such as watermarking and fingerprinting have been developed to discourage piracy [4, 12], however, if no protective measures are taken, an attacker may be able to remove and/or destroy watermarks and fingerprints with relative ease once they have been identified. For this reason, methods such as source code obfuscation [4, 11, 3, 15], code encryption [1, 13] and self verifying code [1, 7] have been developed to help achieve some measure of tamper-resistance.It is, of course, necessary for an attacker to gain a reliable disassembly of some portion of executable code before any intelligent tampering can take place. In fact, even a reliable disassembly in the absence of some sort of control flow graph is not sufficient for serious tampering [15]. Coupled with other methods [9] we propose one method of obfuscating address computations in which the targets of control transfers are made difficult to determine statically. We describe this method in Section 2.", "num_citations": "10\n", "authors": ["271"]}
{"title": "Cold code decompression at runtime\n", "abstract": " Using a software-based technique to dynamically decompress selected code fragments during program execution.", "num_citations": "10\n", "authors": ["271"]}
{"title": "Unspeculation\n", "abstract": " Modern architectures, such as the Intel Itanium, support speculation, a hardware mechanism that allows the early execution of expensive operations possibly even before it is known whether the results of the operation are needed. While such speculative execution can improve execution performance considerably, it requires a significant amount of complex support code to deal with and recover from speculation failures. This greatly complicates the tasks of understanding and re-engineering speculative code. This paper describes a technique for removing speculative instructions from optimized binary programs in a way that is guaranteed to preserve program semantics, thereby making the resulting \"unspeculated\" programs easier to understand and more amenable to reengineering using traditional reverse engineering techniques.", "num_citations": "9\n", "authors": ["271"]}
{"title": "Abstract interpretation and low-level code optimization\n", "abstract": " Abstract interpretation is widely accepted as a natural framework for semantics-based analysis of program properties. However, most formulations of abstract interpretation are in terms of high-level semantic entities that do not adequately address the needs of lowlevel optimizations. In this paper we discuss the role of abstract interpretation in low-level compiler optimization, examine some of its limitations, and consider ways in which they might be addressed.", "num_citations": "9\n", "authors": ["271"]}
{"title": "Automatic simplification of obfuscated JavaScript code\n", "abstract": " Javascript is a scripting language that is commonly used to create sophisticated interactive client-side web applications. It can also be used to carry out browser-based attacks on users. Malicious JavaScript code is usually highly obfuscated, making detection a challenge. This paper describes a simple approach to deobfuscation of JavaScript code based on dynamic analysis and slicing. Experiments using a prototype implementation indicate that our approach is able to penetrate multiple layers of complex obfuscations and extract the core logic of the computation.", "num_citations": "8\n", "authors": ["271"]}
{"title": "Checking program profiles\n", "abstract": " Execution profiles have become increasingly important for guiding code optimization. However, little has been done to develop ways to check automatically that a profile does, in fact, reflect the actual execution behavior of a program. We describe a framework that uses program monitoring techniques in a way that allows the automatic checking of a wide variety of profile data. We also describe our experiences with using an instance of this framework to check edge profiles. The profile checker uncovered profiling anomalies that were previously unknown and that would have been very difficult to identify using existing techniques.", "num_citations": "8\n", "authors": ["271"]}
{"title": "On the complexity of function pointer may-alias analysis\n", "abstract": " This paper considers the complexity of interprocedural function pointer may-alias analysis, i.e., determining the set of functions that a function pointer (in a language such as C) can point to at a point in a program. This information is necessary, for example, in order to construct the control flow graphs of programs that use function pointers, which in turn is fundamental for most dataflow analyses and optimizations. We show that the general problem is complete for deterministic exponential time. We then consider two natural simplifications to the basic (precise) analysis and examine their complexity. The approach described can be used to readily obtain similar complexity results for related analyses such as reachability and recursiveness.", "num_citations": "8\n", "authors": ["271"]}
{"title": "Testing Protocol Robustness the CCS Way.\n", "abstract": " We present a procedure to decide if a finite-state communication protocol provides end-users with a robust communication medium. The underlying model is algebraic and is based on Milner's CCS and observation equivalence, the notion that two processes cannot be distinguished by an external observer. In particular, we test if the protocol's visible behavior is observationally equivalent to the behavior of an ideal communication medium. We develop asynchronous versions of the protocols to illustrate our technique.", "num_citations": "8\n", "authors": ["271"]}
{"title": "Efficient dynamic taint analysis using multicore machines\n", "abstract": " Dynamic taint analysis is an effective method for detecting memory overwrite attacks before they can cause harm. However, initial implementations of the method employed interpretive execution, which causes an execution slowdown of 20 to 40 times. Recent work has reduced execution overhead by means of special hardware or by using dynamic binary instrumentation coupled with heavily optimized implementation of the taint checking code. This paper describes a new approach that uses two threads executing on different processor cores. In particular, one thread executes the original computation and a second thread \u201cshadows\u201d execution of the original to propagate taint values and to check for exploits. The two threads synchronize just enough for the shadow to follow the control flow of the program and to ensure that exploits are caught before they can cause damage. The approach is implemented by means of static binary rewriting. A prototype implementation correctly catches security exploits, with a performance overhead that ranges from a few percent on most programs to a factor of about two on some short-running programs.", "num_citations": "6\n", "authors": ["271"]}
{"title": "Profile-guided specialization of an operating system kernel\n", "abstract": " General-purpose operating systems such as Linux are increasingly replacing custom embedded counterparts on a wide variety of devices. Despite their convenience and flexibility, however, such operating systems may be overly general and thus incur unnecessary performance overheads in these contexts. This paper describes a new approach to mitigating these overheads by automatically specializing the OS kernel for particular execution environments. We use value profiling to identify targets for specialization such as frequent system call parameters. A novel profiling technique is used to identify frequently invoked procedure call sequences within the kernel. This information is used to sidestep the problems arising from indirect function calls when carrying out interprocedural compiler optimization. It drives a variety of compiler optimizations such as function inlining and code specialization that reduce the execution overheads along frequent paths. A prototype implementation that uses the PLTO binary rewriting system to specialize the Linux kernel is described. While overall performance data are mixed, the improvements we see argue for the potential of this approach.", "num_citations": "6\n", "authors": ["271"]}
{"title": "Return value placement and tail call optimization in high level languages\n", "abstract": " This paper discusses the interaction between tail call optimization and the placement of output values in functional and logic programming languages. Implementations of such languages typically rely on fixed placement policies: most functional language implementations return output values in registers, while most logic programming systems return outputs via memory. Such fixed placement policies incur unnecessary overheads in many commonly encountered situations: the former are unable to implement many intuitively iterative computations in a truly iterative manner, while the latter incur a performance penalty due to additional memory references. We describe an approach that determines, based on a low-level cost model for an implementation together with an estimated execution profile for a program, whether or not the output of a procedure should be returned in regsters or in memory. This can be seen as\u00a0\u2026", "num_citations": "6\n", "authors": ["271"]}
{"title": "Flow Analysis of a Simple Class of Dynamic Logic Programs.\n", "abstract": " Research on flow analysis and optimization of logic programs has typically concentrated on programs that are static, ie which do not change at runtime through the use of constructs like Prolog's' assert'. It is often the case, however, that a program may use assert in a way that has only local effects on parts of the program, leaving the rest of the program unaffected. In such cases, it would be desirable to be able to identify such unaffected portions of the program and carry out analysis and optimization on these portions as before. An outline is presented of how this might be done for a simple class of dynamic programs, using a type analysis scheme to estimate the effects of runtime modifications to the program. The author's approach allows static analysis and optimization techniques that have been developed for static programs to be extended to a reasonably large class of'well-behaved'dynamic programs.", "num_citations": "6\n", "authors": ["271"]}
{"title": "Deobfuscation: Improving reverse engineering of obfuscated code\n", "abstract": " In recent years, code obfuscation has attracted attention as a low cost approach to improving software security by making it difficult for attackers to understand the inner workings of proprietary software systems. This paper examines the extent to which current obfuscation techniques succeed in making programs harder to reverse engineer. Our results indicate that many obfuscations, designed to increase the difficulty of static analyses, are easily defeated using simple combinations of straightforward static and dynamic analyses. Our results have applications to both software engineering and software security. In the context of software engineering, we show how dynamic analyses can be used to enhance reverse engineering, even for code that has been designed to be difficult to reverse engineer. For software security, our results serve as an attack model for code obfuscators, and can help with the development of obfuscation techniques that are more resilient to straightforward reverse engineering. 1", "num_citations": "5\n", "authors": ["271"]}
{"title": "Writing efficient programs: performance issues in an undergraduate CS curriculum\n", "abstract": " Performance is an essential aspect of many software systems, and it is important for programmers to understand performance issues. However, most undergraduate curricula do not explicitly cover performance issues---performance monitoring and profiling tools, performance improvement techniques, and case studies---in their curricula. This paper describes how we address this topic as part of a third-year programming course. We focus on tools and techniques for monitoring and improving performance, as well as the interaction between clean program design and performance tuning.", "num_citations": "5\n", "authors": ["271"]}
{"title": "Load redundancy elimination on executable code\n", "abstract": " Optimizations performed at link time or directly applied to final program executables have received increased attention in recent years. This paper discuss the discovery and elimination of redundant load operations in the context of a link time optimizer, an optimization that we call Load Redundancy Elimination (LRE). Our experiments show that between 50% and 75% of a program\u2019s memory references can be considered redundant because they are accessing memory locations that have been referenced less than 200\u2013400 instructions away. We then present three profile-based LRE algorithms targeted at optimizing away these redundancies. Our results show that between 5% and 30% of the redundancy detected can indeed be eliminated, which translates into program speedups in the range of 3% to 8%. We also test our algorithm assuming different cache latencies, and show that, if latencies continue to\u00a0\u2026", "num_citations": "5\n", "authors": ["271"]}
{"title": "Apf: A modular language for fast packet classification\n", "abstract": " Fast packet classi cation| that is, the determination of the destination of a network packet| is of fundamental importance in high-performance network systems. Additionally, for exibility reasons, it is desirable to allow applications to use their own high-level protocols where appropriate. Taken together, this demands a mechanism that permits the speci cation of protocols in a simple, exible, and modular way without sacri cing performance. This paper describes APF, a language for specifying packet classi ers. The simple declarative syntax of this language makes it easy to specify even fairly complex packet structures in a clean and modular way, thereby improving reliability and maintainability. It also e ects a clean separation between the speci cation of packet classi ers and their implementations, thereby making it possible to choose from a variety of implementations with di erent performance tradeo s. In particular, as our experimental results illustrate, it can be compiled to e cient code whose speed can surpass that of well-known packet classi ers such as Path nder and BPF.", "num_citations": "5\n", "authors": ["271"]}
{"title": "Automated bug localization in JIT compilers\n", "abstract": " Many widely-deployed modern programming systems use just-in-time (JIT) compilers to improve performance. The size and complexity of JIT-based systems, combined with the dynamic nature of JIT-compiler optimizations, make it challenging to locate and fix JIT compiler bugs quickly. At the same time, JIT compiler bugs can result in exploitable security vulnerabilities, making rapid bug localization important. Existing work on automated bug localization focuses on static code, ie, code that is not generated at runtime, and so cannot handle bugs in JIT compilers that generate incorrect code during optimization. This paper describes an approach to automated bug localization in JIT compilers, down to the level of distinct optimization phases, starting with a single initial Proof-of-Concept (PoC) input that demonstrates the bug. Experiments using a prototype implementation of our ideas on Google\u2019s V8 JavaScript\u00a0\u2026", "num_citations": "4\n", "authors": ["271"]}
{"title": "A simple client-side defense against environment-dependent web-based malware\n", "abstract": " Web-based malware tend to be environment-dependent, which poses a significant challenge on defending web-based attacks, because the malicious code - which may be exposed and activated only under specific environmental conditions such as the version of the browser - may not be triggered during analysis. This paper proposes a simple approach for defending environment-dependent malware. Instead of increasing analysis coverage in detector, the goal of this technique is to ensure that the client will take the same execution path as the one examined by the detector. This technique is designed to work alongside a detector, it can handle cases existing multi-path exploration techniques are incapable of, and provides an efficient way to identify discrepancies in a JavaScript program's execution behavior in a user's environment compared to its behavior in a sandboxed detector, thereby detecting false\u00a0\u2026", "num_citations": "3\n", "authors": ["271"]}
{"title": "Increasing Undergraduate Involvement in Computer Science Research\n", "abstract": " Current undergraduate Computer Science curricula are generally built around a set of traditional lecture-oriented courses where the student is a passive recipient of knowledge. While easy to implement, such a model has the drawback of presenting the field as a static corpus of facts and techniques. It does little to challenge and engage the brightest of students, or prepare them to participate directly and actively in a highly dynamic and rapidly evolving field. Nor does it give them a sense of engagement, belonging, and ownership in this body of knowledge. This paper describes our experiences with addressing this situation via a model that aims to get undergraduates exposed to, interested in, and involved with research early in their academic careers. We use a set of closely related research-oriented courses, starting with research seminars suitable for freshmen and sophomores, and leading up to advanced projects for juniors and seniors. These courses have the effect of engaging talented undergraduates in research early in their college careers. T This approach has led to a dramatic increase in the amount of undergraduate involvement in academic Computer Science research in our department in the last few years, and resulted in numerous research publications and awards.", "num_citations": "3\n", "authors": ["271"]}
{"title": "Dynamic Path-Based Software Watermarking\n", "abstract": " Software watermarks\u2014bitstrings encoding some sort of identifying information that are embedded into executable programs\u2014are an important tool against software piracy. Most existing proposals for software watermarking have the shortcoming that they can be destroyed via fairly straightforward semantics-preserving code transformations. This paper introduces path-based watermarking, a new approach to software watermarking based on the dynamic branching behavior of programs. We show how error-correcting and tamper-proofing techniques can be used to make path-based watermarks resilient against a wide variety of attacks. Experimental results, using both Java bytecode and IA-32 native code, indicate that even relatively large watermarks can be embedded into programs at modest cost.", "num_citations": "3\n", "authors": ["271"]}
{"title": "Call Forwarding: A Simple Low-Level Code Optimization Technique\n", "abstract": " This paper discusses call forwarding, a simple interprocedural optimization technique for dynamically typed languages. The basic idea behind the optimization is very simple: generate multiple entry points for procedures such that a call to a procedure can be directed to the appropriate entry point, bypassing unnecessary code wherever possible. As shown by our experimental results, this simple optimization can be surprisingly effective, and lead to significant Derformance improvements.", "num_citations": "3\n", "authors": ["271"]}
{"title": "Static analysis of logic programs: an advanced tutorial\n", "abstract": " Static analysis of logic programs | Proceedings of the 1993 international symposium on Logic programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsILPS '93Static analysis of logic programs: an advanced tutorial Article Static analysis of logic programs: an advanced tutorial Share on Author: Saumya Kanti Debray profile image Saumya K. Debray View Profile Authors Info & Affiliations Publication: ILPS '93: Proceedings of the 1993 international symposium on Logic programmingDecember 1993 Pages 43\u201344 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Alerts ! '\u2026", "num_citations": "3\n", "authors": ["271"]}
{"title": "Cost Analysis of Logic Programs\n", "abstract": " Costanalysis of programs has been studied in the context ofimperative and functional programming languages. For logic programs, the problem is complicated by the fact that programs may be nondeterministic and produce multiple solutions. A related problem is that because failure of execution is not an abnormal situation, it is possible to write programs where implicit failures have to be dealt with explicitly in order to get meaningful results. This paper addresses these problems and develops a method for (semi-) automatic analysis of the worst-case cost of a large class of logic programs. The primary contribution of this paper is the development of techniques to deal with nondeterminism and the generation of multiple solutions via backtracking. Applications include program transformation and synthesis, software engineering, and in parallelizing compilers.", "num_citations": "3\n", "authors": ["271"]}
{"title": "Compiler optimizations for low-level redundancy elimination: An application of meta-level prolog primitives\n", "abstract": " Much of the work on applications of meta-level primitives in logic programs focusses on high-level aspects such as source-level program transformation, interpretation, and partial evaluation. In this paper, we show how meta-level primitives can be used in a very simple way for low-level code optimization in compilers. The resulting code optimizer is small, simple, efficient, and easy to modify and retarget. An optimizer based on these ideas is currently being used in a compiler that we have developed for Janus [6].", "num_citations": "3\n", "authors": ["271"]}
{"title": "Automates: Automated model assembly from text, equations, and software\n", "abstract": " Models of complicated systems can be represented in different ways - in scientific papers, they are represented using natural language text as well as equations. But to be of real use, they must also be implemented as software, thus making code a third form of representing models. We introduce the AutoMATES project, which aims to build semantically-rich unified representations of models from scientific code and publications to facilitate the integration of computational models from different domains and allow for modeling large, complicated systems that span multiple domains and levels of abstraction.", "num_citations": "2\n", "authors": ["271"]}
{"title": "Control dependencies in interpretive systems\n", "abstract": " Interpreters and just-in-time (JIT) compilers are ubiquitous in modern computer systems, making it important to have good program analyses for reasoning about such systems. Control dependence, which plays a fundamental role in a number of program analyses, is an important contender in this regard. Existing algorithms for (dynamic) control dependence analysis do not take into account some important runtime characteristics of interpretive computations, and as a result produce results that may be imprecise and/or unsound. This paper describes a new notion of control dependence and an analysis algorithm for interpretive systems. This significantly improves dynamic control dependence information, with corresponding improvements in client analyses such as dynamic program slicing and reverse engineering. To the best of our knowledge, this is the first proposal to reason about low-level dynamic\u00a0\u2026", "num_citations": "2\n", "authors": ["271"]}
{"title": "Code compression\n", "abstract": " Increasingly, we see a trend where programmeable processors are incorporated into a wide variety of everyday devices, ranging from \u201csmart badges,\u201d copy and fax machines, phones, and automobiles to traffic lights and wireless sensor networks. At the same time, the functionality expected of the software deployed on such processors becomes increasingly complex (e.g., general-purpose operating systems such as Linux on cell phones, intrusion-detection and related security security measures on wireless sensor devices). The increasing complexity of such software, and the reliability expected of them, suggest a plausible application of declarative languages. However, programs in declarative languages very often experience a significant increase in code size when they are compiled down to native code. This can be a problem in situations where the amount of memory available is limited. This talk\u00a0\u2026", "num_citations": "2\n", "authors": ["271"]}
{"title": "Signed system calls and hidden fingerprints\n", "abstract": " Remote code injection attacks against computer systems are occurring at an alarming frequency. A crucial aspect of such attacks is that in order to do any real damage, the injected attack code has to execute system calls, and therefore can be foiled by suitably hardening the system call interface. Most current proposals for doing so, however, suffer from various shortcomings, such as relying on special compilers or libraries, or incurring huge runtime overheads, or being vulnerable to mimicry attacks. This paper describes a systematic approach to defending against remote code injection attacks that uses two complementary techniques: cryptographic signatures to protect system calls themselves, and compiler-based techniques to hide code fingerprints that could be exploited for mimicry attacks. Experiments indicate that our approach is effective against a wide variety of attacks at modest cost.", "num_citations": "2\n", "authors": ["271"]}
{"title": "Software Power Optimization via Post-Link-Time Binary Rewriting\u00a3\n", "abstract": " It is well known that compiler optimizations can significantly reduce the energy usage of a program. However, the traditional model of compilation imposes inherent limits on the extent of code optimization possible at compile time. In particular, analyses and optimizations are typically limited to individual procedures, and hence cannot cross procedural and module boundaries as well as the boundaries between application and library code. These limitations can be overcome by carrying out additional code optimization on the object file obtained after linking has been carried out. These optimizations are complementary to those carried out by the compiler. Our experiments indicate that significant improvements in energy usage can be obtained via post-link-time code optimization, even for programs that have been subjected to extensive compile-time optimization.", "num_citations": "2\n", "authors": ["271"]}
{"title": "Link-time improvement of Scheme programs\n", "abstract": " Optimizing compilers typically limit the scope of their analyses and optimizations to individual modules. This has two drawbacks: first, library code cannot be optimized together with their callers, which implies that reusing code through libraries incurs a penalty; and second, the results of analysis and optimization cannot be propagated from an application module written in one language to a module written in another. A possible solution is to carry out (additional) program optimization at link time. This paper describes our experiences with such optimization using two different optimizing Scheme compilers, and several benchmark programs, via alto, a link-time optimizer we have developed for the DEC Alpha architecture. Experiments indicate that significant performance improvements are possible via link-time optimization even when the input programs have already been subjected to high levels of compile\u00a0\u2026", "num_citations": "2\n", "authors": ["271"]}
{"title": "Broadening field specialization\n", "abstract": " Three extensions to the conventional field specialization process are disclosed. The first extension is cross-application value flows, where a value transfers out of one application and subsequently into another application. The second extension is an inter-application analysis. Static and dynamic analysis is performed by a Spiff Toolset not just on the source code of a single application, but also across the data read and written by that application. The third extension is invariant cross-application termination, verifying the possibility of an invariant originating in an application and terminating in a specialization opportunity in a separate application. An ecosystem specification is disclosed to enable such field specialization broadening. This specification states which applications are involved, what input data sources are read, what intermediate and final data products are produced, and what services are invoked, thereby\u00a0\u2026", "num_citations": "1\n", "authors": ["271"]}
{"title": "Malware Analysis: From Large-Scale Data Triage to Targeted Attack Recognition (Dagstuhl Seminar 17281)\n", "abstract": " This report summarizes the program and the outcomes of the Dagstuhl Seminar 17281, entitled\" Malware Analysis: From Large-Scale Data Triage to Targeted Attack Recognition\". The seminar brought together practitioners and researchers from industry and academia to discuss the state-of-the art in the analysis of malware from both a big data perspective and a fine grained analysis. Obfuscation was also considered. The meeting created new links within this very diverse community.", "num_citations": "1\n", "authors": ["271"]}
{"title": "Language-Agnostic Optimization and Parallelization for Interpreted Languages\n", "abstract": " Scientists are increasingly turning to interpreted languages, such as Python, Java, R, Matlab, and Perl, to implement their data analysis algorithms. While such languages permit rapid software development, their implementations often run into performance issues that slow down the scientific process. Source-level approaches for parallelization are problematic for two reasons: first, many of the language features common to these languages can be challenging for the kinds of analyses needed for parallelization; and second, even where such analysis is possible, a language-specific approach implies that each language would need its own parallelizing compiler and/or constructs, resulting in significant duplication of effort. The Science Up To Par project is investigating a radically different approach to this problem: automatic parallelization at the machine code level using trace information. The key to accomplishing\u00a0\u2026", "num_citations": "1\n", "authors": ["271"]}
{"title": "Obfuscation Workshop Report\n", "abstract": " Obfuscation Workshop Report REPORT WORKSHOP Page 2 Page 3 OBFUSCATIONWORKSHOP. IO 3 CONTENTS Obfuscation Going Forward: A Research Agenda Finn Brunton, New York University and Helen Nissenbaum, Cornell Tech and New York University PrivacyVisor: Privacy Protection for Preventing Face Detection from Camera Images Isao Echizen, National Institute of Informatics, Tokyo Circumvention hrough Obfuscation Amir Houmansadr, University of Massachusetts Amherst Political Rhetoric as Obfuscation and Finding Solutions with Neural Networks Nicole Cote and Rob Hammond, New York University Obfuscating Data to Prevent Discrimination Sorelle Friedler, Haverford College Using Ethically-Constrained Game heory to Protect Our Privacy Jefrey Pawlick and Quanyan Zhu, New York University Tandon School of Engineering Identity Obfuscation hrough Fully Functional Avatars Paul Ashley, : , \u2026", "num_citations": "1\n", "authors": ["271"]}
{"title": "Compressing dynamic data structures in operating system kernels\n", "abstract": " Embedded systems are becoming increasingly complex and there is a growing trend to deploy complicated software systems such as operating systems and databases in embedded platforms. It is especially important to improve the efficiency of memory usage in embedded systems because these devices often have limited physical memory. Previous work on improving the efficiency of memory usage in OS kernels has mostly focused on reducing the size of code and global data in the OS kernel. This paper, by contrast, presents dynamic data structure compression, a complementary approach that reduces the runtime memory footprint of dynamic data structures. A prototype implementation for the Linux kernel reduces the memory consumption of the slab allocators in Linux by about 17.5% when running the MediaBench suite, while incurring only minimal increases in execution time (1.9%).", "num_citations": "1\n", "authors": ["271"]}
{"title": "Reverse Engineering Obfuscated Code\n", "abstract": " In recent years, code obfuscation has attracted attention as a low cost approach to improving software security by making it difficult for attackers to understand the inner workings of proprietary software systems. This paper examines techniques for automatic deobfuscation of obfuscated programs, as a step towards reverse engineering such programs. Our results indicate that much of the effects of code obfuscation, designed to increase the difficulty of static analyses, can be defeated using simple combinations of straightforward static and dynamic analyses. Our results have applications to both software engineering and software security. In the context of software engineering, we show how dynamic analyses can be used to enhance reverse engineering, even for code that has been designed to be difficult to reverse engineer. For software security, our results serve as an attack model for code obfuscators, and can help with the development of obfuscation techniques that are more resilient to straightforward reverse engineering.", "num_citations": "1\n", "authors": ["271"]}
{"title": "A multi-faceted defence mechanism against code injection attacks\n", "abstract": " We propose a exible host-based intrusion detection system against remote code injection attacks. There are two main aspects to our approach. The rst is to embed semantic information into executables identifying the locations of legitimate system call instructions; system calls from other locations are treated as intrusions. The modications we propose are transparent to user level processes that do not wish to take advantage of them (so that, for example, it is still possible to run unmodied third-party software), and add more security at minimal cost for those binaries that have the special information present. The second is to back this up using a variety of techniques, including a novel approach to encoding system call traps into the OS kernel, in order to deter mimicry attacks. Experiments indicate that our approach is effective against a wide variety of code injection attacks. 1", "num_citations": "1\n", "authors": ["271"]}
{"title": "Squeeze 0.3. 4-Ghent for Tru64Unix User\u2019s Manual\n", "abstract": " This document describes how Squeeze 0.3. 4 can be build and used to compact statically linked binaries for the Tru64Unix operating system. Squeeze 0.3. 4 has been thoroughly tested on binaries generated for Alpha 21164 and 21264 processors (up till version EV67) and for Tru64Unix versions 4.0 D through 5.1.Section 2 describes how Squeeze is to be build from the source code in the package. Section 3 describes how binaries should be constructed for compaction by Squeeze. Section 4 describes how Squeeze should be executed on the binaries to be compacted. In section 5 some ways of debugging Squeeze are discussed. Finally some platform specific issues are discussed in section 6. We hope to find the time to add some more information on the interal working of Squeeze in the near future.", "num_citations": "1\n", "authors": ["271"]}