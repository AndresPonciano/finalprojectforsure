{"title": "A general model for component-based software reliability\n", "abstract": " Aggregating components into software is a perfect approach to construct software with the maturity of component market. How to analyze software reliability from the reliabilities of its components and architecture should be answered. However, software in most of the proposed reliability analysis methods is static, while software development is a dynamic process, especially for component-based software, where pervasive process is iterative and incremental. Based on function abstractions, we present a general model-component probability transition diagram, which is compatible with different kinds of components and enables reliability tracing through component-based software process.", "num_citations": "66\n", "authors": ["87"]}
{"title": "\u4e00\u79cd\u8f6f\u4ef6\u53ef\u4fe1\u5206\u7ea7\u6a21\u578b\n", "abstract": " \u8f6f\u4ef6\u53ef\u4fe1\u8bc4\u4f30\u662f\u8f6f\u4ef6\u53ef\u4fe1\u7814\u7a76\u4e2d\u7684 O \u4e2a\u65b0\u65b9\u5411, \u76f8\u5173\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u8fd8\u5904\u5728\u7814\u7a76\u9636\u6bb5, \u5728\u5206\u6790\u8f6f\u4ef6\u53ef\u4fe1\u5185\u6db5\u7684\u57fa\u7840\u4e0a, \u63d0\u51fa\u4e86 O \u79cd\u5206\u5c42\u7684\u8f6f\u4ef6\u53ef\u4fe1\u5206\u7ea7\u6a21\u578b, \u8be5\u6a21\u578b\u5b9a\u4e49\u4e86\u8f6f\u4ef6\u53ef\u4fe1\u5c5e\u6027\u6a21\u578b\u4e0e\u8f6f\u4ef6\u53ef\u4fe1\u7b49\u7ea7, \u5efa\u7acb", "num_citations": "24\n", "authors": ["87"]}
{"title": "An empirical study on the usage of fault localization in automated program repair\n", "abstract": " Spectrum-based fault localization (SFL), the technique producing a rank list of statements in descending order of their suspiciousness values, is nowadays widely used in current automated program repair tools. There are two different algorithms for these tools to choose statements selected for modification to produce candidate patches from the list: one is the rank-first algorithm based on suspiciousness rankings of statements, the other is the suspiciousness-first algorithm based on suspiciousness value of statements. However, to our knowledge there is no research work implementing the two algorithms in the same repair tool or comparing their effectiveness. In this paper, we conduct an empirical research based on the automated repair tool Nopol with the benchmark set of Defects4J to compare these two algorithms. Preliminary results suggest that the suspiciousness-first algorithm is not equivalent to the rank-first\u00a0\u2026", "num_citations": "18\n", "authors": ["87"]}
{"title": "Using ELECTRE TRI outranking method to evaluate trustworthy software\n", "abstract": " Trustworthy software evaluation is taken as the multi-criteria decision aiding process in this paper. The use of ELECTRE TRI method for evaluating software trustworthiness is presented. Software under evaluation is compared with some predefined norms and is assigned to one of trust levels. The entire evaluating process is described, including definition of problem situation and formulation, determination of the model and its parameters, and the application of the model. A metric for trustworthiness and an attributes weighting method are also presented. Some practical considerations are discussed in the final part of the paper.", "num_citations": "16\n", "authors": ["87"]}
{"title": "Detecting dom-sourced cross-site scripting in browser extensions\n", "abstract": " In recent years, with the advances in JavaScript engines and the adoption of HTML5 APIs, web applications begin to show a tendency to shift their functionality from the server side towards the client side, resulting in dense and complex interactions with HTML documents using the Document Object Model (DOM). As a consequence, client-side vulnerabilities become more and more prevalent. In this paper, we focus on DOM-sourced Cross-site Scripting (XSS), which is a kind of severe but not well-studied vulnerability appearing in browser extensions. Comparing with conventional DOM-based XSS, a new attack surface is introduced by DOM-sourced XSS where the DOM could become a vulnerable source as well besides common sources such as URLs and form inputs. To discover such vulnerability, we propose a detecting framework employing hybrid analysis with two phases. The first phase is the lightweight\u00a0\u2026", "num_citations": "15\n", "authors": ["87"]}
{"title": "More efficient automatic repair of large-scale programs using weak recompilation\n", "abstract": " Automatically repairing a bug can be a time-consuming process especially for large-scale programs owing to the significant amount of time spent recompiling and reinstalling the patched program. To reduce this time overhead and speed up the repair process, in this paper we present a recompilation technique called weak recompilation. In weak recompilation, we assume that a program consists of a set of components, and for each candidate patch only the altered components are recompiled to a shared library. The original program is then dynamically updated by a function indirection mechanism. The advantage of weak recompilation is that redundant recompilation cost can be avoided, and while the reinstallation cost is completely eliminated as the original executable program is not modified at all. For maximum applicability of weak recompilation we created WAutoRepair, a scalable system for fixing\u00a0\u2026", "num_citations": "14\n", "authors": ["87"]}
{"title": "Extending UML for aspect-oriented architecture modeling\n", "abstract": " Aspect-Oriented Modeling (AOM) is an important part of Aspect-Oriented Software Development (AOSD). Software architecture design is a high-level abstract description of software system. The traditional software architecture design approaches do not provide an independent mechanism for crosscutting concerns, which will lead to increased maintenance overhead, reduced reuse capability and architectural erosion over the lifetime of a system. Therefore, it is necessary to provide a new mechanism for the crosscutting concerns in the architecture modeling. In this paper, the architectural design phase and Aspect-Oriented (AO) concepts are considered jointly. A proposal is presented which introduces Aspect-Oriented concepts to the software architectural design phase to support Aspect-Oriented Architecture Modeling (AOAM). As a result, we give an example to illustrate the approach.", "num_citations": "10\n", "authors": ["87"]}
{"title": "An empirical study on the effect of dynamic slicing on automated program repair efficiency\n", "abstract": " Research on the characteristics of error propagation can guide fault localization more efficiently. Spectrum-based fault localization (SFL) and slice-based fault localization are effective fault localization techniques. The former produces a list of statements in descending order of suspicious values, and the latter generates statements that affect failure statements. We propose a new dynamic slicing and spectrum-based fault localization (DSFL) method, which combines the list of suspicious statements generated by SFL with dynamic slicing, and take the characteristics of error propagation into account. To the best of our knowledge, DSFL has not yet been implemented in automated repair tools. In this study, we use the dynamic slicing tool Javaslicer to determine the error propagation chain of faulty programs and the statements related to failure execution. We implement the DSFL algorithm in the automated repair tool\u00a0\u2026", "num_citations": "8\n", "authors": ["87"]}
{"title": "Domxssmicro: A micro benchmark for evaluating dom-based cross-site scripting detection\n", "abstract": " With the prevalence of JavaScript, Cross-site Scripting based on Document Object Model (DOM-based XSS) has become one of critical threats to client-side Web applications. To detect DOM-based XSS vulnerabilities, a variety of tools have been developed, providing different features and abilities. Both for developers and tool users, the benchmark plays an important role in evaluating the effectiveness of detection tools. However, no widely used standard benchmark exists in the domain of DOM-based XSS. In this paper, we present a micro benchmark named DomXssMicro. DomXssMicro is constructed based on a template extracted from representative vulnerabilities, consisting of six orthogonal components (i.e. Source, Propagation, Transformation, Sink, Trigger and Context). In DomXssMicro, there are 175 test cases in total, each one of which aims at testing a specific property of DOM-based XSS. To illustrate\u00a0\u2026", "num_citations": "7\n", "authors": ["87"]}
{"title": "Automatic fault localization for BIP\n", "abstract": " This paper presents a novel idea of automatic fault localization by exploiting counterexamples generated by a model checker. The key insight is that, if a candidate statement is faulty, it is possible to modify (i.e. correct) this statement so that the counterexample is eliminated. We have implemented the proposed fault localization algorithm for component-based systems modelled in the BIP (Behaviour, Interaction and Priority) language, and conducted the first experimental evaluation on a set of benchmarks with injected faults, showing that our approach is promising and capable of quickly and precisely localizing faults.", "num_citations": "7\n", "authors": ["87"]}
{"title": "An exploratory research of GitHub based on graph model\n", "abstract": " GitHub has accumulated a great number of developers and open source projects. In this research, we utilize property graph model to explore complex relationships and entities of GitHub. We attempt to answer three questions associated with GitHub using the dataset from MSR2014 data challenge. Firstly, we propose a graph based method to find out the cross technology background developers on GitHub. Secondly we define interesting metrics based on discrete entropy to analyze the project imbalance induced by commit action within a software family. The results show that the imbalance of development size induced by root projects is greater than that of development speed. Finally, we sort out the relatively important root projects with two link analysis methods and the experiment result demonstrates that our method is effective.", "num_citations": "7\n", "authors": ["87"]}
{"title": "Towards a proof assistant for interval logics\n", "abstract": " In this paper, we describe an attempt to construct a proof assistant for a family of interval logics, and in our prototype we have covered the Neighbourhood Logic Zhou96a], the Interval Temporal Logic Mos85] and the Mean Value Calculus ZL93], using the veri cation system PVS OSR93] SOR93]. We stress the hierarchy of the system: the basic logics are de ned rst and then exported to the more advanced ones, and so the theorems of the former are available in the later. Other interval logics can be added easily into this framework. We have used the system to verify a number of examples and found that it provides considerable assistance.", "num_citations": "7\n", "authors": ["87"]}
{"title": "Towards a proof assistant for interval logics\n", "abstract": " In this paper, we describe an attempt to construct a proof assistant for a family of interval logics, and in our prototype we have covered the Neighbourhood Logic, the Interval Temporal Logic and the Mean Value Calculus, using the verification system PVS. We stress the hierarchy of the system: the basic logics are defined first and then exported to the more advanced ones, and so the theorems of the former are available in the later. Other interval logics can be added easily into this framework. We have used the system to verify a number of examples and found that it provides considerable assistance.", "num_citations": "7\n", "authors": ["87"]}
{"title": "Alleviating the impact of coincidental correctness on the effectiveness of sfl by clustering test cases\n", "abstract": " Spectrum-based fault localization techniques leverage coverage information to identify the faulty elements of the program via passed and failed runs. However, the effectiveness of these techniques can be affected adversely by coincidental correctness, which occurs when faulty elements are executed, but the program produces the correct output. This paper proposes a clustering-based strategy to improve the effectiveness of spectrum-based fault localization. The basis of this strategy is that test cases in the same cluster have similar behaviors. Our experimental results show that, the percentage of clusters that contain coincidentally correct test cases in clusters which do not contain failed test cases, is usually smaller than the percentage of coincidentally correct test cases in passed test cases. By clustering test cases and reconstructing the coverage matrix, our extensive experiments demonstrated that the fault\u00a0\u2026", "num_citations": "6\n", "authors": ["87"]}
{"title": "Evaluating the strategies of statement selection in automated program repair\n", "abstract": " Automated program repair has drawn significant attention in recent years for its potential to alleviate the heavy burden of debugging activities. Fault localization, which generally provides a rank list of suspicious statements, is necessary for automated repair tools to identify the fault. With such rank list, existing repair tools have two statement selecting strategies for statement modification: suspiciousness-first algorithm (SFA) based on the suspiciousness of statements and rank-first algorithm (RFA) relying on the rank of statements. However, despite the extensive application of SFA and RFA in repair tools and different selecting methods between both strategies, little is known about the performance of the two strategies in automated program repair.                 In this paper we conduct an empirical study to compare the effectiveness of SFA and RFA in automated program repair. We implement SFA and RFA as\u00a0\u2026", "num_citations": "5\n", "authors": ["87"]}
{"title": "An empirical study on interaction factors influencing bug reopenings\n", "abstract": " Bugs can be reopened after they have been closed due to identification of the actual cause, previous incorrect fixing, or better reproducing, etc. Reopened bugs may increase the cost in maintenance, degrade the overall quality of the software product, reduce the trust of users, and bring unnecessary work to the already-busy developers. To minimize the occurrence of bug reopenings, the potential causes and factors should be analyzed. In this paper, we explore 24 interaction factors to study their influence on bug reopenings. The data are extracted from Mylyn logs of four open-source projects. We first verify the negative impacts of bug reopenings. Then, we identify 17 factors that significantly influence the likelihood of bug reopenings using statistic tests. In addition, we build decision trees using interaction factors to predict bug reopenings and achieve good performance.", "num_citations": "5\n", "authors": ["87"]}
{"title": "Bounded model checking of ETL cooperating with finite and looping automata connectives\n", "abstract": " As a complementary technique of the BDD-based approach, bounded model checking (BMC) has been successfully applied to LTL symbolic model checking. However, the expressiveness of LTL is rather limited, and some important properties cannot be captured by such logic. In this paper, we present a semantic BMC encoding approach to deal with the mixture of                       and                      . Since such kind of temporal logic involves both finite and looping automata as connectives, all regular properties can be succinctly specified with it. The presented algorithm is integrated into the model checker ENuSMV, and the approach is evaluated via conducting a series of imperial experiments.", "num_citations": "5\n", "authors": ["87"]}
{"title": "Multi-location program repair strategies learned from past successful experience\n", "abstract": " Automated program repair (APR) has great potential to reduce the effort and time-consumption in software maintenance and becomes a hot topic in software engineering recently with many approaches being proposed. Multi-location program repair has always been a challenge in this field since its complexity in logic and structure. While some approaches do not claim to have the features for solving multi-location bugs, they generate correct patches for these defects in practice. In this paper, we first make an observation on multi-location bugs in Defects4J and divide them into two categories (i.e., similar and relevant multi-location bugs) based on the repair actions in their patches. We then summarize the situation of multi-location bugs in Defects4J fixed by current tools. We analyze the twenty-two patches generated by current tools and propose two feasible strategies for fixing multi-location bugs, illustrating them through two detailed case studies. At last, the experimental results prove the feasibility of our methods with the repair of two bugs that have never been fixed before. By learning from successful experience in the past, this paper points out possible ways ahead for multi-location program repair.", "num_citations": "4\n", "authors": ["87"]}
{"title": "Taint Inference for Cross\u2010Site Scripting in Context of URL Rewriting and HTML Sanitization\n", "abstract": " Currently, web applications are gaining in prevalence. In a web application, an input may not be appropriately validated, making the web application susceptible to cross\u2010site scripting (XSS), which poses serious security problems for Internet users and websites to whom such trusted web pages belong. A taint inference is a type of information flow analysis technique that is useful in detecting XSS on the client side. However, in existing techniques, two current practical issues have yet to be handled properly. One is URL rewriting, which transforms a standard URL into a clearer and more manageable form. Another is HTML sanitization, which filters an input against blacklists or whitelists of HTML tags or attributes. In this paper, we make an analogy between the taint inference problem and the molecule sequence alignment problem in bioinformatics, and transfer two techniques related to the latter over to the former to\u00a0\u2026", "num_citations": "4\n", "authors": ["87"]}
{"title": "Counterexample-preserving reduction for symbolic model checking\n", "abstract": " The cost of LTL model checking is highly sensitive to the length of the formula under verification. We observe that, under some specific conditions, the input LTL formula can be reduced to an easier-to-handle one before model checking. In our reduction, these two formulae need not to be logically equivalent, but they share the same counterexample set w.r.t the model. In the case that the model is symbolically represented, the condition enabling such reduction can be detected with a lightweight effort (e.g., with SAT-solving). In this paper, we tentatively name such technique \u201cCounterexample-Preserving Reduction\u201d (CePRe, for short), and the proposed technique is experimentally evaluated by adapting NuSMV.", "num_citations": "4\n", "authors": ["87"]}
{"title": "Bodhi: Detecting buffer overflows with a game\n", "abstract": " Buffer overflow is one of the most dangerous and common vulnerabilities in CPS software. Despite static and dynamic analysis, manual analysis is still heavily used which is useful but costly. Human computation harness humans' time and energy in a way of playing games to solve computational problems. In this paper we propose a human computation method to detect buffer overflows that does not ask a person whether there is a potential vulnerability, but rather a random person's idea. We implement this method as a game called Bodhi in which each player is shown a piece of code snippet and asked to choose whether their partner would think there is a buffer overflow vulnerability at a given position in the code. The purpose of the game is to make use of the rich distributed human resource to increase effectiveness of manual detection for buffer overflows. The game has been proven to be efficient and enjoyable\u00a0\u2026", "num_citations": "4\n", "authors": ["87"]}
{"title": "Light-weight resource leak testing based on finalisers\n", "abstract": " Despite garbage collectors, programmers must manually manage many non-memory `finite system resources' such as file descriptors and database connections. Unreleased resources result in `resource leaks' that degrade application performance and can even result in system crashes. It is hard to test resource leaks because of their no immediate symptoms in the short run. There are analysis techniques to detect resource leaks statically or dynamically. However, all of them require the formal specification as input, which seriously decrease their practicability for common software developers. In this study, the authors propose an easy-to-use yet effective resource leak testing approach for Java programs based on existing finalisers of Java Application Programming Interface (API) classes. They instrument resource classes to check that the cleanup method of a resource object is called before its finaliser's execution\u00a0\u2026", "num_citations": "3\n", "authors": ["87"]}
{"title": "Efficient automatic program repair using function-based part-execution\n", "abstract": " As an emerging paradigm for automated debugging on software system, automatic program repair plays a more and more important role on computer development. Currently, although there are lots of approaches for automatically repairing errors, they do not work very well due to time-consuming validation especially when the faulty programs equip with some long-running test cases. To suppress the testing cost, we present the technique of function-based part-execution (FPE), by which only the key code, instead of the whole patched program, is executed. In addition, the invariant detection technique is applied to predict imminent program failure with incomplete execution. The controlled experiment on real bug in the PHP program show that our approach performs much better than the original Genprog, a state-of-the-art approach on automatic program repair.", "num_citations": "3\n", "authors": ["87"]}
{"title": "Understanding the non-repairability factors of automated program repair techniques\n", "abstract": " Automated Program Repair (APR) is becoming a hot topic in Software Engineering community with many approaches being proposed and experiments being performed over the years. The results obtained from different experiments can be used as practical guidance to advance APR techniques. However, researchers have generally ignored the biases with respect to the unexpected results generated by various APR techniques, in which case the repair process cannot be finished normally and is terminated with unexpected exceptions (referred to as the non-repairability factors). In this paper, we aim to thoroughly understand the reasons for such non-repairability factors of various APR techniques, thus to provide practical insights for diverse stakeholders to establish an unbiased evaluation of APR techniques. To achieve so, we performed a systematic study on the existing execution logs that are ended with\u00a0\u2026", "num_citations": "2\n", "authors": ["87"]}
{"title": "Towards connecting discrete mathematics and software engineering\n", "abstract": " To enhance training in software development, we argue that students of software engineering should be exposed to software development activities early in the curriculum. This entails meeting the challenge of engaging students in software development before they take the software engineering course. In this paper, we propose a method to connect courses in the software engineering curriculum by setting comprehensive development projects to students in prerequisite courses for software development. Using the Discrete Mathematics (DM) course as an example, we describe the implementation of the proposed method and teaching practices using several practical and comprehensive projects derived from topics in discrete mathematics. Detailed descriptions of the sample projects, their application, and training results are given. Results and lessons learned from applying these practices show that it is a\u00a0\u2026", "num_citations": "2\n", "authors": ["87"]}
{"title": "An initial step towards organ transplantation based on GitHub repository\n", "abstract": " Organ transplantation, which is the utilization of codes directly related to some specific functionalities to complete one\u2019s own program, provides more convenience for developers than the traditional component reuse. However, recent techniques are challenged with the lack of organs for transplantation. Hence, we conduct an empirical study on extracting organs from GitHub repository to explore transplantation based on large-scale data set. We analyze statistics from 12 representative GitHub projects and get the conclusion that 1) there are abundant practical organs existing in commits with \u201cadd\u201d as a key word in the comments; 2) organs in this repository mainly possess four kinds of contents; and 3) approximately 70% of the organs are easy-to-transplant. Implementing our transplantation strategy for different kinds of organs, we manually extract 30 organs in three different programming languages, namely Java\u00a0\u2026", "num_citations": "2\n", "authors": ["87"]}
{"title": "A hybrid approach for tag hierarchy construction\n", "abstract": " Open source resources are playing a more and more important role in software engineering for reuse. However, the dramatically increasing scale of these resources brings great challenges for their management and location. In this study, we propose a hybrid approach for automatic tag hierarchy construction, which combines the tag co-occurrence relations and domain knowledge to build and optimize the hierarchy. We firstly calculate the generality of each tag in accordance with the co-occurrence relationship with others, and construct the hierarchy based on the generality. Then we leverage the domain knowledge of existing hierarchical categories to perform an optimization and promote the final hierarchy. We select 8064 projects in Openhub community and 10703 posts in StackOverflow community as the original data and use the information of the SourceForge community as the domain knowledge\u00a0\u2026", "num_citations": "2\n", "authors": ["87"]}
{"title": "Obfuscated malicious JavaScript detection by machine learning\n", "abstract": " In recent years, malicious JavaScript code has become more and more pervasive and been used by attackers to perform their attacks on the Web. To evade the detection of defense measures, various kinds of obfuscation techniques have been applied by the malicious script, taking advantage of the dynamic nature of JavaScript language. In this paper, we propose a new machine-learning based detection approach aiming at defeating such evasion attempts. Dynamic execution traces are recorded to capture all behaviors performed by the malicious script, including the dynamic generated code. Semantic-based deobfuscation is used to simplify the traces to get more concise and more essential instructions. None-ordered and none-concessive trace patterns are extracted from the deobfuscated traces to represent the intrinsic features for malicious scripts. We evaluated our approach with a large number of dataset collected from the Internet. The empirical results demonstrate that our approach is able to detect obfuscated malicious JavaScript code both effectively and efficiently.", "num_citations": "2\n", "authors": ["87"]}
{"title": "Counterexample-preserving reduction for symbolic model checking\n", "abstract": " The cost of LTL model checking is highly sensitive to the length of the formula under verification. We observe that, under some specific conditions, the input LTL formula can be reduced to an easier-to-handle one before model checking. In such reduction, these two formulae need not to be logically equivalent, but they share the same counterexample set w.r.t the model. In the case that the model is symbolically represented, the condition enabling such reduction can be detected with a lightweight effort (e.g., with SAT-solving). In this paper, we tentatively name such technique \u201ccounterexample-preserving reduction\u201d (CePRe, for short), and the proposed technique is evaluated by conducting comparative experiments of BDD-based model checking, bounded model checking, and property directed reachability-(IC3) based model checking.", "num_citations": "2\n", "authors": ["87"]}
{"title": "Optimizing nop-shadows typestate analysis by filtering interferential configurations\n", "abstract": " Nop-shadows Analysis (NSA) is an efficient static typestate analysis, which can be used to eliminate unnecessary monitoring instrumentations for runtime monitors. In this paper, we propose two optimizations to improve the precision of NSA. Both of the optimizations filter interferential configurations when determining whether a monitoring instrumentation is necessary. We have implemented our optimization methods in Clara and conducted extensive experiments on the DaCapo benchmark. The experimental results indicate that the optimized NSA can further remove unnecessary instrumentations after the original NSA in more than half of the cases, without a significant overhead. In addition, for two cases, all the instrumentations are removed, which implies the program is proved to satisfy the typestate property.", "num_citations": "2\n", "authors": ["87"]}
{"title": "Fault localization via behavioral models\n", "abstract": " Software errors significantly impact software quality, and improvement of fault localization can reduce the expense of debugging. SBFL is promising approach and have received a lot of attention due to its simplicity and effectiveness. However, there are still some limits in this approach. In this paper we address these limits and present a technique to build a novel behavioral model, and propose a corresponding fault localization technique. Furthermore, we performed a study to empirically validate the effectiveness of our approach. And experimental results indicated that our method outperformed other representative SBFL techniques in locating faults in the benchmark. We show that the effectiveness of the technique comes from the guidance of hierarchical behavioral model.", "num_citations": "2\n", "authors": ["87"]}
{"title": "Research on Aspect-Oriented Architecture Modeling\n", "abstract": " Aspect-oriented architecture modeling is an important part of aspect-oriented software development and a hot topic in the field of aspect-oriented research. The traditional software architecture design approaches do not provide an independent mechanism for crosscutting concerns. As a result, it is necessary to provide a new mechanism for crosscutting concerns in architecture modeling. This paper proposes a concept framework for aspect-oriented architecture; then the architectural design phase and the core aspect-oriented concepts in the concept framework are considered jointly. As a result, we propose an aspect-oriented architecture modeling approach which introduces the aspect-oriented concepts to the software architectural design phase.", "num_citations": "2\n", "authors": ["87"]}
{"title": "Research on parallelization of aspect-oriented program\n", "abstract": " Aspect-oriented programming, as an ideal candidate to encapsulate crosscutting functionalities, has been adopted for run-time monitoring, failure forecasting, fault tolerance and etc. While, nowadays aspect-oriented techniques are not used to multi-core computing platforms. To cope with that, this paper makes AspectJ, a typical aspect-oriented programming language, suitable for parallel program, and yet achieves parallelization among aspects and base program. Based on Java multi-thread mechanism, an algorithm has been presented to achieve automatic parallelization according to prescriptive denotations.", "num_citations": "2\n", "authors": ["87"]}
{"title": "Aspect-oriented software development: philosophy and observation\n", "abstract": " Aspect-oriented software development (AOSD) is an emerging and leading field in the literature of software development. This paper discusses the fundamental concepts of AOSD from the perspective of software methodology and the philosophical foundation of aspects. Based on that, this paper tries to make the objectives and core concerns of AOSD clear after analyzing its history.", "num_citations": "2\n", "authors": ["87"]}
{"title": "Asymptotic behavior of order statistic least mean square (OSLMS) algorithms in nonGaussian environments\n", "abstract": " These algorithms modify the ordinary LMS algorithm by applying an OS filtering operation to the instantaneous gradient estimate. The OS operation in OSLMS can reduce the bias on filter coefficient estimates (relative to LMS) when operating in non-Gaussian environments and can also reduce the average squared parameter error when in steady state operation. Some supporting analysis is presented for these effects, and simulation studies are provided. Guidelines are suggested for the selection of the OSLMS algorithms based on the expected noise environment.< >", "num_citations": "2\n", "authors": ["87"]}
{"title": "Lightweight global and local contexts guided method name recommendation with prior knowledge\n", "abstract": " The quality of method names is critical for the readability and maintainability of source code. However, it is often challenging to construct concise method names. To alleviate this problem, a number of approaches have been proposed to automatically recommend high-quality names for methods. Despite being effective, existing approaches meet their bottlenecks mainly in two aspects:(1) the leveraged information is restricted to the target method itself; and (2) lack of distinctions towards the contributions of tokens extracted from different program contexts. Through a large-scale empirical analysis on+ 12M methods from+ 14K real-world projects, we found that (1) the tokens composing a method\u2019s name can be frequently observed in its callers/callees; and (2) tokens extracted from different specific contexts have diverse probabilities to compose the target method\u2019s name. Motivated by our findings, we propose, in this\u00a0\u2026", "num_citations": "1\n", "authors": ["87"]}
{"title": "Language to Code with Open Source Software\n", "abstract": " With the development of deep learning, it has been applied in various field of computer science. Generating computer executable code from natural language descriptions is an urgent problem in the artificial intelligence. This paper proposed a solution based on deep learning for code generation. Encoder-Decoder model is used in our method to convert natural language description into target code. Because of the rapid development of information technology, all aspects of software resources have been greatly enriched. The deep learning model we designed takes the natural language description as input and generates the corresponding object code by extracting the code from the open source software library. We collected natural language descriptions of 20 problems that undergraduate students often encounter in their daily programming. Experimental results show that our method is practicable. Our approach\u00a0\u2026", "num_citations": "1\n", "authors": ["87"]}
{"title": "Dissection on Java Organs in GitHub Repositories\n", "abstract": " Organ transplantation has brought convenience for software reuse and evolution since it was proposed. However, studies about mature, high-quality organs are still insufficient. It is still unclear about the detailed characteristics of organs in the open-source environment. In this paper, we look deep into organs obtained from software evolution processes of the ten large-scale Java repositories hosted on GitHub, aiming at providing practical information for utilizing organs in the open-source environment. We found that: 1) commits use add as a keyword in their comments possess the most organs, occupying 38% of the total amount, but commits with the keyword fix possess the highest locating accuracy (about 57%); 2) developers prefer to add new classes when they bring new functionalities to the projects in that the proportion of class level organs is 40%, more than statement level organs' and function level organs'\u00a0\u2026", "num_citations": "1\n", "authors": ["87"]}
{"title": "A study on code transplantation technique based on program slicing\n", "abstract": " Code transplantation, which is using a function of code from one software directly into another, and make the function work in it. Several tools have been built to transplant code, but there is still no research solved the problem of transplanting open source software. Open source software has the characteristics of coming from unclear source, irregular, and function definition unclear, which is not conducive to transplant. In our research process, we find program slicing can deal with these characteristics, help programmer to get the code related to the function they want to transplant. In this paper, we introduce a method to transplant code from open source software, the results revealed that our method can significantly reduce programmer\u2019s work and can be used to real world open source software.", "num_citations": "1\n", "authors": ["87"]}
{"title": "Analyst-oriented taint analysis by taint path slicing and aggregation\n", "abstract": " Taint analysis determines whether values from untrusted or private sources may flow into security-sensitive or public sinks, and can discover many common security vulnerabilities in both Web and mobile applications. Static taint analysis detects suspicious data flows without running the application and achieves a good coverage. However, most existing static taint analysis tools only focus on discovering taint paths from sources to sinks and do not concern about the requirements of analysts for sanitization check and exploration. The sanitization can make a taint path no more dangerous but should be checked or explored by analysts manually in many cases and the process is very costly. During our preliminary study, we found that many statements along taint paths are not relevant to the sanitization and there are a lot of redundancies among taint paths with the same source or sink. Based on these two observations\u00a0\u2026", "num_citations": "1\n", "authors": ["87"]}
{"title": "Introduction to programming: science or art?\n", "abstract": " In this poster, we report our experience in teaching introductory courses on programming based on program derivation using formal method. Based on an ongoing teaching activity, we present some preliminary results on the students' experiences.", "num_citations": "1\n", "authors": ["87"]}
{"title": "Incremental semantic LTL bounded model checking\n", "abstract": " Bounded model checking has proven to be an efficient method for finding bugs in system designs. In this paper, we present an incremental semantic translation for Bounded model checking and give an incremental algorithm. We implement this method in NuSMV model checker and report encouraging results.", "num_citations": "1\n", "authors": ["87"]}
{"title": "Hierarchical behavioral model for automatic runtime anomaly detection\n", "abstract": " In order to detect runtime errors using program monitoring, specifications are necessary. This paper presents a novel model that describes hierarchical structure of program runtime behavior. The model is composed of elements with granularities from activity, object to action. By observing program behavior as it runs, we construct hierarchical view of program executions and extract behavioral models automatically. After that, we propose an automatic technique for runtime error detection for java programs with a preliminary experiment.", "num_citations": "1\n", "authors": ["87"]}
{"title": "Light-weight test oracles for resource leaks based on finalizers\n", "abstract": " Garbage collectors automatically manage memory in an elegant way. However, finalization performs poorly in reclaiming finite system resources such as file descriptors and database connections, especially in Java. Resource leaks degrade application performance and can even result in system crashes. Unfortunately, it is hard to test resource leaks because they have no immediate symptoms. In this paper, we first investigate Java API classes for the usage of finalizers. Based on the common usage pattern of these finalizers, we then propose a lightweight, easy-to-use yet effective resource leak testing approach called Orbaf. Orbaf instruments resource classes to inspect whether the cleanup method of a resource object is called before its finalizer's execution. Test cases are also optionally instrumented to improve the incidence of object finalization. Orbaf can detect leaks of most system resources and does not\u00a0\u2026", "num_citations": "1\n", "authors": ["87"]}
{"title": "Mining and checking web services behavior\n", "abstract": " As an emerging paradigm for architecting, service-oriented computing plays a more and more important role in information technology. To ensure that web services are working according with the expectation, the research in service behavior about interactions between services is crucial to guarantee no deviation from specification. To detect these deviations, we propose a method to mine and check web service behavior. The method attempts to apply process mining to interactions between services in order to narrow the gap between service consumers and providers. By observing executions of services, we present three levels of abstraction on service behavior: internal behavior, external behavior and workflow behavior. Then we outline the approach to extract behavioral model that provides specification to the activity of service checking. Finally the framework of service behavior checking is presented.", "num_citations": "1\n", "authors": ["87"]}
{"title": "A RUNTIME MONITORING METHOD FOR COMPOSITE WEB SERVICES\n", "abstract": " With the rapid development of computer technology, software is facing increasingly severe dependability requirements. As a result, runtime monitoring has attracted wide public concern as an important way to ensure software dependability. In this paper we analyse and summarise the basic problem and method of traditional runtime monitoring technology, present a basic view in regard to the runtime monitoring method. Then we propose a monitoring framework for special running environment and monitoring demands the composite Web services encountering. The framework uses the event calculus to model behaviour protocol, and achieves effective monitoring of the composite services in the case of being unable to instrument in component services.", "num_citations": "1\n", "authors": ["87"]}
{"title": "A Model of Dependability Concept Based on UML\n", "abstract": " With wide applications of computing systems in the economy and national defense field, the dependability of computing systems has become an important non-functional indicator. In this paper, we establish a concept of dependability model based on UML. Firstly, the concept of dependability is given as a high-level Abstract UML class diagrams. Secondly, we separately describe the UML models of attributes, threats and means, especially the pathology of failure and system fault tolerance technology. Finally, we point out that the dependability of UML-based technology has the potential to become one of the high confidence developing trends.", "num_citations": "1\n", "authors": ["87"]}
{"title": "Research and implementation of real-time features based on aspects\n", "abstract": " For the characterof real-time features crosscut the system, span objects and modules, and blend with core functional modules in the real-time system. Designing and maintaining such systems is full of risks and difficulties and traditional software developing methods couldn\u2019t solve the problems well. Making use of the ability of aspect-oriented techniques can encapsulate crosscutting features into compactunits, the character of real-time features is analyzed and how the real-time features are encapsulated to aspect is researched. The corresponding frames and templates are illustrated for different kinds of real-time features. The work is foundation of further aspect generation, for real-time features, from design models.", "num_citations": "1\n", "authors": ["87"]}
{"title": "An Integrated Environment for Software Reliability Engineering [J]\n", "abstract": " This paper presents our achievements in the key techniques of software reliability and an integrated environment SREE, aiming at solving the principal problems in the practice of software reliability engineering. Based on Eclipse, the system supports the key activities in software reliability engineering: usage model generation and construction, software reliability testing, software reliability estimation and prediction, and reliability tracing and analysis in the software development process.", "num_citations": "1\n", "authors": ["87"]}
{"title": "A Web-Based Platform CLP for Collaborative Learning\n", "abstract": " IT companies pay a lot of attentions to experience than before. Different from traditional education, IT training emphasizes the ability of problem-solving, design and project activities rather than concept knowledge. Accommodating themselves to new conditions, education organizations turn to the methodology of collaborative learning. In the paper, a self-controlled social-similar web-based platform CLP is provided for the training of persons satisfying the requirements from information industry.", "num_citations": "1\n", "authors": ["87"]}
{"title": "Engineering Framework and Evaluation System Implementationof Software Reliability [J]\n", "abstract": " A software reliability engineering framework is suggested in this paper. It normalizes the process of software reliability evaluation. We also give the architecture of software reliability evaluation system and related technologies. We have implemented a software reliability evaluation system, CaSoR (Computer aid Software of Reliability). It is used in practice.", "num_citations": "1\n", "authors": ["87"]}