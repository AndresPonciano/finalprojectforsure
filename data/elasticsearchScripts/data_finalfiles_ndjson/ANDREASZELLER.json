{"title": "Simplifying and isolating failure-inducing input\n", "abstract": " Given some test case, a program fails. Which circumstances of the test case are responsible for the particular failure? The delta debugging algorithm generalizes and simplifies the failing test case to a minimal test case that still produces the failure. It also isolates the difference between a passing and a failing test case. In a case study, the Mozilla Web browser crashed after 95 user actions. Our prototype implementation automatically simplified the input to three relevant user actions. Likewise, it simplified 896 lines of HTML to the single line that caused the failure. The case study required 139 automated test runs or 35 minutes on a 500 MHz PC.", "num_citations": "1087\n", "authors": ["255"]}
{"title": "Locating causes of program failures\n", "abstract": " Which is the defect that causes a software failure? By comparing the program states of a failing and a passing run, we can identify the state differences that cause the failure. However, these state differences can occur all over the program run. Therefore, we focus in space on those variables and values that are relevant for the failure, and in time on those moments where cause transitions occur - moments where new relevant variables begin being failure causes: \"Initially, variable argc was 3; therefore, at shell-sort(), variable a[2] was 0, and therefore, the program failed.\" In our evaluation, cause transitions locate the failure-inducing defect twice as well as the best methods known so far.", "num_citations": "808\n", "authors": ["255"]}
{"title": "Isolating cause-effect chains from computer programs\n", "abstract": " Consider the execution of a failing program as a sequence of program states. Each state induces the following state, up to the failure. Which variables and values of a program state are relevant for the failure? We show how the Delta Debugging algorithm isolates the relevant variables and values by systematically narrowing the state difference between a passing run and a failing run--by assessing the outcome of altered executions to determine wether a change in the program state makes a difference in the test outcome. Applying Delta Debugging to multiple states of the program automatically reveals the cause-effect chain of the failure--that is, the variables and values that caused the failure.In a case study, our prototype implementation successfully isolated the cause-effect chain for a failure of the GNU C compiler: \"Initially, the C program to be compiled contained an addition of 1.0; this caused an addition\u00a0\u2026", "num_citations": "781\n", "authors": ["255"]}
{"title": "Why programs fail: a guide to systematic debugging\n", "abstract": " Why Programs Fail: A Guide to Systematic Debugging is proof that debugging has graduated from a black art to a systematic discipline. It demystifies one of the toughest aspects of software programming, showing clearly how to discover what caused software failures, and fix them with minimal muss and fuss. The fully updated second edition includes 100+ pages of new material, including new chapters on Verifying Code, Predicting Erors, and Preventing Errors. Cutting-edge tools such as FindBUGS and AGITAR are explained, techniques from integrated environments like Jazz. net are highlighted, and all-new demos with ESC/Java and Spec#, Eclipse and Mozilla are included. This complete and pragmatic overview of debugging is authored by Andreas Zeller, the talented researcher who developed the GNU Data Display Debugger (DDD), a tool that over 250,000 professionals use to visualize the data structures of programs while they are running. Unlike other books on debugging, Zeller's text is product agnostic, appropriate for all programming languages and skill levels. The book explains best practices ranging from systematically tracking error reports, to observing symptoms, reproducing errors, and correcting defects. It covers a wide range of tools and techniques from hands-on observation to fully automated diagnoses, and also explores the author's innovative techniques for isolating minimal input to reproduce an error and for tracking cause and effect through a program. It even includes instructions on how to create automated debugging tools. The text includes exercises and extensive references for further study, and a companion website\u00a0\u2026", "num_citations": "779\n", "authors": ["255"]}
{"title": "Yesterday, my program worked. Today, it does not. Why?\n", "abstract": " Imagine some program and a number of changes. If none of these changes is applied (\u201cyesterday\u201d), the program works. If all changes are applied (\u201ctoday\u201d), the program does not work. Which change is responsible for the failure? We present an efficient algorithm that determines the minimal set of failure-inducing changes. Our delta debugging prototype tracked down a single failure-inducing change from 178,000 changed GDB lines within a few hours.", "num_citations": "544\n", "authors": ["255"]}
{"title": "Checking app behavior against app descriptions\n", "abstract": " How do we know a program does what it claims to do? After clustering Android apps by their description topics, we identify outliers in each cluster with respect to their API usage. A\" weather\" app that sends messages thus becomes an anomaly; likewise, a\" messaging\" app would typically not be expected to access the current location. Applied on a set of 22,500+ Android applications, our CHABADA prototype identified several anomalies; additionally, it flagged 56% of novel malware as such, without requiring any known malware patterns.", "num_citations": "470\n", "authors": ["255"]}
{"title": "Detecting object usage anomalies\n", "abstract": " Interacting with objects often requires following a protocol---for instance, a specific sequence of method calls. These protocols are not always documented, and violations can lead to subtle problems. Our approach takes code examples to automatically infer legal sequences of method calls. The resulting patterns can then be used to detect anomalies such as\" Before calling next, one normally calls hasNext\". To our knowledge, this is the first fully automatic defect detection approach that learns and checks methodcall sequences. Our JADET prototype has detected yet undiscovered defects and code smells in five popular open-source programs, including two new defects in AspectJ.", "num_citations": "309\n", "authors": ["255"]}
{"title": "Fuzzing with code fragments\n", "abstract": " Fuzz testing is an automated technique providing random data as input to a software system in the hope to expose a vulnerability. In order to be effective, the fuzzed input must be common enough to pass elementary consistency checks; a JavaScript interpreter, for instance, would only accept a semantically valid program. On the other hand, the fuzzed input must be uncommon enough to trigger exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using a grammar to randomly generate valid programs; the code fragments, however, partially stem from programs known to have caused invalid behavior before. LangFuzz is an effective tool for security testing: Applied on the Mozilla JavaScript interpreter, it discovered a total of 105 new severe vulnerabilities within three months of operation (and thus became one of the top security bug bounty collectors within this period); applied on the PHP interpreter, it discovered 18 new defects causing crashes.", "num_citations": "294\n", "authors": ["255"]}
{"title": "Lightweight defect localization for Java\n", "abstract": " A common method to localize defects is to compare the coverage of passing and failing program runs: A method executed only in failing runs, for instance, is likely to point to the defect. However, some failures, occur only after a specific sequence of method calls, such as multiple deallocations of the same resource. Such sequences can be collected from arbitrary Java programs at low cost; comparing object-specific sequences predicts defects better than simply comparing coverage. In a controlled experiment, our technique pinpointed the defective class in 39% of all test runs.", "num_citations": "283\n", "authors": ["255"]}
{"title": "The impact of tangled code changes\n", "abstract": " When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found up to 15% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6% of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.", "num_citations": "278\n", "authors": ["255"]}
{"title": "Mining object behavior with ADABU\n", "abstract": " To learn what constitutes correct program behavior, one can start with normal behavior. We observe actual program executions to construct state machines that summarize object behavior. These state machines, called object behavior models, capture the relationships between two kinds of methods: mutators that change the state (such as add ()) and inspectors that keep the state unchanged (such as isEmpty ()):\" A Vector object initially is in isEmpty () state; after add (), it goes into\u00ac isEmpty () state\". Our ADABU prototype for JAVA has successfully mined models of undocumented behavior from the AspectJ compiler and the Columba email client; the models tend to be small and easily understandable.", "num_citations": "243\n", "authors": ["255"]}
{"title": "Javalanche: Efficient mutation testing for Java\n", "abstract": " To assess the quality of a test suite, one can use mutation testing-seeding artificial defects (mutations) into the program and checking whether the test suite finds them. Javalanche is an open source framework for mutation testing Java programs with a special focus on automation, efficiency, and effectiveness. In particular, Javalanche assesses the impact of individual mutations to effectively weed out equivalent mutants; it has been demonstrated to work on programs with up to 100,000 lines of code.", "num_citations": "224\n", "authors": ["255"]}
{"title": "The impact of equivalent mutants\n", "abstract": " If a mutation is not killed by a test suite, this usually means that the test suite is not adequate. However, it may also be that the mutant keeps the programpsilas semantics unchanged-and thus cannot be detected by any test.We found such equivalent mutants to be surprisingly common: In an experiment on the JAXEN XPATH query engine, 8/20 = 40% of all mutations turned out to be equivalent. Worse, checking the equivalency took us 15 minutes for a single mutation. Equivalent mutants thus make it impossible to automatically assess test suites by means of mutation testing. To identify equivalent mutants, we are currently investigating the impact of a mutation on the execution: the more a mutation alters the execution, the higher the chance of it being non-equivalent. First experiments assessing the impact on code coverage are promising.", "num_citations": "199\n", "authors": ["255"]}
{"title": "Efficient mutation testing by checking invariant violations\n", "abstract": " Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a mutation is not detected by the test suite, this usually means that the test suite is not adequate. However, it may also be that the mutant keeps the program's semantics unchanged-and thus cannot be detected by any test. Such equivalent mutants have to be eliminated manually, which is tedious.", "num_citations": "192\n", "authors": ["255"]}
{"title": "Generating test cases for specification mining\n", "abstract": " Dynamic specification mining observes program executions to infer models of normal program behavior. What makes us believe that we have seen sufficiently many executions? The typestate miner generates test cases that cover previously unobserved behavior, systematically extending the execution space and enriching the specification. To our knowledge, this is the first combination of systematic test case generation and typestate mining--a combination with clear benefits: On a sample of 800 defects seeded into six Java subjects, a static typestate verifier fed with enriched models would report significantly more true positives, and significantly fewer false positives than the initial models.", "num_citations": "177\n", "authors": ["255"]}
{"title": "DDD\u2014a free graphical front-end for UNIX debuggers\n", "abstract": " The Data Display Debugger (DDD) is a novel graphical user interface to GDB and DBX, the popular UNIX debuggers. Besides \"usual\" features such as viewing source texts and breakpoints, DDD provides a graphical data display, where data structures are displayed as graphs. A simple mouse click dereferences pointers or reveals structure contents. Complex data structures can be explored incrementally and interactively, using automatic layout if preferred. Each time the program stops, the data display reflects the current variable values. DDD has been designed to compete with well-known commercial debuggers; however, it is free software, protected by the GNU general public license. In this paper, we give a quick presentation of DDD and describe its architecture and basic functionality from a technical point of view.", "num_citations": "169\n", "authors": ["255"]}
{"title": "Mining temporal specifications from object usage\n", "abstract": " A caller must satisfy the callee\u2019s precondition\u2014that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Fair Computation Tree Logic (CTL                                            F                                          ) formulas that describe the operations a parameter goes through: \u201cIn parseProperties(String xml), the parameter xml normally stems from getProperties().\u201d Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 169 violations of operational preconditions, uncovering 7 unique defects and 27 unique code smells\u2014with 52% true positives in the 25% top-ranked violations.", "num_citations": "153\n", "authors": ["255"]}
{"title": "Unified versioning through feature logic\n", "abstract": " Software configuration management (SCM) suffers from tight coupling between SCM version-ing models and the imposed SCM processes. In order to adapt SCM tools to SCM processes, rather than vice versa, we propose a unified versioning model, the version set model. Version sets denote versions, components, and configurations by feature terms, that is, Boolean terms over (feature : value)-attributions. Through feature logic, we deduce consistency of abstract configurations as well as features of derived components and describe how features propagate in the SCM process; using feature implications, we integrate change-oriented and version-oriented SCM models. We have implemented the version set model in an SCM system called ICE, for Incremental Configuration Environment. ICE is based on a featured file system  (FFS), where version sets are accessed as virtual files and directories. Using the well\u00a0\u2026", "num_citations": "139\n", "authors": ["255"]}
{"title": "Mining trends of library usage\n", "abstract": " A library is available in multiple versions. Which one should I use? Has it been widely adopted already? Was it a good decision to switch to the newest version? We have mined hundreds of open-source projects for their library dependencies, and determined global trends in library usage. This wisdom of the crowds can be helpful for developers when deciding when to use which version of a library-by helping them avoid pitfalls experienced by other developers, and by showing important emerging trends in library usage.", "num_citations": "122\n", "authors": ["255"]}
{"title": "Learning from 6,000 projects: lightweight cross-project anomaly detection\n", "abstract": " Real production code contains lots of knowledge-on the domain, on the architecture, and on the environment. How can we leverage this knowledge in new projects? Using a novel lightweight source code parser, we have mined more than 6,000 open source Linux projects (totaling 200,000,000 lines of code) to obtain 16,000,000 temporal properties reflecting normal interface usage. New projects can be checked against these rules to detect anomalies-that is, code that deviates from the wisdom of the crowds. In a sample of 20 projects,~ 25% of the top-ranked anomalies uncovered actual code smells or defects.", "num_citations": "106\n", "authors": ["255"]}
{"title": "Mining input grammars from dynamic taints\n", "abstract": " Knowing which part of a program processes which parts of an input can reveal the structure of the input as well as the structure of the program. In a URL http://www.example.com/path/, for instance, the protocol http, the host www.example.com, and the path path would be handled by different functions and stored in different variables. Given a set of sample inputs, we use dynamic tainting to trace the data flow of each input character, and aggregate those input fragments that would be handled by the same function into lexical and syntactical entities. The result is a context-free grammar that reflects valid input structure. In its evaluation, our AUTOGRAM prototype automatically produced readable and structurally accurate grammars for inputs like URLs, spreadsheets or configuration files. The resulting grammars not only allow simple reverse engineering of input formats, but can also directly serve as input for test\u00a0\u2026", "num_citations": "103\n", "authors": ["255"]}
{"title": "Covering and uncovering equivalent mutants\n", "abstract": " Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a test suite fails to detect a mutation, it may also fail to detect real defects\u2014and hence should be improved. However, there are also mutations that keep the program semantics unchanged and thus cannot be detected by any test suite. Such equivalent mutants must be weeded out manually, which is a tedious task. In this paper, we examine whether changes in coverage can be used to detect non\u2010equivalent mutants: If a mutant changes the coverage of a run, it is more likely to be non\u2010equivalent. In a sample of 140 manually classified mutations of seven Java programs with 5000 to 100\u2009000 lines of code, we found that (i) the problem is serious and widespread\u2014about 45% of all undetected mutants turned out to be equivalent; (ii) manual classification takes time\u2014about 15\u2009min per mutation; (iii) coverage\u00a0\u2026", "num_citations": "101\n", "authors": ["255"]}
{"title": "Making students read and review code\n", "abstract": " The Praktomat system allows students to read, review, and assess each other's programs in order to improve quality and style. After a successful submission, the student can retrieve and review a program of some fellow student selected by Praktomat. After the review is complete, the student may obtain reviews and re-submit improved versions of his program. The reviewing process is independent of grading; the risk of plagiarism is narrowed by personalized assignments and automatic testing of submitted programs. In a survey, more than two thirds of the students affirmed that reading each other's programs improved their program quality; this is also confirmed by statistical data.", "num_citations": "101\n", "authors": ["255"]}
{"title": "Lightweight bug localization with AMPLE\n", "abstract": " AMPLE locates likely failure-causing classes by comparing method call sequences of passing and failing runs. A difference in method call sequences, such as multiple deallocation of the same resource, is likely to point to the erroneous class. Such sequences can be collected from arbitrary Java programs at low cost; comparing object-specific sequences predicts defects better than simply comparing coverage. AMPLE comes as a plug-in for the Java IDE Eclipse that is automatically invoked as soon as a JUnit test fails.", "num_citations": "99\n", "authors": ["255"]}
{"title": "Simplifying failure-inducing input\n", "abstract": " Given some test case, a program fails. Which part of the test case is responsible for the particular failure? We show how our delta debugging algorithm generalizes and simplifies some failing input to a minimal test case that produces the failure.", "num_citations": "97\n", "authors": ["255"]}
{"title": "Mining API popularity\n", "abstract": " When designing a piece of software, one frequently must choose between multiple external libraries that provide similar services. Which library is the best one to use? We mined hundreds of open source projects and their external dependencies in order to observe the popularity of their APIs and to give recommendations of the kind: \u201cProjects are moving away from this API element. Consider a change.\u201d Such wisdom of the crowds can provide valuable information to both the API users and the API producers.", "num_citations": "79\n", "authors": ["255"]}
{"title": "(Un-) covering equivalent mutants\n", "abstract": " Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a test suite fails to detect a mutation, it may also fail to detect real defects-and hence should be improved. However, there also are mutations which keep the program semantics unchanged and thus cannot be detected by any test suite. Such equivalent mutants must be weeded out manually, which is a tedious task. In this paper, we examine whether changes in coverage can be used to detect non-equivalent mutants: If a mutant changes the coverage of a run, it is more likely to be non-equivalent. In a sample of 140 manually classified mutations of seven Java programs with 5,000 to 100,000 lines of code, we found that: (a) the problem is serious and widespread-about 45% of all undetected mutants turned out to be equivalent; (b) manual classification takes time-about 15 minutes per mutation; (c) coverage\u00a0\u2026", "num_citations": "79\n", "authors": ["255"]}
{"title": "Minimizing reproduction of software failures\n", "abstract": " A program fails. What now? Taking a single failing run, we record and minimize the interaction between objects to the set of calls relevant for the failure. The result is a minimal unit test that faithfully reproduces the failure at will:\" Out of these 14,628 calls, only 2 are required\". In a study of 17 real-life bugs, our JINSI prototype reduced the search space to 13.7% of the dynamic slice or 0.22% of the source code, with only 1--12 calls left to examine.", "num_citations": "75\n", "authors": ["255"]}
{"title": "Automated debugging: Are we close?\n", "abstract": " Although software engineers have enjoyed tremendous productivity increases as more of their tasks have become automated, debugging remains as labor-intensive and painful as it. was 50 years ago. An engineer or programmer must still set up hypotheses to use in identifying and correcting a failure's root cause. The author describes a new algorithm that promises to relieve programmers of the hit-or-miss approach to debugging. Delta Debugging uses the results of automated testing to systematically narrow the set of failure-inducing circumstances. Programmers supply a test function for each bug and hardcode it into any imperative language. The test function checks a set of changes to determine if the failure is present or if the outcome is unresolved, then feeds that information to the Delta Debugging code. As we discover more about the structure of these circumstances and the resulting causality chain, we\u00a0\u2026", "num_citations": "73\n", "authors": ["255"]}
{"title": "Assessing oracle quality with checked coverage\n", "abstract": " A known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality - and even more sensitive than mutation testing, its much more demanding alternative.", "num_citations": "71\n", "authors": ["255"]}
{"title": "Where is the bug and how is it fixed? an experiment with practitioners\n", "abstract": " Research has produced many approaches to automatically locate, explain, and repair software bugs. But do these approaches relate to the way practitioners actually locate, understand, and fix bugs? To help answer this question, we have collected a dataset named DBGBENCH---the correct fault locations, bug diagnoses, and software patches of 27 real errors in open-source C projects that were consolidated from hundreds of debugging sessions of professional software engineers. Moreover, we shed light on the entire debugging process, from constructing a hypothesis to submitting a patch, and how debugging time, difficulty, and strategies vary across practitioners and types of errors. Most notably, DBGBENCH can serve as reality check for novel automated debugging and repair techniques.", "num_citations": "65\n", "authors": ["255"]}
{"title": "Finding failure causes through automated testing\n", "abstract": " A program fails. Under which circumstances does this failure occur? One single algorithm, the delta debugging algorithm, suffices to determine these failure-inducing circumstances. Delta debugging tests a program systematically and automatically to isolate failure-inducing circumstances such as the program input, changes to the program code, or executed statements.", "num_citations": "65\n", "authors": ["255"]}
{"title": "Profiling java programs for parallelism\n", "abstract": " One of the biggest challenges imposed by multi-core architectures is how to exploit their potential for legacy systems not built with multiple cores in mind. By analyzing dynamic data dependences of a program run, one can identify independent computation paths that could have been handled by individual cores. Our prototype computes dynamic dependences for Java programs and recommends locations to the programmer with the highest potential for parallelization. Such measurements can also provide starting points for automatic, speculative parallelization.", "num_citations": "64\n", "authors": ["255"]}
{"title": "Mining behavior models from enterprise web applications\n", "abstract": " Today's enterprise web applications demand very high release cycles---and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. As specifying such behavior models and writing the necessary scripts manually is a hard task, a possible alternative could be to extract them from existing applications. However, mining such models can be a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present ProCrawl (PROcess CRAWLer), a generic approach to mine behavior models from (multi-user) enterprise web applications. ProCrawl observes the behavior of the application through its user interface\u00a0\u2026", "num_citations": "61\n", "authors": ["255"]}
{"title": "Webmate: a tool for testing web 2.0 applications\n", "abstract": " Quality assurance of Web applications is a challenge, due to the large number and variance of involved components. In particular, rich Web 2.0 applications based on JavaScript pose new challenges for testing, as a simple crawling through links covers only a small part of the functionality. The WEBMATE approach automatically explores and navigates through arbitrary Web 2.0 applications. WEBMATE addresses challenges such as interactive elements, state abstraction, and non-determinism in large applications; we demonstrate its usage for regular application testing as well as for cross-browser testing.", "num_citations": "58\n", "authors": ["255"]}
{"title": "Practical test dependency detection\n", "abstract": " Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and\u00a0\u2026", "num_citations": "56\n", "authors": ["255"]}
{"title": "Mining sandboxes\n", "abstract": " We present sandbox mining, a technique to confine an application to resources accessed during automatic testing. Sandbox mining first explores software behavior by means of automatic test generation, and extracts the set of resources accessed during these tests. This set is then used as a sandbox, blocking access to resources not used during testing. The mined sandbox thus protects against behavior changes such as the activation of latent malware, infections, targeted attacks, or malicious updates.", "num_citations": "47\n", "authors": ["255"]}
{"title": "WebMate: Generating test cases for web 2.0\n", "abstract": " Web applications are everywhere\u2014well tested web applications however are in short supply. The mixture of JavaScript, HTML and CSS in a variety of different browsers makes it virtually impossible to apply static analysis techniques. In this setting, systematic testing becomes a real challenge. We present a technique to automatically generate tests for Web\u00a02.0 applications. Our approach systematically explores and tests all distinct functions of a web application. Our prototype implementation WEBMATE handles interfaces as complex as Facebook and is able to cover up to 7 times as much code as existing tools. The only requirements to use WEBMATE are the address of the application and, if necessary, user name and password.", "num_citations": "46\n", "authors": ["255"]}
{"title": "The future of programming environments: Integration, synergy, and assistance\n", "abstract": " Modern programming environments foster the integration of automated, extensible, and reusable tools. New tools can thus leverage the available functionality and collect data from program and process. The synergy of both will allow the automation of current empirical approaches. This leads to automated assistance in all development decisions for programmers and managers alike: \"For this task, you should collaborate with Joe, because it will likely require risky work on the mailbox class\".", "num_citations": "39\n", "authors": ["255"]}
{"title": "Isolating relevant component interactions with JINSI\n", "abstract": " When a component in a large system fails, developers encounter two problems:(1) reproducing the failure, and (2) investigating the causes of such a failure. Our JINSI tool lets developers capture and replay the interactions between a component and its environment, thus allowing for reproducing the failure at will. In addition, JINSI uses delta debugging to automatically isolate the subset of the interactions that is relevant for the failure. In a first study, JINSI has successfully isolated the relevant interaction of a JAVA component:\" Out of the 32 interactions with the< VendingMachine>(BOB-wasn't sure about this one) component, seven interactions suffice to produce the failure.", "num_citations": "38\n", "authors": ["255"]}
{"title": "Handling version sets through feature logic\n", "abstract": " Software Configuration Management suffers from a multitude of models for version identification and control. We propose a unified approach based on feature logic. Using feature logic, version sets are the basic units of reasoning, making attribution models and version repositories special cases of a more general scheme. Version sets are identified by feature terms, that is, a boolean expression over (name: value)-features. A system is configured by incrementally narrowing the set of versions until each component is contained in one single version. Feature logic ensures early detection of inconsistencies as well as automatic completion of the configuration thread. We have implemented a tool called ICE, realizing the above approach. As ICE uses the common C preprocessor (CPP) representation, one can select, add, or modify arbitrary version subsets as user-readable entities. ICE deduces features and\u00a0\u2026", "num_citations": "38\n", "authors": ["255"]}
{"title": "Detecting behavior anomalies in graphical user interfaces\n", "abstract": " When interacting with user interfaces, do users always get what they expect? For each user interface element in thousands of Android apps, we extracted the Android APIs they invoke as well as the text shown on their screen. This association allows us to detect outliers: User interface elements whose text, context or icon suggests one action, but which actually are tied to other actions. In our evaluation of tens of thousands of UI elements, our BACKSTAGE prototype discovered misleading random UI elements with an accuracy of 73%.", "num_citations": "37\n", "authors": ["255"]}
{"title": "A unified version model for configuration management\n", "abstract": " Integration of configuration management (CM) tools into software development environments raises the need for CM models to interoperate through a unified CM model. A possible foundation is the version set model, a unified model for specifying versions and version operations, where versions, components, and aggregates are grouped into setsaccording to their features, using $ eature logic asa formal base to denote sets and operations and deduce consistency. Version setsgeneralize well-known CM concepts such as components, repositories, workspaces, aggregates, or configurations and allow for unprecedented flexibility in combining these concepts, Arbitrary revisionhwiant combinations of components and aggregates are modeled in a uniform and orthogonal way. We show how the concepts of four central configuration management models\u2014the checlchdcheckout model, the change set model, the\u00a0\u2026", "num_citations": "36\n", "authors": ["255"]}
{"title": "Droidmate: a robust and extensible test generator for android\n", "abstract": " droidmate is a fully automated gui execution generator for Android apps. droidmateexplores an app, ie (a) repeatedly reads at runtime the device gui and monitored calls to Android apis methods and (b) makes a decision what next gui action (click, long-click, text entry, etc.) to execute, based on that data and provided exploration strategy. The process continues until some termination criterion is met. droidmate is (1) fully automatic: after it has been set up, the exploration itself does not require human presence;(2) extensible: without recompiling droidmate, anybody can run it with their own exploration strategy, termination criterion, or a set of monitored methods;(3) robust: tested on 126 apps being in the top 5 in all Google Play categories except Games, it ran successfully on 123 of them;(4) easy to set up: it works on Android devices and emulators out-of-the-box, without root or OS modifications; and (5) easy to modify\u00a0\u2026", "num_citations": "34\n", "authors": ["255"]}
{"title": "Search-based security testing of web applications\n", "abstract": " SQL injections are still the most exploited web application vulnerabilities. We present a technique to automatically detect such vulnerabilities through targeted test generation. Our approach uses search-based testing to systematically evolve inputs to maximize their potential to expose vulnerabilities. Starting from an entry URL, our BIOFUZZ prototype systematically crawls a web application and generates inputs whose effects on the SQL interaction are assessed at the interface between Web server and database. By evolving those inputs whose resulting SQL interactions show best potential, BIOFUZZ exposes vulnerabilities on real-world Web applications within minutes. As a black-box approach, BIOFUZZ requires neither analysis nor instrumentation of server code; however, it even outperforms state-of-the-art white-box vulnerability scanners.", "num_citations": "33\n", "authors": ["255"]}
{"title": "What is the long-term impact of changes?\n", "abstract": " During their life cycle, programs undergo many changes. Each of these changes may introduce new features---or new problems. While most of the impact of a change is immediate, some of the impact may become evident only in the long term. For instance, suppose we make the internals of a component accessible to its clients. In itself, this does not introduce a problem. In the long term, though, this will most likely lead to maintainability issues.", "num_citations": "30\n", "authors": ["255"]}
{"title": "Mining cause-effect-chains from version histories\n", "abstract": " Software reliability is heavily impacted by soft ware changes. How do these changes relate to each other? By analyzing the impacted method definitions and usages, we determine dependencies between changes, resulting in a change genealogy that captures how earlier changes enable and cause later ones. Model checking this genealogy reveals temporal process patterns that encode key features of the software process such as pending development activities: \"Whenever class A is changed, its test case is later updated as well.\" Such patterns can be validated automatically: In an evaluation of four open source histories, our prototype would recommend pending activities with a precision of 60-72%.", "num_citations": "29\n", "authors": ["255"]}
{"title": "Sambamba: A Runtime System for Online Adaptive Parallelization\n", "abstract": " How can we exploit a microprocessor as efficiently as possible? The \u201cclassic\u201d approach is static optimization at compile-time, optimizing a program for all possible uses. Further optimization can only be achieved by anticipating the actual usage profile: If we know, for instance, that two computations will be independent, we can run them in parallel. In the Sambamba project, we replace anticipation by adaptation. Our runtime system provides the infrastructure for implementing runtime adaptive and speculative transformations. We demonstrate our framework in the context of adaptive parallelization. We show the fully automatic parallelization of a small irregular C program in combination with our adaptive runtime system. The result is a parallel execution which adapts to the availability of idle system resources. In our example, this enables a 1.92 fold speedup on two cores while still preventing oversubscription\u00a0\u2026", "num_citations": "28\n", "authors": ["255"]}
{"title": "Quantifying the information leakage in cache attacks via symbolic execution\n", "abstract": " Cache attacks allow attackers to infer the properties of a secret execution by observing cache hits and misses. But how much information can actually leak through such attacks? For a given program, a cache model, and an input, our CHALICE framework leverages symbolic execution to compute the amount of information that can possibly leak through cache attacks. At the core of CHALICE is a novel approach to quantify information leakage that can highlight critical cache side-channel leakage on arbitrary binary code. In our evaluation on real-world programs from OpenSSL and Linux GDK libraries, CHALICE effectively quantifies information leakage: For an AES-128 implementation on Linux, for instance, CHALICE finds that a cache attack can leak as much as 127 out of 128 bits of the encryption key.", "num_citations": "27\n", "authors": ["255"]}
{"title": "Program analysis: A hierarchy\n", "abstract": " Program analysis tools are based on four reasoning techniques:(1) deduction from code to concrete runs,(2) observation of concrete runs,(3) induction from observations into abstractions, and (4) experimentation to find causes for specific effects. These techniques form a hierarchy, where each technique can make use of lower levels, and where each technique induces capabilities and limits of the associated tools.", "num_citations": "27\n", "authors": ["255"]}
{"title": "Parser-directed fuzzing\n", "abstract": " To be effective, software test generation needs to well cover the space of possible inputs. Traditional fuzzing generates large numbers of random inputs, which however are unlikely to contain keywords and other specific inputs of non-trivial input languages. Constraint-based test generation solves conditions of paths leading to uncovered code, but fails on programs with complex input conditions because of path explosion. In this paper, we present a test generation technique specifically directed at input parsers. We systematically produce inputs for the parser and track comparisons made; after every rejection, we satisfy the comparisons leading to rejection. This approach effectively covers the input space: Evaluated on five subjects, from CSV files to JavaScript, our pFuzzer prototype covers more tokens than both random-based and constraint-based approaches, while requiring no symbolic analysis and far fewer\u00a0\u2026", "num_citations": "26\n", "authors": ["255"]}
{"title": "Checked coverage: an indicator for oracle quality\n", "abstract": " A known problem of traditional coverage metrics is that they do not assess oracle quality\u2014that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage\u2014the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open\u2010source projects show that checked coverage is a sure indicator for oracle quality and even more sensitive than mutation testing. Copyright \u00a9 2013 John Wiley & Sons, Ltd.", "num_citations": "26\n", "authors": ["255"]}
{"title": "Localizing bugs in program executions with graphical models\n", "abstract": " We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the Bernoulli graph model on data of the software projects AspectJ and Rhino.", "num_citations": "24\n", "authors": ["255"]}
{"title": "WebMate: Web application test generation in the real world\n", "abstract": " We present Web Mate, a tool for automatically generating test cases for Web applications. Given only the URL of the starting page, Web Mate automatically explores the functionality of a Web application, detecting differences across multiple browsers or operating systems, as well as across different revisions of the same Web application. Web Mate can handle full Web 2.0 functionality and explore sites as complex as Facebook. In addition to autonomously exploring the application, Web Mate can also leverage existing written or recorded test cases, and use these as an exploration base, this combination allows for quick expansion of the existing test base. Originating from research in generating test cases for specification mining, Web Mate is now the core product of a startup specializing in automated Web testing - a transfer that took us two years to complete. We report central lessons learned from this transfer\u00a0\u2026", "num_citations": "23\n", "authors": ["255"]}
{"title": "Mining evolution of object usage\n", "abstract": " As software evolves, so does the interaction between its components. But how can we check if components are updated consistently? By abstracting object usage into temporal properties, we can learn evolution                 patterns that express how object usage evolves over time. Software can then be checked against these patterns, revealing code that is in need of update: \u201cYour check for isValidWidget() is now superseded by checkWidget().\u201d In an evaluation of seven different versions of three open source projects, our LAMARCK tool was able to detect existing code issues with a precision of 33%\u201364% and to prevent such issues with a precision of 90%\u2013100%.", "num_citations": "23\n", "authors": ["255"]}
{"title": "Mining input grammars with AUTOGRAM\n", "abstract": " Knowledge about how a program processes its inputs can help to understand the structure of the input as well as the structure of the program. In a JSON value like [1, true, \"Alice\"], for instance the integer value 1, the boolean value true and the string value \"Alice\" would be handled by different functions or stored in different variables. Our AUTOGRAM tool uses dynamic tainting to trace the data flow of each input character for a set of sample inputs and identifies syntactical entities by grouping input fragments that are handled by the same functions. The resulting context-free grammar reflects the structure of valid inputs and can be used for reverse engineering of formats and can serve as direct input for test generators. A video demonstrating AUTOGRAM is available at https://youtu.be/Iqym60iWBBk.", "num_citations": "22\n", "authors": ["255"]}
{"title": "Detecting information flow by mutating input data\n", "abstract": " Analyzing information flow is central in assessing the security of applications. However, static and dynamic analyses of information flow are easily challenged by non-available or obscure code. We present a lightweight mutation-based analysis that systematically mutates dynamic values returned by sensitive sources to assess whether the mutation changes the values passed to sensitive sinks. If so, we found a flow between source and sink. In contrast to existing techniques, mutation-based flow analysis does not attempt to identify the specific path of the flow and is thus resilient to obfuscation. In its evaluation, our MUTAFLOW prototype for Android programs showed that mutation-based flow analysis is a lightweight yet effective complement to existing tools. Compared to the popular FlowDroid static analysis tool, MutaFlow requires less than 10% of source code lines but has similar accuracy; on 20 tested real-world\u00a0\u2026", "num_citations": "21\n", "authors": ["255"]}
{"title": "Untangling changes\n", "abstract": " When developers commit software changes to a version control system, they often commit unrelated changes in a single transaction\u2014simply because, while, say, fixing a bug in module A, they also came across a typo in module B, and updated a deprecated call in module C. When analyzing such archives later, the changes to A, B, and C are treated as being falsely related. In an evaluation of five Java projects, we found up to 15% of all fixes to consist of multiple unrelated changes, compromising the resulting analyses through noise and bias. We present the first approach to untangle such combined changes after the fact. By taking into account data dependencies, distance measures, change couplings, test impact couplings, and distances in call graphs, our approach is able to untangle tangled changes with a mean success rate of 63\u201375%. Our recommendation is that such untangling be considered as a mandatory step in mining software archives.", "num_citations": "20\n", "authors": ["255"]}
{"title": "Software configuration management: State of the art, state of the practice\n", "abstract": " Which are the open problems in Software Configuration Management SCM? The purpose of this paper is to ignite a discussion on current and future SCM directions. Based on the findings of a Dagstuhl Seminar on the current state of Software Engineering, we assess the state of SCM with the goal to identity effective SCM tasks and solutions, to establish a core body of SCM knowledge, and to denote remaining real-world SCM problems.", "num_citations": "20\n", "authors": ["255"]}
{"title": "XMLMate: Evolutionary XML test generation\n", "abstract": " Generating system inputs satisfying complex constraints is still a challenge for modern test generators. We present XMLMATE, a search-based test generator specially aimed at XML-based systems. XMLMATE leverages program structure, existing XML schemas, and XML inputs to generate, mutate, recombine, and evolve valid XML inputs. Over a set of seven XML-based systems, XMLMATE detected 31 new unique failures in production code, all triggered by system inputs and thus true alarms.", "num_citations": "19\n", "authors": ["255"]}
{"title": "Breeding high-impact mutations\n", "abstract": " Mutation testing was developed to measure the adequacy of a test suite by seeding artificial bugs (mutations) into a program, and checking whether the test suite detects them. An undetected mutation either indicates a insufficiency in the test suite and provides means for improvement, or it is an equivalent mutation that cannot be detected because it does not change the program's semantics. Impact metrics-that quantify the difference between a run of the original and the mutated version of a program-are one way to detectnon-equivalent mutants. In this paper we present a genetic algorithm that aims to produce a set of mutations that have a high impact, are not detected by the test suite, and at the same time are well spread all over the code. We believe that such a set is useful for improving a test suite, as a high impact of a mutation implies it caused a grave damage, which is not detected by the test suite, and that the\u00a0\u2026", "num_citations": "19\n", "authors": ["255"]}
{"title": "Mining the Jazz repository: Challenges and opportunities\n", "abstract": " By integrating various development and collaboration tools into one single platform, the Jazz environment offers several opportunities for software repository miners. In particular, Jazz offers full traceability from the initial requirements via work packages and work assignments to the final changes and tests; all these features can be easily accessed and leveraged for better prediction and recommendation systems. In this paper, we share our initial experiences from mining the Jazz repository. We also give a short overview of the retrieved data sets and discuss possible problems of the Jazz repository and the platform itself.", "num_citations": "19\n", "authors": ["255"]}
{"title": "Visual Debugging with DDD.\n", "abstract": " Provides information on the use of DDD for visual debugging. Steps on how to visualize and use DDD; Graph structures of DDD; Details on the use of plots, a visualization method provided by DDD when dealing with numerical data; Weaknesses and limitations of the debugger.", "num_citations": "19\n", "authors": ["255"]}
{"title": "Guiding app testing with mined interaction models\n", "abstract": " Test generators for graphical user interfaces must constantly choose which UI element to interact with, and how. We guide this choice by mining associations between UI elements and their interactions from the most common applications. Once mined, the resulting UI interaction model can be easily applied to new apps and new test generators. In our experiments, the mined interaction models lead to code coverage improvements of 19.41% and 43.03% on average on two state-of-the-art tools (Droidmate and Droidbot), when executing the same number of actions.", "num_citations": "18\n", "authors": ["255"]}
{"title": "Mining workflow models from web applications\n", "abstract": " Modern business applications predominantly rely on web technology, enabling software vendors to efficiently provide them as a service, removing some of the complexity of the traditional release and update process. While this facilitates shorter, more efficient and frequent release cycles, it requires continuous testing. Having insight into application behavior through explicit models can largely support development, testing and maintenance. Model-based testing allows efficient test creation based on a description of the states the application can be in and the transitions between these states. As specifying behavior models that are precise enough to be executable by a test automation tool is a hard task, an alternative is to extract them from running applications. However, mining such models is a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We\u00a0\u2026", "num_citations": "18\n", "authors": ["255"]}
{"title": "Smooth operations with square operators\u2014The version set model in ICE\n", "abstract": " Implementing software configuration management (SCM) in an organization raises various integration problems. We present the Incremental Configuration Environment (ICE), a novel SCM system providing smooth integration with both the software process and the development environment. ICE is based on the version set model, where versions, components, and configurations are grouped into sets according to their features, using feature logic as a formal base to denote sets and operations and to deduce consistency. Version sets generalize well-known SCM concepts such as components, repositories, workspaces, or configurations and allow for flexibility in combining these concepts. For integration in software development environments, ICE provides a featured file system (FFS), where version sets are represented as files and directories. In the FFS, arbitrary programs can incrementally access and\u00a0\u2026", "num_citations": "18\n", "authors": ["255"]}
{"title": "Systematically covering input structure\n", "abstract": " Grammar-based testing uses a given grammar to produce syntactically valid inputs. To cover program features, it is necessary to also cover input features-say, all URL variants for a URL parser. Our k-path algorithm for grammar production systematically covers syntactic elements as well as their combinations. In our evaluation, we show that this results in a significantly higher code coverage than state of the art.", "num_citations": "17\n", "authors": ["255"]}
{"title": "Sambamba: runtime adaptive parallel execution\n", "abstract": " How can we exploit a microprocessor as efficiently as possible? The\" classic\" approach is static optimization at compile-time, conservatively optimizing a program while keeping all possible uses in mind. Further optimization can only be achieved by anticipating the actual usage profile: If we know, for instance, that two computations will be independent, we can run them in parallel. However, brute force parallelization may slow down execution due to its large overhead. But as this depends on runtime features, such as structure and size of input data, parallel execution needs to dynamically adapt to the runtime situation at hand.", "num_citations": "17\n", "authors": ["255"]}
{"title": "Configuration management with feature logics\n", "abstract": " Feature logics, when used in a software configuration management system, can be used to identify and select versions by their respective features, unifies variant and revision handling, helps detecting configuration conflicts and allows dealing with incomplete configuration specifications.In our model, components are tagged with feature terms, describing their features (or nonfeatures) and identifying both revisions and variants. Selection is done by specification and incremental refinement of the desired features. When a system is to be composed, feature unification infers the set of valid configurations. This set may be presented as an interactive control panel for selecting the final configuration. A prototype, using the C Preprocessor representation for smooth transition from \u201cclassical\u201d approaches, has been implemented as part of the NORA software development system.", "num_citations": "17\n", "authors": ["255"]}
{"title": "Learning user interface element interactions\n", "abstract": " When generating tests for graphical user interfaces, one central problem is to identify how individual UI elements can be interacted with\u2014clicking, long-or right-clicking, swiping, dragging, typing, or more. We present an approach based on reinforcement learning that automatically learns which interactions can be used for which elements, and uses this information to guide test generation. We model the problem as an instance of the multi-armed bandit problem (MAB problem) from probability theory, and show how its traditional solutions work on test generation, with and without relying on previous knowledge. The resulting guidance yields higher coverage. In our evaluation, our approach shows improvements in statement coverage between 18%(when not using any previous knowledge) and 20%(when reusing previously generated models).", "num_citations": "16\n", "authors": ["255"]}
{"title": "O! snap: Cost-efficient testing in the cloud\n", "abstract": " Porting a testing environment to a cloud infrastructure is not straightforward. This paper presents O!Snap, an approach to generate test plans to cost-efficiently execute tests in the cloud. O!Snap automatically maximizes reuse of existing virtual machines, and interleaves the creation of updated test images with the execution of tests to minimize overall test execution time and/or cost. In an evaluation involving 2,600+ packages and 24,900+ test jobs of the Debian continuous integration environment, O!Snap reduces test setup time by up to 88% and test execution time by up to 43.3% without additional costs.", "num_citations": "16\n", "authors": ["255"]}
{"title": "Debugging with DDD\n", "abstract": " Distributed by Free Software Foundation, Inc. 59 Temple Place\u2013Suite 330 Boston, MA 02111-1307 USA ddd and this manual are available via the ddd www page.", "num_citations": "16\n", "authors": ["255"]}
{"title": "Better code, better sharing: on the need of analyzing jupyter notebooks\n", "abstract": " By bringing together code, text, and examples, Jupyter notebooks have become one of the most popular means to produce scientific results in a productive and reproducible way. As many of the notebook authors are experts in their scientific fields, but laymen with respect to software engineering, one may ask questions on the quality of notebooks and their code. In a preliminary study, we experimentally demonstrate that Jupyter notebooks are inundated with poor quality code, eg, not respecting recommended coding practices, or containing unused variables and deprecated functions. Considering the education nature of Jupyter notebooks, these poor coding practices, as well as the lacks of quality control, might be propagated into the next generation of developers. Hence, we argue that there is a strong need to programmatically analyze Jupyter notebooks, calling on our community to pay more attention to the\u00a0\u2026", "num_citations": "15\n", "authors": ["255"]}
{"title": "AccessiLeaks: investigating privacy leaks exposed by the Android accessibility service\n", "abstract": " To support users with disabilities, Android provides the accessibility services, which implement means of navigating through an app. According to the Android developer\u2019s guide:\" Accessibility services should only be used to assist users with disabilities in using Android devices and apps\". However, developers are free to use this service without any restrictions, giving them critical privileges such as monitoring user input or screen content to capture sensitive information. In this paper, we show that simply enabling the accessibility service leaves 72% of the top finance and 80% of the top social media apps vulnerable to eavesdropping attacks, leaking sensitive information such as logins and passwords. A combination of several tools and recommendations could mitigate the privacy risks: We introduce an analysis technique that detects most of these issues automatically, eg in an app store. We also found that these issues can be automatically fixed in almost all cases; our fixes have been accepted by 70% of the surveyed developers. Finally, we designed a notification mechanism which would warn users against possible misuses of the accessibility services; 50% of users would follow these notifications.", "num_citations": "14\n", "authors": ["255"]}
{"title": "Sample-free learning of input grammars for comprehensive software fuzzing\n", "abstract": " Generating valid test inputs for a program is much easier if one knows the input language. We present first successes for a technique that, given a program P without any input samples or models, learns an input grammar that represents the syntactically valid inputs for P -- a grammar which can then be used for highly effective test generation for P . To this end, we introduce a test generator targeted at input parsers that systematically explores parsing alternatives based on dynamic tracking of constraints; the resulting inputs go into a grammar learner producing a grammar that can then be used for fuzzing. In our evaluation on subjects such as JSON, URL, or Mathexpr, our PYGMALION prototype took only a few minutes to infer grammars and generate thousands of valid high-quality inputs.", "num_citations": "14\n", "authors": ["255"]}
{"title": "Versioning system models through description logic\n", "abstract": " In software configuration management, little attention has been paid to the evolution of system models, that is, the description of the components that make up a system, and the relationships between them. We present an extension to the version set model based on description logic, where roles, set-valued features, model relationships between version sets. Relationships are versioned with their components; features are propagated and unified along component relationships, ensuring configuration completeness and consistency. The integrated version set model has been realized in ICE MAKE, a MAKE clone dealing with versioned system models. ICE MAKE constructs arbitrary version sets according to their respective dependencies and deduces features and dependencies as imposed by the propagated configuration constraints.", "num_citations": "14\n", "authors": ["255"]}
{"title": "Mining input grammars from dynamic control flow\n", "abstract": " One of the key properties of a program is its input specification. Having a formal input specification can be critical in fields such as vulnerability analysis, reverse engineering, software testing, clone detection, or refactoring. Unfortunately, accurate input specifications for typical programs are often unavailable or out of date.", "num_citations": "13\n", "authors": ["255"]}
{"title": "Assessing and restoring reproducibility of Jupyter notebooks\n", "abstract": " Jupyter notebooks-documents that contain live code, equations, visualizations, and narrative text-now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells. In this paper, we present an approach to (1) automatically satisfy dependencies between code cells to reconstruct possible execution orders\u00a0\u2026", "num_citations": "13\n", "authors": ["255"]}
{"title": "Poster: Efficient GUI test generation by learning from tests of other apps\n", "abstract": " Generating GUI tests for complex Web applications is hard. There is lots of functionality to explore: The eBay home page, for instance, sports more than 2,000 individual GUI elements that a crawler has to trigger in order to discover the core functionality. We show how to leverage tests of other applications to guide test generation for a new application: Given a test for payments on Amazon, for instance, we can guide test generation on eBay towards payment functionality, exploiting the semantic similarity between UI elements across both applications. Evaluated on three domains, our approach allows to discover \"deep\" functionality in a few steps, which otherwise would require thousands to millions of crawling interactions.", "num_citations": "13\n", "authors": ["255"]}
{"title": "Replaying and isolating failing multi-object interactions\n", "abstract": " When a program fails, there are typically multiple objects that contribute to the failure. Our JINSI tool automatically captures the failure-causing interaction between objects and isolates a sequence of calls that all are relevant for reproducing the failure. In contrast to existing work, JINSI also isolates relevant interaction within the observed component and thus across all layers of a complex application. In a proof of concept, JINSI has successfully isolated the interaction for a failure of the COLUMBA e-mail client, pinpointing the defect:\" Out of the 187,532 interactions in the addressbook component, two incoming calls suffice to reproduce the failure.\"", "num_citations": "13\n", "authors": ["255"]}
{"title": "Isolating intrusions by automatic experiments\n", "abstract": " When dealing with malware infections, one of the first tasks is to find the processes that were involved in the attack. We introduce Malfor, a system that isolates those processes automatically. In contrast to other methods that help analyze attacks, Malfor works by experiments: first, we record the interaction of the system under attack; after the intrusion has been detected, we replay the recorded events in slightly different configurations to see which processes were relevant for the intrusion. This approach has three advantages over deductive approaches: first, the processes that are thus found have been experimentally shown to be relevant for the attack; second, the amount of evidence that must then be analyzed to find the attack vector is greatly reduced; and third, Malfor itself cannot make wrong deductions. In a first experiment, Malfor was able to extract the three processes responsible for an attack from 32 candidates in about six minutes.", "num_citations": "13\n", "authors": ["255"]}
{"title": "Funktionell und verst\u00e4ndlich programmieren\u2013so lernen es die Passauer\n", "abstract": " K\u00f6nnen unsere Studenten tats\u00e4chlich programmieren? Sie k\u00f6nnen: Schlie\u00dflich haben sie alle ein Programmierpraktikum absolviert. Aber: K\u00f6nnen wir sicher sein, da\u00df ihre Programme einen gr\u00f6\u00dferen Test bestehen w\u00fcrden? Haben wir uns \u00fcberzeugt, da\u00df diese Programme verst\u00e4ndlich und wohldokumentiert geschrieben sind? Und: Wissen wir, da\u00df ein Programm auch tats\u00e4chlich von demjenigen stammt, der uns vorgibt, der Autor zu sein?Wir wollen, da\u00df unsere Studenten gut programmieren lernen, und wir glauben, da\u00df sich ihr K\u00f6nnen in angemessener Programmqualit\u00e4t ausdr\u00fcckt. Jede dieser Fragen kann damit durch angemessene Qualit\u00e4tskontrolle beantwortet werden. Qualit\u00e4tskontrolle\u2013das hei\u00dft Personaleinsatz: Hiwis und Mitarbeiter, die (neben der selbstverst\u00e4ndlichen Beratung) Programme testen und gegenlesen. Dieser Aufwand kostet Geld, weshalb man zwischen Programmqualit\u00e4t und Personaleinsatz abw\u00e4gen mu\u00df: je h\u00f6her die Anforderungen, desto gr\u00f6\u00dfer der Aufwand in der Qualit\u00e4tskontrolle.", "num_citations": "13\n", "authors": ["255"]}
{"title": "Die inferenzbasierte Softwareentwicklungsumgebung NORA\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "13\n", "authors": ["255"]}
{"title": "Transferring tests across web applications\n", "abstract": " When manually testing Web applications, humans can go with vague, yet general instructions, such as \u201cadd the product to shopping cart and proceed to checkout\u201d. Can we teach a robot to follow such instructions as well? We show how to leverage tests of other applications to guide test generation for new applications in the same domain: Given a test for payments on Amazon, we guide test generation on eBay towards payment functionality, exploiting the semantic similarity between UI elements across both applications. Evaluated on twelve Web apps in three domains, our approach allows for discovering deep functionality in a few minutes, where an undirected crawler would require days or weeks to accomplish the same task.", "num_citations": "12\n", "authors": ["255"]}
{"title": "Generalized task parallelism\n", "abstract": " Existing approaches to automatic parallelization produce good results in specific domains. Yet, it is unclear how to integrate their individual strengths to match the demands and opportunities of complex software. This lack of integration has both practical reasons, as integrating those largely differing approaches into one compiler would impose an engineering hell, as well as theoretical reasons, as no joint cost model exists that would drive the choice between parallelization methods. By reducing the problem of generating parallel code from a program dependence graph to integer linear programming, generalized task parallelization integrates central aspects of existing parallelization approaches into a single unified framework. Implemented on top of LLVM, the framework seamlessly integrates enabling technologies such as speculation, privatization, and the realization of reductions. Evaluating our implementation\u00a0\u2026", "num_citations": "12\n", "authors": ["255"]}
{"title": "Can we trust software repositories?\n", "abstract": " To acquire data for empirical studies, software engineering researchers frequently leverage software repositories as data sources. Indeed, version and bug databases contain a wealth of data on how a product came to be. To turn this data into knowledge, though, requires deep insights into the specific process and product; and it requires careful scrutiny of the techniques used to obtain the data. The central challenge of the future will thus be to combine both automatic and manual empirical analysis.", "num_citations": "12\n", "authors": ["255"]}
{"title": "Specifications for free\n", "abstract": " Recent advances in software validation and verification make it possible to widely automate the check whether a specification is satisfied. This progress is hampered, though, by the persistent difficulty of writing specifications. Are we facing a \u201cspecification crisis\u201d? By mining specifications from existing systems, we can alleviate this burden, reusing and extending the knowledge of 60 years of programming, and bridging the gap between formal methods and real-world software. In this NFM 2011 invited keynote, I present the state of the art in specification mining, its challenges, and its potential, up to a vision of seamless integration of specification and programming.", "num_citations": "12\n", "authors": ["255"]}
{"title": "Causes and effects in computer programs\n", "abstract": " Debugging is commonly understood as finding and fixing the cause of a problem. But what does ``cause'' mean? How can we find causes? How can we prove that a cause is a cause--or even ``the'' cause? This paper defines common terms in debugging, highlights the principal techniques, their capabilities and limitations.", "num_citations": "12\n", "authors": ["255"]}
{"title": "Datenstrukturen visualisieren und animieren mit DDD\n", "abstract": " Der graphische Debugger DDD ist mit mehr als 250.000 Anwendern ein weitverbreitetes Werkzeug zur Softwarevisualisierung. Neben der f\u00fcr Debugger\u00fcblichen Funktionalit\u00e4t erm\u00f6glicht DDD die Visualisierung von Datenstrukturen im laufenden Programm. DDD kann verzeigerte Strukturen (wie Listen oder B\u00e4ume) als Graphen darstellen, aber auch numerische Felder zwei- oder dreidimensional darstellen und im Programmlauf animieren. Dieser Beitrag beschreibt die technischen Grundlagen der Visualisierung, die verwendeten Plazierungsverfahren und die Animation von Algorithmen.", "num_citations": "12\n", "authors": ["255"]}
{"title": "DroidMate-2: a platform for Android test generation\n", "abstract": " Android applications (apps) represent an ever increasing portion of the software market. Automated test input generators are the state of the art for testing and security analysis. We introduce DRoIDMATE-2 (DM-2), a platform to easily assist both developers and researchers to customize, develop and test new test generators. DM-2 can be used without app instrumentation or operating system modifications, as a test generator on real devices and emulators for app testing or regression testing. Additionally, it provides sensitive resource monitoring or blocking capabilities through a lightweight app instrumentation, out-of-thebox statement coverage measurement through a fully-fledged app instrumentation and native experiment reproducibility. In our experiments we compared DM-2 against DRoIDBoT, a state-of-the-art test generator by measuring statement coverage. Our results show that DM-2 reached 96% of its\u00a0\u2026", "num_citations": "11\n", "authors": ["255"]}
{"title": "Procrawl: Mining test models from multi-user web applications\n", "abstract": " Today's web applications demand very high release cycles--and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. However, specifying such behavior models and writing the necessary scripts manually is a hard task. We present ProCrawl (Process Crawler), a tool that automatically mines (extended) finite-state machines from (multi-user) web applications and generates executable test scripts. ProCrawl explores the behavior of the application by systematically generating program runs and observing changes on the application's user interface. The resulting models can be directly used for effective model-based testing, in particular regression testing.", "num_citations": "11\n", "authors": ["255"]}
{"title": "Automated debugging in Eclipse: (at the touch of not even a button)\n", "abstract": " Recent advances in debugging allow for automatic isolation of failure causes such as failure-inducing input or code changes. So far, these advances required a significant infrastructure, notably program analysis, automated testing, or automated construction. The ECLIPSE environment provides such an infrastructure in an integrated, user-friendly fashion. We show how developers and users of automated debugging tools can greatly benefit from such an integrated infrastructure.", "num_citations": "11\n", "authors": ["255"]}
{"title": "Learning input tokens for effective fuzzing\n", "abstract": " Modern fuzzing tools like AFL operate at a lexical level: They explore the input space of tested programs one byte after another. For inputs with complex syntactical properties, this is very inefficient, as keywords and other tokens have to be composed one character at a time. Fuzzers thus allow to specify dictionaries listing possible tokens the input can be composed from; such dictionaries speed up fuzzers dramatically. Also, fuzzers make use of dynamic tainting to track input tokens and infer values that are expected in the input validation phase. Unfortunately, such tokens are usually implicitly converted to program specific values which causes a loss of the taints attached to the input data in the lexical phase. In this paper, we present a technique to extend dynamic tainting to not only track explicit data flows but also taint implicitly converted data without suffering from taint explosion. This extension makes it possible to\u00a0\u2026", "num_citations": "10\n", "authors": ["255"]}
{"title": "Building fast fuzzers\n", "abstract": " Fuzzing is one of the key techniques for evaluating the robustness of programs against attacks. Fuzzing has to be effective in producing inputs that cover functionality and find vulnerabilities. But it also has to be efficient in producing such inputs quickly. Random fuzzers are very efficient, as they can quickly generate random inputs; but they are not very effective, as the large majority of inputs generated is syntactically invalid. Grammar-based fuzzers make use of a grammar (or another model for the input language) to produce syntactically correct inputs, and thus can quickly cover input space and associated functionality. Existing grammar-based fuzzers are surprisingly inefficient, though: Even the fastest grammar fuzzer Dharma still produces inputs about a thousand times slower than the fastest random fuzzer. So far, one can have an effective or an efficient fuzzer, but not both. In this paper, we describe how to build fast grammar fuzzers from the ground up, treating the problem of fuzzing from a programming language implementation perspective. Starting with a Python textbook approach, we adopt and adapt optimization techniques from functional programming and virtual machine implementation techniques together with other novel domain-specific optimizations in a step-by-step fashion. In our F1 prototype fuzzer, these improve production speed by a factor of 100--300 over the fastest grammar fuzzer Dharma. As F1 is even 5--8 times faster than a lexical random fuzzer, we can find bugs faster and test with much larger valid inputs than previously possible.", "num_citations": "10\n", "authors": ["255"]}
{"title": "Search-based testing and system testing: a marriage in heaven\n", "abstract": " Software test generation can take place at the function level or the system level, and be driven by random, constraint-based, and/or search-based techniques. In this paper, we argue that the best way to generate tests is at the system level, as it avoids false failures due to implicit preconditions, and that for testing at the system level, search-based techniques are best suited as they avoid the combinatorial explosion of conditions due to long paths.", "num_citations": "10\n", "authors": ["255"]}
{"title": "Artifact Evaluation for Publications (Dagstuhl Perspectives Workshop 15452)\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Perspectives Workshop 15452\" Artifact Evaluation for Publications\". This Perspectives Workshop conveyed several stakeholders in artifact evaluation from different communities to assess how artifact evaluation is working and make recommendations to the computer systems research community about several issues with the process.", "num_citations": "10\n", "authors": ["255"]}
{"title": "ESSENTIAL OPEN SOURCE TOOLSET: PROGRAMMING WITH ECL\n", "abstract": " Market_Desc:\u00b7 Programmers working on Linux/Unix platforms Special Features:\u00b7 Covers newest and best open source tools: Ant, Doxygen, Junit, Valgrind, and Bugzilla\u00b7 Includes a whole chapter on Eclipse, which is thecoolest programming environment ever seen'\u00b7 Covers classic tools with modern tutorials About The Book: Programmers increasingly rely on tools and there are some excellent new, often freely available tools available under Linux/Unix. The book presents all those tools and environments which should form the basic toolset for any programmer working in a Unix-like environment. It shows how to use both those tools now considered, as well as a newer range of exciting plug-ins and extras which make a programmers life so much easier and more productive.", "num_citations": "9\n", "authors": ["255"]}
{"title": "Checking app user interfaces against app descriptions\n", "abstract": " Does the advertised behavior of apps correlate with what a user sees on a screen? In this paper, we introduce a technique to statically extract the text from the user interface definitions of an Android app. We use this technique to compare the natural language topics of an app\u2019s user interface against the topics from its app store description. A mismatch indicates that some feature is exposed by the user interface, but is not present in the description, or vice versa. The popular Twitter app, for instance, spots UI elements that al-low to make purchases; however, this feature is not mentioned in its description. Likewise, we identified a number of apps whose user interface asks users to access or supply sensitive data; but this \u201cfeature\u201d is not mentioned in the description. In the long run, analyzing user interface topics and comparing them against external descriptions opens the way for checking general mismatches between\u00a0\u2026", "num_citations": "8\n", "authors": ["255"]}
{"title": "Generalized task parallelism\n", "abstract": " Generalized Task Parallelism - CISPA CISPA Home About Browse Data Privacy Policy Impressum Login Generalized Task Parallelism Zeller, Andreas and Hammacher, Clemens and Doerfert, Johannes and Streit, Kevin and Hack, Sebastian (2015) Generalized Task Parallelism. ACM Transactions on Architecture and Code Optimization (TACO), 12 (1). p. 8. Full text not available from this repository. Item Type: Article Additional Information: pub_id: 1144 Bibtex: StDoHaZeHa_15:Task URL date: None Divisions: Andreas Zeller (Software Engineering, ST) Depositing User: Sebastian Weisgerber Date Deposited: 26 Jul 2017 10:29 Last Modified: 18 Jul 2019 12:12 Primary Research Area: NRA5: Empirical & Behavioral Security URI: https://publications.cispa.saarland/id/eprint/454 Actions Actions (login required) View Item View Item CISPA is powered by EPrints 3 which is developed by the School of Electronics and at of . \u2026", "num_citations": "8\n", "authors": ["255"]}
{"title": "Test complement exclusion: Guarantees from dynamic analysis\n", "abstract": " Modern test generation techniques allow to generate as many executions as needed, combined with dynamic analysis, they allow for understanding program behavior in situations where static analysis is challenged or impossible. However, all these dynamic techniques would still suffer from the incompleteness of testing: If some behavior has not been observed so far, there is no guarantee that it may not occur in the future. In this talk, I introduce a method called Test Complement Exclusion that combines test generation and sand boxing to provide such a guarantee. Test Complement Exclusion will have significant impact in the security domain, as it effectively detects and protects against unexpected changes of program behavior, however, guarantees would also strengthen findings in dynamic software comprehension. First experiments on real-world ANDROID programs demonstrate the feasibility of the approach.", "num_citations": "8\n", "authors": ["255"]}
{"title": "Mining bug data\n", "abstract": " Although software systems control many aspects of our daily life world, no system is perfect. Many of our day-to-day experiences with computer programs are related to software bugs. Although software bugs are very unpopular, empirical software engineers and software repository analysts rely on bugs or at least on those bugs that get reported to issue management systems. So what makes data software repository analysts appreciate bug reports? Bug reports are development artifacts that relate to code quality and thus allow us to reason about code quality, and quality is key to reliability, end-users, success, and finally profit. This chapter serves as a hand-on tutorial on how to mine bug reports, relate them to source code, and use the knowledge of bug fix locations to model, estimate, or even predict source code quality. This chapter also discusses risks that should be addressed before one can achieve\u00a0\u2026", "num_citations": "8\n", "authors": ["255"]}
{"title": "Mining operational preconditions\n", "abstract": " A procedure\u2019s client must satisfy its precondition\u2014that is, reach a state in which the procedure may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We use static analysis to infer the sequence of operations a variable goes through before being used as a parameter:\u201cIn parseProperties (String xml), the parameter xml normally stems from getProperties ().\u201d Such operational preconditions can be learned from code examples and checked to detect anomalies. Applied to A\uf773\uf770\uf765\uf763\uf774 J, our OP-M\uf769\uf76e\uf765\uf772 prototype found 288 violations of operational preconditions, uncovering 9 unique defects and 48 unique code smells.", "num_citations": "8\n", "authors": ["255"]}
{"title": "CALAPPA: a toolchain for mining android applications\n", "abstract": " Software engineering researchers and practitioners working on the Android ecosystem frequently have to do the same tasks over and over: retrieve data from the Google Play store to analyze it, decompile the Dalvik bytecode to understand the behavior of the app, and analyze applications metadata and user reviews. In this paper we present CALAPPA, a highly reusable and customizable toolchain that allows researchers to easily run common analysis tasks on large Android application datasets. CALAPPA includes components to retrieve the data from different Android stores, and comes with a predefined, but extensible, set of modules that can analyze apps metadata and code.", "num_citations": "7\n", "authors": ["255"]}
{"title": "Mining android apps for anomalies\n", "abstract": " How do we know a program does what it claims to do? Our CHABADA prototype can cluster Android\u2122 apps by their description topics and identify outliers in each cluster with respect to their API usage. A \u201cweather\u201d app that sends messages thus becomes an anomaly; likewise, a \u201cmessaging\u201d app would typically not be expected to access the current location and would also be identified. In this paper we present a new approach for anomaly detection that improves the classification results of our original CHABADA paper [1]. Applied on a set of 22,500+ Android applications, our CHABADA prototype can now predict 74% of novel malware and as such, without requiring any known malware patterns, maintains a false positive rate close to 10%.", "num_citations": "7\n", "authors": ["255"]}
{"title": "Search-based program analysis\n", "abstract": " Traditionally, program analysis has been divided into two camps: Static techniques analyze code and safely determine what cannot happen; while dynamic techniques analyze executions to determine what actually has happened. While static analysis suffers from overapproximation, erring on whatever could happen, dynamic analysis suffers from underapproximation, ignoring what else could happen. In this talk, I suggest to systematically generate executions to enhance dynamic analysis, exploring and searching the space of software behavior. First results in fault localization and specification mining demonstrate the benefits of search-based analysis.", "num_citations": "7\n", "authors": ["255"]}
{"title": "A unified configuration management model\n", "abstract": " Integration of configuration management (CM) tools into software development environments raises the need for CM models to interoperate through a unified CM model. We present the version set model, where versions, components, and aggregates are grouped into sets according to their features, using feature logic as a formal base to denote sets and operations and deduce consistency. Version sets generalize well-known CM concepts such as components, repositories, workspaces, aggregates, or configurations. Arbitrary revision/variant combinations of components and aggregates are modeled in a uniform and orthogonal way. We show how the concepts of four central configuration management models\u2014the checkin/checkout model, the change set model, the composition model, and the long transaction model\u2014are encompassed and extended by the version set model, making it a unified basis for modeling, realizing and integrating configuration management tasks. Finally, some conditions for efficient realization are identified, based on our practical experience with the configuration management tool ICE. Although the described operations generally result in exponential time complexity, it turns out that the discussed CM models can be realized and combined without loss of efficiency.", "num_citations": "7\n", "authors": ["255"]}
{"title": "Analyzing the user interface of android apps\n", "abstract": " When interacting with Android apps, users may not always get what they expect. For instance, when clicking on a button labeled\" upload picture\", the app may actually leak the user's location while uploading photos to a cloud service. In this paper we present BACKSTAGE, a static analysis framework that binds UI elements to their corresponding callbacks, and further extracts actions in the form of Android sensitive API calls that may be triggered by events on such UI elements. We illustrate the inner workings of the analysis implemented by BACKSTAGE and then compare it against similar frameworks.", "num_citations": "6\n", "authors": ["255"]}
{"title": "Mining specifications: A roadmap\n", "abstract": " Recent advances in software validation and verification make it possible to widely automate whether a specification is satisfied. This progress is hampered, though, by the persistent difficulty of writing specifications. Are we facing a \u201cspecification crisis\u201d? In this paper, I show how to alleviate the burden of writing specifications by reusing and extending specifications as mined from existing software and give an overview on the state of the art in specification mining, its origins, and its potential.", "num_citations": "6\n", "authors": ["255"]}
{"title": "Debugging debugging: acm sigsoft impact paper award keynote\n", "abstract": " Imagine some program and a number of changes. If none of these changes is applied (\" yesterday\"), the program works. If all changes are applied (\" today\"), the program does not work. Which change is responsible for the failure? This is how the abstract of the paper\" Yesterday, my program worked. Today, it does not. Why?\" started; a paper which, originally published at ESEC/FSE 1999 [12], introduced the concept of delta debugging, one of the most popular automated debugging techniques. This year, this paper receives the ACM SIGSOFT Impact Paper Award, recognizing its influence in the past ten years. In my keynote, I review the state of debugging then and now, share how it can be hard to be simple, what programmers really need, and what research should do (and should not do) to explore these needs and cater to them.", "num_citations": "6\n", "authors": ["255"]}
{"title": "Isolating cause-effect chains with AskIgor\n", "abstract": " AskIgor is a Web service for automatically determining causes of program failures. You submit an executable and two invocations: one where the program fails, and one where it passes. AskIgor compares these two runs and conducts a number of systematic tests to narrow down the set of failure-inducing variable values automatically. This reveals the cause-effect chain of the failure - that is, the variables and values that caused the failure. In a case study, AskIgor successfully isolated the cause-effect chain for a failure of the GNU C compiler: \"initially, the C program to be compiled contained an addition of 1.0; this caused an addition operator in the intermediate RTL representation; this caused a cycle in the RTL tree - and this caused the compiler to crash\".", "num_citations": "6\n", "authors": ["255"]}
{"title": "Delta debugging\n", "abstract": " 7< SELECT NAME=\" priority\" MULTIPLE SIZE= 7> P 8< SELECT NAME=\" priority\" MULTIPLE SIZE= 7> P 9< SELECT NAME=\" priority\" MULTIPLE SIZE= 7> P 10< SELECT NAME=\" priority\" MULTIPLE SIZE= 7> F 11< SELECT NAME=\" priority\" MULTIPLE SIZE= 7> P 12< SELECT NAME=\" priority\" MULTIPLE SIZE= 7> P 13< SELECT NAME=\" priority\" MULTIPLE SIZE= 7> P", "num_citations": "6\n", "authors": ["255"]}
{"title": "Restoring Execution Environments of Jupyter Notebooks\n", "abstract": " More than ninety percent of published Jupyter notebooks do not state dependencies on external packages. This makes them non-executable and thus hinders reproducibility of scientific results. We present SnifferDog, an approach that 1) collects the APIs of Python packages and versions, creating a database of APIs; 2) analyzes notebooks to determine candidates for required packages and versions; and 3) checks which packages are required to make the notebook executable (and ideally, reproduce its stored results). In its evaluation, we show that SnifferDog precisely restores execution environments for the largest majority of notebooks, making them immediately executable for end users.", "num_citations": "5\n", "authors": ["255"]}
{"title": "Automatically Granted Permissions in Android apps: An Empirical Study on their Prevalence and on the Potential Threats for Privacy\n", "abstract": " Developers continuously update their Android apps to keep up with competitors in the market. Such constant updates do not bother end users, since by default the Android platform automatically pushes the most recent compatible release on the device, unless there are major changes in the list of requested permissions that users have to explicitly grant. The lack of explicit user's approval for each application update, however, may lead to significant risks for the end user, as the new release may include new subtle behaviors which may be privacy-invasive. The introduction of permission groups in the Android permission model makes this problem even worse: if a user gives a single permission within a group, the application can silently request further permissions in this group with each update---without having to ask the user.", "num_citations": "5\n", "authors": ["255"]}
{"title": "Inferring input grammars from dynamic control flow\n", "abstract": " A program is characterized by its input model, and a formal input model can be of use in diverse areas including vulnerability analysis, reverse engineering, fuzzing and software testing, clone detection and refactoring. Unfortunately, input models for typical programs are often unavailable or out of date. While there exist algorithms that can mine the syntactical structure of program inputs, they either produce unwieldy and incomprehensible grammars, or require heuristics that target specific parsing patterns. In this paper, we present a general algorithm that takes a program and a small set of sample inputs and automatically infers a readable context-free grammar capturing the input language of the program. We infer the syntactic input structure only by observing access of input characters at different locations of the input parser. This works on all program stack based recursive descent input parsers, including PEG and parser combinators, and can do entirely without program specific heuristics. Our Mimid prototype produced accurate and readable grammars for a variety of evaluation subjects, including expr, URLparse, and microJSON.", "num_citations": "5\n", "authors": ["255"]}
{"title": "If You Can't Kill a Supermutant, You Have a Problem\n", "abstract": " Quality of software test suites can be effectively and accurately measured using mutation analysis. Traditional mutation involves seeding first and sometimes higher order faults into the program, and evaluating each for detection. However, traditional mutants are often heavily redundant, and it is desirable to produce the complete matrix of test cases vs mutants detected by each. Unfortunately, even the traditional mutation analysis has a heavy computational footprint due to the requirement of independent evaluation of each mutant by the complete test suite, and consequently the cost of evaluation of complete kill matrix is exorbitant. We present a novel approach of combinatorial evaluation of multiple mutants at the same time that can generate the complete mutant kill matrix with lower computational requirements. Our approach also has the potential to reduce the cost of execution of traditional mutation analysis\u00a0\u2026", "num_citations": "5\n", "authors": ["255"]}
{"title": "Quantifying the information leak in cache attacks through symbolic execution\n", "abstract": " Cache timing attacks allow attackers to infer the properties of a secret execution by observing cache hits and misses. But how much information can actually leak through such attacks? For a given program, a cache model, and an input, our CHALICE framework leverages symbolic execution to compute the amount of information that can possibly leak through cache attacks. At the core of CHALICE is a novel approach to quantify information leak that can highlight critical cache side-channel leaks on arbitrary binary code. In our evaluation on real-world programs from OpenSSL and Linux GDK libraries, CHALICE effectively quantifies information leaks: For an AES-128 implementation on Linux, for instance, CHALICE finds that a cache attack can leak as much as 127 out of 128 bits of the encryption key.", "num_citations": "5\n", "authors": ["255"]}
{"title": "Software-Metriken\n", "abstract": " Software-Metriken Page 1 Software-Metriken Andreas Zeller Lehrstuhl f\u00fcr Softwaretechnik Universit\u00e4t des Saarlandes, Saarbr\u00fccken 2002-06-12 Software-Metriken Zu den Aufgaben eines Managers geh\u00f6rt die Kontrolle der Software-Entwicklung: 1. Pl\u00e4ne und Standards einrichten 2. Messen der Ausf\u00fchrung gegen Pl\u00e4ne und Standards 3. Korrektur der Abweichungen Eine Software-Metrik definiert, wie eine Kenngr\u00f6\u00dfe eines Software-Produkts oder Software-Prozesses gemessen wird You can\u2019t control what you can\u2019t measure! (DeMarco) Was kann man messen? Im Software-Prozess: Ressourcenaufwand (Mitarbeiter, Zeit, Kosten, . . . ) Fehler Kommunikationsaufwand Im Produkt: Umfang (LOC, % Wiederverwendung, # Prozeduren, . . . ) Komplexit\u00e4t Lesbarkeit (Stil) Entwurfsqualit\u00e4t (Modularit\u00e4t, Bindung, Kopplung, . . . ) Produktqualit\u00e4t (Testergebnisse, Testabdeckung, . . . ) 1 Page 2 Wie mu\u00df man messen? f\u00fcr -: \u2013 \u2013 \u2013 \u2026", "num_citations": "5\n", "authors": ["255"]}
{"title": "Automated debugging in eclipse\n", "abstract": " Your program fails. What is the cause of this failure? In this demo, we present two delta debugging plug-ins for the Eclipse environment which isolate failure causes in the program history and in the program's execution.", "num_citations": "5\n", "authors": ["255"]}
{"title": "Versioning software systems through concept descriptions\n", "abstract": " Software systems are frequently described as a collection of related components. As software systems evolve, changes in components sometimes impose structural changes, that is, in the component relationships. When generic components are used, such as a component family coming in multiple variance dimensions, the number of relationships grows exponentially with the number of instantiations, making maintenance difficult. To track structural changes in systems with generic components, we propose a versioning model based on concept descriptions. Concept descriptions denote version sets by their features, where set-valued features can be used to model relationships between version sets. Relationships are versioned with their components; features are propagated and unified along component relationships, ensuring configuration completeness and consistency. These properties are valuable in system\u00a0\u2026", "num_citations": "5\n", "authors": ["255"]}
{"title": "Abstracting failure-inducing inputs\n", "abstract": " A program fails. Under which circumstances does the failure occur? Starting with a single failure-inducing input (\" The input ((4)) fails\") and an input grammar, the DDSET algorithm uses systematic tests to automatically generalize the input to an abstract failure-inducing input that contains both (concrete) terminal symbols and (abstract) nonterminal symbols from the grammar\u2014for instance,\"((< expr>))\", which represents any expression< expr> in double parentheses. Such an abstract failure-inducing input can be used (1) as a debugging diagnostic, characterizing the circumstances under which a failure occurs (\" The error occurs whenever an expression is enclosed in double parentheses\");(2) as a producer of additional failure-inducing tests to help design and validate fixes and repair candidates (\" The inputs ((1)),((3* 4)), and many more also fail\"). In its evaluation on real-world bugs in JavaScript, Clojure, Lua, and\u00a0\u2026", "num_citations": "4\n", "authors": ["255"]}
{"title": "Active learning of input grammars\n", "abstract": " Knowing the precise format of a program's input is a necessary prerequisite for systematic testing. Given a program and a small set of sample inputs, we (1) track the data flow of inputs to aggregate input fragments that share the same data flow through program execution into lexical and syntactic entities; (2) assign these entities names that are based on the associated variable and function identifiers; and (3) systematically generalize production rules by means of membership queries. As a result, we need only a minimal set of sample inputs to obtain human-readable context-free grammars that reflect valid input structure. In our evaluation on inputs like URLs, spreadsheets, or configuration files, our AUTOGRAM prototype obtains input grammars that are both accurate and very readable - and that can be directly fed into test generators for comprehensive automated testing.", "num_citations": "4\n", "authors": ["255"]}
{"title": "How Developers Debug Software\u2014The DBGBENCH Dataset\n", "abstract": " How do professional software engineers debug computer programs? In an experiment with 27 real bugs that existed in several widely used programs, we invited 12 professional software engineers, who together spent one month on localizing, explaining, and fixing these bugs. This did not only allow us to study the various tools and strategies used to debug the same set of errors. We could also determine exactly which statements a developer would localize as faults, how a developer would diagnose and explain an error, and how a developer would fix an error - all of which software engineering researchers seek to automate. Until now, it has been difficult to evaluate the effectiveness and utility of automated debugging techniques without a user study. We publish the collected data, called DBGBENCH, to facilitate the effective evaluation of automated fault localization, diagnosis, and repair techniques w.r.t. the\u00a0\u2026", "num_citations": "4\n", "authors": ["255"]}
{"title": "Thread-level speculation with kernel support\n", "abstract": " Runtime systems for speculative parallelization can be substantially sped up by implementing them with kernel support. We describe a novel implementation of a thread-level speculation (TLS) system using virtual memory to isolate speculative state, implemented in a Linux kernel module. This design choice not only maximizes performance, but also allows to guarantee soundness in the presence of system calls, such as I/O. Its ability to maintain speedups even on programs with frequent mis-speculation, significantly extends its usability, for instance in speculative parallelization. We demonstrate the advantage of kernel-based TLS on a number of programs from the Cilk suite, where this approach is superior to the state of the art in each single case (7.28 x on average). All systems described in this paper are made available as open source.", "num_citations": "4\n", "authors": ["255"]}
{"title": "Ein Softwaretechnik-Praktikum als Sommerkurs\n", "abstract": " Ein semesterbegleitendes Softwaretechnik-Praktikum verleitet die Teilnehmer dazu, vor lauter Begeisterung und Gruppendruck andere Veranstaltungen zu vernachl\u00e4ssigen. An der Universit\u00e4t des Saarlandes wird das Praktikum daher in der vorlesungsfreien Zeit absolviert, und zwar als sechsw\u00f6chiger Vollzeitkurs mit begleitender Vorlesung. Diese Form wirkt studienverk\u00fcrzend und vereinfacht die Teamarbeit durch Ganztagspr\u00e4senz. Risikomindernde Ma\u00dfnahmen wie ein einheitliches, vorgegebenes Pflichtenheft, spielerische Elemente und automatisch testbare Erfolgskriterien sorgen f\u00fcr hohe Motivation bei reibungslosem Ablauf.", "num_citations": "4\n", "authors": ["255"]}
{"title": "Automatisches Debugging\n", "abstract": " Das Auffinden und Beheben von Fehlern geh\u00f6rt zu den zeitraubendsten T\u00e4tigkeiten in der Software-Entwicklung. Das sogenannte Debugging kann bis zu 50% des Entwicklungsaufwands ausmachen. Trotz dieser \u00f6konomischen Bedeutung sind im Debugging nur wenige Fortschritte zu verzeichnen\u2013wie vor 30 Jahren basiert die Debugging-Praxis im Wesentlichen auf Versuch und Irrtum.", "num_citations": "4\n", "authors": ["255"]}
{"title": "From Automated Testing to Automated Debugging\n", "abstract": " Debugging is still one of the hardest, yet least systematic activities of software engineering. The Delta Debugging algorithm isolates failure causes automatically\u2014by systematically narrowing down failure-inducing circumstances until a minimal set remains. Delta Debugging has been applied to isolate failure-inducing program input (eg a HTML page that makes a Web browser fail), failure-inducing user interaction (eg the keystrokes that make a program crash), or failure-inducing changes to the program code (eg after a failing regression test).", "num_citations": "4\n", "authors": ["255"]}
{"title": "Inferenzbasierte Werkzeuge in NORA\n", "abstract": " Die experimentelle Softwareentwicklungsumgebung NORA strebt die Nutzbarmachung neuer Ergebnisse im Bereich Unifikationstheorie und Deduktionsverfahren f\u00fcr Softwarewerkzeuge an. Gruppiert um eine Bibliothek wiederverwendbarer Softwarekomponenten bietet NORA interaktive Werkzeuge zum Komponentenretrieval mit Spezifikationen und Verwendungsmustern, zum unifikationsbasierten Konfigurationsmanagement sowie zur Inferenz von Varianten-und Konfigurationsstrukturen aus existierenden Quelltexten. NORA ist mit sprachspezifischem Wissen parametrisiert und kann unvollst\u00e4ndige oder inkonsistente Information handhaben. Der Aufsatz gibt eine Ubersicht \u00fcber die Werkzeuge und die verwendeten Inferenzverfahren; abschlie\u00dfend wird die Systemarchitektur und die Kommunikation zwischen den Werkzeugen skizziert.", "num_citations": "4\n", "authors": ["255"]}
{"title": "Locating faults with program slicing: an empirical analysis\n", "abstract": " Statistical fault localization is an easily deployed technique for quickly determining candidates for faulty code locations. If a human programmer has to search the fault beyond the top candidate locations, though, more traditional techniques of following dependencies along dynamic slices may be better suited. In a large study of 457 bugs (369 single faults and 88 multiple faults) in 46 open source C programs, we compare the effectiveness of statistical fault localization against dynamic slicing. For single faults, we find that dynamic slicing was eight percentage points more effective than the best performing statistical debugging formula; for 66% of the bugs, dynamic slicing finds the fault earlier than the best performing statistical debugging formula. In our evaluation, dynamic slicing is more effective for programs with single fault, but statistical debugging performs better on multiple faults. Best results, however, are\u00a0\u2026", "num_citations": "3\n", "authors": ["255"]}
{"title": "When does my program do this? learning circumstances of software behavior\n", "abstract": " A program fails. Under which circumstances does the failure occur? Our Alhazenapproach starts with a run that exhibits a particular behavior and automatically determines input features associated with the behavior in question:(1) We use a grammar to parse the input into individual elements.(2) We use a decision tree learner to observe and learn which input elements are associated with the behavior in question.(3) We use the grammar to generate additional inputs to further strengthen or refute hypotheses as learned associations.(4) By repeating steps 2 and 3, we obtain a theory that explains and predicts the given behavior. In our evaluation using inputs for find, grep, NetHack, and a JavaScript transpiler, the theories produced by Alhazen predict and produce failures with high accuracy and allow developers to focus on a small set of input features:\u201cgrep fails whenever the--fixed-strings option is used in\u00a0\u2026", "num_citations": "3\n", "authors": ["255"]}
{"title": "Testing Apps With Real-World Inputs\n", "abstract": " To test mobile apps, one requires realistic and coherent test inputs. The Link approach for Web testing has shown that knowledge bases such as DBPedia can be a reliable source of semantically coherent inputs. In this paper, we adapt and extend the Link approach towards test generation for mobile applications:", "num_citations": "3\n", "authors": ["255"]}
{"title": "Restoring reproducibility of Jupyter notebooks\n", "abstract": " Jupyter notebooks---documents that contain live code, equations, visualizations, and narrative text---now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells.", "num_citations": "3\n", "authors": ["255"]}
{"title": "Architektur und technologiekomponenten eines digitalen zwillings\n", "abstract": " Kurzfassung Die Konzepte des digitalen Zwillings werden sowohl in der Wissenschaft als auch in der Industrie immer relevanter. Daraus ergeben sich viele verschiedene Sichtweisen. Ein \u00dcberblick \u00fcber das vorherrschende Verst\u00e4ndnis wird in diesem Beitrag gegeben. Dar\u00fcber hinaus werden die notwendigen Aspekte eines digitalen Zwillings erl\u00e4utert und der Bedarf an zuk\u00fcnftiger Forschung zu Schl\u00fcsseltechnologien (Technologiekomponenten) im Kontext digitaler Zwilling begr\u00fcndet. Auf dieser Grundlage werden m\u00f6gliche Vorteile eines digitalen Zwillings \u00fcber seinen Lebenszyklus dargestellt. Weiterhin wird in diesem Beitrag ein ausgearbeiteter Vorschlag f\u00fcr die Architektur eines digitalen Zwillings pr\u00e4sentiert. Der Beitrag schlie\u00dft mit einem Prototyp, in dem Artefakte und Konzepte des digitalen Zwillings eines Lkws exemplarisch implementiert sind.1. Einleitung In der industriellen Produktion erfordern individualisierte Massenproduktion [1],[2] und hoher Wettbewerbsdruck [2] eine st\u00e4ndig wachsende Produktivit\u00e4t gepaart mit zunehmender Flexibilit\u00e4t [3] und Rekonfigurierbarkeit [4]. Ein verbreiteter Ansatz zur Bew\u00e4ltigung solcher Herausforderungen ist der Einsatz von cyber-physischen Systemen (CPS)[4]. CPS sind[5]. Sie versprechen, die adaptive und intelligente \u00dcberwachung, Steuerung und Beeinflussung der physischen Welt zu erm\u00f6glichen [6],[7]. Dabei werden jedoch sowohl die CPS selbst als auch ihr Entwurfsprozess immer komplexer, was f\u00fcr ihre Entwickler und Anwender eine erhebliche Herausforderung darstellt [6].", "num_citations": "3\n", "authors": ["255"]}
{"title": "Mining models\n", "abstract": " Modern Model Checking techniques can easily verify advanced properties in complex software systems. Specifying these models and properties is as hard as ever, though. I present techniques to extract models from legacy systems\u2014models that are precise and complete enough to serve as specifications, and which open the door to modular verification.", "num_citations": "3\n", "authors": ["255"]}
{"title": "Assessing modularity via usage changes\n", "abstract": " Good program design strives towards modularity, that is, limiting the effects of changes to the code. We assess the modularity of software modules by mining change histories: The more a change to a module implementation changes its usage in client code, the lower its modularity. In an early analysis of four different releases of open-source projects, we found that changes can differ greatly in their impact on client code, and that such impact helps in assessing modularity.", "num_citations": "3\n", "authors": ["255"]}
{"title": "When does my program fail?\n", "abstract": " Oops! My program fails. Which are the circumstances under which this failure occurs? Answering this question is one of the first steps in debugging -- and a crucial one, as it helps characterizing, understanding, and classifying the problem. In this paper, we propose a technique to identify failure circumstances automatically. Given a concrete failure, we first compute the path condition leading to the failure and then use a constraint solver to identify, from the constraints in the path condition, the general failure conditions: \"The program fails whenever the credit card number begins with 6, 5, and a non-zero digit.\" A preliminary evaluation of the approach on real programs demonstrates its potential usefulness.", "num_citations": "3\n", "authors": ["255"]}
{"title": "Learning from 6,000 projects: Mining models in the large\n", "abstract": " Models - abstract and simple descriptions of some artifact - are the backbone of all software engineering activities. While writing models is hard, existing code can serve as a source for abstract descriptions of how software behaves. To infer correct usage, code analysis needs usage examples, though, the more, the better. We have built a lightweight parser that efficiently extracts API usage models from source code - models that can then be used to detect anomalies. Applied on the 200 million lines of code of the Gen too Linux distribution, we would extract more than 15 million API constraints, encoding and abstracting the \"wisdom of Linux code\".", "num_citations": "3\n", "authors": ["255"]}
{"title": "Alice Who?\n", "abstract": " TCP connections are generally not authenticated. This is a problem with mitigating factors, because if you spoof the sender address, you usually won\u2019t get the return packets; also, if you are on the same Ethernet, you have to do something about the other party\u2019s ARP daemon. But it\u2019s possible.", "num_citations": "3\n", "authors": ["255"]}
{"title": "Speeding up GUI testing by on-device test generation\n", "abstract": " When generating GUI tests for Android apps, it typically is a separate test computer that generates interactions, which are then executed on an actual Android device. While this approach is efficient in the sense that apps and interactions execute quickly, the communication overhead between test computer and device slows down testing considerably. In this work, we present DD-2, a test generator for Android that tests other apps on the device using Android accessibility services. In our experiments, DD-2 has shown to be 3.2 times faster than its computer-device counterpart, while sharing the same source code.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Carving parameterized unit tests\n", "abstract": " We present a method to automatically extract (\"carve\") parameterized unit tests from system test executions. The unit tests execute the same functions as the system tests they are carved from, but can do so much faster as they call functions directly; furthermore, being parameterized, they can execute the functions with a large variety of randomly selected input values. If a unit-level test fails, we lift it to the system level to ensure the failure can be reproduced there. Our method thus allows to focus testing efforts on selected modules while still avoiding false alarms: In our experiments, running parameterized unit tests for individual functions was, on average, 30~times faster than running the system tests they were carved from.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Mining Sandboxes\n", "abstract": " A technique to confine a computer program to computing resources accessed during automatic testing. Sandbox mining first explores software behavior by means of automatic test generation, and extracts the set of resources accessed during these tests. This set is then used as a sandbox, blocking access to resources not used during testing. The mined sandbox thus protects against behavior changes such as the activation of latent malware, infections, targeted attacks, or malicious updates. The use of test generation makes sandbox mining a fully automatic process that can be run by vendors and end users alike.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Poster: How developers debug software\u2013the DBGBENCH dataset\n", "abstract": " How do professional software engineers debug computer programs? In an experiment with 27 real bugs that existed in several widely used programs, we invited 12 professional software engineers, who together spent one month on localizing, explaining, and fixing these bugs. This did not only allow us to study the various tools and strategies used to debug the same set of errors. We could also determine exactly which statements a developer would localize as faults, how a developer would diagnose and explain an error, and how a developer would fix an error\u2013all of which software engineering researchers seek to automate. Until now, it has been difficult to evaluate the effectiveness and utility of automated debugging techniques without a user study. We publish the collected data, called DBGBENCH, to facilitate the effective evaluation of automated fault localization, diagnosis, and repair techniques wrt the judgement of human experts.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Programmers should still use slices when debugging\n", "abstract": " What is the best technique for fault localization? In a study of 37 real bugs (and 37 injected faults) in more than a dozen open source C programs, we compare the effectiveness of statistical debugging against dynamic slicing\u2014the first study ever to compare the techniques. On average, dynamic slicing is more effective than statistical debugging, requiring programmers to examine only 14%(42 lines) of the code before finding the defect, less than half the effort required by statistical debugging (30% or 157 lines). Best results are obtained by a hybrid approach: If programmers first examine the top five most suspicious locations from statistical debugging, and then switch to dynamic slices, they will need to examine only 11%(35 lines) of the code.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Test generation across multiple layers\n", "abstract": " Complex software systems frequently come in many layers, each realized in a different programming language. This is a challenge for test generation, as the semantics of each layer have to be determined and integrated. An automatic test generator for Java, for instance, is typically unable to deal with the internals of lower level code (such as C-code), which results in lower coverage and fewer test cases of interest. In this paper, we sketch a novel approach to help search-based test generators for Java to achieve better coverage of underlying native code layers. The key idea is to apply test generation to the native layer first, and then to use the inputs to the native test cases as targets for search-based testing of the higher Java layers. We demonstrate our approach on a case study combining KLEE and EVOSUITE.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Fault prediction, localization, and repair\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Seminar 13061 \u201cFault Prediction, Localization, and Repair\u201d.Software debugging, which involves localizing, understanding, and removing the cause of a failure, is a notoriously difficult, extremely time consuming, and human-intensive activity. For this reason, researchers have invested a great deal of effort in developing automated techniques and tools for supporting various debugging tasks. In this seminar, we discussed several different tools and techniques that aid in the task of Fault Prediction, Localization and Repair. The talks encompassed a wide variety of methodologies for fault prediction and localizing, such as", "num_citations": "2\n", "authors": ["255"]}
{"title": "Seeding Bugs to Find Bugs: Beautiful Mutation Testing\n", "abstract": " In principle, everything should be settled, as all tests have passed. However, the fact that all your tests have passed may not mean much if your test suite is not good enough. If your test suite does not properly check the program outcome, for instance, your tests may pass even though the result is wrong. So, how can you check whether your test suite is effective in finding defects? In this chapter, we will explore a simple, elegant, and indeed beautiful way to test the quality of a test suite\u2014namely by systematically seeding artificial defects into the program and checking whether the test suite finds them.", "num_citations": "2\n", "authors": ["255"]}
{"title": "How failures come to be\n", "abstract": " Andreas Zeller Page 1 Andreas Zeller How Failures Come to be Page 2 2 An F-16 (northern hemisphere) Page 3 3 An F-16 (southern hemisphere) Page 4 4 F-16 Landing Gear Page 5 5 The First Bug September 9, 1947 Page 6 6 More Bugs Page 7 Facts on Debugging 7 \u2022 Software bugs are costing ~60 bln US$/yr \u2022 Improvements could reduce cost by 30% \u2022 Validation (including debugging) can easily take up to 50-75% of the development time \u2022 When debugging, some people are three times as efficient than others Page 8 8 A Sample Program sample 9 8 7 $ Output: 7 8 9 sample 11 14 $ Output: 0 11 Page 9 9 How to Debug (Sommerville 2004) Locate error Design error repair Repair error Re-test program Page 10 10 The Traffic Principle T R A F F I C rack the problem eproduce utomate ind Origins ocus solate orrect Page 11 11 The Traffic Principle T R A F F I C rack the problem eproduce utomate ind Origins ocus 1\u2026", "num_citations": "2\n", "authors": ["255"]}
{"title": "Project-specific deletion patterns\n", "abstract": " We apply data mining to version control data in order to detect project-specific deletion patterns---subcomponents or features of the software that were deleted on purpose. We believe that locations that are similar to earlier deletions are likely to be code smells. Future recommendation tools can warn against such smells:\" People who used gets () in the past now use fgets (). Consider a change, too.\"", "num_citations": "2\n", "authors": ["255"]}
{"title": "Learning from software\n", "abstract": " During software development and maintenance, programmers conduct several activities--tracking bug reports, changing the software, discussing features, or running tests. As more and more of these activities are organized using tools, they leave data behind that is automatically accessible in software archives such as change or bug databases. By data mining these archives, one can leverage the resulting patterns and rules to increase program quality and programmer productivity.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Where do bugs come from?\n", "abstract": " The analysis of bug databases reveals that some software components are far more failure-prone than others. Yet it is hard to find properties that are universally shared by failure-prone components. We have mined the Eclipse bug and version databases to map failures to Eclipse components. The resulting data set lists the defect density of all Eclipse components, and may thus help to find features that predict how defect-prone a component will be.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Isolating cause-effect chains in computer systems\n", "abstract": " One of the major tasks in maintaining software systems is understanding how specific effects came to be. This is especially true for effects that cause major harm, and especially challenging for causes that actively prevent discovery. We introduce Malfor, a system that, for any reliably reproducible and observable effect, isolates the processes that cause the effect. We apply Malfor to intrusion analysis\u2014that is, understanding how an intruder gained access to a system\u2014and come up with cause-effect chains that describe how an attack came to be: \u201cAn attacker sent a malicious request to the Web server, which gave him a local shell, by which he gained administrator provileges via a security hole in Perl, and thus installed a new administrator account\u201d. Malfor works by experiments. First, we record the interaction of the system being diagnosed. After the effect (the intrusion) has been detected, we replay the recorded events in slightly different configurations to isolate the processes which were relevant for the effect. While intrusion analysis is among the more spectacular uses of Malfor, the underlying techniques can easily be generalized to arbitrary system behaviors.", "num_citations": "2\n", "authors": ["255"]}
{"title": "Extreme Programming\n", "abstract": " Extreme Programming Page 1 Extreme Programming Andreas Zeller Lehrstuhl f\u00fcr Softwaretechnik Universit\u00e4t des Saarlandes, Saarbr\u00fccken 28. Oktober 2002 Der Code-and-Fix-Zyklus Geeignet f\u00fcr 1-Person-Projekte und Praktikumsaufgaben in den ersten Semestern Ablauf: 1. Code schreiben und testen 2. Code \u201d verbessern\u201c (Fehlerbeseitigung, Erweiterung, Effizienz. . . ) 3. GOTO 1 Wenn das Problem klar spezifiziert ist und eine Person die Implementierung allein bew\u00e4ltigen kann, ist wenig dagegen zu sagen. Software-Krise Jedoch: Termine, Kosten, Qualit\u00e4t sind nicht vorhersehbar. Wartbarkeit und Zuverl\u00e4ssigkeit nehmen kontinuierlich ab ( \u201d Entropie\u201c) Wenn der Programmierer k\u00fcndigt, ist alles vorbei Heutige Projekte umfassen -zig Personen(jahre), die geplant und koordiniert werden m\u00fcssen Sobald Entwickler und Anwender nicht identisch sind, gibt es Meinungsverschiedenheiten \u00fcber den erwarteten/geben -(\u2026", "num_citations": "2\n", "authors": ["255"]}
{"title": "Softwarewerkzeuge\n", "abstract": " Softwarewerkzeuge Page 1 1/107 Softwarewerkzeuge oder Praxis der Softwareentwicklung Prof. Dr.-Ing. Andreas Zeller Lehrerweiterbildung Informatik, Schlo\u00df Dagstuhl, 29.11.2001 Page 2 1/107 Ubersicht Ausgew\u00e4hlte Themen und Werkzeuge der Softwareentwicklung: \u2022 Konfigurationsmanagement mit CVS \u2022 Automatisches Testen mit JUnit \u2022 Extreme Programming Page 3 2/107 Konfigurationsmanagement Ein gro\u00dfes Software-Produkt besteht aus \u2022 Tausenden von Komponenten, \u2022 die von Hunderten oder gar Tausenden Personen entwickelt und gewartet werden, \u2022 die oftmals auch noch auf viele Orte verteilt sind, und an all diesen Komponenten werden von all diesen Personen Anderungen vorgenommen. Die Aufgabe, solche \u00c4nderungen zu organisieren und zu kontrollieren, hei\u00dft Software-Konfigurationsmanagement. Page 4 3/107 Konfigurationsmanagement: Ursprung Konfigurationsmanagement: von der US-\u2026", "num_citations": "2\n", "authors": ["255"]}
{"title": "Incremental configuration management based on feature unification\n", "abstract": " We apply feature logic to the problem of incremental configuration management. Feature logic has originally been developed in computer linguistics as a knowledge representation and inference mechanism. It offers a uniform formalism for the description of variants and revisions, where sets of versions rather than single versions are the basic units of reasoning. Feature logic thus opens a whole algebra of version sets, which includes specific configurations as special cases. Our approach allows for interactive configuration management, where a configuration thread is constructed by adding or modifying configuration constraints until either a complete configuration or an inconsistency can be deduced. A set of versions of a software component can be represented and processed as a single source file enriched with preprocessor statements. Thus, our tool can be used as an intelligent front end to more traditional techniques.", "num_citations": "2\n", "authors": ["255"]}
{"title": "What do all these Buttons do? Statically Mining Android User Interfaces at Scale\n", "abstract": " We introduce FRONTMATTER: a tool to automatically mine both user interface models and behavior of Android apps at a large scale with high precision. Given an app, FRONTMATTER statically extracts all declared screens, the user interface elements, their textual and graphical features, as well as Android APIs invoked by interacting with them. Executed on tens of thousands of real-world apps, FRONTMATTER opens the door for comprehensive mining of mobile user interfaces, jumpstarting empirical research at a large scale, addressing questions such as \"How many travel apps require registration?\", \"Which apps do not follow accessibility guidelines?\", \"Does the user interface correspond to the description?\", and many more. FRONTMATTER and the mined dataset are available under an open-source license.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Fuzzing with fast failure feedback\n", "abstract": " Fuzzing -- testing programs with random inputs -- has become the prime technique to detect bugs and vulnerabilities in programs. To generate inputs that cover new functionality, fuzzers require execution feedback from the program -- for instance, the coverage obtained by previous inputs, or the conditions that need to be resolved to cover new branches. If such execution feedback is not available, though, fuzzing can only rely on chance, which is ineffective. In this paper, we introduce a novel fuzzing technique that relies on failure feedback only -- that is, information on whether an input is valid or not, and if not, where the error occurred. Our bFuzzer tool enumerates byte after byte of the input space and tests the program until it finds valid prefixes, and continues exploration from these prefixes. Since no instrumentation or execution feedback is required, bFuzzer is language agnostic and the required tests execute very quickly. We evaluate our technique on five subjects, and show that bFuzzer is effective and efficient even in comparison to its white-box counterpart.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Bridging the Gap between Unit Test Generation and System Test Generation\n", "abstract": " Common test generators fall into two categories. Generating test inputs at the unit level is fast, but can lead to false alarms when a function is called with inputs that would not occur in a system context. If a generated input at the system level causes a failure, this is a true alarm, as the input could also have come from the user or a third party; but system testing is much slower. In this paper, we introduce the concept of a test generation bridge, which joins the accuracy of system testing with the speed of unit testing. A Test Generation Bridge allows to combine an arbitrary system test generator with an arbitrary unit test generator. It does so by carving parameterized unit tests from system (test) executions. These unit tests run in a context recorded from the system test, but individual parameters are left free for the unit test generator to systematically explore. This allows symbolic test generators such as KLEE to operate on individual functions in the recorded system context. If the test generator detects a failure, we lift the failure-inducing parameter back to the system input; if the failure can be reproduced at the system level, it is reported as a true alarm. Our BASILISK prototype can extract and test units out of complex systems such as a Web/Python/SQLite/C stack; in its evaluation, it achieves a higher coverage than a state-of-the-art system test generator.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Why Does this App Need this Data? Automatic Tightening of Resource Access\n", "abstract": " On mobile operating systems, apps may access resources that are not be needed for their primary functionality. Which are the resources an app actually needs for its core functionality? And what happens if we deny access to other resources? Using a test generator for user interaction, we systematically explore app behavior under varied resource constraints and determine the impact of access restrictions, yielding a minimal set of required privileges for each app and functionality. In our proof of concept on Android apps, our TIARA prototype could block up to 69% of resource accesses while retaining all previously explored functionality.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Testen vernetzter Systeme f\u00fcr die Industrie 4.0: Auszug aus dem Statusreport des VDI/VDE-Fachausschusses 7.25\n", "abstract": " Dass die Komplexit\u00e4t vernetzter Komponenten und deren Entwicklung zunehmen, ist hinl\u00e4nglich bekannt. So zieht sich das Motiv \u201eKomplexit\u00e4tssteigerung \u201cwie ein roter Faden durch die verschiedenen Abschnitte des Statusreports. Deshalb wird auf das Motiv und dessen Auspr\u00e4gung im Anschluss eingegangen. Wir haben folgendes Verst\u00e4ndnis von Komplexit\u00e4t [1]:", "num_citations": "1\n", "authors": ["255"]}
{"title": "Wo ist der Fehler und wie wird er behoben? Ein Experiment mit Softwareentwicklern.\n", "abstract": " Dieser Beitrag ist eine gek\u00fcrzte, deutsche Version unseres Artikels \u201cWhere is the Bug and How is it Fixed? An Experiment with Practitioners\u201d publiziert in dem Berichtsband des elften gemeinsamen Treffens der European Software Engineering Conference und des ACM SIGSOFT Symposium on the Foundations of Software Engineering [B7].Seit mehr als zwanzig Jahren hat die Forschung eine Reihe von Ans\u00e4tzen zur automatischen Lokalisierung, Erkl\u00e4rung und Behebung von Programmfehlern entwickelt. In letzter Zeit haben Forscher auch einige Benchmarks bereitgestellt, die zur empirischen Auswertung solcher Ans\u00e4tze genutzt werden k\u00f6nnen. Mit Hilfe solcher Benchmarks k\u00f6nnen Wissenschaftler empirisch fundierte Behauptungen \u00fcber die Effektivit\u00e4t solcher Werkzeuge und Techniken aufstellen. Zum Beispiel wird oft ein Fehlerlokalisierungswerkzeug als effektiver bewertet, welches einer Anweisung, die w\u00e4hrend der Fehlerbehebung ge\u00e4ndert wurde, einen h\u00f6heren Rang zuweist. Dabei wird angenommen, da\u00df ein Entwickler dieselbe Anweisung als Fehlerursache identifizieren w\u00fcrde. Eine automatische Fehlerbehebung wird als effektiver bewertet, wenn daraufhin alle Regressionstests erfolgreich durchlaufen. Hier wird angenommen, da\u00df ein Entwickler eine solche fehlerbehebende Programm\u00e4nderung akzeptieren oder gar selbst durchf\u00fchren w\u00fcrde. Leider ist der Debuggingprozess in der Realit\u00e4t nicht ganz so einfach, besonders nicht f\u00fcr den Menschen. Wir stellen einen anderen Benchmark zur Verf\u00fcgung; einen Benchmark der eine Realit\u00e4tskontrolle erlaubt.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Model-based testing of end-user collaboration intensive systems\n", "abstract": " Collaboration intensive systems like social networks support the interaction of multiple end-users playing different roles such as\" friend\" or\" post owner\". To ensure that end-users achieve the intended type of collaboration, systematic testing can be an effective means. However, manually creating effective test cases is cumbersome and error prone as the amount of end-users interactions to test grows exponentially with the number of involved end-users and roles.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Engineering und Betrieb Smarter Komponenten in IoT-Netzwerken f\u00fcr die Automatisierung der Produktion\n", "abstract": " Diese Heterogenit\u00e4t, welche sich in zahlreichen Kommunikationsprotokollen und Semantiken ausdr\u00fcckt, wird durch ein gro\u00dfes Produktangebot von Automatisierungskomponenten verst\u00e4rkt. Daher werden Referenzarchitekturen ben\u00f6tigt, die eine m\u00f6glichst einfache Vernetzung unterschiedlichster Komponenten erlauben. F\u00fcr die bereichs\u00fcbergreifende Interoperabilit\u00e4t der Produktionsautomatisierung sind weltweite Standardisierungsbestrebungen im Gang, die eine Referenzarchitektur f\u00fcr das Internet-of-Things (IoT) anstreben.[P40IIC].", "num_citations": "1\n", "authors": ["255"]}
{"title": "Automatic test transfer across applications\n", "abstract": " Building test suites for the Web is hard. We present a novel technique to automatically transfer and adapt existing Selenium test suites from one web application to another. By mapping functional states using topic analysis, we identify which actions yield the same result. This mapping allows for fully automatic test transfer\u2014even across different applications in the same domain: we can take a shopping test that buys a product at Amazon and automatically adapt it to run on eBay or other eCommerce sites. In an evaluation of test traces of 16 real world applications\u2014encompassing more than 1200 user actions\u2014Attaboy was able to successfully transfer 49.3% of all test cases without any human intervention in domains such as eCommerce, knowledge bases, search engines, and news sites.", "num_citations": "1\n", "authors": ["255"]}
{"title": "JTACO: Test Execution for Faster Bounded Verification\n", "abstract": " In bounded program verification a finite set of execution traces is exhaustively checked in order to find violations to a given specification (ie errors). SAT-based bounded verifiers rely on SAT-Solvers as their back-end decision procedure, accounting for most of the execution time due to their exponential time complexity. In this paper we sketch a novel approach to improve SAT-based bounded verification. As modern SAT-Solvers work by augmenting partial assignments, the key idea is to translate some of these partial assignments into JUnit test cases during the SAT-Solving process. If the execution of the generated test cases succeeds in finding an error, the SAT-Solver is promptly stopped. We implemented our approach in JTACO, an extension to the TACO bounded verifier, and evaluate our prototype by verifying parameterized unit tests of several complex data structures.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Inter-Application Communication Testing of Android Applications Using Intent Fuzzing\n", "abstract": " Testing is a crucial stage in the software development process that is used to uncover bugs and potential security threats. If not conducted thoroughly, buggy software may cause erroneous, malicious and even harmful behavior. Unfortunately in most software systems, testing is either completely neglected or not thoroughly conducted. One such example is Google's popular mobile platform, Android OS, where inter-application communication is not properly tested. This is because of the difficulty which it possesses in the development overhead and the manual labour required by developers in setting up the testing environment. Consequently, the lack of Android application testing continues to cause Android users to experience erroneous behavior and sudden crashes, impacting user experience and potentially resulting in financial losses. When a caller application attempts to communicate with a potentially buggy application, the caller application will suffer functional errors or it may even potentially crash. Incidentally, the user will complain that the caller application is not providing the promised functionality, resulting in a devaluation of the application's user rating. Successive failures will no longer be considered as isolated events, potentially crippling developer credibility of the calling application. In this thesis we present an automated tester for inter-application communication in Android applications. The approach used for testing is called Intent based Testing. Android applications are typically divided into multiple components that communicate via intents: messages passed through Android OS to coordinate operations between the different\u00a0\u2026", "num_citations": "1\n", "authors": ["255"]}
{"title": "Fault Prediction, Localization, and Repair (Dagstuhl Seminar 13061)\n", "abstract": " Software debugging, which involves localizing, understanding, and removing the cause of a failure, is a notoriously difficult, extremely time consuming, and human-intensive activity. For this reason, researchers have invested a great deal of effort in developing automated techniques and tools for supporting various debugging tasks. In this seminar, we discussed several different tools and techniques that aid in the task of Fault Prediction, Localization and Repair. The talks encompassed a wide variety of methodologies for fault prediction and localizing, such as-statistical fault localization,-core dump analysis,-taint analysis,-program slicing techniques,-dynamic fault-comprehension techniques,-visualization techniques,-combining hardware and software instrumentation for fault detection and failure prediction,-and verification techniques for checking safety properties of programs. For automatically (or semi-automatically) repairing faulty programs, the talks covered approaches such as-automated repair based on symbolic execution, constraint solving and program synthesis,-combining past fix patterns, machine learning and semantic patch generation-a technique that exploits the intrinsic redundancy of reusable components,-a technique based on memory-access patterns and a coverage matrix,-a technique that determines a combination of mutual-exclusion and order relationships that, once enforced, can prevent buggy interleaving. in addition, this seminar also explored some unusual topics such as Teaching Debugging, using Online Courses. Another interesting topic covered was the low representation of females in computing, and how\u00a0\u2026", "num_citations": "1\n", "authors": ["255"]}
{"title": "Muster der Softwaretechnik-Lehre.\n", "abstract": " Das Organisieren von Softwaretechnik-Projekten ist f\u00fcr neue Dozenten stets eine Herausforderung: Man muss Kunden finden, Projekte definieren, Tutoren schulen, Anforderungen definieren, Terminpl\u00e4ne machen, Regeln aufstellen... und zum Schluss das Projektergebnis bewerten. Eine ungen\u00fcgende Planung und Umsetzung kann schnell das gesamte Projekt gef\u00e4hrden. Daher empfiehlt es sich, f\u00fcr die h\u00e4ufigsten Fragen und Probleme L\u00f6sungen explizit aufzuschreiben. Dies k\u00f6nnen zun\u00e4chst immer wiederkehrende Dinge sein, wie die Frage, wann welche Vorlesungsinhalte ben\u00f6tigt werden, oder wie die Begutachtung von Praktikums-Dokumenten organisiert werden muss. Andere Fragen m\u00f6gen zun\u00e4chst banal erscheinen, machen aber schnell deutlich, warum es sich empfiehlt, Wissen explizit zu dokumentieren:", "num_citations": "1\n", "authors": ["255"]}
{"title": "Hardware and Software: Verification and Testing: 5th International Haifa Verification Conference, HCV 2009, Haifa, Israel, October 19-22, 2009, Revised Selected Papers\n", "abstract": " This book constitutes the thoroughly refereed post proceedings of the 5th International Haifa Verification Conference, HVC 2009, held in Haifa, Israel in October 2009. The 11 revised full papers presented together with four abstracts of invited lectures were carefully reviewed and selected from 23 submissions. The papers address all current issues, challenges and future directions of verification for hardware, software, and hybrid systems and present academic research in the verification of systems, generally divided into two paradigms-formal verification and dynamic verification (testing).", "num_citations": "1\n", "authors": ["255"]}
{"title": "Failure is a four-letter word\n", "abstract": " Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though. Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion. Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies. Results: In our sample set, IROP correctly predicted up to 74% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice. Conclusions: With the abundance of software development data, even the simplest methods can produce \u201cactionable\u201d results.*", "num_citations": "1\n", "authors": ["255"]}
{"title": "Compiler Construction: 15th International Conference, CC 2006, Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2006, Vienna, Austria\u00a0\u2026\n", "abstract": " ETAPS 2006 was the ninth instance of the European Joint Conferences on Theory and Practice of Software. ETAPS is an annual federated conference that was established in 1998 by combining a number of existing and new conferences. This year it comprised? ve conferences (CC, ESOP, FASE, FOSSACS, TACAS), 18 satellite workshops (AC-CAT, AVIS, CMCS, COCV, DCC, EAAI, FESCA, FRCSS, GT-VMT, LDTA, MBT, QAPL, SC, SLAP, SPIN, TERMGRAPH, WITS and WRLA), two tutorials, and seven invited lectures (not including those that were speci? c to the satellite events). We-ceived over 550 submissions to the? ve conferences this year, giving an overall acc-tance rate of 23%, with acceptance rates below 30% for each conference. Congratu-tions to all the authors who made it to the? nal programme! I hope that most of the other authorsstill founda way of participatingin this excitingevent and I hope you will continue submitting. The events that comprise ETAPS address various aspects of the system devel-ment process, including speci? cation, design, implementation, analysis and impro-ment. The languages, methodologies and tools which support these activities are all well within its scope. Di? erent blends of theory and practice are represented, with an inclination towards theory with a practical motivation on the one hand and soundly based practice on the other. Many of the issues involved in software design apply to systems in general, including hardware systems, and the emphasis on software is not intended to be exclusive.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Evaluating a lightweight defect localization tool\n", "abstract": " AMPLE locates likely failure-causing classes by comparing method call sequences of passing and failing runs. A difference in method call sequences, such as multiple deallocation of the same resource, is likely to point to the erroneous class. In this paper, we describe the implementation of AMPLE as well as its evaluation.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Detecting Failure-Related Anomalies in Method Call Sequences\n", "abstract": " This thesis proposes a new approach for defect localization in object-oriented programs. It is based on a comparison of method invocation traces recorded for a failing and several passing runs. Rather than comparing the coverage of executed code, the approach investigates sequences of method calls. For each class of a program, a set of characteristic call sequences is extracted from the invocation trace. These sets are then compared across multiple runs, to find the class whose behavior in the failing run deviates the most from that in the passing runs. Our assumption is, that differences in the sequence sets from passing and failing runs correlate with failure of a program. The program\u2019s classes are ranked such that the class whose behavior deviates the most is ranked highest. This is the recommended order in which classes should be searched to find the defect that caused the program to fail.The approach presented is implemented in a tool called AMPLE and evaluated in several case studies with programs ranging from 4.000 to 112.000 lines of code. Each case study that investigates the predictive power uses known defects or publicly available bug databases as sources for faults. The results show, that comparing sequences of method calls produces significiantly better results than comparing coverage of methods. In one of the case studies, AMPLE was able to pinpoint the defective class in 38% of all test runs.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Isolating Failure-Inducing Input\n", "abstract": " Given some test case, a program fails. Which circumstances of the test case are responsible for the particular failure? The Delta Debugging algorithm generalizes and simplifies some failing test case to a minimal test case that still produces the failure; it also isolates the difference between a working and a failing test case.In a case study, the Mozilla web browser crashed after 95 user actions. Our prototype implementation automatically simplified the input to 3 relevant user actions. Likewise, it simplified 896 lines of HTML to the single line that caused the failure. The case study required 139 automated test runs, or 35 minutes on a 500 MHz PC.", "num_citations": "1\n", "authors": ["255"]}
{"title": "Software configuration with feature logic\n", "abstract": " Software con guration management (SCM) is the discipline for controlling the evolution of software systems. The central problems of SCM are closely related to central arti cial intelligence (AI) topics, such as knowledge representation (how do we represent the features of versions and components, and how does this knowledge involve in time?), con guration (how do we compose a consistent con guration from components, and how do we express constraints?), and planning (how do we construct a software product from a source con guration, and what are the features of this product?). Although the research communities of both SCM and AI work on con guration topics, the knowledge about the mutual problems and methods is still small. We show how feature logic, a description logic with boolean operations, can be used to represent both knowledge about versions and components, as well as to infer the consistency of possible con gurations and thus solve con guration problems in SCM. This interplay of knowledge representation and con guration techniques shows immediate bene cial consequences in SCM, such as the integration and uni cation of SCM versioning concepts. Moreover, SCM may turn out as a playground for testing and validating new AI methods in practice.", "num_citations": "1\n", "authors": ["255"]}
{"title": "The Scientific Method\n", "abstract": " Everything gets written down, formally, so that you know at all times where you are, where you\u2019ve been, where you\u2019re going, and where you want to get. In scientific work and electronics technology this is necessary because otherwise the problems get so complex you get lost in them and confused and forget what you know and what you don\u2019t know and have to give up.", "num_citations": "1\n", "authors": ["255"]}