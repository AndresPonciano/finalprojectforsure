{"title": "CloneCloud: Elastic Execution between Mobile Device and Cloud\n", "abstract": " Mobile applications are becoming increasingly ubiquitous and provide ever richer functionality on mobile devices. At the same time, such devices often enjoy strong connectivity with more powerful machines ranging from laptops and desktops to commercial clouds. This paper presents the design and implementation of CloneCloud, a system that automatically transforms mobile applications to benefit from the cloud. The system is a flexible application partitioner and execution runtime that enables unmodified mobile applications running in an application-level virtual machine to seamlessly off-load part of their execution from mobile devices onto device clones operating in a computational cloud. CloneCloud uses a combination of static analysis and dynamic profiling to partition applications automatically at a fine granularity while optimizing execution time and energy use for a target computation and communication\u00a0\u2026", "num_citations": "2382\n", "authors": ["504"]}
{"title": "Augmented Smartphone Applications Through Clone Cloud Execution.\n", "abstract": " Smartphones enable a new, rich user experience in pervasive computing, but their hardware is still very limited in terms of computation, memory, and energy reserves, thus limiting potential applications. In this paper, we propose a novel architecture that addresses these challenges via seamlessly\u2014but partially\u2014off-loading execution from the smartphone to a computational infrastructure hosting a cloud of smartphone clones. We outline new augmented execution opportunities for smartphones enabled by our CloneCloud architecture.", "num_citations": "667\n", "authors": ["504"]}
{"title": "Implementing declarative overlays\n", "abstract": " Overlay networks are used today in a variety of distributed systems ranging from file-sharing and storage systems to communication infrastructures. However, designing, building and adapting these overlays to the intended application and the target environment is a difficult and time consuming process. To ease the development and the deployment of such overlay networks we have implemented P2, a system that uses a declarative logic language to express overlay networks in a highly compact and reusable form. P2 can express a Narada-style mesh network in 16 rules, and the Chord structured overlay in only 47 rules. P2 directly parses and executes such specifications using a dataflow architecture to construct and maintain overlay networks. We describe the P2 approach, how our implementation works, and show by experiment its promising trade-off point between specification complexity and performance.", "num_citations": "496\n", "authors": ["504"]}
{"title": "Declarative networking: language, execution and optimization\n", "abstract": " The networking and distributed systems communities have recently explored a variety of new network architectures, both for application-level overlay networks, and as prototypes for a next-generation Internet architecture. In this context, we have investigated declarative networking: the use of a distributed recursive query engine as a powerful vehicle for accelerating innovation in network architectures [23, 24, 33]. Declarative networking represents a significant new application area for database research on recursive query processing. In this paper, we address fundamental database issues in this domain. First, we motivate and formally define the Network Datalog (NDlog) language for declarative network specifications. Second, we introduce and prove correct relaxed versions of the traditional semi-na\u00efve query evaluation technique, to overcome fundamental problems of the traditional technique in an asynchronous\u00a0\u2026", "num_citations": "336\n", "authors": ["504"]}
{"title": "Attested append-only memory: Making adversaries stick to their word\n", "abstract": " Researchers have made great strides in improving the fault tolerance of both centralized and replicated systems against arbitrary (Byzantine) faults. However, there are hard limits to how much can be done with entirely untrusted components; for example, replicated state machines cannot tolerate more than a third of their replica population being Byzantine. In this paper, we investigate how minimal trusted abstractions can push through these hard limits in practical ways. We propose Attested Append-Only Memory (A2M), a trusted system facility that is small, easy to implement and easy to verify formally. A2M provides the programming abstraction of a trusted log, which leads to protocol designs immune to equivocation -- the ability of a faulty host to lie in different ways to different clients or servers -- which is a common source of Byzantine headaches. Using A2M, we improve upon the state of the art in Byzantine-fault\u00a0\u2026", "num_citations": "319\n", "authors": ["504"]}
{"title": "The LOCKSS Peer-to-Peer Digital Preservation System\n", "abstract": " The LOCKSS project has developed and deployed in a world-wide test a peer-to-peer system for preserving access to journals and other archival information published on the Web. It consists of a large number of independent, low-cost, persistent Web caches that cooperate to detect and repair damage to their content by voting in \u201copinion polls.\u201d Based on this experience, we present a design for and simulations of a novel protocol for voting in systems of this kind. It incorporates rate limitation and intrusion detection to ensure that even some very powerful adversaries attacking over many years have only a small probability of causing irrecoverable damage before being detected.", "num_citations": "284\n", "authors": ["504"]}
{"title": "The architecture of pier: an internet-scale query processor\n", "abstract": " This paper presents the architecture of PIER1, an Internetscale query engine we have been building over the last three years. PIER is the first general-purpose relational query processor targeted at a peer-to-peer (p2p) architecture of thousands or millions of participating nodes on the Internet. It supports massively distributed, database-style dataflows for snapshot and continuous queries. It is intended to serve as a building block for a diverse set of Internet-scale informationcentric applications, particularly those that tap into the standardized data readily available on networked machines, including packet headers, system logs, and file names. In earlier papers we presented the vision for PIER, its application relevance, and initial simulation results [28, 32]. We have also presented real-world results showing the benefits of using PIER in a p2p filesharing network [41, 43]. In this paper we present, for the first time, a\u00a0\u2026", "num_citations": "281\n", "authors": ["504"]}
{"title": "Peer-to-peer caching schemes to address flash crowds\n", "abstract": " Flash crowds can cripple a webs ite\u2019s performance. Since they are infrequent and unpredictable, these floods do not justify the cost of traditional commercial solutions. We describe Backslash, a collaborative webmirroring system run by a collective of websites that wish to protect themselves from flash crowds. Backslash is built on a distributed hash table overlay and uses the structure of the overlay to cache aggressively a resource that experiences an uncharacteristically high request load. By redirecting requests for that resource uniformly to the created caches, Backslash helps alleviate the effects of flash crowds. We explore cache diffusion techniques for use in such a system and find that probabilistic forwarding improves load distribution albeit not dramatically.", "num_citations": "264\n", "authors": ["504"]}
{"title": "Predicting execution time of computer programs using sparse polynomial regression\n", "abstract": " Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (eg, the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (eg, features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program\u2019s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications\u00a0\u2026", "num_citations": "252\n", "authors": ["504"]}
{"title": "Declarative networking\n", "abstract": " Declarative Networking is a programming methodology that enables developers to concisely specify network protocols and services, which are directly compiled to a dataflow framework that executes the specifications. This paper provides an introduction to basic issues in declarative networking, including language design, optimization, and dataflow execution. We present the intuition behind declarative programming of networks, including roots in Datalog, extensions for networked environments, and the semantics of long-running queries over network state. We focus on a sublanguage we call Network Datalog (NDlog), including execution strategies that provide crisp eventual consistency semantics with significant flexibility in execution. We also describe a more general language called Overlog, which makes some compromises between expressive richness and semantic guarantees. We provide an overview of\u00a0\u2026", "num_citations": "236\n", "authors": ["504"]}
{"title": "Prochlo: Strong privacy for analytics in the crowd\n", "abstract": " The large-scale monitoring of computer users' software activities has become commonplace, eg, for application telemetry, error reporting, or demographic profiling. This paper describes a principled systems architecture---Encode, Shuffle, Analyze (ESA)---for performing such monitoring with high utility while also protecting user privacy. The ESA design, and its Prochlo implementation, are informed by our practical experiences with an existing, large deployment of privacy-preserving software monitoring.", "num_citations": "219\n", "authors": ["504"]}
{"title": "The performance cost of shadow stacks and stack canaries\n", "abstract": " Control flow defenses against ROP either use strict, expensive, but strong protection against redirected RET instructions with shadow stacks, or much faster but weaker protections without. In this work we study the inherent overheads of shadow stack schemes. We find that the overhead is roughly 10% for a traditional shadow stack. We then design a new scheme, the parallel shadow stack, and show that its performance cost is significantly less: 3.5%. Our measurements suggest it will not be easy to improve performance on current x86 processors further, due to inherent costs associated with RET and memory load/store instructions. We conclude with a discussion of the design decisions in our shadow stack instrumentation, and possible lighter-weight alternatives.", "num_citations": "217\n", "authors": ["504"]}
{"title": "A fresh look at the reliability of long-term digital storage\n", "abstract": " Emerging Web services, such as email, photo sharing, and web site archives, must preserve large volumes of quickly accessible data indefinitely into the future. The costs of doing so often determine whether the service is economically viable. We make the case that these applications' demands on large scale storage systems over long time horizons require us to reevaluate traditional system designs. We examine threats to long-lived data from an end-to-end perspective, taking into account not just hardware and software faults but also faults due to humans and organizations. We present a simple model of long-term storage failures that helps us reason about various strategies for addressing some of these threats. Using this model we show that the most important strategies for increasing the reliability of long-term storage are detecting latent faults quickly, automating fault repair to make it cheaper and faster, and\u00a0\u2026", "num_citations": "214\n", "authors": ["504"]}
{"title": "Dynamically partitioning applications between weak devices and clouds\n", "abstract": " Mobile cloud computing applications run diverse workloads under diverse device platforms, networks, and clouds. Traditionally these applications are statically partitioned between weak devices and clouds, thus may be significantly inefficient in heterogeneous environments and workloads. We introduce the notion of dynamic partitioning of applications between weak devices and clouds and argue that this is key to addressing heterogeneity problems. We formulate the dynamic partitioning problem and discuss major research challenges around system support for dynamic partitioning.", "num_citations": "208\n", "authors": ["504"]}
{"title": "Preserving Peer Replicas By Rate-Limited Sampled Voting in LOCKSS\n", "abstract": " The LOCKSS project has developed and deployed in a world-wide test a peer-to-peer system for preserving access to journals and other archival information published on the Web. It consists of a large number of independent, low-cost, persistent web caches that cooperate to detect and repair damage to their content by voting in \"opinion polls.\" Based on this experience, we present a design for and simulations of a novel protocol for voting in systems of this kind. It incorporates rate limitation and intrusion detection to ensure that even some very powerful adversaries attacking over many years have only a small probability of causing irrecoverable damage before being detected.", "num_citations": "174\n", "authors": ["504"]}
{"title": "Friday: Global comprehension for distributed replay\n", "abstract": " Debugging and profiling large-scale distributed applications is a daunting task. We present Friday, a system for debugging distributed applications that combines deterministic replay of components with the power of symbolic, low-level debugging and a simple language for expressing higher-level distributed conditions and actions. Friday allows the programmer to understand the collective state and dynamics of a distributed collection of coordinated application components.", "num_citations": "172\n", "authors": ["504"]}
{"title": "Preserving Peer Replicas by Rate-Limited Sampled Voting\n", "abstract": " The LOCKSS project has developed and deployed in a world-wide test a peer-to-peer system for preserving access to journals and other archival information published on the Web. It consists of a large number of independent, low-cost, persistent web caches that cooperate to detect and repair damage to their content by voting in\" opinion polls.\" Based on this experience, we present a design for and simulations of a novel protocol for voting in systems of this kind. It incorporates rate limitation and intrusion detection to ensure that even some very powerful adversaries attacking over many years have only a small probability of causing irrecoverable damage before being detected.", "num_citations": "172\n", "authors": ["504"]}
{"title": "The mobile people architecture\n", "abstract": " People are the outsiders in the current communications revolution. Computer hosts, pagers, and telephones are the addressable entities throughout the Internet and telephony systems. Human beings, however, still need application-specific tricks to be identified, like email addresses, telephone numbers, and ICQ IDs. The key challenge today is to find people and communicate with them personally, as opposed to communicating merely with their possibly inaccessible machines---cell phones that are turned off or PCs on faraway desktops.We introduce the Mobile People Architecture which aims to put the person, rather than the devices that the person uses, at the endpoints of a communication session. We describe a prototype that performs person-level routing; the prototype allows people to receive communication regardless of the network, device, or application they use, while maintaining their privacy.", "num_citations": "151\n", "authors": ["504"]}
{"title": "Secure history preservation through timeline entanglement\n", "abstract": " A secure timeline is a tamper-evident historic record of the states through which a system goes throughout its operational history. Secure timelines can help us reason about the temporal ordering of system states in a provable manner. We extend secure timelines to encompass multiple, mutually distrustful services, using timeline entanglement. Timeline entanglement associates disparate timelines maintained at independent systems, by linking undeniably the past of one timeline to the future of another. Timeline entanglement is a sound method to map a time step in the history of one service onto the timeline of another, and helps clients of entangled services to get persistent temporal proofs for services rendered that survive the demise or non-cooperation of the originating service. In this paper we present the design and implementation of Timeweave, our service development framework for timeline entanglement based on two novel disk-based authenticated data structures. We evaluate Timeweave's performance characteristics and show that it can be efficiently deployed in a loosely-coupled distributed system of a few hundred services with overhead of roughly 2-8% of the processing resources of a PC-grade system.", "num_citations": "140\n", "authors": ["504"]}
{"title": "Not-a-bot: Improving service availability in the face of botnet attacks\n", "abstract": " A large fraction of email spam, distributed denial-ofservice (DDoS) attacks, and click-fraud on web advertisements are caused by traffic sent from compromised machines that form botnets. This paper posits that by identifying human-generated traffic as such, one can service it with improved reliability or higher priority, mitigating the effects of botnet attacks. The key challenge is to identify human-generated traffic in the absence of strong unique identities. We develop NAB (\u201cNot-A-Bot\u201d), a system to approximately identify and certify human-generated activity. NAB uses a small trusted software component called an attester, which runs on the client machine with an untrusted OS and applications. The attester tags each request with an attestation if the request is made within a small amount of time of legitimate keyboard or mouse activity. The remote entity serving the request sends the request and attestation to a verifier, which checks the attestation and implements an application-specific policy for attested requests. Our implementation of the attester is within the Xen hypervisor. By analyzing traces of keyboard and mouse activity from 328 users at Intel, together with adversarial traces of spam, DDoS, and click-fraud activity, we estimate that NAB reduces the amount of spam that currently passes through a tuned spam filter by more than 92%, while not flagging any legitimate email as spam. NAB delivers similar benefits to legitimate requests under DDoS and click-fraud attacks.", "num_citations": "138\n", "authors": ["504"]}
{"title": "BFT protocols under fire\n", "abstract": " Much recent work on Byzantine state machine replication focuses on protocols with improved performance under benign conditions (LANs, homogeneous replicas, limited crash faults), with relatively little evaluation under typical, practical conditions (WAN delays, packet loss, transient disconnection, shared resources). This makes it difficult for system designers to choose the appropriate protocol for a real target deployment. Moreover, most protocol implementations differ in their choice of runtime environment, crypto library, and transport, hindering direct protocol comparisons even under similar conditions.", "num_citations": "132\n", "authors": ["504"]}
{"title": "Person-level Routing in the Mobile People Architecture.\n", "abstract": " Ubiquitous network connectivity for devices does not automatically imply continuous reachability for people. People move from place to place and switch from one network device to another. As a result, phones ring in empty offices, email cannot reach most cell phones, and spam clogs expensive, lowbandwidth links to laptops. Whereas existing mechanisms have addressed host mobility or the mobility of people within one network, few have allowed people, the ultimate and most important endpoints of communication, to roam freely, without being constrained to one location, one application, one device, or one network 1.", "num_citations": "129\n", "authors": ["504"]}
{"title": "Zeno: eventually consistent byzantine-fault tolerance\n", "abstract": " Many distributed services are hosted at large, shared, geographically diverse data centers, and they use replication to achieve high availability despite the unreachability of an entire data center. Recent events show that non-crash faults occur in these services and may lead to long outages. While Byzantine-Fault Tolerance (BFT) could be used to withstand these faults, current BFT protocols can become unavailable if a small fraction of their replicas are unreachable. This is because existing BFT protocols favor strong safety guarantees (consistency) over liveness (availability).This paper presents a novel BFT state machine replication protocol called Zeno that trades consistency for higher availability. In particular, Zeno replaces strong consistency (linearizability) with a weaker guarantee (eventual consistency): clients can temporarily miss each other\u2019s updates but when the network is stable the states from the individual partitions are merged by having the replicas agree on a total order for all requests. We have built a prototype of Zeno and our evaluation using micro-benchmarks shows that Zeno provides better availability than traditional BFT protocols.", "num_citations": "124\n", "authors": ["504"]}
{"title": "Proof sketches: Verifiable in-network aggregation\n", "abstract": " A work on distributed, in-network aggregation assumes a benign population of participants. Unfortunately, modern distributed systems are plagued by malicious participants. In this paper we present a first step towards verifiable yet efficient distributed, in-network aggregation in adversarial settings. We describe a general framework and threat model for the problem and then present proof sketches, a compact verification mechanism that combines cryptographic signatures and Flajolet-Martin sketches to guarantee acceptable aggregation error bounds with high probability. We derive proof sketches for count aggregates and extend them for random sampling, which can be used to provide verifiable approximations for a broad class of data-analysis queries, e.g., quantiles and heavy hitters. Finally, we evaluate the practical use of proof sketches, and observe that adversaries can often be reduced to much smaller\u00a0\u2026", "num_citations": "124\n", "authors": ["504"]}
{"title": "2 p2p or not 2 p2p?\n", "abstract": " In the hope of stimulating discussion, we present a heuristic decision tree that designers can use to judge how suitable a P2P solution might be for a particular problem. It is based on characteristics of a wide range of P2P systems from the literature, both proposed and deployed. These include budget, resource relevance, trust, rate of system change, and criticality.", "num_citations": "108\n", "authors": ["504"]}
{"title": "Verifiable resource accounting for cloud computing services\n", "abstract": " Cloud computing offers users the potential to reduce operating and capital expenses by leveraging the amortization benefits offered by large, managed infrastructures. However, the black-box and dynamic nature of the cloud infrastructure makes it difficult for them to reason about the expenses that their applications incur. At the same time, the profitability of cloud providers depends on their ability to multiplex several customer applications to maintain high utilization levels. However, this multiplexing may cause providers to incorrectly attribute resource consumption to customers or implicitly bear additional costs thereby reducing their cost-effectiveness. Our position in this paper is that for cloud computing as a paradigm to be sustainable in the long term, we need a systematic approach for verifiable resource accounting. Verifiability here means that cloud customers can be assured that (a) their applications indeed\u00a0\u2026", "num_citations": "102\n", "authors": ["504"]}
{"title": "Path-Exploration Lifting: Hi-Fi Tests for Lo-Fi Emulators\n", "abstract": " Processor emulators are widely used to provide isolation and instrumentation of binary software. However they have proved difficult to implement correctly: processor specifications have many corner cases that are not exercised by common workloads. It is untenable to base other system security properties on the correctness of emulators that have received only ad-hoc testing. To obtain emulators that are worthy of the required trust, we propose a technique to explore a high-fidelity emulator with symbolic execution, and then lift those test cases to test a lower-fidelity emulator. The high-fidelity emulator serves as a proxy for the hardware specification, but we can also further validate by running the tests on real hardware. We implement our approach and apply it to generate about 610,000 test cases; for about 95% of the instructions we achieve complete path coverage. The tests reveal thousands of individual\u00a0\u2026", "num_citations": "98\n", "authors": ["504"]}
{"title": "Induced Churn as Shelter from Routing-Table Poisoning.\n", "abstract": " Structured overlays are an important and powerful class of overlay networks that has emerged in recent years. They are typically targeted at peer-to-peer deployments involving millions of user-managed machines on the Internet. In this paper we address routing-table poisoning attacks against structured overlays, in which adversaries attempt to intercept traffic and control the system by convincing other nodes to use compromised nodes as their overlay network neighbors. In keeping with the fully-decentralized goals of structured overlay design, we propose a defense mechanism that makes minimal use of centralized infrastructure. Our approach, induced churn, utilizes periodic routing-table resets, unpredictable identifier changes, and a rate limit on routing-table updates. Induced churn leaves adversaries at the mercy of chance: they have little opportunity to strategize their positions in the overlay, and cannot entrench themselves in any position that they do acquire. We implement induced churn in Maelstrom, an extension to the broadly used Bamboo distributed hash table. Our Maelstrom experiments over a simulated network demonstrate robust routing with very modest costs in bandwidth and latency, at levels of adversarial activity where unprotected overlays are rendered almost completely useless.", "num_citations": "90\n", "authors": ["504"]}
{"title": "Diverse replication for single-machine byzantine-fault tolerance\n", "abstract": " New single-machine environments are emerging from abundant computation available through multiple cores and secure virtualization. In this paper, we describe the research challenges and opportunities around diversified replication as a method to increase the Byzantine-fault tolerance (BFT) of single-machine servers to software attacks or errors. We then discuss the design space of BFT protocols enabled by these new environments.", "num_citations": "88\n", "authors": ["504"]}
{"title": "Loss and delay accountability for the Internet\n", "abstract": " The Internet provides no information on the fate of transmitted packets, and end systems cannot determine who is responsible for dropping or delaying their traffic. As a result, they cannot verify that their ISPs are honoring their service level agreements, nor can they react to adverse network conditions appropriately. While current probing tools provide some assistance in this regard, they only give feedback on probes, not actual traffic. Moreover, service providers could, at any time, render their network opaque to such tools. We propose Audit, an explicit accountability interface, through which ISPs can pro-actively supply feedback to traffic sources on loss and delay, at administrative-domain granularity. Notably, our interface is resistant to ISP lies and can be implemented with a modest NetFlow modification. On our Click-based prototype, playback of real traces from a Tier-1 ISP reveals less than 2% bandwidth\u00a0\u2026", "num_citations": "84\n", "authors": ["504"]}
{"title": "Mantis: Automatic performance prediction for smartphone applications\n", "abstract": " We present Mantis, a framework for predicting the performance of Android applications on given inputs automatically, accurately, and efficiently. A key insight underlying Mantis is that program execution runs often contain features that correlate with performance and are automatically computable efficiently. Mantis synergistically combines techniques from program analysis and machine learning. It constructs concise performance models by choosing from many program execution features only a handful that are most correlated with the program\u2019s execution time yet can be evaluated efficiently from the program\u2019s input. We apply program slicing to accurately estimate the evaluation cost of a feature and automatically generate executable code snippets for efficiently evaluating features. Our evaluation shows that Mantis predicts the execution time of six Android apps with estimation error in the range of 2.2-11.9% by executing predictor code costing at most 1.3% of their execution time on Galaxy Nexus.", "num_citations": "82\n", "authors": ["504"]}
{"title": "Using queries for distributed monitoring and forensics\n", "abstract": " Distributed systems are hard to build, profile, debug, and test. Monitoring a distributed system - to detect and analyze bugs, test for regressions, identify fault-tolerance problems or security compromises - can be difficult and error-prone. In this paper we argue that declarative development of distributed systems is well suited to tackle these tasks. We present an application logging, monitoring, and debugging facility that we have built on top of the P2 system, comprising an introspection model, an execution tracing component, and a distributed query processor. We use this facility to demonstrate a range of on-line distributed diagnosis tools that range from simple, local state assertions to sophisticated global property detectors on consistent snapshots. These tools are small, simple, and can be deployed piecemeal on-line at any point during a system's life cycle. Our evaluation suggests that the overhead of our approach\u00a0\u2026", "num_citations": "81\n", "authors": ["504"]}
{"title": "Enabling the Archival Storage of Signed Documents.\n", "abstract": " Documents in digital formats are increasingly becoming a common form of expression for anything from rants and opinions to transaction records and contracts. Archiving such documents for the long term, particularly when their only form is digital, can be very important. Sadly, the principal digital expression of an author\u2019s intent, the digital signature, is not fit for long-term archives of documents; signing keys can expire or become compromised, rendering the documents they signed indistinguishable from illicit forgeries. We propose KASTS, an extension of traditional archival storage systems that enables the long-term storage of signed documents. KASTS combines time stamping of signed documents with storage of past signature verification keys. We argue that such an extended archival storage system is feasible and describe one possible design for it1.", "num_citations": "68\n", "authors": ["504"]}
{"title": "Clonecloud: boosting mobile device applications through cloud clone execution\n", "abstract": " Mobile applications are becoming increasingly ubiquitous and provide ever richer functionality on mobile devices. At the same time, such devices often enjoy strong connectivity with more powerful machines ranging from laptops and desktops to commercial clouds. This paper presents the design and implementation of CloneCloud, a system that automatically transforms mobile applications to benefit from the cloud. The system is a flexible application partitioner and execution runtime that enables unmodified mobile applications running in an application-level virtual machine to seamlessly off-load part of their execution from mobile devices onto device clones operating in a computational cloud. CloneCloud uses a combination of static analysis and dynamic profiling to optimally and automatically partition an application so that it migrates, executes in the cloud, and re-integrates computation in a fine-grained manner that makes efficient use of resources. Our evaluation shows that CloneCloud can achieve up to 21.2x speedup of smartphone applications we tested and it allows different partitioning for different inputs and networks.", "num_citations": "66\n", "authors": ["504"]}
{"title": "Evita raced: metacompilation for declarative networks\n", "abstract": " Declarative languages have recently been proposed for many new applications outside of traditional data management. Since these are relatively early research efforts, it is important that the architectures of these declarative systems be extensible, in order to accommodate unforeseen needs in these new domains. In this paper, we apply the lessons of declarative systems to the internals of a declarative engine. Specifically, we describe our design and implementation of Evita Raced, an extensible compiler for the OverLog language used in our declarative networking system, P2. Evita Raced is a metacompiler: an OverLog compiler written in OverLog. We describe the minimalist architecture of Evita Raced, including its extensibility interfaces and its reuse of P2's data model and runtime engine. We demonstrate that a declarative language like OverLog is well-suited to expressing traditional and novel query\u00a0\u2026", "num_citations": "66\n", "authors": ["504"]}
{"title": "Towards verifiable resource accounting for outsourced computation\n", "abstract": " Outsourced computation services should ideally only charge customers for the resources used by their applications. Unfortunately, no verifiable basis for service providers and customers to reconcile resource accounting exists today. This leads to undesirable outcomes for both providers and consumers-providers cannot prove to customers that they really devoted the resources charged, and customers cannot verify that their invoice maps to their actual usage. As a result, many practical and theoretical attacks exist, aimed at charging customers for resources that their applications did not consume. Moreover, providers cannot charge consumers precisely, which causes them to bear the cost of unaccounted resources or pass these costs inefficiently to their customers.", "num_citations": "64\n", "authors": ["504"]}
{"title": "Providing packet obituaries\n", "abstract": " The Internet is transparent to success but opaque to failure. This veil of ignorance prevents ISPs from detecting failures by peering partners, and hosts from intelligently adapting their routes to adverse network conditions. To rectify this, we propose an accountability framework that would tell hosts where their packets have died. We describe a preliminary version of this framework and discuss its viability.", "num_citations": "59\n", "authors": ["504"]}
{"title": "Do You Know Where Your Data Are? Secure Data Capsules for Deployable Data Protection\n", "abstract": " Do you know where your data are? Who can see them? Who can modify them without a trace? Who can aggregate, summarize, and embed them for purposes other than yours? We don\u2019t, and we suspect neither do you. The problem is that we do not have a widely-available mechanism to answer these questions, and yet, paradoxically, all evidence shows that it should have been solved long ago. The problem is critical; incidents involving sensitive data leakage, unauthorized access, and integrity violations (accidental or not) are a daily occurrence [1]. It is well known, as evidenced by the volume of relevant government regulation and pontification from privacy advocates. It is interesting, since it has inspired much research into data confidentiality, integrity, and authorization. Yet publicizing it, regulating it, and talking about it have not led to solving the problem effectively for the vast majority of users. Why?We believe the root of this paradox lies in the disconnect between research, policy, and industry objectives, and the needs of the real world. Research has largely focused on elegance and intellectual exploration, while industry has built expedient solutions for media content protection and enterprise rights management, guided by projected revenue. In either case, the problem is solved for some users, under narrow scenarios, but neither research results nor rights management systems have resulted in broad applicability and deployment. In this paper, we examine the problem of protecting data for all users, and not just for some. We use broad applicability as the driving goal, and explore the challenges and promises of reaching towards that goal\u00a0\u2026", "num_citations": "58\n", "authors": ["504"]}
{"title": "Learning and evaluating contextual embedding of source code\n", "abstract": " Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as BERT, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4 M Python files from GitHub, which we use to pre-train CuBERT, an open-sourced code-understanding BERT model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune CuBERT on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, BiLSTM and Transformer models, as well as published state-of-the-art models, showing that CuBERT outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against CuBERT models as a strong baseline.", "num_citations": "56\n", "authors": ["504"]}
{"title": "The mobile people architecture\n", "abstract": " People are the outsiders in the current communications revolution. Computer hosts, pager terminals, and telephones are addressable entities throughout the Internet and telephony systems. Human beings, however, still need application-speci c tricks to be identi ed, like email addresses, telephone numbers, and ICQ IDs. The key challenge today is to nd people and communicate with them personally, as opposed to communicating merely with their possibly inaccessible machines| cell phones that are turned o, or PCs on faraway desktops.We introduce the Mobile People Architecture, designed to meet this challenge. The main goal of this e ort is to put the person, rather than the devices that the person uses, at the endpoints of a communication session. This architecture introduces the concept of routing between people. To that e ect, we de ne the Personal Proxy, which has a dual role: as a Tracking Agent, the proxy maintains the list of devices or applications through which a person is currently accessible; as a Dispatcher, the proxy directs communications and uses Application Drivers to massage communication bits into a format that the recipient can see immediately. It does all this while protecting the location privacy of the recipient from the message sender. Finally, we substantiate our architecture with ideas about a future prototype that allows the easy integration of new application protocols.", "num_citations": "52\n", "authors": ["504"]}
{"title": "The mobile people architecture\n", "abstract": " People are the outsiders in the current communications revolution. Computer hosts, pager terminals, and telephones are addressable entities throughout the Internet and telephony systems. Human beings, however, still need application-speci c tricks to be identi ed, like email addresses, telephone numbers, and ICQ IDs. The key challenge today is to nd people and communicate with them personally, as opposed to communicating merely with their possibly inaccessible machines| cell phones that are turned o, or PCs on faraway desktops.We introduce the Mobile People Architecture, designed to meet this challenge. The main goal of this e ort is to put the person, rather than the devices that the person uses, at the endpoints of a communication session. This architecture introduces the concept of routing between people. To that e ect, we de ne the Personal Proxy, which has a dual role: as a Tracking Agent, the proxy maintains the list of devices or applications through which a person is currently accessible; as a Dispatcher, the proxy directs communications and uses Application Drivers to massage communication bits into a format that the recipient can see immediately. It does all this while protecting the location privacy of the recipient from the message sender. Finally, we substantiate our architecture with ideas about a future prototype that allows the easy integration of new application protocols.", "num_citations": "52\n", "authors": ["504"]}
{"title": "Oscar: A practical page-permissions-based scheme for thwarting dangling pointers\n", "abstract": " Using memory after it has been freed opens programs up to both data and control-flow exploits. Recent work on temporal memory safety has focused on using explicit lock-and-key mechanisms (objects are assigned a new lock upon allocation, and pointers must have the correct key to be dereferenced) or corrupting the pointer values upon free (). Placing objects on separate pages and using page permissions to enforce safety is an older, well-known technique that has been maligned as too slow, without comprehensive analysis. We show that both old and new techniques are conceptually instances of lock-and-key, and argue that, in principle, page permissions should be the most desirable approach. We then validate this insight experimentally by designing, implementing, and evaluating Oscar, a new protection scheme based on page permissions. Unlike prior attempts, Oscar does not require source code, is compatible with standard and custom memory allocators, and works correctly with programs that fork. Also, Oscar performs favorably\u2013often by more than an order of magnitude\u2013compared to recent proposals: overall, it has similar or lower runtime overhead, and lower memory overhead than competing systems.", "num_citations": "42\n", "authors": ["504"]}
{"title": "\u00fcberSpark: Enforcing Verifiable Object Abstractions for Automated Compositional Security Analysis of a Hypervisor.\n", "abstract": " We present u\u0308berSpark (u\u0308Spark), an innovative architecture for compositional verification of security properties of extensible hypervisors written in C and Assembly. u\u0308Spark comprises two key ideas:(i) endowing low-level system software with abstractions found in higher-level languages (eg, objects, interfaces, function-call semantics for implementations of interfaces, access control on interfaces, concurrency and serialization), enforced using a combination of commodity hardware mechanisms and lightweight static analysis; and (ii) interfacing with platform hardware by programming in Assembly using an idiomatic style (called CASM) that is verifiable via tools aimed at C, while retaining its performance and low-level access to hardware. After verification, the C code is compiled using a certified compiler while the CASM code is translated into its corresponding Assembly instructions. Collectively, these innovations enable compositional verification of security invariants without sacrificing performance. We validate u\u0308Spark by building and verifying security invariants of an existing open-source commodity x86 micro-hypervisor and several of its extensions, and demonstrating only minor performance overhead with low verification costs.", "num_citations": "36\n", "authors": ["504"]}
{"title": "Impeding attrition attacks in P2P systems\n", "abstract": " P2P systems are exposed to an unusually broad range of attacks. These include a spectrum of denial-of-service, or attrition, attacks from low-level packet flooding to high-level abuse of the peer communication protocol. We identify a set of defenses that systems can deploy against such attacks and potential synergies among them. We illustrate the application of these defenses in the context of the LOCKSS digital preservation system.", "num_citations": "35\n", "authors": ["504"]}
{"title": "Verifiable network-performance measurements\n", "abstract": " In the current Internet, there is no clean way for affected parties to react to poor forwarding performance: to detect and assess Service Level Agreement (SLA) violations by a contractual partner, a domain must resort to ad-hoc monitoring using probes. Instead, we propose Network Confessional, a new, systematic approach to the problem of forwarding-performance verification. Our system relies on voluntary reporting, allowing each network domain to disclose its loss and delay performance to its customers and peers and, potentially, a regulator. Most importantly, it enables verifiable performance measurements, ie, domains cannot abuse it to significantly exaggerate their performance. Finally, our system is tunable, allowing each participating domain to determine how many resources to devote to it independently (ie, without any inter-domain coordination), exposing a controllable trade-off between performance\u00a0\u2026", "num_citations": "34\n", "authors": ["504"]}
{"title": "Pre-trained contextual embedding of source code\n", "abstract": " The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; BERT and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be finetuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from GitHub to pre-train a BERT model, which we call Code Understanding BERT (CuBERT). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare finetuned CuBERT against sequence models trained with and without the Word2Vec embeddings. Our results show that CuBERT outperforms the baseline methods by a margin of 2.9-22%. We also show its superiority when finetuned with smaller datasets, and over fewer epochs.", "num_citations": "31\n", "authors": ["504"]}
{"title": "Secure data preservers forweb services\n", "abstract": " We examine a novel proposal wherein a user who hands off her data to a web service has complete choice over the code and policies that constrain access to her data. Such an approach is possible if the web service does not require raw access to the user\u2019s data to implement its functionality; access to a carefully chosen interface to the data suffices. Our data preserver framework rearchitects such web services around the notion of a preserver, an object that encapsulates the user\u2019s data with code and policies chosen by the user. Our framework relies on a variety of deployment mechanisms, such as administrative isolation, software-based isolation (eg, virtual machines), and hardware-based isolation (eg, trusted platform modules) to enforce that the service interacts with the preserver only via the chosen interface. Our prototype implementation illustrates three such web services, and we evaluate the cost of privacy in our framework by characterizing the performance overhead compared to the status quo.", "num_citations": "30\n", "authors": ["504"]}
{"title": "Finally, a use for componentized transport protocols\n", "abstract": " This paper argues a new relevance for an old idea: decomposing transport protocols into a set of resuable building blocks that can be recomposed in different ways depending on application requirements. We conjecture that point-to-point applications may well be adequately served by the existing suite of monolithic protocol implementations, but widely-distributed peer-to-peer systems such as overlays are not: the design space of transport protocols between nodes in a large, highly coordinated system is much larger. We provide several examples of existing systems that have implemented a diverse range of transport protocols, and show how a building-block approach covers these systems well, enabling simple specification of hybrids and variants of the protocols. In particular, we show how all of our examples can be implemented in the networking stack of P2, a multipurpose system for building overlay networks from declarative specifications.", "num_citations": "30\n", "authors": ["504"]}
{"title": "Enabling the long-term archival of signed documents through time stamping\n", "abstract": " In this paper we describe how to build a trusted reliable distributed service across administrative domains in a peer-to-peer network. The application we use to motivate our work is a public key time stamping service called Prokopius. The service provides a secure, verifiable but distributable stable archive that maintains time stamped snapshots of public keys over time. This in turn allows clients to verify time stamped documents or certificates that rely on formerly trusted public keys that are no longer in service or where the signer no longer exists. We find that such a service can time stamp the snapshots of public keys in a network of 148 nodes at the granularity of a couple of days, even in the worst case where an adversary causes the maximal amount of damage allowable within our fault model.", "num_citations": "26\n", "authors": ["504"]}
{"title": "Economic measures to resist attacks on a peer-to-peer network\n", "abstract": " Peer-to-peer systems in which the peers are truly autonomous have valuable properties, including resistance to certain forms of organizational failure and legal attack. Unfortunately, they can be vulnerable to malign peers. In the context of the LOCKSS system, a peer-to-peer digital preservation system for e-journals, we describe a set of techniques that enable a large population of autonomous peers to resist attack by a substantial minority of malign peers endowed with unlimited computational resources. LOCKSS peers are able to detect attacks and alert the community of peer operators before damage becomes irreversible. These techniques include rate limitation and making peers \u201cpay\u201d certain costs by demanding proofs of effort from them.", "num_citations": "25\n", "authors": ["504"]}
{"title": "Historic integrity in distributed systems\n", "abstract": " In an all-digital, all-online setting, long-term secure record keeping is a difficult task. The record-keeping problem comes up with increasing frequency, as we migrate to exclusively digital ways of transacting business. Accountability requires information about the content and the timing of business transactions. In the digital world, ideally, we should be able to tell with conviction when a \u201cdigital event\u201d occurred with respect to other events\u2014such as storing a purchase receipt on a hard drive or signing a contract digitally\u2014and we should be able to avert tampering with events that have been committed to history.", "num_citations": "23\n", "authors": ["504"]}
{"title": "Attrition defenses for a peer-to-peer digital preservation system\n", "abstract": " In peer-to-peer systems, attrition attacks include both traditional, network-level denial of service attacks as well as application-level attacks in which malign peers conspire to waste loyal peers' resources. We describe several defenses for the LOCKSS peer-to-peer digital preservation system that help ensure that application-level attrition attacks even from powerful adversaries are less effective than simple network-level attacks, and that network-level attacks must be intense, widespread, and prolonged to impair the system.", "num_citations": "22\n", "authors": ["504"]}
{"title": "Mantis: Efficient predictions of execution time, energy usage, memory usage and network usage on smart mobile devices\n", "abstract": " We present Mantis, a framework for predicting the computational resource consumption (CRC) of Android applications on given inputs accurately, and efficiently. A key insight underlying Mantis is that program codes often contain features that correlate with performance and these features can be automatically computed efficiently. Mantis synergistically combines techniques from program analysis and machine learning. It constructs concise CRC models by choosing from many program execution features only a handful that are most correlated with the program's CRC metric yet can be evaluated efficiently from the program's input. We apply program slicing to reduce evaluation time of a feature and automatically generate executable code snippets for efficiently evaluating features. Our evaluation shows that Mantis predicts four CRC metrics of seven Android apps with estimation error in the range of 0-11.1 percent by\u00a0\u2026", "num_citations": "18\n", "authors": ["504"]}
{"title": "Mantis: Predicting system performance through program analysis and modeling\n", "abstract": " We present Mantis, a new framework that automatically predicts program performance with high accuracy. Mantis integrates techniques from programming language and machine learning for performance modeling, and is a radical departure from traditional approaches. Mantis extracts program features, which are information about program execution runs, through program instrumentation. It uses machine learning techniques to select features relevant to performance and creates prediction models as a function of the selected features. Through program analysis, it then generates compact code slices that compute these feature values for prediction. Our evaluation shows that Mantis can achieve more than 93% accuracy with less than 10% training data set, which is a significant improvement over models that are oblivious to program features. The system generates code slices that are cheap to compute feature values.", "num_citations": "17\n", "authors": ["504"]}
{"title": "Design considerations for information planes\n", "abstract": " The concept of an information plane has emerged recently as an important part of large, decentralized systems that aspire to be self-managing, ranging from PlanetLab to the Internet itself. In this paper we describe what an information plane is, and report our experiences in developing and deploying an information plane for the PlanetLab platform using the PIER distributed relational query processor. We recount the lessons we have learned from the experience, and the additional directions we intend to explore in the PHI project, which aims at providing an information plane that can grow to serve a significant portion of the Internet.", "num_citations": "17\n", "authors": ["504"]}
{"title": "Making programs forget: enforcing lifetime for sensitive data\n", "abstract": " This paper introduces guaranteed data lifetime, a novel system property ensuring that sensitive data cannot be retrieved from a system beyond a specified time. The trivial way to achieve this is to \u201creboot\u201d; however, this is disruptive from the user\u2019s perspective, and may not even eliminate disk copies. We discuss an alternate approach based on state re-incarnation where data expiry is completely transparent to the user, and can be used even if the system is not designed a priori to provide the property.", "num_citations": "16\n", "authors": ["504"]}
{"title": "Tiered fault tolerance for long-term integrity\n", "abstract": " Fault-tolerant services typically make assumptions about the type and maximum number of faults that they can tolerate while providing their correctness guarantees; when such a fault threshold is violated, correctness is lost. We revisit the notion of fault thresholds in the context of long-term archival storage. We observe that fault thresholds are inevitably violated in longterm services, making traditional fault tolerance inapplicable to the long-term. In this work, we undertake a \u201creallocation of the fault-tolerance budget\u201d of a long-term service. We split the service into service pieces, each of which can tolerate a different number of faults without failing (and without causing the whole service to fail): each piece can be either in a critical trusted fault tier, which must never fail, or an untrusted fault tier, which can fail massively and often, or other fault tiers in between. By carefully engineering the split of a long-term service into pieces that must obey distinct fault thresholds, we can prolong its inevitable demise. We demonstrate this approach with Bonafide, a long-term key-value store that, unlike all similar systems proposed in the literature, maintains integrity in the face of Byzantine faults without requiring self-certified data.", "num_citations": "16\n", "authors": ["504"]}
{"title": "Authenticated Append-only Skip Lists\n", "abstract": " In this work we describe, design and analyze the security of a tamper-evident, append-only data structure for maintaining secure data sequences in a loosely coupled distributed system where individual system components may be mutually distrustful. The resulting data structure, called an Authenticated Append-Only Skip List (AASL), allows its maintainers to produce one-way digests of the entire data sequence, which they can publish to others as a commitment on the contents and order of the sequence. The maintainer can produce efficiently succinct proofs that authenticate a particular datum in a particular position of the data sequence against a published digest. AASLs are secure against tampering even by malicious data structure maintainers. First, we show that a maintainer cannot ``invent'' and authenticate data elements for the AASL after he has committed to the structure. Second, he cannot equivocate by being able to prove conflicting facts about a particular position of the data sequence. This is the case even when the data sequence grows with time and its maintainer publishes successive commitments at times of his own choosing. AASLs can be invaluable in reasoning about the integrity of system logs maintained by untrusted components of a loosely-coupled distributed system.", "num_citations": "16\n", "authors": ["504"]}
{"title": "Conflict-free quorum-based bft protocols\n", "abstract": " Quorum-based Byzantine fault-tolerant protocols for replicated state machines allow replicas to respond to client requests without explicitly agreeing on the request ordering. As long as concurrent write operations do not conflict, quorum-based protocols are more efficient than agreement-based protocols. However, resolving conflicting writes and bringing replicas up-to-date with each other is a principal performance limitation of existing quorum protocols. We present a simple technique based on an un-trusted pre-serializer to completely mask such quorum-based BFT protocols from experiencing conflicting writes. Experimental results show that a non-faulty pre-serializer enables such quorum protocols to retain their efficiency even under significant write contention.", "num_citations": "14\n", "authors": ["504"]}
{"title": "Maelstrom: Churn as shelter\n", "abstract": " Structured overlays are an important and powerful class of overlay networks that has emerged in recent years. They are typically targeted at peer-to-peer deployments involving millions of user-managed machines on the Internet. In this paper we address routing-table poisoning attacks against structured overlays, in which adversaries attempt to intercept traffic and control the system by convincing other nodes to use compromised nodes as their overlay network neighbors. In keeping with the fully-decentralized goals of structured overlay design, we propose a defense mechanism that makes minimal use of centralized infrastructure. Our approach, induced churn, utilizes periodic routing-table resets, unpredictable identifier changes, and a rate limit on routing-table updates. Induced churn leaves adversaries at the mercy of chance: they have little opportunity to strategize their positions in the overlay, and cannot entrench themselves in any position that they do acquire. We implement induced churn in Maelstrom, an extension to the broadly used Bamboo distributed hash table. Our Maelstrom experiments over a simulated network demonstrate robust routing with very modest costs in bandwidth and latency, at levels of adversarial activity where unprotected overlays are rendered almost completely useless.", "num_citations": "14\n", "authors": ["504"]}
{"title": "Public Health for the Internet \u03c6 Towards A New Grand Challenge for Information Management\n", "abstract": " Business incentives have brought us within a small factor of achieving the database community's Grand Challenge set out in the Asilomar Report of 1998. This paper makes the case for a new, focused Grand Challenge: Public Health for the Internet. The goal of PHI (or \u03c6) is to enable collectives of hosts on the Internet to jointly monitor and promote network health by sharing information on network conditions in a peer-to-peer fashion. We argue that this will be a positive effort for the research community for a variety of reasons, both in terms of its technical reach and its societal impact.", "num_citations": "10\n", "authors": ["504"]}
{"title": "Glimmers: Resolving the Privacy/Trust Quagmire\n", "abstract": " Users today enjoy access to a wealth of services that rely on user-contributed data, such as recommendation services, prediction services, and services that help classify and interpret data. The quality of such services inescapably relies on trustworthy contributions from users. However, validating the trustworthiness of contributions may rely on privacy-sensitive contextual data about the user, such as a user's location or usage habits, creating a conflict between privacy and trust: users benefit from a higher-quality service that identifies and removes illegitimate user contributions, but, at the same time, they may be reluctant to let the service access their private information to achieve this high quality.", "num_citations": "9\n", "authors": ["504"]}
{"title": "Declarative networking with distributed recursive query processing\n", "abstract": " There have been recent proposals in the networking and distributed systems literature on declarative networking, where network protocols are declaratively specified using a recursive query language. This represents a significant new application area for recursive query processing technologies from databases. In this paper, we extend upon these recent proposals in the following ways. First, we motivate and formally define the NDlog language for declarative network specifications. We introduce the concept of link-restricted rules, which can be syntactically guaranteed to be executable via single-node derivations and message passing on an underlying network graph. Second, we introduce and prove correct relaxed versions of the traditional semi-naive execution technique that overcome fundamental problems of traditional semi-na\u0131ve evaluation in an asynchronous distributed setting. Third, we consider the dynamics of network state, and formalize the \u201ceventual consistency\u201d of our programs even when bursts of updates can arrive in the midst of query execution. Fourth, we present a number of query optimization opportunities that arise in the declarative networking context, including applications of traditional techniques and new optimizations. Last, we present evaluation results based on an implementation of the above ideas in the P2 declarative networking system, running on 100 machines over the Emulab network testbed.", "num_citations": "8\n", "authors": ["504"]}
{"title": "Identiscape: Tackling the personal online identity crisis\n", "abstract": " Traditional systems refer to a mobile person using the name or address of that person\u2019s communication device. As personal communications become more diverse and popular, this solution is no longer adequate, since mobile people frequently move between different devices and use different communications applications. This lack of identifiers for mobile people causes problems ranging from the inconvenient to the downright dangerous: to locate a person, callers must use potentially multiple email addresses, cell phone numbers, land line phone numbers or instant messaging IDs; callers leave sensitive messages on shared voicemail boxes; and they send communications intended for the previous owner of a telephone number to the next owner. To solve this naming problem, we should be able to name people as the ultimate endpoints of personal communications, regardless of the applications or devices they use.In this paper, we develop a naming scheme for mobile people: we derive its requirements and describe its design and implementation in the context of personal communications. IdentiScape, our prototype personal naming scheme, includes a name service which provides globally available identifiers that persist over time and an online identity repository service which can be locally owned and managed.", "num_citations": "8\n", "authors": ["504"]}
{"title": "Verification with Small and Short Worlds\n", "abstract": " We consider the verification of safety properties in systems with large arrays and data structures. Such systems are common at the low levels of software stacks; examples are hypervisors and CPU emulators. The very large data structures in such systems (e.g., address-translation tables and other caches) make automated verification based on straightforward statespace exploration infeasible. We present S 2 W, a new abstraction-based model-checking methodology to facilitate automated verification of such systems. As a first step, inductive invariant checking is performed. If that fails, we compute an abstraction of the original system by precisely modeling only a subset of state variables while allowing the rest of the state to evolve arbitrarily at each step. This subset of the state constitutes a \u201csmall world\u201d hypothesis, and is extracted from the property. Finally, we verify the safety property on the abstract model using\u00a0\u2026", "num_citations": "7\n", "authors": ["504"]}
{"title": "Notes on the Design of an Internet Adversary\n", "abstract": " The design of the defenses Internet systems can deploy against attack, especially adaptive and resilient defenses, must start from a realistic model of the threat. This requires an assessment of the capabilities of the adversary. The design typically evolves through a process of simulating both the system and the adversary. This requires the design and implementation of a simulated adversary based on the capability assessment. Consensus on the capabilities of a suitable adversary is not evident. Part of the recent redesign of the protocol used by peers in the LOCKSS digital preservation system included a conservative assessment of the adversary's capabilities. We present our assessment and the implications we drew from it as a step towards a reusable adversary specification.", "num_citations": "7\n", "authors": ["504"]}
{"title": "Using hard disks for digital preservation\n", "abstract": " The LOCKSS system is a tool librarians can use to preserve long-term access to content published on the web. It has three main functions. It collects the content by crawling the pub-lisher's web sites, it distributes the content by acting as a proxy for reader's browsers, and it preserves the content through a cooperative process of damage detection and repair. The system uses the hard disk holding the copy used for access as a preservation medium; the cooperative damage detection and repair mechanism eliminates the need for off-line backups on removable media. We describe the LOCKSS system as an example of the techniques needed to use hard disks as a medium for long-term preservation.", "num_citations": "7\n", "authors": ["504"]}
{"title": "Preserving Peer Replicas By Rate-Limited Sampled Voting in LOCKSS\n", "abstract": " The LOCKSS project has developed and deployed in a world-wide test a peer-to-peer system for preserving access to journals and other archival information published on the Web. It consists of a large number of independent, low-cost, persistent web caches that cooperate to detect and repair damage to their content by voting in \"opinion polls.\" Based on this experience, we present a design for and simulations of a novel protocol for voting in systems of this kind. It incorporates rate limitation and intrusion detection to ensure that even some very powerful adversaries attacking over many years have only a small probability of causing irrecoverable damage before being detected.", "num_citations": "7\n", "authors": ["504"]}
{"title": "Building Trusted Distributed Services Across Administrative Domains\n", "abstract": " In this paper we describe how to build a trusted reliable distributed service across administrative domains in a peer-to-peer network. The service we use to motivate our work is a public key time stamping service called Prokopius. The service provides a secure, verifible but distributable stable archive that maintains time stamped snapshots of public keys over time. This in turn allows clients to verify time stamped documents or certificates that rely on formerly trusted public keys that are no longer in service or where the signer no longer exists. We find that such a service can time stamp the snapshots of public keys in a network of 148 nodes at the granularity of a couple of days, even in the worst case where an adversary causes the maximal amount of damage allowable within our fault model.", "num_citations": "6\n", "authors": ["504"]}
{"title": "Symbolic software model validation\n", "abstract": " Modeling is the crucial first step in formal verification. Some models are constructed by humans from source code, while others are extracted automatically by tools. Regardless of how a model is constructed, verification is only as good as the model; therefore, it is essential to validate the model against the implementation it represents. In this paper we present two complementary approaches to software model validation. The first, data-centric model validation, checks that, for data structures relevant to the property being verified, all operations that update these data structures are captured in the model. The second, operation-centric model validation, checks that each operation being modeled is correctly simulated by the model. Both techniques are based on a combination of symbolic execution and satisfiability modulo theories (SMT) solving. We demonstrate the application of our methods on several case studies\u00a0\u2026", "num_citations": "5\n", "authors": ["504"]}
{"title": "Resisting Attrition Attacks on a Peer-to-Peer System\n", "abstract": " Peer-to-peer systems are vulnerable to attrition attacks that include both traditional, network-level denial of service attacks as well as application-level attacks in which malign peers conspire to waste loyal peers\u2019 resources. We describe a set of defenses for the LOCKSS digital preservation system that help ensure that applicationlevel attacks even from powerful adversaries are less effective than network-level attacks, and that network-level attacks must be intense, wide-spread, and prolonged to impair the system.", "num_citations": "5\n", "authors": ["504"]}
{"title": "A Data Capsule Framework For Web Services: Providing Flexible Data Access Control To Users\n", "abstract": " This paper introduces the notion of a secure data capsule, which refers to an encapsulation of sensitive user information (such as a credit card number) along with code that implements an interface suitable for the use of such information (such as charging for purchases) by a service (such as an online merchant). In our capsule framework, users provide their data in the form of such capsules to web services rather than raw data. Capsules can be deployed in a variety of ways, either on a trusted third party or the user's own computer or at the service itself, through the use of a variety of hardware or software modules, such as a virtual machine monitor or trusted platform module: the only requirement is that the deployment mechanism must ensure that the user's data is only accessed via the interface sanctioned by the user. The framework further allows an user to specify policies regarding which services or machines may host her capsule, what parties are allowed to access the interface, and with what parameters. The combination of interface restrictions and policy control lets us bound the impact of an attacker who compromises the service to gain access to the user's capsule or a malicious insider at the service itself.", "num_citations": "4\n", "authors": ["504"]}
{"title": "A historic name-trail service\n", "abstract": " We consider the mobility of personal online identifiers. People change the identifiers through which they are reachable online as they change jobs or residences or Internet service providers. This kind of personal mobility makes reaching people online error-prone. As people move, they do not always know who or what has cached their now obsolete identifiers so as to inform them of the move. Use of these old identifiers can cause delivery failure of important messages, or worse, may cause delivery of messages to unintended recipients. For example, a sensitive email message sent to my now obsolete work address at a former place of employment may reach my unfriendly former boss instead of me. We describe HINTS, a historic name-trail service. This service provides a persistent way to name willing participants online using today's transient online identifiers. HINTS accomplishes this by connecting together the\u00a0\u2026", "num_citations": "4\n", "authors": ["504"]}
{"title": "Defining weakly consistent Byzantine fault-tolerant services\n", "abstract": " We propose a specification for weak consistency in the context of a replicated service that tolerates Byzantine faults. We define different levels of consistency for the replies that can be obtained from such a service---we use a real world application that can currently only tolerate crash faults to exemplify the need for such consistency guarantees.", "num_citations": "3\n", "authors": ["504"]}
{"title": "Proof sketches: Verifiable multi-party aggregation\n", "abstract": " Recent work on distributed aggregation has assumed a benign population of participants. In modern distributed systems, it is now necessary to account for adversarial behavior. In this paper we consider the problem of ensuring verifiable yet efficient results to typical aggregation queries in a distributed, multi-party setting. We describe a general framework for the problem, including the threat model for adversaries that we consider. We then present a mechanism called a proof sketch, which uses a compact combination of cryptographic signatures and Flajolet-Martin sketches to verify that a query answer is within acceptable error bounds with high probability. When verification fails, we provide efficient mechanisms to identify any participants responsible for the perturbed result. We derive proof sketches for count aggregates, and extend them to proof sketches for verifiable random samples, which, in turn, can be used to provide verifiable approximations for a broad class of data-analysis queries, including quantiles and heavy hitters. In addition to our specific proof sketches developed here, we sketch a general framework for developing new proof sketches. Finally, we examine the practical use of proof sketches, and observe that adversaries can often be reduced to much smaller violations in practice than our worst-case bounds suggest.", "num_citations": "3\n", "authors": ["504"]}
{"title": "Induced churn as shelter from routing-table poisoning\n", "abstract": " Structured overlays are an important and powerful class of overlay networks that has emerged in recent years. They are typically targeted at peer-to-peer deployments involving millions of user-managed machines on the Internet. In this paper we address routing-table poisoning attacks against structured overlays, in which adversaries attempt to intercept traffic and control the system by convincing other nodes to use compromised nodes as their overlay network neighbors. In keeping with the fully-decentralized goals of structured overlay design, we propose a defense mechanism that makes minimal use of centralized infrastructure. Our approach, induced churn, utilizes periodic routing-table resets, unpredictable identifier changes, and a rate limit on routing-table updates. Induced churn leaves adversaries at the mercy of chance: they have little opportunity to strategize their positions in the overlay, and cannot entrench themselves in any position that they do acquire. We implement induced churn in Maelstrom, an extension to the broadly used Bamboo distributed hash table. Our Maelstrom experiments over a simulated network demonstrate robust routing with very modest costs in bandwidth and latency, at levels of adversarial activity where unprotected overlays are rendered almost completely useless.", "num_citations": "3\n", "authors": ["504"]}
{"title": "MOMMIE knows best: systematic optimizations for verifiable distributed algorithms\n", "abstract": " Complex distributed algorithms become running systems through an integration with optimizations that target the system\u2019s deployment environment. Although expedient, this approach has disadvantages. First, this often makes implementing the algorithm difficult, since its logic must be composed with the optimizations. Second, proving the guarantees of the implementation is tedious, because the proofs must be derived for the composed algorithm, which may not be directly mappable to the original, unoptimized algorithm. Finally, retargeting the implementation to a different deployment\u2014requiring a different set of optimizations\u2014can be wasteful, since a new composed algorithm must be derived to include this different set of optimizations, including their correctness proofs. We fault the tussle between abstraction and performance as the fundamental cause for this problem. On one hand, algorithm designers need abstraction to simplify the job of identifying and proving invariants or liveness properties about their algorithms. On the other hand, algorithm implementers have faced an age-old conviction (grounded in at least some truth) that abstraction hurts performance, which for the complexity and latencies involved in distributed algorithms often leads to unusability. This trend is evident in the level of detail present in the pseudo-code descriptions of many replicated systems. For example, the PBFT system [4], a replicated state machine based on Byzantine consensus, gives an I/O Automaton specification\u2014arguably a clean formalism intended for proving properties\u2014that nevertheless contains explicit details about cryptographic tools (digital signatures\u00a0\u2026", "num_citations": "2\n", "authors": ["504"]}
{"title": "The Many Faces of Systems Research-and How to Evaluate Them.\n", "abstract": " Improper evaluation of systems papers may result in the loss or delay in publication of possibly important research. This paper posits that systems research papers may be evaluated in one or more of the three dimensions of science, engineering and art. Examples of these dimensions are provided, and methods for evaluating papers based on these dimensions are suggested. In the dimension of science, papers can be judged by how well they actually follow the scientific method, and by the inclusion of proofs or statistical measures of the significance of results. In engineering, the applicability and utility of the research in solving real world problems is the main metric. Finally, we argue that art be considered as a paper category evaluated based on elegance, simplicity, and beauty.", "num_citations": "2\n", "authors": ["504"]}
{"title": "The many faces of systems research: And how to evaluate them\n", "abstract": " Improper evaluation of systems papers may result in the loss or delay in publication of possibly important research. This paper posits that systems research papers may be evaluated in one or more of the three dimensions of science, engineering and art. Examples of these dimensions are provided, and methods for evaluating papers based on these dimensions are suggested. In the dimension of science, papers can be judged by how well they actually follow the scientific method, and by the inclusion of proofs or statistical measures of the significance of results. In engineering, the applicability and utility of the research in solving real world problems is the main metric. Finally, we argue that art be considered as a paper category evaluated based on elegance, simplicity, and beauty.", "num_citations": "2\n", "authors": ["504"]}
{"title": "Small trusted primitives for dependable systems\n", "abstract": " Secure, fault-tolerant distributed systems are difficult to build, to validate, and to operate. Conservative design for such systems dictates that their security and fault tolerance depend on a very small number of assumptions taken on faith; such assumptions are typically called the \"trusted computing base\" (TCB) of a system. However, a rich trade-off exists between larger TCBs and more secure, more faulttolerant, or more efficient systems. In our recent work, we have explored this trade-off by defining \"small,\" generic trusted primitives--for example, an attested, monotonically sequenced FIFO buffer of a few hundred machine words guaranteed to hold appended words until eviction and showing how such primitives can improve the performance, fault tolerance, and security of systems using them. In this article, we review our efforts in generating simple trusted primitives such as an attested circular buffer (called Attested\u00a0\u2026", "num_citations": "1\n", "authors": ["504"]}
{"title": "Auto-Parallelization for Declarative Network Monitoring\n", "abstract": " As distributed computing environments become progressively more complex and dynamic, network monitoring has become increasingly challenging. A typical network requires several monitoring applications performing non-trivial computations with different time and space requirements. The complexity of these applications demands consideration of how to execute efficiently over available resources. Declarative languages offer an attractive alternative to traditional implementations, and open the possibility for static program analysis to exploit opportunities for concurrency. We explore a language-based approach for achieving auto-parallelism through the use of static analysis, and sketch an implementation strategy in the context of the P2 declarative networking system.", "num_citations": "1\n", "authors": ["504"]}