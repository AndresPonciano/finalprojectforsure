{"title": "A practical model for measuring maintainability\n", "abstract": " The amount of effort needed to maintain a software system is related to the technical quality of the source code of that system. The ISO 9126 model for software product quality recognizes maintainability as one of the 6 main characteristics of software product quality, with adaptability, changeability, stability, and testability as subcharacteristics of maintainability. Remarkably, ISO 9126 does not provide a consensual set of measures for estimating maintainability on the basis of a system's source code. On the other hand, the maintainability index has been proposed to calculate a single number that expresses the maintainability of a system. In this paper, we discuss several problems with the MI, and we identify a number of requirements to be fulfilled by a maintainability model to be usable in practice. We sketch a new maintainability model that alleviates most of these problems, and we discuss our experiences with\u00a0\u2026", "num_citations": "411\n", "authors": ["1130"]}
{"title": "Deriving metric thresholds from benchmark data\n", "abstract": " A wide variety of software metrics have been proposed and a broad range of tools is available to measure them. However, the effective use of software metrics is hindered by the lack of meaningful thresholds. Thresholds have been proposed for a few metrics only, mostly based on expert opinion and a small number of observations. Previously proposed methodologies for systematically deriving metric thresholds have made unjustified assumptions about the statistical properties of source code metrics. As a result, the general applicability of the derived thresholds is jeopardized. We designed a method that determines metric thresholds empirically from measurement data. The measurement data for different software systems are pooled and aggregated after which thresholds are selected that (i) bring out the metric's variability between systems and (ii) help focus on a reasonable percentage of the source code volume\u00a0\u2026", "num_citations": "256\n", "authors": ["1130"]}
{"title": "An empirical model of technical debt and interest\n", "abstract": " Cunningham introduced the metaphor of technical debt as guidance for software developers that must trade engineering quality against short-term goals.", "num_citations": "192\n", "authors": ["1130"]}
{"title": "Standardized code quality benchmarking for improving software maintainability\n", "abstract": " We provide an overview of the approach developed by the Software Improvement Group for code analysis and quality consulting focused on software maintainability. The approach uses a standardized measurement model based on the ISO/IEC 9126 definition of maintainability and source code metrics. Procedural standardization in evaluation projects further enhances the comparability of results. Individual assessments are stored in a repository that allows any system at hand to be compared to the industry-wide state of the art in code quality and maintainability. When a minimum level of software maintainability is reached, the certification body of T\u00dcV Informationstechnik GmbH issues a Trusted Product Maintainability certificate for the software product.", "num_citations": "189\n", "authors": ["1130"]}
{"title": "Visitor combination and traversal control\n", "abstract": " The Visitor design pattern allows the encapsulation of polymorphic behavior outside the class hierarchy on which it operates. A common application of Visitor is the encapsulation of tree traversals. A clean separation can be made between the generic parts of the combinator set and the parts that are specific to a particular class hierarchy. The generic parts form a reusable framework. The generic parts form a reusable framework. The specific parts can be generated from a (tree) grammar. Due to this separation, programming with visitor combinators becomes a form of generic programming with significant reuse of (visitor) code.", "num_citations": "147\n", "authors": ["1130"]}
{"title": "Typed combinators for generic traversal\n", "abstract": " Lacking support for generic traversal, functional programming languages suffer from a scalability problem when applied to largescale program transformation problems. As a solution, we introduce functional strategies: typeful generic functions that not only can be applied to terms of any type, but which also allow generic traversal into subterms. We show how strategies are modelled inside a functional language, and we present a combinator library including generic traversal combinators. We illustrate our technique of programming with functional strategies by an implementation of the extract method refactoring for Java.", "num_citations": "131\n", "authors": ["1130"]}
{"title": "A Strafunski application letter\n", "abstract": " Strafunski is a Haskell-centred software bundle for implementing language processing components\u2014most notably program analyses and transformations. Typical application areas include program optimisation, refactoring, software metrics, software re- and reverse engineering.               Strafunski started out as generic programming library complemented by generative tool support to address the concern of generic traversal over typed representations of parse trees in a scalable manner. Meanwhile, Strafunski also encompasses means of integrating external components such as parsers, pretty printers, and graph visualisation tools.               In a selection of case studies, we demonstrate that typed functional programming in Haskell, augmented with Strafunski \u2019s support for generic traversal and external components, is very appropriate for the development of practical language processors. In particular, we\u00a0\u2026", "num_citations": "122\n", "authors": ["1130"]}
{"title": "From spreadsheets to relational databases and back\n", "abstract": " This paper presents techniques and tools to transform spreadsheets into relational databases and back. A set of data refinement rules is introduced to map a tabular datatype into a relational database schema. Having expressed the transformation of the two data models as data refinements, we obtain for free the functions that migrate the data. We use well-known relational database techniques to optimize and query the data. Because data refinements define bi-directional transformations we can map such database back to an optimized spreadsheet. We have implemented the data refinement rules and we constructed Haskell-based tools to manipulate, optimize and refactor Excel-like spreadsheets.", "num_citations": "95\n", "authors": ["1130"]}
{"title": "Dealing with large bananas\n", "abstract": " . Many problems call for a mixture of generic and specific programming techniques. We propose a polytypic programming approach based on generalised (monadic) folds where a separation is made between basic fold algebras that model generic behaviour and updates on these algebras that model specific behaviour. We identify particular basic algebras as well as some algebra combinators, and we show how these facilitate structured programming with updatable fold algebras. This blend of genericity and specificity allows programming with folds to scale up to applications involving large systems of mutually recursive datatypes. Finally, we address the possibility of providing generic definitions for the functions, algebras, and combinators that we propose. 1 Introduction Polytypic programming [JJ97, Hin00] aims at relieving the programmer from repeatedly writing functions of similar functionality for different user-defined datatypes. For example, for any datatype parametric in ff,crus...", "num_citations": "72\n", "authors": ["1130"]}
{"title": "Building Maintainable Software, C# Edition: Ten Guidelines for Future-Proof Code\n", "abstract": " Have you ever felt frustrated working with someone else\u2019s code? Difficult-to-maintain source code is a big problem in software development today, leading to costly delays and defects. Be part of the solution. With this practical book, you\u2019ll learn 10 easy-to-follow guidelines for delivering C# software that\u2019s easy to maintain and adapt. These guidelines have been derived from analyzing hundreds of real-world systems. Written by consultants from the Software Improvement Group (SIG), this book provides clear and concise explanations, with advice for turning the guidelines into practice. Examples for this edition are written in C#, while our companion Java book provides clear examples in that language. Write short units of code: limit the length of methods and constructors Write simple units of code: limit the number of branch points per method Write code once, rather than risk copying buggy code Keep unit interfaces small by extracting parameters into objects Separate concerns to avoid building large classes Couple architecture components loosely Balance the number and size of top-level components in your code Keep your codebase as small as possible Automate tests for your codebase Write clean code, avoiding\" code smells\" that indicate deeper problems", "num_citations": "59\n", "authors": ["1130"]}
{"title": "Seflab: A lab for measuring software energy footprints\n", "abstract": " Hardware dissipates energy because software tells it to. But attributing hardware energy usage to particular software functions is complicated due to distribution, resource sharing, and layering of software. To enable research on energy usage attribution, we have created the Software Energy Footprint Lab. We explain the experimental setup offered by the lab and the measurement and analysis methodology that it supports. We also describe some preliminary results aimed at deciphering hardware dissipation profiles for various types of servers under various forms of software stress. Finally, we provide an outlook of how energy footprint measurements can contribute to a body of knowledge on software-level energy optimization.", "num_citations": "58\n", "authors": ["1130"]}
{"title": "Type-safe two-level data transformation\n", "abstract": " A two-level data transformation consists of a type-level transformation of a data format coupled with value-level transformations of data instances corresponding to that format. Examples of two-level data transformations include XML schema evolution coupled with document migration, and data mappings used for interoperability and persistence.               We provide a formal treatment of two-level data transformations that is type-safe in the sense that the well-formedness of the value-level transformations with respect to the type-level transformation is guarded by a strong type system. We rely on various techniques for generic functional programming to implement the formalization in Haskell.               The formalization addresses various two-level transformation scenarios, covering fully automated as well as user-driven transformations, and allowing transformations that are information-preserving or not. In each\u00a0\u2026", "num_citations": "56\n", "authors": ["1130"]}
{"title": "Measuring dependency freshness in software systems\n", "abstract": " Modern software systems often make use of third-party components to speed-up development and reduce maintenance costs. In return, developers need to update to new releases of these dependencies to avoid, for example, security and compatibility risks. In practice, prioritizing these updates is difficult because the use of outdated dependencies is often opaque. In this paper we aim to make this concept more transparent by introducing metrics to quantify the use of recent versions of dependencies, i.e. The system's \"dependency freshness\". We propose and investigate a system-level metric based on an industry benchmark. We validate the usefulness of the metric using interviews, analyze the variance of the metric through time, and investigate the relationship between outdated dependencies and security vulnerabilities. The results show that the measurements are considered useful, and that systems using\u00a0\u2026", "num_citations": "55\n", "authors": ["1130"]}
{"title": "Faster issue resolution with higher technical quality of software\n", "abstract": " We performed an empirical study of the relation between technical quality of software products and the issue resolution performance of their maintainers. In particular, we tested the hypothesis that ratings for source code maintainability, as employed by the Software Improvement Group (SIG) quality model, are correlated with ratings for issue resolution speed. We tested the hypothesis for issues of type defect and of type enhancement. This study revealed that all but one of the metrics of the SIG quality model show a significant positive correlation with the resolution speed of defects, enhancements, or both.", "num_citations": "54\n", "authors": ["1130"]}
{"title": "Benchmark-based aggregation of metrics to ratings\n", "abstract": " Software metrics have been proposed as instruments, not only to guide individual developers in their coding tasks, but also to obtain high-level quality indicators for entire software systems. Such system-level indicators are intended to enable meaningful comparisons among systems or to serve as triggers for a deeper analysis.Common methods for aggregation range from simple mathematical operations (e.g. addition and central tendency) to more complex methodologies such as distribution fitting, wealth inequality metrics (e.g. Gini coefficient and Theil Index) and custom formulae.However, these methodologies provide little guidance for interpreting the aggregated results or to trace back to individual measurements.To resolve such limitations, a two-stage rating approach has been proposed where (i) measurement values are compared to thresholds to summarize them into risk profiles, and (ii) risk profiles are\u00a0\u2026", "num_citations": "49\n", "authors": ["1130"]}
{"title": "Quality assessment for embedded SQL\n", "abstract": " The access of information systems to underlying relational databases is commonly programmed using embedded SQL queries. Such embedded queries may take the form of string literals that are programmatically concatenated into queries to be submitted to the DBMS, or they may be written in a mixture of the syntax of SQL and a host programming language. The particular ways in which embedded queries are constructed and intertwined with the surrounding code can have significant impact on the understandability, testability, adaptability, and other quality aspects of the overall system. We present an approach to tool-based analysis of the quality of systems that employ embedded SQL queries. The basis of the approach is the identification and reconstruction of embedded queries. These queries are then submitted to a variety of analyses. For example, we chart the relationships of queries to the surrounding code\u00a0\u2026", "num_citations": "48\n", "authors": ["1130"]}
{"title": "Strongly typed rewriting for coupled software transformation\n", "abstract": " Coupled transformations occur in software evolution when multiple artifacts must be modified in such a way that they remain consistent with each other. An important example involves the coupled transformation of a data type, its instances, and the programs that consume or produce it. Previously, we have provided a formal treatment of transformation of the first two: data types and instances. The treatment involved the construction of type-safe, type-changing strategic rewrite systems. In this paper, we extend our treatment to the transformation of corresponding data processing programs.The key insight underlying the extension is that both data migration functions and data processors can be represented type-safely by a generalized abstract data type (GADT). These representations are then subjected to program calculation rules, harnessed in type-safe, type-preserving strategic rewrite systems. For ease of\u00a0\u2026", "num_citations": "48\n", "authors": ["1130"]}
{"title": "A survey-based study of the mapping of system properties to ISO/IEC 9126 maintainability characteristics\n", "abstract": " The ISO/IEC 9126 international standard for software product quality is a widely accepted reference for terminology regarding the multi-faceted concept of software product quality. Based on this standard, the Software Improvement Group has developed a pragmatic approach for measuring technical quality of software products. This quality model introduces another level below the hierarchy defined by ISO/IEC 9126, which consists of system properties such as volume, duplication, unit complexity and others. A mapping between system properties and ISO/IEC 9126 characteristics is defined in a binary fashion: a property either influences a characteristic or not. This mapping embodies consensus among three experts based, in an informal way, on their experience in software quality assessment. We have conducted a survey-based experiment to study the mapping between system properties and quality\u00a0\u2026", "num_citations": "46\n", "authors": ["1130"]}
{"title": "Structure metrics for XML Schema\n", "abstract": " XML schemas are software artifacts claiming an increasingly central role in software construction projects. Schemas are used as interface definitions, data models, protocol specifications, and more. Standards bodies are employing schemas for standards definition and dissemination. Using code generators that accept schemas as input, software components are generated for data interchange and persistence. With increased reliance on schemas comes the necessity of properly embedding these artifacts in the software engineering process. In particular, schema metrics must be developed to enable quantification of schema size, complexity, quality, and other properties, instrumental to retaining control over the software processes in which they are involved. In this paper, we propose a suite of metrics for the XML Schema language that measure structural properties. The metrics are mostly adaptations of existing metrics for other software artifacts, such as programs and grammars. Apart from definitions of the metrics, we report on application of these metrics to a series of open source schemas, using our XsdMetz tool. We suggest how the measurement results may be used to assess potential risks in schemas.", "num_citations": "45\n", "authors": ["1130"]}
{"title": "The essence of strategic programming\u2013an inquiry into trans-paradigmatic genericity\n", "abstract": " Strategic programming is an idiom for generic programming where the concept of a strategy plays a central role. A strategy is a generic, dataprocessing action. Strategies are first-class citizens as witnessed by a combinator style. Two important characteristics of strategies are that they can traverse into compound data, and that they can be customized by type-specific actions. We provide a general definition of strategic programming, and we demonstrate how this idiom can be realized inside several programming language paradigms.", "num_citations": "45\n", "authors": ["1130"]}
{"title": "Grammars as contracts\n", "abstract": " Component-based development of language tools stands in need of meta-tool support. This support can be offered by generation of code \u2014 libraries or full-fledged components \u2014 from syntax definitions. We develop a comprehensive architecture for such syntax-driven meta-tooling in which grammars serve as contracts between components. This architecture addresses exchange and processing both of full parse trees and of abstract syntax trees, and it caters for the integration of generated parse and pretty-print components with tree processing components.               We discuss an instantiation of the architecture for the syntax definition formalism SDF, integrating both existing and newly developed meta-tools that support SDF. The ATerm format is adopted as exchange format. This instantiation gives special attention to adaptability, scalability, reusability, and maintainability issues surrounding language tool\u00a0\u2026", "num_citations": "44\n", "authors": ["1130"]}
{"title": "An empirical study into social success factors for agile software development\n", "abstract": " Though many warn that Agile at larger scale is problematic or at least more challenging than in smaller projects, Agile software development seems to become the norm, also for large and complex projects. Based on literature and qualitative interviews, we constructed a conceptual model of social factors that may be of influence on the success of software development projects in general, and of Agile projects in particular. We also included project size as a candidate success factor. We tested the model on a set of 40 projects from 19 Dutch organizations, comprising a total of 141 project members, Scrum Masters and product owners. We found that project size does not determine Agile project success. Rather, value congruence, degree of adoption of Agile practices, and transformational leadership proved to be the most important predictors for Agile project success.", "num_citations": "42\n", "authors": ["1130"]}
{"title": "Coupled schema transformation and data conversion for XML and SQL\n", "abstract": " A two-level data transformation consists of a type-level transformation of a data format coupled with value-level transformations of data instances corresponding to that format. We have implemented a system for performing two-level transformations on XML schemas and their corresponding documents, and on SQL schemas and the databases that they describe. The core of the system consists of a combinator library for composing type-changing rewrite rules that preserve structural information and referential constraints. We discuss the implementation of the system\u2019s core library, and of its SQL and XML front-ends in the functional language Haskell. We show how the system can be used to tackle various two-level transformation scenarios, such as XML schema evolution coupled with document migration, and hierarchical-relational data mappings that convert between XML documents and SQL databases.", "num_citations": "42\n", "authors": ["1130"]}
{"title": "Grammars as feature diagrams\n", "abstract": " Feature Diagrams (FDs) have been proposed to describe the configuration space of a software system at the problem level. They can also be used to describe the configuration space of the various components at the solution level. We demonstrate the correspondance of FDs to grammars, and we exploit this correspondance to generate solution configurations from problem configurations. To this end, we view configurations as parse trees, and we obtain a solution configuration by flattening this tree and re-parsing it with the solution grammar. The solution configuration is then fed to the autobundle tool to compose and configure a source tree from all required solution components.", "num_citations": "42\n", "authors": ["1130"]}
{"title": "The influence of teamwork quality on software team performance\n", "abstract": " Traditionally, software quality is thought to depend on sound software engineering and development methodologies such as structured programming and agile development. However, high quality software depends just as much on high quality collaboration within the team. Since the success rate of software development projects is low (Wateridge, 1995; The Standish Group, 2009), it is important to understand which characteristics of interactions within software development teams significantly influence performance. Hoegl and Gemuenden (2001) reported empirical evidence for the relation between teamwork quality and software quality, using a six-factor teamwork quality (TWQ) model. This article extends the work of Hoegl and Gemuenden (2001) with the aim of finding additional factors that may influence software team performance. We introduce three new TWQ factors: trust, value sharing, and coordination of expertise. The relationship between TWQ and team performance and the improvement of the model are tested using data from 252 team members and stakeholders. Results show that teamwork quality is significantly related to team performance, as rated by both team members and stakeholders: TWQ explains 81% of the variance of team performance as rated by team members and 61% as rated by stakeholders. This study shows that trust, shared values, and coordination of expertise are important factors for team leaders to consider in order to achieve high quality software team work.", "num_citations": "39\n", "authors": ["1130"]}
{"title": "Design patterns for functional strategic programming\n", "abstract": " We believe that design patterns can be an effective means of consolidating and communicating program construction expertise for functional programming, just as they have proven to be in object-oriented programming. The emergence of combinator libraries that develop a specific domain or programming idiom has intensified, rather than reduced, the need for design patterns. In previous work, we introduced the fundamentals and a supporting combinator library for functional strategic programming. This is an idiom for (general purpose) generic programming based on the notion of a functional strategy: a first-class generic function that can not only be applied to terms of any type, but which also allows generic traversal into subterms and can be customised with type-specific behaviour. This paper seeks to provide practising functional programmers with pragmatic guidance in crafting their own generic programs using\u00a0\u2026", "num_citations": "38\n", "authors": ["1130"]}
{"title": "Type-safe evolution of spreadsheets\n", "abstract": " Spreadsheets are notoriously error-prone. To help avoid the introduction of errors when changing spreadsheets, models that capture the structure and interdependencies of spreadsheets at a conceptual level have been proposed. Thus, spreadsheet evolution can be made safe within the confines of a model. As in any other model/instance setting, evolution may not only require changes at the instance level but also at the model level. When model changes are required, the safety of instance evolution can not be guarded by the model alone.               We have designed an appropriate representation of spreadsheet models, including the fundamental notions of formul\u00e6and references. For these models and their instances, we have designed coupled transformation rules that cover specific spreadsheet evolution steps, such as the insertion of columns in all occurrences of a repeated block of cells. Each model\u00a0\u2026", "num_citations": "37\n", "authors": ["1130"]}
{"title": "Certification of technical quality of software products\n", "abstract": " In this paper we propose a method for certification of technical quality of software products. The certification method employs a layered model of technical quality, based on the ISO/IEC 9126 international standard for software product quality. This model was developed in the context of software assessments conducted on commercial software systems over the course of several years. Using the layered quality model as a basis, we define a three-phase appraisal method that ends in certification of a software product at one of five possible levels. We illustrate the certification method by providing details of its application to twelve open source database management systems and five open source web servers.", "num_citations": "37\n", "authors": ["1130"]}
{"title": "A standard driven software architecture for fully autonomous vehicles\n", "abstract": " The goal of this paper is to design a functional software architecture for fully autonomous vehicles. Existing literature takes a descriptive approach and presents past experiments with autonomous driving or implementations specific to limited domains (e.g. winning a competition). The architectural solutions are often an after-math of building or evolving an autonomous vehicle and not the result of a clear software development life-cycle. A major issue of this approach is that requirements can not be traced with respect to functional components and several components group most functionality. Therefore, it is often difficult to adopt the proposals. In this paper we take a prescriptive approach starting with requirements from an automotive standard. We use a NIST reference architecture for real-time, intelligent, systems and well established architectural patterns to support the design principles. We further examine the\u00a0\u2026", "num_citations": "36\n", "authors": ["1130"]}
{"title": "Identification of application-level energy optimizations\n", "abstract": " Advances in the energy-efficiency of hardware and datacenters may be in vain if application software is developed without awareness of the resources that are consumed in operation. We introduce a pragmatic method for identifying energy-efficiency optimizations in software applications. The method involves the creation of an energy model of an application that allows calculation of energy savings for optimization scenarios. We discuss two industrial cases where such Green Software Scans were applied to an e-government and a mobile banking application. Among the lessons drawn from these cases are that significant energy savings can be realized with targeted modifications in software code, architecture, or configuration, that development of energy-efficient applications requires attention to detail throughout the development process, and that a close collaboration between operations and development is one of the main success factors for energy-efficient applications.", "num_citations": "34\n", "authors": ["1130"]}
{"title": "Faster defect resolution with higher technical quality of software\n", "abstract": " We performed an empirical study of the relation between technical quality of software products and the defect resolution performance of their maintainers. In particular, we tested the hypothesis that ratings for source code maintainability, as employed by the SIG quality model, are correlated with ratings for defect resolution speed. This study revealed that all but one of the metrics of the SIG quality model show a significant positive correlation.", "num_citations": "34\n", "authors": ["1130"]}
{"title": "Adversarial examples-a complete characterisation of the phenomenon\n", "abstract": " We provide a complete characterisation of the phenomenon of adversarial examples - inputs intentionally crafted to fool machine learning models. We aim to cover all the important concerns in this field of study: (1) the conjectures on the existence of adversarial examples, (2) the security, safety and robustness implications, (3) the methods used to generate and (4) protect against adversarial examples and (5) the ability of adversarial examples to transfer between different machine learning models. We provide ample background information in an effort to make this document self-contained. Therefore, this document can be used as survey, tutorial or as a catalog of attacks and defences using adversarial examples.", "num_citations": "33\n", "authors": ["1130"]}
{"title": "A case study in grammar engineering\n", "abstract": " This paper describes a case study about how well-established software engineering techniques can be applied to the development of a grammar. The employed development methodology can be described as iterative grammar engineering and includes the application of techniques such as grammar metrics, unit testing, and test coverage analysis. The result is a grammar of industrial strength, in the sense that it is well-tested, it can be used for fast parsing of high volumes of code, and it allows automatic generation of support for syntax tree representation, traversal, and interchange.", "num_citations": "33\n", "authors": ["1130"]}
{"title": "Discovery-based edit assistance for spreadsheets\n", "abstract": " Spreadsheets can be viewed as a highly flexible end-users programming environment which enjoys wide-spread adoption. But spreadsheets lack many of the structured programming concepts of regular programming paradigms. In particular, the lack of data structures in spreadsheets may lead spreadsheet users to cause redundancy, loss, or corruption of data during edit actions. In this paper, we demonstrate how implicit structural properties of spreadsheet data can be exploited to offer edit assistance to spreadsheet users. Our approach is based on the discovery of functional dependencies among data items which allow automatic reconstruction of a relational database schema. From this schema, new formulas and visual objects are embedded into the spreadsheet to offer features for auto-completion, guarded deletion, and controlled insertion. Schema discovery and spreadsheet enhancement are carried out\u00a0\u2026", "num_citations": "32\n", "authors": ["1130"]}
{"title": "Coupled transformation of schemas, documents, queries, and constraints\n", "abstract": " Coupled transformation occurs when multiple software artifacts must be transformed in such a way that they remain consistent with each other. For instance, when a database schema is adapted in the context of system maintenance, the persistent data residing in the system's database needs to be migrated to conform to the adapted schema. Also, queries embedded in the application code and any declared referential constraints must be adapted to take the schema changes into account. As another example, in XML-to-relational data mapping, a hierarchical XML Schema is mapped to a relational SQL schema with appropriate referential constraints, and the XML documents and queries are converted into relational data and relational queries. The 2LT project is aimed at providing a formal basis for coupled transformation. This formal basis is found in data refinement theory, point-free program calculation, and\u00a0\u2026", "num_citations": "32\n", "authors": ["1130"]}
{"title": "Transformation of structure-shy programs: applied to XPath queries and strategic functions\n", "abstract": " Various programming languages allow the construction of structure-shy programs. Such programs are defined generically for many different datatypes and only specify specific behavior for a few relevant subtypes. Typical examples are XML query languages that allow selection of subdocuments without exhaustively specifying intermediate element tags. Other examples are languages and libraries for polytypic or strategic functional programming and for adaptive object-oriented programming.", "num_citations": "32\n", "authors": ["1130"]}
{"title": "Adversarial Examples on Object Recognition: A Comprehensive Survey\n", "abstract": " Deep neural networks are at the forefront of machine learning research. However, despite achieving impressive performance on complex tasks, they can be very sensitive: Small perturbations of inputs can be sufficient to induce incorrect behavior. Such perturbations, called adversarial examples, are intentionally designed to test the network\u2019s sensitivity to distribution drifts. Given their surprisingly small size, a wide body of literature conjectures on their existence and how this phenomenon can be mitigated. In this article, we discuss the impact of adversarial examples on security, safety, and robustness of neural networks. We start by introducing the hypotheses behind their existence, the methods used to construct or protect against them, and the capacity to transfer adversarial examples between different machine learning models. Altogether, the goal is to provide a comprehensive and self-contained survey of this\u00a0\u2026", "num_citations": "31\n", "authors": ["1130"]}
{"title": "A Tool-based Methodology for Software Portfolio Monitoring.\n", "abstract": " We present a tool-based methodology for monitoring the development and maintenance performed on the software portfolio of a large company. The toolkit on which the methodology is based includes an extendable framework for software analysis and visualization that meets strong demands with respect to scalability and usability. The methodology consists of 3 nested iterations and is carried out by software engineers with very strong consultancy skills. The shortest iteration consists in applying the toolkit to the software portfolio to obtain and register basic facts such as metrics and dependencies. In the middle iteration, the engineers interpret and evaluate the newly registered facts. The findings are reported to IT management together with recommendations about how to react to the findings. In particular, one kind of recommendation is to carry out a Software Risk Assessment on a selected system or project. Finally, the longest iteration is the publication of an annual software report, which summarizes the monitoring results of the previous year.", "num_citations": "30\n", "authors": ["1130"]}
{"title": "What is the value of your software?\n", "abstract": " Assessment of the economic value of software systems is useful in contexts such as capitalization on the balance sheet and due diligence prior to acquisition. Current accounting practice in determining software value is based on the cost spent in software development. This approach fails to account for the efficiency with which software has been produced or the quality of the product. This paper proposes three alternative models for determining the production value of software, based on the notions of technical debt and interest. We applied the models to 367 proprietary systems developed by a range of different organisations using a range of different programming languages. We present the valuation results and discuss the weaknesses and strengths of the models.", "num_citations": "28\n", "authors": ["1130"]}
{"title": "Maintainability index revisited\u2013position paper\n", "abstract": " The amount of effort needed to maintain a software system is related to the technical quality of the source code of a system. In the past, a Maintainability Index has been proposed to calculate a single number that determines the maintainability of a system. The authors have identified some problems with that index, and propose a Maintainability Model which alleviates most of these problems.", "num_citations": "28\n", "authors": ["1130"]}
{"title": "A practical model for rating software security\n", "abstract": " This paper introduces a model for rating software security based on the ISO 25010 standard for software product quality. To rate software security, the authors define eleven system properties, which reflect how a typical software product addresses the confidentiality, integrity, non-repudiation, accountability and authenticity. The paper presents these properties, how to rate them, and how to aggregate the ratings.", "num_citations": "27\n", "authors": ["1130"]}
{"title": "A practical model for evaluating the energy efficiency of software applications\n", "abstract": " Evaluating the energy efficiency of software applications currently is an ad-hoc affair, since no practical and widely applicable model exists for this purpose. The need for such an evaluation model is pressing given the sharp increase in energy demand generated by the ICT industry. In particular, we need to get in control of our software applications since they play a key role in driving the consumption of energy. This paper proposes ME3SA, a Model for Evaluating the Energy Efficiency of Software Applications. ME3SA provides a practical breakdown of energy efficiency into measurements that can be applied to software applications in relation to the quantity of work they deliver. This approach makes it possible to measure and control energy efficiency similar to other software qualities such as performance efficiency or maintainability. Furthermore, we report on a case study in which the model was applied to an operational software system of the Software Improvement Group (SIG), a software advisory firm based in the Netherlands. The case study provides evidence that the proposed model is able to identify energy consumption hotspots and efficiency bottlenecks within software applications.", "num_citations": "26\n", "authors": ["1130"]}
{"title": "Model-based programming environments for spreadsheets\n", "abstract": " Spreadsheets can be seen as a flexible programming environment. However, they lack some of the concepts of regular programming languages, such as structured data types. This can lead the user to edit the spreadsheet in a wrong way and perhaps cause corrupt or redundant data.We devised a method for extraction of a relational model from a spreadsheet and the subsequent embedding of the model back into the spreadsheet to create a model-based spreadsheet programming environment. The extraction algorithm is specific for spreadsheets since it considers particularities such as layout and column arrangement. The extracted model is used to generate formulas and visual elements that are then embedded in the spreadsheet helping the user to edit data in a correct way.We present preliminary experimental results from applying our approach to a sample of spreadsheets from the EUSES Spreadsheet\u00a0\u2026", "num_citations": "25\n", "authors": ["1130"]}
{"title": "Benchmarking technical quality of software products\n", "abstract": " To enable systematic comparison of technical quality of (groups of) software products, we have collected measurement data of a wide range of systems into a benchmark repository. The measurements were taken over the course of several years of delivering software assessment services to corporations and public institutes. The granularity of the collected data follows the layered structure of a model for software product quality, based on the ISO/IEC 9126 international standard, which we developed previously. In this paper, we describe the design of our benchmark repository, and we explain how it can be used to perform comparisons of systems. To provide a concrete illustration of the concept without revealing confidential data, we use a selection of open source systems as example.", "num_citations": "25\n", "authors": ["1130"]}
{"title": "Monitoring the quality of outsourced software\n", "abstract": " Outsourcing application development or maintenance, especially offshore, creates a greater need for hard facts to manage by. We have developed a tool-based method for software monitoring which has been deployed over the past few years in a diverse set of outsourcing situations. In this paper we outline the method and underlying tools, and through several case reports we recount our experience with their application.", "num_citations": "25\n", "authors": ["1130"]}
{"title": "Strategic Term Rewriting and Its Application to a Vdm-sl to Sql Conversion\n", "abstract": " We constructed a tool, called VooDooM, which converts datatypes in Vdm-sl into Sql relational data models. The conversion involves transformation of algebraic types to maps and products, and pointer introduction. The conversion is specified as a theory of refinement by calculation. The implementation technology is strategic term rewriting in Haskell, as supported by the Strafunski bundle. Due to these choices of theory and technology, the road from theory to practise is straightforward.", "num_citations": "25\n", "authors": ["1130"]}
{"title": "Functional transformation systems\n", "abstract": " . We demonstrate how functional programming can be used as a platform for developing integrated transformation systems that are typed, scalable, easy to use, and adaptable. The kernel of such a functional transformation system for a certain language is a transformation framework generated from the language's grammar. The framework provides a fold traversal scheme and adaptable basic traversals. From the grammar a parser and pretty-printer is also generated. There is also support for storage and interchange of intermediate results. A software renovation case study illustrates our approach and its practical value. 1 Introduction Transformation systems, ie, systems which implement program transformations, are essential in many application areas, including software renovation [5, 2], reverse engineering,(domain-specific) language implementation, and program optimization. Transformations and the accompanying analyses are usually implemented as traversals over syntax trees apply...", "num_citations": "25\n", "authors": ["1130"]}
{"title": "Generic traversal over typed source code representations\n", "abstract": " General rights", "num_citations": "23\n", "authors": ["1130"]}
{"title": "Adoption and effects of software engineering best practices in machine learning\n", "abstract": " Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner.Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components.Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models.Results. Our findings indicate\u00a0\u2026", "num_citations": "22\n", "authors": ["1130"]}
{"title": "Metrication of SDF grammars\n", "abstract": " Metrication of grammars is an important instrument in grammar engineering, in particular to monitor iterative grammar development. In this report, we discuss the definition and implementation of metrics for the syntax definition formalism SDF. Two tools for SDF metrication have been developed: SdfMetz for static metrication of grammars themselves, and SdfCoverage for metrication of associated parser tests. We applied these tools to SDF grammars for a range of languages. We present and interpret the collected data.", "num_citations": "21\n", "authors": ["1130"]}
{"title": "Towards high performance software teamwork\n", "abstract": " Context: Research indicates that software quality, to a large extent, depends on cooperation within software teams [1] Since software development is a creative process that involves human interaction in the context of a team, it is important to understand the teamwork factors that influence performance. Objective: We present a study design in which we aim to examine the factors within software development teams that have significant influence on the performance of the team. We propose to consider factors such as communication, coordination of expertise, cohesion, trust, cooperation, and value diversity. The study investigates whether and to which extent these factors correlate with a performance of the team. In order to capture a variety of relevant teamwork factors, we created a new model extending the work of Hoegl and Gemuenden [2] and Liang et al.[3] Method: The study is based on quantitative research by\u00a0\u2026", "num_citations": "20\n", "authors": ["1130"]}
{"title": "Interpretation of source code clusters in terms of the ISO/IEC-9126 maintainability characteristics\n", "abstract": " Clustering is a data mining technique that allows the grouping of data points on the basis of their similarity with respect to multiple dimensions of measurement. It has also been applied in the software engineering domain, in particular to support software quality assessment based on source code metrics. Unfortunately, since clusters emerge from metrics at the source code level, it is difficult to interpret the significance of clusters at the level of the quality of the entire system. In this paper, we propose a method for interpreting source code clusters using the ISO/IEC 9126 software product quality model. Several methods have been proposed to perform quantitative assessment of software systems in terms of the quality characteristics defined by ISO/IEC 9126. These methods perform mappings of low-level source code metrics to high-level quality characteristics by various aggregation and weighting procedures. We\u00a0\u2026", "num_citations": "18\n", "authors": ["1130"]}
{"title": "Strong types for relational databases\n", "abstract": " Haskell's type system with multi-parameter constructor classes and functional dependencies allows static (compile-time) computations to be expressed by logic programming on the level of types. This emergent capability has been exploited for instance to model arbitrary-length tuples (heterogeneous lists), extensible records, functions with variable length argument lists, and (homogenous) lists of statically fixed length (vectors). We explain how type-level programming can be exploited to define a strongly-typed model of relational databases and operations on them. In particular, we present a strongly typed embedding of a significant subset of SQL in Haskell. In this model, meta-data is represented by type-level entities that guard the semantic correctness of database operations at compile time. Apart from the standard relational database operations, such as selection and join, we model functional dependencies\u00a0\u2026", "num_citations": "18\n", "authors": ["1130"]}
{"title": "Building Software Teams: Ten Best Practices for Effective Software Development\n", "abstract": " Why does poor software quality continue to plague enterprises of all sizes in all industries? Part of the problem lies with the process, rather than individual developers. This practical guide provides ten best practices to help team leaders create an effective working environment through key adjustments to their process. As a follow-up to their popular book, Building Maintainable Software, consultants with the Software Improvement Group (SIG) offer critical lessons based on their assessment of development processes used by hundreds of software teams. Each practice includes examples of goalsetting to help you choose the right metrics for your team. Achieve development goals by determining meaningful metrics with the Goal-Question-Metric approach Translate those goals to a verifiable Definition of Done Manage code versions for consistent and predictable modification Control separate environments for each stage in the development pipeline Automate tests as much as possible and steer their guidelines and expectations Let the Continuous Integration server do much of the hard work for you Automate the process of pushing code through the pipeline Define development process standards to improve consistency and simplicity Manage dependencies on third party code to keep your software consistent and up to date Document only the most necessary and current knowledge", "num_citations": "15\n", "authors": ["1130"]}
{"title": "Constraint-aware schema transformation\n", "abstract": " Data schema transformations occur in the context of software evolution, refactoring, and cross-paradigm data mappings. When constraints exist on the initial schema, these need to be transformed into constraints on the target schema. Moreover, when high-level data types are refined to lower level structures, additional target schema constraints must be introduced to balance the loss of structure and preserve semantics.We introduce an algebraic approach to schema transformation that is constraint-aware in the sense that constraints are preserved from source to target schemas and that new constraints are introduced where needed. Our approach is based on refinement theory and point-free program transformation. Data refinements are modeled as rewrite rules on types that carry point-free predicates as constraints. At each rewrite step, the predicate on the reduct is computed from the predicate on the redex. An\u00a0\u2026", "num_citations": "15\n", "authors": ["1130"]}
{"title": "Static estimation of test coverage\n", "abstract": " Test coverage is an important indicator for unit test quality. Tools such as Clover compute coverage by first instrumenting the code with logging functionality, and then logging which parts are executed during unit test runs. Since computation of test coverage is a dynamic analysis, it presupposes a working installation of the software. In the context of software quality assessment by an independent third party, a working installation is often not available. The evaluator may not have access to the required libraries or hardware platform. The installation procedure may not be automated or documented. In this paper, we propose a technique for estimating test coverage at method level through static analysis only. The technique uses slicing of static call graphs to estimate the dynamic test coverage. We explain the technique and its implementation. We validate the results of the static estimation by statistical comparison to\u00a0\u2026", "num_citations": "15\n", "authors": ["1130"]}
{"title": "Generative and Transformational Techniques in Software Engineering: International Summer School, GTTSE 2005, Braga, Portugal, July 4-8, 2005. Revised Papers\n", "abstract": " This tutorial book presents an augmented selection of material presented at the International Summer School on Generative and Transformational Techniques in Software Engineering, GTTSE 2005. The book comprises 7 tutorial lectures presented together with 8 technology presentations and 6 contributions to the participants workshop. The tutorials combine foundations, methods, examples, and tool support. Subjects covered include feature-oriented programming and the AHEAD tool suite; program transformation with reflection and aspect-oriented programming, and more.", "num_citations": "15\n", "authors": ["1130"]}
{"title": "Type-safe functional strategies\n", "abstract": " We demonstrate how the concept of strategies originating from term rewriting can be introduced in a typed, functional setting. We provide a model of strategies based on a further generalisation of updatable, monadic, generalised fold algebras. We show how strategies can be used as a structuring device for functional programming.", "num_citations": "15\n", "authors": ["1130"]}
{"title": "Tutorial on strategic programming across programming paradigms\n", "abstract": " 5 Design Patterns and Programming Idioms 36 5.1 Traversal idioms................................. 36 5.2 Transformation idioms............................. 37 5.3 Variation points................................. 37 5.4 Object-oriented................................. 37 5.5 Functional.................................... 38 i", "num_citations": "14\n", "authors": ["1130"]}
{"title": "Transformation of structure-shy programs with application to XPath queries and strategic functions\n", "abstract": " Various programming languages allow the construction of structure-shy programs. Such programs are defined generically for many different datatypes and only specify specific behavior for a few relevant subtypes. Typical examples are XML query languages that allow selection of subdocuments without exhaustively specifying intermediate element tags. Other examples are languages and libraries for polytypic or strategic functional programming and for adaptive object-oriented programming.In this paper, we present an algebraic approach to transformation of declarative structure-shy programs, in particular for strategic functions and XML queries. We formulate a rich set of algebraic laws, not just for transformation of structure-shy programs, but also for their conversion into structure-sensitive programs and vice versa. We show how subsets of these laws can be used to construct effective rewrite systems for\u00a0\u2026", "num_citations": "13\n", "authors": ["1130"]}
{"title": "Tactical Safety Reasoning. A Case for Autonomous Vehicles.\n", "abstract": " Self driving cars have recently attracted academia and industry interest. As planning algorithms become responsible for critical decisions, many questions concerning traffic safety arise. An increased automation level demands proportional impact on safety requirements, currently governed by the ISO 26262 standard. However, ISO 26262 sees safety as a functional property of a system and fails to cover emergent concerns related to autonomous decisions. In order to fill this gap we propose the field of tactical safety, which extends safety analysis to planning and execution of driving maneuvers, response to traffic events or autonomous system failures. It is meant to complement, not to replace functional safety properties of a system and allows the analysis of autonomous agents from a safe behavior point of view. We draw the requirements for tactical safety from an automotive standard which defines functional\u00a0\u2026", "num_citations": "11\n", "authors": ["1130"]}
{"title": "Detecting cross-language dependencies generically\n", "abstract": " In order to evaluate large, heterogeneous information systems (i.e., comprising modules developed in diverse programming languages) a method to detect dependencies among these modules is needed. Although there is a variety of methods that can detect dependencies within a single programming language, the available cross-language detection methods use extensive language specific information to parse and analyse modules written in different languages. In this paper, a new method for detecting cross-language dependencies is proposed. This method is generic, yet accurate and can support new languages with minimal effort. To evaluate the method, a tool was created and a series of experiments was conducted on a small case study for which dependencies had been extracted manually. The evaluation shows that the method is effective, extensible and easily explainable.", "num_citations": "11\n", "authors": ["1130"]}
{"title": "Governance of spreadsheets through spreadsheet change reviews\n", "abstract": " We present a pragmatic method for management of risks that arise due to spreadsheet use in large organizations. We combine peer-review, tool-assisted evaluation and other pre-existing approaches into a single organization-wide approach that reduces spreadsheet risk without overly restricting spreadsheet use. The method was developed in the course of several spreadsheet evaluation assignments for a corporate customer. Our method addresses a number of issues pertinent to spreadsheet risks that were raised by the Sarbanes-Oxley act.", "num_citations": "11\n", "authors": ["1130"]}
{"title": "Generalized LR parsing in Haskell\n", "abstract": " Parser combinators elegantly and concisely model generalised LL parsers in a purely functional language. They nicely illustrate the concepts of higherorder functions, polymorphic functions and lazy evaluation. Indeed, parser combinators are often presented as a motivating example for functional programming. Generalised LL, however, has an important drawback: it does not handle (direct nor indirect) left recursive context-free grammars. In a different context, the (non-functional) parsing community has been doing a considerable amount of work on generalised LR parsing. Such parsers handle virtually any context-free grammar. Surprisingly, little work has been done on generalised LR by the functional programming community ([9] is a good exception).In this report, we present a concise and elegant implementation of an incremental generalised LR parser generator and interpreter in Haskell. For good computational complexity, such parsers rely heavily on lazy evaluation. Incremental evaluation is obtained via function memoisation. An implementation of our generalised LR parser generator is available as the HaGLR tool. We assess the performance of this tool with some benchmark examples.", "num_citations": "11\n", "authors": ["1130"]}
{"title": "Improving the adoption of dynamic web security vulnerability scanners\n", "abstract": " Security vulnerabilities remain present in many web applications despite the improving knowledge base on vulnerabilities. Attackers can exploit such security vulnerabilities to extract critical data from web applications and their users. Many dynamic security vulnerability scanners exist that try to automatically find such security vulnerabilities. We studied the adoption of these tools and found out they are rarely used by web developers during the development process of a web application. Through interviews, we investigated the main cause of the lack of adoption is the difficulty to use such tools. In order to improve the adoption of dynamic security vulnerability scanners, we introduce the Universal Penetration Testing Robot (UPeTeR). UPeTeR is a class library that allows web developers to easily set relevant data for many dynamic vulnerability scanners by providing them with an abstraction of required configuration data. Plugins, ideally created by experts of the scanners, transform this abstraction into an optimal setup of such scanners. A prototype has been created which was used to validate UP-eTeR\u2019s acceptance by web developers at the Software Improvement Group, a software consultancy company in the Netherlands. The acceptance experiment demonstrated that web developers are willing to try out and work with UPeTeR.", "num_citations": "10\n", "authors": ["1130"]}
{"title": "SIG/T\u00dcViT evaluation criteria trusted product maintainability: Guidance for producers\n", "abstract": " This document is a companion to the SIG/T\u00dcViT Evaluation Criteria Trusted Product Maintainability [SIG 2015]. The SIG/T\u00dcViT Evaluation Criteria for the quality mark T\u00dcViT Trusted Product Maintainability are intended for the standardized evaluation and certification of the technical quality of the source code of software products. The purpose of such evaluation and certification is to provide an instrument to developers for guiding improvement of the products they create and enhance, and to acquirers for comparing, selecting, and accepting pre-developed software.", "num_citations": "10\n", "authors": ["1130"]}
{"title": "SIG\n", "abstract": " Software product quality is a major concern for those that develop, maintain, enhance, acquire, or use software products or software-intensive systems. Technical quality of software products pertains to the ease and speed by which the software allows itself to be modified to keep pace with the changing needs of its users or other stakeholders.", "num_citations": "10\n", "authors": ["1130"]}
{"title": "Energy-efficiency indicators for e-services\n", "abstract": " Great strides have been made to increase the energy efficiency of hardware, data center facilities, and network infrastructure. These Green IT initiatives aim to reduce energy-loss in the supply chain from energy grid to computing devices. However, the demand for computation comes from software applications that perform business services. Therefore, to measure and improve efficiency for entire systems, energy-efficiency indicators are needed at the level of services. We have designed an initial set of indicators for energy-efficiency of e-services and we have tested them on two e-government services of the Dutch national government. We explain how these indicators serve as a starting point for energy-optimization initiatives, supported by appropriate contractual agreements between service owners and suppliers.", "num_citations": "10\n", "authors": ["1130"]}
{"title": "Matching objects without language extension\n", "abstract": " Pattern matching is a powerful programming concept which has proven its merits in declarative programming. The absence of pattern-matching in object-oriented programming languages is felt especially when tackling source code processing problems. But existing proposals for pattern matching in such languages rely on language extension, which makes their adoption overly intrusive. We propose an approach to support pattern matching in mainstream object-oriented languages without language extension. In this approach, a pattern is a first-class entity, which can be created, be passed as argument, and receive method invocations, just like any other object. We demonstrate how our approach can be used in conjunction with existing parser generators to perform pattern matching on various kinds of abstract syntax representation. We elaborate our approach to include concrete syntax patterns, and mixing of patterns and visitors for the construction of sophisticated syntax tree traversals.", "num_citations": "10\n", "authors": ["1130"]}
{"title": "Strategic polymorphism requires just two combinators!\n", "abstract": " In previous work, we introduced the notion of functional strategies: first-class generic functions that can traverse terms of any type while mixing uniform and type-specific behaviour. Functional strategies transpose the notion of term rewriting strategies (with coverage of traversal) to the functional programming paradigm. Meanwhile, a number of Haskell-based models and combinator suites were proposed to support generic programming with functional strategies. In the present paper, we provide a compact and matured reconstruction of functional strategies. We capture strategic polymorphism by just two primitive combinators. This is done without commitment to a specific functional language. We analyse the design space for implementational models of functional strategies. For completeness, we also provide an operational reference model for implementing functional strategies (in Haskell). We demonstrate the generality of our approach by reconstructing representative fragments of the Strafunski library for functional strategies.", "num_citations": "10\n", "authors": ["1130"]}
{"title": "Towards a benchmark for the maintainability evolution of industrial software systems\n", "abstract": " The maintainability of software is an important cost factor for organizations across all industries, as maintenance makes up approximately 40% to 70% of the total development costs of a software system. Organizations are often stuck in the situation where software maintenance costs dominate IT budgets, leaving no room for enhancement and innovation. Building a benchmark for maintainability evolution is helpful in this context because it can help organizations decide on software improvement or replacement strategies. The prototype benchmark we study in this paper shows that software volume and maintainability levels are strong determinants of future maintainability evolution rates. We further describe the data collection and cleaning procedures that were applied to approximately 1,750 industrial software systems, and we provide an exploratory analysis of the resulting benchmark dataset.", "num_citations": "9\n", "authors": ["1130"]}
{"title": "The use of UML class diagrams and its effect on code change-proneness\n", "abstract": " The goal of this study is to investigate the use of UML and its impact on the change proneness of the implementation code. We look at whether the use of UML class diagrams, as opposed to using no modeling notation, influences code change proneness. Furthermore, using five design metrics we measure the quality of UML class diagrams and explore its correlation with code change proneness. Based on a UML model of an industrial system and multiple snapshots of the implementation code, we have found that at the system level the change proneness of code modeled using class diagrams is lower than that of code that is not modeled at all. However, we observe different results when performing the analysis at different system levels (eg, subsystem and sub subsystem). Additionally, we have found significant correlations between class diagram size, complexity, and level of detail and the change proneness of\u00a0\u2026", "num_citations": "9\n", "authors": ["1130"]}
{"title": "Change is the Constant: keynote\n", "abstract": " Change is the Constant : keynote Radboud Repository Radboud Repository \u2192 Collections Radboud University \u2192 Academic publications \u2192 View Item Fulltext present in this item Find Full text Thumbnail Fulltext: 103928.pdf Format: PDF Description: publisher's version Embargo: until further notice Title: Change is the Constant : keynote Author(s): Visser, JMW Publication year: 2012 Source: Ercim News, vol. 2012, iss. 88, (2012), pp. 3 ISSN: 0926-4981 Publication type: Article / Letter to editor Please use this identifier to cite or link to this item : https://hdl.handle.net/2066/103928 https://hdl.handle.net/2066/103928 Display more details Upload Full Text Subject: Software Science Organization: Software Science Journal title: Ercim News Volume: vol. 2012 Issue: iss. 88 Page start: p. 3 Page end: p. 3 This item appears in the following Collection(s) Faculty of Science [27835] Electronic publications [83781] Freely accessible \u2026", "num_citations": "9\n", "authors": ["1130"]}
{"title": "Generative and Transformational Techniques in Software Engineering II\n", "abstract": " The second instance of the international summer school on Generative and Transformational Techniques in Software Engineering (GTTSE 2007) was held in Braga, Portugal, during July 2\u20137, 2007. This volume contains an augmented selection of the material presented at the school, including full tutorials, short tutorials, and contributions to the participants workshop. The GTTSE summer school series brings together PhD students, lecturers, technology presenters, as well as other researchers and practitioners who are interested in the generation and the transformation of programs, data, models, metamodels, documentation, and entire software systems. This concerns many areas of software engineering: software reverse and re-engineering, model-driven engineering, automated software engineering, generic language technology, to name a few. These areas differ with regard to the specific sorts of metamodels\u00a0\u2026", "num_citations": "9\n", "authors": ["1130"]}
{"title": "CAMILA revival: VDM meets Haskell\n", "abstract": " We have experimented with modeling some of the key concepts of the VDM specification language inside the functional programming language Haskell. For instance, VDM\u2019s sets and maps are directly available as data types defined in standard libraries; we merely needed to define some additional functions to make the match complete. A bigger challenge is posed by VDM\u2019s data type invariants, and pre-and postconditions. For these we resorted to Haskell\u2019s constructor class mechanism, and its support for monads. This allows us to switch between different modes of evaluation (eg with or without property checking) by simply coercing user defined functions and operations to different specific types.", "num_citations": "9\n", "authors": ["1130"]}
{"title": "Energy efficiency optimization of application software\n", "abstract": " The application software design has a major impact on the energy efficiency of a computing system. But research on the subject is still in its infancy. What is the energy efficiency of software? How can it be measured? What are guidelines for the development of energy efficient software? In this paper, we set out to find an answer to these questions and motivate the need for a dedicated research area for application software energy efficiency. We place this subject in the context of other initiatives for green and sustainable computing and clarify central concepts. Furthermore we give an overview of different existing approaches for both measuring and optimization of software energy efficiency.", "num_citations": "8\n", "authors": ["1130"]}
{"title": "Development of an industrial strength grammar for VDM\n", "abstract": " This report describes the development of an industrial strength grammar for the VDM specification language. We present both the development process and its result. The employed methodology can be described as iterative grammar engineering and includes the application of techniques such as grammar metrication, unit testing, and test coverage analysis. The result is a VDM grammar of industrial strength, in the sense that it is well-tested, it can be used for fast parsing of high volumes of VDM specifications, and it allows automatic generation of support for syntax tree representation, traversal, and interchange.", "num_citations": "8\n", "authors": ["1130"]}
{"title": "Extended static checking by strategic rewriting of pointfree relational expressions\n", "abstract": " Binary relational algebra provides semantic foundations for major areas of computing, such as database design, state-based specification, and functional programming. Remarkably, static checking support in these areas fails to exploit the full semantic content of relations. In particular, properties such as the simplicity or injectivity of relations are not statically enforced in operations that manipulate relations, such as database queries, state transitions, or composition of functional components.We describe how a pointfree treatment of relations, their properties, their operators, and the laws that govern them can be captured in a type-directed strategic rewriting system for transformation of relational expressions. This rewriting tool can be used to simplify relational proof obligations and ultimately reduce them to tautologies. We demonstrate how such reductions provide extended static checking (ESC) for design contraints commonly found in software modeling and development.", "num_citations": "7\n", "authors": ["1130"]}
{"title": "Evolving algebras\n", "abstract": " Evolving algebras provide models for arbitrary computational processes. They can at once be viewed as abstract machines, and as formal speci cations. Thus, the evolving algebra formalism combines two perspectives on computational processes: speci cation methods and computational models. Since evolving algebras were rst proposed by Yuri Gurevich Gur91] Gur95], they have been the subject of ongoing research. The graduation project, the results of which are presented in this thesis report, aimed to make a dual contribution to this research. On the one hand, an attempt has been made to advance the development of the theory of evolving algebras. On the other hand, a tool has been created to support the design and use of particular evolving algebras. These theoretical and more applied e orts have been undertaken conjointly.The dual focus of the project is re ected in the structure of this thesis report, which consists of two parts. The rst part is concerned with the theory of evolving algebras, while the second part is dedicated to the subject of automated support for evolving algebras.", "num_citations": "7\n", "authors": ["1130"]}
{"title": "Automatic event detection for software product quality monitoring\n", "abstract": " Collecting product metrics during development or maintenance of a software system is an increasingly common practice that provides insight and control over the evolution of a product's quality. An important challenge remains in interpreting the vast amount of data as it is being collected and in transforming it into actionable information. We present an approach for discovering significant events in the development process from the associated stream of product measurement data. At the heart of our approach lies the view of measurement data streams as functions for which derivatives can be calculated. In a manner inspired by Statistical Process Control, a certain number of data points are then selected as events worthy of further inspection. We apply our approach in an industrial setting, namely as support to the Software Monitoring service provided by the Software Improvement Group. In particular, we report on an\u00a0\u2026", "num_citations": "6\n", "authors": ["1130"]}
{"title": "Understanding the relation between information technology capability and organizational performance\n", "abstract": " Influenced by the resource-based view (RBV), we examined the understudied relationship between IT capability and organizational performance for small Dutch Not-For-Profit organizations (NFPs). We conceptualized IT capability as four separate IT competencies: IT alignment, technical, relational and employee IT competence and used survey data collected from 112 respondents. Our results show that IT alignment, relational and employee IT competence impact organizational performance positively. On the contrary, technical IT competence has negative impact on organizational performance. Our study provides preliminary empirical evidence to better understand the relation between IT capability and organizational performance of NPFs.", "num_citations": "5\n", "authors": ["1130"]}
{"title": "Improving the usefulness of alerts generated by automated static analysis tools\n", "abstract": " Static analysis tools are programs that analyze software without executing it. They can be simple style checkers or follow intricate rules to efficiently find problems often overlooked by developers. Unfortunately, the alerts generated by those tools are not always correct. The high number of false positives has been found to be one of the major reasons why such tools have not been widely adopted. One approach to improve these tools is to post-process the generated alerts and only report actionable ones, ie true positives which will be acted upon by the developers. In this work, we evaluate several machine-learning classifiers that use historic alert data to classify new alerts as actionable or not. We build a framework called Autobugs to collect the necessary information. It runs a static analysis tool on past revisions of a software project, saves the generated alerts and computes the lifetime-based actionability label. This is then used to train a linear support vector machine (SVM), a nonlinear SVM and a decision tree on three similar open-source forum-software projects written in PHP. We evaluate each classifiers for each project individually as well as the application of a trained model on a different project. Based on the results, we constructed an additional classifier, which only takes into account the lifetime of an alert, classifying younger ones as actionable. It outperforms the other algorithms for our sample software-projects. i", "num_citations": "5\n", "authors": ["1130"]}
{"title": "Stepwise refinement of an abstract state machine for WHNF-reduction of lambda terms\n", "abstract": " In this paper the authors present a series consisting of three abstract state machines (ASMs) for the problem of finding the weak head normal form (WHNF) of an arbitrary term of the pure lambda-calculus. The authors use a method of stepwise refinement in which three stages of refinement are clearly distinguished. In the first stage an ASM is constructed which is a high-level specification of the problem. From this ASM a second ASM is derived by refinement of static functions. Finally, the third ASM is obtained from the second by a refinement replacing static universes by dynamic ones. The adequacy of each refinement is assessed. Both kinds of refinement provide further experience with hierarchical ASM design by stepwise refinement.", "num_citations": "5\n", "authors": ["1130"]}
{"title": "A security analysis of the ETSI ITS vehicular communications\n", "abstract": " This paper analyses security aspects of the ETSI ITS standard for co-operative transport systems, where cars communicate with each other (V2V) and with the roadside (V2I) to improve traffic safety and make more efficient use of the road system. We focus on the initial information exchange between vehicles and the road side infrastructure responsible for authentication and authorisation, because all the security aspects for these interactions are regulated in the ETSI ITS standards. Other services running in vehicular networks are open to choose application-specific security requirements and implement them using features from the ETSI ITS standard. We note some possibilities for replay attacks that, although they have limited impact, could be prevented using simple techniques, some of which are directly available in the ETSI ITS standard.", "num_citations": "4\n", "authors": ["1130"]}
{"title": "Generative and Transformational Techniques in Software Engineering IV: International Summer School, GTTSE 2011, Braga, Portugal, July 3-9, 2011, Revised and Extended Papers\n", "abstract": " This tutorial volume includes revised and extended lecture notes of six long tutorials, five short tutorials, and one peer-reviewed participant contribution held at the 4th International Summer School on Generative and Transformational Techniques in Software Engineering, GTTSE 2011. The school presents the state of the art in software language engineering and generative and transformational techniques in software engineering with coverage of foundations, methods, tools, and case studies.", "num_citations": "4\n", "authors": ["1130"]}
{"title": "Practices for Engineering Trustworthy Machine Learning Applications\n", "abstract": " Following the recent surge in adoption of machine learning (ML), the negative impact that improper use of ML can have on users and society is now also widely recognised. To address this issue, policy makers and other stakeholders, such as the European Commission or NIST, have proposed high-level guidelines aiming to promote trustworthy ML (i.e., lawful, ethical and robust). However, these guidelines do not specify actions to be taken by those involved in building ML systems. In this paper, we argue that guidelines related to the development of trustworthy ML can be translated to operational practices, and should become part of the ML development life cycle. Towards this goal, we ran a multi-vocal literature review, and mined operational practices from white and grey literature. Moreover, we launched a global survey to measure practice adoption and the effects of these practices. In total, we identified 14 new practices, and used them to complement an existing catalogue of ML engineering practices. Initial analysis of the survey results reveals that so far, practice adoption for trustworthy ML is relatively low. In particular, practices related to assuring security of ML components have very low adoption. Other practices enjoy slightly larger adoption, such as providing explanations to users. Our extended practice catalogue can be used by ML development teams to bridge the gap between high-level guidelines and actual development of trustworthy ML systems; it is open for review and contribution", "num_citations": "3\n", "authors": ["1130"]}
{"title": "Towards using probabilistic models to design software systems with inherent uncertainty\n", "abstract": " The adoption of machine learning (ML) components in software systems raises new engineering challenges. In particular, the inherent uncertainty regarding functional suitability and the operation environment makes architecture evaluation and trade-off analysis difficult. We propose a software architecture evaluation method called Modeling Uncertainty During Design (MUDD) that explicitly models the uncertainty associated to ML components and evaluates how it propagates through a system. The method supports reasoning over how architectural patterns can mitigate uncertainty and enables comparison of different architectures focused on the interplay between ML and classical software components. While our approach is domain-agnostic and suitable for any system where uncertainty plays a central role, we demonstrate our approach using as example a perception system for autonomous driving.", "num_citations": "3\n", "authors": ["1130"]}
{"title": "THE IMPACT OF CULTURE ON THE BUSINESS VALUE OF IT\u2013AN EXAMINATION FROM TWO SECTORS\n", "abstract": " This study examines the scarcely studied influence of organizational culture and IT capability on organizational performance. We surveyed 143 managers within the for-profit sector and 151 managers within the not-for-profit sector and our study demonstrate that interactions with organizational culture matter for organizational performance. For instance, interactions between organizational culture and IT capability may dampen an otherwise positive effect of IT capability on performance. We provide preliminary empirical evidence on the importance of organizational culture and demonstrate that without this contextual information, findings about business value of IT may be incomplete.", "num_citations": "3\n", "authors": ["1130"]}
{"title": "Evaluating the testing quality of software defined infrastructures\n", "abstract": " Software defined infrastructures are computer infrastructures expressed in computer code. As software defined infrastructures is a young technology the body of scientific knowledge on the subject is small. This study aims at identification of quality aspects of software defined infrastructure projects by interviewing practitioners in the field. The result is a quality model containing quality characteristics of software defined infrastructures.During the interviews on quality of software defined infrastructures, testing emerged as one of the most important quality aspects. This study therefore also aims at creating a testing quality model for evaluating the testing quality of software defined infrastructures. This was done by applying a testing quality model made by the Software Improvement Group (SIG) for traditional software to software defined infrastructures and by interviewing practitioners on their opinion of the existing model. The result is an improved testing quality model which is better suited for software defined infrastructures.", "num_citations": "3\n", "authors": ["1130"]}
{"title": "HASDF: a generalized LR-parser generator for Haskell\n", "abstract": " Language-centered software engineering requires language technology that (i) handles the full class of context-free grammars, and (ii) accepts grammars that contain syntactic  information only. The syntax definition formalism SDF  combined with GLR-parser generation offers such technology.  We propose to make SDF and GLR-parsing available for use  with various programming languages.  We have done so for the functional programming language  Haskell.  By combining Haskell data type definitions  with the syntax definition formalism SDF we have designed  HASDF. HASDF is a domain-specific language in which the  concrete syntax of an arbitrary context-free language can  be defined in combination with the Haskell data types that  represent its abstract syntax.  We have implemented a tool  that generates a GLR-parser and an unparser from a HASDF  definition.", "num_citations": "3\n", "authors": ["1130"]}
{"title": "An Empirical Study of Software Architecture for Machine Learning\n", "abstract": " Specific developmental and operational characteristics of machine learning (ML) components, as well as their inherent uncertainty, demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture forML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings, and (iii) a survey to quantitatively validate the challenges and their solutions. In total, we compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability or interoperability. Using the survey, we were able to establish a link between architectural solutions and software quality attributes; which enabled us to provide twenty architectural tactics used for satisfying individual quality requirements of systems with ML components. Altogether, the results can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Classification model for predicting cost slippage in governmental ICT projects\n", "abstract": " In this paper we present a classification model for predicting cost slippage using data mining techniques. The model uses the initial planning of an ICT project in terms of budget and schedule and then predicts the category of cost slippage of the project. Three categories are distinguished where low slippage is considered normal, medium slippage requires attention, and large slippage requires action. The model was trained and validated with two data sets from the US Federal IT Dashboard and the Dutch Rijks-ICT-Dashboard, respectively that hold project data for large governmental ICT projects. The classification model is intended as a project management tool for sponsors and managers of large ICT projects in the public sector.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Software risk management in practice: Shed light on your software product\n", "abstract": " You can't control what you can't measure. And you can't decide if you are wandering around in the dark. Risk management in practice requires shedding light on the internals of the software product in order to make informed decisions. Thus, in practice, risk management has to be based on information about artifacts (documentation, code, and executables) in order to detect (potentially) critical issues. This tutorial presents experiences from industrial cases worldwide on qualitative and quantitative measurement of software products. We present our lessons learned as well as consolidated experiences from practice and provide a classification scheme of applicable measurement techniques. Participants of the tutorial receive an introduction to the techniques in theory and then apply them in practice in interactive exercises. This enables participants to learn how to shed light on the internals of their software and how to\u00a0\u2026", "num_citations": "2\n", "authors": ["1130"]}
{"title": "What Affects Information Technology Capability: A Meta-Analysis on Aspects that Influence Information Technology Capability\n", "abstract": " The extent to which a firm deploys IT resources in conjunction with other resources is sometimes referred to as IT capability. The aim of this paper is to investigate what affects IT capability. To understand the drivers that affect IT capability, we systematically examined prior literature. Through a combination of deductive and inductive coding we have identified four generic IT capability aspects (organisational, people, relational and technical IT competencies). We then analyse how these four aspects affected IT capability by calculating effect sizes. _x000D_ Early results suggest most variance in IT capability can be explained by organisational IT competence. However, upon further analyses these results became inconclusive. Nevertheless, we believe an extension of our study would advance insights on aspects affecting IT capability. We discuss the limitations of our study and suggest directions for future research.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Measuring dependency freshness in software systems\n", "abstract": " Modern software systems often make use of external dependencies to speed up development and reduce cost. These dependencies have to be updated to ensure the flexibility, security, and stability of the system. In this thesis we analyze the dependency update behavior of industry systems. Several measurements are presented to quantify how outdated an individual dependency is, as well as a benchmark-based metric to rate a system as a whole. The system-level metric is validated through three different methods. First, the usefulness of the metric is validated using interviews with practitioners. Secondly, the metric is checked for flatlining when a system is monitored over time. Finally, the metric\u2019s relationship with reported security vulnerabilities in dependencies is investigated. The latter validation step shows that systems with outdated dependencies are more than four times as likely to have security issues in their external dependencies.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Monitoring software quality at large scale\n", "abstract": " Monitoring Software Quality at Large Scale Radboud Repository Radboud Repository \u2192 Collections Radboud University \u2192 Academic publications \u2192 View Item There is no fulltext present in this item. Find Full text Title: Monitoring Software Quality at Large Scale Author(s): Bouwers, E.; John, P.; Visser, J. Publication year: 2014 Source: Ercim News, vol. 99, iss. Oktober, (2014), pp. 17-18 ISSN: 0926-4981 Publication type: Article / Letter to editor Please use this identifier to cite or link to this item : https://hdl.handle.net/2066/132719 https://hdl.handle.net/2066/132719 Display more details Upload Full Text Subject: Software Science Organization: Software Science Journal title: Ercim News Volume: vol. 99 Issue: iss. Oktober Page start: p. 17 Page end: p. 18 This item appears in the following Collection(s) Faculty of Science [27564] Academic publications [178837] Academic output Radboud University Search Repository \u2026", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Issue handling performance in proprietary software projects\n", "abstract": " Software maintenance tasks are mainly related to fixing defects and implementing new features. Higher efficiency in performing such tasks is therefore going to reduce the costs of maintenance. A previous study involving open source systems has shown that higher software maintainability corresponds to faster speed in fixing defects [1]. In this paper we replicate the previous study by mining bug report data of three proprietary software projects. In one of the projects, a correlation between higher software maintainability and faster defect resolution is confirmed. The quality of issue handling process (e.g., issue registration accuracy and completeness, scope and complexity of issue workflow) should be considered in further research as it might explain the circumstances under which the correlation can be observed.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Mining the Dutch National ICT Dashboard\n", "abstract": " The Dutch National ICT Dashboard, launched on the 18th of May 2011, is a public website that provides Dutch citizens with an overview of the large and/or high-risk IT projects of the national government, based on information provided by the various ministries to the parliament. The top pages of the dashboard show overall scores of each project, while underlying pages provide more detailed information per project.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Observing unit test maturity in the wild\n", "abstract": " This talk provides an anecdotal account of unit testing maturity and its evolution in Dutch IT organizations as we have observed it during the last 7 years in our IT consultancy practice.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "Matching Objects\n", "abstract": " Pattern matching is a powerful programming concept which has proven its merits in declarative programming paradigms. We propose a light-weight approach to support pattern matching in objectoriented languages. Our technique requires no language extension. Instead, we provide a generic, reflective matching algorithm that switches to type-specific behavior where possible. We detail the realization of our approach within Java.Additionally, we generalize the pattern-matching idiom with active patterns, and matching at arbitrary depth. This is achieved through an elegant integration of our pattern-matching approach with visitor combinator programming. Previously, we introduced generic visitor combinators as a technique for modeling term rewriting strategies in object-oriented setting. The result is a light-weight, flexible, but quite powerful technique for navigation and manipulation of object graphs.", "num_citations": "2\n", "authors": ["1130"]}
{"title": "GraphRepo: Fast Exploration in Software Repository Mining\n", "abstract": " Mining and storage of data from software repositories is typically done on a per-project basis, where each project uses a unique combination of data schema, extraction tools, and (intermediate) storage infrastructure. We introduce GraphRepo, a tool that enables a unified approach to extract data from Git repositories, store it, and share it across repository mining projects. GraphRepo usesNeo4j, an ACID-compliant graph database management system, and allows modular plug-in of components for repository extraction (drillers), analysis (miners), and export (mappers). The graph enables a natural way to query the data by removing the need for data normalisation. GraphRepo is built in Python and offers multiple ways to interface with the rich Python ecosystem and with big data solutions. The schema of the graph database is generic and extensible. Using GraphRepo for software repository mining offers several advantages versus creating project-specific infrastructure: (i) high performance for short-iteration exploration and scalability to large data sets (ii) easy distribution of extracted data(e.g., for replication) or sharing of extracted data among projects, and (iii) extensibility and interoperability. A set of benchmarks on four open source projects demonstrate that GraphRepo allows very fast querying of repository data, once extracted and indexed. More information can be found in the project's documentation (available at https://tinyurl.com/grepodoc) and in the project's repository (available at https://tinyurl.com/grrepo). A video demonstration isalso available online (https://tinyurl.com/grrepov)", "num_citations": "1\n", "authors": ["1130"]}
{"title": "Security by design in Azure DevOps pipelines, a case study at SpendLab technology\n", "abstract": " This research investigates the impact of adding security tools in CI pipelines on \u201csecurity by design\u201d in software development. There are multiple ways of improving security by design in CI pipelines, and there is existing research in this area. However, not much research is done about actually measuring the improvements made.This thesis starts off with a systematic review of existing approaches, where we conclude that static application security testing and open source security scanning are both methods used in CI pipelines to improve security by design. Secondly, we construct a Goal-Question-Metric model for measuring security improvements achieved by such pipeline extensions.", "num_citations": "1\n", "authors": ["1130"]}
{"title": "SIG/T\u00dcViT Evaluation Criteria Trusted Product Maintainability: Guidance for producers Version 6.1\n", "abstract": " Software product quality is a major concern for those that develop, maintain, enhance, acquire, or use software products or software-intensive systems. Technical quality of software products pertains to the ease and speed by which the software allows itself to be modified to keep pace with the changing needs of its users or other stakeholders.", "num_citations": "1\n", "authors": ["1130"]}
{"title": "Generative and transformational techniques in software engineering III\n", "abstract": " The third instance of the international summer school on Generative and Transformational Techniques in Software Engineering (GTTSE 2009) was held in Braga, Portugal, July 6\u201311, 2009. In this volume, you find revised and extended lecture notes for most of the long and short summer-school tutorials as well as a small number of peer-reviewed papers that originated from the participants\u2019 workshop.The mission of the GTTSE summer school series is to bring together PhD students, lecturers, as well as other researchers and practitioners who are interested in the generation and the transformation of programs, data, models, metamodels, documentation, and entire software systems. This mission crosscuts many areas of software engineering, eg, software reverse and re-engineering, model-driven engineering, automated software engineering, generic language technology, software language engineering\u2014to name\u00a0\u2026", "num_citations": "1\n", "authors": ["1130"]}
{"title": "Galois: A language for proofs using galois connections and fork algebras\n", "abstract": " Galois is a domain specific language supported by the Galculator interactive proof-assistant prototype. Galculator uses an equational approach based on Galois connections with indirect equality as an additional inference rule. Galois allows for the specification of different theories in a point-free style by using fork algebras, an extension of relation algebras with expressive power of first-order logic. The language offers sub-languages to derive proof rules from Galois connections, to express proof tactics, and to organize axioms and theorems into modular definitions. In this paper, we describe how the algebraic theory underlying the proof-method drives the design of the Galois language. We provide the syntax and semantics of important fragments of Galois and show how they are hierarchically combined into a complete language.", "num_citations": "1\n", "authors": ["1130"]}
{"title": "Industrial Realities of Program Comprehension (IRPC 2008)\n", "abstract": " In the working session on Industrial Realities of Program Comprehension (IRPC 2008), experience and ideas are shared regarding the challenges and opportunities of industrial application of program comprehension techniques. Participants work together to formulate a set of useful guidelines for introducing new program comprehension techniques into industrial practice as well as consolidating and increasing their use.", "num_citations": "1\n", "authors": ["1130"]}
{"title": "SdfMetz: Extraction of Metrics and Graphs From Syntax Definitions\u2014Tool Demonstration\u2014\n", "abstract": " We developed SdfMetz, a tool for the extraction of metrics and graphs from syntax descriptions. SdfMetz supports various input languages, such as SDF and the syntax formalisms of DMS, ANTLR, and Bison. Among the extracted metrics are size and complexity metrics, feature metrics, and structure metrics. Some metrics are extracted directly from grammars, such as adaptations of the NPath and cyclomatic complexity metrics. Several structure metrics, such as tree impurity and recursiveness, are based on various kinds of grammar dependency graphs. The metrics and graphs can be emitted in several formats to allow their subsequent visualization and (statistical) analysis. We present the functionality of the tool, its implementation, and its use for grammar comparison and analysis.", "num_citations": "1\n", "authors": ["1130"]}
{"title": "Strong Types for Relational Data Stored in Databases or Spreadsheets\n", "abstract": " Relational data stored in databases and spreadsheets often present inconsistencies. Moreover, data integrity checking in a low level programming environment, such as the one provided by spreadsheets, is error-prone.", "num_citations": "1\n", "authors": ["1130"]}