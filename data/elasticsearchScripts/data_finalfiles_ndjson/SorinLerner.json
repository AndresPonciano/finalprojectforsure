{"title": "ESP: Path-sensitive program verification in polynomial time\n", "abstract": " In this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the property-related behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full path-sensitive analysis. We have implemented this\" property simulation\" algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to. fprintf in the source code of gcc are guaranteed to print to valid, open files. Our results show\u00a0\u2026", "num_citations": "712\n", "authors": ["1811"]}
{"title": "Mojo: A dynamic optimization system\n", "abstract": " Dynamic optimization systems have the flexibility to adapt program execution to changing scenarios and differing hardware configurations. Previous work has shown that this flexibility can be exploited for sizeable performance improvement. Yet, the work to date has been chiefly targeted towards running the SPEC benchmarks on scientific workstations. We contend that this technology is also important to the desktop computing environment where running large, complex commercial software applications is commonplace.In this paper, we describe work that has been accomplished over the past several months at Microsoft Research to design and develop a dynamic software optimization system called Mojo. In particular, we present implementation details for the x86 architecture--Mojo\u2019s initial target. Additionally, we present our experience in supporting exception handling and multithreaded applications on Windows 2000 along with preliminary performance measurements.", "num_citations": "190\n", "authors": ["1811"]}
{"title": "Automatically proving the correctness of compiler optimizations\n", "abstract": " We describe a technique for automatically proving compiler optimizations sound, meaning that their transformations are always semantics-preserving. We first present a domain-specific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimization-specific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial\u00a0\u2026", "num_citations": "176\n", "authors": ["1811"]}
{"title": "Automated soundness proofs for dataflow analyses and transformations via local rules\n", "abstract": " We present Rhodium, a new language for writing compiler optimizations that can be automatically proved sound. Unlike our previous work on Cobalt, Rhodium expresses optimizations using explicit dataflow facts manipulated by local propagation and transformation rules. This new style allows Rhodium optimizations to be mutually recursively defined, to be automatically composed, to be interpreted in both flow-sensitive and -insensitive ways, and to be applied interprocedurally given a separate context-sensitivity strategy, all while retaining soundness. Rhodium also supports infinite analysis domains while guaranteeing termination of analysis. We have implemented a soundness checker for Rhodium and have specified and automatically proven the soundness of all of Cobalt's optimizations plus a variety of optimizations not expressible in Cobalt, including Andersen's points-to analysis, arithmetic-invariant\u00a0\u2026", "num_citations": "154\n", "authors": ["1811"]}
{"title": "Composing dataflow analyses and transformations\n", "abstract": " Dataflow analyses can have mutually beneficial interactions. Previous efforts to exploit these interactions have either (1) iteratively performed each individual analysis until no further improvements are discovered or (2) developed \"super-analyses\" that manually combine conceptually separate analyses. We have devised a new approach that allows analyses to be defined independently while still enabling them to be combined automatically and profitably. Our approach avoids the loss of precision associated with iterating individual analyses and the implementation difficulties of manually writing a super-analysis. The key to our approach is a novel method of implicit communication between the individual components of a super-analysis based on graph transformations. In this paper, we precisely define our approach; we demonstrate that it is sound and it terminates; finally we give experimental results showing that in\u00a0\u2026", "num_citations": "127\n", "authors": ["1811"]}
{"title": "WitchDoctor: IDE support for real-time auto-completion of refactorings\n", "abstract": " Integrated Development Environments (IDEs) have come to perform a wide variety of tasks on behalf of the programmer, refactoring being a classic example. These operations have undeniable benefits, yet their large (and growing) number poses a cognitive scalability problem. Our main contribution is WitchDoctor - a system that can detect, on the fly, when a programmer is hand-coding a refactoring. The system can then complete the refactoring in the background and propose it to the user long before the user can complete it. This implies a number of technical challenges. The algorithm must be 1) highly efficient, 2) handle unparseable programs, 3) tolerate the variety of ways programmers may perform a given refactoring, 4) use the IDE's proven and familiar refactoring engine to perform the refactoring, even though the the refactoring has already begun, and 5) support the wide range of refactorings present in\u00a0\u2026", "num_citations": "116\n", "authors": ["1811"]}
{"title": "Protecting C++ Dynamic Dispatch Through VTable Interleaving.\n", "abstract": " With new defenses against traditional control-flow attacks like stack buffer overflows, attackers are increasingly using more advanced mechanisms to take control of execution. One common such attack is vtable hijacking, in which the attacker exploits bugs in C++ programs to overwrite pointers to the virtual method tables (vtables) of objects. We present a novel defense against this attack. The key insight of our approach is a new way of laying out vtables in memory through careful ordering and interleaving. Although this layout is very different from a traditional layout, it is backwards compatible with the traditional way of performing dynamic dispatch. Most importantly, with this new layout, checking the validity of a vtable at runtime becomes an efficient range check, rather than a set membership test. Compared to prior approaches that provide similar guarantees, our approach does not use any profiling information, has lower performance overhead (about 1%) and has lower code bloat overhead (about 1.7%).", "num_citations": "83\n", "authors": ["1811"]}
{"title": "Equality-based translation validator for LLVM\n", "abstract": " We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks.", "num_citations": "62\n", "authors": ["1811"]}
{"title": "Validating high-level synthesis\n", "abstract": " The growing design-productivity gap has made designers shift toward using high-level languages like C, C++ and Java to do system-level design. High-Level Synthesis (HLS) is the process of generating Register Transfer Level (RTL) design from these initial high-level programs. Unfortunately, this translation process itself can be buggy, which can create a mismatch between what a designer intends and what is actually implemented in the circuit. In this paper, we present an approach to validate the result of HLS against the initial high-level program using insights from translation validation, automated theorem proving and relational approaches to reasoning about programs. We have implemented our validating technique and have applied it to a highly parallelizing HLS framework called SPARK. We present the details of our algorithm and experimental results.", "num_citations": "59\n", "authors": ["1811"]}
{"title": "Translation validation of high-level synthesis\n", "abstract": " The growing complexity of systems and their implementation into silicon encourages designers to look for ways to model designs at higher levels of abstraction and then incrementally build portions of these designs - automatically or manually - from these high-level specifications. Unfortunately, this translation process itself can be buggy, which can create a mismatch between what a designer intends and what is actually implemented in the circuit. Therefore, checking if the implementation is a refinement or equivalent to its initial specification is of tremendous value. In this paper, we present an approach to automatically validate the implementation against its initial high-level specification using insights from translation validation, automated theorem proving, and relational approaches to reasoning about programs. In our experiments, we first focus on concurrent systems modeled as communicating sequential\u00a0\u2026", "num_citations": "55\n", "authors": ["1811"]}
{"title": "Interactive parser synthesis by example\n", "abstract": " Despite decades of research on parsing, the construction of parsers remains a painstaking, manual process prone to subtle bugs and pitfalls. We present a programming-by-example framework called Parsify that is able to synthesize a parser from input/output examples. The user does not write a single line of code. To achieve this, Parsify provides: (a) an iterative algorithm for synthesizing and refining a grammar one example at a time, (b) an interface that provides immediate visual feedback in response to changes in the grammar being refined, and (c) a graphical mechanism for specifying example parse trees using only textual selections. We empirically demonstrate the viability of our approach by using Parsify to construct parsers for source code drawn from Verilog, SQL, Apache, and Tiger.", "num_citations": "45\n", "authors": ["1811"]}
{"title": "Generating compiler optimizations from proofs\n", "abstract": " We present an automated technique for generating compiler optimizations from examples of concrete programs before and after improvements have been made to them. The key technical insight of our technique is that a proof of equivalence between the original and transformed concrete programs informs us which aspects of the programs are important and which can be discarded. Our technique therefore uses these proofs, which can be produced by translation validation or a proof-carrying compiler, as a guide to generalize the original and transformed programs into broadly applicable optimization rules. We present a category-theoretic formalization of our proof generalization technique. This abstraction makes our technique applicable to logics besides our own. In particular, we demonstrate how our technique can also be used to learn query optimizations for relational databases or to aid programmers in\u00a0\u2026", "num_citations": "39\n", "authors": ["1811"]}
{"title": "Beyond refactoring: a framework for modular maintenance of crosscutting design idioms\n", "abstract": " Despite the automated refactoring support provided by today's IDEs many program transformations that are easy to conceptualize--such as improving the implementation of a design pattern--are not supported and are hence hard to perform. We propose an extension to the refactoring paradigm that provides for the modular maintenance of crosscutting design idioms, supporting both substitutability of design idiom implementations and the checking of essential constraints. We evaluate this new approach through the design and use of Arcum, an IDE-based mechanism for declaring, checking, and evolving crosscutting design idioms.", "num_citations": "30\n", "authors": ["1811"]}
{"title": "System and method for performing a path-sensitive verification on a program\n", "abstract": " Described is a method and system that performs path-sensitive verification on programs having any code base size. The method maintains a symbolic store that includes symbolic states. Each symbolic state includes a concrete state and an abstract state. The abstract state identifies a state in which the property being tested currently exists. The concrete state identifies other properties of the program. The symbolic store is updated at each node in a logic path of the program with changes in the abstract state and the concrete state. The updates occur such that the symbolic states associated with a particular edge of any node will not have identical abstract states. Rather, in this case, the symbolic states are merged by combining the concrete states to include content that is similar in both symbolic states. In addition, the concrete state determines relevant paths to proceed along in the logic path.", "num_citations": "27\n", "authors": ["1811"]}
{"title": "Towards verification of hybrid systems in a foundational proof assistant\n", "abstract": " Unsafe behavior of hybrid systems can have disastrous consequences, motivating the need for formal verification of the software running on these systems. Foundational verification in a proof assistant such as Coq is a promising technique that can provide extremely strong, foundational, guarantees about software systems. In this paper, we show how to apply this technique to hybrid systems. We define a TLA-inspired formalism in Coq for reasoning about hybrid systems and use it to verify two quadcopter modules: the first limits the quadcopter's velocity and the second limits its altitude. We ran both of these modules on an actual quadcopter, and they worked as intended. We also discuss lessons learned from our experience foundationally verifying hybrid systems.", "num_citations": "26\n", "authors": ["1811"]}
{"title": "Retrofitting fine grain isolation in the Firefox renderer\n", "abstract": " Firefox and other major browsers rely on dozens of third-party libraries to render audio, video, images, and other content. These libraries are a frequent source of vulnerabilities. To mitigate this threat, we are migrating Firefox to an architecture that isolates these libraries in lightweight sandboxes, dramatically reducing the impact of a compromise.", "num_citations": "25\n", "authors": ["1811"]}
{"title": "Taming wildcards in Java's type system\n", "abstract": " Wildcards have become an important part of Java's type system since their introduction 7 years ago. Yet there are still many open problems with Java's wildcards. For example, there are no known sound and complete algorithms for subtyping (and consequently type checking) Java wildcards, and in fact subtyping is suspected to be undecidable because wildcards are a form of bounded existential types. Furthermore, some Java types with wildcards have no joins, making inference of type arguments for generic methods particularly difficult. Although there has been progress on these fronts, we have identified significant shortcomings of the current state of the art, along with new problems that have not been addressed.", "num_citations": "25\n", "authors": ["1811"]}
{"title": "Programming Languages\n", "abstract": " \u2022 Done alone, unless otherw ise specified\u2013Will use plagiarism detection softw are\u2013Please see academ ic integrity statem ent\u2022 Scoring: Style+ Test suite\u2022 Forget Java/C: Im m erse yourself in new PL\u2022 Assignm ent# 1 on w eb. Due Jan 19, 4: 59: 00pm", "num_citations": "25\n", "authors": ["1811"]}
{"title": "Dead store elimination (still) considered harmful\n", "abstract": " Dead store elimination is a widely used compiler optimization that reduces code size and improves performance. However, it can also remove seemingly useless memory writes that the programmer intended to clear sensitive data after its last use. Security-savvy developers have long been aware of this phenomenon and have devised ways to prevent the compiler from eliminating these data scrubbing operations.", "num_citations": "22\n", "authors": ["1811"]}
{"title": "Codespells: how to design quests to teach java concepts\n", "abstract": " Serious games are a good approach to teaching computer science [7]. But there are still complications that arise, for example, no access to an instructor. This paper presents a study conducted using CodeSpells, a 3D immersive video game that aims to teach novice programmers basic Java concepts [3]. This paper specifically addresses the design of the quests in CodeSpells that provide scaffolding to support students in learning. The study analyzed how 16 students aged 8--12 understood and modified basic Java programs to complete quests. Based on game-play from an exploratory study, quests were added to engage students earlier and in more complex code edits. Both student understanding of programming and their comfort with modifying code was studied. This paper presents findings and lessons learned in quest design, and shows that quest design should set the expectation for students to engage with\u00a0\u2026", "num_citations": "21\n", "authors": ["1811"]}
{"title": "Polymorphic blocks: Formalism-inspired UI for structured connectors\n", "abstract": " We present a novel block-based UI called Polymorphic Blocks, in which a connector's shape visually represents the structure of the data being passed through the connector. We use Polymorphic Blocks to add visual type information to block-based programming environments like Blockly or Scratch. We also use Polymorphic Blocks to represent logical proofs. In this context, if we erase all symbols, our UI becomes a puzzle game, where solving the puzzle amounts to building a proof. We show through a user study that our Logical Puzzle Game is faster, more fun, and more engaging than an equivalent pen-and-paper interface.", "num_citations": "20\n", "authors": ["1811"]}
{"title": "Automated refinement checking of concurrent systems\n", "abstract": " Stepwise refinement is at the core of many approaches to synthesis and optimization of hardware and software systems. For instance, it can be used to build a synthesis approach for digital circuits from high level specifications. It can also be used for post-synthesis modification such as in engineering change orders (ECOs). Therefore, checking if a system, modeled as a set of concurrent processes, is a refinement of another is of tremendous value. In this paper, we focus on concurrent systems modeled as communicating sequential processes (CSP) and show their refinements can be validated using insights from translation validation, automated theorem proving and relational approaches to reasoning about programs. The novelty of our approach is that it handles infinite state spaces in a fully automated manner. We have implemented our refinement checking technique and have applied it to a variety of\u00a0\u2026", "num_citations": "19\n", "authors": ["1811"]}
{"title": "Automatic inference of optimizer flow functions from semantic meanings\n", "abstract": " Previous work presented a language called Rhodium for writing program analyses and transformations, in the form of declarative flow functions that propagate instances of user-defined dataflow fact schemas. Each dataflow fact schema specifies a semantic meaning, which allows the Rhodium system to automatically verify the correctness of the user's flow functions. In this work, we have reversed the roles of the flow functions and semantic meanings: rather than checking the correctness of the user-written flow functions using the facts' semantic meanings, we automatically infer correct flow functions solely from the meanings of the dataflow fact schemas. We have implemented our algorithm for inferring flow functions from fact schemas in the context of the Whirlwind compiler, and have used this implementation to infer flow functions for a variety of fact schemas. The automatically generated flow functions cover most of\u00a0\u2026", "num_citations": "19\n", "authors": ["1811"]}
{"title": "Inferring loop invariants through gamification\n", "abstract": " In today's modern world, bugs in software systems incur significant costs. One promising approach to improve software quality is automated software verification. In this approach, an automated tool tries to prove the software correct once and for all. Although significant progress has been made in this direction, there are still many cases where automated tools fail. We focus specifically on one aspect of software verification that has been notoriously hard to automate: inferring loop invariants that are strong enough to enable verification. In this paper, we propose a solution to this problem through gamification and crowdsourcing. In particular, we present a puzzle game where players find loop invariants without being aware of it, and without requiring any expertise on software verification. We show through an experiment with Mechanical Turk users that players enjoy the game, and are able to solve verification tasks that\u00a0\u2026", "num_citations": "18\n", "authors": ["1811"]}
{"title": "Formal verification of stability properties of cyber-physical systems\n", "abstract": " We increasingly rely on computers to interact with the physical world for us. At the large end, software underlies the control systems of commercial aircraft and power plants, and at the small end it controls medical devices and hobbyist UAVs. The failure of any of these systems can have severe consequences which are often measured in the loss of human lives. Formal verification has proven a promising approach to achieving very strong guarantees in more classic areas of computer science. In this work we present an overview of our experiences formalizing stability properties of cyber-physical systems (CPSs) using the Coq proof assistant. In particular, we describe and contrast two approaches for proving the stability of the linear, one-dimensional proportional controller (P-controller) depicted in Figure 1. This system runs in a loop where the controller sets the velocity (v) of the system and then the position (x) evolves continuously according to the differential equation x= v for at most\u2206 time while v remains constant. The goal of the controller is to move the system to x= 0.", "num_citations": "18\n", "authors": ["1811"]}
{"title": "Projection Boxes: On-the-fly Reconfigurable Visualization for Live Programming\n", "abstract": " Live programming is a regime in which the programming environment provides continual feedback, most often in the form of runtime values. In this paper, we present Projection Boxes, a novel visualization technique for displaying runtime values of programs. The key idea behind projection boxes is to start with a full semantics of the program, and then use projections to pick a subset of the semantics to display. By varying the projection used, projection boxes can encode both previously known visualization techniques, and also new ones. As such, projection boxes provide an expressive and configurable framework for displaying runtime information. Through a user study we demonstrate that (1) users find projection boxes and their configurability useful (2) users are not distracted by the always-on visualization (3) a key driving force behind the need for a configurable visualization for live programming lies with the\u00a0\u2026", "num_citations": "15\n", "authors": ["1811"]}
{"title": "Safety verification using barrier certificates with application to double integrator with input saturation and zero-order hold\n", "abstract": " It is desirable to increase the control sample period in cyber-physical systems such that the available processing power and data transfer are minimized. In this paper, a safety verification technique is proposed which allows a trade-off between the size of the sample period and the convergence rate. The proposed technique is applied to a double integrator with input saturation and zero-order hold. An expression for the control law and an explicit relationship between sample period and control parameters are presented. It is shown that the proposed control law drives all state trajectories initiated in the safe set to the origin without violating safety criteria as long as the sample period remains sufficiently small. Moreover, if the control signal is bounded between the proposed limits, it is shown analytically that a pilot-based configuration also remains in the safe set. The effectiveness of the proposed technique is verified\u00a0\u2026", "num_citations": "13\n", "authors": ["1811"]}
{"title": "Generating correctness proofs with neural networks\n", "abstract": " Foundational verification allows programmers to build software which has been empirically shown to have high levels of assurance in a variety of important domains. However, the cost of producing foundationally verified software remains prohibitively high for most projects, as it requires significant manual effort by highly trained experts. In this paper we present Proverbot9001, a proof search system using machine learning techniques to produce proofs of software correctness in interactive theorem provers. We demonstrate Proverbot9001 on the proof obligations from a large practical proof project, the CompCert verified C compiler, and show that it can effectively automate what were previously manual proofs, automatically producing proofs for 28% of theorem statements in our test dataset, when combined with solver-based tooling. Without any additional solvers, we exhibit a proof completion rate that is a 4X\u00a0\u2026", "num_citations": "12\n", "authors": ["1811"]}
{"title": "Towards foundational verification of cyber-physical systems\n", "abstract": " The safety-critical aspects of cyber-physical systems motivate the need for rigorous analysis of these systems. In the literature this work is often done using idealized models of systems where the analysis can be carried out using high-level reasoning techniques such as Lyapunov functions and model checking. In this paper we present VERIDRONE, a foundational framework for reasoning about cyber-physical systems at all levels from high-level models to C code that implements the system. VERIDRONE is a library within the Coq proof assistant enabling us to build on its foundational implementation, its interactive development environments, and its wealth of libraries capturing interesting theories ranging from real numbers and differential equations to verified compilers and floating point numbers. These features make proof assistants in general, and Coq in particular, a powerful platform for unifying foundational\u00a0\u2026", "num_citations": "12\n", "authors": ["1811"]}
{"title": "REPLica: REPL instrumentation for Coq analysis\n", "abstract": " Proof engineering tools make it easier to develop and maintain large systems verified using interactive theorem provers. Developing useful proof engineering tools hinges on understanding the development processes of proof engineers. This paper breaks down one barrier to achieving that understanding: remotely collecting granular data on proof developments as they happen.", "num_citations": "10\n", "authors": ["1811"]}
{"title": "Modular deductive verification of sampled-data systems\n", "abstract": " Unsafe behavior of cyber-physical systems can have disastrous consequences, motivating the need for formal verification of these kinds of systems. Deductive verification in a proof assistant such as Coq is a promising technique for this verification because it (1) justifies all verification from first principles,(2) is not limited to classes of systems for which full automation is possible, and (3) provides a platform for proving powerful, higher-order modularity theorems that are crucial for scaling verification to complex systems. In this paper, we demonstrate the practicality, utility, and scalability of this approach by developing in Coq sound and powerful rules for modular construction and verification of sampled-data cyber-physical systems. We evaluate these rules by using them to verify a number of non-trivial controllers enforcing safety properties of a quadcopter, eg a geo-fence. We show that our controllers are realistic by\u00a0\u2026", "num_citations": "9\n", "authors": ["1811"]}
{"title": "C-to-verilog translation validation\n", "abstract": " To offset the high engineering cost of digital circuit design, hardware engineers are looking increasingly toward high-level languages such as C and C++ to implement their designs. To do this, they employ High-Level Synthesis (HLS) tools that translate their high-level specifications down to a hardware description language such as Verilog. Unfortunately, HLS tools themselves employ sophisticated optimization passes that may have bugs that silently introduce errors in realized hardware. The cost of such errors is high, as hardware is costly or impossible to repair if software patching is not an option. In this work, we present a translation validation approach for verifying the correctness of the HLS translation process. Given an initial C program and the generated Verilog code, our approach establishes their equivalence without relying on any intermediate results or representations produced by the HLS tool. We\u00a0\u2026", "num_citations": "9\n", "authors": ["1811"]}
{"title": "Small-Step Live Programming by Example\n", "abstract": " Live programming is a paradigm in which the programming environment continually displays runtime values. Program synthesis is a technique that can generate programs or program snippets from examples.\\deltextThis paper presents a new programming paradigm called Synthesis-Aided Live Programming that combines these two prior ideas in a synergistic way. When using Synthesis-Aided Live Programming, programmers can change the runtime values displayed by the live\\addtextPrevious works that combine the two have taken a holistic approach to the way examples describe the behavior of functions and programs. This paper presents a new programming paradigm called Small-Step Live Programming by Example that lets the user apply Programming by Example locally. When using Small-Step Live Programming by Example, programmers can change the runtime values displayed by the live visualization to\u00a0\u2026", "num_citations": "8\n", "authors": ["1811"]}
{"title": "Cobalt: A language for writing provably-sound compiler optimizations\n", "abstract": " We overview the current status and future directions of the Cobalt project. Cobalt is a domain-specific language for implementing compiler optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. The design of Cobalt engenders a natural inductive strategy for proving the soundness of optimizations. This strategy is fully automated by requiring an automatic theorem prover to discharge a small set of simple proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. The\u00a0\u2026", "num_citations": "8\n", "authors": ["1811"]}
{"title": "Gobi: WebAssembly as a practical path to library sandboxing\n", "abstract": " Software based fault isolation (SFI) is a powerful approach to reduce the impact of security vulnerabilities in large C/C++ applications like Firefox and Apache. Unfortunately, practical SFI tools have not been broadly available. Developing SFI toolchains are a significant engineering challenge. Only in recent years have browser vendors invested in building production quality SFI tools like Native Client (NaCl) to sandbox code. Further, without committed support, these tools are not viable, e.g. NaCl has been discontinued, orphaning projects that relied on it. WebAssembly (Wasm) offers a promising solution---it can support high performance sandboxing and has been embraced by all major browser vendors---thus seems to have a viable future. However, Wasm presently only offers a solution for sandboxing mobile code. Providing SFI for native application, such as C/C++ libraries requires additional steps. To reconcile the different worlds of Wasm on the browser and native platforms, we present Gobi. Gobi is a system of compiler changes and runtime support that can sandbox normal C/C++ libraries with Wasm---allowing them to be compiled and linked into native applications. Gobi has been tested on libjpeg, libpng, and zlib. Based on our experience developing Gobi, we conclude with a call to arms to the Wasm community and SFI research community to make Wasm based module sandboxing a first class use case and describe how this can significantly benefit both communities. Addendum: This short paper was originally written in January of 2019. Since then, the implementation and design of Gobi has evolved substantially as some of the issues\u00a0\u2026", "num_citations": "6\n", "authors": ["1811"]}
{"title": "Parsimony: An IDE for example-guided synthesis of lexers and parsers\n", "abstract": " We present Parsimony, a programming-by-example development environment for synthesizing lexers and parsers by example. Parsimony provides a graphical interface in which the user presents examples simply by selecting and labeling sample text in a text editor. An underlying synthesis engine then constructs syntactic rules to solve the system of constraints induced by the supplied examples. Parsimony is more expressive and usable than prior programming-by-example systems for parsers in several ways: Parsimony can (1) synthesize lexer rules in addition to productions, (2) solve for much larger constraint systems over multiple examples, rather than handling examples one-at-a-time, and (3) infer much more complex sets of productions, such as entire algebraic expression grammars, by detecting instances of well-known grammar design patterns. The results of a controlled user study across 18 participants\u00a0\u2026", "num_citations": "6\n", "authors": ["1811"]}
{"title": "High-Level Verification\n", "abstract": " The growth in size and heterogeneity of System-on-Chip (SOC) design makes their design process from initial specification to IC implementation complex. System-level design methods seek to combat this complexity by shifting increasing design burden to high-level languages such as SystemC and SystemVerilog. Such languages not only make a design easier to describe using high-level abstractions, but also provide a path for systematic implementation through refinement and elaboration of such descriptions. In principle, this can enable a greater exploration of design alternatives and thus better design optimization than possible using lower level design methods. To achieve these goals, however, verification capabilities that seek to validate designs at higher levels as well their equivalences with lower level implementations are crucially needed. To the extent possible given the large space of design alternatives, such validation must be formal to ensure the design and important properties are provably correct against various implementation choices. In this paper, we present a survey of high-level verification techniques that are used for both verification and validation of high-level designs, that is, designs modeled using high-level programming languages. These techniques include those based on model checking, theorem proving and approaches that integrate a combination of the above methods. The high-level verification approaches address verification of properties as well as equivalence checking with refined implementations. We also focus on techniques that use information from the synthesis process for improved validation. Finally, we\u00a0\u2026", "num_citations": "6\n", "authors": ["1811"]}
{"title": "Towards a verified range analysis for JavaScript JITs\n", "abstract": " We present VeRA, a system for verifying the range analysis pass in browser just-in-time (JIT) compilers. Browser developers write range analysis routines in a subset of C++, and verification developers write infrastructure to verify custom analysis properties. Then, VeRA automatically verifies the range analysis routines, which browser developers can integrate directly into the JIT. We use VeRA to translate and verify Firefox range analysis routines, and it detects a new, confirmed bug that has existed in the browser for six years.", "num_citations": "5\n", "authors": ["1811"]}
{"title": "User-guided synthesis of interactive diagrams\n", "abstract": " Interactive diagrams are expensive to build, requiring significant programming experience. The cost of building such diagrams often prevents novice programmers or non-programmers from doing so. In this paper, we present user-guided techniques that transform a static diagram into an interactive one without requiring the user to write code. We also present a tool called EDDIE that prototypes these techniques. We evaluate EDDIE through:(1) a case study in which we use EDDIE to implement existing real-world diagrams from the literature and (2) a usability session with target users in which subjects build several diagrams in EDDIE and provide feedback on EDDIE's user experience. Our experiments demonstrate that EDDIE is usable and expressive, and that EDDIE enables real-world diagrams to be implemented without requiring programming expertise.", "num_citations": "5\n", "authors": ["1811"]}
{"title": "Latent variable models for predicting file dependencies in large-scale software development\n", "abstract": " When software developers modify one or more files in a large code base, they must also identify and update other related files. Many file dependencies can be detected by mining the development history of the code base: in essence, groups of related files are revealed by the logs of previous workflows. From data of this form, we show how to detect dependent files by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we find that LVMs improve the performance of related file prediction over current leading methods.", "num_citations": "5\n", "authors": ["1811"]}
{"title": "Automatically proving the correctness of program analyses and transformations\n", "abstract": " In this dissertation, I describe a technique for automatically proving compiler optimizations sound, meaning that their transformations are always semantics-preserving. I first present a domain-specific language, called Rhodium, for implementing optimizations using local propagation and transformation rules that manipulate explicit dataflow facts. Then I describe a technique for automatically proving the soundness of Rhodium optimizations. The technique requires an automatic theorem prover to discharge a simple proof obligation for each propagation and transformation rule.", "num_citations": "5\n", "authors": ["1811"]}
{"title": "Addressing common crosscutting problems with arcum\n", "abstract": " Crosscutting is an inherent part of software development and can typically be managed through modularization: A module's stable properties are defined in an interface while its likely-to-change properties are encapsulated within the module [19]. The crosscutting of the stable properties, such as class and method names, can be mitigated with automated refactoring tools that allow, for example, the interface's elements to be renamed [9, 18]. However, often the crosscutting from design idioms (such as design patterns and coding styles) are so specific to the program's domain that their crosscutting would not likely have been anticipated by the developers of an automated refactoring system.", "num_citations": "4\n", "authors": ["1811"]}
{"title": "\u0414\u043e\u0432\u0435\u0440\u044f\u0439, \u043d\u043e \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0439: SFI safety for native-compiled Wasm\n", "abstract": " WebAssembly (Wasm) is a platform-independent bytecode that offers both good performance and runtime isolation. To implement isolation, the compiler inserts safety checks when it compiles Wasm to native machine code. While this approach is cheap, it also requires trust in the compiler\u2019s correctness\u2014trust that the compiler has inserted each necessary check, correctly formed, in each proper place. Unfortunately, subtle bugs in the Wasm compiler can break\u2014and have broken\u2014isolation guarantees. To address this problem, we propose verifying memory isolation of Wasm binaries post-compilation. We implement this approach in VeriWasm, a static offline verifier for native x86-64 binaries compiled from Wasm; we prove the verifier\u2019s soundness, and find that it can detect bugs with no false positives. Finally, we describe our deployment of VeriWasm at Fastly.", "num_citations": "3\n", "authors": ["1811"]}
{"title": "High-Level Verification: Methods and Tools for Verification of System-Level Designs\n", "abstract": " Given the growing size and heterogeneity of Systems on Chip (SOC), the design process from initial specification to chip fabrication has become increasingly complex. This growing complexity provides incentive for designers to use high-level languages such as C, SystemC, and SystemVerilog for system-level design. While a major goal of these high-level languages is to enable verification at a higher level of abstraction, allowing early exploration of system-level designs, the focus so far for validation purposes has been on traditional testing techniques such as random testing and scenario-based testing. This book focuses on high-level verification, presenting a design methodology that relies upon advances in synthesis techniques as well as on incremental refinement of the design process. These refinements can be done manually or through elaboration tools. This book discusses verification of specific properties in designs written using high-level languages, as well as checking that the refined implementations are equivalent to their high-level specifications. The novelty of each of these techniques is that they use a combination of formal techniques to do scalable verification of system designs completely automatically. The verification techniques presented in this book include methods for verifying properties of high-level designs and methods for verifying that the translation from high-level design to a low-level Register Transfer Language (RTL) design preserves semantics. Used together, these techniques guarantee that properties verified in the high-level design are preserved through the translation to low-level RTL.", "num_citations": "3\n", "authors": ["1811"]}
{"title": "When refactoring acts like modularity: Keeping options open with persistent condition checking\n", "abstract": " Oftentimes the changes required to improve the design of code are crosscutting in nature and thus easier to perform with the assistance of automated refactoring tools. However, the developers of such refactoring tools cannot anticipate every practical transformation, particularly those that are specific to the program's domain. We demonstrate Arcum, a declarative language for describing and performing both general and domain-specific transformations.", "num_citations": "3\n", "authors": ["1811"]}
{"title": "Retrofitting Fine Grain Isolation in the Firefox Renderer (Extended Version)\n", "abstract": " Firefox and other major browsers rely on dozens of third-party libraries to render audio, video, images, and other content. These libraries are a frequent source of vulnerabilities. To mitigate this threat, we are migrating Firefox to an architecture that isolates these libraries in lightweight sandboxes, dramatically reducing the impact of a compromise. Retrofitting isolation can be labor-intensive, very prone to security bugs, and requires critical attention to performance. To help, we developed RLBox, a framework that minimizes the burden of converting Firefox to securely and efficiently use untrusted code. To enable this, RLBox employs static information flow enforcement, and lightweight dynamic checks, expressed directly in the C++ type system. RLBox supports efficient sandboxing through either software-based-fault isolation or multi-core process isolation. Performance overheads are modest and transient, and have only minor impact on page latency. We demonstrate this by sandboxing performance-sensitive image decoding libraries ( libjpeg and libpng ), video decoding libraries ( libtheora and libvpx ), the libvorbis audio decoding library, and the zlib decompression library. RLBox, using a WebAssembly sandbox, has been integrated into production Firefox to sandbox the libGraphite font shaping library.", "num_citations": "2\n", "authors": ["1811"]}
{"title": "Seamless Integration of Coding and Gameplay: Writing Code Without Knowing it.\n", "abstract": " Numerous designers and researchers have called for seamless integration of education and play in educational games. In the domain of games that teach coding, seamless integration has not been achieved. We present a system (The Orb Game) to demonstrate an extreme level of integration: in which the coding is so seamlessly integrated that players do not realize they are coding. Our evaluation shows that the integration was successful: players felt that they were playing a game and did not realize they were programming. We also present a generalized framework called Programming by Gaming (PbG) to guide the design of other such games.", "num_citations": "2\n", "authors": ["1811"]}
{"title": "A framework for the checking and refactoring of crosscutting concepts\n", "abstract": " Programmers employ crosscutting concepts, such as design patterns and other programming idioms, when their design ideas cannot be efficiently or effectively modularized in the underlying programming language. As a result, implementations of these crosscutting concepts can be hard to change even when the code is well structured. In this article, we describe Arcum, a system that supports the modular maintenance of crosscutting concepts. Arcum can be used to both check essential constraints of crosscutting concepts and to substitute crosscutting concept implementations with alternative implementations. Arcum is complementary to existing refactoring systems that focus on meaning-preserving program transformations at the programming-language-semantics level, because Arcum focuses on transformations at the conceptual level. We present the underpinnings of the Arcum approach and show how Arcum\u00a0\u2026", "num_citations": "2\n", "authors": ["1811"]}
{"title": "A Computational Stack for Cross-Domain Acceleration\n", "abstract": " Domain-specific accelerators obtain performance benefits by restricting their algorithmic domain. These accelerators utilize specialized languages constrained to particular hardware, thus trading off expressiveness for high performance. The pendulum has swung from one hardware for all domains (general-purpose processors) to one hardware per individual domain. The middle-ground on this spectrum\u2013which provides a unified computational stack across multiple, but not all, domains\u2013 is an emerging and open research challenge. This paper sets out to explore this region and its associated tradeoff between expressiveness and performance by defining a cross-domain stack, dubbed PolyMath. This stack defines a high-level cross-domain language (CDL), called PMLang, that in a modular and reusable manner encapsulates mathematical properties to be expressive across multiple domains\u2013Robotics, Graph\u00a0\u2026", "num_citations": "1\n", "authors": ["1811"]}
{"title": "Gamification to aid the learning of test coverage concepts\n", "abstract": " The ability to effectively and efficiently test software is an important practice in software testing that is often under-emphasized in computer science education. Many students find learning about testing to be uninteresting and difficult to learn. This causes numerous students to develop inadequate testing habits, which can be detrimental to their professional careers. To encourage students to develop better testing habits, we used gamification to make the learning experience more engaging and enjoyable. In this paper we explore this idea by integrating gamification and statement coverage into a turn-based game called CoverBot. To test the effectiveness of CoverBot with respect to both teaching statement coverage and increasing engagement and enjoyment, we conducted a user study. We found that gamification makes the learning about statement coverage more engaging and enjoyable while also enhancing the\u00a0\u2026", "num_citations": "1\n", "authors": ["1811"]}
{"title": "Focused Live Programming with Loop Seeds\n", "abstract": " Live programming is a paradigm in which the programmer can visualize the runtime values of the program each time the program changes. The promise of live programming depends on using test cases to run the program and thereby provide these runtime values. In this paper we show that in some situations test cases are insufficient in a fundamental way, in that there are no test inputs that can drive certain incomplete loops to produce useful data, a problem we call the loop-datavoid problem. The problem stems from the fact that useful data inside the loop might only be produced after the loop has been fully written. To solve this problem, we propose a paradigm called Focused Live Programming with Loop Seeds, in which the programmer provides hypothetical values to start a loop iteration, and then the programming environment focuses the live visualization on this hypothetical loop iteration. We introduce the\u00a0\u2026", "num_citations": "1\n", "authors": ["1811"]}
{"title": "Automated refinement checking of CSP programs\n", "abstract": " Communicating Sequential Processes (CSP) is a calculus for describing concurrent systems as a collection of processes communicating over channels. Checking that one CSP program is a refinement of another has many applications, including type-checking concurrent systems, and refinement-based synthesis and optimization of hardware circuits. In this paper, we describe a new approach to automatic refinement checking of CSP programs that uses insights from translation validation, automated theorem proving, and relational approaches to reasoning about programs. Unlike previous approaches to CSP refinement, our technique can handle infinite state spaces in a fully automated manner. We have implemented our refinement checking technique and have applied it to a variety of refinements. We present the details of our algorithm and experimental results. As an example, we were able to automatically\u00a0\u2026", "num_citations": "1\n", "authors": ["1811"]}
{"title": "Automatic inference of dataflow analyses\n", "abstract": " Much of the work of defining compiler optimizations is in writing dataflow analysis flow functions. We leverage some properties of the Rhodium optimization language to automatically infer sound dataflow analyses. Our technique infers 77% of the rules written by hand by an expert and infers many rules that cover cases omitted in the handwritten rules.", "num_citations": "1\n", "authors": ["1811"]}