{"title": "Safety verification of deep neural networks\n", "abstract": " Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it. With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety. We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT). We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image. We enable\u00a0\u2026", "num_citations": "680\n", "authors": ["1399"]}
{"title": "Reachability Analysis of Deep Neural Networks with Provable Guarantees\n", "abstract": " Verifying correctness of deep neural networks (DNNs) is challenging. We study a generic reachability problem for feed-forward DNNs which, for a given set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency, scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.", "num_citations": "180\n", "authors": ["1399"]}
{"title": "Feature-guided black-box safety testing of deep neural networks\n", "abstract": " Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. Most existing approaches for crafting adversarial examples necessitate some knowledge (architecture, parameters, etc) of the network at hand. In this paper, we focus on image classifiers and propose a feature-guided black-box approach to test the safety of deep neural networks that requires no such knowledge. Our algorithm employs object detection techniques such as SIFT (Scale Invariant Feature Transform) to extract features from an image. These features are converted into a mutable saliency distribution, where high probability is assigned to pixels that affect the composition of the image with respect to the human visual system. We formulate the crafting of adversarial examples as a two-player turn-based stochastic game, where the first player\u2019s objective is to minimise\u00a0\u2026", "num_citations": "176\n", "authors": ["1399"]}
{"title": "A game-based approximate verification of deep neural networks with provable guarantees\n", "abstract": " Despite the improved accuracy of deep neural networks, the discovery of adversarial examples has raised serious safety concerns. In this paper, we study two variants of pointwise robustness, the maximum safe radius problem, which for a given input sample computes the minimum distance to an adversarial example, and the feature robustness problem, which aims to quantify the robustness of individual features to adversarial perturbations. We demonstrate that, under the assumption of Lipschitz continuity, both problems can be approximated using finite optimisation by discretising the input space, and the approximation has provable guarantees, i.e., the error is bounded. We then show that the resulting optimisation problems can be reduced to the solution of two-player turn-based games, where the first player selects features and the second perturbs the image within the feature. While the second player aims to\u00a0\u2026", "num_citations": "70\n", "authors": ["1399"]}
{"title": "An epistemic strategy logic\n", "abstract": " This article presents an extension of temporal epistemic logic with operators that can express quantification over agent strategies. Unlike previous work on alternating temporal epistemic logic, the semantics works with systems whose states explicitly encode the strategy being used by each of the agents. This provides a natural way to express what agents would know were they to be aware of some of the strategies being used by other agents. A number of examples that rely on the ability to express an agent\u2019s knowledge about the strategies being used by other agents are presented to motivate the framework, including reasoning about game-theoretic equilibria, knowledge-based programs, and information-theoretic computer security policies. Relationships to several variants of alternating temporal epistemic logic are discussed. The computational complexity of model checking the logic and several of its fragments\u00a0\u2026", "num_citations": "61\n", "authors": ["1399"]}
{"title": "Symbolic model checking epistemic strategy logic\n", "abstract": " This paper presents a symbolic BDD-based model checking algorithm for an epistemic strategy logic with observational semantics. The logic has been shown to be more expressive than several variants of ATELand therefore the algorithm can also be used for ATEL model checking. We implement the algorithm in a model checker and apply it to an application on train control system. The performance of the algorithm is also reported, with a comparison showing improved results over a previous partially symbolic approach for ATEL model checking.", "num_citations": "43\n", "authors": ["1399"]}
{"title": "Probabilistic alternating-time temporal logic of incomplete information and synchronous perfect recall\n", "abstract": " A probabilistic variant of ATL* logic is proposed to work with multi-player games of incomplete information and synchronous perfect recall. The semantics of the logic is settled over probabilistic interpreted system and partially observed probabilistic concurrent game structure. While unexpectedly, the model checking problem is in general undecidable even for single-group fragment, we find a fragment whose complexity is in 2-EXPTIME. The usefulness of this fragment is shown over a land search scenario.", "num_citations": "28\n", "authors": ["1399"]}
{"title": "Symbolic model checking of probabilistic knowledge\n", "abstract": " This paper describes an algorithm for model checking a fragment of the logic of knowledge and probability in multi-agent systems, with respect to a perfect recall interpretation of knowledge and agents' subjective probability. The algorithm has been implemented in the epistemic model checker MCK. Some experiments with the implemented algorithm are reported, in which some properties of agents' probabilistic knowledge are verified in two security protocols: Chaum's Dining Cryptographers protocol, and a protocol for Oblivious Transfer due to Rivest.", "num_citations": "27\n", "authors": ["1399"]}
{"title": "Reasoning about cognitive trust in stochastic multiagent systems\n", "abstract": " We consider the setting of stochastic multiagent systems modelled as stochastic multiplayer games and formulate an automated verification framework for quantifying and reasoning about agents\u2019 trust. To capture human trust, we work with a cognitive notion of trust defined as a subjective evaluation that agent A makes about agent B\u2019s ability to complete a task, which in turn may lead to a decision by A to rely on B. We propose a probabilistic rational temporal logic PRTL*, which extends the probabilistic computation tree logic PCTL* with reasoning about mental attitudes (beliefs, goals, and intentions) and includes novel operators that can express concepts of social trust such as competence, disposition, and dependence. The logic can express, for example, that \u201cagent A will eventually trust agent B with probability at least p that B will behave in a way that ensures the successful completion of a given task.\u201d We study\u00a0\u2026", "num_citations": "26\n", "authors": ["1399"]}
{"title": "Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the  Norm\n", "abstract": " Deployment of deep neural networks (DNNs) in safety- or security-critical systems requires provable guarantees on their correct behaviour. A common requirement is robustness to adversarial perturbations in a neighbourhood around an input. In this paper we focus on the  norm and aim to compute, for a trained DNN and an input, the maximal radius of a safe norm ball around the input within which there are no adversarial examples. Then we define global robustness as an expectation of the maximal safe radius over a test data set. We first show that the problem is NP-hard, and then propose an approximate approach to iteratively compute lower and upper bounds on the network's robustness. The approach is \\emph{anytime}, i.e., it returns intermediate bounds and robustness estimates that are gradually, but strictly, improved as the computation proceeds; \\emph{tensor-based}, i.e., the computation is conducted over a set of inputs simultaneously, instead of one by one, to enable efficient GPU computation; and has \\emph{provable guarantees}, i.e., both the bounds and the robustness estimates can converge to their optimal values. Finally, we demonstrate the utility of the proposed approach in practice to compute tight bounds by applying and adapting the anytime algorithm to a set of challenging problems, including global robustness evaluation, competitive  attacks, test case generation for DNNs, and local robustness evaluation on large-scale ImageNet DNNs. We release the code of all case studies via GitHub.", "num_citations": "22\n", "authors": ["1399"]}
{"title": "A logic of probabilistic knowledge and strategy\n", "abstract": " The ability of reasoning about knowledge and strategy is key to the autonomy of an intelligent system of multiple players. In this paper, we study the logic of knowledge and strategy in stochastic multiagent systems, where the system\u2019s behaviour is determined by both the behaviour of the players and by some random elements. Players have incomplete information about the system and do not have memory. A logic PATEL\u2217, whose semantics is based on partiallyobserved concurrent game structures, is proposed. The computational complexities of model checking the logic and its sublogics are solved.", "num_citations": "20\n", "authors": ["1399"]}
{"title": "A temporal logic of strategic knowledge\n", "abstract": " The paper presents an extension of temporal epistemic logic that adds\" strategic\" agents in a way that allows standard epistemic operators to capture what agents could deduce from knowledge of the strategies of some subset of the set of agents. A number of examples are presented to demonstrate the broad applicability of the framework, including reasoning about implementations of knowledge-based programs, game theoretic solution concepts and notions from computer security. It is shown that notions from several variants of alternating temporal epistemic logic can be expressed. The framework is shown to have a decidable model checking problem.", "num_citations": "18\n", "authors": ["1399"]}
{"title": "Improved bounded model checking for a fair branching-time temporal epistemic logic\n", "abstract": " Bounded model checking is a verification technique based on searching for counter-examples to the validity of the specification using an encoding to propositional sastisfiability. The paper identifies a number of inefficiencies in prior encodings for bounded model checking for a logic of knowledge and branching time. An alternate encoding is developed, and theoretical and experimental results are presented that show this leads to improved performance of bounded model checking for a range of examples.", "num_citations": "17\n", "authors": ["1399"]}
{"title": "Safety verification for deep neural networks with provable guarantees\n", "abstract": " Computing systems are becoming ever more complex, increasingly often incorporating deep learning components. Since deep learning is unstable with respect to adversarial perturbations, there is a need for rigorous software development methodologies that encompass machine learning. This paper describes progress with developing automated verification techniques for deep neural networks to ensure safety and robustness of their decisions with respect to input perturbations. This includes novel algorithms based on feature-guided search, games, global optimisation and Bayesian methods.", "num_citations": "15\n", "authors": ["1399"]}
{"title": "The complexity of epistemic model checking: Clock semantics and branching time\n", "abstract": " In the clock semantics for epistemic logic, two situations are indistinguishable for an agent when it makes the same observation and the time in the situations is the same. The paper characterizes the complexity of model checking branching time logics of knowledge in finite state systems with respect to the clock semantics.", "num_citations": "13\n", "authors": ["1399"]}
{"title": "Model checking knowledge in pursuit evasion games\n", "abstract": " In a pursuit-evasion game, one or more pursuers aim to discover the existence of, and then capture, an evader. The paper studies pursuit-evasion games in which players may have incomplete information concerning the game state. A methodology is presented for the application of a model checker for the logic of knowledge and time to verify epistemic properties in such games. Experimental results are provided from a number of case studies that validate the feasibility of the approach.", "num_citations": "12\n", "authors": ["1399"]}
{"title": "CNN-GCN aggregation enabled boundary regression for biomedical image segmentation\n", "abstract": " Accurate segmentation of anatomic structure is an essential task for biomedical image analysis. Recent popular object contours regression based segmentation methods have increasingly attained researchers\u2019 attentions. They made a new starting point to tackle segmentation tasks instead of commonly used dense pixels classification methods. However, because of the nature of CNN based network (lack of spatial information) and the difficulty of this methodology itself (need of more spatial information), these methods needed extra process to maintain more spatial features, which may cause longer inference time or tedious design and inference process. To address the issue, this paper proposes a simple, intuitive deep learning based contour regression model. We develop a novel multi-level, multi-stage aggregated network to regress the coordinates of the contour of instances directly in an end-to-end\u00a0\u2026", "num_citations": "10\n", "authors": ["1399"]}
{"title": "Reliability validation of learning enabled vehicle tracking\n", "abstract": " This paper studies the reliability of a real-world learning-enabled system, which conducts dynamic vehicle tracking based on a high-resolution wide-area motion imagery input. The system consists of multiple neural network components \u2013 to process the imagery inputs \u2013 and multiple symbolic (Kalman filter) components \u2013 to analyse the processed information for vehicle tracking. It is known that neural networks suffer from adversarial examples, which make them lack robustness. However, it is unclear if and how the adversarial examples over learning components can affect the overall system-level reliability. By integrating a coverage-guided neural network testing tool, DeepConcolic, with the vehicle tracking system, we found that (1) the overall system can be resilient to some adversarial examples thanks to the existence of other components, and (2) the overall system presents an extra level of uncertainty which\u00a0\u2026", "num_citations": "10\n", "authors": ["1399"]}
{"title": "Diagnosability in concurrent probabilistic systems\n", "abstract": " Diagnosability is a key attribute of systems to enable the detection of failure events by partial observations. This paper addresses the diagnosability in concurrent probabilistic systems. Four different notions (L-, P-, A-, and AA-diagnosability) are characterised by formulas of a logic of knowledge, time and probability. Also, we investigate the computational complexities of verifying them: the L-diagnosability is NL-complete, the A-diagnosability is PTIME-complete, and the P-diagnosability is in PSPACE.", "num_citations": "10\n", "authors": ["1399"]}
{"title": "Regression of Instance Boundary by Aggregated CNN and GCN\n", "abstract": " This paper proposes a straightforward, intuitive deep learning approach for (biomedical) image segmentation tasks. Different from the existing dense pixel classification methods, we develop a novel multi-level aggregation network to directly regress the coordinates of the boundary of instances in an end-to-end manner. The network seamlessly combines standard convolution neural network (CNN) with Attention Refinement Module (ARM) and Graph Convolution Network (GCN). By iteratively and hierarchically fusing the features across different layers of the CNN, our approach gains sufficient semantic information from the input image and pays special attention to the local boundaries with the help of ARM and GCN. In particular, thanks to the proposed aggregation GCN, our network benefits from direct feature learning of the instances\u2019 boundary locations and the spatial information propagation across the image\u00a0\u2026", "num_citations": "9\n", "authors": ["1399"]}
{"title": "Coverage-Guided Testing for Recurrent Neural Networks\n", "abstract": " Recurrent neural networks (RNNs) have been applied to a broad range of applications, including natural language processing, drug discovery, and video recognition. Their vulnerability to input perturbation is also known. Aligning with a view from software defect detection, this article aims to develop a coverage-guided testing approach to systematically exploit the internal behavior of RNNs, with the expectation that such testing can detect defects with high possibility. Technically, the long short-term memory network (LSTM), a major class of RNNs, is thoroughly studied. A family of three test metrics are designed to quantify not only the values but also the temporal relations (including both stepwise and bounded-length) exhibited when LSTM processing inputs. A genetic algorithm is applied to efficiently generate test cases. The test metrics and test case generation algorithm are implemented into a tool testRNN, which\u00a0\u2026", "num_citations": "8\n", "authors": ["1399"]}
{"title": "Normative multiagent systems: a dynamic generalization\n", "abstract": " Social norms are powerful formalism in coordinating autonomous agents' behaviour to achieve certain objectives. In this paper, we propose a dynamic normative system to enable the reasoning of the changes of norms under different circumstances, which cannot be done in the existing static normative systems. We study two important problems (norm synthesis and norm recognition) related to the autonomy of the entire system and the agents, and characterise the computational complexities of solving these problems.", "num_citations": "7\n", "authors": ["1399"]}
{"title": "Bounded model checking of strategy ability with perfect recall\n", "abstract": " The paper works with a logic which has the expressiveness to quantify over strategies of bounded length. The semantics of the logic is based on systems with multiple agents. Agents have incomplete information about the underlying system state and their strategies are based on perfect recall memory over observations and local actions. The computational complexity of model checking is shown to be PSPACE-complete. We give two BDD-based model checking algorithms. The algorithms are implemented in a model checker and experimental results are reported to show their applications.", "num_citations": "7\n", "authors": ["1399"]}
{"title": "Model checking for reasoning about incomplete information games\n", "abstract": " GDL-II is a logic-based knowledge representation formalism used in general game playing to describe the rules of arbitrary games, in particular those with incomplete information. In this paper, we use model checking to automatically verify that games specified in GDL-II satisfy desirable temporal and knowledge conditions. We present a systematic translation of GDL-II to a model checking language, prove the translation to be correct, and demonstrate the feasibility of applying model checking tools for GDL-II games by four case studies.", "num_citations": "7\n", "authors": ["1399"]}
{"title": "Gaze-based intention anticipation over driving manoeuvres in semi-autonomous vehicles\n", "abstract": " Anticipating a human collaborator's intention enables safe and efficient interaction between a human and an autonomous system. Specifically, in the context of semiautonomous driving, studies have revealed that correct and timely prediction of the driver's intention needs to be an essential part of Advanced Driver Assistance System (ADAS) design. To this end, we propose a framework that exploits drivers' time-series eye gaze and fixation patterns to anticipate their real-time intention over possible future manoeuvres, enabling a smart and collaborative ADAS that can aid drivers to overcome safety-critical situations. The method models human intention as the latent states of a hidden Markov model and uses probabilistic dynamic time warping distributions to capture the temporal characteristics of the observation patterns of the drivers. The method is evaluated on a data set of 124 experiments from 75 drivers\u00a0\u2026", "num_citations": "6\n", "authors": ["1399"]}
{"title": "Model Checking Probabilistic Epistemic Logic for Probabilistic Multiagent Systems\n", "abstract": " \u00a9 2018 International Joint Conferences on Artificial Intelligence.All right reserved. In this work we study the model checking problem for probabilistic multiagent systems with respect to the probabilistic epistemic logic PETL, which can specify both temporal and epistemic properties. We show that under the realistic assumption of uniform schedulers, i.e., the choice of every agent depends only on its observation history, PETL model checking is undecidable. By restricting the class of schedulers to be memoryless schedulers, we show that the problem becomes decidable. More importantly, we design a novel algorithm which reduces the model checking problem into a mixed integer non-linear programming problem, which can then be solved by using an SMT solver. The algorithm has been implemented in an existing model checker and experiments are conducted on examples from the IPPC competitions.", "num_citations": "6\n", "authors": ["1399"]}
{"title": "Microbial control of phytopathogenic nematodes\n", "abstract": " Phytopathogenic nematodes, mainly comprised of plant parasitic nematodes, cause serious losses in a variety of agricultural crops worldwide. However, because traditional nematicides are associated with major environmental and health concerns, developing safe and effective nematicides is urgently needed. Among the recent developments, biocontrol measures using nematophagous microorganisms                                 have shown significant promise and attracted much attention. Nematophagous microorganisms are the most important natural enemies of phytopathogenic nematodes and these microorganisms employ a variety of physical, chemical, and biochemical mechanisms to attack nematodes. This chapter introduces nematophagous microorganisms as well as their virulence factors against nematodes                                                 .", "num_citations": "6\n", "authors": ["1399"]}
{"title": "Synthesizing strategies for epistemic goals by epistemic model checking: an application to pursuit evasion games\n", "abstract": " The paper identifies a special case in which the complex problem of synthesis from specifications in temporal-epistemic logic can be reduced to the simpler problem of model checking such specifications. An application is given of strategy synthesis in pursuit-evasion games, where one or more pursuers with incomplete information aim to discover theexistence of an evader. Experimental results are provided to evaluate the feasibility of the approach.", "num_citations": "6\n", "authors": ["1399"]}
{"title": "Generalizing Universal Adversarial Attacks Beyond Additive Perturbations\n", "abstract": " The previous study has shown that universal adversarial attacks can fool deep neural networks over a large set of input images with a single human-invisible perturbation. However, current methods for universal adversarial attacks are based on additive perturbation, which cause misclassification when the perturbation is directly added to the input images. In this paper, for the first time, we show that a universal adversarial attack can also be achieved via non-additive perturbation (e.g., spatial transformation). More importantly, to unify both additive and non-additive perturbations, we propose a novel unified yet flexible framework for universal adversarial attacks, called GUAP, which is able to initiate attacks by additive perturbation, non-additive perturbation, or the combination of both. Extensive experiments are conducted on ImageNet dataset with several deep neural network models including GoogLeNet, VGG and\u00a0\u2026", "num_citations": "5\n", "authors": ["1399"]}
{"title": "Towards the Quantification of Safety Risks in Deep Neural Networks\n", "abstract": " Safety concerns on the deep neural networks (DNNs) have been raised when they are applied to critical sectors. In this paper, we define safety risks by requesting the alignment of the network's decision with human perception. To enable a general methodology for quantifying safety risks, we define a generic safety property and instantiate it to express various safety risks. For the quantification of risks, we take the maximum radius of safe norm balls, in which no safety risk exists. The computation of the maximum safe radius is reduced to the computation of their respective Lipschitz metrics - the quantities to be computed. In addition to the known adversarial example, reachability example, and invariant example, in this paper we identify a new class of risk - uncertainty example - on which humans can tell easily but the network is unsure. We develop an algorithm, inspired by derivative-free optimization techniques and accelerated by tensor-based parallelization on GPUs, to support efficient computation of the metrics. We perform evaluations on several benchmark neural networks, including ACSC-Xu, MNIST, CIFAR-10, and ImageNet networks. The experiments show that, our method can achieve competitive performance on safety quantification in terms of the tightness and the efficiency of computation. Importantly, as a generic approach, our method can work with a broad class of safety risks and without restrictions on the structure of neural networks.", "num_citations": "5\n", "authors": ["1399"]}
{"title": "How does Weight Correlation Affect Generalisation Ability of Deep Neural Networks?\n", "abstract": " This paper studies the novel concept of weight correlation in deep neural networks and discusses its impact on the networks' generalisation ability. For fully-connected layers, the weight correlation is defined as the average cosine similarity between weight vectors of neurons, and for convolutional layers, the weight correlation is defined as the cosine similarity between filter matrices. Theoretically, we show that, weight correlation can, and should, be incorporated into the PAC Bayesian framework for the generalisation of neural networks, and the resulting generalisation bound is monotonic with respect to the weight correlation. We formulate a new complexity measure, which lifts the PAC Bayes measure with weight correlation, and experimentally confirm that it is able to rank the generalisation errors of a set of networks more precisely than existing measures. More importantly, we develop a new regulariser for training, and provide extensive experiments that show that the generalisation error can be greatly reduced with our novel approach.", "num_citations": "5\n", "authors": ["1399"]}
{"title": "testRNN: Coverage-guided Testing on Recurrent Neural Networks\n", "abstract": " Recurrent neural networks (RNNs) have been widely applied to various sequential tasks such as text processing, video recognition, and molecular property prediction. We introduce the first coverage-guided testing tool, coined testRNN, for the verification and validation of a major class of RNNs, long short-term memory networks (LSTMs). The tool implements a generic mutation-based test case generation method, and it empirically evaluates the robustness of a network using three novel LSTM structural test coverage metrics. Moreover, it is able to help the model designer go through the internal data flow processing of the LSTM layer. The tool is available through: https://github.com/TrustAI/testRNN under the BSD 3-Clause licence.", "num_citations": "5\n", "authors": ["1399"]}
{"title": "The complexity of model checking succinct multiagent systems\n", "abstract": " This paper studies the complexity of model checking multiagent systems, in particular systems succinctly described by two practical representations: concurrent representation and symbolic representation. The logics we concern include branching time temporal logics and several variants of alternating time temporal logics.", "num_citations": "5\n", "authors": ["1399"]}
{"title": "Bounded planning for strategic goals with incomplete information and perfect recall.\n", "abstract": " The paper proposes an OBDD-based bounded model checking algorithm for alternating-time temporal logic in systems of incomplete information and multiple players. Players are assumed to have perfect recall memory over their observations and local actions. The algorithm is implemented in a model checker and experimental results are reported to show its applications in bounded planning for strategic goals. The computational complexity of model checking is also addressed.", "num_citations": "5\n", "authors": ["1399"]}
{"title": "Practical Verification of Neural Network Enabled State Estimation System for Robotics\n", "abstract": " We study for the first time the verification problem on learning-enabled state estimation systems for robotics, which use Bayes filter for localisation, and use deep neural network to process sensory input into observations for the Bayes filter. Specifically, we are interested in a robustness property of the systems: given a certain ability to an adversary for it to attack the neural network without being noticed, whether or not the state estimation system is able to function with only minor loss of localisation precision? For verification purposes, we reduce the state estimation systems to a novel class of labelled transition systems with payoffs and partial order relations, and formally express the robustness property as a constrained optimisation objective. Based on this, practical verification algorithms are developed. As a major case study, we work with a real-world dynamic tracking system that uses a Kalman filter (a special case\u00a0\u2026", "num_citations": "4\n", "authors": ["1399"]}
{"title": "Symbolic synthesis for epistemic specifications with observational semantics\n", "abstract": " The paper describes a framework for the synthesis of protocols for distributed and multi-agent systems from specifications that give a program structure that may include variables in place of conditional expressions, together with specifications in a temporal epistemic logic that constrain the values of these variables. The epistemic operators are interpreted with respect to an observational semantics. The framework generalizes the notion of knowledge-based program proposed by Fagin et al (Dist. Comp. 1997). An algorithmic approach to the synthesis problem is developed that computes all solutions, using a reduction to epistemic model checking, that has been implemented using symbolic techniques. An application of the approach to synthesize mutual exclusion protocols is presented.", "num_citations": "4\n", "authors": ["1399"]}
{"title": "Spatial Uncertainty-Aware Semi-Supervised Crowd Counting\n", "abstract": " Semi-supervised approaches for crowd counting attract attention, as the fully supervised paradigm is expensive and laborious due to its request for a large number of images of dense crowd scenarios and their annotations. This paper proposes a spatial uncertainty-aware semi-supervised approach via regularized surrogate task (binary segmentation) for crowd counting problems. Different from existing semi-supervised learning-based crowd counting methods, to exploit the unlabeled data, our proposed spatial uncertainty-aware teacher-student framework focuses on high confident regions' information while addressing the noisy supervision from the unlabeled data in an end-to-end manner. Specifically, we estimate the spatial uncertainty maps from the teacher model's surrogate task to guide the feature learning of the main task (density regression) and the surrogate task of the student model at the same time. Besides, we introduce a simple yet effective differential transformation layer to enforce the inherent spatial consistency regularization between the main task and the surrogate task in the student model, which helps the surrogate task to yield more reliable predictions and generates high-quality uncertainty maps. Thus, our model can also address the task-level perturbation problems that occur spatial inconsistency between the primary and surrogate tasks in the student model. Experimental results on four challenging crowd counting datasets demonstrate that our method achieves superior performance to the state-of-the-art semi-supervised methods. Code is available at: https://github. com/smallmax00/SUA_crowd_counting", "num_citations": "3\n", "authors": ["1399"]}
{"title": "Adaptable and Verifiable BDI Reasoning\n", "abstract": " Long-term autonomy requires autonomous systems to adapt as their capabilities no longer perform as expected. To achieve this, a system must first be capable of detecting such changes. In this position paper, we describe a system architecture for BDI autonomous agents capable of adapting to changes in a dynamic environment and outline the required research. Specifically, we describe an agent-maintained self-model with accompanying theories of durative actions and learning new action descriptions in BDI systems.", "num_citations": "3\n", "authors": ["1399"]}
{"title": "Model checking probabilistic knowledge: A PSPACE case\n", "abstract": " Model checking probabilistic knowledge of memoryful semantics is undecidable, even for a simple formula concerning the reachability of probabilistic knowledge of a single agent. This result suggests that the usual approach of tackling undecidable model checking problems, by finding syntactic restrictions over the logic language, may not suffice. In this paper, we propose to work with an additional restriction that agent's knowledge concerns a special class of atomic propositions. A PSPACE-complete case is identified with this additional restriction, for a logic language combining LTL with limit-sure knowledge of a single agent.", "num_citations": "3\n", "authors": ["1399"]}
{"title": "Model checking games for a fair branching-time temporal epistemic logic\n", "abstract": " Model checking games are instances of Hintikka\u2019s game semantics for logic used for purposes of debugging systems verification models. Previous work in the area has developed these games for branching time logic. The paper develops an extension to a logic that adds epistemic operators, and interprets the branching time operators with respect to fairness constraints. The implementation of the extended games in the epistemic model checker MCK is described.", "num_citations": "3\n", "authors": ["1399"]}
{"title": "Statistical Certification of Acceptable Robustness for Neural Networks\n", "abstract": " Neural network robustness measurement is a critical step before deploying neural network applications. However, existing methods, such as neural network verification and validation, do not fully meet our criteria for robustness measurement. From the industrial point-of-view, this paper proposes to use statistical robustness certificates (SRC) for measuring the robustness of neural networks against random noises as well as semantic perturbations and tries to bridge between verification and validation methods through Hoeffding Inequality. Our experiments show that our method is accurate in comparing robustness of different neural networks and has polynomial time complexity which leads to 3x-30x boost in efficiency compared to related methods. Together with the intrinsic statistical guarantee, the issued certificates are considered practical in comparing the robustness of various commercial neural networks.", "num_citations": "2\n", "authors": ["1399"]}
{"title": "Adversarial Robustness of Deep Learning: Theory, Algorithms, and Applications\n", "abstract": " This tutorial aims to introduce the fundamentals of adversarial robustness of deep learning, presenting a well-structured review of up-to-date techniques to assess the vulnerability of various types of deep learning models to adversarial examples. This tutorial will particularly highlight state-of-the-art techniques in adversarial attacks and robustness verification of deep neural networks (DNNs). We will also introduce some effective countermeasures to improve robustness of deep learning models, with a particular focus on adversarial training. We aim to provide a comprehensive overall picture about this emerging direction and enable the community to be aware of the urgency and importance of designing robust deep learning models in safety-critical data analytical applications, ultimately enabling the end-users to trust deep learning classifiers. We will also summarize potential research directions concerning the\u00a0\u2026", "num_citations": "2\n", "authors": ["1399"]}
{"title": "An accident prediction architecture based on spatio\u2010clock stochastic and hybrid model for autonomous driving safety\n", "abstract": " Collaborative and autonomous driving vehicles combine hardware and software complex processes, also are heavily dependent on and influenced by the world of physical and cyber interactions. They have enabled many new features and advanced functionalities, such as stochastic and hybrid natures, mobile spatial topologies, and time\u2010critical dependability. However, the existing modeling and verification techniques have not established faith in proving correctness and safety. Spatial and time collision avoidance remains crucial obstacles on the path to becoming ubiquitous and dependable. In order to ensure safety, we first design an accident prediction architecture in system design\u2010time and run\u2010time stages. We apply it on collaborative and autonomous overtaking systems involving spatial\u2010 and time\u2010critical accident predictions. Then, we develop a novel and dedicated spatio\u2010clock stochastic specification\u00a0\u2026", "num_citations": "2\n", "authors": ["1399"]}
{"title": "Tutorials on Testing Neural Networks\n", "abstract": " Deep learning achieves remarkable performance on pattern recognition, but can be vulnerable to defects of some important properties such as robustness and security. This tutorial is based on a stream of research conducted since the summer of 2018 at a few UK universities, including the University of Liverpool, University of Oxford, Queen's University Belfast, University of Lancaster, University of Loughborough, and University of Exeter. The research aims to adapt software engineering methods, in particular software testing methods, to work with machine learning models. Software testing techniques have been successful in identifying software bugs, and helping software developers in validating the software they design and implement. It is for this reason that a few software testing techniques -- such as the MC/DC coverage metric -- have been mandated in industrial standards for safety critical systems, including the ISO26262 for automotive systems and the RTCA DO-178B/C for avionics systems. However, these techniques cannot be directly applied to machine learning models, because the latter are drastically different from traditional software, and their design follows a completely different development life-cycle. As the outcome of this thread of research, the team has developed a series of methods that adapt the software testing techniques to work with a few classes of machine learning models. The latter notably include convolutional neural networks, recurrent neural networks, and random forest. The tools developed from this research are now collected, and publicly released, in a GitHub repository: \\url{https://github.com/TrustAI\u00a0\u2026", "num_citations": "2\n", "authors": ["1399"]}
{"title": "Safety and reliability of deep learning: (brief overview)\n", "abstract": " Robotics and Autonomous Systems (RAS) become ever more relying on deep learning components to support their perception and decision making. Given RAS will inevitably be applied to safety critical applications, efforts are needed to ensure that the deep learning is safe and reliable. In this lecture, I will give a brief overview on recent progress in the verification and validation techniques for deep learning, focusing on two major safety and reliability risks, ie, robustness and generalisation. We consider formal verification, statistical evaluation, reliability assessment, and runtime monitoring techniques, all of which complement with each other in providing assurance to the reliability of deep learning in operation. The challenges and future directions will also be discussed.", "num_citations": "2\n", "authors": ["1399"]}
{"title": "Abstraction and Symbolic Execution of Deep Neural Networks with Bayesian Approximation of Hidden Features\n", "abstract": " Intensive research has been conducted on the verification and validation of deep neural networks (DNNs), aiming to understand if, and how, DNNs can be applied to safety critical applications. However, existing verification and validation techniques are limited by their scalability, over both the size of the DNN and the size of the dataset. In this paper, we propose a novel abstraction method which abstracts a DNN and a dataset into a Bayesian network (BN). We make use of dimensionality reduction techniques to identify hidden features that have been learned by hidden layers of the DNN, and associate each hidden feature with a node of the BN. On this BN, we can conduct probabilistic inference to understand the behaviours of the DNN processing data. More importantly, we can derive a runtime monitoring approach to detect in operational time rare inputs and covariate shift of the input data. We can also adapt existing structural coverage-guided testing techniques (i.e., based on low-level elements of the DNN such as neurons), in order to generate test cases that better exercise hidden features. We implement and evaluate the BN abstraction technique using our DeepConcolic tool available at https://github.com/TrustAI/DeepConcolic.", "num_citations": "2\n", "authors": ["1399"]}
{"title": "CasGCN: Predicting future cascade growth based on information diffusion graph\n", "abstract": " Sudden bursts of information cascades can lead to unexpected consequences such as extreme opinions, changes in fashion trends, and uncontrollable spread of rumors. It has become an important problem on how to effectively predict a cascade' size in the future, especially for large-scale cascades on social media platforms such as Twitter and Weibo. However, existing methods are insufficient in dealing with this challenging prediction problem. Conventional methods heavily rely on either hand crafted features or unrealistic assumptions. End-to-end deep learning models, such as recurrent neural networks, are not suitable to work with graphical inputs directly and cannot handle structural information that is embedded in the cascade graphs. In this paper, we propose a novel deep learning architecture for cascade growth prediction, called CasGCN, which employs the graph convolutional network to extract structural features from a graphical input, followed by the application of the attention mechanism on both the extracted features and the temporal information before conducting cascade size prediction. We conduct experiments on two real-world cascade growth prediction scenarios (i.e., retweet popularity on Sina Weibo and academic paper citations on DBLP), with the experimental results showing that CasGCN enjoys a superior performance over several baseline methods, particularly when the cascades are of large scale.", "num_citations": "2\n", "authors": ["1399"]}
{"title": "Feature-guided black-box safety testing of deep neural networks\n", "abstract": " In this paper, for the efficient generation of a test case x, they consider (1) an LP-based approach by fixing the activation pattern ap [x] according to a given input x, and (2) encoding a prefix of the network, instead of the entire network, with respect to a given neuron pair.", "num_citations": "2\n", "authors": ["1399"]}
{"title": "ATL Strategic Reasoning Meets Correlated Equilibrium\n", "abstract": " This paper is motivated by analysing a Google selfdriving car accident, ie, the car hit a bus, with the framework and the tools of strategic reasoning by model checking. First of all, we find that existing ATL model checking may find a solution to the accident with irrational joint strategy of the bus and the car. This leads to a restriction of treating both the bus and the car as rational agents, by which their joint strategy is an equilibrium of certain solution concepts. Second, we find that a randomly-selected joint strategy from the set of equilibria may result in the collision of the two agents, ie, the accident. Based on these, we suggest taking Correlated Equilibrium (CE) as agents\u2019 joint strategy and optimising over the utilitarian value which is the expected sum of the agents\u2019 total rewards. The language ATL is extended with two new modalities to express the existence of a CE and a unique CE, respectively. We implement the extension into a software model checker and use the tool to analyse the examples in the paper. We also study the complexity of the model checking problem.", "num_citations": "2\n", "authors": ["1399"]}
{"title": "Symbolic synthesis of knowledge-based program implementations with synchronous semantics\n", "abstract": " This paper deals with the automated synthesis of implementations of knowledge-based programs with respect to two synchronous semantics (clock and synchronous perfect recall). An approach to the synthesis problem based on the use of symbolic representations is described. The method has been implemented as an extension to the model checker MCK. Two applications of the implemented synthesis system are presented: the muddy children puzzle (where performance is compared to an explicit state method for a related problem implemented in the model checker DEMO), and a knowledge-based program for a dynamic leader election problem in a ring of processes.", "num_citations": "2\n", "authors": ["1399"]}
{"title": "A Little Energy Goes a Long Way: Energy-Efficient, Accurate Conversion from Convolutional Neural Networks to Spiking Neural Networks\n", "abstract": " Spiking neural networks (SNNs) offer an inherent ability to process spatial-temporal data, or in other words, realworld sensory data, but suffer from the difficulty of training high accuracy models. A major thread of research on SNNs is on converting a pre-trained convolutional neural network (CNN) to an SNN of the same structure. State-of-the-art conversion methods are approaching the accuracy limit, i.e., the near-zero accuracy loss of SNN against the original CNN. However, we note that this is made possible only when significantly more energy is consumed to process an input. In this paper, we argue that this trend of \"energy for accuracy\" is not necessary -- a little energy can go a long way to achieve the near-zero accuracy loss. Specifically, we propose a novel CNN-to-SNN conversion method that is able to use a reasonably short spike train (e.g., 256 timesteps for CIFAR10 images) to achieve the near-zero accuracy loss. The new conversion method, named as explicit current control (ECC), contains three techniques (current normalisation, thresholding for residual elimination, and consistency maintenance for batch-normalisation), in order to explicitly control the currents flowing through the SNN when processing inputs. We implement ECC into a tool nicknamed SpKeras, which can conveniently import Keras CNN models and convert them into SNNs. We conduct an extensive set of experiments with the tool -- working with VGG16 and various datasets such as CIFAR10 and CIFAR100 -- and compare with state-of-the-art conversion methods. Results show that ECC is a promising method that can optimise over energy consumption and accuracy\u00a0\u2026", "num_citations": "1\n", "authors": ["1399"]}
{"title": "Graph-based Region and Boundary Aggregation for Biomedical Image Segmentation\n", "abstract": " Segmentation is a fundamental task in biomedical image analysis. Unlike the existing region-based dense pixel classification methods or boundary-based polygon regression methods, we build a novel graph neural network (GNN) based deep learning framework with multiple graph reasoning modules to explicitly leverage both region and boundary features in an end-to-end manner. The mechanism extracts discriminative region and boundary features, referred to as initialized region and boundary node embeddings, using a proposed Attention Enhancement Module (AEM). The weighted links between cross-domain nodes (region and boundary feature domains) in each graph are defined in a data-dependent way, which retains both global and local cross-node relationships. The iterative message aggregation and node update mechanism can enhance the interaction between each graph reasoning module\u2019s\u00a0\u2026", "num_citations": "1\n", "authors": ["1399"]}
{"title": "Safety and robustness for deep learning with provable guarantees\n", "abstract": " Computing systems are becoming ever more complex, with decisions increasingly often based on deep learning components. A wide variety of applications are being developed, many of them safety-critical, such as self-driving cars and medical diagnosis. Since deep learning is unstable with respect to adversarial perturbations, there is a need for rigorous software development methodologies that encompass machine learning components. This lecture will describe progress with developing automated verification and testing techniques for deep neural networks to ensure safety and robustness of their decisions with respect to bounded input perturbations. The techniques exploit Lipschitz continuity of the networks and aim to approximate, for a given set of inputs, the reachable set of network outputs in terms of lower and upper bounds, in anytime manner, with provable guarantees. We develop novel algorithms\u00a0\u2026", "num_citations": "1\n", "authors": ["1399"]}
{"title": "Multiclock Constraint System Modelling and Verification for Ensuring Cooperative Autonomous Driving Safety\n", "abstract": " CADS (cooperative autonomous driving systems) are software-intensive and safety-critical reactive systems and give great promise to our daily life, but system errors may not be identified in the design stage until the implement stage, and the cost to correct them will be more expensive later than the early stage. For designing trustworthy autonomous software systems, we have to deal with multiclock constraint models. SysML (System Modeling Language) meets increasing adoption in order to carry out system-level modelling and verification against abstract representations, but it suffers from semantic ambiguities in the design of safety-critical autonomous systems. The main objective is to investigate methods for coping with the design and analysis models simultaneously and to achieve semantic consistency based on mathematical foundations and formal model transformation. In this paper, we propose a method to combine the requirement modelling process with analysis process together for CADS safety and reliability guarantee. Firstly, we extend SysML metamodels and construct SysML profile for the CADS domain that could improve modelling correctness and enhance reusability. An instantiated CADS model has been designed by means of adopting a profile containing different key functional and nonfunctional attributes and behaviors. Secondly, we define formal syntax and semantic notations for modelling elements in the SysML state machine diagram and show transformation rules between the state machine diagram and the CCSL (Clock Constraint Specification Language) model. Semantic preservation is also proved using the\u00a0\u2026", "num_citations": "1\n", "authors": ["1399"]}
{"title": "Formal Verification of Robustness and Resilience of Learning-Enabled State Estimation Systems for Robotics\n", "abstract": " This paper presents a formal verification guided approach for a principled design and implementation of robust and resilient learning-enabled systems. We focus on learning-enabled state estimation systems (LE-SESs), which have been widely used in robotics applications to determine the current state (e.g., location, speed, direction, etc.) of a complex system. The LE-SESs are networked systems composed of a set of connected components including Bayes filters for localisation, and neural networks for processing sensory input. We study LE-SESs from the perspective of formal verification, which determines the satisfiability of a system model against the specified properties. Over LE-SESs, we investigate two key properties - robustness and resilience - and provide their formal definitions. To enable formal verification, we reduce the LE-SESs to a novel class of labelled transition systems, named {PO}2-LTS in the paper, and formally express the properties as constrained optimisation objectives. We prove that the robustness verification is NP-complete. Based on {PO}2-LTS and the optimisation objectives, practical verification algorithms are developed to check the satisfiability of the properties on the LE-SESs. As a major case study, we interrogate a real-world dynamic tracking system which uses a single Kalman Filter (KF) - a special case of Bayes filter - to localise and track a ground vehicle. Its perception system, based on convolutional neural networks, processes a high-resolution Wide Area Motion Imagery (WAMI) data stream. Experimental results show that our algorithms can not only verify the properties of the WAMI tracking system but also\u00a0\u2026", "num_citations": "1\n", "authors": ["1399"]}
{"title": "Reconfigurability in Reactive Multiagent Systems.\n", "abstract": " Reactive agents are suitable for representing physical resources in manufacturing control systems. An important challenge of agent-based manufacturing control systems is to develop formal and structured approaches to support their specification and verification. This paper proposes a logic-based approach, by generalising that of model checking multiagent systems, for the reconfigurability of reactive multiagent systems. Two reconfigurability scenarios are studied, for the resulting system being a monolithic system or an individual module, and their computational complexity results are given.", "num_citations": "1\n", "authors": ["1399"]}
{"title": "Congruence Formats for Weak Readiness Equivalence and Weak Possible Future Equivalence\n", "abstract": " Weak equivalences are important behavioral equivalences in the course of specifying and analyzing reactive systems using process algebraic languages. In this paper, we propose a series of weak equivalences named weak parametric readiness equivalences, which take two previously known behavioral equivalences, i.e. the weak readiness equivalence and the weak possible future equivalence, as their special cases. More importantly, based on the idea of structural operational semantics, a series of rule formats are presented to guarantee congruence for these weak parametric readiness equivalences, i.e. to show that the proposed rule formats can guarantee the congruence of their corresponding weak parametric readiness equivalences. This series of rule formats reflects the differences in the weak parametric readiness equivalences. We conclude that when the weak parametric readiness equivalences\u00a0\u2026", "num_citations": "1\n", "authors": ["1399"]}
{"title": "Weak parametric failure equivalences and their congruence formats\n", "abstract": " Weak equivalences are important behavioral equivalences in the course of specifying and analyzing the reactive systems using process algebraic languages. In this paper, we propose a series of weak equivalences named weak parametric failure equivalences, which take two previously-known behavioral equivalences, ie, the weak failure equivalence and the weak impossible future equivalence, as their special cases. More importantly, based on the idea of the structural operational semantics, a series of rule formats are further presented to congruence format for their corresponding weak parametric failure equivalences, ie, a specific equivalence is further congruent in any languages satisfying its corresponding congruence format. This series of rule formats reflect the gradual changes in the weak parametric failure equivalences. We conclude that, when the weak parametric failure equivalences become coarser, their corresponding rule formats turn tighter.", "num_citations": "1\n", "authors": ["1399"]}