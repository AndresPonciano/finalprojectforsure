{"title": "\"Cloning considered harmful\" considered harmful: Patterns of cloning in software\n", "abstract": " Literature on the topic of code cloning often asserts that duplicating code within a software system is a bad practice, that it causes harm to the system\u2019s design and should be avoided. However, in our studies, we have found significant evidence that cloning is often used in a variety of ways as a principled engineering tool. For example, one way to evaluate possible new features for a system is to clone the affected subsystems and introduce the new features there, in a kind of sandbox testbed. As features mature and become stable within the experimental subsystems, they can be migrated incrementally into the stable code base; in this way, the risk of introducing instabilities in the stable version is minimized. This paper describes several patterns of cloning that we have observed in our case studies and discusses the advantages and disadvantages associated with using them. We also examine through a\u00a0\u2026", "num_citations": "703\n", "authors": ["111"]}
{"title": "Using origin analysis to detect merging and splitting of source code entities\n", "abstract": " Merging and splitting source code entities is a common activity during the lifespan of a software system; as developers rethink the essential structure of a system or plan for a new evolutionary direction, so must they be able to reorganize the design artifacts at various abstraction levels as seems appropriate. However, while the raw effects of such changes may be plainly evident in the new artifacts, the original context of the design changes is often lost. That is, it may be obvious which characters of which files have changed, but it may not be obvious where or why moving, renaming, merging, and/or splitting of design elements has occurred. In this paper, we discuss how we have extended origin analysis (Q. Tu et al., 2002), (M.W. Godfrey et al., 2002) to aid in the detection of merging and splitting of files and functions in procedural code; in particular, we show how reasoning about how call relationships have changed\u00a0\u2026", "num_citations": "345\n", "authors": ["111"]}
{"title": "A reference architecture for web browsers\n", "abstract": " A reference architecture for a domain captures the fundamental subsystems common to systems of that domain as well as the relationships between these subsystems. Having a reference architecture available can aid both during maintenance and at design time: it can improve understanding of a given system, it can aid in analyzing tradeoffs between different design options, and it can serve as a template for designing new systems and re-engineering existing ones. In this paper, we examine the history of the Web browser domain and identify several underlying phenomena that have contributed to its evolution. We develop a reference architecture for Web browsers based on two well known open source implementations, and we validate it against two additional implementations. Finally, we discuss our observations about this domain and its evolutionary history; in particular, we note that the significant reuse of\u00a0\u2026", "num_citations": "172\n", "authors": ["111"]}
{"title": "An integrated approach for studying architectural evolution\n", "abstract": " Studying how a software system has evolved over time is difficult, time consuming, and costly; existing techniques are often limited in their applicability, are hard to extend, and provide little support for coping with architectural change. The paper introduces an approach to studying software evolution that integrates the use of metrics, software visualization, and origin analysis, which is a set of techniques for reasoning about structural and architectural change. Our approach incorporates data from various statistical and metrics tools, and provides a query engine as well as a Web-based visualization and navigation interface. It aims to provide an extensible, integrated environment for aiding software maintainers in understanding the evolution of long-lived systems that have undergone significant architectural change. We use the evolution of GCC as an example to demonstrate the uses of various functionalities of\u00a0\u2026", "num_citations": "160\n", "authors": ["111"]}
{"title": "Growth, evolution, and structural change in open source software\n", "abstract": " Our recent work has addressed how and why software systems evolve over time, with a particular emphasis on software architecture and open source software systems [2, 3, 6]. In this position paper, we present a short summary of two recent projects. First, we have performed a case study on the evolution of the Linux kernel [3], as well as some other open source software (OSS) systems. We have found that several OSS systems appear not to obey some of\" Lehman's laws\" of software evolution [5, 7], and that Linux in particular is continuing to grow at a geometric rate. Currently, we are working on a detailed study of the evolution of one of the subsystems of the Linux kernel: the SCSI drivers subsystem. We have found that cloning, which is usually considered to be an indicator of lazy development and poor process, is quite common and is even considered to be a useful practice. Second, we are developing a tool\u00a0\u2026", "num_citations": "140\n", "authors": ["111"]}
{"title": "Supporting the analysis of clones in software systems\n", "abstract": " Code duplication is a well\u2010documented problem in industrial software systems. There has been considerable research into techniques for detecting duplication in software, and there are several effective tools to perform this task. However, there have been few detailed qualitative studies into how cloning actually manifests itself within software systems. This is primarily due to the large result sets that many clone\u2010detection tools return; these result sets are very difficult to manage without complementary tool support that can scale to the size of the problem, and this kind of support does not currently exist. In this paper we present an in\u2010depth case study of cloning in a large software system that is in wide use, the Apache Web server; we provide insights into cloning as it exists in this system, and we demonstrate techniques to manage and make effective use of the large result sets of clone\u2010detection tools. In our case\u00a0\u2026", "num_citations": "134\n", "authors": ["111"]}
{"title": "Secrets from the monster: Extracting Mozilla\u2019s software architecture\n", "abstract": " As large systems evolve, their architectural integrity tends to decay. Reverse engineering tools, such as PBS [7, 19], Rigi [15], and Acacia [5], can be used to acquire an understanding of a system\u2019s \u201cas-built\u201d architecture and in so doing regain control over the system. A problem that has impeded the widespread adoption of reverse engineering tools is the tight coupling of their subtools, including source code \u201cfact\u201d extractors, visualization engines, and querying mechanisms; this coupling has made it difficult, for example, for users to employ alternative extractors that might have different strengths or understand different source languages.The TAXFORM project has sought to investigate how different reverse engineering tools can be integrated into a single framework by providing mappings to and from common data schemas for program \u201cfacts\u201d[2]. In this paper, we describe how we successfully integrated the Acacia C and C++ fact extractors into the PBS system, and how we were then able to create software architecture models for two large software systems: the Mozilla web browser (over two million lines of C++ and C) and the VIM text editor (over 160,000 lines of C).", "num_citations": "134\n", "authors": ["111"]}
{"title": "Cloning by accident: An empirical study of source code cloning across software systems\n", "abstract": " One of the key goals of open source development is the sharing of knowledge, experience, and solutions that pertain to a software system and its problem domain. Source code cloning is one way in which expertise can be reused across systems; cloning is known to have been used in several open source projects, such as the SCSI drivers of the Linux kernel. In this paper, we discuss two case studies in which we performed clone detection on several open source systems within the same domain. In the first case study we examined nine text editors written in C, and in the second study we examined eight X-Windows window managers written in C and C++. To our surprise, we found little evidence of \"true\" cloning activity, but we did notice a significant number of \"accidental\" clones - that is, code fragments that are similar due to the precise protocols they must use when interacting with a given API or set of libraries\u00a0\u2026", "num_citations": "112\n", "authors": ["111"]}
{"title": "Toward a taxonomy of clones in source code: A case study\n", "abstract": " Code cloning\u2014that is, the gratuitous duplication of source code within a software system\u2014is an endemic problem in large, industrial systems [9, 7]. While there has been much research into techniques for clone detection and analysis, there has been relatively little empirical study on characterizing how, where, and why clones occur in industrial software systems. In this paper, we present a preliminary categorization scheme for code clones, and we discuss how we have applied this taxonomy in a case study performed on the file system subsystem of the Linux operating system. Our case study yielded many surprising results, including that cloning is rampant both within particular file system implementations and across different ones, and that as many as 13% of the 4407 functions that are more than six lines long were involved in a clone-pair relationship.", "num_citations": "111\n", "authors": ["111"]}
{"title": "Aiding comprehension of cloning through categorization\n", "abstract": " Management of duplicated code in software systems is important in ensuring its graceful evolution. Commonly clone detection tools return large numbers of detected clones with little or no information about them, making clone management impractical and unscalable. We have used taxonomy of clones to augment current clone detection tools in order to increase the user comprehension of duplication of code within software systems and filter false positives from the clone set. We support our arguments by means of 2 case studies, where we found that as much as 53% of clones can be grouped to form function clones or partial function clones and we were able to filter out as many as 65% of clones as false positives from the reported clone pairs.", "num_citations": "102\n", "authors": ["111"]}
{"title": "The build-time software architecture view\n", "abstract": " Research and practice in the application of software architecture has reaffirmed the need to consider software systems from several distinct points of view. Previous work by P. Kruchten (1995) and C. Hofmeister et al. (2000) suggests that four or five points of view may be sufficient: the logical view (i.e., the domain object model), the (static) code view, the process/concurrency view, the deployment/execution view, plus scenarios and use-cases. We have found that some classes of software systems exhibit interesting and complex build-time properties that are not explicitly addressed by previous models. In this paper, we present the idea of build-time architectural views. We explain what they are, how to represent them, and how they fit into traditional models of software architecture. We present three case studies of software systems with interesting build-time architectural views, and show how modelling their build-time\u00a0\u2026", "num_citations": "93\n", "authors": ["111"]}
{"title": "Architectural repair of open source software\n", "abstract": " As a software system evolves, its architecture will drift. System changes are often done without considering their effects on the system structure. These changes often introduce structural anomalies between the concrete (as-built) and the conceptual (as-designed) architecture which can impede program understanding. The problem of architectural drift is especially pronounced in open source systems, where many developers work in isolation on distinct features with little co-ordination. The authors present their experiences with repairing the architectures of two large open source systems (the Linux operating system kernel and the VIM text editor) to aid program understanding. For both systems, we were successful in removing many structural anomalies from their architectures.", "num_citations": "90\n", "authors": ["111"]}
{"title": "Improved tool support for the investigation of duplication in software\n", "abstract": " Code duplication is a well documented problem in software systems. There has been considerable research into techniques for detecting duplication in software, and there are several effective tools to perform this task. However, a common problem with such tools is that the result set returned can be too large to handle without complementary tool support. The goal of this paper is to describe the criteria for a complete tool that is designed to aid in the comprehension of cloning within a software system. Furthermore, we present a prototype of such a tool and demonstrate the value of its features through a case study on the Apache httpd Web server. For example, in our study we found that a single subsystem comprising only 17% of the system code contained 38.8% of the clones.", "num_citations": "83\n", "authors": ["111"]}
{"title": "Detecting API usage obstacles: A study of iOS and Android developer questions\n", "abstract": " Software frameworks provide sets of generic functionalities that can be later customized for a specific task. When developers invoke API methods in a framework, they often encounter obstacles in finding the correct usage of the API, let alone to employ best practices. Previous research addresses this line of questions by mining API usage patterns to induce API usage templates, by conducting and compiling interviews of developers, and by inferring correlations among APIs. In this paper, we analyze API-related posts regarding iOS and Android development from a Q&A Web site, stackoverflow.com. Assuming that API-related posts are primarily about API usage obstacles, we find several iOS and Android API classes that appear to be particularly likely to challenge developers, even after we factor out API usage hotspots, inferred by modelling API usage of open source iOS and Android applications. For each API with\u00a0\u2026", "num_citations": "81\n", "authors": ["111"]}
{"title": "Architecture and evolution of the modern web browser\n", "abstract": " A reference architecture for a domain captures the fundamental subsystems common to systems of that domain, as well as the relationships between these subsystems. A reference architecture can be useful both at design time and during maintenance: it can improve understanding of a given system, aid in analyzing trade-offs between different design options, or serve as a template for designing new systems and reengineering existing ones.We examine the history of the web browser domain and identify several underlying forces that have contributed to its evolution. We develop a reference architecture for web browsers based on two well-known open source implementations, and we validate it against five additional implementations. We discuss the maintenance implications of different strategies for code reuse and identify several underlying evolutionary phenomena in the web browser domain; namely, emergent domain boundaries, convergent evolution, and tension between open and closed source development approaches.", "num_citations": "77\n", "authors": ["111"]}
{"title": "Software Bertillonage: Determining the provenance of software development artifacts\n", "abstract": " Deployed software systems are typically composed of many pieces, not all of which may have been created by the main development team. Often, the provenance of included components\u2014such as external libraries or cloned source code\u2014is not clearly stated, and this uncertainty can introduce technical and ethical concerns that make it difficult for system owners and other stakeholders to manage their software assets. In this work, we motivate the need for the recovery of the provenance of software entities by a broad set of techniques that could include signature matching, source code fact extraction, software clone detection, call flow graph matching, string matching, historical analyses, and other techniques. We liken our provenance goals to that of Bertillonage, a simple and approximate forensic analysis technique based on bio-metrics that was developed in 19th century France before the advent of\u00a0\u2026", "num_citations": "69\n", "authors": ["111"]}
{"title": "From whence it came: Detecting source code clones by analyzing assembler\n", "abstract": " To date, most clone detection techniques have concentrated on various forms of source code analysis, often by analyzing token streams. In this paper, we introduce a complementary technique of analyzing generated assembler for clones. This approach is appealing as it is mostly impervious to trivial changes in the source, with compilation serving as a kind of normalization technique. We have built detectors to analyze both Java VM code as well as GCC Linux assembler for C and C++. In the paper, we describe our approach and show how it can serve as a valuable complementary semantic approach to syntactic source code based detection.", "num_citations": "55\n", "authors": ["111"]}
{"title": "Detecting merging and splitting using origin analysis\n", "abstract": " Merging and splitting source code artifacts is a common activity during the lifespan of a software system; as developers rethink the essential structure of a system or plan for a new evolutionary direction, so must they be able to reorganize the design artifacts at various abstraction levels as seems appropriate. However, while the raw effects of such changes may be plainly evident in the new artifacts, the original intent of the design changes is often lost. In this paper, we discuss how we have extended origin analysis [10, 5] to aid in the detection of merging and splitting of files and functions in procedural code; in particular, we show how reasoning about how call relationships have changed can aid a developer in locating where merges and splits have occurred, thereby helping to recover information about the intent of the design change. We also describe a case study of these techniques (as implemented in the Beagle tool) using the PostgreSQL database as the candidate system.", "num_citations": "50\n", "authors": ["111"]}
{"title": "Concept identification in object-oriented domain analysis: Why some students just don't get it\n", "abstract": " Anyone who has taught object-oriented domain analysis or any other software process requiring concept identification has undoubtedly observed that some students just don't get it. Our evaluation of the work of over 740 University of Waterloo students on over 135 software requirements specifications during the last four years supports this same observation. The students' task was to specify a telephone exchange or a voice-over-IP telephone system and the related accounts management subsystem, based on models they developed using object-oriented analysis. A detailed comparative study of three much smaller specifications, all of an elevator system, suggests that object orientation is poorly suited to domain analysis, even of small-sized domains, and that the difficulties we have observed are independent both of the size of the system under specification and of the overall abilities of the students.", "num_citations": "45\n", "authors": ["111"]}
{"title": "Connecting architecture reconstruction frameworks\n", "abstract": " A number of standalone tools are designed to help developers understand software systems. These tools operate at different levels of abstraction, from low level source code to software architectures. Although recent proposals have suggested how code-level frameworks can share information, little attention has been given to the problem of connecting software architecture level frameworks. In this paper, we describe the TA Exchange Format (TAXForm) exchange format for frameworks at the software architecture level. By defining mappings between TAXForm and formats that are used within existing frameworks, we show how TAXForm can be used as a \u201cbinding glue\u201d to achieve interoperability between these frameworks without having to modify their internal structure.", "num_citations": "45\n", "authors": ["111"]}
{"title": "Secure and portable database extensibility\n", "abstract": " The functionality of extensible database servers can be augmented by user-defined functions (UDFs). However, the server's security and stability are concerns whenever new code is incorporated. Recently, there has been interest in the use of Java for database extensibility. This raises several questions: Does Java solve the security problems? How does it affect efficiency?", "num_citations": "44\n", "authors": ["111"]}
{"title": "Tracking structural evolution using origin analysis\n", "abstract": " Many long term studies of software evolution have made the simplifying assumption that the system's architecture and low-level structure is relatively stable. In our past work, we have found that this is often untrue; therefore, we have sought to investigate ways to detect and model structural change in software systems through a technique we call origin analysis [6] and supported a tool called Beagle [7]. In this position paper, we present a summary of our recent and ongoing work in this area, and we argue that more attention needs to be paid to techniques for understanding architectural and structural evolution of software systems.", "num_citations": "42\n", "authors": ["111"]}
{"title": "A market-based bug allocation mechanism using predictive bug lifetimes\n", "abstract": " Bug assignment in large software projects is typically a time-consuming and tedious task, effective assignment requires that bug triagers hold significant contextual information about both the reported bugs and the pool of available developers. In this paper, we propose an auction-based multiagent mechanism for assigning bugs to developers that is intended to minimize backlogs and overall bug lifetime. In this approach, developers and triagers are both modeled as intelligent software agents working on behalf of individuals in a multiagent environment. Upon receiving a bug report, triager agents auction off the bug and collect the requests. Developer agents compute their bids as a function of the developer's profile, preferences, current schedule of assigned bugs, and estimated time-to-fix of the bug. This value is then sent to the triager agent for the final decision. We use the Eclipse and Firefox bug repositories to\u00a0\u2026", "num_citations": "40\n", "authors": ["111"]}
{"title": "Recommending clones for refactoring using design, context, and history\n", "abstract": " Developers know that copy-pasting code (aka code cloning) is often a convenient shortcut to achieving a design goal, albeit one that carries risks to the code quality over time. However, deciding which, if any, clones should be eliminated within an existing system is a daunting task. Fixing a clone usually means performing an invasive refactoring, and not all clones may be worth the effort, cost, and risk that such a change entails. Furthermore, sometimes cloning fulfils a useful design role, and should not be refactored at al. And clone detection tools often return very large result sets, making it hard to choose which clones should be investigated and possibly removed. In this paper, we propose an automated approach to recommend clones for refactoring by training a decision tree-based classifier. We analyze more than 600 clone instances in three medium-to large-sized open source projects, and we collect features\u00a0\u2026", "num_citations": "39\n", "authors": ["111"]}
{"title": "Semantic grep: Regular expressions + relational abstraction\n", "abstract": " Searching source code is one of the most common activities of software engineers. Text editors and other support tools normally provide searching based on lexical expressions (regular expressions). Some more advanced editors provide a way to add semantic direction to some of the searches. Recent research has focused on advancing the semantic options available to text-based queries. Most of these results make use of heavy weight relational database management technology. In this paper we explore the extension of lexical pattern matching by means of light weight relational queries, implemented using a tool called grok. A \"semantic grep\" (sgrep) command was implemented, which translates queries in a mixed algebraic and lexical language into a combination of grok queries and grep commands. This paper presents the design decisions behind sgrep, and example queries that can be posed. The paper\u00a0\u2026", "num_citations": "37\n", "authors": ["111"]}
{"title": "Recommending Posts Concerning API Issues in Developer Q&A Sites\n", "abstract": " API design is known to be a challenging craft, as API designers must balance their elegant ideals against \"real-world\" concerns, such as utility, performance, backwards compatibility, and unforeseen emergent uses. However, to date, there is no principled method to collect or analyze API usability information that incorporates input from typical developers. In practice, developers often turn to Q&A websites such as stackoverflow.com (SO) when seeking expert advice on API use, the popularity of such sites has thus led to a very large volume of unstructured information that can be searched with diligence for answers to specific questions. The collected wisdom within such sites could, in principle, be of great help to API designers to better support developer needs, if only it could be collected, analyzed, and distilled for practical use. In this paper, we present a methodology that combines several techniques, including\u00a0\u2026", "num_citations": "33\n", "authors": ["111"]}
{"title": "Clone detection by exploiting assembler\n", "abstract": " In this position paper, we describe work-in-progress in detecting source code clones by means of analyzing and comparing the assembler that is produced when the source code is compiled.", "num_citations": "30\n", "authors": ["111"]}
{"title": "An industrial case study of program artifacts viewed during maintenance tasks\n", "abstract": " Research on maintenance task structure modeling has so far examined only how often program artifacts are modified, and what information can be deduced from modification records. However, developers often access artifacts that they do not change, and this information is not modeled or recorded by current research systems. In this paper, we describe an exploratory industrial case study that we have conducted to investigate this issue; we found that within a given maintenance task, the software artifacts that are viewed but not changed outnumber the changed artifacts over 10% of the time. We further found that including information about which artifacts were changed and which were only viewed was key to a mature understanding of the tasks that the developers were performing. Finally, we discuss how creating a repository that captures both the viewed-only and modified artifact accesses can yield further\u00a0\u2026", "num_citations": "30\n", "authors": ["111"]}
{"title": "Build system issues in multilanguage software\n", "abstract": " Building software from source is often viewed as a \u201csolved problem\u201d by software engineers, as there are many mature, well-known tools and techniques. However, anecdotal evidence suggests that these tools often do not effectively address the complexities of building multilanguage software. To investigate this apparent problem, we have performed a qualitative study on a set of five multilanguage open source software packages. Surprisingly, we found build system problems that prevented us from building many of these packages out-of-the-box. Our key finding is that there are commonalities among build problems that can be systematically addressed. In this paper, we describe the results of this exploratory study, identify a set of common build patterns and anti-patterns, and outline research directions for improving the build process. One such finding is that multilanguage packages avoid certain build problems by\u00a0\u2026", "num_citations": "28\n", "authors": ["111"]}
{"title": "Going green: An exploratory analysis of energy-related questions\n", "abstract": " The popularity of smartphones - small computers that run on battery power - has exploded in the last decade. Unsurprisingly, power consumption is an overarching concern for mobile app developers, who are anxious to learn about power-related problems that are encountered by others. In this paper, we present an empirical study exploring the characteristics of energy-related questions posed in Stack Overflow, issues faced by the developers, and the most significantly discussed APIs. We extracted a sample of 5009 Stack Overflow questions, and manually analyzed 1000 posts of Android-related energy questions. Our study shows that developers are most concerned about energy-related issues that concern improper implementations, sensor, and radio utilization.", "num_citations": "26\n", "authors": ["111"]}
{"title": "A taxonomy of clones in source code: The re-engineers most wanted list\n", "abstract": " Code cloning\u2014that is, the gratuitous duplication of source code within a software system\u2014is an endemic problem in large, industrial systems [6, 5]. While there has been much research into techniques for clone detection and analysis, there has been relatively little empirical study on characterizing how, where, and why clones occur in industrial software systems. Our current research is to perform an in-depth analysis of code cloning in real software systems and to build a taxonomy of types of code duplication.Code duplication, or code cloning, is generally believed to be common in software systems [6, 10, 12, 11, 8, 1, 5]. Various problems are associated with code duplication, including increased code size and increased maintenance costs. Other problems associated with code duplication can be found in [6, 12, 8, 1, 5]. While clone detection is an area of active research, and several tools exist to facilitate code clone detection, there has been relatively little empirical research on the types of clones that are found, or where they are found. A code clone pair is a pair of source code segments that are structurally or syntactically similar. One of the segments is usually a copy of the other, perhaps with minor changes. Code cloning occurs when developers create two identical or similar code artifacts inside a software system. For example, developers may copy and paste code. Several methods exist for detecting code clones in software, such as simple string matching [6], using statistical fingerprints of code segments [7], function metrics matching [10, 12, 11], parameterized string matching [1, 8], and program graph comparison [5]. In our current research\u00a0\u2026", "num_citations": "26\n", "authors": ["111"]}
{"title": "What does control flow really look like? Eyeballing the cyclomatic complexity metric\n", "abstract": " Assessing the understandability of source code remains an elusive yet highly desirable goal for software developers and their managers. While many metrics have been suggested and investigated empirically, the McCabe cyclomatic complexity metric (CC) - which is based on control flow complexity - seems to hold enduring fascination within both industry and the research community despite its known limitations. In this work, we introduce the ideas of Control Flow Patterns (CFPs) and Compressed Control Flow Patterns (CCFPs), which eliminate some repetitive structure from control flow graphs in order to emphasize high-entropy graphs. We examine eight well-known open source Java systems by grouping the CFPs of the methods into equivalence classes, and exploring the results. We observed several surprising outcomes: first, the number of unique CFPs is relatively low, second, CC often does not accurately\u00a0\u2026", "num_citations": "25\n", "authors": ["111"]}
{"title": "System-level usage dependency analysis of object-oriented systems\n", "abstract": " Uncovering, modelling, and understanding architectural level dependencies of software systems is a key task for software maintainers. However, current dependency analysis techniques for object-oriented software are targeted at the class or method level; this is because most dependencies\u2014such as instantiates, references, and calls\u2014must be interpreted in the context of one or more class hierarchies. In this paper, we propose an approach, called the High-level Object Dependency Graph (HODG), that captures all possible usage dependencies among coarse-grained entities. Based on the new model, we further propose a set of dependency analysis methods. Finally, we present an exploratory case study using HODGs\u2014supported by an automated analysis tool\u2014of the Apache Ant build system; we show how HODG analysis can help maintainers capture external properties of coarse-grained entities, and better\u00a0\u2026", "num_citations": "22\n", "authors": ["111"]}
{"title": "Unified use case statecharts: Case studies\n", "abstract": " This paper presents the results of case studies evaluating a method of unifying use cases (UCs) to derive a unified statechart model of the behavior of the domain of a proposed computer-based system. An evaluation of the unification method, the obtained statechart model of the domain, the method\u2019s and model\u2019s feedback on the UCs themselves, and how the method is used in requirements engineering practice was carried out by examining 58 software requirements specifications produced by 189 upper-year undergraduate and graduate students. The results of these studies independently confirm some of the benefits of building a unified SC mentioned in the works of Glinz; Whittle and Schumann; and Harel, Kugler, and Pnueli.", "num_citations": "21\n", "authors": ["111"]}
{"title": "Four interesting ways in which history can teach us about software\n", "abstract": " This paper outlines four kinds of studies that we have undertaken in trying to understand various aspects of a software system's evolutionary history. In each instance, the studies have involved detailed examination of real software systems based on \"facts\" extracted from various kinds of source artifact repositories, as well as the development of accompanying tools to aid in the extraction, abstraction, and comprehension processes. We briefly discuss the goals, results, and methodology of each approach.", "num_citations": "21\n", "authors": ["111"]}
{"title": "Requirements specifications and recovered architectures as grounded theories\n", "abstract": " This paper describes the classic grounded theory (GT) process as a method to discover GTs to be subjected to later empirical validation. The paper shows that a well conducted instance of requirements engineering or of architecture recovery resembles an instance of the GT process for the purpose of discovering the requirements specification or recovered architecture artifact that the requirements engineering or architecture recovery produces. Therefore, this artifact resembles a GT.", "num_citations": "20\n", "authors": ["111"]}
{"title": "Formal specification in metamorphic programming\n", "abstract": " Formal specification methods have not been embraced wholeheartedly by the software development industry. We believe that a large part of industry's reluctance is due to semantic gaps that are encountered when attempting to integrate formal specification with other stages of the software development process. Semantic gaps necessitate a dramatic shift in a programmer's mode of thought, and undergoing many such shifts during the development of a software system is inefficient We identify semantic gaps in the software development process and show how they can be minimized by an approach called metamorphic programming that operates in-the-large and in-the-small. The main contribution that metamorphic programming makes to formal specification is to clarify the ways in which specifications can be merged smoothly into the software development lifecycle.", "num_citations": "19\n", "authors": ["111"]}
{"title": "Architecture recovery of dynamically linked applications: A case study\n", "abstract": " Most previously published case studies in architecture recovery have been performed on statically linked software systems. Due to the increase in use of middleware technologies, such as CORBA, and object-oriented programming concepts, such as polymorphism, there is an opportunity and a need to analyze architectures of these dynamically linked systems. This paper presents the results of software architecture extraction of the Nautilus file manager, which employs CORBA in its implementation. A combination of existing static analysis and use-case modeling architecture recovery techniques was used, with the expectation of complex but complete architecture extraction of a system such as Nautilus. We have found that this combined approach, named Dynamo-1, presented in this paper provided successful focused architecture recovery and guidance for future work in the complete architecture recovery of\u00a0\u2026", "num_citations": "18\n", "authors": ["111"]}
{"title": "A study of cloning in the Linux SCSI drivers\n", "abstract": " To date, most research on software code cloning has concentrated on detection and analysis techniques and their evaluation, and most empirical studies of cloning have investigated cloning within single system versions. In this paper, we present the results of a longitudinal study of cloning among the SCSI drivers for the Linux operating system that spans 16 years of evolution. We have chosen the SCSI driver subsystem as a test subject as it is known that cloning has been embraced by these developers as a design practice: when a new SCSI card comes out that is similar to an old one, but different enough to warrant its own implementation, a new driver may be cloned from an existing one. We discuss the results of our qualitative and quantitative analyses, including how the layered architecture of the SCSI subsystem seems to have affected the use of cloning as a design tool, the likelihood of consistent and\u00a0\u2026", "num_citations": "16\n", "authors": ["111"]}
{"title": "Identifying architectural change patterns in object-oriented systems\n", "abstract": " As an object-oriented system evolves, its architecture tends to drift away from the original design. Knowledge of how the system has changed at coarse-grained levels is key to understanding the de facto architecture, as it helps to identify potential architectural decay and can provide guidance for further maintenance activities. However, current studies of object-oriented software changes are mostly targeted at the class or method level. In this paper, we propose a new approach to modeling object-oriented software changes at coarse-grained levels. We take snapshots of an object-oriented system, represent each version of the system as a hybrid model, and detect software changes at coarse-grained level by comparing two hybrid models. Based on this approach, we further identify a collection of change patterns, which help interpret how system changes at the architecture level. Finally, we present an exploratory\u00a0\u2026", "num_citations": "16\n", "authors": ["111"]}
{"title": "The build/comprehend pipelines\n", "abstract": " Large software systems often have complex subparts and complex build processes, and engage in subtle relationships with the underlying technologies from which they are designed and constructed. Most reverse engineering toolkits ignore the attributes and relationships of system construction; instead, they concentrate on static relationships among externally visible source code elements. This paper takes the position that the comprehension process for a large software system should mimic the system\u2019s build process.", "num_citations": "16\n", "authors": ["111"]}
{"title": "Enhancing domain-specific software architecture recovery\n", "abstract": " Performing software architecture analysis and recovery on a large software system is expensive and time consuming; when it is done at all, it is often performed within a narrow context, focused on a few areas of particular concern. However, for a long-lived system within a well understood application domain, the costs for performing detailed architecture recovery may be amortized over several generations of the system; the resulting models can also be broadened and put into context by incorporating information about the history and anticipated future evolution of both the application and its underlying domain. This paper proposes a systematic approach for organizing application domain knowledge into a unified structure called the Architectural Domain Assets Set (ADAS). The ADAS structure builds on previous research, as well as our experience in performing an architecture recovery of IBM's DB2. Our initial\u00a0\u2026", "num_citations": "15\n", "authors": ["111"]}
{"title": "Extracting source models from Java programs: Parse, disassemble, or profile\n", "abstract": " Source models of software systems are often created during re-engineering to aid in performing tasks such as reachability analysis and software architecture recovery. It is therefore vital to be able to create source models that are both detailed and accurate. However, in practice the creation of these models is difficult and error prone: extraction tools often tell only part of the story.We have been working on the automated extraction of source models for programs written in Java. Our approach considers Java programs from three points of view: parsing, disassembly, and profiling. We have found that these three techniques have advantages that are complementary. Parsing source code provides the most detailed information, but it is the most complex to implement. Disassembling Java byte code gives similar results to parsing, but is less complex technically. Profiling provides the least amount of detail, but does give important feedback on run-time behaviour, such as polymorphic function calls and reflective instantiation of objects by string input.", "num_citations": "15\n", "authors": ["111"]}
{"title": "Revisiting bug triage and resolution practices\n", "abstract": " Bug triaging is an error-prone, tedious and time-consuming task. However, little qualitative research has been done on the actual use of bug tracking systems, bug triage, and resolution processes. We are planning to conduct a qualitative study to understand the dynamics of bug triage and fixing process, as well as bug reassignments and reopens. We will study interviews conducted with Mozilla Core and Firefox developers to get insights into the primary obstacles developers face during the bug fixing process. Is the triage process flawed? Does bug review slow things down? Does approval takes too long? We will also categorize the main reasons for bug reassignments and reopens. We will then combine results with a quantitative study of Firefox bug reports, focusing on factors related to bug report edits and number of people involved in handling the bug.", "num_citations": "14\n", "authors": ["111"]}
{"title": "Practical data exchange for reverse engineering frameworks: Some requirements, some experience, some headaches\n", "abstract": " Reverse engineering systems hold great promise in aiding developers regain control over long-lived software projects whose architecture has been allowed to \"drift\". However, it is well known that these systems have relative strengths and weaknesses, and to date relatively little work has been done on integrating various subtools within other reverse engineering systems. The design of a common interchange format for data used by reverse engineering tools is therefore of critical importance.In this position paper, we describe some of our previous work with TAXFORM (Tuple Attribute eXchange FORMat) [2,6], and in integrating various \"fact extractors\" into the PBS reverse engineering system. For example, we have recently created translation mechanisms that enable the Acacia system's C and C++ extractors to be used within PBS, and we have used these mechanisms to create software architecture models of two\u00a0\u2026", "num_citations": "14\n", "authors": ["111"]}
{"title": "A program understanding environment based on the \"Star\" approach to tool integration\n", "abstract": " CSC'94: Proceedings of the 22nd annual ACM computer science conference on Scaling up: meeting the challenge of complexity in real-world computing applications: meeting the challenge of complexity in real-world computing applications March 1994 Pages 60\u201365 https://doi. org/10.1145/197530.197557", "num_citations": "14\n", "authors": ["111"]}
{"title": "Connecting software architecture recovery frameworks\n", "abstract": " There is a wide variety of standalone tools that are designed to help recover the design of software systems. These tools operate at different levels of abstraction, from the \u201ccode-level\u201d to software architectures. To date, no significant reuse has been possible between tools because there is no standard format for encoding the facts these tools derive from the system implementation. Recent work has made progress on sharing information between tools that operate at the \u201ccode-level\u201d. This paper reviews existing exchange formats and discusses requirements for a framework to exchange information at the software architecture level.", "num_citations": "13\n", "authors": ["111"]}
{"title": "Detecting feature-interaction symptoms in automotive software using lightweight analysis\n", "abstract": " Modern automotive software systems are large, complex, and feature rich; they can contain over 100 million lines of code, comprising hundreds of features distributed across multiple electronic control units (ECUs), all operating in parallel and communicating over a CAN bus. Because they are safety-critical systems, the problem of possible Feature Interactions (FIs) must be addressed seriously; however, traditional detection approaches using dynamic analyses are unlikely to scale to the size of these systems. We are investigating an approach that detects static source-code patterns that are symptomatic of FIs. The tools report Feature-Interaction warnings, which can be investigated further by engineers to determine if they represent true FIs and if those FIs are problematic. In this paper, we present our preliminary toolchain for FI detection. First, we extract a collection of static \u201cfacts\u201d from the source code, such as\u00a0\u2026", "num_citations": "12\n", "authors": ["111"]}
{"title": "An industrial case study of Coman's automated task detection algorithm: What Worked, What Didn't, and Why\n", "abstract": " Programmers need explicit tool support for software maintenance tasks, and a prerequisite for this is an understanding of where the boundaries between distinct tasks lie. Asking developers to indicate manually when they switch tasks is disruptive to their normal work flow, so researchers have sought ways to infer task boundaries automatically based on the content of the interaction histories with the IDE. Coman previously reported a fully automated algorithm that achieved 80% accuracy in a lab validation study. In this paper, we evaluate the use of this algorithm within an industrial setting. We found two problems: first, a large number of the tasks identified are in fact only sessions or subparts of a larger task; second, the demonstrable effects of interruptions are not considered. We argue that the problem of task boundary detection consists of two sub-problems: first, detecting task sessions; and second, linking task\u00a0\u2026", "num_citations": "12\n", "authors": ["111"]}
{"title": "Understanding interaction differences between newcomer and expert programmers\n", "abstract": " Newcomer and expert programmers often interact with development artifacts differently. Ideally, software development tools should support these different styles of work. In this paper, we describe our investigations into the interaction difference between newcomers and experts, regarding two properties that characterize repetition of programmer interaction: temporal locality and interaction coupling recurrence. We describe our approach, research questions and planned methodology.", "num_citations": "12\n", "authors": ["111"]}
{"title": "Compiling clones: What happens?\n", "abstract": " Most clone detection techniques have focused on the analysis of source code, however, sometimes stakeholders have access only to compiled code. To address this, some approaches have been developed for finding similarities at the binary level. However, the precise relationships between source-level and binary-level similarities remains unclear: While a compiler will preserve the semantics of the source code in the transformation to an executable, the resulting binary may differ significantly in structure, including the addition and deletion of entities in the source model. Also, compilation sometimes acts as a kind of normalization, transforming syntactically different but semantically similar structures into the same binary-level representation. In this paper, we describe a preliminary study into the effects of the javac Java compiler on the results of clone detection. We use CCFinderX -- which can perform clone\u00a0\u2026", "num_citations": "11\n", "authors": ["111"]}
{"title": "A case study in architectural analysis: The evolution of the modern web browser\n", "abstract": " A reference architecture for a domain captures the fundamental subsystems common to systems of that domain, as well as the relationships between these subsystems. A reference architecture can be useful both at design time and during maintenance: it can improve understanding of a given system, aid in analyzing trade-offs between different design options, or serve as a template for designing new systems and reengineering existing ones.In this paper, we use a semi-automated analysis method to investigate the architecture and evolution of a well known application domain: the web browser. We examine the history of the web browser domain and identify several underlying forces that have contributed to its evolution. We develop a reference architecture for web browsers based on two well-known open source implementations, and we validate it against five additional implementations. We discuss the maintenance and comprehension implications of different strategies for code reuse and identify several underlying evolutionary phenomena in the web browser domain; namely, emergent domain boundaries, convergent evolution, and tension between open and closed source development approaches.", "num_citations": "11\n", "authors": ["111"]}
{"title": "Increasing quality of conceptual models: Is object-oriented analysis that simple?\n", "abstract": " Several researchers have recently indicated an urgent need for re-evaluation and validation of the various software engineering abstraction techniques, and object orientation in particular. This paper presents three questionable practices and one promising direction with respect to achieving high quality analysis models. This work is based on five years of observation of more than 700 students working on software requirements specifications of a small telephone exchange and a related accounts management system.", "num_citations": "11\n", "authors": ["111"]}
{"title": "DASHboards: Enhancing developer situational awareness\n", "abstract": " Issue trackers monitor the progress of software development\" issues\", such as bug fixes and discussions about features. Typically, developers subscribe to issues they are interested in through the tracker, and are informed of changes and new developments via automated email. In practice, however, this approach does not scale well, as developers may receive large volumes of messages that they must sort through using their mail client; over time, it becomes increasingly challenging for them to maintain awareness of the issues that are relevant to their activities and tasks. To address this problem, we present a tool called DASH that is implemented in the form of personalized views of issues; developers indicate issues of interest and DASH presents customized views of their progress and informs them of changes as they occur.", "num_citations": "10\n", "authors": ["111"]}
{"title": "Regression-based utilization prediction algorithms: an empirical investigation.\n", "abstract": " Predicting future behavior reliably and efficiently is vital for systems that manage virtual services; such systems must be able to balance loads within a cloud environment to ensure that service level agreements (SLAs) are met at a reasonable expense. These virtual services while often comparatively idle are occasionally heavily utilized. Standard approaches to modeling system behavior (by analyzing the totality of the observed data, such as regression based approaches) tend to predict average rather than exceptional system behavior and may ignore important patterns of change over time. Consequently, such approaches are of limited use in providing warnings of future peak utilization within a cloud environment. Skewing predictions to better fit peak utilizations, results in poor fitting to low utilizations, which compromises the ability to accurately predict peak utilizations, due to false positives.In this paper, we present an adaptive approach that estimates, at run time, the best prediction value based on the performance of the previously seen predictions. This algorithm has wide applicability. We applied this adaptive technique to two large-scale real world case studies. In both studies, the results show that the adaptive approach is able to predict low, medium, and high utilizations accurately, at low cost, by adapting to changing patterns within the input time series. This facilitates better proactive management and placement of systems running within a cloud.", "num_citations": "10\n", "authors": ["111"]}
{"title": "Understanding software artifact provenance\n", "abstract": " In a well designed software system, units of related functionality are organized into modules and classes, which are in turn arranged into inheritance trees, package hierarchies, components, libraries, frameworks, and services. The trade-offs between simplicity versus flexibility and power are carefully considered, and interfaces are designed that expose the key functional properties of a component while hiding much of the complexity of the implementation details. However, over time the design integrity of a well-engineered system tends to decay as new features are added, as new quality attributes are emphasized, and as old architectural knowledge is lost when experienced development personnel shift to new jobs. Consequently, as developers and as users we often find ourselves looking at a piece of functionality or other design artifact and wondering, \u201cWhy is this here?\u201d That is, we would like to examine the\u00a0\u2026", "num_citations": "9\n", "authors": ["111"]}
{"title": "Method and apparatus for pattern-based system design analysis using a meta model\n", "abstract": " A method for analyzing a target system that includes obtaining a characteristics model, loading the characteristics model into a meta model, obtaining a plurality of characteristics from the target system using a characteristics extractor, wherein each of the plurality of characteristics is associated with the characteristics model, storing each of the plurality of characteristics obtained from the target system in a characteristics store, and analyzing the target system by issuing at least one query to the characteristics store to obtain an analysis result, wherein the issuing the at least one query comprises verifying the at least one query using the meta model.", "num_citations": "9\n", "authors": ["111"]}
{"title": "Controlled-release sedative-hypnotic compositions and methods related thereto\n", "abstract": " Controlled-release formulations providing a \u201cpulsed\u201d plasma profile of a Sedative-hypnotic compounds having a particularly short half-life are provided. The formulation contains a Sedative-hypnotic compound or precursor thereof that is metabolized to generate a Sedative-hypnotic com pound in Vivo, wherein the compound has a mean plasma half life ranging from 0.1 to 2 hours, and at least one release retardant Such that, following administration of the formu lation to a patient, the patient has Specified pulsed plasma profile for the Sedative-hypnotic compound as disclosed herein. In a preferred embodiment, the Sedative-hypnotic compound is NBI-34060.", "num_citations": "9\n", "authors": ["111"]}
{"title": "A Hybrid Program Model for Object-Oriented Reverse Engineering\n", "abstract": " A commonly used strategy to address the scalability challenge in object-oriented reverse engineering is to synthesize coarse-grained representations, such as package diagrams. However, the traditional coarse-grained representations are poorly suited to object-oriented program comprehension as they can be difficult to map to the domain object models, contain little real detail, and provide few clues to the design decisions made during development. In this paper, we propose a hybrid model of object-oriented software that blends the use of classes and entities at different levels of granularity. Each coarse-grained entity represents a set of software objects, and contains the complete static description of the objects it represents. This hybrid model allows maintainers to understand objects as independent units, and focus on the their external properties and their interrelationships at different levels of granularity. We\u00a0\u2026", "num_citations": "8\n", "authors": ["111"]}
{"title": "Software and biological evolution: Some common principles, mechanisms, and a definition\n", "abstract": " In this position paper, we explore some of the principles and mechanisms that are shared by both software evolution and biological evolution. Our goal is to highlight some of the commonalities between them, to point out some interesting questions that are raised, and to engender discussions within the software engineering research community on this topic. Our preliminary discussion suggests that new light can be shed on the nature of software evolution itself, and we hypothesize that further study will lead to a better understanding of the various forces that underlie software evolution. We conclude by proposing a new definition for software evolution that reflects this perspective.", "num_citations": "8\n", "authors": ["111"]}
{"title": "Investigating intentional clone refactoring\n", "abstract": " Software clone refactoring has been studied from many perspectives, including empirical research on clone refactoring history, IDE supportfor tracking clone change, and recommendation systems for clonemanagement. Most of the work relies on having access to and being ableto analyze the history of clone refactoring. However, refactoring clonedcode is not equivalent to clone management, as code refactoring can bemotivated by goals unrelated to cloning. In this position paper, weintroduce a dataset of intentional clone refactoring, which is producedby keywords matching in commit messages within the version control systemof Linux kernel. By investigating two important clone evolution scenarios---clone removal and inconsistent changes---in subsystems of Linuxkernel, we find that intentional clone refactoring accounts for only asmall proportion of all detected clone evolution.", "num_citations": "7\n", "authors": ["111"]}
{"title": "Attribute Based Software Evolution: Patterns and Product Line Forecasting\n", "abstract": " This paper presents a new systematic approach for the study of software evolution. The approach is based on the use of functional and quality attributes to recover, document, and apply knowledge about how and why software systems evolve. In the first part of paper, we discuss how the study of software evolution in terms of attributes makes it possible to draw parallels between software evolution and other types of evolution, such as natural evolution. This allows us to analyze their similarities and to map some of the results and methodologies from these other fields to software evolution.In the second part of paper, we discuss how the use of attributes can make knowledge about how a system has evolved more easily applicable to other attribute-based techniques of software engineering, including goal-driven requirements engineering processes [1, 19], Architecture Tradeoff Analysis Method (ATAM)[11], and Attribute-Based Architectural Styles [13]. In particular, we emphasize attribute-based evolution patterns as a practical form of preserving and applying knowledge about how a software system evolves, and we describe their use in product line design and forecasting. Finally, we present an abbreviated example history of the Linux SCSI device driver subsystem.", "num_citations": "7\n", "authors": ["111"]}
{"title": "Teaching software engineering to a mixed audience\n", "abstract": " This paper describes some observations derived from teaching a course in software engineering to a mixed audience of undergraduates and professional Master's degree students at Cornell University. We describe our philosophical goals in teaching the course, some of the problems we encountered, some of the unexpected results, and what we intend to do differently next time.", "num_citations": "7\n", "authors": ["111"]}
{"title": "We have all of the clones, now what? Toward integrating clone analysis into software quality assessment\n", "abstract": " Cloning might seems to be an unconventional way of designing and developing software, yet it is very widely practised in industrial development. The cloning research community has made substantial progress on modeling, detecting, and analyzing software clones. Although there is continuing discussion on the real role of clones on software quality, our community may agree on the need for advancing clone management techniques. Current clone management techniques concentrate on providing editing tools that allow developers to easily inspect clone instances, track their evolution, and check change consistency. In this position paper, we argue that better clone management can be achieved by responding to the fundamental needs of industry practitioners. And the possible research directions include a software problem-oriented taxonomy of clones, and a better structured clone detection report. We believe\u00a0\u2026", "num_citations": "6\n", "authors": ["111"]}
{"title": "Understanding source package organization using the hybrid model\n", "abstract": " Within a large, object-oriented software system it is common to partition the classes into a set of packages, which implicitly serve as a set of coarsely-grained logical design units. However, as such a system evolves and design drift sets in, it becomes increasingly challenging for developers - especially those who are new to the project - to comprehend the underlying criteria behind the package-level design of the system. This problem is exacerbated by the fact that in most object-oriented programming languages the package (or namespace) construct has little semantics beyond that of a simple container, and so fails to capture the essential properties of the objects that its contained classes represent. In this paper, we propose an approach to uncovering package partitioning criteria by analyzing the collaboration patterns between packages. Our analysis approach is based on the Hybrid Model, a program model that\u00a0\u2026", "num_citations": "6\n", "authors": ["111"]}
{"title": "A lightweight architecture recovery process\n", "abstract": " In this paper, we present an overview of a lightweight approach for software architecture recovery. The main advantages of the process are the lightweight recovery of architectural semantics, and the compatibility with the highly iterative adaptive development processes that involve extensive architectural refactorings.", "num_citations": "5\n", "authors": ["111"]}
{"title": "Visualizing architectural evolution\n", "abstract": " Traditionally, the structure of large software systems has been viewed from two orthogonal perspectives: the snapshot view\", which shows the architecture of a single version of a software system, its components, and their interdependencies, and the evolutionary view\", which shows how individual components typically, source files have changed over time 5. In this paper we describe some of our experiences in merging these two points of view. In particular, we describe how we model the evolution of large-scale architectural components of a system. We observe the change in a system's architecture by extracting and visualizing the relationships that have changed as a result of the evolution. Our approach was applied to three non-trivial programs as test cases: gmake, grep, and wget. We discuss our approach, our results and observations in one of these case studies, as well as plans for future work which includes the examination of multiple versions at the same time.", "num_citations": "5\n", "authors": ["111"]}
{"title": "Analyzing assembler to eliminate dead functions: An industrial experience\n", "abstract": " Industrial software systems often contain fragments of code that are vestigial, that is, they were created long ago for a specific purpose but are no longer useful within the current design of the system. In this work, we describe how we have adapted some research tools to remove such code, we use a hybrid static analysis approach of both source code and assembler to construct a model of the system, and then use graph querying to detect possible dead functions. Suspected dead functions are then commented out of the source. The system is then rebuilt and run against existing test suites to verify that the removals do not affect the semantics of the system. Finally, we discuss the results of performing this technique on a large and long-lived industrial software system as well as a large open source system.", "num_citations": "4\n", "authors": ["111"]}
{"title": "Enhancing domain-specific software architecture recovery\n", "abstract": " Performing software architecture analysis and recovery on a large software system is expensive and time consuming; when it is done at all, it is often performed within a narrow context, focused on a few areas of particular concern. However, for a long-lived system within a well understood application domain, the costs for performing detailed architecture recovery may be amortized over several generations of the system; the resulting models can also be broadened and put into context by incorporating information about the history and anticipated future evolution of both the application and its underlying domain. This paper", "num_citations": "4\n", "authors": ["111"]}
{"title": "Representing build-time software architecture views with UML\n", "abstract": " We have found that some classes of software systems exhibit interesting and complex build-time properties that are not explicitly address by existing models of software architecture. In this paper, we briefly explain the idea of", "num_citations": "4\n", "authors": ["111"]}
{"title": "Defining, transforming, and exchanging high-level schemas\n", "abstract": " The reverse engineering research community has been investigating mechanisms for exchanging data and partial results between reverse engineering tools (Woods et al., 1998; Holt et al., 2000). Among the many subproblems inherent in implementing a standard exchange format (SEF) is the design of schemas that represent views of source code at various levels of abstraction. We briefly discuss some of the issues in defining, transforming, and exchanging high-level schemas, including previous work on the TAXFORM project (Bowman et al., 2000) as well as discussions at the Workshop on Standard Exchange Format (WoSEF) (Holt et al., 2000).", "num_citations": "4\n", "authors": ["111"]}
{"title": "JDuck: Building a software engineering tool in Java as a CS2 project\n", "abstract": " This paper describes our experiences in having students build a software engineering tool as a course project in a CS2 course. The tool, which we called JDuck (Java Documenter of Code, oK), was modelled on the javadoc tool that is part of Sun Microsystem's standard Java Development Kit (JDK). That is, a working version of JDuck would be able to read in Java source code and generate HTML files that summarize the basic structure of the provided classes. We discuss how we set up the project, what we think the students learned, what they told us they learned, and what we would do differently next time.", "num_citations": "4\n", "authors": ["111"]}
{"title": "Mea culpa: How developers fix their own simple bugs differently from other developers\n", "abstract": " In this work, we study how the authorship of code affects bug-fixing commits using the SStuBs dataset, a collection of single-statement bug fix changes in popular Java Maven projects. More specifically, we study the differences in characteristics between simple bug fixes by the original author -- that is, the developer who submitted the bug-inducing commit -- and by different developers (i.e., non-authors). Our study shows that nearly half (i.e., 44.3%) of simple bugs are fixed by a different developer. We found that bug fixes by the original author and by different developers differed qualitatively and quantitatively. We observed that bug-fixing time by authors is much shorter than that of other developers. We also found that bug-fixing commits by authors tended to be larger in size and scope, and address multiple issues, whereas bug-fixing commits by other developers tended to be smaller and more focused on the bug itself. Future research can further study the different patterns in bug-fixing and create more tailored tools based on the developer's needs.", "num_citations": "3\n", "authors": ["111"]}
{"title": "Connecting the dots: anomaly and discontinuity detection in large-scale systems\n", "abstract": " Cloud providers and data centers rely heavily on forecasts to accurately predict future workload. This information helps them in appropriate virtualization and cost-effective provisioning of the infrastructure. The accuracy of a forecast greatly depends upon the merit of performance data fed to the underlying algorithms. One of the fundamental problems faced by analysts in preparing data for use in forecasting is the timely identification of data discontinuities. A discontinuity is an abrupt change in a time-series pattern of a performance counter that persists but does not recur. Analysts need to identify discontinuities in performance data so that they can (a) remove the discontinuities from the data before building a forecast model and (b) retrain an existing forecast model on the performance data from the point in time where a discontinuity occurred. There exist several approaches and tools to help analysts identify\u00a0\u2026", "num_citations": "3\n", "authors": ["111"]}
{"title": "Extracting artifact lifecycle models from metadata history\n", "abstract": " Software developers and managers make decisions based on the understanding they have of their software systems. This understanding is both built up experientially and through investigating various software development artifacts. While artifacts can be investigated individually, being able to summarize characteristics about a set of development artifacts can be useful. In this paper we propose lifecycle models as an effective way to gain an understanding of certain development artifacts. Lifecycle models capture the dynamic nature of how various development artifacts change over time in a graphical form that can be easily understood and communicated. Lifecycle models enables reasoning of the underlying processes and dynamics of the artifacts being analyzed. In this paper we describe how lifecycle models can be generated and demonstrate how they can be applied to the code review process of a\u00a0\u2026", "num_citations": "2\n", "authors": ["111"]}
{"title": "Examining the effects of global data usage on software maintainability\n", "abstract": " As the useful life expectancy of software continues to increase, the task of maintaining the source code has become the dominant phase of the software life-cycle. In order to improve the ability of software to age and successfully evolve over time, it is important to identify system design and programming practices which may result in increasing the difficulty of maintaining the source code. This study attempts to correlate the use of global data to the maintainability of several widely deployed, large scale software projects as they evolve over time. Two measures are proposed to quantify the maintenance effort of a project. The first measure compares the number of CVS revisions for all source files in a release to the number of revisions applied to the files where the usage of global data is most prevalent. A second degree of change is characterized by contrasting the amount of source code that was changed overall with\u00a0\u2026", "num_citations": "2\n", "authors": ["111"]}
{"title": "Domain system statecharts: The good, the bad, and the ugly\n", "abstract": " This paper presents the results of case studies evaluating our method of unifying use cases (UCs) to derive a unified StateChart (SC) model of the behavior of the domain system (DS) of a proposed computer-based system. An evaluation of the unification method, the obtained SC model of the DS, the method\u2019s and model\u2019s feedback on the UCs themselves, and how the method is used in requirements engineering practice was carried out by examining 46 software requirements specifications produced by 149 upper-year undergraduate and graduate students. The results of our studies independently confirm some of the benefits of building a unified SC mentioned in the works of Glinz; Whittle and Schumann; and Harel, Kugler, and Pnueli, who have developed formal treatments of unifying UCs using SCs and, in two cases, have built tools implementing their treatments.", "num_citations": "2\n", "authors": ["111"]}
{"title": "Attribute-based evolution patterns for product lines\n", "abstract": " Several recent achievements in software architecture and requirements engineering are based on the study and use of quality attributes; these include goal-driven requirements engineering processes [1, 14], Architecture Tradeoff Analysis Method (ATAM)[7], and Attribute-Based Architectural Styles [13]. This paper presents an initial study of the evolution of the systems in terms of quality attributes. The purpose of the study is to put the knowledge about evolution in the form directly usable by other quality attribute based techniques. The paper emphasizes the use of the attributebased evolution patterns in the reconstruction and design of product line architectures.", "num_citations": "2\n", "authors": ["111"]}
{"title": "Visual formalisms for configuration management\n", "abstract": " As reuse of software components becomes more commonplace, being able to understand, manipulate and reason about software system architectures acquires new importance. Although commercial software development environments have addressed many of the issues of configuration management, there is still a need for visual formalisms that can aid in representing and manipulating architectures of software systems. This paper introduces ConForm Configuration Formalism, a graphical notation for representing configurations of software systems. Several of the basic concepts of ConForm were inspired by the C Mesa language 8; however, ConForm is both language and tool independent. ConForm is notable because it is both a visual and a formal approach to representing software architectures.", "num_citations": "2\n", "authors": ["111"]}
{"title": "Spectur: a Specification Language for the Programmer\n", "abstract": " Spectur (Speci cation language after Turing) has been designed as a language for formal speci cation. The goals of the language include: ease of use by a speci er to write a software system speci cation from an informal description; ease of use by an implementor to build a program from a speci cation; ease of understanding by a user as documentation; ease of use by a mathematician to prove program correctness. The Spectur method of specifying a program module consists of declaring abstracted data structures, and specifying the procedures and functions of the module in terms of pre-and post-conditions on those data structures. Spectur is notable for providing notation and data structures that are both mathematically precise and immediately recognizable by programmers. A non-trivial example speci cation of a small operating system has been done using Spectur. We discuss this example, as well as the usefulness and potential applications of Spectur.", "num_citations": "2\n", "authors": ["111"]}
{"title": "mel- model extractor language for extracting facts from models\n", "abstract": " There is a large body of research on extracting models from code-related artifacts to enable model-based analyses of large software systems. However, engineers do not always have access to the entire code base of a system: some components may be procured from third-party suppliers based on a Model specification or their code may be generated automatically from Models.", "num_citations": "1\n", "authors": ["111"]}
{"title": "A study on the effects of exception usage in open-source C++ systems\n", "abstract": " Exception handling (EH) is a feature common to many modern programming languages, including C++, Java, and Python, that allows error handling in client code to be performed in a way that is both systematic and largely detached from the implementation of the main functionality. However, C++ developers sometimes choose not to use EH, as they feel that its use increases complexity of the resulting code: new control flow paths are added to the code, \"stack unwinding\" adds extra responsibilities for the developer to worry about, and EH arguably detracts from the modular design of the system. In this paper, we perform an exploratory empirical study of the effects of exceptions usage in 2721 open source C++ systems taken from GitHub. We observed that the number of edges in an augmented call graph increases, on average, by 22% when edges for exception flow are added to a graph. Additionally, about 8 out of\u00a0\u2026", "num_citations": "1\n", "authors": ["111"]}
{"title": "Why provenance matters\n", "abstract": " We are increasingly seeking to extract more and more information from our software development artifacts to infer high-level understanding of our products and the processes that create them. However, this begs many questions about the artifacts, their inter-relatedness, their histories, and the quality of the information that can be extracted. That is, the provenance of the artifacts must be studied to be able to answer these kinds of questions; this chapter explores the notion of software artifact provenance.", "num_citations": "1\n", "authors": ["111"]}
{"title": "Detecting Discontinuities in Large Scale Systems\n", "abstract": " Cloud providers and data centers rely heavily on forecasts to accurately predict future workload. This information helps them in appropriate virtualization and cost-effective provisioning of the infrastructure. The accuracy of a forecast greatly depends upon the merit of performance data fed to the underlying algorithms. One of the fundamental problems faced by analysts in preparing data for use in forecasting is the timely identification of data discontinuities. A discontinuity is an abrupt change in a time-series pattern of a performance counter that persists but does not recur. Analysts need to identify discontinuities in performance data so that they can a) remove the discontinuities from the data before building a forecast model and b) retrain an existing forecast model on the performance data from the point in time where a discontinuity occurred. There exist several approaches and tools to help analysts identify anomalies\u00a0\u2026", "num_citations": "1\n", "authors": ["111"]}
{"title": "Correlating Interaction Coupling with Static Coupling\u2014Two Exploratory Studies\n", "abstract": " Programmer interactions contain fine-grained details about multi-dimensional software development. This also means that by capturing and mining programmer interaction history, we can reason about various dimensions of software development, such as software design, programmer expertise, and tools. As an initial step towards this goal, we correlate interaction coupling (IC) with static coupling to explore the link between programmer interaction and various factors. We performed two exploratory case studies. Based on IDE interaction histories from twelve programmers and six software systems, we investigate how often and why IC may be present when there is no obvious static dependence in the underlying source code. We have observed that design, programmer expertise, and a mismatch between design and development may all had effects on ICs. We also introduce the idea of conceptual interaction coupling (CIC) and discuss its potential use in reasoning about software development.", "num_citations": "1\n", "authors": ["111"]}
{"title": "Introduction to the special issue on software repository mining in 2009\n", "abstract": " This special issue of Empirical Software Engineering consists of revised and extended versions of four selected papers originally presented at the 6th IEEE Working Conference on Mining Software Repositories (MSR 2009). The conference was held in Vancouver, Canada on May 16\u201317, and was co-located with the 2009 ACM/IEEE International Conference on Software Engineering (ICSE 2009). This conference brings together researchers who share an interest in advancing the science and practice of software engineering via the analysis of data stored in software repositories. Software repository mining research emerged in the late 1990s in conjunction with the ready availability of software configuration management, mailing list, and bug tracking repositories from open source projects; the first Mining Software Repositories Workshop was held in 2004 in Edinburgh. In the past, researchers were able to access\u00a0\u2026", "num_citations": "1\n", "authors": ["111"]}
{"title": "Clone Detection: How accurate is your data set?\n", "abstract": " Duplication of code in software systems is considered to be a serious problem that can affect a systems maintainability and extendability. It is reported that 10-15% of code in a software system is involved in cloning. However, because of the difficultly of objectively measuring the number of false positives in a clone result set, the accuracy of these reports is difficult to evaluate. Although an important topic, little work has been done in the area of evaluating the accuracy of clone detection methods. In this paper we propose a study to estimate the number of false positives that are likely to be in a data set in an objective way by measuring the number of clones found in a large body of unrelated code. We also propose a method to measure the impact of external factors such as programing idioms and API protocols on the detected results set. The results of this work will provide tools and knowledge to better evaluate the current state of the art of clone detection research.", "num_citations": "1\n", "authors": ["111"]}
{"title": "A Lightweight Process for Architecture Recovery: From Code to Domain Requirements and Back Again\n", "abstract": " For many information systems, both the problem domain and the supporting computing infrastructure change over time. As new features are added, new environments are supported, and old defects are fixed, the cumulative effects of these maintenance activities often pull the design elements of the system in different directions, causing architectural drift, conceptual inconsistencies, and a widening of the gap between requirements and code. Explicitly modeling the software architecture of a system as a part of the maintenance process can aid in lessening the negative side effects of maintenance, as the software architecture model serves as a partial bridge between the requirements of the business domain and the source code. In this paper, we present a lightweight process for architecture recovery that aids developers in creating and maintaining software architecture models. The process is designed to be practical for the recovery of architectures of small to mid-sized software systems; it is based on and extends the PBS tool architecture recovery approach and goal-based requirements engineering theory.", "num_citations": "1\n", "authors": ["111"]}