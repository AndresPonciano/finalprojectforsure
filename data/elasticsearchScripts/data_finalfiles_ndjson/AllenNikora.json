{"title": "Investigation of logistic regression as a discriminant of software quality\n", "abstract": " Investigates the possibility that logistic regression functions (LRFs), when used in combination with Boolean discriminant functions (BDFs), which we had previously developed, would improve the quality classification ability of BDFs when used alone; this was found to be the case. When the union of a BDF and LRF was used to classify quality, the predictive accuracy of quality and inspection cost was improved over that of using either function alone for the Space Shuttle. Also, the LRFs proved useful for ranking the quality of modules in a build. The significance of these results is that very high-quality classification accuracy (1.25% error) can be obtained while reducing the inspection cost incurred in achieving high quality. This is particularly important for safety-critical systems. Because the methods are general and not particular to the Shuttle, they could be applied to other domains. A key part of the LRF development\u00a0\u2026", "num_citations": "88\n", "authors": ["484"]}
{"title": "Developing fault predictors for evolving software systems\n", "abstract": " Over the past several years, we have been developing methods of predicting the fault content of software systems based on measured characteristics of their structural evolution. In previous work, we have shown there is a significant linear relationship between code churn, a synthesized metric, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code churn. We have begun a new investigation of this relationship with a flight software technology development effort at the jet propulsion laboratory (JPL) and have progressed in resolving the limitations of the earlier work in two distinct steps. First, we have developed a standard for the enumeration of faults. Second, we have developed a practical framework for automating the measurement of these faults. we analyze the measurements of structural evolution and fault counts obtained from the JPL flight software\u00a0\u2026", "num_citations": "76\n", "authors": ["484"]}
{"title": "Understanding the nature of software evolution\n", "abstract": " Over the past several years, we have been developing methods of measuring the change characteristics of evolving software systems. Not all changes to software systems are equal. Some changes to these systems are very small and have low impact on the system as a whole. Other changes are substantial and have a very large impact of the fault proneness of the complete system. In this study we will identify the sources of variation in the set of software metrics used to measure the system. We will then study the change characteristics to the system over a large number of builds. We have begun a new investigation in these areas in collaboration with a flight software technology development effort at the Jet Propulsion Laboratory (JPL) and have progressed in resolving the limitations of the earlier work in two distinct steps. First, we have developed a standard for the enumeration of faults. This new standard permits\u00a0\u2026", "num_citations": "44\n", "authors": ["484"]}
{"title": "Automated identification of LTL patterns in natural language requirements\n", "abstract": " Analyzing requirements for consistency and checking them for correctness can require significant effort, particularly if they have not been maintained with a requirements management tool (e.g., DOORS) or specified in a machine-readable notation. By restricting the number of requirements being analyzed, fewer opportunities exist for introducing errors into the analysis. This can be accomplished by subsetting the requirements and analyzing one subset at a time.Previous work showed that simple natural language processing and machine learning techniques can be used to identify temporal requirements within a set of natural language requirements. This paper builds on that work by detailing our results in applying these techniques to a set of natural-language temporal requirements taken from a current JPL mission and determining whether a requirement is one of the most frequently occurring types of temporal\u00a0\u2026", "num_citations": "43\n", "authors": ["484"]}
{"title": "Toward a quantifiable definition of software faults\n", "abstract": " An important aspect of developing models relating the number and type of faults in a software system to a set of structural measurement is defining what constitutes a fault. By definition, a fault is a structural imperfection in a software system that may lead to the system's eventually failing. A measurable and precise definition of what faults are makes it possible to accurately identify and count them, which in turn allows the formulation of models relating fault counts and types to other measurable attributes of a software system. Unfortunately, the most widely-used definitions are not measurable; there is no guarantee that two different individuals looking at the same set of failure reports and the same set of fault definitions will count the same number of underlying faults. The incomplete and ambiguous nature of current fault definitions adds a noise component to the inputs used in modeling fault content. If this noise\u00a0\u2026", "num_citations": "40\n", "authors": ["484"]}
{"title": "Applying software reliability engineering in the 1990s\n", "abstract": " This paper reviews the progress in software-reliability since 1975, and discusses the best tools and practices that can be applied today. The best development practices are recommended herein, for managing the reliability of software. The development of software-reliability models and user-friendly tool kits is described. These tools allow software-reliability to be measured, tracked, and improved to meet the customer-specified reliability. The ability to measure software-reliability promotes development focus, and consequently, its improvement. The 1990s have seen much growth in the software content of products. The problems have grown as the software mushrooms both in size and complexity. Further, software is increasingly important in safety critical applications. To counter these concerns, there has been a great collective effort to improve the reliability and quality of software, through better development-focus\u00a0\u2026", "num_citations": "37\n", "authors": ["484"]}
{"title": "Software faults: A quantifiable definition\n", "abstract": " An important aspect of developing models relating the number and type of faults in a software system to a set of structural measurement is defining what constitutes a fault. By definition, a fault is a structural imperfection in a software system that may lead to the system's eventually failing. A measurable and precise definition of what faults are makes it possible to accurately identify and count them, which in turn allows the formulation of models relating fault counts and types to other measurable attributes of a software system. Unfortunately, the most widely used definitions are not measurable\u2014there is no guarantee that two different individuals looking at the same set of failure reports and the same set of fault definitions will count the same number of underlying faults. The incomplete and ambiguous nature of current fault definitions adds a noise component to the inputs used in modeling fault content. If this noise\u00a0\u2026", "num_citations": "30\n", "authors": ["484"]}
{"title": "A heuristic approach for software reliability prediction: the equally-weighted linear combination model.\n", "abstract": " This paper proposes a heuristic approach to addressing the software reliability modeling problem. The heuristic approach is based on a linear combination of three popular software reliability models. A simple, predetermined combination is suggested by assigning equal weights to each component model for the final delivery of the software reliability prediction. In a preliminary examination, this Equally-Weighted Linear Combination (ELC) Model is judged to perform well when applied to three published software failure data sets. We further present five other sets of software failure data taken recently from major projects at the Jet Propulsion Laboratory, and apply the ELC model as well as six other popular models for a detailed comparison and evaluation. A number of statistical techniques are used to determine the applicability of these software reliability models. Our evaluation results indicate that the proposed ELC\u00a0\u2026", "num_citations": "30\n", "authors": ["484"]}
{"title": "Determining fault insertion rates for evolving software systems\n", "abstract": " In developing a software system, we would like to be able to estimate the way in which the fault content changes during its development, as well as determining the locations having the highest concentration of faults. In the phases prior to test, however, there may be very little direct information regarding the number and location of faults. This lack of direct information requires the development of a fault surrogate from which the number of faults and their location can be estimated. We develop a fault surrogate based on changes in relative complexity, a synthetic measure which has been successfully used as a fault surrogate in previous work. We show that changes in the relative complexity can be used to estimate the rates at which faults are inserted into a system between successive revisions. These rates can be used to continuously monitor the total number of faults inserted into a system, the residual fault content\u00a0\u2026", "num_citations": "27\n", "authors": ["484"]}
{"title": "Software system defect content prediction from development process and product characteristics\n", "abstract": " Society has become increasingly dependent on software controlled systems (eg, banking systems, nuclear power station control systems, and air traffic control systems). These systems have been growing in complexity--the number of lines of source code in the Space Shuttle, for instance, is estimated to be 10 million, and the number of lines of source code that will fly aboard Space Station Alpha has been estimated to be up to 100 million. As we become more dependent on software systems, and as they grow more complex, it becomes necessary to develop new methods to ensure that the systems perform reliably.", "num_citations": "27\n", "authors": ["484"]}
{"title": "Software evolution and the fault process\n", "abstract": " In developing a software system, we would like to estimate the way in which the fault content changes during its development, as well determine the locations having the highest concentration of faults.", "num_citations": "24\n", "authors": ["484"]}
{"title": "An experiment in determining software reliability model applicability\n", "abstract": " Most reported experience with software reliability models is from a project's testing phases, during which researchers have little control over the failure data. Since failure data can be noisy and distorted, reported procedures for determining model applicability may be incomplete. To gain additional insight into this problem, we generated forty sets of data by drawing samples from two distributions, which were used as inputs to six different software reliability models. We used several different methods to analyze the applicability of the models. We expected that a model would perform best on the data sets created to comply with the model's assumptions, but initially found that this was not always the case. More detailed examination showed that a model using a data set created to satisfy its assumptions tended to have better prequential likelihood bias, and bias trend measures, although the Kolmogorov-Smirnov test\u00a0\u2026", "num_citations": "22\n", "authors": ["484"]}
{"title": "Best current practice of sre\n", "abstract": " This chapter describes the best current practice (BCP) for doing software reliability engineering (SRE) as adopted by AT&T. It represents an integrated consensus of some 70 software project managers and engineers based on practices employed by a large number of projects within AT&T. Consequently, to maintain its integrity, we have not incorporated material from other sources. The practice of SRE provides the software engineer or manager the means to predict, estimate, and measure the rate of failure occurrences in software (including firmware). Such measures are understandable to your customer. Using SRE in the context of software engineering, you can:", "num_citations": "21\n", "authors": ["484"]}
{"title": "Classifying requirements: Towards a more rigorous analysis of natural-language specifications\n", "abstract": " Requirements play a pivotal role in planning, selection, development, testing and operation of NASA's missions. Starting from mission objectives, requirements are successively decomposed into finer detail and ultimately allocated to individual components. This decomposition process is sometimes referred to as \"requirements flowdown\" between successive levels. The correctness of this decomposition is obviously critical to mission success: if a higher level requirement is improperly decomposed into lower level requirements, the subsequent development from that stage onwards will be jeopardized. A task to determine how to improve the rigor of requirements flowdown analyses has recently been completed. Since requirements continue to be written in natural language, the task focused on examining only natural language specifications for a JPL space mission system. One particular aspect of flowdown analysis\u00a0\u2026", "num_citations": "17\n", "authors": ["484"]}
{"title": "Experiments in automated identification of ambiguous natural-language requirements\n", "abstract": " Recent research indicates that a significant proportion of the failures observed in operational space mission systems can be traced back to ambiguous requirements. Frequently, the functionality and behavior specified by the defective requirement is insufficiently detailed (eg, information about the expected operational context is missing) or the requirement is phrased so as to permit multiple interpretations. Recent analysis of several thousand anomalies observed in operational space missions shows that about 20% are due to either operator error or faulty procedures. Again, many of these anomalies can be traced back to requirements exhibiting the same types of ambiguities. Since a typical space mission is specified by several thousand requirements, a significant amount of effort may need to be expended to discover ambiguous requirements. Many development organizations still conduct this type of discovery manually, after the requirements have been specified\u2013in addition to requiring significant effort, manual analysis is also error-prone.Previous work has shown the utility of machine learners and simple natural language processing techniques (eg, parts of speech tagging) for supporting the identification of a relatively small number of temporal requirements within a large body of system requirements. We extend this work to the problem of identifying the ambiguous requirements within the complete set of requirements for the flight and ground components of a recently-launched space mission. Initial results indicate that these techniques can be effective in automatically identifying ambiguous requirements. Unlike heuristic techniques, new\u00a0\u2026", "num_citations": "16\n", "authors": ["484"]}
{"title": "An approach to the measurement of software evolution\n", "abstract": " Our current work involves developing methods of measuring changes to evolving software systems. We study a system's change characteristics over a large number of builds using the distinct sources of variation in the software metrics used to measure the system. We have been collaborating with a flight software technology development effort at the Jet Propulsion Laboratory (JPL) and have progressed in resolving the limitations of our earlier work in two distinct steps. First, we have developed a repeatable and consistent fault enumeration methodology, allowing them to be precisely and accurately measured. Second, we have developed a practical framework for automating fault measurement, which we applied to the JPL software system during its development. Every change to the system was measured and every identified fault was tracked to a specific code module. Our analysis indicates that measures of the\u00a0\u2026", "num_citations": "16\n", "authors": ["484"]}
{"title": "Fault injection experiment results in space borne parallel application programs\n", "abstract": " Development of the REE Commercial-Off-The-Shelf (COTS) based space-borne supercomputer requires a detailed knowledge of system behavior in the presence of Single Event Upset (SEU) induced faults. When combined with a hardware radiation fault model and mission environment data in a medium grained system model, experimentally obtained fault behavior data can be used to: predict system reliability, availability and performance; determine optimal fault detection methods and boundaries; and define high ROI fault tolerance strategies. The REE project has developed a fault injection suite of tools and a methodology for experimentally determining system behavior statistics in the presence of application level SEU induced transient faults. Initial characterization of science data application code for an autonomous Mars Rover geology application indicates that this code is relatively insensitive to SEUs and\u00a0\u2026", "num_citations": "15\n", "authors": ["484"]}
{"title": "A systematic and comprehensive tool for software reliability modeling and measurement\n", "abstract": " Sufficient work has been done to demonstrate that software reliability models can be used to monitor reliability growth over a useful range of software development projects. However, due to the lack of appropriate tools, the application of software reliability models as a means for project management is not as widespread as it might be. The existing software reliability modeling and measurement programs are either difficult for a nonspecialist to use, or short of a systematic and comprehensive capability in the software reliability measurement practice. To address the ease-of-use and the capability issues, the authors have prototyped a software reliability modeling tool called CASRE, a Computer-Aided Software Reliability Estimation tool. Implemented with a systematic and comprehensive procedure as its framework, CASRE will encourage more widespread use of software reliability modeling and measurement as a\u00a0\u2026", "num_citations": "13\n", "authors": ["484"]}
{"title": "Building high\u2010quality software fault predictors\n", "abstract": " Over the past several years, we have been developing software fault predictors based on a system's measured structural evolution. We have previously shown significant linear relationships between code churn, a set of synthesized metrics, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code churn. A limiting factor in this and other such investigations has been the absence of a quantitative, consistent and repeatable definition of what constitutes a fault. The rules for fault definition were not sufficiently rigorous to provide unambiguous, repeatable fault counts. Within the framework of a space mission software development effort at the Jet Propulsion Laboratory we have developed a standard for the precise enumeration of faults. This new standard permits software faults to be measured directly from configuration control documents. We compared the new method\u00a0\u2026", "num_citations": "11\n", "authors": ["484"]}
{"title": "Estimating rates of fault insertion and test effectiveness in software systems\n", "abstract": " In developing a software system, we would like to estimate the total number of faults inserted into a software system, the residual fault content of that system at any given time, and the efficacy of the testing activity in executing the code containing the newly inserted faults.", "num_citations": "11\n", "authors": ["484"]}
{"title": "Assurance of model-based fault diagnosis\n", "abstract": " Autonomy is an increasingly important technology for robotic scientific and commercial spacecraft. An important motivation for developing onboard autonomy is to enable quick response to dynamic environment and situations, including fault conditions that a spacecraft may encounter. The reliability of such autonomous capabilities depends on the quality of their knowledge of a spacecraft's health state. Model-based approaches to fault management, i.e. model-based fault diagnosis (MBFD), is one approach to continuously verify correct behavior in addition to diagnosing symptoms to estimate the spacecraft's health state. The proper functioning of MBFD is dependent on 1) the quality of the model that is analyzed and compared to the outputs of onboard sensors to estimate the system's health state, and 2) the correct functioning of the diagnosis engine that interrogates the model and compares its analysis to observed\u00a0\u2026", "num_citations": "10\n", "authors": ["484"]}
{"title": "How simple is software defect detection\n", "abstract": " Software defect detectors input structural metrics of code and output a prediction of how faulty a code module might be. Previous studies have shown that such metrics many be confused by the high correlation between metrics. To resolve this, feature subset selection (FSS) techniques such as principal components analysis can be used to reduce the dimensionality of metric sets in hopes of creating smaller and more accurate detectors. This study benchmarks several FSS techniques and reports several studies where a large set metrics were reduced to a handful with little loss of detection accuracy. This result raises the possibility that software defect detection may be much simpler than previously believed.", "num_citations": "10\n", "authors": ["484"]}
{"title": "Toward extended change types for analyzing software faults\n", "abstract": " This research extends an existing source code change taxonomy that was designed to analyze change coupling. The extension expands change types related to statements in order to achieve more granular data about the type of statement that is changed. The extended taxonomy is evaluated to determine if it can be applied to software fault analysis. We found that the extended change types occur consistently and with high frequency in fault fixes for Eclipse 2.0 and 3.0. Faults were then clustered according to the source code changes and analyzed. We found that the types and sizes of clusters are highly correlated, indicating some consistency in the patterns of the fault fixes. Finally, we performed an initial investigation to determine whether faults in the same cluster have similar characteristics. Our results indicate that many of the change types can be used to characterize the type of fault that has been fixed\u00a0\u2026", "num_citations": "9\n", "authors": ["484"]}
{"title": "IV and V Issues in Achieving High Reliability and Safety in Critical Control System Software\n", "abstract": " Risk analysis and integrated verification and validation are two important elements in a plan for ensuring the safety of critical software systems. We describe an approach we are currently developing for integrating risk analysis, and metrics analysis, and propose a fault predictor that would integrate the results of these activities. Practical difficulties associated with our approach are also discussed, as are limitations of the proposed predictor.  We conclude with a discussion of what as been learned to date, and with suggestions for future work.", "num_citations": "9\n", "authors": ["484"]}
{"title": "Failure assesment\n", "abstract": " Three questions to which software developers want accurate, precise answers are \u201cHow can the software system fail?\u201d, \u201cWhat bad things will happen if the software fails?\u201d, and \u201cHow many failures will the software experience?\u201d. In this paper we discuss several of the most prevalent and useful techniques that have been devised to answer these questions. For each technique, we present its purpose and background, describe the process of performing the technique, and evaluate it in a discussion section. We also discuss lessons learned from practice, describe available tools and resources to help the practitioner select and implement failure assessment techniques, and identify some future directions for the topic.", "num_citations": "7\n", "authors": ["484"]}
{"title": "A practical software fault measurement and estimation framework\n", "abstract": " Over the past several years, researchers have been investigating methods of estimating the number of faults inserted into a software system during its development. One technique that has been developed is based on the observation that the amount of measured structural change between subsequent versions of a software system is strongly related to the number of faults inserted between those versions. If the structural evolution of a software system can be measured during the entire implementation activity, it is possible to estimate the number of total number of faults that have been inserted. Furthermore, the number of faults remaining when the system is turned over to operations can be estimated by subtracting the number of faults actually discovered and removed during testing and other fault identification activities (eg, inspections) from the estimated total number of inserted faults. To make use of this technique\u00a0\u2026", "num_citations": "7\n", "authors": ["484"]}
{"title": "The effects of fault counting methods on fault model quality\n", "abstract": " Over the past few years, we have been developing software fault predictors based on a system's measured structural evolution. We have previously shown there is a significant linear relationship between code chum, a set of synthesized metrics, and the rate at which faults are inserted into the system in terms of number of faults per unit change in code chum. A limiting factor in this and other investigations of a similar nature has been the absence of a quantitative, consistent, and repeatable definition of what constitutes a fault. The rules for fault definition were not sufficiently rigorous to provide unambiguous, repeatable fault counts. Within the framework of a space mission software development effort at the Jet Propulsion Laboratory (JPL) we have developed a standard for the precise enumeration of faults. This new standard permits software faults to be measured directly from configuration control documents. Our\u00a0\u2026", "num_citations": "6\n", "authors": ["484"]}
{"title": "Software Fault Detection and Removal Effort-based Reliability Estimation Model\n", "abstract": " Relative importance and complexity of recent software is getting increased because the software is needed to provide considerable amount of functions and high performance. Therefore, developing reliable software is importantly issued. In order to develop reliable software, it is necessary to manage software reliability at the early phases, but most reliability estimation models are used at system or operational test phases. In order to develop highly reliable software, it is necessary to manage software reliability at the early test phases based on characteristic of the phases that is developers and testers are not separated and developers perform test and debug activities together. Therefore, a new reliability estimation model considering test and debug time together is necessarily needed. In this paper, we propose a new reliability estimation model to manage reliability of individual units from the early test phases and in order to show how to fit the model to actual data and usefulness, we collected industrial data and used it for the experiment.", "num_citations": "5\n", "authors": ["484"]}
{"title": "Predicting deviations in software quality by using relative critical value deviation metrics\n", "abstract": " We develop a new metric, relative critical value deviation (RCVD), for classifying and predicting software quality. The RCVD is based on the concept that the extent to which a metric's value deviates from its critical value, normalized by the scale of the metric, indicates the degree to which the item being measured does not conform to a specified norm. For example, the deviation in body temperature above 98.6 Fahrenheit degrees is a surrogate for fever. Similarly, the RCVD is a surrogate for the extent to which the quality of software deviates from acceptable norms (e.g., zero discrepancy reports). Early in development, surrogate metrics are needed to make predictions of quality before quality data are available. The RCVD can be computed for a single metric or multiple metrics. Its application is in assessing newly developed modules by their quality in the absence of quality data. The RCVD is a part of the larger\u00a0\u2026", "num_citations": "5\n", "authors": ["484"]}
{"title": "Demonstrations of system-level autonomy for spacecraft\n", "abstract": " System-level autonomy refers to autonomously meeting the crosscutting needs of a system through awareness and coordinated control spanning the system's breadth of capabilities. In contrast to function-level autonomy, which focuses on capabilities required to achieve a specific function such as surface navigation or image recognition, system-level autonomy addresses the needs to coordinate and manage activities and resources, and estimate the state, across subsystems. This paper describes demonstrations that were conducted on a spacecraft workstation testbed. The autonomy was provided by system-level planning and execution integrated with system-level estimators of orbit knowledge and spacecraft hardware health. These components are embedded in a system-level framework defining how goals are formed and executed, which elements exist, and how control authority is distributed among\u00a0\u2026", "num_citations": "4\n", "authors": ["484"]}
{"title": "Automated specification-based test case generation using SCR\n", "abstract": " Automated Specification-Based Test Case Generation Using SCR Page 1 JPL IT Symposium - Automated Specification-Based Test Case Generation Using SCR n and Software Systems Automated Specification-Based Test Case Generation Using SCR JPL IT Symposium November 4,2002 Allen Nikora Constance L. Heitmeyer Quality Assurance Section Jet Propulsion Laboratory Head, Software Engineering Section Naval Research Laboratory The work described in this paper was carried out at the Jet Propulsion Laboratory, California Institute of Technology. This work was sponsored by the Software Engineering Technology element of JPL\u2019s Center for Space Mission Information and Software Systems. Date: 4 November, 2002 1 Software Engineering Technology Page 2 JPL IT Symposium - Automated Specification-Based Test Case Generation Using SCR. n and Software Systems A utom ated Specification-Test 4,\u2026", "num_citations": "4\n", "authors": ["484"]}
{"title": "Computer Aided Software Reliability Estimation\n", "abstract": " CASRE (Computer Aided Software Reliability Estimation) is a software reliability measurement tool that runs in the Microsoft Windows TM environment. Although there are several software reliability measurement tools currently available, CASRE differs from these in the following important respects:1. Plots of the failure data used as inputs to the models are displayed simultaneously with the text. Failure data text is shown in one window, while plots are shown in another. Changes made to the text of the failure data are automatically reflected in the displayed plots.", "num_citations": "4\n", "authors": ["484"]}
{"title": "Issues and Methods for Assessing COTS Reliability, Maintainability, and Availability\n", "abstract": " Many vendors produce products that are not domain specific (eg, network server) and have limited functionality (eg, mobile phone). In contrast, many customers of COTS develop systems that am domain specific (eg, target tracking system) and have great variability in functionality (eg, corporate information system). This discussion takes the viewpoint of how the customer can ensure the quality of COTS components. In evaluating the benefits and costs of using COTS, we must consider the environment in which COTS will operate. Thus we must distinguish between using a non-mission critical application like a spreadsheet program to produce a budget and a mission critical application like military strategic and tactical operations. Whereas customers will tolerate an occasional bug in the former, zero tolerance is the rule in the latter. We emphasize the latter because this is the arena where there are major unresolved problems in the application of COTS. Furthermore, COTS components may be embedded in the larger customer system. We refer to these as embedded systems. These components must be reliable, maintainable, and available, and must be with the larger system in order for the customer to benefit from the advertised advantages of lower development and maintenance costs. Interestingly, when the claims of COTS advantages are closely examined, one finds that to a great extent these COTS components consist of hardware and office products, not mission critical software [1]. Obviously, COTS components are different from custom components with respect to one or more of the following attributes: source, development paradigm, safety\u00a0\u2026", "num_citations": "4\n", "authors": ["484"]}
{"title": "On-board model based fault diagnosis for cubesat attitude control subsystem: Flight data results\n", "abstract": " Self-sufficient, robotic spacecraft require estimates of their hardware health state in order to project future system state and plan actions toward achieving mission goals. In this paper, we report on integration of a Model-Based Fault Diagnosis (MBFD) model and reasoning engine into flight software leveraging the Arcsecond Space Telescope Enabling Research in Astrophysics (ASTERIA) mission, including test results against captured flight data using the ASTERIA system testbed. Our effort integrated the Model-based Off-Nominal State Identification and Detection (MONSID) model-based reasoning system, developed by Okean Solutions, into ASTERIA flight software using the F Prime software framework. The MONSID engine was supplied with a model of the Blue Canyon Technologies XACT attitude control system (ACS) and tested against flight data and seeded fault tests. While we were unable to conduct an on\u00a0\u2026", "num_citations": "3\n", "authors": ["484"]}
{"title": "Improving the Accuracy of Space Mission Software Anomaly Frequency Estimates\n", "abstract": " Anomaly data can be used to estimate baseline values for operational mission software anomaly frequencies; these estimates can be used for future missions to determine whether software reliability is improving. The accuracy of anomaly frequency estimates can be affected by characteristics of the anomaly data and the problem reporting system maintaining that data. We have been using text mining and machine learning techniques to address one of these issues, in which the number of software-related anomalies is incorrectly reported because the problem reporting system does not tag them correctly. Results to date indicate that these techniques may substantially increase the accuracy of anomaly frequency estimates.", "num_citations": "3\n", "authors": ["484"]}
{"title": "\u2018A Linear Combination Software Reliability Modeling Tool with A Graphically-Oriented User Interface\n", "abstract": " In our recent work, we have shown that forming linear combination of model results tends to yield more accurate predictions of software reliability. Using linear combinations also simplifies the practitioner's task of deciding which model or models to apply to a particular development effort. Currently, no commercially available tools permit such combinations to be formed within the environment provided by the tool. Most software reliability modeling tools also do not take advantage of the high-resolution displays available today. Performing actions within the tool may be awkward, and the output of the tools may be understandable only to a specialist. We propose a software reliability modeling tool that allows users to formulate linear combination models, that can be operated by non-specialists, and that produces results in a form undetstandable by software developers and managements", "num_citations": "3\n", "authors": ["484"]}
{"title": "Software reliability and security: challenges and crosscutting themes\n", "abstract": " Security has emerged as one of the most significant challenges to organizations that develop software system. As software systems involvement in managing financial systems, infrastructure, and industrial systems increases, the potential consequences of unauthorized access to those systems becomes more and more severe. We examine relationships between software reliability engineering and cybersecurity to develop more effective ways of assessing and improving system security.", "num_citations": "2\n", "authors": ["484"]}
{"title": "Developing formal correctness properties from natural language requirements\n", "abstract": " \u25c6 Map each word in requirement to unique numerical ID\u25c6 Map each POS tag to unique numerical ID\u25c6 Concatenate requirements text word list, POS tag list\u25aa Use only first 200 elements of word list, POS tag list\u25c6 Apply supervised discretization1\u25aa Many classifiers require discrete data input", "num_citations": "2\n", "authors": ["484"]}
{"title": "Extending the use of measurement\n", "abstract": " Purpose-develop a method for predicting software reliability in the life cycle phases prior to test. Acceptable model forms were:-measures leading directly to reliability/failure-predictions that could be translated to failure rate predictions rates (ems., error density) Details given in [McCa187]", "num_citations": "2\n", "authors": ["484"]}
{"title": "Inserting software fault measurement techniques into development efforts\n", "abstract": " Over the past several years, techniques for estimating software fault content based on measurements of a system's structural evolution during its implementation have been developed. Proper application of the techniques will yield a detailed map of the faults that have been inserted into the system. This information can be used by development organizations to better control the number of residual faults in the operational system. There are several issues that must be resolved if these techniques are to be successfully inserted into a development effort. These issues are identified, as are possibilities for their resolution.", "num_citations": "2\n", "authors": ["484"]}
{"title": "Practical issues in implementing software reliability measurement\n", "abstract": " Many ways of estimating software systems' reliability, or reliability-related quantities, have been developed over the past several years.", "num_citations": "2\n", "authors": ["484"]}
{"title": "Practical issues in estimating fault content and location in software systems\n", "abstract": " ~ _ ures of Relative Critical value Deviation (RCVD) can be used in classifying the quality of software during theOver the past several years, techniques have been quality control and prediction process. 4* 5 Using failure developed to discriminate between, fault-prone software data from the Space Transportation System Primary modules and those that are not, and to estimate a soft- Avionics Software System (STS PASS), these functions", "num_citations": "2\n", "authors": ["484"]}
{"title": "Issues and Methods for Assessing COTS Reliability, Maintainability, and Availability\n", "abstract": " This discussion takes the viewpoint of how the customer can ensure the quality of COTS components.", "num_citations": "2\n", "authors": ["484"]}
{"title": "Software Reliability Measurement Experience\n", "abstract": " The key components in the SRE process, as described in Chap. 6, include reliability objective specification, operational profile determination, reliability modeling and measurement, and reliability validation. These techniques were applied to several internal projects developed within Jet Propulsion Laboratory (JPL) and Bell Communications Research (Bellcore). The project background, reliability engineering procedures, data collection efforts, modeling results, data analyses, and reliability measurements for these projects are presented in this chapter. Model comparisons for the software reliability applications, lessons learned with regard to the engineering effort, and directions for current and future software reliability investigations are also provided. One major thing we observed is that for the failure data we analyzed, no one model was consistently the best. It was frequently the case that a model that had\u00a0\u2026", "num_citations": "2\n", "authors": ["484"]}
{"title": "The software reliability engineering process\n", "abstract": " Ted Keller Loral, Inc. John D. Musa AT&T Bell Labs Allen P. Nikora Jet Propulsion Laboratory This subgroup will define a process infrastructure for what reliability activities need to be performed by what people over the life cycle of a software product. The process infrastructure will not only address what a software engineer needs to do to ensure the reliability of software products but will also stipulate what roles others involved in the software development process play in ensuring software reliability. This process description will be used by the Standards Integration Subgroup to recommend which standards should be applied during each activity and identify gaps where standards are needed. Likewise, the process infrastructure will help the Measurements Subgroup identify what measures and models are needed to support specified software development activities.", "num_citations": "2\n", "authors": ["484"]}
{"title": "Assuring Correctness, Completeness, and Performance for Model-Based Fault Diagnosis Systems\n", "abstract": " The robotic scientific and commercial spacecraft industries are currently trending towards the development of onboard autonomous capabilities for responding quickly to dynamic environments and rapidly changing situations. Model-based fault diagnosis (MBFD) is an approach to estimating a spacecraft's health state by continuously verifying accurate behavior and diagnosing off-nominal behavior. Proper functioning of MBFD depends on 1) the quality of the diagnostic system model that is analyzed and compared to commands and onboard measurements to estimate a system's health state, and 2) the correct functionality of the diagnosis engine interrogating the model and comparing its analyses to observed system behavior. Our goal is to develop Verification and Validation (V&V) techniques for a MBFD system to provide the necessary confidence in its ability to estimate the health of on-board spacecraft\u00a0\u2026", "num_citations": "1\n", "authors": ["484"]}
{"title": "SFRAT\u2013An Extendable Software Reliability Assessment Tool\n", "abstract": " Since its release in the mid-1990s, the Microsoft Windows-based software reliability modeling tool CASRE has been downloaded over 3000 times from the Open Channel Foundation's website. It was also included on the CDROM distributed with the Handbook of Software Reliability Engineering (M. Lyu, ed). In the years since it was first released, however, CASRE has become more difficult to use. This is mainly because there have been no updates since 2000. The last version of Windows on which CASRE would reliably execute was Windows XP, and since it was developed explicitly for Windows, it is not feasible to run it on other platforms. Software development and acquisition organizations continued to be interested in using tools of the same type as CASRE. In 2013, the U.S. Naval Air Systems Command (NAVAIR) contacted the authors at the Jet Propulsion Laboratory and the University of Massachusetts to\u00a0\u2026", "num_citations": "1\n", "authors": ["484"]}
{"title": "Applying software reliability growth models to DOD systems\n", "abstract": " Presents a collection of slides covering the following topics: software development; software failure; software quality; and safety-critical software.", "num_citations": "1\n", "authors": ["484"]}
{"title": "Predicting fault content for evolving software systems\n", "abstract": " Untitled Page 1 Page 2 Outline JPL California Institute of Technology 2 Page 3 r relationships between: er of faults repaired. ftware fault models stem\u2019s structural evolution. consistent, repeatable measurement of stt of what constitutes a fault. Its at same leve ted, minimal or no visible impact on a of California Institute of Technology +a1 3 Page 4 S 0 0 > W CZI c) cn m > Y .I > 0 cd .I CI Page 5 JPL California Institute of Technology :yla03] 5 Page 6 6 ' I Page 7 Page 8 Page 9 Page 10 f\u2019 JPL California Institute of Technology Identifying and Counting Faults urate software fault prediction depends on e, measurable definition of a fault ing definition of fault in measurable terms Std 729-1 983, \u201cIEEE Standard Glossary oftware Engineering Terminology\u201d [lEEE83] Std 982.1-1 988, \u201cIEEE Standard td 1044-1 993, \u201cIEEE Standard cation for Software Anomalies\u201d [IEEE99] to Produce Reliable gon Workshop on \u20191 1-1 3, 1997) [Niko97] v\u2026", "num_citations": "1\n", "authors": ["484"]}
{"title": "A JPL Software Reliability study and a Windows-Based Software Reliability Tool\n", "abstract": " Recently, a study was undertaken at the Jet Propulsion Laboratory to determine the applicability of current software reliability measurement techniques to JPI, development efforts. One of\u2019the findings was that there is no known method for identifying the model. most applicable to a development effort prior to test [3]. This can be mitigated, however, by executing several models at once and using statistical methods to identify the model that is most likely to produce accurate results [1, 2]. The results of the study has also shown that more accurate predictions can be produced by combining the results of several models in a linear fashion [3] or by recalibrating a model based on its predictive bias [4].During the study, several of the currently available tools [5] were used. We found that many of them were difficult. to use.! l\u2019he outputs of many tools are presented in tabular rather than graphic form, and include quantities such as model parameter estimates which would not be understood by most software developers and managers. Most of the tools also had limited graphics capability, producing a limited variety of character-based plots. Finally, operating some the tools was more difficult than it might be, since many of the tools had command-line or batch file command interfaces rather than pull-down menus or direct manipulation.", "num_citations": "1\n", "authors": ["484"]}