{"title": "Software testing\n", "abstract": " Software Testing | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSoftware Testing ABSTRACT No abstract available. Index Terms (auto-classified) 1.Software Testing 1.General and reference 1.Cross-computing tools and techniques 1.Verification 2.Mathematics of computing 1.Mathematical software 3.Social and professional topics 1.Professional topics 1.Management of computing and information systems 1.System management 1.Quality assurance 4.Software and its engineering 1.Software creation and management 1.Software verification and validation 1.Software defect analysis 1.Software testing and \u2026", "num_citations": "230\n", "authors": ["547"]}
{"title": "Evaluating inheritance depth on the maintainability of object-oriented software\n", "abstract": " This empirical research was undertaken as part of a multi-method programme of research to investigate unsupported claims made of object-oriented technology. A series of subject-based laboratory experiments, including an internal replication, tested the effect of inheritance depth on the maintainability of object-oriented software. Subjects were timed performing identical maintenance tasks on object-oriented software with a hierarchy of three levels of inheritance depth and equivalent object-based software with no inheritance. This was then replicated with more experienced subjects. In a second experiment of similar design, subjects were timed performing identical maintenance tasks on object-oriented software with a hierarchy of five levels of inheritance depth and the equivalent object-based software.               The collected data showed that subjects maintaining object-oriented software with three levels\u00a0\u2026", "num_citations": "209\n", "authors": ["547"]}
{"title": "Further experiences with scenarios and checklists\n", "abstract": " Software inspection is one of the best methods of verifying software documents. Software inspection is a complex process, with many possible variations, most of which have received little or no evaluation. This paper reports on the evaluation of one component of the inspection process, detection aids, specifically using Scenario or Checklist approaches. The evaluation is by subject-based experimentation, and is currently one of three independent experiments on the same hypothesis. The paper describes the experimental process, the resulting analysis of the experimental data, and attempts to compare the results in this experiment with the other experiments. This replication is broadly supportive of the results from the original experiment, namely, that the Scenario approach is superior to the Checklist approach; and that the meeting component of a software inspection is not an effective defect detection\u00a0\u2026", "num_citations": "176\n", "authors": ["547"]}
{"title": "Comparing and combining software defect detection techniques: a replicated empirical study\n", "abstract": " This report describes an empirical study comparing three defect detection techniques: a) code reading by stepwise abstraction, b) functional testing using equivalence partitioning and boundary value analysis, and c) structural testing using branch coverage. It is a replication of a study that has been carried out at least four times previously over the last 20 years. This study used 47 student subjects to apply the techniques to small C programs in a fractional factorial experimental design. The major findings of the study are: a) that the individual techniques are of broadly similar effectiveness in terms of observing failures and finding faults, b) that the relative effectiveness of the techniques depends on the nature of the program and its faults, c) these techniques are consistently much more effective when used in combination with each other. These results contribute to a growing body of empirical evidence that\u00a0\u2026", "num_citations": "151\n", "authors": ["547"]}
{"title": "Machine learning for estimation of building energy consumption and performance: a review\n", "abstract": " Ever growing population and progressive municipal business demands for constructing new buildings are known as the foremost contributor to greenhouse gasses. Therefore, improvement of energy efficiency of the building sector has become an essential target to reduce the amount of gas emission as well as fossil fuel consumption. One most effective approach to reducing CO2 emission and energy consumption with regards to new buildings is to consider energy efficiency at a very early design stage. On the other hand,efficient energy management and smart refurbishments can enhance energy performance of the existing stock. All these solutions entail accurate energy prediction for optimal decision making. In recent years, artificial intelligence (AI) in general and machine learning (ML) techniques in specific terms have been proposed for forecasting of building energy consumption and performance. This paper\u00a0\u2026", "num_citations": "139\n", "authors": ["547"]}
{"title": "Investigating the viability of mental models held by novice programmers\n", "abstract": " This paper describes an investigation into the viability of mental models used by novice programmers at the end of a first year Java programming course. The qualitative findings identify the range of mental models of value and reference assignment held by the participants. The quantitative analysis reveals that approximately one third of students held non-viable mental models of value assignment and only 17% of students held a viable mental model of reference assignment. Further, in terms of a comparison between the participants' mental models and their performance in in-course assessments and final examination, it was found that students with viable mental models performed significantly better than those with non-viable models. These findings are used to propose a more\" constructivist\" approach to teaching programming based on the integration of\" cognitive conflict\" and program visualisation.", "num_citations": "121\n", "authors": ["547"]}
{"title": "Multi-method research: An empirical investigation of object-oriented technology\n", "abstract": " There is a general acceptance that software engineering research should be supported by empirical evaluation. To make real progress researchers must address the difficulties caused by the human-intensive nature of software development as well as experimental validity. This paper proposes the use of multi-method empirical research programs, as an alternative to `single-shot' empirical studies, to help address these problems.The multi-method approach is based on the combination of complementary empirical research methods. The intention is that the complementary nature of the research methods compensate for weaknesses inherent in individual methods. It is argued that the multi-method approach potentially provides benefits in terms of more robust conclusions, development and investigation of research hypotheses in an evolutionary manner, and increased understanding of research results.This paper\u00a0\u2026", "num_citations": "115\n", "authors": ["547"]}
{"title": "Object-oriented inspection in the face of delocalisation\n", "abstract": " Software inspection is now widely accepted as an effective technique for defect detection. This acceptance is largely based on studies using procedural program code. This paper presents empirical evidence that raises significant questions about the application of inspection to object-oriented code.", "num_citations": "111\n", "authors": ["547"]}
{"title": "Replication of experimental results in software engineering\n", "abstract": " Carrying out empirical studies is widely held to be of importance. A view less widely held is that experiments should be replicated externally (ie by di erent researchers) to both verify and validate the original results.This paper serves two main functions. First, the need for external replications is established. The role of replication in experimental software engineering is discussed. Without the con rming power of external replications, results in experimental software engineering should only be provisionally accepted, if at all. An extension to the framework for experimentation in software engineering by Basili et al 5] is proposed to di erentiate between the various kinds of internal and external replication and their powers of con rmation and to allow a better appreciation of the context of a piece of empirical work. Second, this paper presents a concrete example of an external replication of an experiment which tested the bene ts to maintenance of using modular code against non-modular (monolithic) code. The results of the original experiment by Korson 32, 33] showed that a modular program could be maintained signi cantly faster than an equivalent monolithic version of the same program under the condition that modularity has been used to implement information hiding which localizes changes required by a modi cation. The results of our replication, however, were strikingly di erent from those of the original and showed no signi cant di erence between the average times taken to maintain modular and monolithic code. An inductive analysis was undertaken to investigate the reasons for this di erence. Evidence was uncovered suggesting an ability e\u00a0\u2026", "num_citations": "111\n", "authors": ["547"]}
{"title": "A novel software visualisation model to support software comprehension\n", "abstract": " Current software visualisation tools do not address the full range of software comprehension requirements. This paper proposes a novel software visualisation model for supporting object-oriented software comprehension that is intended to address the shortcomings of existing tools. We discuss the previous work that prompted us to develop this model. An initial model is then presented, based on multiple levels of abstraction, multiple perspectives of the software system, and the integration of statically and dynamically extracted information. We review the evaluation tasks used in our previous work and those from the software visualisation and comprehension literature to produce a refined set of evaluation tasks. We then use these tasks to perform an initial assessment of the proposed model. The refined model is then defined more formally. Finally, a concrete example of the use of the model to generate abstraction\u00a0\u2026", "num_citations": "107\n", "authors": ["547"]}
{"title": "The Java system dependence graph\n", "abstract": " The program dependence graph was introduced by Ottenstein and Ottenstein in 1984. It was suggested to be a suitable internal program representation for monolithic programs, for the purpose of carrying out certain software engineering operations such as slicing and the computation of program metrics. Since then, Horwitz et al. have introduced the multiprocedural equivalent system dependence graph. Several authors have proposed object-oriented dependence graph construction approaches. Every approach provides its own benefits, some of which are language specific. We present a Java system dependence graph which draws on the strengths of a range of earlier works and adapts them, if necessary, to the Java language. It also provides guidance on the construction of the graph, identifies potential research topics based on it and shows a completed graph with a slice highlighted for a small, but realistic\u00a0\u2026", "num_citations": "103\n", "authors": ["547"]}
{"title": "Investigating and improving the models of programming concepts held by novice programmers\n", "abstract": " The teaching of introductory computer programming seems far from successful, with many first-year students performing more poorly than expected. One possible reason for this is that novices hold \u2018non-viable\u2019 mental models (internal explanations of how something works) of key programming concepts which then cause misconceptions and difficulties. An initial study investigated the apparent viability of novices' models of fundamental programming concepts, focusing on value and reference assignment. This revealed that many students appeared to hold \u2018non-viable\u2019 mental models of these key concepts and that those students who appeared to hold viable mental models performed significantly better in programming tasks than those with non-viable models. To address this, a teaching model integrating cognitive conflict and program visualisation is proposed. A series of studies found that this teaching model is\u00a0\u2026", "num_citations": "97\n", "authors": ["547"]}
{"title": "Statistical power and its subcomponents\u2014missing and misunderstood concepts in empirical software engineering research\n", "abstract": " Recently we have witnessed a welcomed increase in the amount of empirical evaluation of Software Engineering methods and concepts. It is hoped that this increase will lead to establishing Software Engineering as a well-defined subject with a sound scientifically proven underpinning rather than a topic based upon unsubstantiated theories and personal belief. For this to happen the empirical work must be of the highest standard. Unfortunately producing meaningful empirical evaluations is a highly hazardous activity, full of uncertainties and often unseen difficulties. Any researcher can overlook or neglect a seemingly innocuous factor, which in fact invalidates all of the work. More serious is that large sections of the community can overlook essential experimental design guidelines, which bring into question the validity of much of the work undertaken to date.In this paper, the authors address one such factor\u00a0\u2026", "num_citations": "95\n", "authors": ["547"]}
{"title": "The development and evaluation of three diverse techniques for object-oriented code inspection\n", "abstract": " We describe the development and evaluation of a rigorous approach aimed at the effective and efficient inspection of object-oriented (OO) code. Since the time that inspections were developed they have been shown to be powerful defect detection strategies. However, little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. This suggests that inspection techniques may not be currently being deployed to their best effect in the context of large-scale OO systems. Work to date has revealed three significant issues that need to be addressed - the identification of chunks of code to be inspected, the order in which the code is read, and the resolution of frequent nonlocal references. Three techniques are developed with the aim of addressing these issues: one based on a checklist, one focused on constructing\u00a0\u2026", "num_citations": "90\n", "authors": ["547"]}
{"title": "Replication's role in software engineering\n", "abstract": " We provide motivation for researchers to replicate experiments in software engineering. The ideology of replication is discussed. We address the question: Is an experiment worth repeating? The current lack of replication studies is highlighted. We make clear that exact replication is unattainable and we draw on our first experience of performing an external replication. To categorise various kinds of replication, we propose a simple extension to Basili et al.\u2019s framework for experimentation in software engineering. We present guidance as to the level of reported detail required to enable others perform a replication. Our conclusion is that there is only one route for empirical software engineering to follow: to make available laboratory packages of experimental materials to facilitate internal and external replications, especially the latter, which have greater confirming power.", "num_citations": "82\n", "authors": ["547"]}
{"title": "A review of awareness in distributed collaborative software engineering\n", "abstract": " Software development projects are inherently cooperative, requiring many developers to coordinate their efforts to produce complex systems. An integral part of this process is the development and maintenance of a shared understanding concerning the state of software project tasks, artefacts, and the activities and expertise of developers working on the project. In co\u2010located settings, this vital awareness information that concerns developers, either directly or tangentially, is typically acquired using formal, regular meetings, informal personal interruptions and electronic communication. Developing and maintaining such an awareness are far more difficult in distributed teams where developers are separated by time and space. The goal of this review is to help bridge the gap between the theoretical benefits of distributed collaborative software development and the practical impediments to successful implementation\u00a0\u2026", "num_citations": "80\n", "authors": ["547"]}
{"title": "A comparative evaluation of dynamic visualisation tools\n", "abstract": " Despite their potential applications in software comprehension, it appears that dynamic visualisation tools are seldom used outside the research laboratory. This paper presents an empirical evaluation of five dynamic visualisation tools - AVID, Jinsight, jRMTool, Together ControlCenter diagrams and Together ControlCenter debugger. The tools were evaluated on a number of general software comprehension and specific reverse engineering tasks using the HotDraw objectoriented framework. The tasks considered typical comprehension issues, including identification of software structure and behaviour, design pattern extraction, extensibility potential, maintenance issues, functionality location, and runtime load. The results revealed that the level of abstraction employed by a tool affects its success in different tasks, and that tools were more successful in addressing specific reverse engineering tasks than general software comprehension activities. It was found that no one tool performs well in all tasks, and some tasks were beyond the capabilities of all five tools. This paper concludes with suggestions for improving the efficacy of such tools.", "num_citations": "76\n", "authors": ["547"]}
{"title": "An empirical evaluation of defect detection techniques\n", "abstract": " This report describes an empirical study comparing three defect detection techniques: (a) code reading by stepwise abstraction, (b) functional testing using equivalence partitioning and boundary value analysis, and (c) structural testing using branch coverage. It is a replication of a study that has been carried out at least four times previously over the last 20 years. This study used 47 student subjects to apply the techniques to small C programs in a fractional factorial experimental design. The major findings of the study are: (a) that the individual techniques are of broadly similar effectiveness in terms of observing failures and finding faults, (b) that the relative effectiveness of the techniques depends on the nature of the program and its faults, (c) these techniques are consistently much more effective when used in combination with each other. These results contribute to a growing body of empirical evidence that\u00a0\u2026", "num_citations": "75\n", "authors": ["547"]}
{"title": "A review of tool support for software inspection\n", "abstract": " Inspection is widely believed to be the most cost-effective method for detecting defects in documents produced during the software development lifecycle. However, it is by its very nature a labour intensive process. This has led to work on computer support for the process which should increase the efficiency and effectiveness beyond what is currently possible with a solely manual process. In this paper we describe the scope for tool support for the inspection process and review currently available products. We conclude that no single tool available fills all the identified needs of inspection.< >", "num_citations": "72\n", "authors": ["547"]}
{"title": "Practical code inspection techniques for object-oriented systems: an experimental comparison\n", "abstract": " Although inspection is established as an effective mechanism for detecting defects in procedural systems, object-oriented systems have different structural and execution models. This article describes the development and empirical investigation of three different techniques for reading OO code during inspection.", "num_citations": "61\n", "authors": ["547"]}
{"title": "Further investigations into the development and evaluation of reading techniques for object-oriented code inspection\n", "abstract": " This paper describes the development and experimental evaluation of a rigorous approach for effective object-oriented (OO) code inspection. Since their development, inspections have been shown to be powerful defect detection strategies but little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. Previous investigations have demonstrated that the delocalised nature of OO software-the resolution of frequent non-local references, and the incongruous relationship between its static and dynamic representations, are primary inhibitors to its effective inspection. The experiment investigates a set of three complementary code reading techniques devised specifically to address these problems: one based on a checklist adapted to address the identified problems of OO inspections, one focused on the\u00a0\u2026", "num_citations": "61\n", "authors": ["547"]}
{"title": "The effect of inheritance on the maintainability of object-oriented software: an empirical study\n", "abstract": " The empirical study was undertaken as part of a programme of research to explore unsupported claims about the object-oriented paradigm: a series of experiments tested the effect of inheritance on the maintainability of object-oriented software. Subjects were asked to modify object-oriented software with a hierarchy of 3 levels of inheritance depth and equivalent object-based software with no inheritance. The collected timing data showed that subjects maintaining object-oriented software using inheritance performed the modification tasks, on average, approximately 20% quicker than those maintaining equivalent object-based software with no inheritance. An initial inductive analysis revealed that 2 out of 3 subjects performed faster when maintaining the object-oriented software with inheritance. The findings are sufficiently important that attempts to verify the results should be made by independent researchers\u00a0\u2026", "num_citations": "61\n", "authors": ["547"]}
{"title": "Identifying and addressing problems in object-oriented framework reuse\n", "abstract": " This paper describes the results of a long-term empirical investigation into object-oriented framework reuse. The aim is to identify the major problems that occur during framework reuse and the impact of current documentation techniques on these problems. Four major reuse problems are identified: understanding the functionality of framework components; understanding the interactions between framework components; understanding the mapping from the problem domain to the framework implementation; understanding the architectural assumptions in the framework design. Two forms of documentation are identified as having the potential to address these problems, namely pattern languages and micro-architecture descriptions. An in-depth, qualitative analysis suggests that, although pattern languages do provide useful support in terms of introducing framework concepts, this can be bypassed by\u00a0\u2026", "num_citations": "60\n", "authors": ["547"]}
{"title": "A comparative evaluation of program comprehension measures\n", "abstract": " This paper describes an experiment to evaluate measures of program comprehension. The wide variety of approaches to measuring comprehension means that it is difficult to compare measures and have confidence in the reliability and accuracy of measures. Following a review of comprehension measures employed in a number of empirical studies, four essential measures-maintenance, mental simulation, static and subjective (self-ranking)-are identified. These measures are then evaluated using a group of 157 novice programmers. The results indicate that the measures based on mental simulation are the most reliable, followed by maintenance based tasks. Static tasks appear to be notoriously unreliable, and subjective measures are cheap and worth using (along with another measure). Advice is also given on the creation of questions to measure comprehension.", "num_citations": "55\n", "authors": ["547"]}
{"title": "Genetic algorithms and the automatic generation of test data\n", "abstract": " Although it is well understood to be a generally undecidable problem, a number of attempts have been made over the years to develop systems to automatically generate test data to achieve a level of coverage (branch coverage for example). These approaches have ranged at early attempts at symbolic execution to more recent dynamic approaches and, despite their variety (and varying degrees of success), all the systems developed have involved a detailed analysis of the program or system under test. In a departure from this approach, this paper describes a system developed to explore the use of genetic algorithms to generate test data to automatically meet a level of coverage. Genetic algorithms are commonly applied to search problems within AI. They maintain a population of structures that evolve according to rules of selection, mutation and reproduction. Each individual in the environment receives a measure of its tness in the environment. Reproduction selects individuals with high tness values in the population, and through crossover and mutation of such individuals, a new population is derived from which individuals may be even better tted to their environment. Translating these concepts to the problem of test data generation, the population is the set of test data, each element in the set (eg a group of data items used in one run of the program) is an individual, and the tness of an individual corresponds to the coverage it achieves of the program under test.A system has been developed to support this process. It takes the program to be tested (currently in C) and instruments it with probes to provide feedback on the coverage achieved\u00a0\u2026", "num_citations": "55\n", "authors": ["547"]}
{"title": "Computer aided software testing using genetic algorithms\n", "abstract": " Although it is well understood to be a generally undecidable problem, a number of attempts have been made over the years to develop systems to automatically generate test data. These approaches have ranged from early attempts at symbolic execution to more recent approaches based on, for example, dynamic data flow analysis or constraint satisfaction. Despite their variety (and varying degrees of success), all the systems developed have involved a detailed analysis of the program or system under test and have encountered problems (such as handling of procedure calls, efficiently finding solutions to systems of predicates and dealing with problems of scale) which have hindered their progress from research prototype to commercial tool. The approach described in this paper uses the ideas of Genetic Algorithms (GAs) to automatically develop a set of test data to achieve a level of coverage (branch coverage in this case). Using GAs neatly sidesteps many of the problems encountered by other systems in attempting to automatically generate test data.", "num_citations": "53\n", "authors": ["547"]}
{"title": "Systematic object-oriented inspection-an empirical study\n", "abstract": " Software inspection is recognised as an effective defect detection technique, but research has suggested that its performance on object-oriented code may suffer as a result of the delocalised nature of the software. This leads to problems of how to segment a system into chunks, what reading strategy should be adopted to read those chunks, and how to make available necessary non-local information. This paper presents the results of an empirical investigation that compared a systematic, abstraction-driven inspection reading technique with an ad-hoc approach in an attempt to investigate these issues. The analysis shows that using the systematic technique does not significantly improve an inspector's overall defect detection performance. The systematic technique does, however, seem to have potential to help address delocalisation through the creation of abstract specifications, encourage a deeper understanding\u00a0\u2026", "num_citations": "48\n", "authors": ["547"]}
{"title": "Using bug report similarity to enhance bug localisation\n", "abstract": " Bug localisation techniques are proposed as a method to reduce the time developers spend on maintenance, allowing them to quickly find source code relevant to a bug. Some techniques are based on information retrieval methods, treating the source code as a corpus and the bug report as a query. While these have shown success, there remain a number of little-exploited additional sources of information which could enhance the techniques, including the textual similarity between bug reports themselves. Based on successful results in detecting duplicate bug reports, this work asks: if duplicate bugs reports, which by definition are fixed in the same source location, can be detected through the use of similar language, can bugs which are in the same location but not duplicates be detected in the same way? A technique using this information is implemented and evaluated on 372 bugs across 4 projects, and is\u00a0\u2026", "num_citations": "46\n", "authors": ["547"]}
{"title": "The role of comprehension in software inspection\n", "abstract": " In spite of code inspections having been demonstrated as an effective defect detection process, little work has been done to determine how this process best supports the object-oriented paradigm. In contrast, this paradigm (or at least its questionable manifestation in C++) is well supported by tools that purport to aid comprehension. These tools typically take the form of visualisation tools designed to assist in the maintenance process, and it is natural to consider that these tools (or adaptations thereof) might also support inspection. However, since these tools claim to aid comprehension, it is important to consider the role of comprehension in inspection. Or put simply, does comprehension matter, or are there simple techniques in existence which are similarly effective in detecting defects? This paper presents the issues associated with inspections (and the complications presented by the object-oriented paradigm\u00a0\u2026", "num_citations": "42\n", "authors": ["547"]}
{"title": "Using developer activity data to enhance awareness during collaborative software development\n", "abstract": " Software development is a global activity unconstrained by the bounds of time and space. A major effect of this increasing scale and distribution is that the shared understanding that developers previously acquired by formal and informal face-to-face meetings is difficult to obtain. This paper proposes a shared awareness model that uses information gathered automatically from developer IDE interactions to make explicit orderings of tasks, artefacts and developers that are relevant to particular work contexts in collaborative, and potentially distributed, software development projects. The research findings suggest that such a model can be used to: identify entities (developers, tasks, artefacts) most associated with a particular work context in a software development project; identify relevance relationships amongst tasks, developers and artefacts e.g. which developers and artefacts are currently most relevant to\u00a0\u2026", "num_citations": "40\n", "authors": ["547"]}
{"title": "Using cognitive conflict and visualisation to improve mental models held by novice programmers\n", "abstract": " Recent research has found that many novice programmers often hold non-viable mental models of basic programming concepts such as assignment and object reference, which can limit their potential to develop programming skills. This paper proposes a constructivist-based teaching model that integrates cognitive conflict and program visualisation with the aim of supporting novice programmers in the formulation of appropriate mental models. The results of an initial empirical study produced three findings of note. Firstly, a teaching model based on either visualisation alone or cognitive conflict integrated with visualisation can help students develop viable models of value assignment. Secondly, there was evidence to suggest that cognitive conflict integrated with visualisation outperformed visualisation alone in helping students develop viable models of the more challenging concept of object reference assignment\u00a0\u2026", "num_citations": "38\n", "authors": ["547"]}
{"title": "Feature location and extraction using landmarks and barriers\n", "abstract": " Identifying and isolating the source code associated with a particular feature is a problem that frequently arises in many maintenance tasks. The delocalised nature of object-oriented systems, where the code associated with a feature is distributed across many interrelated objects, makes this problem particularly challenging. This paper presents an approach that combines 'landmark' methods that have a key role in the execution of a particular feature with slicing to create a call graph of related code. The size of this call graph is constrained by the identification of 'barrier' methods which exclude parts of the graph that are not of interest. The approach is supported by a tool, and the evaluation on three open-source systems yields encouraging results and demonstrates the practical applicability of the technique.", "num_citations": "35\n", "authors": ["547"]}
{"title": "Automating the software inspection process\n", "abstract": " Inspection is widely believed to be the most cost-effective method for detecting defects in documents produced during the software development lifecycle. However, it is by its very nature a labour intensive process. This has led to work on computer support for the process which should increase the efficiency and effectiveness beyond what is currently possible with a solely manual process. In this paper, we first of all describe current approaches to automation of the inspection process. There are four main areas of inspection which have been the target for computer support: document handling, individual preparation, meeting support and metrics collection. We then describe five tools which have been developed to support the inspection process and compare the capabilities of these tools. This is followed by a fuller discussion of the features which could be provided by computer support for inspection and the\u00a0\u2026", "num_citations": "35\n", "authors": ["547"]}
{"title": "Comparing text\u2010based and dependence\u2010based approaches for determining the origins of bugs\n", "abstract": " Identifying bug origins \u2013 the point where erroneous code was introduced \u2013 is crucial for many software engineering activities, from identifying process weaknesses to gathering data to support bug detection tools. Unfortunately, this information is not usually recorded when fixing bugs, and recovering it later is challenging. Recently, the text approach and the dependence approach have been developed to tackle this problem. Respectively, they examine textual and dependence\u2010related changes that occurred prior to a bug fix. However, only limited evaluation has been carried out, partially because of a lack of available implementations and of datasets linking bugs to origins. To address this, origins of 174 bugs in three projects were manually identified and compared to a simulation of the approaches. Both approaches were partially successful across a variety of bugs \u2013 achieving 29\u201379% precision and 40\u201370% recall\u00a0\u2026", "num_citations": "32\n", "authors": ["547"]}
{"title": "Verification of Results in Software Maintenance Through External Replication.\n", "abstract": " Empirical studies carried out to help understand the problems of software maintenance are widely held to be of value. A view perhaps less widely recognised within the software engineering domain is that experiments should be replicated both internally and externally to validate the results and build up a cohesive body of knowledge. This paper presents the external replication findings of an experiment which tested the benefits to maintenance of using modular code against nonmodular (monolithic) code. The results of our replication were strikingly different from those of the original which showed that a modular program could be maintained significantly faster than an equivalent monolithic version. An inductive analysis, undertaken to investigate the reasons for this, uncovered evidence of an ability effect and the suggestion that the experiment may have been too artificial.< >", "num_citations": "31\n", "authors": ["547"]}
{"title": "Improving the mental models held by novice programmers using cognitive conflict and Jeliot visualisations\n", "abstract": " Recent research has found that many novice programmers often hold non-viable mental models of basic programming concepts which can limit their potential to develop appropriate programming skills. Previous work by the authors suggests that a teaching model that integrates cognitive conflict and program visualisation can help novices formulate appropriate mental models. This paper first outlines a'concepts roadmap'that provides an ordered approach to learning programming concepts allowing students to build on fundamental base knowledge. It then reports the results of a series of studies investigating the use of the Jeliot visualisation tool as the visualisation component of the proposed learning model when applied to these concepts. The findings include: the ease with which Jeliot can be tailored to visualise a range of concepts using a variety of examples; the Jeliot visualisation of object reference was too\u00a0\u2026", "num_citations": "30\n", "authors": ["547"]}
{"title": "A systematic literature review of machine learning techniques for software maintainability prediction\n", "abstract": " ContextSoftware maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process.ObjectiveThe major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models.MethodThe review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018.ResultsWe survey 56\u00a0\u2026", "num_citations": "29\n", "authors": ["547"]}
{"title": "Investigating data-flow coverage of classes using evolutionary algorithms\n", "abstract": " It is not unusual for a software development organization to expend 40% of total project effort on testing, which can be a very laborious and time-consuming process. Therefore, there is a big necessity for test automation. This paper describes an approach to automatically generate test-data for OO software exploiting a Genetic Algorithm (GA) to achieve high levels of data-flow (du) coverage. A proof-of-concept tool is presented. The experimental results from testing six Java classes helped us identify three categories of problematic test targets, and suggest that in the future full du coverage with a reasonable computational cost may be possible if we overcome these obstacles.", "num_citations": "29\n", "authors": ["547"]}
{"title": "Understanding object-oriented source code from the behavioural perspective\n", "abstract": " Comprehension is a key activity that underpins a variety of software maintenance and engineering tasks. The task of understanding object-oriented systems is hampered by the fact that the code segments that are related to a user-level function tend to be distributed across the system. We introduce a tool-supported code extraction technique that addresses this issue. Given a minimal amount of information about a behavioural element of the system that is of interest (such as a use-case), it extracts a trail of the methods (and method invocations) through the system that are needed in order to achieve an understanding of the implementation of the element of interest. We demonstrate the feasibility of our approach by implementing it as part of a code extraction tool, presenting a case study and evaluating the approach and tool against a set of established criteria for program comprehension tools.", "num_citations": "27\n", "authors": ["547"]}
{"title": "Understanding software testing\n", "abstract": " Understanding Software Testing \u0645\u0631\u0648\u0631 \u0645\u0648\u0636\u0648\u0639 \u0646\u0648\u0639 Pbook Article Article in Press Letter Review Journal Paper Thesis Editorial Note Ebook Conference Abstract NLM (Medline) Short Survey KMU Journal Erratum Ubiquity Press Elsevier Ltd Conference Paper American Association for Cancer Research Inc. c/o Michael Kenyon, ch. de la Pecholettaz 6, Epalinges, Switzerland. info@frontiersin.orgFrontiers Media SA Elsevier Inc. Wolters Kluwer Medknow Publications info@frontiersin.orgFrontiers Media SA Blackwell Publishing Ltd European Respiratory Society diversity@mdpi.comMDPI AG Oxford University Press Page Press Publications Elsevier Inc support@bmj.comBMJ Publishing Group cususerv@lancet.comLancet Publishing Group Elsevier BV Springer Workshops and Webinars United Kingdom. info@biomedcentral.comBioMed Central Ltd Georg Thieme Verlag info@royensoc.co.ukBlackwell Publishing Ltd \u2026", "num_citations": "27\n", "authors": ["547"]}
{"title": "Hybridizing evolutionary testing with artificial immune systems and local search\n", "abstract": " Search-based test data generation has been a considerably active research field recently. Several local and global search approaches have been proposed, but the investigation of artificial immune system (AIS) algorithms has been extremely limited. Our earlier results from testing six Java classes, exploiting a genetic algorithm (GA) to measure data- flow coverage, helped us identify a number of problematic test scenarios. We subsequently proposed a novel approach for the utilization of clonal selection. This paper investigates whether the properties of this algorithm (memory, combination of local and global search) can be beneficial in our effort to address these problems, by presenting comparative experimental results from the utilization of a GA (combined with AIS and simple local search (LS)) to test the same classes. Our findings suggest that the hybridized approaches usually outperform the GA, and there are\u00a0\u2026", "num_citations": "25\n", "authors": ["547"]}
{"title": "Identifying and addressing problems in framework reuse\n", "abstract": " Object-oriented frameworks are a powerful form of reuse but they can be difficult to understand and reuse correctly. Over the last decade a large range of candidate documentation techniques have been proposed to address this difficulty. There is little research, however, to identify the specific problems that arise during framework reuse and to evaluate documentation techniques in terms of these problems. This paper reports on a long-term investigation that firstly identifies four fundamental problems of framework reuse: mapping, understanding functionality, understanding interactions and understanding the framework architecture. It then describes two forms of documentation specifically developed to address the mapping, interaction and functionality problems namely a pattern language and a set of micro architectures. An in-depth, qualitative analysis of these two documentation types evaluates the key strengths\u00a0\u2026", "num_citations": "25\n", "authors": ["547"]}
{"title": "Issues on the object-oriented paradigm: A questionnaire survey\n", "abstract": " The object-oriented paradigm is becoming increasingly popular, apparently as the result of expert opinion and anecdotal evidence, and not on the basis of sound empirical data. This questionnaire survey was undertaken as part of a programme of research to validate unsupported claims about the paradigm. The questionnaire follows structured interviews of experienced object-oriented users 6] with the intention to con rm the ndings on a wider user group. It was posted to relevant electronic newsgroups and to members of an object-oriented (postal) mailing list. The survey received 167 responses to the electronic questionnaire and 119 responses (30% response rate) to the postal version. Results show that respondents are of the view that:(i) the object-oriented paradigm has advantages over other paradigms in terms of ease of analysis and design, programmer productivity, software reuse, and ease of maintenance,(ii) inheritance can introduce di culties when trying to understand object-oriented software,(iii) missing design documentation and poor or inappropriate design are prevalent problems,(iv) maintenance causes degradation of object-oriented software, but less frequently than conventional software, and (v) C++ has many de ciencies in comparison to other purer object-oriented languages. It is concluded that the questionnaire survey has been a successful method for gathering empirical data and identifying areas for further investigation within a programme of empirical research.", "num_citations": "25\n", "authors": ["547"]}
{"title": "Applying inspection to object\u2010oriented code\n", "abstract": " The benefits of the object\u2010oriented paradigm are widely cited. At the same time, inspection is deemed to be the most cost\u2010effective means of detecting defects in software products. Why then, is there no published experience, let alone quantitative data, on the application of inspection to object\u2010oriented systems? This paper describes the facilities of the object\u2010oriented paradigm and the issues that these raise when inspecting object\u2010oriented code. Several problems are caused by the disparity between the static code structure and its dynamic runtime behaviour. The large number of small methods in object\u2010oriented systems can also cause problems. This is followed by a description of three areas which may help mitigate problems found. Firstly, the use of various programming techniques may assist in making object\u2010oriented code easier to inspect. Secondly, improved program documentation can help the inspector\u00a0\u2026", "num_citations": "23\n", "authors": ["547"]}
{"title": "Structured interviews on the object-oriented paradigm\n", "abstract": " The method of structured interviewing is an appropriate means of conducting a primary investigation when beginning a programme of research involving di erent empirical techniques because (i) interviewing experienced users from academia and industry should determine if academics are exploring issues that industrialists deem important,(ii) interviewing users from industry helps provide external validity,(iii) users of object-oriented systems have their own views on the bene ts and drawbacks of the paradigm. These opinions could then be used as evidence to decide whether an empirically unsupported practice is justi ed and warrants further, more controlled investigation, and (iv) these views can identify issues that have not been previously considered.Structured interviews, each based on a template of 23 questions, were carried out with 13 experienced object-oriented users to explore problems discussed in the object-oriented literature concerned with high level system understanding, inheritance, software maintenance, and other issues. Interviews lasted between 30 minutes and 2 hours.", "num_citations": "22\n", "authors": ["547"]}
{"title": "Bug localisation through diverse sources of information\n", "abstract": " Many approaches have been proposed to address the problem of bug localisation - taking a bug report and recommending to developers the possible locations of the bug in the project. However, these can often require significant up-front work from developers, and are not widely adopted. Furthermore, those techniques which do not require this up-front investment are often far from accurate, and do not take advantage of all of the information that they could. We propose a technique for combining information from multiple, novel sources of information about a project and a bug, and use this to recommend bug locations to developers. We also identify how this technique could be used to create a low-effort tool for bug localisation, with the aim of increasing developer adoption. We evaluate the technique on 1143 bugs in three open-source projects, and find that it can be used to increase the number of bugs where the\u00a0\u2026", "num_citations": "21\n", "authors": ["547"]}
{"title": "Electronic bulletin board distributed questionnaires for exploratory research\n", "abstract": " The use of electronic bulletin boards is increasing dramat ically ; they are now a significant source of opinion and experience-related commentary from a wide variety of people over a large range of topics. As such, they are a major information resource and potentially suitable as a vehicle for questionnaire distribution. To date, there has been no formal discussion of this vehicle - a deficiency this paper attempts to address. It discusses the advantages and disadvantages of this medium, and compares it against the other possible alternatives. We believe this comparison shows that the medium has a great deal to offer and a wide degree of applicability, especially within the area of exploratory research.The main deficiency with the medium is that it poten tially suffers from a large-scale self-selection bias. To inves tigate this, we have conducted a questionnaire study through this medium, and subsequently replicated it\u00a0\u2026", "num_citations": "20\n", "authors": ["547"]}
{"title": "Replication's role in experimental computer science\n", "abstract": " The role of replication in experimental computer science is discussed. Various kinds of internal and external replication are di erentiated. Without the con rming power of external replications, results in experimental computer science should only be provisionally accepted. An extension to Basili et al's framework for experimentation in software engineering is proposed to more fully di erentiate between the various kinds of replication and their powers of con rmation.", "num_citations": "19\n", "authors": ["547"]}
{"title": "Defining the problems of framework reuse\n", "abstract": " Frameworks are an attractive form of reuse due to their paradigmatic simplicity. Unfortunately their size and complexity makes understanding how to use them difficult. In addition documentation to support framework reuse often lacks experimental validation and there is little understanding of what artefacts must be documented to increase the effectiveness of documentation techniques. This report describes an empirical investigation into framework reuse. Its aim is to identify the major problems of reuse and the impact of current documentation techniques on these problems. A qualitative approach is employed and developer experiences are captured from three separate reuse studies using multiple forms of data capture. Four major reuse problems are identified by this report. Understanding the functionality of components; the interactions between components; the mapping from the problem domain to the framework\u00a0\u2026", "num_citations": "18\n", "authors": ["547"]}
{"title": "Software testing\u2014searching for the missing link\n", "abstract": " Software testing has been the subject of active research for approaching 30\u00a0years. In that time, there have been developments such as the invention of techniques and tools, the morphosis of these techniques and tools to deal with the adoption of new development paradigms and programming languages, and analytical and empirical comparisons of techniques to improve our understanding of their relative merits. Impressive though these developments are, there still remains the fundamental problem of trying to relate the results of testing to some objective attribute of the program (such as number of faults). It is the search for this missing link that is attracting much of the recent research in the area. Any progress made towards this will have a considerable impact on our understanding of the subject and ultimately on software itself.", "num_citations": "18\n", "authors": ["547"]}
{"title": "Use case to source code traceability: The developer navigation view point\n", "abstract": " Requirements traceability is a challenge for modern software projects where task dependencies and technical expertise are spread across system developers, abstract model representations such as use cases, and a myriad of code artefacts. This paper presents an approach that monitors the navigation trails left by developers when building code artefacts to realise project use cases. These trails are analysed to generate a relevance ranking of entities that constitute a traceability link between uses cases and code artefacts and the developers responsible for them. Investigation in a software development scenario shows that a range of use case traceability questions can be answered through visualisations which present ordered relevance lists of the entities associated with use cases and by the use of trace graphs where the size of nodes show the importance, or 'information centrality', of system entities.", "num_citations": "17\n", "authors": ["547"]}
{"title": "Danger theory and intrusion detection: Possibilities and limitations of the analogy\n", "abstract": " Metaphors derived from Danger Theory, a hypothesized model of how the human immune system works, have been applied to the intrusion detection domain. The major contribution in this area, is the dendritic cell algorithm (DCA). This paper presents an in-depth analysis of results obtained from two previous experiments, regarding the suitability of the danger theory analogy in constructing intrusion detection systems for web applications. These detectors would be capable of detecting novel attacks while improving on the limitations of anomaly-based intrusion detectors. In particular, this analysis investigates which aspects of this analogy are suitable for this purpose, and which aspects of the analogy are counterproductive if utilized in the way originally suggested by danger theory. Several suggestions are given for those aspects of danger theory that are identified to require modification, indicating the\u00a0\u2026", "num_citations": "16\n", "authors": ["547"]}
{"title": "Towards a benchmark for the evaluation of software testing techniques\n", "abstract": " Despite the existence of a great number of software testing techniques we are largely ignorant of their respective powers as software engineering methods. It is argued that more experimental work in software testing is necessary in order to place testing techniques onto a scale of measurement, or classify them in such a way that is useful to the software engineer. Current experimental practices are examined using a parametric framework and are shown to contribute little towards a cohesive and useful body of knowledge. The idea of a benchmark repository of faulty and correct software is explored enabling unification of diverse experimental results. Such a unification should start the process of moving towards an evaluation taxonomy of testing methods.", "num_citations": "16\n", "authors": ["547"]}
{"title": "Separating passing and failing test executions by clustering anomalies\n", "abstract": " Developments in the automation of test data generation have greatly improved efficiency of the software testing process, but the so-called oracle problem (deciding the pass or fail outcome of a test execution) is still primarily an expensive and error-prone manual activity. We present an approach to automatically detect passing and failing executions using cluster-based anomaly detection on dynamic execution data based on firstly, just a system\u2019s input/output pairs and secondly, amalgamations of input/output pairs and execution traces. The key hypothesis is that failures will group into small clusters, whereas passing executions will group into larger ones. Evaluation on three systems with a range of faults demonstrates this hypothesis to be valid\u2014in many cases small clusters were composed of at least 60\u00a0% failures (and often more). Concentrating the failures in these small clusters substantially reduces the\u00a0\u2026", "num_citations": "15\n", "authors": ["547"]}
{"title": "A 3-dimensional relevance model for collaborative software engineering spaces\n", "abstract": " Today's large software projects are often characterised by distributed environments with numerous developers separated in space and/or time. This separation means that the common understanding and tacit knowledge that is a feature of closely co-located project teams is very hard to come by. As a consequence, relatively simple tasks such as identifying functionally related modules or finding individuals who are experts in aspects of the system become more challenging and time-consuming. This paper presents a continuum of relevance index (CRI) model that uses information gathered from developer IDE interactions to generate orderings of relevant tasks, project artefacts and developers. A case study is used to demonstrate how the model can be used to attain a shared knowledge and common understanding of the extent to which tasks, artefacts and developers are relevant in a group development work context.", "num_citations": "15\n", "authors": ["547"]}
{"title": "Using smartphones in cities to crowdsource dangerous road sections and give effective in-car warnings\n", "abstract": " The widespread day-to-day carrying of powerful smartphones gives opportunities for crowd-sourcing information about the users' activities to gain insight into patterns of use of a large population in cities. Here we report the design and initial investigations into a crowdsourcing approach for sudden decelerations to identify dangerous road sections. Sudden brakes and near misses are much more common than police reportable accidents but under exploited and have the potential for more responsive reaction than waiting for accidents. We also discuss different multimodal feedback conditions to warn drivers approaching a dangerous zone. We believe this crowdsourcing approach gives cost and coverage benefits over infrastructural smart-city approaches but that users need incentivized for use.", "num_citations": "14\n", "authors": ["547"]}
{"title": "Practical code inspection for object-oriented systems\n", "abstract": " This paper describes a series of three empirical studies devoted to the development of a rigorous approach for effective inspections of object-oriented (OO) code. Since the time that inspections were developed they have been shown to be powerful defect detection strategies. However, little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. This suggests that inspection techniques may not be currently being deployed to their best effect in the context of large-scale OO systems. The studies reveal three significant issues that need to be addressedthe identification of chunks of code to be inspected, the order in which the code is read, and the resolution of frequent non local references. The sequence of experiments builds up a complement of three techniques: one based on a checklist, one focussed on constructing abstract specifications, and the last centred on the route that a use-case takes through a system. It is demonstrated that the checklist is the most effective approach but that the other techniques also have strengths and so for the best results in a practical situation a combination of techniques is recommended.", "num_citations": "14\n", "authors": ["547"]}
{"title": "A structural testing method for JSP designed programs\n", "abstract": " A new structural testing method (STM) is presented for use with JSP (Jackson structured programming) designed programs. The method involves the creation of a path expression from the program structure and subsequent expansion of the expression into a set of paths using an algebra described in the paper. The treatment of each JSP construct is dealt with in detail. A worked example demonstrates the use of the method to generate test cases. The method is compared in terms of coverage to other well\u2010known structural testing methods. The advantages and disadvantages of the method are discussed and further work currently being undertaken by the authors is outlined.", "num_citations": "14\n", "authors": ["547"]}
{"title": "Automatic test-data generation: An immunological approach\n", "abstract": " In previous research, we presented an approach to automatically generate test-data for object-oriented software exploiting a genetic algorithm (GA) to achieve high levels of data-flow coverage. The experimental results from testing six Java classes helped us identify a number of problematic test targets, and suggest that in the future full data-flow coverage with a reasonable computational cost may be possible if we overcome these obstacles. To this end, the investigation of artificial immune system (AIS) algorithms was chosen. This paper provides a brief summary of our previous work and an introduction to both human and artificial immune system. We then suggest a framework for the application of AIS algorithms to the problem of automated testing, followed by some thoughts on why and how these algorithms can be beneficial in our effort to improve the performance of our previously implemented GA. Finally, our\u00a0\u2026", "num_citations": "13\n", "authors": ["547"]}
{"title": "A collaborative approach to learning programming: A hybrid learning model\n", "abstract": " The use of cooperative working as a means of developing collaborative skills has been recognised as vital in programming education. This paper presents results obtained from preliminary work to investigate the effectiveness of Pair Programming as a collaborative learning strategy and also its value towards improving programming skills within the laboratory. The potential of Problem Based Learning as a means of further developing cooperative working skills along with problem solving skills is also examined and a hybrid model encompassing both strategies outlined.", "num_citations": "12\n", "authors": ["547"]}
{"title": "A survey of experiences amongst object-oriented practitioners\n", "abstract": " The object-oriented paradigm is becoming increasingly popular as a result of expert opinion and anecdotal evidence and not on the basis of sound empirical data. The questionnaire survey was undertaken as part of a programme of research to validate unsupported claims about the paradigm. The questionnaire follows structured interviews of experienced object-oriented developers with the intention of confirming the findings on a wider practitioner group. It was posted to relevant electronic newsgroups and to members of an object-oriented (postal) mailing list. The survey received 167 responses to the electronic questionnaire and 119 responses (30% response rate) to the postal version. Results show that respondents are of the view that: (i) the object-oriented paradigm has advantages over other paradigms in terms of ease of analysis and design, programmer productivity, software reuse, and ease of\u00a0\u2026", "num_citations": "12\n", "authors": ["547"]}
{"title": "Using machine learning to classify test outcomes\n", "abstract": " When testing software it has been shown that there are substantial benefits to be gained from approaches which exercise unusual or unexplored interactions with a system - techniques such as random testing, fuzzing, and exploratory testing. However, such approaches have a drawback in that the outputs of the tests need to be manually checked for correctness, representing a significant burden for the software engineer. This paper presents a strategy to support the process of identifying which tests have passed or failed by combining clustering and semi-supervised learning. We have shown that by using machine learning it is possible to cluster test cases in such a way that those corresponding to failures concentrate into smaller clusters. Examining the test outcomes in cluster-size order has the effect of prioritising the results: those that are checked early on have a much higher probability of being a failing test. As\u00a0\u2026", "num_citations": "11\n", "authors": ["547"]}
{"title": "Towards the experimental evaluation of Software testing techniques\n", "abstract": " Despite the existence of a large number of software testing techniques we are largely ignorant of their respective powers as software engineering methods. It is argued that more experimental work in software testing is necessary in order to place testing techniques onto a scale of measurement other than the nominal. Current experimental practices are examined using a parametric framework and are shown to contribute little towards a cohesive and useful body of knowledge. A number of suggestions are made regarding how experimentation may progress at a faster and more productive rate.", "num_citations": "11\n", "authors": ["547"]}
{"title": "A heuristic-based approach to code-smell detection\n", "abstract": " Encapsulation and data hiding are central tenets of the object oriented paradigm. Deciding what data and behaviour to form into a class and where to draw the line between its public and private details can make the difference between a class that is an understandable, flexible and reusable abstraction and one which is not. This decision is a difficult one and may easily result in poor encapsulation which can then have serious implications for a number of system qualities. It is often hard to identify such encapsulation problems within large software systems until they cause a maintenance problem (which is usually too late) and attempting to perform such analysis manually can also be tedious and error prone. Two of the common encapsulation problems that can arise as a consequence of this decomposition process are data classes and god classes. Typically, these two problems occur together \u2013 data classes are lacking in functionality that has typically been sucked into an over-complicated and domineering god class. This paper describes the architecture of a tool which automatically detects data and god classes that has been developed as a plug-in for the Eclipse IDE. The technique has been evaluated in a controlled study on two large open source systems which compare the tool results to similar work by Marinescu, who employs a metrics-based approach to detecting such features. The study provides some valuable insights into the strengths and weaknesses of the two approaches", "num_citations": "10\n", "authors": ["547"]}
{"title": "Evaluating the effect of inheritance on the maintainability of object-oriented software\n", "abstract": " This empirical study was undertaken as part of a programme of research to explore unsupported claims about the paradigm: a series of experiments tested the effect of inheritance on the maintainability of object-oriented software. Subjects were asked to modify object-oriented software with a hierarchy of 3 levels of inheritance depth and equivalent object-based software with no inheritance. In an experiment of similar design, subjects were asked to modify object-oriented software with a hierarchy of 5 levels of inheritance depth and the equivalent object-based software. The collected timing data showed that subjects maintaining object-oriented software with 3 levels of inheritance depth performed the modification tasks significantly quicker than those maintaining equivalent object-based software with no inheritance. In contrast, subjects maintaining the object-oriented software with 5 levels of inheritance depth were outperformed by the subjects maintaining the equivalent object-based software (although no statistical significance was obtained). ft is concluded that the findings are sufficiently important that attempts to verify the results should be made by independent researchers. Subsequent studies should seek to scale up the findings to the maintenance of more complex software by professional programmers.", "num_citations": "10\n", "authors": ["547"]}
{"title": "Software testing: A selected annotated bibliography\n", "abstract": " A collection of 69 references drawn from books, papers and conference proceedings on the subject of software testing is presented. Each reference is accompanied by a paragraph describing its contents. The aim when selecting the references was to minimize the number and maximize the coverage of the subject. For this reason, the bibliography is primarily intended for the researcher or practitioner who is relatively new to the subject. The bibliography is organized according to the following sections: general introductions; background; testing methods; experimental studies; and tools.", "num_citations": "10\n", "authors": ["547"]}
{"title": "Automatically classifying test results by semi-supervised learning\n", "abstract": " A key component of software testing is deciding whether a test case has passed or failed: an expensive and error-prone manual activity. We present an approach to automatically classify passing and failing executions using semi-supervised learning on dynamic execution data (test inputs/outputs and execution traces). A small proportion of the test data is labelled as passing or failing and used in conjunction with the unlabelled data to build a classifier which labels the remaining outputs (classify them as passing or failing tests). A range of learning algorithms are investigated using several faulty versions of three systems along with varying types of data (inputs/outputs alone, or in combination with execution traces) and different labelling strategies (both failing and passing tests, and passing tests alone). The results show that in many cases labelling just a small proportion of the test cases - as low as 10% - is sufficient\u00a0\u2026", "num_citations": "9\n", "authors": ["547"]}
{"title": "Cast with GAs-automatic test data generation via evolutionary computation\n", "abstract": " Although it is well understood to be a generally undecidable problem, a number of attempts have been made over the years to develop systems to automatically generate test data to achieve a level of coverage (branch coverage for example). These approaches have ranged at early attempts at symbolic execution to more recent dynamic approaches and, despite their variety (and varying degrees of success), all the systems developed have involved a detailed analysis of the program or system under test. In a departure from this approach, this paper describes a system developed to explore the use of genetic algorithms to generate test data to automatically meet a level of coverage. (5 pages)", "num_citations": "8\n", "authors": ["547"]}
{"title": "Predicting software maintainability in object-oriented systems using ensemble techniques\n", "abstract": " Prediction of the maintainability of classes in object-oriented systems is a significant factor for software success, however it is a challenging task to achieve. To date, several machine learning models have been applied with variable results and no clear indication of which techniques are more appropriate. With the goal of achieving more consistent results, this paper presents the first set of results in an extensive empirical study designed to evaluate the capability of bagging models to increase accuracy prediction over individual models. The study compares two major machine learning based approaches for predicting software maintainability: individual models (regression tree, multilayer perceptron, k-nearest neighbors and m5rules), and an ensemble model (bagging) that are applied to the QUES data set. The results obtained from this study indicate that k-nearest neighbors model outperformed all other individual\u00a0\u2026", "num_citations": "7\n", "authors": ["547"]}
{"title": "Text entry tap accuracy and exploration of tilt controlled layered interaction on Smartwatches\n", "abstract": " Design of text entry on small screen devices, eg smartwatches, faces two related challenges: trading off a reasonably sized keyboard area against space to display the entered text and the concern over\" fat fingers\". This paper investigates tap accuracy and revisits layered interfaces to explore a novel layered text entry method. A two part user study identifies preferred typing and reading tilt angles and then investigates variants of a tilting layered keyboard against a standard layout. We show good typing speed (29 wpm) and very high accuracy on the standard layout-contradicting fears of fat-fingers limiting watch text-entry. User feedback is positive towards tilting interaction and we identify\u223c 14 tilt as a comfortable typing angle. However, layering resulted in slightly slower and more erroneous entry. The paper contributes new data on tilt angles and key offsets for smartwatch text entry and supporting evidence for the\u00a0\u2026", "num_citations": "7\n", "authors": ["547"]}
{"title": "A multi-method approach to performing empirical research\n", "abstract": " Software costs are rising, as are problems with software quality and delivering software on schedule and within the development budget; as a consequence more researchers are arguing that substantial empirical investigation is necessary to develop techniques and methods to address these problems. This paper introduces a multi-method approach to conducting empirical research and discusses the benefits this approach offers over that of a single method empirical study. The results of a three phased multi-method investigation into the object-oriented paradigm are then summarised. It is concluded that the multi-method approach is a suitable method which allows focused empirical work to be conducted from more general empirical studies. Also, the approach provides confirmatory power; subsequent conclusions derived from the research are more likely to hold weight. Keywords: empirical, experiment, multi-method, object-oriented. Dept. Computer Science, University of Strathclyde, Livingstone Tow...", "num_citations": "7\n", "authors": ["547"]}
{"title": "An external replication of a korson experiment\n", "abstract": " Carrying out empirical studies to understand the problems of software engineering is widely held to be of importance. A view less widely held within the software engineering community is that experiments should be replicated externally (ie by di erent researchers) to both verify and validate the original results and so build up a sound body of knowledge (as opposed to drawing conclusions from a single study). This paper presents the results of an external replication of an experiment which tested the bene ts to maintenance of using modular code against non-modular (monolithic) code. The results of the original experiment 8, 9] showed that a modular program could be maintained signi cantly faster than an equivalent monolithic version of the same program under the condition that modularity has been used to implement information hiding which localizes changes required by a modi cation.The results of our replication, however, were strikingly di erent from those of the original and showed no signi cant di erence between the average times taken to maintain modular and monolithic code. An inductive analysis was undertaken to investigate the reasons for this di erence. Evidence was uncovered suggesting an ability e ect (which was not observed in the original experiment), a lack of realism in the context in which subjects were asked to perform tasks, di ering degrees of subjects' understanding of the programs, di erent approaches taken by subjects towards making the required modi cations, and possible de ciencies of subject monitoring. This evidence, along with evidence for other sources of variability, is discussed in some detail. It is concluded\u00a0\u2026", "num_citations": "7\n", "authors": ["547"]}
{"title": "Building test oracles by clustering failures\n", "abstract": " In recent years, software testing research has produced notable advances in the area of automated test data generation, but the corresponding oracle problem (a mechanism for determine the (in)correctness of an executed test case) is still a major problem. In this paper, we present a preliminary study which investigates the application of anomaly detection techniques (based on clustering) to automatically build an oracle using a system's input/output pairs, based on the hypothesis that failures will tend to group into small clusters. The fault detection capability of the approach is evaluated on two systems and the findings reveal that failing outputs do indeed tend to congregate in small clusters, suggesting that the approach is feasible and has the potential to reduce by an order of magnitude the numbers of outputs that would need to be manually examined following a test run.", "num_citations": "6\n", "authors": ["547"]}
{"title": "Investigating effort prediction of web-based applications using CBR on the ISBSG dataset\n", "abstract": " As web-based applications become more popular and more sophisticated, so does the requirement for early accurate estimates of the effort required to build such systems. Case-based reasoning (CBR) has been shown to be a reasonably effective estimation strategy, although it has not been widely explored in the context of web applications. This paper reports on a study carried out on a subset of the ISBSG dataset to examine the optimal number of analogies that should be used in making a prediction. The results show that it is not possible to select such a value with confidence, and that, in common with other findings in different domains, the effectiveness of CBR is hampered by other factors including the characteristics of the underlying dataset (such as the spread of data and presence of outliers) and the calculation employed to evaluate the distance function (in particular, the treatment of numeric and categorical data).", "num_citations": "6\n", "authors": ["547"]}
{"title": "Improving the viability of mental models held by novice programmers\n", "abstract": " Recent research has found that many novice programmers often hold non-viable mental models of basic programming concepts such as assignment and object reference. This paper proposes a constructivist-based teaching model, integrating a cognitive conflict strategy with program visualization, with the aim of improving novice programmers\u2019 mental models. The results of a preliminary empirical study suggest that, for the relatively straightforward concept of assignment, tight integration of program visualization with a cognitive conflict event that highlights a student\u2019s inappropriate understanding can help improve students\u2019 non-viable mental models. 14 out of 18 participants who held non-viable mental models of the assignment process successfully changed their model to be viable as a result of the proposed teaching model.", "num_citations": "6\n", "authors": ["547"]}
{"title": "Software testing using analysis and design based techniques\n", "abstract": " This paper presents a way of generating acceptance and systems test data from documents produced as part of the traditional structured approaches to systems analysis and design. Entity\u2010relationship diagrams are analysed in a fashion similar to boundary\u2010value analysis to generate combinations of entities that should be supported by the system. Data flow diagrams are analysed in a way that generates tests to exercise the passage of each data item through the system. This information is augmented with details from the process specifications to ensure, firstly, that the specifications themselves are tested to branch coverage level and secondly, that no infeasible tests are generated. Finally, the \u2018entity life histories\u2019 are examined and tests generated to evaluate the full range of possible states experienced by the entities within the system. The result is a common pool of test data requirements and expected results\u00a0\u2026", "num_citations": "6\n", "authors": ["547"]}
{"title": "Evaluation of random forest and ensemble methods at predicting complications following cardiac surgery\n", "abstract": " Cardiac patients undergoing surgery face increased risk of postoperative complications, due to a combination of factors, including higher risk surgery, their age at time of surgery and the presence of co-morbid conditions. They will therefore require high levels of care and clinical resources throughout their perioperative journey (i.e. before, during and after surgery). Although surgical mortality rates in the UK have remained low, postoperative complications on the other hand are common and can have a significant impact on patients\u2019 quality of life, increase hospital length of stay and healthcare costs. In this study we used and compared several machine learning methods \u2013 random forest, AdaBoost, gradient boosting model and stacking \u2013 to predict severe postoperative complications after cardiac surgery based on preoperative variables obtained from a surgical database of a large acute care hospital in\u00a0\u2026", "num_citations": "5\n", "authors": ["547"]}
{"title": "Application of ensemble techniques in predicting object-oriented software maintainability\n", "abstract": " While prior object-oriented software maintainability literature acknowledges the role of machine learning techniques as valuable predictors of potential change, the most suitable technique that achieves consistently high accuracy remains undetermined. With the objective of obtaining more consistent results, an ensemble technique is investigated to advance the performance of the individual models and increase their accuracy in predicting software maintainability of the object-oriented system. This paper describes the research plan for predicting object-oriented software maintainability using ensemble techniques. First, we present a brief overview of the main research background and its different components. Second, we explain the research methodology. Third, we provide expected results. Finally, we conclude summary of the current status.", "num_citations": "5\n", "authors": ["547"]}
{"title": "A preliminary evaluation of text-based and dependency-based techniques for determining the origins of bugs\n", "abstract": " A crucial step in understanding the life cycle of software bugs is identifying their origin. Unfortunately this information is not usually recorded and recovering it at a later date is challenging. Recently two approaches have been developed that attempt to solve this problem: the text approach and the dependency approach. However only limited evaluation has been carried out on their effectiveness so far, partially due to the lack of data sets linking bugs to their introduction. Producing such data sets is both time-consuming and challenging due to the subjective nature of the problem. To improve this, the origins of 166 bugs in two open-source projects were manually identified. These were then compared to a simulation of the approaches. The results show that both approaches were partially successful across a variety of different types of bugs. They achieved a precision of 29%-79% and a recall of 40%-70%, and could\u00a0\u2026", "num_citations": "4\n", "authors": ["547"]}
{"title": "Artifical immune systems, danger theory, and the oracle problem\n", "abstract": " The oracle problem - the mechanism by which the output associated with a test is determined to pass or fail - is an important, but frequently neglected, challenge for software testing researchers. Artificial immune systems offer a potentially interesting avenue of attack but using their power as classifiers to distinguish between pass outputs and fail outputs. Danger theory looks for other signals to improve this classification. The aim of this paper is to consider how AIS and danger theory may be applied to the oracle problem.", "num_citations": "4\n", "authors": ["547"]}
{"title": "An XP inspired test-oriented life-cycle production strategy for building embedded biomedical applications\n", "abstract": " The construction of embedded biomedical applications is an under explored topic. The status quo is for practitioners to utilize a production process which possesses no specific focus; meanwhile, the marketplace requires highly demanding characteristics from these products. The principal requirement is that most of these products need to be effectively defect free. This demands that the production process be directed towards this objective; and hence the focus of this paper is our initial attempts at designing and implementing such a process. Our new process is developed around transforming a subset of extreme programming from the world of desktop applications into a methodology for this new domain. The paper also discusses our experiences in developing test frameworks to support the domain and our objectives. Finally, the paper provides some pointers on our future plans for tackling the many unresolved\u00a0\u2026", "num_citations": "4\n", "authors": ["547"]}
{"title": "Estimating fault numbers remaining after testing\n", "abstract": " Testing is an essential component of the software development process, but also one which is exceptionally difficult to manage and control. For example, it is well understood that testing techniques are not guaranteed to detect all faults, but more frustrating is that after the application of a testing technique the tester has little or no knowledge of how many faults might still be left undiscovered. This paper investigates the performance of a range of capture-recapture models to determine the accuracy with which they predict the number of defects remaining after testing. The models are evaluated with data from two empirical testing-related studies and from one larger publicly available project and the factors affecting the accuracy of the models are analysed. The paper also considers how additional information (such as structural coverage data) may be used to improve the accuracy of the estimates. The results\u00a0\u2026", "num_citations": "3\n", "authors": ["547"]}
{"title": "Replication of software engineering experiments\n", "abstract": " Carrying out empirical studies is slowly becoming widely held to be of importance by the software engineering community. A view perhaps less widely held is that experiments should be replicated externally to both verify and generalise the original results. This paper serves a number of purposes. The need for external replications is established and the role of replication in experimental software engineering is discussed. Without the confirming power of external replications, results in experimental software engineering should only provisionally be accepted, if at all. The paper then draws heavily on the authors' experiences in externally replicating three software engineering experiments (Kor86, PVB95, KL96]) to provide guidance on three, relatively neglected areas: the improving of experimentalrecipes' during replication to either focus or generalise results, the use of alternative data analysis techniques, specifically rule induction, to seek alternative explanations when the results of replications differ, and packaging of experiments for replication. The facilitation of replication through properly constructed replication packages places formidable but largely unrewarding demands on the original experimenter. Yet without appropriately constructed and documented replication packages systematic and cumulative empirical investigation will be difficult to achieve. It is demonstrated that a modest generalisation of the characterisation scheme, or framework, proposed by Lott and Rombach LR96], will serve well as the basis for the reporting, packaging and recipe improvement of software engineering experiments for the purposes of external replication. It\u00a0\u2026", "num_citations": "3\n", "authors": ["547"]}
{"title": "Object fault tolerance\n", "abstract": " Traditional modular redundancy based upon software fault tolerance techniques, such as N-version programming and recovery blocks, is now widely known and its use is on the increase. By contrast, data fault tolerance techniques have been largely ignored. The most likely reason for this omission is the perception that implementations are highly complex. Fortunately this situation has been greatly improved by object oriented systems which supply many of the required facilities. These allow data fault tolerance techniques to take their place alongside software design diversity techniques. Currently the areas of software fault tolerance and object-oriented techniques have been developed separately. This paper describes an approach to merge these two areas, providing a framework to utilise object-oriented approaches to achieve software fault tolerance incorporating both design and data diversity techniques.", "num_citations": "3\n", "authors": ["547"]}
{"title": "Criticisms of an empirical study of recursion and iteration\n", "abstract": " The importance of empirical studies in software engineering cannot be understated. Care must be taken, however, that the experimental methods employed are appropriate and that they are executed with due scientific rigour. A critical commentary is presented of a paper by Sinha and Vessey [6] which attempted empirical evaluations of a number of propositions concerned with the notion of cognitive fit (ie the notion that problem solving can be facilitated by use of appropriate representations and tools.) There are concerns about the scale, nature, and representation of the experimental tasks. The entire experiment relies on the concept of a naturally recursive task. There are also concerns about the lack of information about the subjects and their use of the programming environments. Some criticisms are also presented of the analysis of the results and the discussion. It is concluded that the interpretations given in [6\u00a0\u2026", "num_citations": "3\n", "authors": ["547"]}
{"title": "A systematic review of feature selection techniques in software quality prediction\n", "abstract": " Background: Feature selection techniques are important factors for improving machine learning models because they increase prediction accuracy and decrease the time to create a model. Recently, feature selection techniques have been employed on software quality prediction problems with different results and no clear indication of which techniques are frequently used.Objective: This study aims to conduct a systematic review of the application of feature selection techniques in software quality prediction and answers eight research questions.Method: The review evaluates 15 papers in 9 journals and 6 conference proceedings from 2007 to 2017 using the standard systematic literature review method.Results: The results obtained from this study reveal that the filter feature selection method was the most commonly used in the studies (60%) and RELIEF was the most employed among this method, and a limited\u00a0\u2026", "num_citations": "2\n", "authors": ["547"]}
{"title": "Achieving anomaly detection effectiveness beyond the symmetric error lower bound, in web-based systems\n", "abstract": " Web-based systems are increasingly becoming the target of security attacks due to the hostile deployment environment presented by the Internet. Whilst anomaly detection promises to provide the next generation type of Intrusion Detection Systems (IDS), with a potential to detect novel attacks without attack signature characterization, the detection effectiveness associated with this type of IDS is still very poor. The symmetric error lower bound and normal behavior profiling are the main causes for this current state. This paper investigates how danger theory inspired Artificial Immune Systems (AIS) are able to address these problems, both from a theoretical analysis point of view as well by investigating classification accuracy of the Dendritic Cell Algorithm (DCA), that represents the most important danger model based AIS published so far. In particular it is shown that whilst this approach provides the potential for\u00a0\u2026", "num_citations": "2\n", "authors": ["547"]}
{"title": "Changing Programming Paradigm\u2014An Empirical Investigation\n", "abstract": " This paper outlines an investigation into the ability of experienced programmers to effectively change programming paradigm. In particular it investigates programmers changing from imperative to object-oriented programming. This is achieved by comparing code samples from the new and experienced object-oriented programmers. Each sample is presented to a tool which makes a number of automatic measurements. The comparison has revealed large style differences between the two sets of programmers. The two groups having very different ideas on what constitutes an object, resulting in significant variations in the concepts of the responsibility or extent of a class, and inheritance and associated operations. These differences will typically result in major variations in the quality and reusability of the software produced.", "num_citations": "2\n", "authors": ["547"]}
{"title": "Testing and modern software development methodologies\n", "abstract": " The author looks at the subject of testing and modern software development methodologies in the context of critical systems. The demands of testing are high and so methodologies can be viewed in the way that they contribute towards 'good' testing. It is suggested how testing theory may be used to drive methodology development.< >", "num_citations": "2\n", "authors": ["547"]}
{"title": "Machine learning techniques for automated software fault detection via dynamic execution data: empirical evaluation study\n", "abstract": " The biggest obstacle of automated software testing is the construction of test oracles. Today, it is possible to generate enormous amount of test cases for an arbitrary system that reach a remarkably high level of coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Previous work by the authors has explored the use of unsupervised and semi-supervised learning techniques to develop test oracles so that the correctness of software outputs and behaviours on new test cases can be predicated [1],[2],[10], and experimental results demonstrate the promise of this approach. In this paper, we present an evaluation study for test oracles based on machine-learning approaches via dynamic execution data (firstly, input/output pairs and secondly, amalgamations of input/output pairs and execution traces) by comparing their effectiveness with existing\u00a0\u2026", "num_citations": "1\n", "authors": ["547"]}
{"title": "Determining the Best Prediction Accuracy of Software Maintainability Models Using Auto-WEKA\n", "abstract": " Highly accurate prediction of software maintainability models is a significant requirement to achieve software quality assurance. Development an accurate prediction model may involve on trying several types of machine learning models with different configurations that include tuning parameters and selected features. However, this is a difficult and very time-consuming task to implement. In this paper, we report on the experience of using a new, rapid automated tool to identify the best prediction accuracy of a software maintainability model, namely Auto-WEKA, applied to sets of different models with various configurations. Auto-WEKA is applied to five datasets collected from real-world open-source software systems. The mean magnitude relative error (MMRE) value is used to evaluate the accuracy of predictive models, along with ZeroR model to compare selected model performance with the baseline. The\u00a0\u2026", "num_citations": "1\n", "authors": ["547"]}
{"title": "An Empirical Comparison of Two Different Strategies to Automated Fault Detection: Machine Learning Versus Dynamic Analysis\n", "abstract": " Software testing is an established method to ensure software quality and reliability, but it is an expensive process. In recent years, the automation of test case generation has received significant attention as a way to reduce costs. However, the oracle problem (a mechanism for determine the (in) correctness of an executed test case) is still major problem which has been largely ignored. Recent work has shown that building a test oracle using the principles of anomaly detection techniques (mainly semisupervised/ unsupervised learning models based on dynamic execution data consisting of an amalgamation of input/output pairs and execution traces) is able to demonstrate a reasonable level of success in automatically detect passing and failing execution [1], [2]. In this paper, we present a comparison study between our machine-learning based approaches and an existing techniques from the specification mining\u00a0\u2026", "num_citations": "1\n", "authors": ["547"]}
{"title": "Characterization of a Danger Context for Detecting Novel Attacks Targeting Web-Based Systems\n", "abstract": " Web-based systems presently constitute a primary target for security attacks, given the novel technology these employ, internet connectivity allowing for the possibility for remote attacks and automated malware propagation, and the security-critical nature of the applications (eg ecommerce) that these host. Despite this threat, the type of intrusion detection employed in these systems is mostly still limited to signature-based schemes that are prone to false positives and unable to detect novel attacks. Anomaly detection promises to provide the much required leap towards novel attack detection, but the presently associated false positives rate prohibits their deployment. Danger theorybased detection, inspired from a hypothetic model of the human immune system, promises to provide the possibility of detecting novel attacks at a high rate of detection effectiveness. This type of detection works by correlating monitored\u00a0\u2026", "num_citations": "1\n", "authors": ["547"]}
{"title": "A web-based learning model for improving programming students' mental models\n", "abstract": " Recent research has found that many programming students often hold non-viable mental models of basic programming concepts such as assignment and object reference. To improve those students\u2019 mental models, a constructivist-based learning model, integrating a cognitive conflict strategy with program visualisation, was proposed by the authors. In addition, a web-based learning environment has been developed to offer a practical tool for instructors and students to use the proposed learning model for teaching and learning. This paper describes this learning environment and also presents a preliminary study that was conducted to investigate the performance of this learning environment. The results of this study reveal that the learning environment is effective in helping students construct viable mental models of a relative simple concept, namely value assignment. The current aim of this work is to extend the environment to cover a number of key programming concepts and to make it available to fellow researchers and instructors for further investigation in their own teaching contexts.", "num_citations": "1\n", "authors": ["547"]}
{"title": "AWeb-BASED LEARNING ENVIRONMENT FOR IMPROVING PROGRAMMING STUDENTS\u2019MENTAL MODELS\n", "abstract": " Recent research has found that many programming students often hold non-viable mental models of basic programming concepts such as assignment and object reference. To improve those students\u2019 mental models, a constructivist-based learning model, integrating a cognitive conflict strategy with program visualisation, was proposed by the authors. In addition, a web-based learning environment has been developed to offer a practical tool for instructors and students to use the proposed learning model for teaching and learning. This paper describes this learning environment and also presents a preliminary study that was conducted to investigate the performance of this learning environment. The results of this study reveal that the learning environment is effective in helping students construct viable mental models of a relative simple concept, namely value assignment. The current aim of this work is to extend the environment to cover a number of key programming concepts and to make it available to fellow researchers and instructors for further investigation in their own teaching contexts.", "num_citations": "1\n", "authors": ["547"]}
{"title": "Collecting and Categorising Faults in Object-Oriented Code\n", "abstract": " A range of techniques exist to identify and isolate faults in object-oriented code, but development and evaluation of these techniques is hampered by the fact that there is very little information about the nature of object-oriented faults. This paper describes our experiences of using open source software projects to build up a picture of common faults in OO software. Open source projects are a rich source of such data but the identification of faults from problem reports is a non-trivial exercise. Heeding existing warnings against employing tree-based classification schemes, the attribute categorization scheme of Weyuker and Ostrand is adapted for the OO paradigm and employed to describe the 71 identified faults. These descriptions are then used to create an initial treebased fault model. The resulting fault model is of course partial (we are limited by the types of bugs we have observed), but it provides a useful starting\u00a0\u2026", "num_citations": "1\n", "authors": ["547"]}
{"title": "RWE SCHOTT Solar's commercial PV system experience\n", "abstract": " RWE SCHOTT Solar, Inc. is a leading manufacturer and supplier of photovoltaic modules and systems for the on-grid, commercial photovoltaic market in the United States. RWE SCHOTT Solar's product line includes the ASE-300 photovoltaic module and the SunRoof FS freestanding flat roof mounting system. Since 2003, RWE SCHOTT Solar has deployed more than 5 MW of grid-connected photovoltaic systems on commercial flat roofs incorporating these products. This paper describes some of RWE SHOTT Solar's experiences in developing, deploying, and supporting these systems in the US.", "num_citations": "1\n", "authors": ["547"]}
{"title": "Empirical evaluation of software quality attributes\n", "abstract": " Why has there been so little progress in the quantitative evaluation of those quality attributes concerned with software design? This paper seeks to provide an answer by arguing that progress is dependent on the adoption of a programme of cumulative empirical research. Such a programme consists of original empirical research, independent replication to investigate and verify original results, and, critically, recipe-improvement replication in an attempt to produce more generalized results. Furthermore, a programme of cumulative empirical research is dependent upon quality attributes being communally defined and soundly related to appropriate measures.Empirical research is necessary to demonstrate that quality attributes, and their quantification, are well-founded. Replication is necessary to investigate and confirm original results. Thereafter, recipe-improvement addresses the significant problems of subject, environment and domain variability associated with the empirical study of software quality attributes. Recipe-improvement research is essential if results are to be progressed from the specific to the general.", "num_citations": "1\n", "authors": ["547"]}
{"title": "A Simple Dynamic Analyser for C+\n", "abstract": " Attempting to develop, test and maintain large object oriented software systems is a di cult task. This paper presents a small, portable dynamic analyser for C++ which instruments the code and provides feedback on the execution trace of the software. This information may be used by testers and maintainers as well as novices trying to understand the complexities of object-oriented software. The structure of the tool is described along with an example. The output from the tool is explained and it is shown how a simple reporting tool can analyse the output to provide test coverage information. Future work involves developing further tools to visualise the execution of object-oriented software.", "num_citations": "1\n", "authors": ["547"]}
{"title": "Applying object-oriented construction to fault tolerant systems\n", "abstract": " This paper investigates the application of object-oriented construction to fault tolerant systems. The resulting system provides traditional fault tolerance within objects, but also a new form of fault tolerance between objects: object diversity. Object diversity extends current practice by integrating diversity in two directions: data and algorithm. This resulting form will allow increased diversity to be incorporated within fault tolerant systems. Further benefits are derived from the use of the inheritance hierarchy as a natural source of redundant components. As class libraries (both general and application specific) grow, more and more \"free\" redundant components will become available, yielding increasing savings on production costs.< >", "num_citations": "1\n", "authors": ["547"]}