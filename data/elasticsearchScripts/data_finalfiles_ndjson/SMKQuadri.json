{"title": "Business intelligence: an integrated approach\n", "abstract": " Business intelligence systems combine operational and historical data with analytical tools to present valuable and competitive information to business planners and decision makers. The objective of Business intelligence (BI) is to improve the timeliness and quality of information, and enable managers to be able to better understand the position of their firm as in comparison to competitors. Business intelligence applications and technologies can help companies to analyze changing trends in market share; changes in customer behavior and spending patterns; customers\u2019 preferences; company capabilities; and market conditions. Business intelligence can be used to help analysts and managers determine which adjustments are most likely to respond to changing trends. The emergence of the data warehouse as a repository, advances in data cleansing, increased capabilities of hardware and software, and the emergence of the web architecture all combine to create a richer business intelligence environment than was available previously. In this paper, an attempt has been made to present a framework for building a BI system.While the business world is rapidly changing and the business processes are becoming more and more complex making it more difficult for managers to have comprehensive understanding of business environment. The factors of globalization, deregulation, mergers and acquisitions, competition and technological innovation, have forced companies to re-think their business strategies and many large companies have resorted to Business Intelligence (BI) techniques to help them understand and control business processes\u00a0\u2026", "num_citations": "91\n", "authors": ["433"]}
{"title": "Information availability: An insight into the most important attribute of information security\n", "abstract": " This paper presents an in-depth understanding of Availability, which is one of the important pillars of Information Security and yet is not taken too seriously while talking about the security of an information system. The paper highlights the importance of Availability w.r.t. Security of information and the other attributes of security and also gives a realistic shape to the existing CIA triad security model. An in-depth understanding of the various factors that can impact the Availability of an information system (Software, Hardware and Network) is given. The paper also gives a categorization of the type of Availability that a system can have. The paper also explains the relation between Availability and other security attributes and also explains through what issues an information system may go while providing Availability.", "num_citations": "55\n", "authors": ["433"]}
{"title": "Effects of the disastrous pandemic COVID 19 on learning styles, activities and mental health of young Indian students-a machine learning approach\n", "abstract": " Word Health Organization declared coronavirus disease 2019 (COVID-19) as pandemic on 11 March 2020. After that on 14 March 2020 the Ministry of Home Affairs, India decided to treat COVID-19 as a \"notified disaster\" due to the spurt in the cases related to coronavirus in the country, leading to a complete shut down from 24 March 2020. This has affected all sectors of the country including the education sector. The near-total closure of schools, colleges, and universities has disrupted academic activities at various levels. The objective of this online survey study is to understand the day to day living, activities, learning styles, and mental health of young students of India during this unprecedented crisis and assess how they are adapting to the new e-learning styles and how they are managing their social lives.", "num_citations": "43\n", "authors": ["433"]}
{"title": "Improving Software Reliability using Software Engineering Approach- A Review\n", "abstract": " Software Reliability is an important facet of software quality. Software reliability is the probability of the failure free operation of a computer program for a specified period of time in a specified environment. Software Reliability is dynamic and stochastic. It differs from the hardware reliability in that it reflects design perfection, rather than manufacturing perfection. This article provides an overview of Software Reliability which can be categorized into: modeling, measurement and improvement, and then examines different modeling technique and metrics for software reliability, however, there is no single model that is universal to all the situations. The article will also provide an overview of improving software reliability and then provides various ways to improve software reliability in the life cycle of software development.", "num_citations": "39\n", "authors": ["433"]}
{"title": "Software reliability growth modeling with new modified weibull testing\u2013effort and optimal release policy\n", "abstract": " In software development life cycle, software testing is one of the most important tasks; and in the testing, software reliably is very important aspect for any category of software systems. A number of testing-effort functions for software reliability growth model based on non-homogeneous Poisson process (NHPP) have been proposed in the past. Although these models are quite helpful for software developers and have been widely accepted and applied in the industries and research centers, we still need to put more testing-effort functions into software reliability growth model for accuracy on estimate of the parameters. In this paper, we will consider the case where the time dependent behaviors of testingeffort expenditures are described by New Modified Weibull Distribution (NMWD). Software Reliability Growth Models (SRGM) based on the NHPP are developed which incorporates the (NMWD) testing-effort expenditure during the softwaretesting phase. It is assumed that the error detection rate to the amount of testing-effort spent during the testing phase is proportional to the current error content. Model parameters are estimated by Least Square and Maximum Likelihood estimation techniques, and software measures are investigated through numerical experiments on real data from various software projects. The evaluation results are analyzed and compared with other existing models to show that the proposed SRGM with (NMWD) testing-effort has a fairly better faults prediction capability and it depicts the real-life situation more faithfully. This model can be applied to a wide range of software system. In addition, the optimal release policy for this\u00a0\u2026", "num_citations": "36\n", "authors": ["433"]}
{"title": "Equivalence class partitioning and boundary value analysis-a review\n", "abstract": " The purpose of this paper is to carry out a detailed review on the plethora of information available on two testing techniques which fall under functional testing methodology. A detailed analysis of equivalence class partitioning and boundary value analysis has been carried out based wholly and solely on literature survey. These techniques have been comprehensively unfolded and also the strengths and weaknesses have been highlighted. This paper can be studied before carrying out any empirical study regarding the efficiency of these two testing techniques. Then, it's only after these analytical and empirical investigations that we can come up with some sort of solid framework to effectively compare these two testing techniques with each other and also with other testing techniques.", "num_citations": "35\n", "authors": ["433"]}
{"title": "Business intelligence: An integrated approach\n", "abstract": " Business intelligence systems combine operational and historical data with analytical tools to present valuable and competitive information to business planners and decision makers. The objective of Business intelligence (BI) is to improve the timeliness and quality of information, and enable managers to be able to better understand the position of their firm as in comparison to competitors. Business intelligence applications and technologies can help companies to analyze changing trends in market share; changes in customer behavior and spending patterns; customers' preferences; company capabilities; and market conditions. Business intelligence can be used to help analysts and managers determine which adjustments are most likely to respond to changing trends. The emergence of the data warehouse as a repository, advances in data cleansing, increased capabilities of hardware and software, and the\u00a0\u2026", "num_citations": "31\n", "authors": ["433"]}
{"title": "Decision support systems: concepts, progress and issues\u2013A review\n", "abstract": " Agriculture is a complex process of air, water, weather, soil, plants, animals and micro-organisms, which are uneven in distribution. Because of the uncertainty and risk associated with agricultural production \u2013 due to vulnerability in weather, variability in soil, infestation of pests and insects \u2013 some most important decisions based on \u201cwhat if\u201d depends on the existing knowledge base of current and future physical conditions like soil and climate, yield and prices, crop and area. If the past pattern and exact impact of agriculture-associated resources are known, one can predict the likely occurrences of certain crop-related attributes in advance, so that farmers can put together safeguards by tiding over the controllable attributes, which are likely to have serious impact on crop growth and yield. Thus the quality of decision-making can play an important role in complex and uncertain situations.             Empirical\u00a0\u2026", "num_citations": "29\n", "authors": ["433"]}
{"title": "Big data promises value: is hardware technology taken onboard?\n", "abstract": " Purpose                \u2013 The purpose of this paper is to explore the challenges posed by Big Data to current trends in computation, networking and storage technology at various stages of Big Data analysis. The work aims to bridge the gap between theory and practice, and highlight the areas of potential research.                                        Design/methodology/approach                \u2013 The study employs a systematic and critical review of the relevant literature to explore the challenges posed by Big Data to hardware technology, and assess the worthiness of hardware technology at various stages of Big Data analysis. Online computer-databases were searched to identify the literature relevant to: Big Data requirements and challenges; and evolution and current trends of hardware technology.                                        Findings                \u2013 The findings reveal that even though current hardware technology has not evolved with\u00a0\u2026", "num_citations": "25\n", "authors": ["433"]}
{"title": "Dovetailing of business intelligence and knowledge management: An integrative framework\n", "abstract": " The rapid advancement in Information and Communication Technology is driving a revolutionary change in the way organizations do business. The fast growing capabilities of both generating and collecting data has generated an imperative need for new techniques and tools that can intelligently and automatically transform the processed data into valuable information and knowledge for effective decision making. Business intelligence (BI) plays an important role extracting valuable information and discovering the hidden patterns in internal as well as external sources of data. The main purpose of the BI is to improve the knowledge with information that allows managers to make effective decisions to achieve organizational objectives. However majority of organizational knowledge is in unstructured form or in the minds of its employees. On the other hand, Knowledge Management (KM) encompasses both tacit and explicit knowledge to enhance s the organizations performance by providing collaborative tools to learn, create and share the knowledge within the organization. Therefore, it is imperative for the organizations to integrate BI with KM. The purpose of this paper is to discuss the importance of integration of BI with KM and provide a framework to integrate BI and KM.", "num_citations": "20\n", "authors": ["433"]}
{"title": "Identification of Kashmiri script in a bilingual document image\n", "abstract": " Script identification is a very important field in the area of pattern recognition & document image analysis. Commendable work has been proposed and implemented to recognize various common scripts in unilingual, bilingual and multilingual contexts. So far, diminutive work has been presented for Kashmiri script identification. In this paper, we are describing and experimentally testing our approach for identification of Kashmiri script with respect to English script which comprises a text document image. Two important and simple features are used for identification of scripts: Horizontal Profile Coefficients (Peaks) & Horizontal Profile Valleys.", "num_citations": "15\n", "authors": ["433"]}
{"title": "Effects of using filter based feature selection on the performance of machine learners using different datasets\n", "abstract": " Data preprocessing is a very important task in machine learning applications. It includes the methods of data cleaning, normalization, integration, transformation, reduction, feature extraction and selection. Feature selection is the technique for selecting smaller feature subsets from the superset of original features/attributes in order to avoid irrelevant and additional features/attributes in the dataset and hence increases the accuracy rate of machine learning algorithms. However, the problem exists when the further removal of such features results in the decrease of the accuracy rate. Therefore, we need to find an optimal subset of features that is neither too large nor too small from the superset of original features. This paper reviews different feature selection methods-filter, wrapper and embedded, that help in selecting the optimal feature subsets. Further, the paper shows effects of feature selection on different machine learning algorithms-NaiveBayes, RandomForest and kNN). The results have shown different effects on the accuracy rates while selecting the features at different margins.", "num_citations": "15\n", "authors": ["433"]}
{"title": "Analysis and Evaluating Security of Component-Based Software Development: A Security Metrics Framework.\n", "abstract": " Evaluating the security of software systems is a complex problem for the research communities due to the multifaceted and complex operational environment of the system involved. Many efforts towards the secure system development methodologies like secSDLC by Microsoft have been made but the measurement scale on which the security can be measured got least success. As with a shift in the nature of software development from standalone applications to distributed environment where there are a number of potential adversaries and threats present, security has been outlined and incorporated at the architectural level of the system and so is the need to evaluate and measure the level of security achieved. In this paper we present a framework for security evaluation at the design and architectural phase of the system development. We have outlined the security objectives based on the security requirements of the system and analyzed the behavior of various software architectures styles. As the component-based development (CBD) is an important and widely used model to develop new large scale software due to various benefits like increased reuse, reduce time to market and cost. Our emphasis is on CBD and we have proposed a framework for the security evaluation of Component based software design and derived the security metrics for the main three pillars of security, confidentiality, integrity and availability based on the component composition, dependency and inter component data/information flow. The proposed framework and derived metrics are flexible enough, in way that the system developer can modify the metrics according\u00a0\u2026", "num_citations": "15\n", "authors": ["433"]}
{"title": "After-deletion data recovery: myths and solutions\n", "abstract": " Data security is primarily concerned with restricting unauthorised access to information \u2013 both persistent and transient. Persistent information is more vulnerable to attack as the prey is always available to the predator. That is why the \u2018delete\u2019 key exists. However, simple file deletion is not enough to ensure data security.1 Operating systems give an illusion of file deletion by just invalidating the filename and stripping it of the allocated data blocks. This means the information residing within these data blocks exists even after file deletion. These data blocks may be allocated to some new file in future and, finally, may get overwritten with new information. Sometimes this time gap is beneficial as it allows recovery of files deleted accidentally. However, this time gap also allows malicious users to recover the confidential and sensitive information residing within deleted files.", "num_citations": "12\n", "authors": ["433"]}
{"title": "Review of FAT Data Structure of FAT32 file system\n", "abstract": " FAT file system is the most primitive, compatible and simple file system which still sustains in this era on digital devices, such as mini MP3 players, smart phones, and digital cameras. This file system is supported by almost all operating systems because of its simplicity and legacy. This paper presents review of the basic design technique, constraints and formulas of the most important and building block data structure of FAT32 file system; the FAT data structure.", "num_citations": "12\n", "authors": ["433"]}
{"title": "Information Integration for Heterogeneous Data Sources\n", "abstract": " Information Retrieval from heterogeneous information systems is required but challenging at the same as data is stored and represented in different data models in different information systems. Information integrated from heterogeneous data sources into single data source are faced upon by major challenge of information transformation-were in different formats and constraints in data transformation are used in data integration for the purpose of integrating information systems, at the same is not cost effective.This paper introduces ideaof Information integration based on search criteria from heterogeneous data sources into single data source. Every element of information source such as entity, field, and relation is mapped to component of new single text source-created every time heterogeneous information systems are searched and result is saved into new text file. This approach allows us to create new text file and delete existing file, modifiying wrapper, making modifications later and managing data retrieval in a simple unified style. This architecture is flexible enough to incorporate variety of data models and query capabilities by various protocols. It is possible to select logically tied information from all available legacy data sources.", "num_citations": "11\n", "authors": ["433"]}
{"title": "Data warehouse implementation of examination databases\n", "abstract": " A data warehouse is an asset for an enterprise and exists for the benefit of an entire enterpriseincluding business unit, individual customer, Student etc. Data in a data warehouse does not conform specifically to the preferences of any single enterprise entity. Instead, it is intended to provide data to the entire enterprise in such a way that all members can use the data in the warehouse throughout its lifespan [7]. This work explores using the star schema forAutomation of a Data Warehouse. An implementation of a data warehouse for an Examination Automation System is presented as an example.", "num_citations": "11\n", "authors": ["433"]}
{"title": "Restfs: Secure data deletion using reliable & efficient stackable file system\n", "abstract": " After deletion data recovery is trivial and can be performed by novice hackers. Secure deletion of data can be achieved by overwriting the file's metadata and user data during its deletion. We propose restFS, a reliable and efficient stackable file system, to fulfill the reliability and efficiency lacking in existing transparent per-file secure data deletion file system extensions. restFS is design compatible with all file systems which export block allocation map of a file to VFS and is currently implemented for EXT2 file system. Instead of overwriting at file level found in existing techniques, restFS overwrites at block level for reliability and efficiency. We evaluated the efficiency of restFS using Postmark benchmark and results indicate that restFS can save 28-98% of block overwrites which otherwise need necessarily to be overwritten in existing overwriting techniques. In addition to this, it can reduce the number of write commands\u00a0\u2026", "num_citations": "11\n", "authors": ["433"]}
{"title": "Evaluating various learning techniques for efficiency\n", "abstract": " Machine learning is a vast field and has a broad range of applications including natural language processing, medical diagnosis, search engines, speech recognition, game playing and a lot more. A number of machine learning algorithms have been developed for different applications. However no single machine learning algorithm can be used appropriately for all learning problems. It is not possible to create a general learner for all problems because there are varied types of real world datasets that cannot be handled by a single learner. In this paper we present an evaluation of various state-of-the-art machine learning algorithms using WEKA (Waikato Environment for Knowledge Analysis) for a real world learning problem-credit approval used in banks. First we provide a brief description about WEKA. After that we describe the learning problem and the dataset that we have used in our experiments. Later we explain the machine learning methods that we have evaluated. Finally we provide description about our experimental setup and procedure and discuss the conclusion and the result.", "num_citations": "10\n", "authors": ["433"]}
{"title": "A quick review of on-disk layout of some popular disk file systems\n", "abstract": " Disk file systems are being researched since the inception of first magnetic disk in 1956 by IBM. As such, many good disk file system designs have been drafted and implemented. Every file system design addressed a problem at the time of its development and efficiently mitigated it. The augmented or new designs rectified the flaws in previous designs or provided a new concept in file system design. As such, there are many file systems that have been successfully d in operating systems. Among these designs, some file systems have made an influential impact on the file system design because of their capability to cope up with change in hardware technology and/or user requirements or because of their innovation in file system ign or because time favored them which allowed them to find space in popular operating systems. In this paper, we vide a quick review of on-disk layout of some popular disk file systems across many popular platforms like Windows, Linux and Macintosh. The goal of this paper is to explore the on-disk layout of these file systems to identify the various layout policies and data structures they exploit which made them to be adapted by their native and other operating systems.", "num_citations": "10\n", "authors": ["433"]}
{"title": "Towards the Benchmarking of Routing Protocols for Adhoc Wireless Networks. 1\n", "abstract": " An ad-hoc network is a self-supporting collection of mobile nodes that happen to exist within a close proximity in an interval of time. The adhoc networks use a set of rules for transforming the data from one wireless station to another wireless station which we call as routing protocols. In recent years, many routing protocols have been developed, but the information about the evaluation of performance of these routing protocols is not fully understandable. In our paper we are going to propose a benchmark which will take atmost constraints ie (Network", "num_citations": "10\n", "authors": ["433"]}
{"title": "Design of novel QCA-based half/full subtractors\n", "abstract": " Quantum-dot cellular automata (QCA) is an emerging nanotechnology and a possible alternative to the related issues of traditional complementary metal oxide semiconductors. It offers advantages such as high device density, high operating speed and ultra-low power consumption compared with semiconductor transistor-based technologies. Adder and subtractor circuits are the fundamental arithmetic logic circuits used in digital systems. The performance of digital systems can directly affect the operation of adder and subtractor circuits. In this direction, several QCA digital approaches have been devised to increase the performance of adder and subtractor circuits. In this paper, novel half/full subtractors using a single-layer scheme of QCA cells are proposed. The proposed structures have the most beneficial features of low power consumption, reduced area, less latency (clock delays) and circuit complexity\u00a0\u2026", "num_citations": "9\n", "authors": ["433"]}
{"title": "Prediction of angiographic disease status using rule based data mining techniques\n", "abstract": " Data mining is the process of uncovering the fluctuating hidden patterns or trends in the data that is not immediately apparent by just summarizing the data. It can help in predicting the future (predictive analytics) in addition to explain the current or past situation (descriptive analytics). After the interpretation of information, knowledge can be extracted by identifying relationships among patterns. Various data mining (machine learning) algorithms have been provided for extracting the nuggets of knowledge from medical datasets in the field of diagnostics. This paper discusses various machine learning techniques that have been evaluated using heart disease dataset for the prediction of class ie angiographic disease status (diameter narrowing). The main aim is to search a model that accurately predicts the class of the unknown records. The evaluation has been performed using WEKA software tool that helps in comparing the various techniques on the basis of certain important evaluation measures.", "num_citations": "9\n", "authors": ["433"]}
{"title": "Information Translation: A Practitioners Approach\n", "abstract": " 20th century resulted in accumulation of two thingswires and data, while both brought enormous success to organization in specific and information technology in general, 21st century is all about management. Industry realized need to get rid of wires and integrate/manage data present everywhere around us. Fiber & Wi-Fi is replacement to wires; however data integration/management is still challenge at large because of varying underlying structure, format, operating system etc. In this paper we propose/introduce various methods of data transformation at application level without having to modifying underlying structure of data storage.", "num_citations": "9\n", "authors": ["433"]}
{"title": "Star Schema Implementation for Automation of Examination Records\n", "abstract": " Data warehousing systems enable enterprise managers to acquire and integrate information from heterogeneous sources and to query very large databases efficiently. Data in a data warehouse does not conform specifically to the preferences of any single enterprise entity. Instead, it is intended to provide data to the entire enterprise in such a way that all members can use the data in the warehouse throughout its lifespan [7]. This work explores using the star schema for Automation of a Data Warehouse. An implementation of a data warehouse for an Examination Automation System is presented as an example.", "num_citations": "9\n", "authors": ["433"]}
{"title": "Mapping cloud computing in university e-governance system\n", "abstract": " PurposeThe purpose of this paper is to propose a model to map the on-premise computing system of the university with cloud computing for achieving an effective and reliable university e-governance (e-gov) system.Design/methodology/approachThe proposed model incorporates the university\u2019s internal e-gov system with cloud computing in order to achieve better reliability, accessibility and availability of e-gov services while keeping the recurring expenditure low. This model has been implemented (and tested on a university e-gov system) in the University of Kashmir (UOK); case study of this implementation has been chosen as the research methodology to discuss and demonstrate the proposed model.FindingsAccording to the results based on practical implementation, the proposed model is ideal for e-governed systems as it provided adequate cost savings and high availability (HA) with operational ease, apart\u00a0\u2026", "num_citations": "8\n", "authors": ["433"]}
{"title": "Poster: Dr. watson provides data for post-breach analysis\n", "abstract": " Nowadays security systems have become highly sophisticated. However, breaches are inevitable. Nevertheless, post-breach analysis is performed to assess the severity of the breach and to trace the intruder's actions. This paper proposes drWatson, a layered file system that in case of an illegitimate file system access provides data for post-breach analysis to assess the severity of the breach and to trace the intruder's actions. drWatson, when mounted on top of any concrete file system, works by logging all the operations along with their date time stamps targeted to the below mounted file system.", "num_citations": "8\n", "authors": ["433"]}
{"title": "A Study of Analytically Improving the Reliability of Software\n", "abstract": " The reliability of an item/system is the probability that an item/system performs a specified function under specified operational and environmental conditions at and throughout a specified time quantitatively, reliability is probability of success. It differs from the hardware reliability in that it reflects design perfection, rather than manufacturing perfection. An attribute of resiliency and structural solidity. Reliability measures the level of risk and the likelihood of potential application failures. It also measures the defects injected due to modifications made to the software. The goal for checking and monitoring reliability is to reduce and prevent application downtime, application outages and errors that directly users, and enhance the image of IT and its impact on a company's business performance. This paper tends to explore only the literature review of software reliability and also provide an overview of improving it.", "num_citations": "8\n", "authors": ["433"]}
{"title": "Open Source Code Doesn't Always Help: Case of File System Development.\n", "abstract": " Purpose: One of the most significant and attractive features of Open Source Software (OSS), other than its cost, is its open source code. It is available in both flavours; system and application. It can be customized and ported as per the requirements of the end user. As most of the system software run in the kernel mode of operating system and system programmers constitute a small chunk of the programmers, the code customization of Open Source System Software is less realized practically. In this paper, the authors present file system development as a case of Kernel Mode System Software development and argue that customization of Open Source Code available for file systems is not preferred. To support the argument, the authors discuss various challenges that a developer faces in this process. Furthermore, the authors look into the user mode file system development for possible solution and discuss the architecture, advantages and limitations of most popular and widely used framework called File system in User-Space (FUSE). Finally, the authors conclude that the user mode alternative for file system development and/or extension supersedes kernel mode development. Design/Methodology/Approach: The broad domain, complexity, irregularity and limitations of kernel development environment are made as a base to put forth our argument. Moreover, the existence of rich and capable user-mode file system development frameworks are used to supplement the argument. Findings: The research highlights the fact that kernel mode file system development is difficult, bug prone, time consuming, exhaustive and so on, even with source code\u00a0\u2026", "num_citations": "8\n", "authors": ["433"]}
{"title": "A novel reversible logic gate and its systematic approach to implement cost-efficient arithmetic logic circuits using QCA\n", "abstract": " Quantum-dot cellular automata, is an extremely small size and a powerless nanotechnology. It is the possible alternative to current CMOS technology. Reversible QCA logic is the most important issue at present time to reduce power losses. This paper presents a novel reversible logic gate called the F-Gate. It is simplest in design and a powerful technique to implement reversible logic. A systematic approach has been used to implement a novel single layer reversible Full-Adder, Full-Subtractor and a Full Adder\u2013Subtractor using the F-Gate. The proposed Full Adder\u2013Subtractor has achieved significant improvements in terms of overall circuit parameters among the most previously cost-efficient designs that exploit the inevitable nano-level issues to perform arithmetic computing. The proposed designs have been authenticated and simulated using QCADesigner tool ver. 2.0.3.", "num_citations": "7\n", "authors": ["433"]}
{"title": "Entropy based script identification of a multilingual document image\n", "abstract": " Automatic Document Image Analysis has been a prime field of research in the past few decades. Script Identification is an essential part of automatic document image analysis. Script is essentially the text of a written document and languages are written using them. A huge set of techniques have been proposed and many scripts, foreign & domestic, have been identified. But so far, trivial work has been reported for the identification of Kashmiri script. In this paper we are proposing & experimentally testing identification of Kashmiri script collectively with three other related scripts viz. Roman, Devanagri & Urdu using entropy. First, a set of training images are experimented to prepare the knowledge base and later the actual samples have been evaluated. The proposed system offers an accuracy rate of 98.50%.", "num_citations": "7\n", "authors": ["433"]}
{"title": "Contrast enhancement and smoothing of CT images for diagnosis\n", "abstract": " Medical sciences currently rely on the imaging technology and post processing of these medical images in order to diagnose the diseases. The acquisition of such images may get affected by inherent noise which can lead to false perception of the diagnostic image. To perceive these images correctly the effective image enhancement techniques are the foremost requirement to improve the quality and level of perception. In this paper we apply various image enhancement techniques on CT images. Resultant images are inspected visually by radiologists for performance evaluation. Median filter removes noise and smoothen CT images effectively and outperforms other Filtering techniques. While as adaptive histogram equalization enhances the contrast and outperforms other contrast enhancement techniques.", "num_citations": "6\n", "authors": ["433"]}
{"title": "Deep learning for apple diseases: classification and identification\n", "abstract": " Diseases cause huge economic loss to the apple industry every year. Timely identification of these diseases is challenging for the farmers as the symptoms produced by different diseases may be similar and sometimes present simultaneously. This paper is an attempt to provide the timely and accurate identification of apple diseases from plant leaves. In this study, we propose a deep learning approach for identification and classification of apple diseases. First part of this study is dataset creation which includes data collection and labelling. Next, we train a convolutional neural network (CNN) model on the prepared dataset for automatic classification of apple diseases. CNNs are end-to-end learning algorithms which perform automatic feature extraction and learn complex features directly from raw images, making them suitable for a wide variety of computer vision tasks. The model parameters were initialised using\u00a0\u2026", "num_citations": "5\n", "authors": ["433"]}
{"title": "Performance augmentation of a FAT filesystem by a hybrid storage system\n", "abstract": " In this paper, we propose segregation and dispersal of hot-zone & cold-zone of a FAT filesystem over a hybrid-storage system for performance gains. Specifically, we propose hFAT, a high performance FAT32 filesystem design, that stores the most frequently accessed metadata of files on a solid-state storage drive while as actual contents on the magnetic drive. The idea is to eliminate the head positioning latency incurred by FAT filesystem operations while accessing metadata & userdata disk areas. After exercising the hFAT filesystem using Sprite LFS small-file benchmark, we found that hFAT design can reduce the latency incurred by FAT32 filesystem operations by a minimum of 25%, 10% and 90% during writing, reading and deleting a large number of small files respectively, if a solid-state storage device having latency lesser or equal to 10% of that of magnetic disk is used in addition.", "num_citations": "5\n", "authors": ["433"]}
{"title": "Generic Search Optimization for Heterogeneous Data Sources\n", "abstract": " Data Retrieval is still a pervasive challenge faced in applications that need to query across multiple autonomous and heterogeneous data sources. There is decent amount of standardization as far as World-Wide Web is concerned, while google is universal access tool to search and determine source of the information user requires there is still no such tool that can be implemented at enterprise level where there are multitude of data sources and organization users are still facing difficulty in accessing data available on the intranet of the organization and not on the WWW, in order to access such data users within the organizations need to know a lot including location, access techniques etc while still data consistency & redundancy is beyond the scope of common organization user/s.This paper introduces GENERIC SEARCH PRINCPLE: Solution making use of Knowledge base where in users of the organization irrespective of their technical ability, data source knowledge and location can search heterogeneous data sources including legacy data sources of organization and retrieve information, also taking into consideration user attributes like his/her location, work profile, designation etc so as to make search more relevant and results more precise.", "num_citations": "5\n", "authors": ["433"]}
{"title": "Design Considerations for Developing Disk File System\n", "abstract": " File system design has never been a straight forward task. Moreover, designing and developing a disk file system is a complex case of file system development. From time to time, since the inception of first magnetic disk in 1956, many disk file systems were drafted and implemented to fit the need of users and/or to cope up with the change in hardware technology. This has resulted into many objective specific disk file systems and hence, no generalized design guidelines or criteria have been developed. In this paper, we take some historical facts and current trends in digital world as a base to figure out 3 basic design parameters for designing and developing a disk file system which are affected by the change in hardware technology and user requirements. In each identified design parameter, we give a brief introduction of some novel approach to mitigate the parameter. Furthermore, we also introduce a new file system benchmarking technique to overcome problems found in existing techniques. The goal of this paper is to organize the design considerations for developing a disk file system and thus, help a file system designer to efficiently design and develop a new file system from scratch or refine or fine tune existing ones.", "num_citations": "5\n", "authors": ["433"]}
{"title": "Software reliability simulation: Process, approaches and methodology\n", "abstract": " Reliability is probably the most crucial factor to put ones hand up for in any engineering process. Quantitatively, reliability gives a measure (quantity) of quality, and the quantity can be properly engineered using appropriate reliability engineering process. Software Reliability Modeling has been one of the much-attracted research domains in Software Reliability Engineering, to estimate the current state as well as predict the future state of the software system reliability. This paper aims to raise awareness about the usefulness and importance of simulation in support of software reliability modeling and engineering. Simulation can be applied in many critical and touchy areas and enables one to address issues before they these issues become problems. This paper brings to fore some key concepts in simulation-based software reliability modeling. This paper suggests that the software engineering community could exploit simulation to much greater advantage which include cutting down on software development costs, improving reliability, narrowing down the gestation period of software development, fore-seeing the software development process and the software product itself and so on.", "num_citations": "5\n", "authors": ["433"]}
{"title": "Script identification: a review\n", "abstract": " Humans have been able to express their inner self through various means like speaking and writing. Languages, which form a tool for communication, have been used in different civilizations for expression. Languages, which are spoken as well as written, have been written using symbols which form the script. Since the advent of computers automated documents are being produced which chiefly contains the text written in one or more scripts. As the amount of these documents grows their automatic processing becomes inevitable. One of the key aspects of this automatic processing is the identification of the scripts with which the documents are written known as script identification. Since many decades now various script identification systems have been developed which work for various types of scripts and under various scenarios as per the requirements of the users. In this piece of work, an effort has been\u00a0\u2026", "num_citations": "4\n", "authors": ["433"]}
{"title": "Genetic Algorithm for Biomarker Search Problem and Class Prediction\n", "abstract": " In the field of optimization, Genetic Algorithm that incorporates the process of evolution plays an important role in finding the best solution to a problem. One of the main tasks that arise in the medical field is to search a finite number of factors or features that actually affect or predict the survival of the patients especially with poor prognosis disease, thus helping them in early diagnosis. This paper discusses the various steps that are performed in genetic algorithm and how it is going to help in extracting knowledge out of high dimensional medical dataset. The more the attributes or features, the more difficult it is to correctly predict the class of that sample or instance. This is because of inefficient, useless, noisy attributes in the dataset. So, here the main aim is to search the features or genes that can strongly predict the class of subject (patient) ie healthy or cancerous and thus help in early detection and treatment.", "num_citations": "4\n", "authors": ["433"]}
{"title": "A literature Survey of Image Denoising Techniques in the Spatial Domain\n", "abstract": " Noise removal is a challange faced by the research community around the globe. Since its inception, a lot of work has been carried out to identify and remove noise from signals/images. Researchers have attempted procedures in various domains to better the performance in noise removal. Although, generally classified into two domains viz. Spatial domain and frequency domain, image denoising techniques have been studied in length and breadth to get better results. This paper attemps to provide a literature survey of denoising techniques focussing on spatial domain denoising techniques, later to be followed by survey in other domains.", "num_citations": "4\n", "authors": ["433"]}
{"title": "Density based script identification of a multilingual document image\n", "abstract": " Automatic Pattern Recognition field has witnessed enormous growth in the past few decades. Being an essential element of Pattern Recognition, Document Image Analysis is the procedure of analyzing a document image with the intention of working out the contents so that they can be manipulated as per the requirements at various levels. It involves various procedures like document classification, organizing, conversion, identification and many more. Since a document chiefly contains text, Script Identification has grown to be a very important area of this field. A Script comprises the text of a document or a manuscript. It is a scheme of written characters and symbols used to write a particular language. Languages are written using scripts, but script itself is made up of symbols. Every language has its own set of symbols used for writing it. Sometimes different languages are written using the same script, but with marginal modification. Script Identification has been performed for unilingual, bilingual and multilingual document images. But, negligible work has been reported for Kashmiri script. In this paper, we are analyzing and experimentally testing statistical approach for identification of Kashmiri script in a document image along with Roman, Devanagari & Urdu scripts. The identification is performed on offline machine-printed scripts and yields promising results.", "num_citations": "4\n", "authors": ["433"]}
{"title": "Comparative study of streaming data mining techniques\n", "abstract": " In order to extract fresh knowledge out of the data present in a data warehouse, a wide range of knowledge discovery techniques have been provided that process the data in multiple passes. But nowadays, we are facing a challenge of handling massive data in a proper and timely manner so as to extract useful information (knowledge) from streaming data. Such massive streaming data cannot be stored in our limited storage and due to its continuous flow we need to process it in single pass. Various algorithms have been provided in order to perform the single pass extraction of knowledge from streaming data; however, no single data mining algorithm can be used applicably for all the problems because of the different kinds of real data sets or synthetic data sets. This paper discusses various streaming data mining techniques and compares the algorithms taking into consideration some evaluation measures in an\u00a0\u2026", "num_citations": "4\n", "authors": ["433"]}
{"title": "ONVAREF: A decision support system for onion varietal reference\n", "abstract": " This paper presents the development of a decision support system (DSS) called ONVAREF for screening of new onion varieties being released. The system is a windows oriented, user-friendly, database driven, having excellent graphical user interface (GUI), designed using 4GL, which works on searching and matching principle to spawn large database comprising of dozens of characteristics suitable for judging originality of a variety under screening. It is also capable of assessing closeness of existing varieties with the one being screened. The accuracy of decision making process provided by ONVAREF has been found 93.33% as compared to 53.33% by manual screening. It is 123% time efficient as compared to paper based assessment manual from which the computer programme was evolved.       Key words: Decision support system, modeling, database driven decision support system (DSS), database\u00a0\u2026", "num_citations": "4\n", "authors": ["433"]}
{"title": "suvfs: A virtual file system in userspace that supports large files\n", "abstract": " As a consequence of voluminous growth and proliferation of digital data, the file system size and count limitations have become problematic. The fact is that file systems use fixed length fields within their metadata structures to keep track of volume size, file size and file count, and hence need design (and source) modification to cope up with this growth. In this paper, we propose a Scalable User-space Virtual File System, namely suvfs, which when mounted on top of any file system extends its capability to store and process large files not natively supported by the file system. It works by exploiting the concept of virtual unification to present a large virtual file that spans over a number of legitimate sized physical files. It does so without modifying user applications, system libraries, system calls and even file systems. We implemented it using FUSE framework & evaluated it for performance overhead added by layering it\u00a0\u2026", "num_citations": "4\n", "authors": ["433"]}
{"title": "User Desired Information Translation\n", "abstract": " With the advent of computerization primary goal of organization across the globe was automation of their system, this result in massive collection of data in respective of organization business logic and process, not much was thought about integration of application and data. Once a blessing became huge problem in organizations, data all over the organization was becoming difficult to manage and inconsistency of data resulted in creation of team not meant for development but data management.Many organizations have started reinvesting in data management in the form of creation of Data Warehouse and again organisation across the globe are not stressing upon user needs and demands but only focusing on integration of heterogeneous data sources with goal of making data centralised and consistent by creating Warehouse.", "num_citations": "4\n", "authors": ["433"]}
{"title": "Open Source Systems and Engineering: Strengths, Weaknesses and Prospects\n", "abstract": " Purpose: This paper reviews the open source software systems (OSSS) and the open source software engineering with reference to their strengths, weaknesses and prospects. Though, it is not possible to spell out the better of the two software engineering processes, the paper outlines the areas where the open source methodology holds edge over conventional closed source software engineering. Then, the weaknesses are also highlighted, which tilt the balance the other way. Design/Methodology/Approach: The study is based on the works carried out earlier by the scholars regarding the potentialities and shortcomings of OSSS. Findings: A mix of strengths and weaknesses make it hard to pronounce open source as the panacea. However, the open source does have very promising prospect; owing to its radical approach to the established software engineering principles, it has spectacularly managed to carve a \u201cmainstream\u201d role, that too in just over a few decades.", "num_citations": "4\n", "authors": ["433"]}
{"title": "A brief summary of file system forensic techniques\n", "abstract": " A computer needs a method for long term storage of data which is provided by secondary storage device. File System consists of data structures & algorithms to store, update, delete & retrieve data residing in secondary storage device in a hierarchy of files and directories. File System layout & data structures contain enough information for forensic analysts to trace the activities that might have yielded a crime which are not apparently visible and hence can produce the evidence in current digital computer forensics. In this paper, we summarize in brief some of the well established file system forensic techniques.", "num_citations": "4\n", "authors": ["433"]}
{"title": "Benchmarking criteria for file system benchmarks\n", "abstract": " Comparing products based solely on technical merits and specifications is rarely useful for predicting actual performance, so the most common approach is to empirically evaluate their performance using some workload and performance gathering tool. Benchmarking file systems is a process in which some specific workload is run on a specific system in order to get performance data. This way file system performance is evaluated accurately. However, the benchmarks proposed in file system research papers suffer from several problems. Although many design criteria have been proposed for development of good file system benchmarks, very few benchmarks follow them completely. In this paper, we try to summarize all the generally accepted design criteria proposed in various research papers for file system benchmarks based on the various problems that they had identified. The goal of this paper is to remove the\u00a0\u2026", "num_citations": "4\n", "authors": ["433"]}
{"title": "DWT based color image watermarking: a review\n", "abstract": " In today\u2019s knowledge driven society and technology driven economy, when data is considered to be one of the most important corporate resource of an organization, equally important is the security of the same to prevent unauthorized users to access and manipulate it. Watermarking being an appropriate solution for claiming authorization has prompted many researchers to work in this direction and accordingly a substantial amount of research work has been done using different schemes on various\u00a0media\u00a0types. The main impetus in writing this paper is to provide a broader view as how much work has been carried so far and what are the different dimensions that have been taken into consideration to watermark color images using discrete wavelet transformation. The paper will help prospective\u00a0researchers to gain more insight about the work that has been done and pave a way for them to design and implement\u00a0\u2026", "num_citations": "3\n", "authors": ["433"]}
{"title": "Emerging Role of Artificial Intelligence for Disaster Management Based on Microblogged Communication\n", "abstract": " Catastrophic disasters are striking more often in recent years forcing researchers to put a lot of emphasis on managing disaster information and establishing the state-of-art technologies to minimize the losses. During any natural or man-made disaster, microblogging platforms are increasingly being used to share time-sensitive information. In recent years, these platforms have become an important source to gather news about the current situation, supply of food and medicines and checking on family and friends. Artificial Intelligence (AI) based information retrieval systems serve as a novel way of gaining situation awareness and planning for relief operations during disasters. This paper summarizes various AI techniques that are being applied to process the data posted on popular microblogging platforms at the time of disaster. Case studies of few disasters in the last decade are presented which clearly emphasize that the advances in social networking platforms and information and communication technologies have created enormous opportunities for circulation and consumption of time critical information during emergencies. This study clearly illustrates the impact of the application of new and emerging AI technologies in managing disasters.", "num_citations": "3\n", "authors": ["433"]}
{"title": "Software Cost Estimation Based on the Hybrid Model of Input Selection Procedure and Artificial Neural Network\n", "abstract": " Software effort estimation is the forecasting of development effort and development time needed to develop any software project. It is considered to be the very primary step of software development process and at the same time considered to be the key task as accurate assessments of growth of the current project, its delivery exactness and its cost control can only be achieved once desired estimation is accurate. And at broader perspective an accurate estimation of a currently developing software product will result in landing the organization in a better schedule of its futuristic software projects too. With due above reason, software effort estimation has received a considerable amount of attention of many researchers from past so many decades. In this paper, software cost estimation is done by first performing a proposed input selection procedure to get the relevant set of cost drivers and leaving behind the irrelevant\u00a0\u2026", "num_citations": "3\n", "authors": ["433"]}
{"title": "Deep Domain Adaptation Approach for Classification of Disaster Images\n", "abstract": " In the last decade, emergency responders, government organizations and disaster response agencies have certainly acknowledged that microblogging platforms such as Twitter and Instagram can be an important source of actionable information at the time of disaster. However, researchers feel that analyzing social media data for disasters using supervised machine learning algorithms is still a challenge. During the first few hours of any disaster when labeled data is not available, the learning process gets much delayed. The first objective of this study is to explore the domain adaptation techniques by making use of labeled data of some previous disaster along with the abundance of unlabeled data that is available for the current disaster. The second objective is to apply these domain adaptation techniques on disaster-related imagery data from the microblogging platforms since imagery data has largely been unexplored as compared to textual content. To achieve these objectives, domain adaptation methods would be applied to classify the images of an ongoing disaster as informative versus non-informative. This study, for the first time, proposes a semi-supervised domain adaptation technique where the classifier is trained on three types of data, labeled data of the previous disaster, unlabeled data of current disaster and a small batch of labeled data of current disaster. Our experiments have been performed on Twitter images corresponding to three disasters. The experimental results show that there is an improvement in the accuracy of the classification model if a small batch of labeled target images is also added along with the unlabeled\u00a0\u2026", "num_citations": "2\n", "authors": ["433"]}
{"title": "Software Testing Approach for Cloud Applications (STACA)\u2013Methodology, Techniques & Tools\n", "abstract": " As earliest the software defects uncovered and fixed in STLC, the lesser the amount required to fix it. With the advent of cloud computing a lot of new opportunities for business opens, especially in the field of software testing & maintenance. The cloud testing methodology is the set of techniques, tools and process to be followed while undergoing tests for cloud service. A new methodology is proposed to test the cloud which is known as SUPerB methodology, deals with cloud security, user acceptance, performance and business requirements. Successful implementation of SUPerB methodology for cloud testing might establish a strong foundation for an organization to lead in the market. Software vulnerabilities give rise to the cyber-crime and related risk associated with it just due to lapses in security policies, which increases the security breaches in the business. Efficiency of a system can be measured in terms of\u00a0\u2026", "num_citations": "2\n", "authors": ["433"]}
{"title": "An improved particle swarm optimisation-based functional link artificial neural network model for software cost estimation\n", "abstract": " Software cost estimation is the forecast of development effort and time needed to develop a software project. Estimating software cost is endlessly proving to be a difficult problem and thus catches the attention of many researchers. Recently, the usage of meta-heuristic techniques for software cost estimation is increasingly growing. In this paper, we are proposing a technique consisting of functional link artificial neural network model and particle swarm optimisation algorithm as its training algorithm. Functional link artificial neural network is a high order feedforward artificial neural network consisting of an input layer and an output layer. It reduces the computational complexity and has got the fast learning ability. Particle swarm optimisation does optimisation by iteratively improving a candidate solution. The proposed model has been evaluated on promising datasets using magnitude of relative error and its median as\u00a0\u2026", "num_citations": "2\n", "authors": ["433"]}
{"title": "Structure identification and IO space partitioning in a nonlinear fuzzy system for prediction of patient survival after surgery\n", "abstract": " PurposeAs far as the treatment of most complex issues in the design is concerned, approaches based on classical artificial intelligence are inferior compared to the ones based on computational intelligence, particularly this involves dealing with vagueness, multi-objectivity and good amount of possible solutions. In practical applications, computational techniques have given best results and the research in this field is continuously growing. The purpose of this paper is to search for a general and effective intelligent tool for prediction of patient survival after surgery. The present study involves the construction of such intelligent computational models using different configurations, including data partitioning techniques that have been experimentally evaluated by applying them over realistic medical data set for the prediction of survival in pancreatic cancer patients.Design/methodology/approachOn the basis of the\u00a0\u2026", "num_citations": "2\n", "authors": ["433"]}
{"title": "METRIC FOR EVALUATING AVAILABILITY OF AN INFORMATION SYSTEM: AQuantitative APPROACH BASED ON COMPONENT DEPENDENCY\n", "abstract": " The purpose of the paper is to present a metric for availability based on the design of the information system. The availability metric proposed in this paper is twofold, based on the operating program and network delay metric of the information system (For the local bound component composition the availability metric is purely based on the software/operating program, for the remote bound component composition the metric incorporates the delay metric of the network). The aim of the paper is to present a quantitative availability metric derived from the component composition of an Information System, based on the dependencies among the individual measurable components of the system. The metric is used for measuring and evaluating availability of an information system from the security perspective, the measurements may be done during the design phase or may also be done after the system is fully functional. The work in the paper provides a platform for further research regarding the quantitative security metric (based on the components of an information system ie user, hardware, operating program and the network.) for an information system that addresses all the attributes of information and network security.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Artificial Bee Colony-Trained Functional Link Artificial Neural Network Model for Software Cost Estimation\n", "abstract": " Software cost estimation is forecasting the amount of developmental effort and time needed, while developing any software system. A good volume of software cost prediction models ranging from the very old algorithmic models to expert judgement to non-algorithmic models have been proposed so far. Each of these models has their advantage and disadvantage in estimating the development effort. Recently, the usage of meta-heuristic techniques for software cost estimations is increasingly growing. So in this paper, we are proposing an approach, which consists of functional link ANN and artificial bee colony algorithm as its training algorithm for delivering most accurate software cost estimation. FLANN reduces the computational complexity in multilayer neural network, and does not has any hidden layer, and thus has got fast learning ability. In our model, we are using MRE, MMRE and MdMRE as a\u00a0\u2026", "num_citations": "2\n", "authors": ["433"]}
{"title": "Accelerated dynamic programming on gpu: a study of speed up and programming approach\n", "abstract": " GPUs (Graphics processing units) can be used for general purpose parallel computation. Developers can develop parallel programs running on GPUs using different computing architectures like CUDA or OpenCL. The Optimal Matrix Chain Multiplication problem is an optimization problem to find the optimal order for multiplying a chain of matrices. The optimal order of multiplication depends only on the dimensions of the matrices. It is known that this problem can be solved by dynamic programming technique using O (n3)-time complexity algorithm and a work space of size O (n2). The main contribution of this paper is to present a parallel implementation of this O (n3)-time algorithm on a GPU and to assess the amount of programming effort required to develop this parallel implementation when compared to a similar sequential implementation.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Understanding and mitigating security issues in sun NFS\n", "abstract": " Sun NFS is a simple, efficient and elegant way to access files residing on a remote server from a wide variety of clients. However, due to it being split into three pieces (client, protocol and server) it is vulnerable to security breaches.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Morphological Analysis from the Raw Kashmiri Corpus Using Open Source Extract Tool\n", "abstract": " Purpose: Morphological information is a key part when we consider the design of any machine translation engine, any information retrieval system or any natural language processing application. It is important to investigate how lexicon development can be automated maintaining the quality which makes it of use for the applications, since manual development can be highly time consuming task. The paper describe how we can simply provide the extraction rules along with raw texts which can guide the computerized extraction of morphological information with the help of the extract tool like Extract v2. 0. Design/methodology/approach: We used Extract v2. 0 which is an open source tool for extracting linguistic information from raw text, and in particular inflectional information on words based on the word forms appearing in the text. The input to the Extract is a file containing, an un-annotated Kashmiri corpus and a file containing the Extract rules for the language. The tools output is the list of analyses; each analysis consists of a sequence of words annotated with a identifier that describes some linguistic information about the word. Findings: The study includes the fundamental extraction rules which can guide the Extract tool v2. 0 to extract the inflectional information and help in the development of a full lexicon that can be use for developing different applications in the natural language applications. The major contributions of the study are:\u2022 Orthography component: A Unicode Infrastructure to accommodate Perso-Arabic script of Kashmiri.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Adaptive hybrid pos cache based semantic language model\n", "abstract": " This paper presents a language model as an improvement over the stochastic language model for developing a syntactic structure based on word dependencies in local and non local domain. The model copes with the issues of limited amount of training material and the exploitation of the linguistic constraints of the language. The proposed model is a dynamic probabilistic model which uses word dependencies based on their part of speech tags along with the tri-gram Model but also takes care of the influence of the word which are very far from the word being considered in a text and stores the word history in a dynamic cache for information mining using long distance dependency. The model based on second order Hidden Markov Model has been used and an improvement of 2% has been observed in the word error rate and 4% reduction in the perplexity when compared to the normal tri-gram model.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Data Warehouse Implementation of Examination Databases\n", "abstract": " A data warehouse is a relational database that is designed for query and analysis rather than for transaction processing. It usually contains historical data derived from transaction data, but it can include data from other sources. In addition to a relational database, a data warehouse environment includes an extraction, transportation, transformation, and loading (ETL) solution, an online analytical processing (OLAP) engine, client analysis tools, and other applications that manage the process of gathering data and delivering it to business users [2][10]. This was the case at University of Kashmir (UOK) Examination Department, where such a project brought together various attributes of Examination System which included Conduct, Secrecy, Transit, Tabulation, Accounts and other related data sources in to an integrated data warehouse.", "num_citations": "2\n", "authors": ["433"]}
{"title": "On Way to Acquiring Reliability Growth in Software Systems\n", "abstract": " Reliability of a software system has been one of the driving forces for the various software engineering processes and methodologies that led to their evolution and sophistication. The concerns for reliability of a software system surface very early on during the development phases of the software system. When it comes to acquisition of reliability, we should not immediately get model-oriented; instead every minutia of software development life cycle should be given its due.This paper outlines the areas where reliability needs to pick up. This paper underlines the fact that it is only through carefully controlled and carefully applied software engineering process that software reliability growth can be achieved. It emphasizes on the acquisition of reliability of software systems as soon as the conception phase. We trace the reliability concerns from the early stage to the fully functional stage. The terms \u201chard reliability\u201d and \u201csoft reliability\u201d are used.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Choosing between windows and Linux file systems for a novice user\n", "abstract": " Windows and Linux operating systems have dominated personal computers which are being used for wide variety of applications. Novice users form a considerably large portion of the PC users and the file system is one of the most common part of operating system interacted by such users. File system is an essential part of an operating system which dictates overall system performance and application specific performance. Thus, evaluating file systems is necessary to help novice user choose an appropriate file system as per his needs. Although researchers have evaluated file systems and compared the results, but they take much technical aspects into consideration which may deviate from the aspects a novice user is interested in and hence, the conclusion may not be appropriate for a novice user. In this paper, we evaluate the performance of two Windows native file systems viz. FAT32 and NTFS and two Linux native file systems viz. Ext2 and Ext3 under Linux operating system keeping in view the interests of a novice user.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Efficient handling of large storage: a comparative study of some disk file systems\n", "abstract": " Since the invention of magnetic disk, many changes have been made to its hardware technology to increase its performance and capacity. Although disk performance has improved but is relatively less as compared to continuous increase in disk capacity. As such, the design and development of new disk file system and refinement of old ones will need to focus on efficient store and retrieval of data for large storage disks. In this paper, we try to compare some common disk file systems on the basis of Storage Limits, File System Metadata, File System Characteristics & Allocation Strategy to point out the file system that can serve as a template for future file systems and old ones to support large storage capacities.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Some notable reliability techniques for disk file systems\n", "abstract": " File system operations include data operations and metadata operations. Data operations act upon actual user data while metadata operations modify the structure of the file system, like creating, deleting, or renaming files, directories, etc. During a metadata operation, the system must ensure that data are written to disk in such a way that the file system can be recovered to a consistent state after a system crash. In this paper we look at some most notable techniques which ensure reliability of disk file systems against system crashes and failures.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Framework for Automation of Cloud-Application Testing using Selenium (FACTS)\n", "abstract": " A framework is an amalgamation of integrated tools, libraries, utilities and its coordination to interact with the other automation components. The motive of designing a test automation framework is to implement uniform standards towards automation throughout the organization to achieve the desired outcomes. Test automation framework is required to maintain the standardization of the activities performed to manipulate the operations (addition, modification and deletion) on the test scripts and functions easily, to provide the scalability and reliability. Tools, process and testing team are the three key player of test automation. The proposed framework is based on the consideration of test environment (includes language binding, IDE, automation tools and testing framework). Selenium WebDriver is used as web application automation framework to execute test across multiple browsers that supports multiple operating system with variant programming language. Selection of WebDriver automation tool for this framework is due to their internal architecture to directly communicate with browser for fast execution. The combination of test automation tools (ie Java, Eclipse, Selenium and TestNG) makes JEST for designing test automation framework. Both Positive and negative test must be performed to verify functionalities of applications to handle unusual exceptions.", "num_citations": "2\n", "authors": ["433"]}
{"title": "Emerging Role of Intelligent Techniques for Effective Detection and Prediction of Mental Disorders\n", "abstract": " It has been established and accepted that mental disorders pose one of the most prominent health challenges worldwide. Information retrieval from mental health data which may be explicit in electronic health records and clinical notes or may be implicit in social media postings has vast potential to detect, distinguish and diagnose the status of mental health of individuals and aid in managing this problem. This paper summarizes some recent studies that apply the state of art Artificial Intelligence (AI) techniques to mental health data. The paper summarizes that newly emerging AI technologies hold a decent promise and can be leveraged to predict, assist in the diagnosis and management of mental disorders. The role of AI in this area becomes particularly important in a scenario where there is a worldwide dearth of qualified professionals who can deal with mental health disorders, where the cost of these services is high and the people suffering from these problems often refrain from availing these services due to social stigma associated with it.", "num_citations": "1\n", "authors": ["433"]}
{"title": "Imaging Techniques for Cancer Diagnosis and Scope for Enhancement\n", "abstract": " Imaging techniques are used to create images of structure, function and pathology of human body organs for cancer diagnosis. Various imaging techniques like X-ray, Positron Emission Tomography (PET), Ultrasound (US), MRI etc. are used for cancer diagnosis. These imaging techniques have gone through Lot of advancements during last few years. These techniques vary in the technology and application. Various artifacts exist in these imaging techniques and images produced by theses imaging techniques. These artifacts can be exploited to enhance the imaging technique and the images produced by them.", "num_citations": "1\n", "authors": ["433"]}
{"title": "A First Hand Survey of Frequency Domain Denoising Algorithms and Techniques\n", "abstract": " World over, research community is facing the challenge of noise removal in multimedia viz. Images, audio, video etc. Attempts have been made, since long, to identify and remove this noise. Both, Spatial as well as Frequency domains have been thoroughly explored to devise newer and better techniques to remove these foreign bodies from the original signals. This paper is a step to carry forward the literature survey into frequency domain from the previous paper that focused on the spatial domain. It is further expected to be followed by literature survey on denoising in the Artificial Neural Network (ANN) context. The intent is to have a comprehensive survey on image denoising and use it for designing new techniques and extend existing ones to further their performance.", "num_citations": "1\n", "authors": ["433"]}
{"title": "Scaling up for the streaming data\n", "abstract": " Knowledge has always been the success factor for any organization (business/technical). Survey 2012 shows that every day about 2.5 quintillion (2.5\u00d7 1018) bytes of data were created. As a result we are facing a challenge of handling such voluminous, potentially infinite, fast changing, temporally ordered data streams in a proper and timely manner so as to extract useful knowledge from that. However, due to its tremendous volume, we cannot store the whole of the streaming data in our limited or finite storage and due to its continuous flow we have to process it in a single pass, in contrast to the warehoused data where we could go through the data in multiple passes. In addition to this, we have to work in a limited amount of time. So, time and space are the important aspects that are taken into consideration while handling the streams of data. This paper discusses and compares those issues in the light of some sketching and counting algorithms and provides application oriented data-flow architecture for processing the streaming data along with the Granularity based approach that takes into consideration the resource awareness and adaptation for data stream mining algorithms. Further, since Analysts are mostly interested either in the recent data or in the broader view of the data, so this paper discusses a dynamic H-cube to facilitate multi-resolution analysis of streaming data wherein the Partial materialization is performed and computations are done on the fly using a tilted time frame.", "num_citations": "1\n", "authors": ["433"]}
{"title": "Examining a Pipelined Approach for Information Extraction with respect to machine learning\n", "abstract": " Pipelining is a process in which a complex task is divided into many stages that are solved sequentially. A pipeline is composed of a number of elements(processes, threads, co routines, etc.), arranged in such a way so that the output of each element is fed as input to the next in the sequence. Many machine learning problems are also solved using a pipeline model. Pipelining plays a very important role in applying the machine learning solutions efficiently to various natural language processing problems. The use of pipelining results in the better performance of these systems. However, these systems usually result in considerable computational complexity. For this reason researchers were motivated for using active learning for these systems. Reason of using active learning is that these algorithms perform better than the traditional learning algorithms keeping the training data same. In this paper we discuss an\u00a0\u2026", "num_citations": "1\n", "authors": ["433"]}
{"title": "Towards Understanding Theoretical Developments in Natural Language Processing\n", "abstract": " Natural Language Processing (NLP) is that field of computer science which consists of interfacing computer representations of information with natural languages used by humans. It examines the use of computers in understanding and manipulating the natural language text and speech. The main aim of the researchers in this field is to collect the necessary details about how natural languages are being used and understood by humans. They use these details to develop the tools for making the computers understand and manipulate the natural languages to perform the desired tasks. In this paper we describe some of the theoretical developments that have influenced research in NLP. We also discuss automatic abstracting and information retrieval in natural language processing applications. We conclude with a discussion on Natural Language Interfaces, NLP software and the future research in NLP.", "num_citations": "1\n", "authors": ["433"]}
{"title": "Io bound property: a system perspective evaluation and behavior trace of file system\n", "abstract": " File systems have been mostly benchmarked as per the application perspective. This approach hides all the underlying complexities of the system including the actual I/O being done with the secondary storage device like magnetic disk. The IO bound property of a file system is necessarily to be evaluated because the most dominant performance limiting factor of a file system is its I/O operation with secondary storage device. This IO bound property of file system dictates the quantity and frequency of IO that a file system does with secondary storage device. In this paper, we argue system perspective of file system benchmarks and develop a benchmark to evaluate some common disk file systems for IO bound property. The goal of this paper is to better understand the behavior of file systems and unveil the low level complexities faced by file systems.", "num_citations": "1\n", "authors": ["433"]}