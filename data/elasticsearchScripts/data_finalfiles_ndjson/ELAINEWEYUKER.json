{"title": "Selecting software test data using data flow information\n", "abstract": " This paper defines a family of program test data selection criteria derived from data flow analysis techniques similar to those used in compiler optimization. It is argued that currently used path selection criteria, which examine only the control flow of a program, are inadequate quate. Our procedure associates with each point in a program at which a variable is defined, those points at which the value is used. Several test data selection criteria, differing in the type and number of these associations, are defined and compared.", "num_citations": "1509\n", "authors": ["148"]}
{"title": "Evaluating software complexity measures\n", "abstract": " A set of properties of syntactic software complexity measures is proposed to serve as a basis for the evaluation of such measures. Four known complexity measures are evaluated and compared using these criteria. This formalized evaluation clarifies the strengths and weaknesses of the examined complexity measures, which include the statement count, cyclomatic number, effort measure, and data flow complexity measures. None of these measures possesses all nine properties, and several are found to fail to possess particularly fundamental properties; this failure calls into question their usefulness in measuring synthetic complexity.< >", "num_citations": "1215\n", "authors": ["148"]}
{"title": "Computability, complexity, and languages: fundamentals of theoretical computer science\n", "abstract": " Computability, Complexity, and Languages is an introductory text that covers the key areas of computer science, including recursive function theory, formal languages, and automata. It assumes a minimal background in formal mathematics. The book is divided into five parts: Computability, Grammars and Automata, Logic, Complexity, and Unsolvability. Computability theory is introduced in a manner that makes maximum use of previous programming experience, including a\" universal\" program that takes up less than a page. The number of exercises included has more than tripled. Automata theory, computational logic, and complexity theory are presented in a flexible manner, and can be covered in a variety of different arrangements.", "num_citations": "817\n", "authors": ["148"]}
{"title": "An applicable family of data flow testing criteria\n", "abstract": " The authors extend the definitions of the previously introduced family of data flow testing criteria to apply to programs written in a large subset of Pascal. They then define a family of adequacy criteria called feasible data flow testing criteria, which are derived from the data-flow testing criteria. The feasible data flow testing criteria circumvent the problem of nonapplicability of the data flow testing criteria by requiring the test data to exercise only those definition-use associations which are executable. It is shown that there are significant differences between the relationships among the data flow testing criteria and the relationships among the feasible data flow testing criteria. The authors discuss a generalized notion of the executability of a path through a program unit. A script of a testing session using their data flow testing tool, ASSET, is included.< >", "num_citations": "774\n", "authors": ["148"]}
{"title": "On testing non-testable programs\n", "abstract": " A frequently invoked assumption in program testing is that there is an oracle (i.e. the tester or an external mechanism can accurately decide whether or not the output produced by a program is correct). A program is non-testable if either an oracle does not exist or the tester must expend some extraordinary amount of time to determine whether or not the output is correct. The reasonableness of the oracle assumption is examined and the conclusion is reached that in many cases this is not a realistic assumption. The consequences of assuming the availability of an oracle are examined and alternatives investigated.", "num_citations": "738\n", "authors": ["148"]}
{"title": "Analyzing partition testing strategies\n", "abstract": " In this paper, partition testing strategies are assessed analytically. An investigation of what conditions affect the efficacy of partition testing is performed, and comparisons of the fault detection capabilities of partition testing and random testing are made. The effects of subdomain modifications on partition testing\u2019s ability to detect faults are also studied.", "num_citations": "502\n", "authors": ["148"]}
{"title": "Testing component-based software: A cautionary tale\n", "abstract": " Components designed for reuse are expected to lower costs and shorten the development life cycle, but this may not prove so simple. The author emphasizes the need to closely examine a problematic aspect of component reuse: the necessity and potential expense of validating components in their new environments.", "num_citations": "467\n", "authors": ["148"]}
{"title": "Experience with performance testing of software systems: issues, an approach, and case study\n", "abstract": " An approach to software performance testing is discussed. A case study describing the experience of using this approach for testing the performance of a system used as a gateway in a large industrial client/server transaction processing application is presented.", "num_citations": "378\n", "authors": ["148"]}
{"title": "Automatically generating test data from a Boolean specification\n", "abstract": " This paper presents a family of strategies for automatically generating test data for any implementation intended to satisfy a given specification that is a Boolean formula. The fault detection effectiveness of these strategies is investigated both analytically and empirically, and the costs, assessed in terms of test set size, are compared.< >", "num_citations": "364\n", "authors": ["148"]}
{"title": "Data flow analysis techniques for test data selection\n", "abstract": " This paper examines a family of program test data selection criteria derived from data flow analysis techniques similar to those used in compiler optimization. It is argued that currently used path selection criteria which examine only the control flow of a program are inadequate. Our procedure associates with each point in a program at which a variable is defined, those points at which the value is used. Several related path criteria, which differ in the number of these associations needed to adequately test the program, are defined and compared.", "num_citations": "322\n", "authors": ["148"]}
{"title": "A formal analysis of the fault-detecting ability of testing methods\n", "abstract": " Several relationships between software testing criteria, each induced by a relation between the corresponding multisets of subdomains, are examined. The authors discuss whether for each relation R and each pair of criteria, C/sub 1/ and C/sub 2/, R(C/sub 1/, C/sub 2/) guarantees that C/sub 1/ is better at detecting faults than C/sub 2/ according to various probabilistic measures of fault-detecting ability. It is shown that the fact that C/sub 1/ subsumes C/sub 2/ does not guarantee that C/sub 1/ is better at detecting faults. Relations that strengthen the subsumption relation and that have more bearing on fault-detecting ability are introduced.< >", "num_citations": "280\n", "authors": ["148"]}
{"title": "Axiomatizing software test data adequacy\n", "abstract": " A test data adequacy criterion is a set of rules used to determine whether or not sufficient testing has been performed. A general axiomatic theory of test data adequacy is developed, and five previously proposed adequacy criteria are examined to see which of the axioms are satisfied. It is shown that the axioms are consistent, but that only two of the criteria satisfy all of the axioms.", "num_citations": "267\n", "authors": ["148"]}
{"title": "The automatic generation of load test suites and the assessment of the resulting software\n", "abstract": " Three automatic test case generation algorithms intended to test the resource allocation mechanisms of telecommunications software systems are introduced. Although these techniques were specifically designed for testing telecommunications software, they can be used to generate test cases for any software system that is modelable by a Markov chain provided operational profile data can either be collected or estimated. These algorithms have been used successfully to perform load testing for several real industrial software systems. Experience generating test suites for five such systems is presented. Early experience with the algorithms indicate that they are highly effective at detecting subtle faults that would have been likely to be missed if load testing had been done in the more traditional way, using hand-crafted test cases. A domain-based reliability measure is applied to systems after the load testing\u00a0\u2026", "num_citations": "241\n", "authors": ["148"]}
{"title": "Monitoring smoothly degrading systems for increased dependability\n", "abstract": " A strategy is presented for determining when it is advantageous to take some action to restore a system to full capacity. A determination is made of the types of data that need to be collected and circumstances under which the strategy is likely to be useful. Production traffic data is presented for a very large industrial telecommunications project, and the strategy is applied. An investigation is made of when the application of the strategy leads to increased system availability and decreased packet loss experienced by users.", "num_citations": "239\n", "authors": ["148"]}
{"title": "The evaluation of program-based software test data adequacy criteria\n", "abstract": " In earlier work, a preliminary set of axioms for software test data adequacy was introduced in order to formalize properties which should be satisfied by any good program-based adequacy criterion. Here, we extend this work by augmenting the set with additional axioms which substantially strengthen the set. In doing so, we rule out several types of unsuitable notions of adequacy.", "num_citations": "219\n", "authors": ["148"]}
{"title": "A framework for testing database applications\n", "abstract": " Database systems play an important role in nearly every modern organization, yet relatively little research effort has focused on how to test them. This paper discusses issues arising in testing database systems and presents an approach to testing database applications. In testing such applications, the state of the database before and after the user's operation plays an important role, along with the user's input and the system output. A tool for populating the database with meaningful data that satisfy database constraints has been prototyped. Its design and its role in a larger database application testing tool set are discussed.", "num_citations": "183\n", "authors": ["148"]}
{"title": "The cost of data flow testing: An empirical study\n", "abstract": " A family of test data adequacy criteria employing data-flow information was previously proposed, and a theoretical complexity analysis was performed. The author describes an empirical study to determine the actual cost of using these criteria. The aim is to establish the practical usefulness of these criteria in testing software and provide a basis for predicting the amount of testing needed for a given program. The first goal of the study is to confirm the belief that the family of software testing criteria considered is practical to use. An attempt is made to show that even as the program size increases, the amount of testing, expressed in terms of the number of test cases sufficient to satisfy a given criterion, remains modest. Several ways of evaluating this hypothesis are explored. The second goal is to provide the prospective user of these criteria with a way of predicting the number of test cases that will be needed to satisfy a\u00a0\u2026", "num_citations": "182\n", "authors": ["148"]}
{"title": "An AGENDA for testing relational database applications\n", "abstract": " Database systems play an important role in nearly every modern organization, yet relatively little research effort has focused on how to test them. This paper discusses issues arising in testing database systems, presents an approach to testing database applications, and describes AGENDA, a set of tools to facilitate the use of this approach. In testing such applications, the state of the database before and after the user's operation plays an important role, along with the user's input and the system output. A framework for testing database applications is introduced. A complete tool set, based on this framework, has been prototyped. The components of this system are a parsing tool that gathers relevant information from the database schema and application, a tool that populates the database with meaningful data that satisfy database constraints, a tool that generates test cases for the application, a tool that checks the\u00a0\u2026", "num_citations": "181\n", "authors": ["148"]}
{"title": "Comparison of program testing strategies\n", "abstract": " A person testing a program has many methods to choose from, but little solid information about how these methods compare. Where analytic comparisons do exist, their significance is often in doubt. In this paper we examine various comparisons that have been used or proposed for test data selection and adequacy criteria. We characterize them by type and identify their strengths and weaknesses. We examine useful properties of comparisons and study the relationship between analytical and probabilistic comparisons. We find that analytical comparisons provide information of limited value, and that probabilistic comparisons overcome some of these limitations.", "num_citations": "154\n", "authors": ["148"]}
{"title": "Provable improvements on branch testing\n", "abstract": " This paper compares the fault-detecting ability of several software test data adequacy criteria. It has previously been shown that if C/sub 1/ properly covers C/sub 2/, then C/sub 1/ is guaranteed to be better at detecting faults than C/sub 2/, in the following sense: a test suite selected by independent random selection of one test case from each subdomain induced by C/sub 1/ is at least as likely to detect a fault as a test suite similarly selected using C/sub 2/. In contrast, if C/sub 1/ subsumes but does not properly cover C/sub 2/, this is not necessarily the case. These results are used to compare a number of criteria, including several that have been proposed as stronger alternatives to branch testing. We compare the relative fault-detecting ability of data flow testing, mutation testing, and the condition-coverage techniques, to branch testing, showing that most of the criteria examined are guaranteed to be better than\u00a0\u2026", "num_citations": "150\n", "authors": ["148"]}
{"title": "More experience with data flow testing\n", "abstract": " Experience is provided about the cost and effectiveness of the Rapps-Weyuker data flow testing criteria. This experience is based on studies using a suite of well-known numerical programs, and supplements an earlier study (Weyuker 1990) using different types of programs. The conclusions drawn in the earlier study involving cost are confirmed in this study. New observations about tester variability and cost assessment, as well as fault detection, are also provided.< >", "num_citations": "142\n", "authors": ["148"]}
{"title": "The complexity of data flow criteria for test data selection\n", "abstract": " Rapps and Weyuker [6, 7] introduced a family of test data selection criteria based on data flow analysis as used in optimizing compilers [l]. Most test data selection criteria rely solely on the program\u2019s control flow characteristics. By incorporating data flow information into the selection procedure, it is possibJe to focus on associations between physically disjoint portions of the program which are related by the flow of data. A commonly used argument for the necessity of control flow based test data selection criteria is: How can one feel confident about the behavior of a portion of a program which has never been executed by any test case? We concur, but ask in addition: If the result of some computation has never been used by any subsequent computation, how can one feel confident that the proper computations have been performed? Similar intuition has been used in [4] and [5].", "num_citations": "118\n", "authors": ["148"]}
{"title": "Pseudo-oracles for non-testable programs\n", "abstract": " The most commonly used method of validating a program is by testing. The programmer typically runs the program on some test cases, and if and when they run correctly, the program is considered to be correct.", "num_citations": "108\n", "authors": ["148"]}
{"title": "Software performance testing based on workload characterization\n", "abstract": " A major concern of most businesses is their ability to meet customers' performance requirements. This paper describes our workload-based approach to performance testing, and includes a case study that demonstrates the application of this approach to a large, industrial software system. For this system, we collected data in the field to determine the current production usage, and then assessed the performance of the system under both current workloads, and those likely to be encountered in the future. This led to the identification of a software bottleneck, which, had it occurred in the field rather than in the test lab, would have likely had significant consequences.", "num_citations": "100\n", "authors": ["148"]}
{"title": "An extended domain-based model of software reliability\n", "abstract": " A definition of software reliability is proposed in which reliability is treated as a generalization of the probability of correctness of the software in question. A tolerance function is introduced as a method of characterizing an acceptable level of correctness. This in turn is used, together with the probability function defining the operational input distribution, as a parameter of the definition of reliability. It is shown that the definition can be used to provide many natural models of reliability by varying the tolerance function and that it may be reasonably approximated using well-chosen test sets. It is also shown that there is an inherent limitation to the measurement of reliability using finite test sets.< >", "num_citations": "100\n", "authors": ["148"]}
{"title": "A simplified domain-testing strategy\n", "abstract": " A simplified form of domain testing is proposed that is substantially cheaper than previously proposed versions, and is applicable to a much larger class of programs. In particular, the traditional restrictions to programs containing only linear predicates and variables defined over continuous domains are removed. In addition, an approach to path selectlon is proposed to be used m conjunction with the new strategy.", "num_citations": "98\n", "authors": ["148"]}
{"title": "Assessing test data adequacy through program inference\n", "abstract": " Despite the almost universal reliance on testing as the means of locating software errors and its long history of use, few criteria have been proposed for deciding when software has been thoroughly tested. As a basis for the development of usable notions of test data adequacy, an abstract definition is proposed and examined, and approximations to this definition are considered.", "num_citations": "98\n", "authors": ["148"]}
{"title": "The applicability of program schema results to programs\n", "abstract": " Several classes of programs, which are the analogues of previously investigated classes of program schemas, are defined. Decidability and translatability questions are considered for these classes of programs, as well as the applicability of these results to the theories of optimization and program testing. The usefulness of the schema model is studied by considering the inheritability of schema properties by programs, and conversely, the inheritability of program properties by schemas.", "num_citations": "86\n", "authors": ["148"]}
{"title": "Performance testing of software systems\n", "abstract": " Approaches to software performance testing are discussed. A case study describing the experience of using these approaches for testing the performance of a system used as a gateway in a large industrial client/server transaction processing application is presented.", "num_citations": "67\n", "authors": ["148"]}
{"title": "The role of modeling in the performance testing of e-commerce applications\n", "abstract": " An e-commerce scalability case study is presented in which both traditional performance testing and performance modeling were used to help tune the application for high performance. This involved the creation of a system simulation model as well as the development of an approach for test case generation and execution. We describe our experience using a simulation model to help diagnose production system problems, and discuss ways that the effectiveness of performance testing efforts was improved by its use.", "num_citations": "66\n", "authors": ["148"]}
{"title": "Testing software to detect and reduce risk\n", "abstract": " The risk of a piece of software is defined to be the expected cost due to operational failures of the software. Notions of the risk detected by a testing technique and the risk reduction due to a technique are introduced and are used to analytically compare the effectiveness of testing techniques. It is proved that if a certain relation holds between testing techniques A and B, then A is guaranteed to be at least as good as B at detecting and reducing risk, regardless of the particular faults in the program under test or their costs. These results can help practitioners choose an appropriate technique for testing software when risk reduction is the goal.", "num_citations": "62\n", "authors": ["148"]}
{"title": "DATA FLOW TESTING TOOL.\n", "abstract": " ASSET is a tool which uses data-flow information to aid in selection and evaluation of software test data. The original version of ASSET accepted input programs written in a very simple subset of Pascal having only simple variables. The data-flow-testing theory on which ASSET is based is summarized, and the implementation of an enhanced version of ASSET which allows input programs which use arrays is described. Sample input programs are analyzed.", "num_citations": "62\n", "authors": ["148"]}
{"title": "Generating test suites for software load testing\n", "abstract": " Three automatic test case generation algorithms intended to test the resource allocation of telecommunications software systems are introduced. Although these techniques were specifically designed for testing telecommunications software, they can be used to generate test cases for any software system that is modelable by a Markov chain. In addition, three new stochastic measures of effectiveness are presented to be used to assess these algorithms. Each of these measures is a variant of a mutation score, but incorporates the association of a probability y coefficient with each mutant, These algorithms have been used successfully to perform load testing for some real industrial software systems. We present empirical results for five such systems. Early experience with the algorithms indicate that they are highly effective at detecting subtle faults that would have been likely to be missed if load testing had been done\u00a0\u2026", "num_citations": "60\n", "authors": ["148"]}
{"title": "ASSET: A SYSTEM TO SELECT AND EVALUATE TESTS.\n", "abstract": " A description is given of ASSET, a tool which uses information about a program's data flow to aid in selecting test data for the program and to evaluate test data adequacy. ASSET is based on the family of data flow test selection and test data adequacy criteria developed by S. Rapps and EJ Weyuker (1981). ASSET accepts as input a program written in a subset of Pascal, a set of test data, and one of the data flow adequacy criteria and indicates to what extent the criterion has been satisfied by the test data.", "num_citations": "55\n", "authors": ["148"]}
{"title": "Can we measure software testing effectiveness?\n", "abstract": " The paper examines the issues of measuring and comparing the effectiveness of testing criteria. It argues that measurement, in the usual sense, is generally not possible, but that comparison is. In particular, it argues that uniform relations that guarantee that one criterion is better at fault detection than another, according to certain types of well-understood probabilistic measures of effectiveness, are especially valuable.< >", "num_citations": "52\n", "authors": ["148"]}
{"title": "Data flow testing in the presence of unexecutable paths\n", "abstract": " CiNii \u8ad6\u6587 - Data Flow Testing in the Presence of Unexecutable Paths CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3 \u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Data Flow Testing in the Presence of Unexecutable Paths FRANKL PG \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 FRANKL PG \u53ce\u9332\u520a\u884c\u7269 Proc. Workshop on Software Testing Proc. Workshop on Software Testing, 1986 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Program Slicing\u6280\u8853\u3068 \u30c6\u30b9\u30c8\uff0c\u30c7\u30d0\u30c3\u30b0\uff0c\u4fdd\u5b88\u3078\u306e\u5fdc\u7528 \u4e0b\u6751 \u9686\u592b \u60c5\u5831\u51e6\u7406 33(9), 1078-1086, 1992-09-15 \u53c2\u8003\u6587\u732e42\u4ef6 \u88ab\u5f15\u7528\u6587\u732e17\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10006727263 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf \u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 /\u3067\u3067\u2026", "num_citations": "52\n", "authors": ["148"]}
{"title": "Reliability testing of rule-based systems\n", "abstract": " Rule-based software systems are becoming more common in industrial settings, particularly to monitor and control large, real-time systems. The authors describe an algorithm for reliability testing of rule-based systems and their experience using it to test an industrial network surveillance system.", "num_citations": "51\n", "authors": ["148"]}
{"title": "Some observations on partition testing\n", "abstract": " The term \u201cpartition testing\u201d, in its broadest sense, refers to a very general family of testing strategies. The primary characteristic is that the program\u2019s input domain is divided into subsets, with the tester selecting one or more element from each subdomain. In this paper, we shall not restrict the term \u201cpartition\u201d to the formal mathematical meaning of a division into disjoint subsets which together span the space being considered. Instead we shall use it in the more familiar sense to refer to a division into, possibly overlapping, subsets. The goal of such a partitioning is to make the division in such a way that when the tester selects test cases based on the subsets, the resulting test set is a good representation of the entire domain. Ideally, the partition divides the domain into disjoint subdomains with the property that within each subdomain, either the program produces the correct answer for every element or the program\u00a0\u2026", "num_citations": "49\n", "authors": ["148"]}
{"title": "An empirical study of the complexity of data flow testing\n", "abstract": " An empirical study to determine the cost of using a family of test data adequacy criteria based on data-flow information is described. Evidence shows that even as the program size increases, the amount of testing, expressed in terms of the number of test cases sufficient to satisfy a given criterion, remains modest. A method of predicting the number of test cases that will be needed to satisfy a given criterion for a given program is explored.<>", "num_citations": "48\n", "authors": ["148"]}
{"title": "Translatability and decidability questions for restricted classes of program schemas\n", "abstract": " Two new classes of schemas are introduced: the reachable schemas and the semifree schemas. A schema is reachable if every statement in the schema is executed under some interpretation. A schema is semifree if every test in the schema is necessary in the sense that each exit of the test is taken under some interpretation.It is shown that most of the standard decision problems are unsolvable for schemas in these two classes, and that there can be no algorithm which effectively translates an arbitrary schema into an equivalent reachable or semifree schema, even though such equivalent schemas always exist. These classes are also compared to the free and liberal schemas, and interclass translatability questions are investigated. It is demonstrated that every reachable schema can be effectively translated into a semifree schema, even though it is not decidable whether a reachable schema is semifree.", "num_citations": "48\n", "authors": ["148"]}
{"title": "The oracle assumption of program testing\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "43\n", "authors": ["148"]}
{"title": "Deriving workloads for performance testing\n", "abstract": " An approach is presented to compare the performance of an existing production platform and a proposed replacement architecture. The traditional approach to such a comparison is to develop software for the proposed platform, build the new architecture, and collect performance measurements on both the existing system in production and the new system in the development environment. In this paper we propose a new way to design an application\u2010independent workload for doing such a performance evaluation. We demonstrate the applicability of our approach by describing our experience using it to help an industrial organization determine whether or not a proposed architecture would be adequate to meet their organization's performance requirements.", "num_citations": "42\n", "authors": ["148"]}
{"title": "A metric for predicting the performance of an application under a growing workload\n", "abstract": " A new software metric, designed to predict the likelihood that the system will fail to meet its performance goals when the workload is scaled, is introduced. Known as the PNL (Performance Nonscalability Likelihood) metric, it is applied to a study of a large industrial system, and used to predict at what workloads bottlenecks are likely to appear when the presented workload is significantly increased. This allows for intelligent planning in order to minimize disruption of acceptable performance for customers. The case study also outlines our performance testing approach and presents the major steps required to identify current production usage and to assess the software performance under current and future workloads.", "num_citations": "40\n", "authors": ["148"]}
{"title": "Metrics to assess the likelihood of project success based on architecture reviews\n", "abstract": " Architecture audits are performed very early in the software development lifecycle, typically before low level design or code implementation has begun. An empirical study was performed to assess metrics developed to predict the likelihood of risk of failure of a project. The study used data collected during 50 architecture audits performed over a period of two years for large industrial telecommunications systems. The purpose of such a predictor was to identify at a very early stage, projects that were likely to be at high risk of failure. This would enable the project to take corrective action before significant resources had been expended using a problematic architecture. Detailed information about seven of the 50 projects is presented, and a discussion of how the proposed metric rated each of these projects is presented., A comparison is made of the metric's evaluation and the assessment of the project made by\u00a0\u2026", "num_citations": "38\n", "authors": ["148"]}
{"title": "Investigating metrics for architectural assessment\n", "abstract": " An empirical study is described using data collected during 50 architecture audits of large industrial telecommunications systems performed over a period of two years. The goal of this study was to develop metrics that could be used to differentiate between projects that are at high risk of failure and those at low risk. These metrics would be computed following an architecture audit before coding has begun, allowing measures to be taken to correct identified problems very early in the development lifecycle. Detailed information is presented about the use of the proposed risk prediction metric for seven of the systems for which we provided architecture audits.", "num_citations": "36\n", "authors": ["148"]}
{"title": "Monitoring for security intrusion using performance signatures\n", "abstract": " A new approach for detecting security attacks on software systems by monitoring the software system performance signatures is introduced. We present a proposed architecture for security intrusion detection using off-the-shelf security monitoring tools and performance signatures. Our approach relies on the assumption that the performance signature of the well-behaved system can be measured and that the performance signature of several types of attacks can be identified. This assumption has been validated for operations support systems that are used to monitor large infrastructures and receive aggregated traffic that is periodic in nature. Examples of such infrastructures include telecommunications systems, transportation systems and power generation systems. In addition, significant deviation from well-behaved system performance signatures can be used to trigger alerts about new types of security attacks. We\u00a0\u2026", "num_citations": "35\n", "authors": ["148"]}
{"title": "Ensuring stable performance for systems that degrade\n", "abstract": " A new approach that is useful in identifying and eliminating performance degradation occurring in aging software is proposed. A customer-affecting metric is used to initiate the restoration of such a system to full capacity. A case study is described in which, by simulating an industrial software system, we are able to show that by monitoring a customer-affecting metric and frequently comparing its degradation to the performance objective, we can ensure system stability at a very low cost.", "num_citations": "33\n", "authors": ["148"]}
{"title": "An analytical comparison of the fault-detecting ability of data flow testing techniques\n", "abstract": " Compares several data flow based software testing criteria to one another and to branch testing. The fact that criterion C/sub 1/ subsumes criterion C/sub 2/, does not guarantee that C/sub 1/ is better at detecting faults than C/sub 2/. However, if a certain stronger relation between the criteria holds, then for any program and any specification, C/sub 1/ is guaranteed to be better at detecting faults than C/sub 2/ in the following sense: a test suite selected by independent random selection of one test case from each C/sub 1/ subdomain is at least as likely to detect a fault as a suite similarly selected using C/sub 2/. It is shown that under those conditions, the expected number of failure-causing inputs in the C/sub 1/ test suite. These results are used to compare a number of data flow testing criteria to one another and to branch testing.< >", "num_citations": "33\n", "authors": ["148"]}
{"title": "A metric to predict software scalability\n", "abstract": " Software system scalability is an important issue for most businesses. It is essential that as the customer base increases, and therefore the system has to deal with significantly increased loads, the system is prepared to handle the increased traffic so that the users do not encounter unacceptable system performance. For this reason we introduce a new metric, the PNL metric, that can be used to predict the likely loads at which the probability of performance problems will exceed acceptable levels. A case study is described that demonstrates the application of the PNL metric to a large industrial software system. A description of the steps taken to model the software and collect data is provided, as well as the computation of the PNL metric and implications derived from the computation for this system. This information was used by the project to help plan for additional capacity so that the performance experienced by\u00a0\u2026", "num_citations": "31\n", "authors": ["148"]}
{"title": "Using failure cost information for testing and reliability assessment\n", "abstract": " A technique for incorporating failure cost information into algorithms designed to automatically generate software-load-testing suites is presented. A previously introduced reliability measure is also modified to incorporate this cost information. examples are presented to show the usefulness of including cost information when testing or assessing software.", "num_citations": "31\n", "authors": ["148"]}
{"title": "On the adequacy of Weyuker's test data adequacy axioms. Reply\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "28\n", "authors": ["148"]}
{"title": "Methods and opportunities for rejuvenation in aging distributed software systems\n", "abstract": " In this paper we describe several methods for detecting the need for software rejuvenation in mission critical systems that are subjected to worm infection, and introduce new software rejuvenation algorithms. We evaluate these algorithms\u2019 effectiveness using both simulation studies and analytic modeling, by assessing the probability of mission success. The system under study emulates a Mobile Ad-Hoc Network (MANET) of processing nodes. Our analysis determined that some of our rejuvenation algorithms are quite effective in maintaining a high probability of mission success while the system is under explicit attack by a worm infection.", "num_citations": "27\n", "authors": ["148"]}
{"title": "Evaluation techniques for improving the quality of very large software systems in a cost-effective way\n", "abstract": " Evaluation techniques for creating software systems that are both very large and very high quality are discussed. Central to this approach are careful architecture assessment, the automation of test case generation to make it efficient and tester-independent, and ways of minimizing the cost of regression testing to encourage that it be done thoroughly and systematically.", "num_citations": "27\n", "authors": ["148"]}
{"title": "Assessing the fault-detecting ability of testing methods\n", "abstract": " It is important to be able to assess the fault detection capabilities of proposed software testing techniques. It is not enough for a researcher to intro-duce new techniques and assure us that they are good. There must be precise ways of comparing criteria and rating them based on their fault detection ability.With this in mind, several relationships between software testing criteria are studied in this paper. For each of these relations R, we investigate whether R (C1, G\u2019z) guarantees that criterion Cl is better at detecting faults than C\u20192, according to various probabilistic measures.", "num_citations": "24\n", "authors": ["148"]}
{"title": "Empirical software engineering research-the good, the bad, the ugly\n", "abstract": " The Software Engineering Research community has slowly recognized that empirical studies are an important way of validating ideas and increasingly our community has stopped accepting the sufficiency of arguing that a smart person has come up with the idea and therefore it must be good. This has led to a flood of Software Engineering papers that contain at least some form of empirical study. However, not all empirical studies are created equal, and many may not even provide any useful information or value. We survey the gradual shift from essentially no empirical studies, to a small number of ones of questionable value, and look at what we need to do to insure that our empirical studies really contribute to the state of knowledge in the field. Thus we have the good, the bad, and the ugly. What are we as a community doing correctly? What are we doing less well than we should be because we either don't have\u00a0\u2026", "num_citations": "22\n", "authors": ["148"]}
{"title": "A formal notion of program-based test data adequacy\n", "abstract": " We propose a definition of the notion of adequacy of software test data and discuss justification, difficulties, and properties of the notion. It is not the purpose of this paper to suggest a definite practically applicable criterion of test data adequacy. Rather we present a theoretical analysis which, it is believed, gives insight into such questions as:(a) For a given program, what points must belong to a test set in order that it may be deemed adequate?(b) For a given program, how many points must belong to an adequate test set?(c) What kind of approximation to\" correctness\" can be provided by the knowledge that a program has been\" adequately\" tested?We believe, in general, that an adequacy criterion should be invoked only after the test data fails to expose errors. Clearly, as long as there is an element of the test set on which the program does not agree with the specification, we know that the test data is still doing its job and that testing (and subsequent debugging) must continue.(In this paper, we ignore the question of whether and how we can tell if a program agrees with a specification at a particular point. However, see Davis and Weyuker, 1981; Weyuker, 1982.) Once the program does agree with the specification on all elements of a set of test data, we must decide whether the testing phase can end, and hence we will need to invoke some kind of adequacy criterion. For a given program P, we write P (c)= b to mean that the program P on input c halts with output b. We write P~ Q (P is equivalent to Q), where P and Q are programs, to mean that P (c)= Q (c) for every input c. In particular this implies that for each c, P (c) is defined if and only if Q (c) is\u00a0\u2026", "num_citations": "22\n", "authors": ["148"]}
{"title": "Ensuring system performance for cluster and single server systems\n", "abstract": " A new approach that is useful in identifying and eliminating performance degradation occurring in aging software is proposed. A customer-affecting metric is used to initiate the restoration of such a system to full capacity. A case study is described in which, by simulating an industrial software system, we are able to show that by monitoring a customer-affecting metric and frequently comparing its degradation to the performance objective, we can ensure system stability at a very low cost.", "num_citations": "21\n", "authors": ["148"]}
{"title": "Metric space-based test-data adequacy criteria\n", "abstract": " Since software testing cannot ordinarily be expected to provide conclusive evidence that a program is correct, software engineers have had to be satisfied with the vague notion of a set of test data being adequate for a given program. In this paper a theoretical model is provided for the notion of adequacy. Adequacy criteria are seen as serving to distinguish a given program from a certain class of programs. In particular, notions of distance between programs are studied, and adequacy of a test set is taken to mean that the set successfully distinguishes the program being tested from all programs that are sufficiently near to it, and differ in input-output behaviour from the given program. Certain points, called critical, are identified which must occur in every adequate test set. Finally, lower bounds are obtained on the size of test sets which are minimally adequate, in the sense that they have no adequate proper subsets.", "num_citations": "19\n", "authors": ["148"]}
{"title": "Estimating the CPU utilization of a rule-based system\n", "abstract": " Rule-based software systems have become very common in telecommunications settings, particularly to monitor and control workflow management of large networks. At the same time, shorter deployment cycles are frequently necessary which has led to modifications being made to the rule base, without a full assessment of the impact of these new rules through extensive performance testing.An approach is presented that helps assess the performance of rule-based systems, in terms of its CPU utilization, by using modeling and analysis. A case study is presented applying this approach to a large rule-based system that is used to monitor a very large industrial telecommunications network.", "num_citations": "18\n", "authors": ["148"]}
{"title": "Using operational distributions to judge testing progress\n", "abstract": " Although most software test data adequacy criteria proposed as a way to assess the progress of testing rely on the coverage of the code, the coverage of the specification, or the percentage of the input domain that has been exercised, none of these are really good indicators of how thoroughly the software has been tested. Instead we propose that the assessment be based on the percentage of the probability mass associated with the test suite. This requires that data be collected to determine how the software is used in practice over a period of time, so that this can be properly assessed. In that way a project is able to accurately determine how testing is progressing.", "num_citations": "18\n", "authors": ["148"]}
{"title": "Using performance signatures and software rejuvenation for worm mitigation in tactical manets\n", "abstract": " In this paper, we propose a new approach for mitigation of worm propagation through tactical Mobile Ad-Hoc Networks (MANETs) which is based upon performance signatures and software rejuvenation. Three application performance signature and software rejuvenation algorithms are proposed and analyzed. These algorithms monitor critical applications' responsiveness and trigger actions for software rejuvenation when host resources degrade due to a co-resident worm competing for host resources. We analyze the effectiveness of our algorithms through analytic modeling and detailed, extensive simulation studies. The key performance metrics investigated are application response time, mean time between rejuvenations and the steady state probability of host infection. We also use simulation models to investigate several design and parameter tuning issues. We investigate the relationship between the rate at\u00a0\u2026", "num_citations": "17\n", "authors": ["148"]}
{"title": "Program schemas with semantic restrictions\n", "abstract": " Paterson introduced the notions of freedom and liberality as semantic restrictions on the class of schemas. He felt that such restricted Might have solvable decision problems, realistic models of what classes of schemas and also be would generally be considered\" good\" programs. With the latter viewpoint in mind, two new classes of schemas are introduced in this thesis: the reachable schemas and the semi free schemas. Several decision problems for these classes, and translatability between these classes and other semantically restricted schema classes are studied. Relationships between various classes of schemas are also considered. An investigation is made of the preservation of semantic properties by is morph SM, functional Similarity, strong equivalency and weak equivalence. This allows a distinction to be made between properties, which are inherent in the class of functions represented by a schema, regardless of the algorithm used to compute it, and those properties which are dependent upon the algorithm or its encoding. Two types of interpretations for schemas are defined: point wise interpretations and function interpretations. The relationships between these types of interpretations are studied. It is seen that corresponding to the set of all finite consistent paths of a schema executed under point wise interpretations, there is a single effectively constructible function interpretation of that schema which has the same set of finite consistent-paths. The equivalence of certain semantic properties of schemas follows from this result. This result also facilitates the consideration of the inheritance of schema properties by programs, and the\u00a0\u2026", "num_citations": "16\n", "authors": ["148"]}
{"title": "Automated generation of test cases using a performability model\n", "abstract": " The authors present a new approach for the automated generation of test cases to be used for demonstrating the reliability of large industrial mission-critical systems. In this study they extend earlier work by using a performability model to track resource usage and resource failures. Results from the transient Markov chain analysis are used to estimate the software reliability at a given system execution time.", "num_citations": "14\n", "authors": ["148"]}
{"title": "Thinking formally about testing without a formal specification\n", "abstract": " Practitioners test software every day and have to make decisions about what techniques to use to select test data. This paper discusses what it means for one test data selection criterion to be more effective than another. Several proposed comparison relations are presented and discussed. Deficiencies are highlighted, and a discussion of how these relations evolved is presented. The usefulness of empirical studies is also considered.", "num_citations": "12\n", "authors": ["148"]}
{"title": "Using the consequence of failures for testing and reliability assessment\n", "abstract": " This paper presents a technique for incorporating cost or consequence information into algorithms designed to automatically generate software load testing suites. It also modifies previously introduced reliability measures to incorporate this cost information.", "num_citations": "12\n", "authors": ["148"]}
{"title": "How to judge testing progress\n", "abstract": " It is usual to base the assessment of software testing progress on a coverage measure such as code coverage or specification coverage, or on the percentage of the input domain exercised. In this paper it is argued that these characteristics do not provide good indications of the degree to which the software has been tested. Instead we propose that the assessment of testing progress be based on the total percentage of the probability mass that corresponds to the test cases selected and run. To do this, it is necessary to collect data that profiles how the software will be used once it is operational in the field. By so doing, we are able to accurately determine how much testing has been done, and whether it has met the standards of completeness for the product under consideration.", "num_citations": "10\n", "authors": ["148"]}
{"title": "Re-estimation of software reliability after maintenance\n", "abstract": " Case Western Reserve Univ. 10900 Euclid Ave. Cleveland, OH 44106+ 1 216 368 6884 andy@ ces. cwru. eduThe common practice of reusing test cases for regression testing is incompatible with estimating the reliability of modified software, because dependencies between software changes and previous test results lead to estimation bias. Statistical testing with random operational inputs can produce valid reliability estimates, but its cost may be excessive unless the nature of software changes is considered. We propose an economical way of estimating the reliability of modified software under certain common circumstances. It calls for updating a previous reliability estimate by estimating differences in the behavior of successive software versions. The older version is used as a relative oracle to reduce the number of executions that must be checked manually for conformance to requirements. Our approach is\u00a0\u2026", "num_citations": "10\n", "authors": ["148"]}
{"title": "Estimating the software reliability of smoothly degrading systems\n", "abstract": " Presents the application of a domain-based reliability measure to systems that can be represented by Markov chains. A load testing algorithm is presented, and the measure is applied to assess the reliability of these systems after they have been tested. Data are presented for three industrial telecommunications systems that had been tested using the load testing algorithm, tracking the reliability as a function of the degree of system degradation experienced.< >", "num_citations": "10\n", "authors": ["148"]}
{"title": "Predicting project risk from architecture reviews\n", "abstract": " A metric is proposed to predict the risk of a project's failure. It is a simplification of a metric proposed by A. Avritzer et al. (Proc. 5th Internat. Symp. on Software Metrics, pp. 4-10, Nov. 1998). The metric is based on findings made during an architecture review of the project. An empirical study involving 36 large industrial telecommunications projects is included, describing our experience with using this metric.", "num_citations": "9\n", "authors": ["148"]}
{"title": "Difficulties measuring software risk in an industrial environment\n", "abstract": " Software risk is intended to reflect loss due to software failure. This has traditionally been computed by taking the product of two things: a probability of occurrence and the cost associated with failures. Applying these definitions in practice, however, may be much harder than it at first appears. There are two types of problems that affect the applicability and usefulness of such a computation: that the user has to know detailed information that is not normally available, and that most risk definitions do not use relevant information that is available, including information derived from testing. A definition of risk is introduced that will be usable in industrial settings. We also explore ways of incorporating information about how the software has been tested, the degree to which the software has been tested, and the observed results.", "num_citations": "8\n", "authors": ["148"]}
{"title": "Issues in interoperability and performance verification in a multi-orb telecommunications environment\n", "abstract": " The rapid changes in the telecommunications environment demand that new services are developed and deployed quickly, and that legacy systems are integrated seamlessly. While emerging technologies and standards such as CORBA allow rapid solution integration via shared reusable components, new issues arise for interoperability and performance assurance in highly heterogeneous environments. In this paper, we first identify these issues in a multi-vendor and multi-ORB telecommunications environment. We then propose a systematic approach for interoperability verification using a reference ORB and industry-standard test suites. We extend our interoperability testing framework to performance verification, where the scope of the tests is detailed and the design strategy is analyzed.", "num_citations": "8\n", "authors": ["148"]}
{"title": "In defense of coverage criteria\n", "abstract": " This work presents the interconnection scheme for Greek School Network that is heavily relied on well-established metropolitan optical networks. The case of the metropolitan optical network located at the Municipality of Kalamata is analyzed and the potential large-scale deployment of this networking scheme is examined. Also, critical security aspects are briefly discussed.", "num_citations": "7\n", "authors": ["148"]}
{"title": "A generalized domain-based definition of software reliability\n", "abstract": " A definition of software reliability is proposed in which reliability is treated as a generalization of the probability of correctness of the software in question. The definition is parametrized by the distribution characterizing the operational environment, and by a tolerance function characterizing a notion of degree of correctness. It is shown that the definition can be used to provide many natural models of reliability by varying the tolerance function, and that it may be reasonably approximated using well-chosen test sets. It is proved that, under fairly weak conditions, one cannot hope to measure reliability exactly by using finite test sets.", "num_citations": "7\n", "authors": ["148"]}
{"title": "Detecting failed processes using fault signatures\n", "abstract": " A strategy is presented for automatically identifying processes that have failed. A determination is made of the types of data that need to be collected, and circumstances under which the approach is likely to be useful. The strategy is applied to generate signatures for three different types of workloads, and several different resources. A typical failure is injected into the process, and the associated signatures are presented for the same workloads and resources. A measure as defined that is used to determine whether or not a signature is likely to be indicative of a faulty process.", "num_citations": "5\n", "authors": ["148"]}
{"title": "The automated generation of test cases using an extended domain based reliability model\n", "abstract": " We present a new approach for the automated generation of test cases to be used for demonstrating the reliability of large industrial mission-critical systems. In this paper we extend earlier work by adding failure tracking and transient Markov chain analysis. Results from the transient Markov chain analysis are used to estimate the software reliability at a given system execution time.", "num_citations": "4\n", "authors": ["148"]}
{"title": "Empirical Studies as a Basis for Technology Transfer\n", "abstract": " The ultimate goal of all software engineering research should be to have an impact on the way software is engineered. This might involve making it more dependable, more economical to produce, more easily understood, or in some way improve the production or quality of software. This should be true whether the research is theoretical in nature or more pragmatic. In the former case the timescale for impact will likely be longer than one would expect if the research involves the proposal of a new specification, development, architecture or testing technique, or a new metric or way of assessing some software artifact. Nonetheless, it should ultimately allow practitioners to build \u201cbetter\u201d systems.", "num_citations": "4\n", "authors": ["148"]}
{"title": "Enforcing quality of service of distributed objects\n", "abstract": " Three algorithms designed to enforce different quality of service criteria are presented, as well as empirical assessments of the algorithms for three large industrial telecommunications systems. These assessments are made in terms of the simulated performance of each system on average loads selected from operational distributions collected during beta release and field use. In addition, synthetic heavy loads designed to cause the overall CPU utilization rates to exceed 90% of capacity were run. The algorithms build on previously defined load testing algorithms, and use parameters and operational distributions computed for that purpose. This makes the quality of service enforcement algorithms particularly efficient. The primary bases for the assessment of the algorithms were the overall deviation of the response time from the average, and the fraction of service requests that were throttled from clients under\u00a0\u2026", "num_citations": "4\n", "authors": ["148"]}
{"title": "Modifications of the program scheme model\n", "abstract": " In [7] Paterson defined a formal model for abstract programs and proved many results about such schemas. In this paper we place minor modifications on the Paterson model, and consider the significance of these changes.", "num_citations": "4\n", "authors": ["148"]}
{"title": "Software engineering research: from cradle to grave\n", "abstract": " Although this is a talk about the design of predictive models to determine where faults are likely to be in the next release of a large software system, the primary focus of the talk is the process that was followed when doing this type of software engineering research. We follow the project from problem inception (cradle) to productization (grave), describing each of the intermediate stages to try to give a picture of why such research takes so long, and also why it is necessary to perform each of the steps.", "num_citations": "3\n", "authors": ["148"]}
{"title": "Quality of service enforcement for distributed objects\n", "abstract": " As the number of industrial organisations that adopt the use of shared distributed objects as the vehicle for implementing large software systems increases, new quality issues arise. Four algorithms are presented that are designed to enforce different quality of service criteria for shared distributed objects that have been tested using a previously introduced test generation algorithm. Empirical assessments of these algorithms for three large systems are also presented. These assessments are made in terms of the simulated performance of each system on average loads selected from operational distributions collected during beta release and field use, assuming both Poisson and bursty arrival processes. Heavy loads designed to cause the overall CPU utilisation rates to exceed 90% of capacity were also run.", "num_citations": "3\n", "authors": ["148"]}
{"title": "Computability, Complexity and Languages\n", "abstract": " Martin Davies, Ron Segal & Elaine Weyuker, Computability, Complexity and Languages - PhilPapers Sign in | Create an account PhilPapers PhilPeople PhilArchive PhilEvents PhilJobs PhilPapers home Syntax Advanced Search Syntax Advanced Search Syntax Advanced Search Computability, Complexity and Languages Martin Davies, Ron Segal & Elaine Weyuker Academic Press (1994) Authors DD Martin Abstract This article has no associated abstract. (fix it) Keywords No keywords specified (fix it) Categories Computability in Philosophy of Computing and Information (categorize this paper) Buy this book Find it on Amazon.com Options Edit this record Mark as duplicate Export citation Find it on Scholar Request removal from index Translate to english Revision history Download options PhilArchive copy Upload a copy of this paper Check publisher's policy Papers currently archived: 57,953 External links This no (\u2026", "num_citations": "3\n", "authors": ["148"]}
{"title": "ACM-W celebrates women in computing\n", "abstract": " Computer science is no longer the hot, high-enrollment field it once was.", "num_citations": "1\n", "authors": ["148"]}
{"title": "Data Flow Testing\n", "abstract": " Data flow testing is a family of white\u2010box testing techniques, relying on information deriving from both the flow of control and the flow of data through the program. Most other white\u2010box testing strategies rely only on the program's flow of control. A second distinction between this family of testing strategies and other white\u2010box strategies is that data flow strategies assure that different parts of the code that are related by the flow of data are tested together by requiring that test cases be chosen that exercise these related parts together. In contrast to this, most other white\u2010box strategies, such as statement testing or branch testing, require only that every identified program element be exercised at some point, but not necessarily in combination with other specified program elements. This means that certain meaningful code interactions may go uninspected and hence certain classes of faults may be overlooked even though\u00a0\u2026", "num_citations": "1\n", "authors": ["148"]}
{"title": "Reply to Some Critical Remarks on a Hierarchy of Fault-Detecting Abilities of Test Methods\n", "abstract": " The Computer Society is an association of people with professional interest in the field of computers. All members of the IEEE are eligible for membership in the Society and will receive this TRANSACTIONS upon payment of the annual Society membership fee of $28.00 plus an annual subscription fee of $30.00. Members of certain professional societies and other computer professionals are also eligible to be members of the Computer Society. For information on joining, write to IEEE Computer Society, 1730 Massachussetts Avenue NW, Washington, DC 20036-1903. Member copies of Transactions are, for personal use only.", "num_citations": "1\n", "authors": ["148"]}
{"title": "Reduction of a Discrete Event Simulation to a Markov Chain\n", "abstract": " An event-driven, time-based stochastic simulation model may be modified in a straightforward manner to satisfy the axioms of a discrete-state, continuous-time Markov chain. The method requires casting the probability distribution of each random variable representing a time interval of the process into the form of a series of exponentially distributed stages. An algorithm for this transformation was developed. The technique is demonstrated by means of a simple example of a computer system simulation. The Markov chain representation of the simulation can be solved numerically; this solution provides an independent verification of the logic of the simulation. 9 figures.", "num_citations": "1\n", "authors": ["148"]}
{"title": "Security in operating systems: separating the roles of rights\n", "abstract": " Security in operating systems - CERN Document Server CERN Accelerating science Sign in Directory CERN Document Server Access articles, reports and multimedia content in HEP Main menu Search Submit Help Personalize Your alerts Your baskets Your comments Your searches Home > Security in operating systems Information Discussion (0) Files Preprint Report number NYU-CS-TR-78-003 Title Security in operating systems : separating the roles of rights If you experience any problem watching the video, click the download button below Download Embed Show number of views Author(s) Weyuker, EJ Affiliation (New York Univ.) Imprint 1978. - 21 p. Subject category Computing and Computers Back to search Record created 1990-01-28, last modified 2014-12-15 Similar records Add to personal basket Export as BibTeX, MARC, MARCXML, DC, EndNote, NLM, RefWorks CERN Document Server :: Search :: :: ::\u2026", "num_citations": "1\n", "authors": ["148"]}