{"title": "Recovering traceability links between code and documentation\n", "abstract": " Software system documentation is almost always expressed informally in natural language and free text. Examples include requirement specifications, design documents, manual pages, system development journals, error logs, and related maintenance reports. We propose a method based on information retrieval to recover traceability links between source code and free text documents. A premise of our work is that programmers use meaningful names for program items, such as functions, variables, types, classes, and methods. We believe that the application-domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high-level concepts with program concepts and vice-versa. We apply both a probabilistic and a vector space information retrieval model in two case studies to trace C\u00a0\u2026", "num_citations": "1230\n", "authors": ["303"]}
{"title": "Development and evaluation of a virtual campus on Second Life: The case of SecondDMI\n", "abstract": " Video games and new communication metaphors are quickly changing today\u2019s young people habits. Considering the actual e-learning scenarios, embedded in a fully technological enabled environment it is crucial to take advantage of this kind of capabilities to let learning process gain best results.This paper presents a virtual campus created using Second Life which provides four distinct types of virtual space: common student campus, collaborative zones, lecture rooms and recreational areas. Second Life environments and objects have been designed and programmed to support synchronous lectures and collaborative learning. The Second Life virtual world has also been equipped with supporting tools enabling students and teachers to navigate among multimedia contents. Second Life and an ad-hoc developed Moodle plug-in have been integrated to naturally enrich the environment with LMS services\u00a0\u2026", "num_citations": "531\n", "authors": ["303"]}
{"title": "Conditioned program slicing\n", "abstract": " Slicing is a technique to decompose programs based on the analysis of the control and data flow. In the original Weiser's definition, a slice consists of any subset of program statements preserving the behaviour of the original program with respect to a program point and a subset of the program variables (slicing criterion), for any execution path. We present conditioned slicing, a general slicing model based on statement deletion. A conditioned slice consists of a subset of program statements which preserves the behaviour of the original program with respect to a slicing criterion for a given set of execution paths. The set of initial states of the program that characterise these paths is specified in the form of a first order logic formula on the input variables. We also show how slices deriving from other statement deletion based slicing models can be defined as conditioned slices. This is used to formally define a partial\u00a0\u2026", "num_citations": "304\n", "authors": ["303"]}
{"title": "Program slicing: Methods and applications\n", "abstract": " Program slicing is a viable method to restrict the focus of a task to specific sub-components of a program. Examples of applications include debugging, testing, program comprehension, restructuring, downsizing, and parallelization. The paper discusses different statement deletion based slicing methods, together with algorithms and applications to software engineering.", "num_citations": "236\n", "authors": ["303"]}
{"title": "Understanding function behaviors through program slicing\n", "abstract": " We present conditioned slicing as a general slicing framework for program comprehension. A conditioned slice consists of a subset of program statements which preserves the behavior of the original program with respect to a set of program executions. The set of initial states of the program that characterize these executions is specified in terms of a first order logic formula on the input variables of the program. Conditioned slicing allows a better decomposition of the program giving the maintainer the possibility to analyze code fragments with respect to different perspectives. We also show how slices produced with traditional slicing methods can be reduced to conditioned slices. Conditioned slices can be identified by using symbolic execution techniques and dependence graphs.", "num_citations": "208\n", "authors": ["303"]}
{"title": "Automatic generation of visual programming environments\n", "abstract": " We have developed the visual language compiler-compiler (VLCC) system to automatically generate visual programming environments. VLCC is a grammar based system that can support implementation of any visual language by assisting the language designer in defining the language's graphical objects, syntax, and semantics. The final result of the generation process includes an integrated environment with a visual editor and a compiler for the defined visual language. In VLCC, graphical tools define visual languages to create both graphical objects and composition rules. Visual editors enable language designers to directly and visually manipulate the syntax of these languages. To capture the widest range of visual languages, the VLCC system can be configured for a specific language class. Different language classes can be characterized depending on their graphical objects' structure and on the way they\u00a0\u2026", "num_citations": "143\n", "authors": ["303"]}
{"title": "A parsing methodology for the implementation of visual systems\n", "abstract": " The Visual Language Compiler-Compiler (VLCC) is a grammar-based graphical system for the automatic generation of visual programming environments. In this paper the theoretical and algorithmic issues of VLCC are discussed in detail. The parsing methodology we present is based on the \"positional grammar\" model. Positional grammars naturally extend context-free grammars by considering new relations in addition to string concatenation. Thanks to this, most of the results from LR parsing can be extended to the positional grammars inheriting the well known LR technique efficiency. In particular, we provide algorithms to implement a YACC-like tool embedded in the VLCC system for automatic compiler generation of visual languages described by positional grammars.", "num_citations": "114\n", "authors": ["303"]}
{"title": "Design pattern recovery through visual language parsing and source code analysis\n", "abstract": " In this paper we propose an approach for recovering structural design patterns from object-oriented source code. The recovery process is organized in two phases. In the first phase, the design pattern instances are identified at a coarse-grained level by considering the design structure only and exploiting a parsing technique used for visual language recognition. Then, the identified candidate patterns are validated by a fine-grained source code analysis phase. The recognition process is supported by a tool, namely design pattern recovery environment, which allowed us to assess the retrieval effectiveness of the proposed approach on six public-domain programs and libraries.", "num_citations": "108\n", "authors": ["303"]}
{"title": "Migrating legacy systems to the web: an experience report\n", "abstract": " A key to successfully moving to the Internet while salvaging past investments in centralised, mainframe-oriented software development is migrating core legacy applications towards Web-enabled client-server architectures. This paper presents the main results and lessons learned from a migration project aimed at integrating an existing COBOL system into a Web-enabled infrastructure. The original system has been decomposed into its user interface and server (application logic and database) components. The user interface has been migrated into a Web browser shell using Microsoft Active Server Pages (ASP) and VBScript. The server component has been wrapped with dynamic load libraries written in Microfocus Object COBOL, loaded into Microsoft Internet Information Server (IIS), and accessed by the ASP pages.", "num_citations": "105\n", "authors": ["303"]}
{"title": "A specification driven slicing process for identifying reusable functions\n", "abstract": " We present a new program slicing process for identifying and extracting code fragments implementing functional abstractions. The process is driven by the specification of the function to be isolated, given in terms of a precondition and a postcondition. Symbolic execution techniques are used to abstract the preconditions for the execution of program statements and predicates. The recovered conditions are then compared with the precondition and the postcondition of the functional abstraction. The statements whose preconditions are equivalent to the pre and postconditions of the specification are candidate to be the entry and exit points of the slice implementing the abstraction. Once the slicing criterion has been identified the slice is isolated using algorithms based on dependence graphs. The process has been specialized for programs written in the C language. Both symbolic execution and program slicing are\u00a0\u2026", "num_citations": "102\n", "authors": ["303"]}
{"title": "Assessing effort estimation models for corrective maintenance through empirical studies\n", "abstract": " We present an empirical assessment and improvement of the effort estimation model for corrective maintenance adopted in a major international software enterprise. Our study was composed of two phases. In the first phase we used multiple linear regression analysis to construct effort estimation models validated against real data collected from five corrective maintenance projects. The model previously adopted by the subject company used as predictors the size of the system being maintained and the number of maintenance tasks. While this model was not linear, we show that a linear model including the same variables achieved better performances. Also we show that greater improvements in the model performances can be achieved if the types of the different maintenance tasks is taken into account. In the second phase we performed a replicated assessment of the effort prediction models built in the previous\u00a0\u2026", "num_citations": "100\n", "authors": ["303"]}
{"title": "Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics\n", "abstract": " The goal of this paper is to investigate and assess the ability of explanatory models based on design metrics to describe and predict defect counts in an object-oriented software system. Specifically, we empirically evaluate the influence of design decisions to defect behavior of the classes in two products from the commercial software domain. Information provided by these models can help in resource allocation and serve as a base for assessment and future improvements.We use innovative statistical methods to deal with the peculiarities of the software engineering data, such as non-normally distributed count data. To deal with overdispersed data and excess of zeroes in the dependent variable, we use negative binomial (NB) and zero-inflated NB regression in addition to Poisson regression.Furthermore, we form a framework for comparison of models\u2019 descriptive and predictive ability. Predictive capability of the\u00a0\u2026", "num_citations": "99\n", "authors": ["303"]}
{"title": "Identifying reusable functions using specification driven program slicing: a case study\n", "abstract": " We present the results of a case study in identifying and isolating reusable functions from C programs. The work exploits and specializes to programs written in C the theoretical framework of specification driven program slicing, a new program slicing process for isolating code fragments implementing functional abstractions. The specification of the function to be isolated, given in terms of a precondition and a postcondition, is used to identify a suitable slicing criterion. The preconditions for the execution of program statements and predicates are abstracted by using symbolic execution and compared with the conditions of the specification. The statements whose preconditions are equivalent to the pre and postconditions of the functional abstraction are candidates for entry and exit points of the slice implementing the abstraction. Once the slicing criterion has been identified, the slice can be isolated using algorithms\u00a0\u2026", "num_citations": "94\n", "authors": ["303"]}
{"title": "Recovering code to documentation links in OO systems\n", "abstract": " Software system documentation is almost always expressed informally, in natural language and free text. Examples include requirement specifications, design documents, user manual pages, system development journals, error logs and related maintenance reports. We propose an approach to establish and maintain traceability links between the source code and free-text documents. A premise of our work is that programmers use meaningful names for program's items, such as functions, variables, types, classes and methods. We believe that the application domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high-level concepts with program concepts, and vice versa. In this paper, the approach is applied to software written in an object-oriented (OO) language, namely C++, to trace\u00a0\u2026", "num_citations": "90\n", "authors": ["303"]}
{"title": "Developing legacy system migration methods and tools for technology transfer\n", "abstract": " This paper presents the research results of an ongoing technology transfer project carried out in cooperation between the University of Salerno and a small software company. The project is aimed at developing and transferring migration technology to the industrial partner. The partner should be enabled to migrate monolithic multi\u2010user COBOL legacy systems to a multi\u2010tier Web\u2010based architecture. The assessment of the legacy systems of the partner company revealed that these systems had a very low level of decomposability with spaghetti\u2010like code and embedded control flow and database accesses within the user interface descriptions. For this reason, it was decided to adopt an incremental migration strategy based on the reengineering of the user interface using Web technology, on the transformation of interactive legacy programs into batch programs, and the wrapping of the legacy programs. A\u00a0\u2026", "num_citations": "83\n", "authors": ["303"]}
{"title": "Business process reengineering and workflow automation: a technology transfer experience\n", "abstract": " In the last few years many public and private organizations have been changing the way of thinking their business processes to improve the quality of delivered services while achieving better efficiency and efficacy. This paper presents results and lessons learned from an on-going technology-transfer research project aimed at introducing service and technology innovation within a peripheral public administration while transferring enabling workflow methodologies and technologies to local Small and Medium Enterprises (SMEs). We discuss a process reverse engineering approach and its application in the technology transfer project. We also discuss an approach for evaluation and assessment of workflow technology and present a prototype implementation for a selected process of the subject organization.", "num_citations": "79\n", "authors": ["303"]}
{"title": "Maintaining traceability links during object\u2010oriented software evolution\n", "abstract": " This paper presents a method to build and maintain traceability links and properties of a set of object\u2010oriented software releases. The method recovers an \u2018as is\u2019 design from C++ software releases, compares recovered designs at the class interface level, and helps the user to deal with inconsistencies by pointing out regions of code where differences are concentrated. The comparison step exploits edit distance and a maximum match algorithm. The method has been experimented with on two freely available C++ systems. Results as well as examples of applications to the visualization of the traceability information and to the estimation of the size of changes during maintenance are reported in the paper. Copyright \u00a9 2001 John Wiley & Sons, Ltd.", "num_citations": "71\n", "authors": ["303"]}
{"title": "Tracing object-oriented code into functional requirements\n", "abstract": " Software system documentation is almost always expressed informally, in natural language and free text. Examples include requirement specifications, design documents, manual pages, system development journals, error logs and related maintenance reports. We propose an approach to establish and maintain traceability links between source code and free text documents. A premise of our work is that programmers use meaningful names for program items, such as functions, variables, types, classes, and methods. We believe that the application-domain knowledge that programmers process when writing the code is often captured by the mnemonics for identifiers; therefore, the analysis of these mnemonics can help to associate high level concepts with program concepts, and vice-versa. The approach is applied to software written in an object oriented language, namely Java, to trace classes to functional\u00a0\u2026", "num_citations": "68\n", "authors": ["303"]}
{"title": "Design pattern recovery by visual language parsing\n", "abstract": " We propose an object oriented (OO) design pattern recovery approach which makes use of a design pattern library, expressed in terms of visual grammars, and based on a visual language parsing technique. We also present a visual environment, which supports the pattern recognition process by automatically retrieving design patterns from imported UML class diagrams. The visual environment has been automatically generated through the VLDesk system, starting from a description of the design pattern grammar.", "num_citations": "67\n", "authors": ["303"]}
{"title": "Evaluating legacy system migration technologies through empirical studies\n", "abstract": " We present two controlled experiments conducted with master students and practitioners and a case study conducted with practitioners to evaluate the use of MELIS (Migration Environment for Legacy Information Systems) for the migration of legacy COBOL programs to the web. MELIS has been developed as an Eclipse plug-in within a technology transfer project conducted with a small software company [16]. The partner company has developed and marketed in the last 30 years several COBOL systems that need to be migrated to the web, due to the increasing requests of the customers. The goal of the technology transfer project was to define a systematic migration strategy and the supporting tools to migrate these COBOL systems to the web and make the partner company an owner of the developed technology. The goal of the controlled experiments and case study was to evaluate the effectiveness of\u00a0\u2026", "num_citations": "61\n", "authors": ["303"]}
{"title": "Web site reengineering using RMM\n", "abstract": " CiNii \u8ad6\u6587 - Web Site Reengineering Using RMM CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3 ] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7 \u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8 \u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 Web Site Reengineering Using RMM ANTONIOL G. \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 ANTONIOL G. \u53ce\u9332\u520a\u884c\u7269 Proc. 2nd International Workshop on Web Site Evolution, 2000 Proc. 2nd International Workshop on Web Site Evolution, 2000, 2000 \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u52d5\u7684\u89e3\u6790\u306b\u3088\u308bWeb\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30fb\u30e2\u30c7\u30eb\u62bd\u51fa\u652f\u63f4\u624b\u6cd5 \u5b89\u90e8 \u9ebb\u91cc , \u798f\u7530 \u5065\u592a\u90ce , \u5800 \u96c5\u6d0b , \u7530\u4e95 \u79c0\u6a39 , \u6839\u8def\u9298 \u5d07 , \u5c0f\u91ce \u5eb7\u4e00 , \u5927\u91ce \u7fa9\u592b \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u8ad6\u6587\u8a8c 46(3), 683-693, 2005-03-15 \u53c2\u8003\u6587\u732e22\u4ef6 CiNii\u5229\u7528\u8005\u30a2\u30f3\u30b1\u30fc\u30c8 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID/\u2026", "num_citations": "61\n", "authors": ["303"]}
{"title": "SLMeeting: supporting collaborative work in Second Life\n", "abstract": " Second Life is a virtual world which is often used for the synchronous meeting of teams. However, supporting distributed meeting goes beyond supporting user activities during the meeting itself, because it is also necessary to facilitate their coordination, arrangement and set up.", "num_citations": "57\n", "authors": ["303"]}
{"title": "Improving behavioral design pattern detection through model checking\n", "abstract": " Recovering design pattern instances in a software system can help maintainers to understand its design and implementation. In this paper we present a fully automated design pattern recovery approach that analyzes the behavior of pattern instances both statically and dynamically. In particular, the proposed approach exploits model checking to statically verify the behavioral aspects of design pattern instances. To this end, we encode the properties defining the correct behavior of a pattern as LTL (Linear Temporal Logic) formulae and the sequence diagram representing the possible interaction traces among the objects involved in the candidate instances as PROMELA specifications. To verify whether the LTL properties are satisfied by the candidates we employ the SPIN model checking tool. The dynamic analysis of the pattern behavior is performed through a code instrumentation and monitoring phase applied\u00a0\u2026", "num_citations": "53\n", "authors": ["303"]}
{"title": "ADAMS: an Artefact-based Process Support System.\n", "abstract": " We present ADAMS (ADvanced Artefact Management System), a Web-based system that integrates project management features such as resource allocation and process control and artefact management features, such as coordination of cooperative workers and artefact versioning, as well as context-awareness and artefact traceability. Maintaining traceability links (dependencies) between artefacts supports management of changes during incremental and iterative software development in a flexible way. Basically, the traceability layer is used to propagate events concerning changes to an artefact to the dependent artefacts, thus also increasing the context awareness in the project.", "num_citations": "52\n", "authors": ["303"]}
{"title": "Behavioral pattern identification through visual language parsing and code instrumentation\n", "abstract": " In this paper we present a new technique able to recover behavioral design pattern instances which combines static analysis, based on visual language parsing, with dynamic analysis, based on source code instrumentation. In particular, the dynamic analysis is performed through the automatic instrumentation of the method calls involved in the candidate pattern instances identified during static analysis. The results obtained from a program monitoring activity are matched against the definitions of the pattern behaviors expressed in terms of monitoring grammars. We also present and discuss the results of a case study on JHotDraw 5.1 software library performed to assess the retrieval effectiveness of the proposed approach.", "num_citations": "51\n", "authors": ["303"]}
{"title": "Effort estimation for corrective software maintenance\n", "abstract": " This paper reports on an empirical study aiming at constructing cost estimation models for corrective maintenance projects. Data available were collected from five maintenance projects currently carried out by a large software enterprise. The resulting models, constructed using multivariate linear regression techniques, allow to estimate the costs of a project conducted according to the adopted maintenance processes. Model performances on future observations were achieved by taking into account different corrective maintenance task typologies, each affecting the effort in a different way, and assessed by means of a cross validation which guarantees a nearly unbiased estimate of the prediction error. The constructed models are currently adopted by the subject company.", "num_citations": "48\n", "authors": ["303"]}
{"title": "A decisional framework for legacy system management\n", "abstract": " Making a decision about how to evolve a legacy system cannot be made spontaneously; rather, it requires a decisional framework that takes into account several factors including software value, risk analysis, and cost estimation. We present a decisional framework to manage legacy systems that exploits an assessment model and a taxonomy of maintenance interventions a legacy system can undergo during its life-cycle. The decisional framework has been defined within a pilot project involving a major international software enterprise. The project aims at assessing and improving the current practices of the organization and at experimenting software maintenance processes conducted by teams distributed at different sites in a cooperative networking environment.", "num_citations": "48\n", "authors": ["303"]}
{"title": "Assessing the maintenance processes of a software organization: an empirical analysis of a large industrial project\n", "abstract": " The use of statistical process control methods can determine the process capability of sustaining stable levels of variability, so that processes will yield predictable results. This enables to prepare achievable plans, meet cost estimates and scheduling commitments, and deliver required product functionality and quality with acceptable and reasonable reliability. We present initial results of applying statistical analysis methods to the maintenance processes of a software organization rated at the CMM level 3 that is currently planning the assessment to move to the CMM level 4. In particular, we present results from an empirical study conducted on the massive adaptive maintenance process of the organization. We analyzed the correlation between the maintenance size and productivity metrics. The resulting models allow to estimate the costs of a project conducted according to the adopted maintenance processes\u00a0\u2026", "num_citations": "47\n", "authors": ["303"]}
{"title": "A collaborative augmented campus based on location-aware mobile technology\n", "abstract": " Mobile devices are changing the way people work and communicate. Most of the innovative devices offer the opportunity to integrate augmented reality in mobile applications, permitting the combination of the real world with virtual information. This feature can be particularly useful to enhance informal and formal didactic actions based on student collaboration. This paper describes a \u201ccollaborative campus\u201d, originated in the physical architectural space, but exposing learning contents and social information structured as augmented virtual areas. ACCampus, a mobile augmented reality system, supporting the sharing of contextualized information is proposed. This system combines the world perceived by the phone camera with information concerning student location and community, enabling users to share multimedia information in location-based content areas. User localization is initially detected through QR\u00a0\u2026", "num_citations": "45\n", "authors": ["303"]}
{"title": "Integrating document and workflow management tools using XML and web technologies: a case study\n", "abstract": " A critical point for developing successful information systems for distributed organisations is the need for integrating heterogeneous technologies and tools. This paper reports a case study of integrating two key enabling technologies, namely workflow and document management. Integration is achieved by combining several approaches, including software engineering and hypertexts. In this way, we raise the integration problem from the level of a purely technical issue to a level of conceptual modelling: integration is not focused solely on the information/software systems but involves, and is driven by, the related business processes and the documents they deal with.", "num_citations": "45\n", "authors": ["303"]}
{"title": "Qualifying reusable functions using symbolic execution\n", "abstract": " We present an approach to reverse engineering source code modules to abstract their interface and functional specifications. A fine-grained program representation for C programs, the Combined C Graph (CGG), is used to perform dynamic analysis of a module by means of symbolic execution techniques. We focus on problems like pointer variables and expressions containing embedded side-effects and control flows. The role of the user interaction to understand the module behaviour and to deal with invariant assertions is also discussed. The functional specifications are produced in the form of first order logic formulas called preconditions and postconditions.", "num_citations": "45\n", "authors": ["303"]}
{"title": "A framework of syntactic models for the implementation of visual languages\n", "abstract": " We present a framework of syntactic models for the definition and implementation of visual languages. We analyze a wide range of existing visual languages and, for each of them, we propose a characterization according to a syntactic model. The framework has been implemented in the Visual Language Compiler-Compiler (VLCC) system. VLCC is a practical, flexible and extensible tool for the automatic generation of visual programming environments which allows to implement visual languages once they are modeled according to a syntactic model.", "num_citations": "43\n", "authors": ["303"]}
{"title": "Incremental migration strategies: Data flow analysis for wrapping\n", "abstract": " Incremental migration strategies entail the decomposition of large legacy systems in components that can be independently and selectively replaced; this reduces the costs and risks of a migration program. The legacy components are encapsulated into object wrappers and used (through the wrapper interface) in their original form until new components take up their functions with an acceptable level of reliability. The decomposition of legacy programs in components to be encapsulated in different wrappers involves reengineering activities for creating a new program for each component. Data flow analysis methods are needed for identifying the formal parameters in the interfaces of such programs. We present the approach defined within the project ERCOLE, a research project aiming at migrating legacy systems towards object-oriented platforms.", "num_citations": "41\n", "authors": ["303"]}
{"title": "Development and evaluation of a system enhancing Second Life to support synchronous role\u2010based collaborative learning\n", "abstract": " Research and commercial interest toward 3D virtual worlds are recently growing because they probably represent the new direction for the next generation of web applications. Although these environments present several features that are useful for informal collaboration, structured collaboration is required to effectively use them in a working or in a didactical setting. This paper presents a system supporting synchronous collaborative learning by naturally enriching Learning Management System services with meeting management and multimedia features. Monitoring and moderation of discussions are also managed at a single group and at the teaching level. The Second Life (SL) environment has been integrated with two ad hoc developed Moodle plug\u2010ins and SL objects have been designed, modeled, and programmed to support synchronous role\u2010based collaborative activities. We also enriched SL with tools\u00a0\u2026", "num_citations": "40\n", "authors": ["303"]}
{"title": "Supporting distributed software development with fine-grained artefact management\n", "abstract": " Distributed software development is increasingly becoming a common practice in the software industry. The increased complexity of software systems also reflects in the complexity of design documentation, thus requiring a specific tool support for change and configuration management in distributed development settings. We present the fine-grained versioning management approach adopted in the ADAMS artefact management system, focusing on support to high level documentation versioning. We also present the results of experimenting the tool in software development projects developed at the University of Salerno", "num_citations": "39\n", "authors": ["303"]}
{"title": "Understanding SQL through iconic interfaces\n", "abstract": " Visual query languages represent an evolution, in terms of understandability and adaptability, with respect to traditional textual languages. We present an iconic query system that enables the interaction of a novice user with a relational database. Our goal is to help a novice user to learn and comprehend the relational data model and a textual query language such as SQL, through the use of the iconic metaphore. In this sense our approach is different from most of the visual query systems proposed in the literature that present the user with a higher level query language, hiding the underlying data model. We also present results from an experiment conducted with first year students to evaluate the effectiveness of our approach.", "num_citations": "39\n", "authors": ["303"]}
{"title": "Enhancing collaborative synchronous UML modelling with fine-grained versioning of software artefacts\n", "abstract": " Software development teams are composed of people with different knowledge and skills, who contribute to a project from often widely dispersed locations. Software development in geographically distributed environments creates software engineering challenges due to the interaction among members of distributed teams and the management of consistency and concurrency among project artefacts. In this paper, we propose Synchronous collaborative modelling Tool Enhanced with VErsioning management (STEVE) a collaborative tool supporting distributed Unified Modelling Language (UML) modelling of software systems. The tool provides a communication infrastructure enabling the concurrent editing of the same UML diagram at the same time by distributed developers. Complex UML diagrams are decomposed and managed in a fine-grained hierarchy of sub-artefacts, thus providing change and\u00a0\u2026", "num_citations": "37\n", "authors": ["303"]}
{"title": "Reengineering web applications based on cloned pattern analysis\n", "abstract": " Web applications are subject to continuous and rapid evolution. Often it happens that programmers indiscriminately duplicate Web pages without considering systematic development and maintenance methods. This practice creates code clones that make Web applications hard to maintain and reuse. This paper presents an approach for reengineering Web applications based on clone analysis that aims at identifying and generalizing static and dynamic pages and navigational patterns of a Web application. Clone analysis is also helpful for identifying literals that can be generated from a database. A case study is described which shows how the proposed approach can be used for restructuring the navigational structure of a Web application by removing redundant code.", "num_citations": "35\n", "authors": ["303"]}
{"title": "An extensible system for source code analysis\n", "abstract": " Constructing code analyzers may be costly and error prone if inadequate technologies and tools are used. If they are written in a conventional programming language, for instance, several thousand lines of code may be required even for relatively simple analyses. One way of facilitating the development of code analyzers is to define a very high-level domain-oriented language and implement an application generator that creates the analyzers from the specification of the analyses they are intended to perform. This paper presents a system for developing code analyzers that uses a database to store both a no-loss fine-grained intermediate representation and the results of the analyses. The system uses an algebraic representation, called F(p), as the user-visible intermediate representation. Analyzers are specified in a declarative language, called F(p)-l, which enables an analysis to be specified in the form of a\u00a0\u2026", "num_citations": "35\n", "authors": ["303"]}
{"title": "An Eclipse plug-in for the detection of design pattern instances through static and dynamic analysis\n", "abstract": " The extraction of design pattern information from software systems can provide conspicuous insight to software engineers on the software structure and its internal characteristics. In this demonstration we present ePAD, an Eclipse plug-in for recovering design pattern instances from object-oriented source code. The tool is able to recover design pattern instances through a structural analysis performed on a data model extracted from source code, and a behavioral analysis performed through the instrumentation and the monitoring of the software system. ePAD is fully configurable since it allows software engineers to customize the design pattern recovery rules and the layout used for the visualization of the recovered instances.", "num_citations": "34\n", "authors": ["303"]}
{"title": "Managing coordination and cooperation in distributed software processes: the GENESIS environment\n", "abstract": " We present the GENESIS platform (GEneralised eNvironment for procEsS management in cooperatIve Software engineering), the outcome of a research project aiming at designing and developing a noninvasive and open\u2010source system to support software engineering processes in a highly distributed environment. The system supports the cooperation and coordination in software processes as its process modeling language enables the decomposition of complex processes into subprocesses that can be distributed and executed at different organizational sites. In GENESIS, workflow management technologies have been integrated with artifact management and communication services to meet the necessary requirements of managing the cooperation among distributed teams. Its strengths are a powerful activity management, covering all the main aspects of the life cycle of an activity; an efficient and flexible\u00a0\u2026", "num_citations": "33\n", "authors": ["303"]}
{"title": "Positional grammars: A formalism for LR-like parsing of visual languages\n", "abstract": " Positional grammars naturally extend context-free grammars for string languages to grammars for visual languages by considering new relations in addition to string concatenation. Thanks to this analogy, most results from LR parsing can be extended to positional grammars while preserving its well known efficiency. The positional grammar model is the underlying formalism of the VLCC (Visual Language Compiler-Compiler) system for the automatic generation of visual programming environments. VLCC inherits and extends to the visual field, concepts and techniques of compiler generation tools like YACC. Due to their nature, the positional grammars are a very suitable formalism for processing languages integrating visual and textual constructs.", "num_citations": "33\n", "authors": ["303"]}
{"title": "Identifying Clones in Dynamic Web Sites Using Similarity Thresholds.\n", "abstract": " We propose an approach to automatically detect duplicated pages in dynamic Web sites and on the analysis of both the page structure, implemented by specific sequences of HTML tags, and the displayed content. In addition, for each pair of dynamic pages we also consider the similarity degree of their scripting code. The similarity degree of two pages is computed using different similarity metrics for the different parts of a web page based on the Levenshtein string edit distance. We have implemented a prototype to automate the clone detection process on web applications developed using JSP technology and used it to validate our approach in a case study.", "num_citations": "32\n", "authors": ["303"]}
{"title": "An integrated environment for reuse reengineering C code\n", "abstract": " The paper presents an integrated environment implemented in Prolog for reuse reengineering existing C systems. Different tools developed in the RE2 project are integrated in the environment through sharing a fine-grained representation for C programs, the Combined C Graph (CCG). Different views of a system can be abstracted and visualised from the data-base of Prolog facts implementing its CCG representation. Software metric tools evaluate the reengineering costs, while reengineering operations are expressed as transformation rules and a symbolic executor allows the production of the reusable module's specification.", "num_citations": "31\n", "authors": ["303"]}
{"title": "A two phase approach to design pattern recovery\n", "abstract": " In this paper we present a two phase approach to the recovery of structural design pattern. In the first phase, the design pattern instances are identified at a course-grained level by considering the design structure only and using a visual language parsing technique. Then, the identified candidate patterns are validated by a fine-grained source code analysis phase. The latter phase is an enhancement of a previous approach developed by the authors aiming at improving the results of precision and time performances. The retrieval effectiveness of the approach is assessed by applying the recovery technique on four software systems", "num_citations": "29\n", "authors": ["303"]}
{"title": "A strategy and an Eclipse based environment for the migration of legacy systems to multi-tier web-based architectures\n", "abstract": " We present an incremental approach to the migration of non decomposable COBOL applications to a Web-enabled multi-tier architecture. The relevant software components of the target architecture, namely the communication middleware and the generator of graphical user interfaces, are developed once for all in order to reduce the migration effort. An Eclipse plug-in has also been developed to support the software engineer in the migration of the graphical user interface and in the restructuring and wrapping of the original legacy code. A pilot project on a COBOL legacy system evolved during the last thirty years has been used to experiment the migration strategy and the plug-in", "num_citations": "28\n", "authors": ["303"]}
{"title": "Clustering algorithms and latent semantic indexing to identify similar pages in web applications\n", "abstract": " In this paper, we analyze some clustering algorithms that have been widely employed in the past to support the comprehension of Web applications. To this end, we have defined an approach to identify static pages that are duplicated or cloned at the content level. This approach is based on a process that first computes the dissimilarity between Web pages using latent semantic indexing, a well known information retrieval technique, and then groups similar pages using clustering algorithms. We consider five instances of this process, each based on three variants of the agglomerative hierarchical clustering algorithm, a divisive clustering algorithm, k-means partitional clustering algorithm, and a widely employed partitional competitive clustering algorithm, namely Winner Takes All. In order to assess the proposed approach, we have used the static pages of three Web applications and one static Web site.", "num_citations": "25\n", "authors": ["303"]}
{"title": "Case studies of visual language based design patterns recovery\n", "abstract": " In this paper, we present case studies of recovering structural design patterns from object-oriented source code. The proposed recovery technique is based on the use of visual language parsing techniques, and is supported by a visual environment automatically produced by a grammar based visual environment generator. We have applied the recovery technique to public-domain programs and libraries obtaining encouraging results. In particular, for the considered software our recovery approach is characterized by higher recall and precision values with respect to other recovery techniques", "num_citations": "25\n", "authors": ["303"]}
{"title": "Virtual Worlds, do we really need the third dimension to support collaborative learning\n", "abstract": " In this paper we report on an empirical study that assessed the value added by a Second Life based meeting system to a collaborative learning activity, as compared to a meeting system based on synchronous text-based communication. The experiment results show that the adoption of a 3D virtual environment does not either improve the perceived level of comfort with communication or introduce distraction during the activity, whereas the user perception of the feature offered is positive.", "num_citations": "24\n", "authors": ["303"]}
{"title": "Supporting jigsaw-based collaborative learning in second life\n", "abstract": " In this paper we describe how to exploit the 3D programmable virtual world provided by second life to create an environment and a location for collaborative learning. To this aim second life objects have been modeled and programmed to support the synchronous role-based collaborative activities required by the jigsaw learning technique in a 3D virtual meeting setting. We have also integrated this approach with Moodle, in such a way to naturally enrich LMS services with meeting management, set-up features, and synchronous collaborative learning.", "num_citations": "24\n", "authors": ["303"]}
{"title": "Web site reuse: cloning and adapting\n", "abstract": " A common requirement when developing Web sites is a reduced time to operation. Reuse of conceptual views and code components from exiting operational sites is a key to reducing the development time and risks while improving the quality of the site under development. The authors outline an approach to reuse of existing sites by means of cloning and adaptations. The starting point is a repository of conceptual views of existing Web sites, corresponding code components, and traceability links between the conceptual views and the code. We use UML diagrams to express the conceptual views. The requirements of a new site are matched against the conceptual models in the repository to identify reuse candidates, and the traceability links are then exploited to isolate the corresponding code components and to devise any needed adaptations.", "num_citations": "24\n", "authors": ["303"]}
{"title": "Introducing workflow management in software maintenance processes\n", "abstract": " Software organizations are moving from traditional software factory models towards virtual organization models, where distributed teams converge in a temporary network with the aim of integrating different competences or solving problems in a cooperative way. Most workflow management systems of last generation are web based and this makes them a viable enabling technology for remodeling both the organization structure and its processes in order to move towards a virtual organization model and increase its competitiveness. We present a case study of introducing workflow technologies in a large software enterprise. In particular, a workflow-based prototype implementation for the management of the ordinary maintenance process is discussed.", "num_citations": "24\n", "authors": ["303"]}
{"title": "Understanding cloned patterns in web applications\n", "abstract": " We propose a tool to identify and analyze cloned patterns in Web applications using clone analysis and clustering of static and dynamic Web pages. The tool first detects cloned pages, which are then grouped into clusters as well as the groups of links between clusters. In this way the navigational schema is simplified and the duplicated functionalities corresponding to cloned navigational patterns can be more easily analyzed. The tool also allows filtering out uninteresting parts of the restructured navigational schema, thus to further improve the understanding.", "num_citations": "22\n", "authors": ["303"]}
{"title": "Identifying similar pages in web applications using a competitive clustering algorithm\n", "abstract": " We present an approach based on Winner Takes All (WTA), a competitive clustering algorithm, to support the comprehension of static and dynamic Web applications during Web application reengineering. This approach adopts a process that first computes the distance between Web pages and then identifies and groups similar pages using the considered clustering algorithm. We present an instance of application of the clustering process to identify similar pages at the structural level. The page structure is encoded into a string of HTML tags and then the distance between Web pages at the structural level is computed using the Levenshtein string edit distance algorithm. A prototype to automate the clustering process has been implemented that can be extended to other instances of the process, such as the identification of groups of similar pages at content level. The approach and the tool have been evaluated in\u00a0\u2026", "num_citations": "21\n", "authors": ["303"]}
{"title": "Detecting the behavior of design patterns through model checking and dynamic analysis\n", "abstract": " We present a method and tool (ePAD) for the detection of design pattern instances in source code. The approach combines static analysis, based on visual language parsing and model checking, and dynamic analysis, based on source code instrumentation. Visual language parsing and static source code analysis identify candidate instances satisfying the structural properties of design patterns. Successively, model checking statically verifies the behavioral aspects of the candidates recovered in the previous phase. We encode the sequence of messages characterizing the correct behaviour of a pattern as Linear Temporal Logic (LTL) formulae and the sequence diagram representing the possible interaction traces among the objects involved in the candidates as Promela specifications. The model checker SPIN verifies that candidates satisfy the LTL formulae. Dynamic analysis is then performed on the obtained\u00a0\u2026", "num_citations": "20\n", "authors": ["303"]}
{"title": "A component-based visual environment development process\n", "abstract": " We present the Component-Based Visual Environment Development (CB-VED) process for building visual language environments and introduce the Visual Language Desk (VLDesk) system supporting its implementation. The proposed approach is based on software reuse at different granularity levels and enables incremental development. The VLDesk exploits all the knowledge gained from the development of the Visual Language Compiler-Compiler tool extending its functionalities with many adjunctive features useful in the presented development process. One of the aims of this research consists of the application of software engineering techniques to the incremental development of visual language environments.", "num_citations": "20\n", "authors": ["303"]}
{"title": "Assessing massive maintenance processes: an empirical study\n", "abstract": " We present an empirical study from the experience of a major. international software enterprise in conducting massive adaptive maintenance projects with a close deadline. The adopted process entails the decomposition of the application portfolio into loosely coupled work-packets that can be independently and incrementally worked out by teams distributed on different sites. The study analyzes the correlation between maintenance size and productivity metrics of a large Y2K project. The resulting models allows to estimate the costs of a project conducted according to the adopted massive maintenance process and distribute them among the different phases.", "num_citations": "18\n", "authors": ["303"]}
{"title": "Identifying cloned navigational patterns in web applications\n", "abstract": " Web Applications are subject to continuous and rapid evolution. Often programmers indiscriminately duplicate Web pages without considering systematic development and maintenance methods. This practice creates code clones that make Web Applications hard to maintain and reuse. We present an approach to identify duplicated functionalities in Web Applications through cloned navigational pattern analysis. Cloned patterns can be generalized in a reengineering process, thus to simplify the structure and future maintenance of the Web Applications. The proposed method first identifies pairs of cloned pages by analyzing similarity at structure, content, and scripting code. Two pages are considered clones if their similarity is greater than a given threshold. Cloned pages are then grouped into clusters and the links connecting pages of two clusters are grouped too. An interconnection metric has been defined on the links between two clusters to express the effort required to reengineer them as well as to select the patterns of interest. To further reduce the comprehension effort, we filter out links and nodes of the clustered navigational schema that do not contribute to the identification of cloned navigational patterns. A tool supporting the proposed approach has been developed and validated in a case study.", "num_citations": "16\n", "authors": ["303"]}
{"title": "Automating the management of software maintenance workflows in a large software enterprise: a case study\n", "abstract": " This case study presents the results from a pilot project aimed at introducing workflow management technologies and a Web\u2010based software tool in a large software enterprise. In particular, we analyzed and modeled the workflows and documents at the site of the ordinary maintenance process and implemented a prototype for the management of the process using a commercial\u2010Web\u2010based workflow management system. This paper reports on the experience gained from a 10\u2010month project, which included the experimental use at a single site of the workflow prototype for 4 months in an industrial setting involving more than 800 maintenance service requests on a large software system. Copyright \u00a9 2002 John Wiley & Sons, Ltd.", "num_citations": "16\n", "authors": ["303"]}
{"title": "Concurrent fine-grained versioning of UML models\n", "abstract": " Concurrent versioning of source code is a common and well-established practice to manage concurrency and consistency within source code repository. With the growing complexity of nowadays software systems, the need for high level representations of the system to develop becomes inevitable. These software models evolve together with the software system, thus requiring versioning management. Moreover, software models are often the result of cooperative work by different software engineers, that need to update them even concurrently.Unfortunately, the available concurrent versioning tools, do not provide an adequate support for this type of software artifacts. We propose a solution to manage versioning and concurrency for software models (in particular, UML models), that consists of a fine-grained concurrent modeling approach. The approach has been implemented and integrated in an artifact\u00a0\u2026", "num_citations": "15\n", "authors": ["303"]}
{"title": "Towards efficient parsing of diagrammatic languages\n", "abstract": " Many models have been presented to specify visual languages and big efforts are being made to characterize a class of visual languages which is expressive enough and, at the same time, efficient to parse. Along this direction, the positional grammar model has been defined to extend the LR parsing techniques and parse efficiently not only the string languages but also iconic languages. In this paper, we present an extension of the positional grammar model in order to describe a wide variety of diagrammatic languages. We show that this new formalism allows the construction of an efficient LR-like parser for visual languages of very practical interest.", "num_citations": "15\n", "authors": ["303"]}
{"title": "Migrating legacy system to the Web: A business process reengineering oriented approach\n", "abstract": " The Internet is an extremely important new technology that is changing the way in which organizations conduct their business and interact with their partners and customers. To take advantage of the Internet open architecture, most companies are applying business reengineering with the aim of moving from hierarchical centralized structures to networked decentralized business units cooperating with one another. As a consequence, the way in which software information systems are conceived, designed, and built is changing too. Monolithic, mainframe-based systems are being replaced by distributed, Web-centric, component-based systems with an open architecture. Ideally, business process reengineering should entail the adoption of new software systems designed to satisfy the new needs of the redesigned business. However, economic and technical constraints make it impossible in most cases to discard the\u00a0\u2026", "num_citations": "13\n", "authors": ["303"]}
{"title": "Migrating legacy video lectures to multimedia learning objects\n", "abstract": " Video lectures are an old distance learning approach that offers only basic interaction and retrieval features to the user. Thus, to follow the new learning paradigms, we need to re\u2010engineer the e\u2010learning processes while preserving the investments made in the past. In this paper we present an approach for migrating video lectures to multimedia learning objects. Two essential problems are tackled: the detection of slide transitions and the generation of the learning objects. To this aim, the video of the lecture is scanned to detect the slide changes, while the learning object metadata and the slide pictures are extracted from the presentation document. A tool named VLMigrator (video lecture migrator) has been developed to support the migration of video lectures and the restructuring of their contents in terms of learning objects. Both the migration strategy and the tool have been experimented in a case study\u00a0\u2026", "num_citations": "12\n", "authors": ["303"]}
{"title": "Deriving workflow enactment rules from UML activity diagrams: a case study\n", "abstract": " In the last years, there has been a growing interest in enhancing the semantics of UML diagrams to automatically derive executable process models. We present a case study of mapping UML activity diagrams with object flow onto the process modeling language of a distributed process support system called GENESIS. UML activity diagrams do not support all the GENESIS control flow and dataflow rules. As a consequence, the syntax and semantics of this type of UML diagrams need to be enriched to make them suitable for process modeling in GENESIS.", "num_citations": "12\n", "authors": ["303"]}
{"title": "Generating applications directly on the mobile device: an empirical evaluation\n", "abstract": " This paper presents an investigation, based on the combined use of two techniques: a questionnaire-based survey and an empirical analysis, to assess the effectiveness and efficacy of the MicroApp environment to support End-Users in the visual composition of their own applications directly on their mobile phone. The satisfaction of the End-Users has been investigated as well. The context of this study was constituted of students, administrative personnel and consultants of the University of Salerno. The survey shows a positive satisfaction degree of all the involved subjects, while the empirical analysis reveals that the use of the Micro App tool increases the efficiency and, in case of complex tasks, also the simplicity with respect to the use of a PC-based similar tool proposed by Google.", "num_citations": "10\n", "authors": ["303"]}
{"title": "Using a competitive clustering algorithm to comprehend web applications\n", "abstract": " We propose an approach based on winner takes all, a competitive clustering algorithm, to support the comprehension of static and dynamic Web applications. The process first computes the distances between the Web pages and then identifies similar pages through the winner takes all clustering algorithm. Two different instances of the process are presented to identify similar pages at structural and content level, respectively. The first instance encodes the page structure into a string and then uses the Levenshtein algorithm to achieve the distances between pairs of pages. On the other hand, to group similar pages at content level we use the latent semantic indexing to produce the page representations as vectors in the concept space. The Euclidean distance is then computed between the vectors to achieve the distances between the pages to be given as input to the adopted clustering algorithm. A prototype to\u00a0\u2026", "num_citations": "10\n", "authors": ["303"]}
{"title": "Improving artefact quality management in advanced artefact management system with distributed inspection\n", "abstract": " Advanced artefact management system (ADAMS) is an artefact-based process support system for the management of human resources, projects and software artefacts. This system puts great emphasis on the artefact life cycle by associating software engineers with the different operations that can be performed on a given artefact. Managing the quality of software artefacts is considered as one of the main issues. To this end, ADAMS integrates web-based artefact inspection tool (WAIT), a web-based system implementing a distributed inspection process. A case study has been accomplished to evaluate both the integration of WAIT in ADAMS and the provided quality management support. The main result of this empirical investigation is that the integrated system provides an effective support for the management of the quality of software artefacts.", "num_citations": "9\n", "authors": ["303"]}
{"title": "Evaluating distributed inspection through controlled experiments\n", "abstract": " Inspection methods can be classified according to their discipline and flexibility. The discipline concerns the formal aspect of an inspection method, whereas the flexibility is strongly related to the simplicity of organising and conducting a meeting. The majority of the available distributed inspection methods have a high level of discipline and flexibility as they are based on a well-defined process and the discussion among team members is easily organised and conducted. In this study the authors present two controlled experiments to evaluate the effectiveness and the efficacy of a distributed inspection process to discover defects within source code. In particular, the first experiment compares the distributed inspection method proposed to a disciplined but not flexible method (i.e. the Fagan's inspection process). In the second experiment the authors investigate differences between the same distributed inspection\u00a0\u2026", "num_citations": "9\n", "authors": ["303"]}
{"title": "Recovering traceability links using information retrieval tools: a controlled experiment\n", "abstract": " Recovering Traceability Links using Information Retrieval Tools: a Controlled Experiment IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS IRIS Universit\u00e0 degli Studi del Molise Catalogo Ricerca 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno Recovering Traceability Links using Information Retrieval Tools: a Controlled Experiment Italiano Italiano Italiano Italiano English English Recovering Traceability Links using Information Retrieval Tools: a Controlled Experiment / DE LUCIA A; OLIVETO R; TORTORA G. - (2007), pp. 46-55. ((Intervento presentato al convegno International Symposium on Grand Challeges in Traceability tenutosi a Lexington, Kentucky, USA nel March 22-23, 2007. Scheda breve Scheda completa Titolo: Recovering Traceability Links using Information Retrieval Tools: a : \u2026", "num_citations": "9\n", "authors": ["303"]}
{"title": "Integrating document and workflow management systems\n", "abstract": " A critical point for developing successful information systems for distributed organisations is the need for integrating heterogeneous technologies and tools. This paper reports on an experience of integrating two key enabling technologies, namely workflow and document management.", "num_citations": "9\n", "authors": ["303"]}
{"title": "Towards automating dynamic analysis for behavioral design pattern detection\n", "abstract": " The detection of behavioral design patterns is more accurate when a dynamic analysis is performed on the candidate instances identified statically. Such a dynamic analysis requires the monitoring of the candidate instances at run-time through the execution of a set of test cases. However, the definition of such test cases is a time-consuming task if performed manually, even more, when the number of candidate instances is high and they include many false positives. In this paper we present the results of an empirical study aiming at assessing the effectiveness of dynamic analysis based on automatically generated test cases in behavioral design pattern detection. The study considered three behavioral design patterns, namely State, Strategy, and Observer, and three publicly available software systems, namely JHotDraw 5.1, QuickUML 2001, and MapperXML 1.9.7. The results show that dynamic analysis based on\u00a0\u2026", "num_citations": "8\n", "authors": ["303"]}
{"title": "ePadEvo: A tool for the detection of behavioral design patterns\n", "abstract": " In this demonstration we present ePAD evo , an Eclipse plug-in for recovering design pattern instances from object-oriented source code. The tool is able to recover design pattern instances through a static analysis performed on a data model extracted from source code, and a dynamic analysis performed through the instrumentation and the monitoring of the software system. Dynamic analysis is performed with automatically generated test cases exploiting the EvoSuite tool.", "num_citations": "8\n", "authors": ["303"]}
{"title": "Recovering design rationale from email repositories\n", "abstract": " Rationale is the justification behind decisions taken during the software development process. The usefulness of rationale pervades the entire software lifecycle. However, it is during maintenance that the benefits of rationale management are most evident, as it provides an insight into the motivations and the reasoning behind decisions taken during the original design and implementation. One of the strongest limitation to the capturing of rationale information during development concerns its time-consuming and disruptive nature that cause many organizations to consider rationale management costs excessive. A possible solution is to extract and capture rationale information when it is needed. This can be done by analyzing documents shared or exchanged among software engineers during the development process. In this paper, we propose to supports the software engineer during the rationale capturing by\u00a0\u2026", "num_citations": "8\n", "authors": ["303"]}
{"title": "COMOVER: Concurrent model versioning\n", "abstract": " Concurrent versioning of source code is a common and well-established practice to manage concurrency and consistency within source code repository. Similarly to source code, software models are often the result of cooperative work by different software engineers, that need to update them even concurrently. Unfortunately, modeling tools rarely provide support for concurrency and consistency. On the other hand, the available concurrent versioning tools do not provide an adequate support for software models. In this paper we present COMOVER (COncurrent MOdel VERsioning), a tool that integrates software modeling features with versioning and concurrency management as well as model elements sharing and exchanging.", "num_citations": "8\n", "authors": ["303"]}
{"title": "Assessing the effectiveness of a distributed method for code inspection: A controlled experiment\n", "abstract": " We propose a distributed inspection method that tries to minimise the synchronous collaboration among team members to identify defects in software artefacts. The approach consists of identifying conflicts on the potential defects and then resolving them using an asynchronous discussion before performing a traditional synchronous meeting. This approach has been implemented in a Web based tool and assessed through a controlled experiment with master students in Computer Science at the University of Salerno. The tool presented provides automatic merge and conflict highlighting functionalities to support the inspectors during the pre-meeting refinement phase and provides the moderator with information about the inspection progress as a decision support. The tool also supports a synchronous inspection meeting to discuss about unsolved conflicts. However, by analysing the data collected during a\u00a0\u2026", "num_citations": "8\n", "authors": ["303"]}
{"title": "GENESIS: A flexible and distributed environment for cooperative software engineering\n", "abstract": " GENESIS: A Flexible and Distributed Environment for Cooperative Software Engineering IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS Archivio della ricerca dell'Universit\u00e0 degli studi del Sannio Catalogo Ricerca 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno GENESIS: A Flexible and Distributed Environment for Cooperative Software Engineering Italiano Italiano English GENESIS: A Flexible and Distributed Environment for Cooperative Software Engineering / Aversano L; De Lucia A; Gaeta M; Ritrovato P. - (2003), pp. 497-502. ((Intervento presentato al convegno Proceedings of the Fifteenth International Conference on Software Engineering {\\&} Knowledge Engineering (SEKE'2003),. Scheda breve Scheda completa Titolo: GENESIS: A Flexible and Distributed Environment for Autori : , \u2026", "num_citations": "8\n", "authors": ["303"]}
{"title": "Efficient Parsing of Data-Flow Graphs.\n", "abstract": " Data flow graphs are very popular for the definition of visual interfaces. Many graphical languages including flow chart, electric and logic circuit, chemical structure, and visual programming languages are in fact based on it. A key problem for the practicality of visual languages in man-machine interaction is the efficient and accurate recognition of visual patterns. In this paper, we present a new approach to describe and efficiently parse data-flow graphs by exploiting results from the LR parsing theory.", "num_citations": "8\n", "authors": ["303"]}
{"title": "The software development workbench WSDW\n", "abstract": " This paper presents the architecture and some tools of the software development workbench WSDW. The authors propose a structure-oriented workbench, in which interactive software tools are integrated through sharing a unique high level program representation, satisfying the request of independence from the source language. The data structure representing programs, the web structure, is based upon the mathematical concept of relation and it is easily implemented as a Prolog data base. Program transformations, given as web transformations, can be expressed as rewriting rules, so that software tools can be implemented as sets of rewriting rules and then added to the WSDW.< >", "num_citations": "8\n", "authors": ["303"]}
{"title": "An approach and an eclipse based environment for data migration\n", "abstract": " This paper presents an incremental approach to migrate COBOL non decomposable data-intensive applications based on files to a modern relational database management system. This approach has been defined and developed within a technology transfer project carried out in cooperation between the University of Salerno and a small industrial partner. The developed approach first reengineers the original database and then adapts the code of the legacy application to enable the communication with reengineered database. A wrapper enables the communication between the legacy code and the new database. To support the software engineer in the different phases of the defined approach an Eclipse plug-in has also been developed. Finally, both the plug-in and the approach have been assessed on a legacy system of our industrial partner.", "num_citations": "7\n", "authors": ["303"]}
{"title": "Evolution of software composition mechanisms: a survey\n", "abstract": " In engineering, developing a system always implies designing its structure, by providing a proper decomposition into separate parts and relationships among them. This approach allows engineers to address the complexity of a system by applying a divide-andconquer approach\u2014that is, by recursively focusing on limited subsystems and on the interaction between them.In this setting, it is possible to identify two phases: the definition of the behavior of single parts and the composition of parts. Composition is the way the whole system is constructed by binding components together. In this phase, engineers focus on how the relations among parts are established, rather than on the specific internal structure or behavior of the components.", "num_citations": "7\n", "authors": ["303"]}
{"title": "Empirical studies in software maintenance and evolution\n", "abstract": " While most researchers agree on the need for empirical validation of theoretical results, two main issues remain unaddressed: first, few such studies are actually performed and second, coordination among different studies is very rare. This working session aim at bringing together the researchers interested in conducting empirical studies in maintenance and evolution. The goal is to define an important topic, design a family of experiments, provide the basis to conduct a set of coordinated experiments to advance the state of empirical evidence in this area.", "num_citations": "7\n", "authors": ["303"]}
{"title": "Assessing legacy system migration technologies through controlled experiments\n", "abstract": " We present the results of two controlled experiments aimed at assessing MELIS (migration environment for legacy information systems), a tool developed within a technology transfer project to support the migration of COBOL legacy information systems to a J2EE web-enabled multi-tier target architecture. The first controlled experiment was conducted within an academic research laboratory with master students in Computer Science at the University of Salerno, while a replicated experiment was conducted in the laboratory of our partner company with both professional programmers and academic researchers. The results revealed that the use of MELIS decrease the time to migrate legacy systems to the web with respect to the use of traditional development tools. Also, less expert software engineers benefit more of the use of MELIS.", "num_citations": "7\n", "authors": ["303"]}
{"title": "Workflow management in the GENESIS environment\n", "abstract": " We present the workflow management system of GENESIS (Generalised ENvironment for procESs management in cooperatIve Software engineering), an on-going research project aiming at designing and developing a non-invasive and open source system to support software engineering processes in a highly distributed environment. The process modelling language enables the decomposition of complex processes into sub-processes that can be distributed and executed at different organizational sites. In GENESIS, workflow management technologies have been integrated with artefact management and communication services to meet the necessary requirement of managing cooperation among software engineers from different locations.", "num_citations": "7\n", "authors": ["303"]}
{"title": "Empirical analysis of massive maintenance processes\n", "abstract": " We present initial results of applying statistical control techniques to the massive maintenance processes of a software organization rated at the CMM level 3. In particular, we present results from an empirical study conducted on a large massive adaptive maintenance project. In a previous study (2001) we analyzed the correlation between the maintenance size and productivity metrics and produced models to estimate the costs of a project conducted according to the adopted maintenance processes. In this paper we analyze data about the single phases of the process and, in particular, the distribution of the effort among the phases and causes of variations.", "num_citations": "7\n", "authors": ["303"]}
{"title": "Specifying code analysis tools\n", "abstract": " Customised code analysis tools for the maintenance and evolution of existing software systems can be created by storing program information in a database, and using an application generator to translate the high-level specifications of the analyses the tools are intended to perform. We present a high-level domain-specific language for the specification of program analysis tools that exploit an algebraic program representation called F(p). The algebraic representation is a compact program view which describes the static composition of the control structures and the set of the resulting potential executions. Operands of the algebraic expression (that represent the program's constructs) are used as indexes to access information stored in the database. The specification language provides facilities for the traversal of the program representation and access to the associated information in the database. The program\u00a0\u2026", "num_citations": "7\n", "authors": ["303"]}
{"title": "Identifying reusable functions in code using specification driven techniques\n", "abstract": " The work described in this thesis addresses the field of software reuse. Software reuse is widely considered as a way to increase the productivity and improve the quality and reliability of new software systems. Identifying, extracting and reengineering software. components which implement abstractions within existing systems is a promising cost-effective way to create reusable assets. Such a process is referred to as reuse reengineering. A reference paradigm has been defined within the RE(^2) project which decomposes a reuse reengineering process in five sequential phases. In particular, the first phase of the reference paradigm, called Candidature phase, is concerned with the analysis of source code for the identification of software components implementing abstractions and which are therefore candidate to be reused. Different candidature criteria exist for the identification of reuse-candidate software components. They can be classified in structural methods (based on structural properties of the software) and specification driven methods (that search for software components implementing a given specification).In this thesis a new specification driven candidature criterion for the identification and the extraction of code fragments implementing functional abstractions is presented. The method is driven by a formal specification of the function to be isolated (given in terms of a precondition and a post condition) and is based on the theoretical frameworks of program slicing and symbolic execution. Symbolic execution and theorem proving techniques are used to map the specification of the functional abstractions onto a slicing criterion. Once the\u00a0\u2026", "num_citations": "7\n", "authors": ["303"]}
{"title": "The Tool Development Language TDL for the Software Development Environment WSDW.\n", "abstract": " A tool development language (TDL) for the software development workbench WSDW (Web structure oriented Software Development Workbench) is presented. TDL is a graphical language which allows a user to create new tools for software manipulation in the WSDW environment WSDW is astructure-oriented workbench, in which interactive software tools share a unique data structure representing programs which is based upon the mathematical concept of relation. The web structure [1 1] is the basic high level representation of programs within the environment. Manipulations of programs can be expressed as web transformations. A tool of WSDW performs a sequence of web transformations. Hence, the elementary statements in a TDL program are web transformations which are expressed graphically.", "num_citations": "7\n", "authors": ["303"]}
{"title": "Rhotics: New data and perpectives\n", "abstract": " This book provides an insight into the patterns of variation and change of rhotics in different languages and from a variety of perspectives. It sheds light on the phonetics, the phonology, the socio-linguistics and the acquisition of/r/-sounds in languages as diverse as Dutch, English, French, German, Greek, Hebrew, Italian, Kuikuro, Malayalam, Romanian, Slovak, Tyrolean and Washili Shingazidja thus contributing to the discussion on the unity and uniqueness of this group of sounds.This book provides an insight into the patterns of variation and change of rhotics in different languages and from a variety of perspectives. It sheds light on the phonetics, the phonology, the socio-linguistics and the acquisition of/r/-sounds in languages as diverse as Dutch, English, French, German, Greek, Hebrew, Italian, Kuikuro, Malayalam, Romanian, Slovak, Tyrolean and Washili Shingazidja thus contributing to the discussion on the unity and uniqueness of this group of sounds.", "num_citations": "6\n", "authors": ["303"]}
{"title": "MELIS: an Eclipse based environment for the migration of legacy systems to the web\n", "abstract": " In this demonstration, we present MELIS (migration environment for legacy information systems) an integrated environment for the migration of legacy systems to a multi-tier Web-based architecture. This environment has been developed as an Eclipse plug-in to support the software engineer in the migration of the graphical user interface, and in the restructuring and wrapping of the original legacy code", "num_citations": "6\n", "authors": ["303"]}
{"title": "Does software error/defect identification matter in the Italian industry?\n", "abstract": " The authors present the results of a descriptive survey to ascertain the relevance and the typology of the software error/ defect identification methods/approaches used in the industrial practice. This study involved industries/organisations that develop and sell software as a main part of their business or develop software as an integral part of their products or services. The results indicated that software error/defect identification is very relevant and regard almost the totality of the interviewed companies. The most widely used and popular practice is testing. An increasing interest has been also manifested in distributed inspection methods.", "num_citations": "5\n", "authors": ["303"]}
{"title": "Augmented reality mobile applications: Challenges and solutions\n", "abstract": " Mobile devices are radically changing the way in which we work and communicate. The most innovative smartphones provide several features that support, without additional hardware, the adoption of Augmented Reality interfaces in mobile applications, enabling the combination of the real world with virtual information on the device screen. In the original vision, Augmented Reality enabled only to superimpose relevant information on the mobile device, using knowledge of the user's position and other context. More recently, following the web 2.0 direction, new solutions allow the user to create and share information. In this paper, we review related work, identify the most relevant patents and detail the challenges in this field. We also describe the main results and patents adopting this technology in different application fields and outline future developments.", "num_citations": "5\n", "authors": ["303"]}
{"title": "An investigation of clustering algorithms in the identification of similar web pages\n", "abstract": " In this paper we investigate the effect of using clustering algorithms in the reverse engineering field to identify pages that are similar either at the structural level or at the content level. To this end, we have used two instances of a general process that only differ for the measure used to compare web pages. In particular, two web pages at the structural level and at the content level are compared by using the Levenshtein edit distances and Latent Semantic Indexing, respectively. The static pages of two web applications and one static web site have been used to compare the results achieved by using the considered clustering algorithms both at the structural and content level. On these applications we generally achieved comparable results. However, the investigation has also suggested some heuristics to quickly identify the best partition of web pages into clusters among the possible partitions both at the structural and at the content level.", "num_citations": "5\n", "authors": ["303"]}
{"title": "Second life technological transfer to companies: the case study of the cc ict-sud centre\n", "abstract": " Many well known international business organizations are interested in virtual worlds considered as a valid support to distance cooperation and training. This paper describes the Second Life experience, gained by the authors to support the distance training activities proposed by the Competency Centre CC ICT-Sud, in the context of a course focused on showing how virtual worlds support collaborative work. The course was targeted to show distance collaboration tools and methodologies to company representatives, public administration employees and graduates auditors. The teaching modalities adopted were both of teaching and collaborative types. The latter allowed to experiment with collaborative methodologies that were introduced in the synchronous distance lectures. The results obtained are, in the main, encouraging.", "num_citations": "5\n", "authors": ["303"]}
{"title": "Comparing inspection methods using controlled experiments\n", "abstract": " Objective: In this paper we present an empirical study that was aimed at comparing three software inspection methods, in terms of needed time, precision, and recall values. The main objective of this study is to provide software engineers with some insight into choosing the inspection method to adopt. Method: We conducted a controlled experiment and a replication. These experiments involved 48 Master students in Computer Science at the University of Salerno. In the experiments, 6 academic researchers were also involved. The students had to discover defects within a software artefact using inspection methods that differ in terms of discipline and flexibility. In particular, we selected a disciplined but not flexible method (the Fagan\u2019s process), a disciplined and flexible method (a virtual inspection), and a flexible but not disciplined method (the pair inspection). Results: We observed a significant difference in favour of the Pair Inspection method for the time spent to perform the tasks. The data analysis also revealed a significant difference in favour of the Fagan\u2019s inspection process for precision. Finally, the effect of the inspection method on the recall is not significant. Conclusions: The empirical investigation showed that the discipline and flexibility of an inspection method affect both the time needed to identify defects and the precision of the inspection results. In particular, more flexible methods require less time to inspect a software artefact, while more disciplined methods enable the identification of a lower number of false defects.", "num_citations": "5\n", "authors": ["303"]}
{"title": "Integrating a Distributed Inspection Tool Within an Artefact Management System.\n", "abstract": " We propose a web based inspection tool addressing the problem of software inspection within a distributed development environment. This tool implements an inspection method that tries to minimise the synchronous collaboration among team members using an asynchronous discussion to resolve the conflicts before the traditional synchronous meeting. The tool also provides automatic merging and conflict highlighting functionalities to support the reviewers during the pre-meeting refinement phase. Information about the inspection progress, which can be a valuable support to make inspection process related decisions is also provided. The inspection tool has been integrated within an artefact management system, thus allowing the planning, scheduling, and enactment of the inspection within the development process and integrating the review phase within the overall artefact lifecycle.", "num_citations": "5\n", "authors": ["303"]}
{"title": "Role based reengineering of Web applications\n", "abstract": " We present an approach based on roles and access policies to improve security management of Web applications. The approach first identifies the roles users have in the application, and then the software resources they can access based on the assigned role. Roles and resources are then used to design access policies by means of a visual language based tool providing a metaphor-oriented layer above the well-known role based access control (RBAC) model. A network infrastructure based on a policy enforcement point (PEP) and a policy decision point (PDP) is used to enforce these policies. The proposed approach has been used in a preliminary case study.", "num_citations": "5\n", "authors": ["303"]}
{"title": "Recovering Traceability links between requirement artefacts: a case study\n", "abstract": " Recovering Traceability Links between Requirement Artefacts: a Case Study IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS IRIS Universit\u00e0 degli Studi del Molise Catalogo Ricerca 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno Recovering Traceability Links between Requirement Artefacts: a Case Study Italiano Italiano Italiano Italiano English English Recovering Traceability Links between Requirement Artefacts: a Case Study / DE LUCIA A; FASANO F; FRANCESE R; OLIVETO R. - (2004), pp. 453-456. ((Intervento presentato al convegno International Workshop on Knowledge Oriented Maintenance (SEKE KOM) tenutosi a Banff, Alberta, Canada nel June 20-24, 2004. Scheda breve Scheda completa Titolo: Recovering Traceability Links between Requirement Artefacts: a Case Study : \u2026", "num_citations": "5\n", "authors": ["303"]}
{"title": "An eclipse plug-in for the identification of design pattern variants\n", "abstract": " Design patterns are not only beneficial to the forward engineering process but they also help typical reverse engineering activities such as design recovery and program understanding. Indeed, conspicuous insight on the software structure and its internal characteristics are provided by design patterns recovered from source code. In this paper, we present an Eclipse plugin implementing a reverse engineering tool supporting the detection of design patterns and their implementation variants. The plug-in exploits a technique able to recover design pattern instances by combining static analysis, based on visual language parsing, with dynamic analysis, based on source code instrumentation. In particular, the dynamic analysis is performed through the automatic instrumentation of the method calls involved in the candidate pattern instances identified during static analysis. The results obtained from a program monitoring activity are matched against the definitions of the pattern behaviors expressed in terms of monitoring grammars. The plug-in allows software engineers to customize the layout used for the visualization of the recovered instances and the structural and behavioral properties of the design patterns to be recovered.", "num_citations": "4\n", "authors": ["303"]}
{"title": "Comparing clustering algorithms for the identification of similar pages in web applications\n", "abstract": " In this paper, we analyze some widely employed clustering algorithms to identify duplicated or cloned pages in web applications. Indeed, we consider an agglomerative hierarchical clustering algorithm, a divisive clustering algorithm, k-means partitional clustering algorithm, and a partitional competitive clustering algorithm, namely Winner Takes All (WTA). All the clustering algorithms take as input a matrix of the distances between the structures of the web pages. The distance of two pages is computed applying the Levenshtein edit distance to the strings that encode the sequences of HTML tags of the web pages.", "num_citations": "4\n", "authors": ["303"]}
{"title": "A partitioning method for efficient system-level diagnosis\n", "abstract": " We propose a partitioning method for an adaptive distributed system-level diagnosis in arbitrary network topologies. It utilizes a biconnected component as a partitioning unit. In an adaptive distributed system-level diagnosis, testing assignment algorithm is performed before each node performs actual diagnosis to reduce the number of tests in the system. Existing testing assignment algorithms adopt non-partitioning approach covering the whole system, so they incur unnecessary extra message traffic and time. In our method, the whole system is partitioned into small groups (biconnected components), and testing assignment is performed within each group. By exploiting the property of an articulation point of a biconnected component, initial testing assignment of our method performs better than non-partitioning approach by reducing the number of nodes involved in testing assignment. It also localizes the testing\u00a0\u2026", "num_citations": "4\n", "authors": ["303"]}
{"title": "Improving corrective maintenance effort prediction: an empirical study\n", "abstract": " This paper reports on an empirical study aiming at improving the cost prediction model currently used in a major software enterprise. We used a multiple regression model and the data collected from two corrective maintenance projects. The improvement of the model performances is achieved by taking into account different corrective maintenance task typologies, each affecting the effort in a different way.", "num_citations": "4\n", "authors": ["303"]}
{"title": "Webcentric business process pre engineering\n", "abstract": " Migrating legacy systems towards Web-enabled, clientserver architectures is only an aspect and cannot be considered as the solution to a web-centric reengineering of the business processes supported by these systems.", "num_citations": "4\n", "authors": ["303"]}
{"title": "A Mobile Augmented Reality system supporting co-located Content Sharing and Displaying\n", "abstract": " Augmented Reality interfaces allow user to interact with a mixed reality where virtual information is superimposed to the physical environment. The technological evolution of mobile devices offers accelerometers and magnetic field sensors as input controllers permitting to transform mobile devices in 3D User Interfaces. In this paper, we investigate how Augmented Reality and 3D User Interfaces, provided only by mobile devices, can support co-located teams in face-to-face interaction during meetings. Following the \u201cCooperative Building\u201d metaphor, users collaboratively create multimedia content in Augmented Reality areas and lately present it during the meeting. Multiple coordinated areas have been designed to provide support to group discussion. All these areas are associated to a specific physical location in the meeting room. Specific augmented areas are also available to support content projection\u00a0\u2026", "num_citations": "3\n", "authors": ["303"]}
{"title": "Towards automatic clustering of similar pages in web applications\n", "abstract": " In this paper, we propose an automatic approach to group web pages that are similar at the content level. The approach uses the Levenshtein string edit distance and Latent Semantic Indexing to compute page dissimilarity and then groups them using iteratively a Graph-Theoretic clustering algorithm. To automate the clustering process a prototype has been implemented and used to assess the proposed approach on three web sites.", "num_citations": "3\n", "authors": ["303"]}
{"title": "Genesis workflow: Managing distributed software processes\n", "abstract": " We present the workflow management system of GENESIS (GEneralised eNvironment for procEsS management in cooperatIve Software engineering), an on-going research project aiming at designing and developing a non-invasive and open source system to support software engineering processes in a highly distributed environment. The process modelling language enables the decomposition of complex processes into sub-processes that can be distributed and exe-cuted at different organizational sites. In GENESIS, workflow management technologies have been integrated with artefact management and communication services to meet the necessary requirement of managing cooperation among software engineers from different locations.", "num_citations": "3\n", "authors": ["303"]}
{"title": "Workshop on cooperative supports for distributed software engineering processes\n", "abstract": " Globally distributed software development challenges traditional techniques of software engineering and new approached to solved communication, collaboration and coordination problems are to be sought. This workshop intends to gather practitioners and researchers from academia, industry, and government, to review the current state of the practice, to report on, and to present issues and solutions in the general area of computer supported cooperative methodologies and technologies applied to software engineering processes.", "num_citations": "3\n", "authors": ["303"]}
{"title": "Evolving ispell: A case study of program understanding for reuse\n", "abstract": " Text processing has proven helpful in a number of software engineering tasks. We discuss how a morphological analyser for the Italian language, and its associated linguistic resources, have been developed by reusing and evolving an existing system, Ispell, which is an open-source spell-checker. The need to develop such an analyser derives from the need to improve the traceability link recovery process described by G. Antoniol et al. (2000, 2002). This paper shows how the program understanding exercise was useful to develop a system in a specialized application domain in which we had a very limited background knowledge.", "num_citations": "3\n", "authors": ["303"]}
{"title": "A visual framework for the definition and execution of reverse engineering processes\n", "abstract": " In this paper we present a visual framework developed as an Eclipse plug-in to define and execute reverse engineering processes aimed at comprehending traditional and web based information systems. Processes are defined in terms of UML activity diagrams, where predefined or newly developed software components can be associated to each activity. Components implemented using either traditional programming languages or software environments for data analysis (i.e., MATLAB or R) can be reused. Once the process has been fully defined the software engineer executes it to reverse engineering and comprehend software systems. The proposed visual framework has been evaluated on two case studies.", "num_citations": "2\n", "authors": ["303"]}
{"title": "A service oriented collaborative distributed learning object management system\n", "abstract": " Learning Objects are stored in repositories and spread through Internet. The educational sector needs to share good quality educational contents, which can be reused and adopted in several contexts. In this paper we present a Service Oriented and SCORM conformant system, named CD-LOMAS (Collaborative Distributed Learning Object MAnagement System), supporting the sharing of contents and collaborative teaching in a highly distributed environment. Complex Learning Objects are decomposed into simpler Learning Objects that can be distributed at different sites. The adoption of a service oriented architecture enables to federate distributed Learning Management System at content level. CD-LOMAS has been developed as a MOODLE plug-in and is equipped with an extensible service oriented architecture supporting artifact management features, such as coordination of cooperative teaching\u00a0\u2026", "num_citations": "2\n", "authors": ["303"]}
{"title": "Distributed workflow management based on UML and web services\n", "abstract": " The definition and the management of business processes are considered a relevant issue to support organizations in their activities. Indeed, in the last few years many organizations have been changing their business processes to keep competitive in the global market. Workflow management is an emerging technology enabling process performance improvement in a cooperative working environment. In particular, a workflow management system (WfMS) enables processes automation through the integration, the coordination, and communication of both human and automatic task of business processes. WfMSs provide a process definition language (PDL) for modeling business processes. A PDL sentence is named process model and is enacted by a component of the WfMS, namely the process engine. The main task of this component is executing the enactment rules and the activities specified in the Process\u00a0\u2026", "num_citations": "2\n", "authors": ["303"]}
{"title": "Assessing effort prediction models for corrective software maintenance\n", "abstract": " We present an assessment of an empirical study aiming at building effort estimation models for corrective maintenance projects. We show results from the application of the prediction models to a new corrective maintenance project within the same enterprise and the same type of software systems used in a previous study. The data available for the new project are finer grained according to the indications devised in the first study. This allowed to improve the confidence in our previous empirical analysis by confirming most of the hypotheses made and to provide other useful indications to better understand the maintenance process of the company in a quantitative way.", "num_citations": "2\n", "authors": ["303"]}
{"title": "A Communication Protocol for Distributed Process Management\n", "abstract": " Large scale software development processes imply the coordination and cooperation of several sites with a large number of people and sub processes. We present an asynchronous communication protocol for distributed process management adopted within the GENESIS (Generalized ENvironment for procESs management in cooperatIve Software engineering) project. The GENESIS process management sub-subsystem enables distributed process modeling and enactment on different organizational sites through an event dispatching architecture.", "num_citations": "2\n", "authors": ["303"]}
{"title": "Rapid Development of Process Modeling Tools.\n", "abstract": " We present an approach for the rapid development and evolution of visual environments for modelling distributed software engineering processes. The definition of the process modeling language takes into account the requirements of the customer that directly participates in the development process. The development process is supported by the VLDesk, an integrated set of grammar-based tools for the definition and automatic generation of visual environments. The produced visual environment enables an organization to quickly design distributed process models and generate the corresponding XML code that specifies the activities with its elements, including actors and artifacts produced and the transitions expressed in the form of event-condition-action rules. In this way the designed process model can be easily instantiated for a specific project and enacted by any workflow engine supporting a programmable eventcondition-action paradigm.", "num_citations": "2\n", "authors": ["303"]}
{"title": "Program comprehension in a reuse reengineering environment\n", "abstract": " Program comprehension is the most expensive activity of software maintenance. The di erent phases of a reuse reengineering process involves comprehension activities for understanding the structure of existing systems, the functionality implemented by a reuse-candidate module and the reengineering e ort. We present an integrated environment implemented in Prolog for reuse reengineering existing C systems. Di erent tools developed in the RE2 project are integrated in the environment through sharing a ne-grained representation for C program, the Combined C Graph (CCG). Di erent views of a system can be abstracted and visualised from the data-base of Prolog facts implementing its CCG representation. Software metric tools evaluate the reengineering costs, while reengineering operations are expressed as transformation rules and a symbolic executor allows the production of the module's speci cation.", "num_citations": "2\n", "authors": ["303"]}
{"title": "Impact of Design Pattern Implementation Variants on the Retrieval Effectiveness of a Recovery Tool: An Exploratory Study\n", "abstract": " This paper investigates how  implementation variants  of design patterns impact on the retrieval effectiveness of a design pattern recovery tool. Specifically, we first defined several implementation variants of Adapter and Observer design patterns, by introducing constraints or relaxations on their canonical form. Then, we analyze the relationship between the complexity of these definitions and the precision and time needed by a design pattern recovery process we proposed in the past. To this end, we apply ePAD, an Eclipse plug-in for design pattern recovery, to eight software systems. We show that there exist interesting issues about the relationship between the complexity of the defined variants and the precision and time needed to recover their instances.", "num_citations": "1\n", "authors": ["303"]}
{"title": "Parallel Genetic Algorithms in the Cloud\n", "abstract": " Genetic Algorithms (GAs) are a metaheuristic search technique belonging to the class of Evolutionary Algorithms (EAs). They have been proven to be effective in addressing several problems in many fields but also suffer from scalability issues that may not let them find a valid application for real world problems. Thus, the aim of providing highly scalable GA-based solutions, together with the reduced costs of parallel architectures, motivate the research on Parallel Genetic Algorithms (PGAs).Cloud computing may be a valid option for parallelisation, since there is no need of owning the physical hardware, which can be purchased from cloud providers, for the desired time, quantity and quality. There are different employable cloud technologies and approaches for this purpose, but they all introduce communication overhead. Thus, one might wonder if, and possibly when, specific approaches, environments and models show better performance than sequential versions in terms of execution time and resource usage. This thesis investigates if and when GAs can scale in the cloud using specific approaches. Firstly, Hadoop MapReduce is exploited designing and developing an open source framework, ie, elephant56, that reduces the effort in developing and speed up GAs using three parallel models. The performance of the framework is then evaluated through an empirical study. Secondly, software containers and message queues are employed to develop, deploy and execute PGAs in the cloud and the devised system is evaluated with an empirical study on a commercial cloud provider. Finally, cloud technologies are also explored for the\u00a0\u2026", "num_citations": "1\n", "authors": ["303"]}
{"title": "Mining Version Histories for Detecting Code Smells\n", "abstract": " Mining Version Histories for Detecting Code Smells IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS Archivio della ricerca dell'Universit\u00e0 degli studi del Sannio Catalogo Ricerca 1 Contributo su Rivista 1.1 Articolo in rivista Mining Version Histories for Detecting Code Smells Italiano Italiano English Mining Version Histories for Detecting Code Smells / Fabio, Palomba; Gabriele, Bavota; Di Penta, M; Rocco, Oliveto; Denys, Poshyvanyk; Andrea De, Lucia. - In: IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. - ISSN 0098-5589. - 41:5(2015), pp. 462-489. Scheda breve Scheda completa Titolo: Mining Version Histories for Detecting Code Smells Autori interni: DI PENTA, Massimiliano mostra contributor esterni Data di pubblicazione: 2015 Rivista: IEEE TRANSACTIONS ON SOFTWARE ENGINEERING Handle: http://hdl..\u2026", "num_citations": "1\n", "authors": ["303"]}
{"title": "Trasferimento tecnologico alle imprese attraverso Second Life: il caso di studio del Centro di Competenza CC ICT-Sud\n", "abstract": " Molte organizzazioni aziendali, note anche a livello internazionale, stanno rivolgendo la propria attenzione verso i mondi virtuali che si rivelano una valida piattaforma di supporto alla collaborazione e alla formazione. In questo articolo descriviamo l\u2019esperienza di utilizzo di Second Life maturata dagli autori per supportare l\u2019attivit\u00e0 formativa a distanza svolta dal Centro di Competenza CC ICT-Sud nell\u2019ambito di un corso sui metodi e gli strumenti di supporto al lavoro collaborativo avente per destinatari esponenti di aziende, della pubblica amministrazione e laureati. Le modalit\u00e0 didattiche adottate sono state sia di tipo \u201cteaching\u201d, sia di tipo collaborativo. Queste ultime hanno consentito di sperimentare le metodologie collaborative che sono state proposte nelle lezioni frontali sincrone. I risultati ottenuti sono, nel complesso incoraggianti.", "num_citations": "1\n", "authors": ["303"]}
{"title": "Enhancing Rationale Management with Second Life Meetings.\n", "abstract": " In this paper we investigate how Second Life meeting can be used to discuss software decisions and capture rationale information during synchronous meetings, especially among geographically distributed software engineering teams. To this aim, a system supporting the management of collaborative activities in Second Life, named SLMeeting, has been enhanced with rationale management features and integrated with ADAMS, a software project and artefact management system. We also present the Rationale Management subsystem of ADAMS, enabling to capture, update and retrieve rationale information. This integration supports the interchange of rationale information among the communication infrastructure, the issue base, and the project management system.", "num_citations": "1\n", "authors": ["303"]}
{"title": "Modern Web Application Development\n", "abstract": " The World Wide Web was introduced in 1994 with the goal of making it possible to access information from any source in a consistent and simple way. Developed at CERN, in Geneva, Switzerland, it was aimed at physicists and other scientists who generate huge amounts of data and documents and need to share them with other scientists. Hypertext was adopted as a simple way to both give access to documents and link them together. The HTTP protocol was designed to allow one computer\u2014the client computer\u2014to request data and documents from another computer\u2014the server computer\u2014so that it could make that document available to the users on the client computer. In this way, the World Wide Web was viewed as a vast repository of information that provided access to a large number of users. This view of the Web as a static repository has evolved considerably over time. Now the Web is a sophisticated platform offering a vast array of tools and components to application developers. A new generation of applications offers users the opportunities to communicate, collaborate, and even update the capabilities of the application. Applications support small businesses or communities of users as well as large company businesses. The Web continues to evolve at a rapid pace. There are many ideas about what trends will predominate in the future. One all-encompassing concept is to consider the emerging trends as forming the basis of Web 2.0. Compared to traditional Web", "num_citations": "1\n", "authors": ["303"]}
{"title": "CD-LOMAS: A Collaborative Distributed Learning Object Management System.\n", "abstract": " Learning Objects are stored in repositories and spread through Internet. The educational sector needs to share good quality educational contents, which can be reused and adopted in several contexts. In this paper we present CD-LOMAS (Collaborative Distributed Learning Object MAnagement System) to support the sharing of contents and the collaboration on their development in a highly distributed environment. Complex Learning Objects are decomposed into simpler Learning Objects that can be distributed at different sites. In CD-LOMAS artifact management features, such as coordination of cooperative workers and versioning, are integrated with context-awareness.", "num_citations": "1\n", "authors": ["303"]}
{"title": "Working Conference on Reverse Engineering 2005\n", "abstract": " Editorial: Working Conference on Reverse Engineering 2005: Information and Software Technology: Vol 49, No 3 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Information and Software Technology Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInformation and Software TechnologyVol. , No. Editorial: Working Conference on Reverse Engineering 2005 article Editorial: Working Conference on Reverse Engineering 2005 Share on Authors: Andrea de Lucia profile image Andrea De Lucia University of Salerno, Italy University of Salerno, Italy View Profile , Susan Elliott Sim profile image Susan Elliott Sim University of California, Irvine of , Info :\u2026", "num_citations": "1\n", "authors": ["303"]}
{"title": "Introducing Legacy System Migration Technologies in an Academic Context: a Controlled Experiment\n", "abstract": " To verify whether or not migration technologies can be adopted, systematic and quantitative evaluations should be performed. In this paper we present the results of a controlled experiment aimed at assessing the usefulness of an Eclipse plug-in, named MELIS (Migration Environment for Legacy Information Systems). This plug-in has been developed to support the migration of legacy information systems to a web-enabled multi-tier target architecture according with an incremental migration strategy. The context of the experiment was constituted of master students in Computer Science at the University of Salerno. The subjects without COBOL programming experience, while half of them had J2EE programming experience. The results of the experiment confirmed that the use of MELIS increases the productivity with respect to the use of traditional development tools.", "num_citations": "1\n", "authors": ["303"]}
{"title": "Introducing Legacy System Migration Technologies in a Software Company: a Controlled Experiment\n", "abstract": " Transferring reverse engineering and migration technologies to industry requires to evaluate whether they potentially fulfill industry needs. We present a controlled experiment aimed at assessing the usefulness of a migration tool, named MELIS (Migration Environment for Legacy Information Systems) in an industrial setting. MELIS is an Eclipse plug-in conceived to migrate legacy information systems to a web-enabled multi-tier target architecture. This plug-in supports the software engineer in the migration of the graphical user interface and in the restructuring and wrapping of the original legacy code. The context of the experiment was constituted of professional programmers, with COBOL programming experience, and academic subjects, with J2EE programming experience. The results revealed that on average the use of MELIS increases the productivity of a factor 4 with respect to the use of traditional\u00a0\u2026", "num_citations": "1\n", "authors": ["303"]}
{"title": "Information retrieval models for recovering traceability links between code and documentation\n", "abstract": " Information Retrieval Models for Recovering Traceability Links between Code and Documentation IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS Archivio della ricerca dell'Universit\u00e0 degli studi del Sannio Catalogo Ricerca 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno Information Retrieval Models for Recovering Traceability Links between Code and Documentation Italiano Italiano English Information Retrieval Models for Recovering Traceability Links between Code and Documentation / Giuliano Antoniol; Canfora G; Gerardo Casazza; Andrea De Lucia. - (2000). Scheda breve Scheda completa Titolo: Information Retrieval Models for Recovering Traceability Links between Code and Documentation Autori interni: CANFORA, Gerardo mostra contributor esterni Data di pubblicazione: : ://..\u2026", "num_citations": "1\n", "authors": ["303"]}
{"title": "An overview of structural and specification driven candidature criteria for reuse reengineering processes\n", "abstract": " One of the most promising ways to make the population of a repository of reusable assets cost e ective and to obtain useful results in the short time is by extracting and reengineering them from existing software. A reuse reengineering process consists of the set of activities for identifying software components implementing abstractions, reengineering them according to a prede ned template, associating them with their interface and functional speci cation and populating a repository with the reusable assets so obtained. Code scavenging consists in searching existing software systems for source code components that implement software abstractions. We present an overview of code scavenging techniques with reference to the rst phase of the RE2 project reference paradigm for setting up reuse reengineering processes. Several program representations proposed in the literature for software maintenance and in particular useful for reverse engineering and reengineering are also described.", "num_citations": "1\n", "authors": ["303"]}
{"title": "A Controlled Experiment to Assess the Effectiveness of a Distributed Method for Code Inspection\n", "abstract": " In this report we propose a distributed inspection method, which tries to minimize the synchronous collaboration among team members to identify defects in software artifacts. The main idea of our approach consists of identifying conflicts on the potential defects and then resolving them using an asynchronous inspection meeting before performing a traditional synchronous meeting. A synchronous inspection meeting is often not performed since it is generally used to remove conflicts on the defects that the inspection members have not solved. This is because at the end of the asynchronous meeting unsolved conflicts are rare or absent at all. This approach has been implemented in a web based tool and assessed using a controlled experiment, which context was constituted of master students in Computer Science at the University of Salerno.The remainder of the report is organized as follows: Section 2 summarizes the Fagan\u2019s method. The migration process and the tool are described in Section 3, while the controlled experiment and the achieved results are presented in Section 4. Final remarks, lesson learned, and future work conclude the document.", "num_citations": "1\n", "authors": ["303"]}
{"title": "Automatic Clustering of Similar Web Pages\n", "abstract": " In this report, we propose an automatic approach to group web pages that are similar at the content level. The approach uses the Levenshtein string edit distance and Latent Semantic Indexing to compute page dissimilarity and then groups them using iteratively a Graph-Theoretic clustering algorithm. To automate the clustering process a prototype has been implemented and used to assess the proposed approach on three web sites.", "num_citations": "1\n", "authors": ["303"]}