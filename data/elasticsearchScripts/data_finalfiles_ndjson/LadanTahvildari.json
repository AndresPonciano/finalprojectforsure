{"title": "Self-adaptive software: Landscape and research challenges\n", "abstract": " Software systems dealing with distributed applications in changing environments normally require human supervision to continue operation in all conditions. These (re-)configuring, troubleshooting, and in general maintenance tasks lead to costly and time-consuming procedures during the operating phase. These problems are primarily due to the open-loop structure often followed in software development. Therefore, there is a high demand for management complexity reduction, management automation, robustness, and achieving all of the desired quality requirements within a reasonable cost and time range during operation. Self-adaptive software is a response to these demands; it is a closed-loop system with a feedback loop aiming to adjust itself to changes during its operation. These changes may stem from the software system's self (internal causes, e.g., failure) or context (external events, e.g., increasing\u00a0\u2026", "num_citations": "1708\n", "authors": ["1131"]}
{"title": "Autonomic computing: emerging trends and open problems\n", "abstract": " The increasing heterogeneity, dynamism and interconnectivity in software applications, services and networks led to complex, unmanageable and insecure systems. Coping with such a complexity necessitates to investigate a new paradigm namely Autonomic Computing. Although academic and industry efforts are beginning to proliferate in this research area, there are still a lots of open issues that remain to be solved. This paper proposes a categorization of complexity in I/T systems and presents an overview of autonomic computing research area. The paper also discusses a summary of the major autonomic computing systems that have been already developed both in academia and industry, and finally outlines the underlying research issues and challenges from a practical as well as a theoretical point of view.", "num_citations": "200\n", "authors": ["1131"]}
{"title": "The effects of time constraints on test case prioritization: A series of controlled experiments\n", "abstract": " Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time\u00a0\u2026", "num_citations": "196\n", "authors": ["1131"]}
{"title": "An empirical study of the effect of time constraints on the cost-benefits of regression testing\n", "abstract": " Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. Therefore, we conducted an experiment to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our results show that time constraints can indeed play a significant role in determining both the cost-effectiveness of prioritization, and the relative cost-benefit tradeoffs among techniques, with important implications for\u00a0\u2026", "num_citations": "102\n", "authors": ["1131"]}
{"title": "Size-constrained regression test case selection using multicriteria optimization\n", "abstract": " To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results\u00a0\u2026", "num_citations": "92\n", "authors": ["1131"]}
{"title": "A prioritization approach for software test cases based on bayesian networks\n", "abstract": " An important aspect of regression testing is to prioritize the test cases which need to be ordered to execute based on specific criteria. This research work presents a novel approach to prioritizing test cases in order to enhance the rate of fault detection. Our approach is based on probability theory and utilizes Bayesian Networks (BN) to incorporate source code changes, software fault-proneness, and test coverage data into a unified model. As a proof of concept, the proposed approach is applied to eight consecutive versions of a large-size software system. The obtained results indicate a significant increase in the rate of fault detection when a reasonable number of faults are available.", "num_citations": "90\n", "authors": ["1131"]}
{"title": "A metric-based heuristic framework to detect object-oriented design flaws\n", "abstract": " One of the important activities in re-engineering process is detecting design flaws. Such design flaws prevent an efficient maintenance, and further development of a system. This research proposes a novel metric-based heuristic framework to detect and locate object-oriented design flaws from the source code. It is accomplished by evaluating design quality of an object-oriented system through quantifying deviations from good design heuristics and principles. While design flaws can occur at any level, the proposed approach assesses the design quality of internal and external structure of a system at the class level which is the most fundamental level of a system. In a nutshell, design flaws are detected and located systematically in two phases using a generic OO design knowledge-base. In the first phase, hotspots are detected by primitive classifiers via measuring metrics indicating a design feature (e.g. complexity\u00a0\u2026", "num_citations": "80\n", "authors": ["1131"]}
{"title": "Adaptive action selection in autonomic software using reinforcement learning\n", "abstract": " The planning process in autonomic software aims at selecting an action from a finite set of alternatives for adaptation. This is an abstruse problem due to the fact that software behaviour is usually very complex with numerous number of control variables. This research work focuses on proposing a planning process and specifically an action selection technique based on \"Reinforcement Learning\" (RL). We argue why, how, and when RL can be beneficial for an autonomic software system. The proposed approach is applied to a simulated model of a news web application. Evaluation results show that this approach can learn to select appropriate actions in a highly dynamic environment. Furthermore, we compare this approach with another technique from the literature, and the results suggest that it can achieve similar performance in spite of no expert involvement.", "num_citations": "77\n", "authors": ["1131"]}
{"title": "Towards a goal\u2010driven approach to action selection in self\u2010adaptive software\n", "abstract": " Self\u2010adaptive software is a closed\u2010loop system, since it continuously monitors its context (i.e. environment) and/or self (i.e. software entities) in order to adapt itself properly to changes. We believe that representing adaptation goals explicitly and tracing them at run\u2010time are helpful in decision making for adaptation. While goal\u2010driven models are used in requirements engineering, they have not been utilized systematically yet for run\u2010time adaptation. To address this research gap, this article focuses on the deciding process in self\u2010adaptive software, and proposes the Goal\u2010Action\u2010Attribute Model (GAAM). An action selection mechanism, based on cooperative decision making, is also proposed that uses GAAM to select the appropriate adaptation action(s). The emphasis is on building a light\u2010weight and scalable run\u2010time model which needs less design and tuning effort comparing with a typical rule\u2010based approach\u00a0\u2026", "num_citations": "75\n", "authors": ["1131"]}
{"title": "StarMX: A framework for developing self-managing Java-based systems\n", "abstract": " Realizing self-managing systems poses several development and operational challenges. Reusable software frameworks assist in addressing these challenges by utilizing appropriate patterns, and also providing essential runtime services for self-managing systems. This paper presents the StarMX framework, designed for building self-managing Java-based applications. It is a generic framework based on standards and well-established principles, and supports common tasks in the development of such systems. StarMX facilitates creating the management closed loop using various mechanisms such as action policies. The framework architecture and its utilization process, along with an example of its application are presented in this paper. Moreover, quality attributes and autonomic characteristics of the proposed framework are discussed.", "num_citations": "74\n", "authors": ["1131"]}
{"title": "Using bayesian belief networks to predict change propagation in software systems\n", "abstract": " During software evolution, developers modify various modules to handle new requirements or to fix existing bugs. Such changes usually propagate to related modules throughout the system. Program comprehension techniques are able to predict this change propagation phenomenon. In this paper, we introduce a novel approach that predicts the possible affected system modules, given a change in the system. We use Bayesian Belief Networks as a probabilistic tool to make such predictions in a systematic way. This novel technique mainly relies on two sources of information: dependency metrics (calculated using static analysis) and change history extracted from a version control repository. We evaluate our approach by examining all significant revisions of Azureusl, an open-source Java system. The results show that the predicted change probabilities reflect actual module changes even in the early stages of the\u00a0\u2026", "num_citations": "67\n", "authors": ["1131"]}
{"title": "An empirical study on bayesian network-based approach for test case prioritization\n", "abstract": " A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of\u00a0\u2026", "num_citations": "60\n", "authors": ["1131"]}
{"title": "Test case prioritization using lexicographical ordering\n", "abstract": " Test case prioritization aims at ordering test cases to increase the rate of fault detection, which quantifies how fast faults are detected during the testing phase. A common approach for test case prioritization is to use the information of previously executed test cases, such as coverage information, resulting in an iterative (greedy) prioritization algorithm. Current research in this area validates the fact that using coverage information can improve the rate of fault detection in prioritization algorithms. The performance of such iterative prioritization schemes degrade as the number of ties encountered in prioritization steps increases. In this paper, using the notion of lexicographical ordering, we propose a new heuristic for breaking ties in coverage based techniques. Performance of the proposed technique in terms of the rate of fault detection is empirically evaluated using a wide range of programs. Results indicate that the\u00a0\u2026", "num_citations": "49\n", "authors": ["1131"]}
{"title": "A probabilistic approach to predict changes in object-oriented software systems\n", "abstract": " Predicting the changes in the next release of a software system has become a quest during its maintenance phase. Such a prediction can help managers to allocate resources more appropriately which results in reducing costs associated with software maintenance activities. A measure of change-proneness of a software system also provides a good understanding of its architectural stability. This research work proposes a novel approach to predict changes in an object oriented software system. The rationale behind this approach is that in a well-designed software system, feature enhancement or corrective maintenance should affect a limited amount of existing code. The goal is to quantify this aspect of quality by assessing the probability that each class will change in a future generation. Our proposed probabilistic approach uses the dependencies obtained from the UML diagrams, as well as other data extracted\u00a0\u2026", "num_citations": "49\n", "authors": ["1131"]}
{"title": "Achieving dynamic adaptation via management and interpretation of runtime models\n", "abstract": " In this article, we present a generic model-centric approach for realizing fine-grained dynamic adaptation in software systems by managing and interpreting graph-based models of software at runtime. We implemented this approach as the Graph-based Runtime Adaptation Framework (GRAF), which is particularly tailored to facilitate and simplify the process of evolving and adapting current software towards runtime adaptivity. As a proof of concept, we present case study results that show how to achieve runtime adaptivity with GRAF and sketch the framework's capabilities for facilitating the evolution of real-world applications towards self-adaptive software. The case studies also provide some details of the GRAF implementation and examine the usability and performance of the approach.", "num_citations": "47\n", "authors": ["1131"]}
{"title": "Change prediction in object-oriented software systems: A probabilistic approach.\n", "abstract": " An estimation of change-proneness of parts of a software system is an active topic in the area of software engineering. Such estimates can be used to predict changes to different classes of a system from one release to the next. They can also be used to estimate and possibly reduce the effort required during the development and maintenance phase by balancing the amount of developers\u2019 time assigned to each part of a software system. This research work proposes a novel approach to predict changes in an object-oriented software system. The rationale behind this approach is that in a well-designed software system, feature enhancement or corrective maintenance should affect a limited amount of existing code. Our goal is to quantify this aspect of quality by assessing the probability that each class will change in a future generation. Our proposed probabilistic approach uses the dependencies obtained from the UML diagrams, as well as other code metrics extracted from source code of several releases of a software system using reverse engineering techniques. These measures, combined with the change log of the software system and the expected time of next release, are used in an automated manner to predict whether a class will change in the next release of the software system. The proposed systematic approach has been evaluated on a multiversion medium sized open source project namely JFlex, the Fast Scanner Generator for Java. The obtained results indicate the simplicity and accuracy of our approach in the comparison with existing methods referred in the literature.", "num_citations": "46\n", "authors": ["1131"]}
{"title": "A comparative study of the performance of IR models on duplicate bug detection\n", "abstract": " Open source projects incorporate bug triagers to help with the task of bug report assignment to developers. One of the tasks of a triager is to identify whether an incoming bug report is a duplicate of a pre-existing report. In order to detect duplicate bug reports, a triager either relies on his memory and experience or on the search capabilities of the bug repository. Both these approaches can be time consuming for the triager and may also lead to the misidentification of duplicates. In the literature, several approaches to automate duplicate bug report detection have been proposed. However, there has not been an exhaustive comparison of the performance of different IR models, especially with topic-based ones such as LSI and LDA. In this paper, we compare the performance of the traditional vector space model (using different weighting schemes) with that of topic based models, leveraging heuristics that incorporate\u00a0\u2026", "num_citations": "43\n", "authors": ["1131"]}
{"title": "Evaluating architectural stability using a metric-based approach\n", "abstract": " Architectural stability refers to the extent software architecture is flexible to endure evolutionary changes while leaving the architecture intact. Approaches to evaluate software architectures for stability can be retrospective or predictive. Retrospective evaluation looks at successive releases of a software system to analyze how smoothly the evolution has taken place. Predictive evaluation examines a set of likely changes and shows the architecture can endure these changes. This paper proposes a metric-based approach to evaluate architectural stability of a software system by combining these two traditional analysis techniques. Such an approach performs on the fact bases extracted from the source code by reverse engineering techniques. We also present experimental results by applying the proposed approach to analyze the architectural stability across different versions of two spreadsheet systems", "num_citations": "39\n", "authors": ["1131"]}
{"title": "A service-oriented componentization framework for java software systems\n", "abstract": " In the fast growing global market for services, service-oriented computing has drastically changed the way in which we develop software systems. Providing competitive services to these markets will be critical to the success of businesses and organizations. Some competitive services have already been implemented in existing systems. In this paper, we present a novel service-oriented componentization framework that automatically supports: i) identifying critical business services embedded in an existing Java system by utilizing graph representations of the system models, ii) realizing each identified service as a self-contained component that can be deployed as a single unit, and iii) transforming the object-oriented design into a service-oriented architecture. A toolkit implementing our framework has been developed as an Eclipse rich client platform (RCP). Our initial evaluation has shown that our framework is\u00a0\u2026", "num_citations": "34\n", "authors": ["1131"]}
{"title": "Quality-driven object-oriented re-engineering framework\n", "abstract": " This work presents a framework for providing quality based re-engineering of object-oriented systems (Tahvildari, 2003). The framework allows for specific design and quality requirements (performance and maintainability) of the target migrant system to be considered during the reengineering process. Quality requirements for the migrant system can be encoded using soft-goal interdependency graphs and be associated with specific software transformations that need to be carried out for the specific target quality requirement to be achieved. These transformations can be applied as a series of the iterative and incremental steps to the source code. An evaluation procedure can be used at each transformation step to determine whether specific goals have been achieved.", "num_citations": "32\n", "authors": ["1131"]}
{"title": "Prioritizing requirements-based regression test cases: A goal-driven practice\n", "abstract": " Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach\u00a0\u2026", "num_citations": "28\n", "authors": ["1131"]}
{"title": "Reconstructing traceability between bugs and test cases: An experimental study\n", "abstract": " In manual testing, testers typically follow the steps listed in the bug report to verify whether a bug has been fixed or not. Depending on time and availability of resources, a tester may execute some additional test cases to ensure test coverage. In the case of manual testing, the process of finding the most relevant manual test cases to run is largely manual and involves tester expertise. From a usability standpoint, the task of finding the most relevant test cases is tedious as the tester typically has to switch between the defect management tool and the test case management tool in order to search for test cases relevant to the bug at hand. In this paper, we use IR techniques to recover trace ability between bugs and test cases with the aim of recommending test cases for bugs. We report on our experience of recovering trace ability between bugs and test cases using techniques such as Latent Semantic Indexing (LSI) and\u00a0\u2026", "num_citations": "23\n", "authors": ["1131"]}
{"title": "Categorization of object-oriented software metrics\n", "abstract": " As software engineering matures as a discipline, practitioners are looking for ways to use measurements in the engineering of their products. As development technology changes, they find their metrics must change as well. Object oriented software development requires a different approach from traditional development methods including the metrics used to evaluate the software. It means that traditional metrics for procedural approaches are not adequate for evaluating object oriented software, primarily because they are not designed to measure basic elements like classes, objects, polymorphism and message passing. Even when adjusted to syntactically analyze object oriented software, they can only capture a small part of such software and therefore can just provide a weak quality indication. The article focuses on the specific demands placed on measures by object oriented software development, and also\u00a0\u2026", "num_citations": "23\n", "authors": ["1131"]}
{"title": "A weighted voting mechanism for action selection problem in self-adaptive software\n", "abstract": " Self-adaptive software is a closed-loop system which aims at adjusting itself in response to changes at runtime. Such a system is required to monitor domain events, detect significant changes, decide how to react, and act in order to execute the decisions. This paper focuses on the deciding process particularly for application-level adaptation actions. For this purpose, a weighted voting mechanism has been proposed which makes decisions based on a Goal-Action- Attribute Model (GAAM). The decision-making algorithm traverses GAAM, determines activated goals and feasible actions for voting, and ultimately selects an action as the social choice. The proposed mechanism and GAAM are evaluated within a simulated model of a news web site.", "num_citations": "22\n", "authors": ["1131"]}
{"title": "Graf: graph-based runtime adaptation framework\n", "abstract": " One approach for achieving runtime adaptability in software is to use application frameworks that are tailored for the development of self-adaptive systems. In this paper, we present the Graph-based Runtime Adaptation Framework (GRAF), which enables adaptivity by creating, managing, and interpreting graph-based models of software at runtime. Having a generic graph representation in our approach allows for flexible adaptation via query and transformation operations. The framework is especially suited for the migration of legacy applications towards adaptive software and attempts to reduce necessary changes to the original software. As a proof of concept, we conduct a comprehensive case study of migrating the legacy game Jake2 to achieve runtime adaptivity using GRAF.", "num_citations": "21\n", "authors": ["1131"]}
{"title": "Dynamic prioritization in regression testing\n", "abstract": " Although used extensively in industry, regression testing is challenging from both a process management as well as a resource management perspective. In literature, proposed test case prioritization techniques assume a constant pool of test cases with non-changing coverage during the regression testing process, and therefore they work with a fixed, prioritized test suite. However, in practice, test cases and their coverage metrics may change during regression testing due to modifications of software artefacts (e.g. due to bug fixing). For example, modifying obsolete test cases or source code may change the coverage metrics during the process. This may lead to some changes in test case priorities. Dealing with manual tests cases, scheduling test case execution in shared environments and other constraints in practice may cause the same effect. In this paper, we highlight these challenges in industrial regression\u00a0\u2026", "num_citations": "21\n", "authors": ["1131"]}
{"title": "An effort prediction framework for software defect correction\n", "abstract": " This article tackles the problem of predicting effort (in person\u2013hours) required to fix a software defect posted on an Issue Tracking System. The proposed method is inspired by the Nearest Neighbour Approach presented by the pioneering work of Weiss et al. (2007) [1]. We propose four enhancements to Weiss et al. (2007) [1]: Data Enrichment, Majority Voting, Adaptive Threshold and Binary Clustering. Data Enrichment infuses additional issue information into the similarity-scoring procedure, aiming to increase the accuracy of similarity scores. Majority Voting exploits the fact that many of the similar historical issues have repeating effort values, which are close to the actual. Adaptive Threshold automatically adjusts the similarity threshold to ensure that we obtain only the most similar matches. We use Binary Clustering if the similarity scores are very low, which might result in misleading predictions. This uses common\u00a0\u2026", "num_citations": "21\n", "authors": ["1131"]}
{"title": "Change support in adaptive software: A case study for fine-grained adaptation\n", "abstract": " Adaptive software is a closed-loop system which aims at adjusting itself in different situations at runtime. This paper looks at adaptation as changes in the context of dynamic software evolution, and proposes a conceptual model for these changes based on Activity Theory. This model consists of a hierarchy of activities making changes, and the objectives motivating these changes. This model is an attempt towards establishing a formal framework for designing adaptive software systems. While the proposed model is applicable to any type of adaptation, at different levels of granularity of various software systems, the paper focuses only on fine-grained adaptation changes. As a case study, a mission-critical e-commerce system, TPC-W, isused to apply the proposed model and evaluate the effectiveness of fine-grained adaptation changes. The conducted set of experiments aims at evaluating self-optimizing and self\u00a0\u2026", "num_citations": "21\n", "authors": ["1131"]}
{"title": "A quality-driven approach to enable decision-making in self-adaptive software\n", "abstract": " Self-adaptive software is a closed-loop system aims at altering itself in response to changes at runtime. Such a system, normally, requires monitoring, detecting (analyzing), deciding (planning), and acting (effecting) processes to fulfill adaptation requirements. This research mainly focuses on developing a quality-driven framework to facilitate realizing the deciding process. The framework is required to capture goals of adaptation, utility information, and domain characteristics in a knowledge-base.", "num_citations": "21\n", "authors": ["1131"]}
{"title": "Search-based duplicate defect detection: an industrial experience\n", "abstract": " Duplicate defects put extra overheads on software organizations, as the cost and effort of managing duplicate defects are mainly redundant. Due to the use of natural language and various ways to describe a defect, it is usually hard to investigate duplicate defects automatically. This problem is more severe in large software organizations with huge defect repositories and massive number of defect reporters. Ideally, an efficient tool should prevent duplicate reports from reaching developers by automatically detecting and/or filtering duplicates. It also should be able to offer defect triagers a list of top-N similar bug reports and allow them to compare the similarity of incoming bug reports with the suggested duplicates. This demand has motivated us to design and develop a search-based duplicate bug detection framework at BlackBerry. The approach follows a generalized process model to evaluate and tune the\u00a0\u2026", "num_citations": "20\n", "authors": ["1131"]}
{"title": "Temporal software change prediction using neural networks\n", "abstract": " Predicting changes in software entities (e.g. source files) that are more likely to change can help in the efficient allocation of the project resources. A powerful change prediction tool can improve maintenance and evolution tasks in software projects in terms of cost and time factors. The vast majority of research works have focused on determining \"where\" the most change-prone entities are, and \"how\" the change will be propagated through a system. This article suggests that knowing \"when\" changes are likely to happen can also provide another consideration for managers and developers to plan their maintenance activities more efficiently. To address this issue, a Neural Network-based Temporal Change Prediction (NNTCP) framework is proposed. This novel framework indicates \"where\" the changes are likely to happen (i.e. hot spots), and then adds the time dimension to predict \"when\" it may occur. In proving this\u00a0\u2026", "num_citations": "20\n", "authors": ["1131"]}
{"title": "Deep spherical quantization for image search\n", "abstract": " Hashing methods, which encode high-dimensional images with compact discrete codes, have been widely applied to enhance large-scale image retrieval. In this paper, we put forward Deep Spherical Quantization (DSQ), a novel method to make deep convolutional neural networks generate supervised and compact binary codes for efficient image search. Our approach simultaneously learns a mapping that transforms the input images into a low-dimensional discriminative space, and quantizes the transformed data points using multi-codebook quantization. To eliminate the negative effect of norm variance on codebook learning, we force the network to L_2 normalize the extracted features and then quantize the resulting vectors using a new supervised quantization technique specifically designed for points lying on a unit hypersphere. Furthermore, we introduce an easy-to-implement extension of our quantization technique that enforces sparsity on the codebooks. Extensive experiments demonstrate that DSQ and its sparse variant can generate semantically separable compact binary codes outperforming many state-of-the-art image retrieval methods on three benchmarks.", "num_citations": "19\n", "authors": ["1131"]}
{"title": "Fast cosine similarity search in binary space with angular multi-index hashing\n", "abstract": " Given a large dataset of binary codes and a binary query point, we address how to efficiently find    codes in the dataset that yield the largest cosine similarities to the query. The straightforward answer to this problem is to compare the query with all items in the dataset, but this is practical only for small datasets. One potential solution to enhance the search time and achieve sublinear cost is to use a hash table populated with binary codes of the dataset and then look up the nearby buckets to the query to retrieve the nearest neighbors. However, if codes are compared in terms of cosine similarity rather than the Hamming distance, then the main issue is that the order of buckets to probe is not evident. To examine this issue, we first elaborate on the connection between the Hamming distance and the cosine similarity. Doing this allows us to systematically find the probing sequence in the hash table. However, solving the\u00a0\u2026", "num_citations": "18\n", "authors": ["1131"]}
{"title": "Employing aspect composition in adaptive software systems: A case study\n", "abstract": " Adaptive software is a closed-loop system which aims at adjusting itself at runtime in different situations. Such a system needs a set of sensors to monitor attributes of itself and its operating environment. Furthermore, it requires a set of effectors in order to make changes in its entities. These changes are essential for fulfilling system's non-functional and functional requirements. Aspect-Oriented Programming (AOP) is a promising way to develop these sensors and effectors through static and dynamic composition of advices. This paper presents the experience of employing aspect composition in engineering a sample adaptive software. The main objectives are exploring the difficulties of utilizing this approach, and investigating the effectiveness of aspect-based adaptation actions. A J2EE bookstore application, TPC-W, was selected as the case study, to instrument sensors by the aid of static aspects, and effectors\u00a0\u2026", "num_citations": "17\n", "authors": ["1131"]}
{"title": "JComp: A reuse-driven componentization framework for Java applications\n", "abstract": " Program componentization has been proved to be an effective way to increase the speed and cost-effectiveness of reusing, maintaining, and understanding existing software systems. JComp is a componentization framework that supports semi-automatically extracting reusable components from an existing Java system and automatically transforming the existing system into a component-based system. Based on the automated class dominance analysis and domain knowledge, JComp introduces a novel technique to incrementally identify reusable components in a low cost but high precision way. Also, JComp provides a methodology to automate the transformation of existing object-oriented systems into component-based systems.", "num_citations": "17\n", "authors": ["1131"]}
{"title": "Defect prioritization in the software industry: challenges and opportunities\n", "abstract": " Defect prioritization is a decision making process wherein stakeholders determine the temporal order of open defects to be fixed. It is critical to the software development lifecycle as the decisions made during this process directly affect release planning, resource management, and maintenance costs. In fact, defect prioritization is complex as many factors need to be taken into consideration and the decisions made can be subjective or incorporate inherent knowledge and intuition of decision makers. We believe that managing the complexities of the decision making process can provide valuable support and help in uncovering any inconsistencies in the interpretation of criteria to prioritize defects. In this paper, we explore the defect triaging process in Research In Motion to gain a better understanding of the shortcomings and challenges of the current practices. Based on our findings, we sketch some research\u00a0\u2026", "num_citations": "14\n", "authors": ["1131"]}
{"title": "The (5+ 1) architectural view model for cloud applications.\n", "abstract": " Existing software architecture frameworks focus on application development, rather than the dynamic evolution of applications at runtime. Their view models reflect design considerations, more than service operations. However, the quality of a cloud application depends on its configuration and the architecture of its service model. For this reason, we need a view model that is constructed around deployment. This paper proposes a (5+ 1) architectural view model, where each view corresponds to a different perspective on cloud application deployment. The (5+ 1) view model has been realized as a layered, domain specific modeling language, and the capabilities of this language have been illustrated using a representative domain example. The model was derived by investigating the process of architecting cloud applications, and then providing a set of meta-models to describe cloud applications within their ecosystem.", "num_citations": "13\n", "authors": ["1131"]}
{"title": "Online nearest neighbor search in binary space\n", "abstract": " We revisit the K Nearest Neighbors (KNN) problem in large binary datasets which is of major importance in several applied areas. The goal is to find the K nearest items in a dataset to a query point where both the query and the items lie in the Hamming cube. The problem is addressed in its online setting, that is, data items are inserted sequentially into the dataset. To accommodate efficient similarity search and fast insertion of new items, we propose a data structure that partitions the feature space based on the Hamming weights of the binary codes and their substrings. Empirical evaluations on a large-scale dataset show significant speedup over the linear scan baseline.", "num_citations": "12\n", "authors": ["1131"]}
{"title": "On the road to holistic decision making in adaptive security\n", "abstract": " Security is a critical concern in today's software systems. Besides the interconnectivity and dynamic nature of network systems, the increasing complexity in modern software systems amplifies the complexity of IT security. This fact leaves attackers one step ahead in exploiting vulnerabilities and introducing new cyberattacks. The demand for new methodologies in addressing cybersecurity is emphasized by both private and national corporations. A practical solution to dynamically manage the high complexity of IT security is adaptive security, which facilitates analysis of the system's behaviour and hence the prevention of malicious attacks in complex systems. Systems that feature adaptive security detect and mitigate security threats at runtime with little or no administrator involvement. In these systems, decisions at runtime are balanced according to quality and performance goals. This article describes the necessity of holistic decision making in such systems and paves the road to future research.", "num_citations": "11\n", "authors": ["1131"]}
{"title": "Software evolution towards model-centric runtime adaptivity\n", "abstract": " Runtime adaptivity is a promising direction towards achieving adaptive behavior for software systems that operate within highly dynamic and non-deterministic environments. Model-centric approaches have proven to be able to successfully address various aspects of runtime adaptivity. In this paper, we propose a target architecture for self-adaptive software systems and show how it facilitates adaptation by interpreting models at runtime. Our approach supports adaptivity using models, which are causally connected to the software application. These models can be queried and transformed dynamically in reaction to changes in the software system's operating environment. We demonstrate how to implement an infrastructure to support the target architecture, and how to prepare and integrate non-adaptive software to comply with this architecture.", "num_citations": "11\n", "authors": ["1131"]}
{"title": "A requirement-based software testing framework: An industrial practice\n", "abstract": " Testing heterogeneous software systems raises new research challenges. Requirement-based testing is a promising approach for testing such systems. Research in motion (RIM), whose main product is indeed heterogeneous, has initiated an academia-industry collaborative project to investigate the automation of their requirement-based testing process. In this paper, we first identify research challenges observed through that project and then, in order to address those challenges, propose a framework that assists testers in three steps of test planning, generation, and regression.", "num_citations": "11\n", "authors": ["1131"]}
{"title": "E-BUS: a toolkit for extracting business services from java software systems\n", "abstract": " E-BUS (Extracting BUsiness Services) is an integrated environment to extract and model critical business services embedded in Java systems by utilizing graphic representations and transformations of system models. The extracted business services can be realized as self-contained components. Our evaluation has shown that E-BUS is effective and scalable in identifying and extracting business services from large Java legacy systems.", "num_citations": "11\n", "authors": ["1131"]}
{"title": "Towards a framework to incorporate NFRs into UML models\n", "abstract": " Despite the fact that Non-Functional Requirements (NFRs) are very difficult to achieve and at the same time are expensive to deal with, a few research works have focused on them as first class requirements in a development process. We propose a framework to incorporate NFRs, as reusable components, with standard UML notations. Such a framework can also integrate those reusable NFRs with the extracted UML representations of legacy systems during the reverse engineering process. This novel research work uses standard XMI representation of UML models without proposing any extension to it. As a proof of concept, a small case study of a Credit Card System is presented.", "num_citations": "11\n", "authors": ["1131"]}
{"title": "A policy-based decision making approach for orchestrating autonomic elements\n", "abstract": " Autonomic systems encompass elements, components, and sometimes smaller autonomic sub-systems. Composition, integration and orchestration at different levels of such autonomic systems are significant issues in the interoperability of their constituent elements. This research focuses on orchestration as an important process of interoperability in an abstract autonomic system model. Policy-based management has potential to play an important role in orchestrating constituent components of autonomic systems/elements, and also to fill the gap between business and IT objectives. This research applies policy-based orchestration using crisp and fuzzy policies to reach the business goals of the system, and to obey the constraints defined by the service level agreement (SLA). First, a simulated model has been used in MATLAB/Simulink environment, and then the system is implemented by Agent Building and\u00a0\u2026", "num_citations": "11\n", "authors": ["1131"]}
{"title": "Online nearest neighbor search using hamming weight trees\n", "abstract": " Nearest neighbor search is a basic and recurring proximity problem that has been studied for several decades. The goal is to preprocess a dataset of points so that we can quickly report the closet point(s) to any query point. Many recent applications of NNS involve datasets that are very large and dynamic, that is items of data items become available gradually. In this study, we propose a data structure for solving NNS for dynamic binary data where both query and dataset items are represented as binary strings. The proposed tree data structure, called the Hamming Weight Tree, is simple and as the names suggests, is based on partitioning the feature space of binary strings by exploiting the Hamming weights of the binary codes and their substrings. Given a Hamming Weight Tree of binary codes, we propose two search algorithms that accommodate nearest neighbor search for two different distance functions, the\u00a0\u2026", "num_citations": "10\n", "authors": ["1131"]}
{"title": "Strategy-aware mitigation using Markov games for dynamic application-layer attacks\n", "abstract": " Targeted and destructive nature of strategies used by attackers to break down the system require mitigation approaches with dynamic awareness. In the domain of adaptive software security, the adaptation manager of a self-protecting software is responsible for selecting countermeasures to prevent or mitigate attacks immediately. Making a right decision in each and every situation is one of the most challenging aspects of engineering self-protecting software systems. Inspired by the game theory, in this research work, we model the interactions between the attacker and the adaptation manager as a two-player zero-sum Markov game. Using this game-theoretic approach, the adaptation manager can refine its strategies in dynamic attack scenarios by utilizing what has learned from the system's and adversary's actions. We also present how this approach can be fitted to the well-known MAPE-K architecture model. As\u00a0\u2026", "num_citations": "10\n", "authors": ["1131"]}
{"title": "Using dynamic execution data to generate test cases\n", "abstract": " The testing activities of the Software Verification and Validation (SV&V) team at Research In Motion (RIM) are requirements-based, which is commonly known as requirements-based testing (RBT). This paper proposes a novel approach to enhance the current RBT process at RIM, by utilizing historical testing data from previous releases, static analysis of the modified source code, and real-time execution data. The main focus is on the test case generation phase and the objective is to increase the effectiveness and efficiency of test cases in such a way that overall testing is improved. The enhanced process not only automatically generates effective test cases but also seeks to achieve high test coverage and low defect escape rate.", "num_citations": "9\n", "authors": ["1131"]}
{"title": "Assessing the impact of using design-pattern-based systems\n", "abstract": " Design patterns are micro {architectures, high level building blocks which describe the solutions to speci c problems in software design. These solutions have generally been developed and evolved over time. It has been claimed that design patterns are capable of:(i) simplifying the design, implementation, and maintenance of complex systems, and (ii) improving the quality of software systems. However, in all the prior works reported in the literature, the claimed evidence has been of a qualitative nature. This research deals with collecting metrics which can be used to measure the impact of design {pattern {based systems on maintainability of parallel and distributed applications. In addition, we also measure the software complexity of these applications based on the de nition of complexity by other researchers. To the best of our knowledge, this is the rst work to evaluate the in uence of design {pattern {based systems in a quantitative manner. iv", "num_citations": "9\n", "authors": ["1131"]}
{"title": "Mitigating dynamic attacks using multi-agent game-theoretic techniques\n", "abstract": " Emerging technologies such as mobile and cloud computing have given rise to new security vulnerabilities and challenges. At the same time, attackers utilize these technologies to initiate sophisticated attacks and exploit known and unknown vulnerabilities. A unique characteristic of recent attacks is their dynamic nature which allows attackers to stay stealth from Intrusion Detection Systems (IDSs). The proactive and dynamic nature of these security attacks make their detection and consequently their mitigation challenging. This demands fast reacting adaptive systems that are capable of detecting and mitigating threats on the fly. Our novel approach aims at addressing this demand by engineering a Self-Protecting Software (SPS) that incorporates attacker's possible strategies when selecting countermeasures. To achieve this goal, we propose a technique to model objectives of the attacker and SPS by the aid of\u00a0\u2026", "num_citations": "7\n", "authors": ["1131"]}
{"title": "Impact of Using Pattern-Based Systems on the Qualities of Parallel Applications.\n", "abstract": " Abstract Design patterns are micro architectures, high-level building blocks which describe the solutions to speci c problems in software design. These solutions have generally been developed and evolved over time. It has been claimed that design patterns are capable of:(i) simplifying the design, implementation, and maintenance of complex systems, and (ii) improving the quality of software systems. However, in all the prior works reported in the literature, the claimed evidence has been of a qualitative nature. This research deals with collecting metrics which can be used to measure the impact of design-pattern-based systems on maintainability of parallel applications. In addition, we also measure the software complexity of these applications based on the de nition of complexity by other researchers. To the best of our knowledge, this is the rst work to evaluate the in uence of designpattern-based systems in a quantitative manner.", "num_citations": "6\n", "authors": ["1131"]}
{"title": "A Bayesian game decision-making model for uncertain adversary types\n", "abstract": " Adaptive application security involves making decisions under uncertainties such as the time, the power, or the damage of potential attacks. One of the uncertainties that has been largely ignored in the literature is the intention of the adversaries. The majority of research focuses on characteristics of attacks (eg, their request arrival rates), whereas characteristics of attackers/adversaries (eg, their intentions and strategies) are neglected. In today's sophisticated attacks, in order to confuse defense systems, adversaries may initiate an attack that exhibits a scenario similar to another attack but has an entirely different malicious goal (eg, to break down the server or to harm a specific user in the system). In such cases, incorporating uncertainty about the type of adversaries into the decision model helps to choose a proper countermeasure for protecting the software system efficiently. In this paper, we present a Bayesian\u00a0\u2026", "num_citations": "5\n", "authors": ["1131"]}
{"title": "\u2019Architectural Recovery of JBoss Application Server\u2019\n", "abstract": " This report addresses analysis of the architecture of an object-oriented system written in Java\u2013JBoss Application Server. By selecting this case study, we follow two major objectives. First, we want to use object-oriented entities as building blocks of software architecture. Second, due to success of JBoss as an open source J2EE platform in the market and among developers, and its high modular and scalable design, it is an apt case study for Software Architectural Recovery.In a nutshell, we aim to make a bridge from object-oriented system hierarchy and its design rules to generic building blocks, relations, and architectural rules in JBoss Application Server. We propose how a subset of object-oriented design rules is able to evaluate architecture of a software system in terms of maintainability and reusability. We also propose a set of hypothesis describing a lightweight methodology to express the architectural flaws of JBoss system based on source code analysis approach.", "num_citations": "5\n", "authors": ["1131"]}
{"title": "Introducing adaptivity to achieve longevity for software\n", "abstract": " Long living software systems (LLSSs) must provide the flexibility to react to changes in their operating environment as well as to changes in the user's requirements, even during operation. Self-adaptive software systems (SASSs) face adaptivity at runtime within predefined bounds. Yet, not all types of necessary variations can be anticipated and unforeseen changes to software may happen. Thus, systems that are meant to live in such an open-ended world must provide self-adaptivity (micro adaptation), but there is an additional need for adaptability of the system so that it can be adjusted externally (macro adaptation). This paper gives an overview of the graphbased runtime adaptation framework (GRAF) and sketches how it targets both types of adaptation.", "num_citations": "4\n", "authors": ["1131"]}
{"title": "A framework to incorporate non functional requirements into UML models\n", "abstract": " Despite the fact that Non-Functional Requirements (NFRs) are very difficult to achieve and at the same time are expensive to deal with, a few research works have focused on them as first class requirements in a development process. We propose a framework to incorporate NFRs, as reusable components, into standard UML notations. Such a framework can also integrate those reusable NFRs with the extracted UML representations of legacy systems during the reverse engineering process. This novel research work uses standard XMI representation of UML models without proposing any extension to it.", "num_citations": "4\n", "authors": ["1131"]}
{"title": "Coping with requirements changes in software verification and validation\n", "abstract": " The testing activities of the Software Verification & Validation (SV&V) team at Research In Motion (RIM) are requirements-based, which is commonly known as requirements-driven testing (RDT). Software requirements are continuously changing, which has an important impact on the RDT process. This paper describes the major challenges in coping with requirements changes in the software verification and validation processes and indicates how those challenges are being addressed at RIM.", "num_citations": "4\n", "authors": ["1131"]}
{"title": "Software bugs\n", "abstract": " The sections in this article are", "num_citations": "4\n", "authors": ["1131"]}
{"title": "A hybrid analysis framework to evaluate runtime behavior of oo systems\n", "abstract": " Since software is often deployed in safety critical applications, there is a constant need to know whether a system is behaving correctly and reliably in its environment. This research work integrates concepts of static and dynamic analyses to verify the behavioral correctness of a Java software system based on certain safety properties. We also apply the proposed framework on a sample Java application.", "num_citations": "3\n", "authors": ["1131"]}
{"title": "A coordination mechanism for self-healing and self-optimizing disciplines\n", "abstract": " There is an increasing demand for autonomic systems, which offer controlling complexity through a decentralized, multi-discipline and policy-based paradigm. In practice, only one discipline is often taken into account, while having shared resources and policies, it is required to coordinate dfferent disciplines. This research addresses the problem of coordinating self-healing and self-optimizing in autonomic elements by generic modelling of disciplines, and proposing a coordination mechanism.", "num_citations": "3\n", "authors": ["1131"]}
{"title": "Evolving legacy systems through a multi-objective decision process\n", "abstract": " Our previous work on improving the quality of object-oriented legacy systems includes: i) devising a quality-driven re-engineering framework (L. Tahvildari et al., 2003); ii) proposing a software transformation framework based on soft-goal interdependency graphs to enhance quality (L. Tahvildari and K. Kontogiannis, 2002); and iii) investigating the usage of metrics for detecting potential design flaws (L. Tahvildari and K. Kontogiannis, 2004). This paper defines a decision making process that determines a list of source-code improving transformations among several applicable transformations. The decision-making process is developed on a multi-objective decision analysis technique. This type of technique is necessary as there are a number of different, and sometimes conflicting, criterion among non-functional requirements. For the migrant system, the proposed approach uses heuristic estimates to guide the\u00a0\u2026", "num_citations": "2\n", "authors": ["1131"]}
{"title": "Coordinating self-healing and self-optimizing disciplines in autonomic elements\n", "abstract": " There is an increasing demand for self-adaptivity in software systems. Such systems offer the promise of controlling complexity through the achievement of self-governance (autonomy) and self-management (autonomicity). In a nutshell, they are based on three principles of separation of concerns, decentralization and policy-based management. In practice, often only one concern has been taken into account to make this paradigm a realization. However having common policies and shared resources, it is required to coordinate different concerns. This research work addresses the problem of coordinating two autonomic disciplines namely, self-healing and self-optimizing, in autonomic elements. First a generic form of these two disciplines is presented, and then the effect of applying a novel coordination mechanism on an experiment will be discussed. vi", "num_citations": "1\n", "authors": ["1131"]}
{"title": "Policy-based orchestration of autonomic elements\n", "abstract": " Autonomic systems encompass of components, applications and systems. Composition, integration and orchestration at different levels of such autonomic systems are significant issues in their design, evolution and operation. This research focuses on orchestration in a simulated autonomic system. Policy-based management has potential to play an important role in orchestrating constituent components of autonomic systems (autonomic elements) more effectively and also filling the gap between business and IT objectives. We use policy-based orchestration, by crisp and fuzzy policies, to reach the business goals of the system and to obey the constraints defined by the Service Level Agreement (SLA).", "num_citations": "1\n", "authors": ["1131"]}
{"title": "Quality-driven object-oriented software reengineering\n", "abstract": " Object-orientation in software engineering defines methods and techniques that assist software engineers to build large, flexible, modular, and reusable systems. Many of the systems that have been built over the past decade using object-oriented techniques like inheritance, encapsulation, polymorphism, information hiding, and late binding are already considered legacy systems. It is of no surprise that it is already difficult to maintain these systems. Since these legacy systems often incorporate undocumented valuable business logic, throwing away or rebuilding them from scratch in a more flexible manner is a very difficult and risky task. A possible solution to this problem is software re-engineering, that is retaining the functionality of such systems while improving their quality characteristics.In this context, we need a comprehensive framework that allows for the definition and enactment of the re-engineering process\u00a0\u2026", "num_citations": "1\n", "authors": ["1131"]}
{"title": "Testing challenges in adoption of component-based software\n", "abstract": " The adoption of components in development of complex software systems can surely have various benefits. Their testing, however, is still one of the open issues in software engineering. On the other hand, building high quality and reusable components is very important for component-based software development projects. This means that the testability of software components is one of the important factors determining the quality of components and their adoption as extensions to commonly used office suites and middle-ware platforms. To develop such high quality components, we need to answer concerning component testing and component testability. This paper shares our thoughts and understanding of component testability, and discusses and identifies the challenges and issues concerning to the testing of evolving component-based software systems.", "num_citations": "1\n", "authors": ["1131"]}