{"title": "Local-based active classification of test report to assist crowdsourced testing\n", "abstract": " In crowdsourced testing, an important task is to identify the test reports that actually reveal fault-true fault, from the large number of test reports submitted by crowd workers. Most existing approaches towards this problem utilized supervised machine learning techniques, which often require users to manually label a large amount of training data. Such process is time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labeling while still being able to achieve good performance is crucial. Active learning is one potential technique to address this challenge, which aims at training a good classifier with as few labeled data as possible. Nevertheless, our observation on real industrial data reveals that existing active learning approaches generate poor and unstable performances on crowdsourced testing data. We analyze the deep reason and find that the dataset has significant local biases. To\u00a0\u2026", "num_citations": "56\n", "authors": ["238"]}
{"title": "Towards Effectively Test Report Classification to Assist Crowdsourced Testing\n", "abstract": " Context: Automatic classification of crowdsourced test reports is important due to their tremendous sizes and large proportion of noises. Most existing approaches towards this problem focus on examining the performance of different machine learning or information retrieval techniques, and most are evaluated on open source dataset. However, our observation reveals that these approaches generate poor and unstable performances on real industrial crowdsourced testing data. We further analyze the deep reason and find that industrial data have significant local bias, which degrades existing approaches.Goal: We aim at designing an approach to overcome the local bias in industrial data and automatically classifying true fault from the large amounts of crowdsourced reports.Method: We propose a cluster-based classification approach, which first clusters similar reports together and then builds classifiers based on\u00a0\u2026", "num_citations": "46\n", "authors": ["238"]}
{"title": "FixerCache: Unsupervised caching active developers for diverse bug triage\n", "abstract": " Context: Bug triage aims to recommend appropriate developers for new bugs in order to reduce time and effort in bug resolution. Most previous approaches for bug triage are supervised. Before recommending developers, these approaches need to learn developers' bug-fix preferences via building and training models using text-information of developers' historical bug reports.Goal: In this paper, we empirically address three limitations of supervised bug triage approaches and propose FixerCache, an unsupervised approach for bug triage by caching developers based on their activeness in components of products.Method: In FixerCache, each component of a product has a dynamic developer cache which contains prioritized developers according to developers' activeness scores. Given a new bug report, FixerCache recommends fixers with high activeness in developer cache to participate in fixing the new bug\u00a0\u2026", "num_citations": "43\n", "authors": ["238"]}
{"title": "KSAP: An approach to bug report assignment using KNN search and heterogeneous proximity\n", "abstract": " ContextBug report assignment, namely, to assign new bug reports to developers for timely and effective bug resolution, is crucial for software quality assurance. However, with the increasing size of software system, it is difficult to assign bugs to appropriate developers for bug managers.ObjectiveThis paper propose an approach, called KSAP (K-nearest-neighbor search and heterogeneous proximity), to improve automatic bug report assignment by using historical bug reports and heterogeneous network of bug repository.MethodWhen a new bug report was submitted to the bug repository, KSAP assigns developers for the bug report by using a two-phase procedure. The first phase is to search historically-resolved similar bug reports to the new bug report by K-nearest-neighbor (KNN) method. The second phase is to rank the developers who contributed to those similar bug reports by heterogeneous proximity\u00a0\u2026", "num_citations": "36\n", "authors": ["238"]}
{"title": "DevNet: exploring developer collaboration in heterogeneous networks of bug repositories\n", "abstract": " During open source software development and maintenance, bug fixing is a result of developer collaboration. Understanding the structure of developer collaboration could be helpful for effective and efficient bug fixing. Most prior work on exploring developer collaboration in bug repositories only considers a particular form of developer collaboration. However, in real software bug repositories, developers collaborate with each other via multiple ways, e.g., commenting bugs, tossing bugs, and assigning bugs. In this paper, we present DevNet, a framework for representing and analyzing developer collaboration in bug repositories based on heterogeneous developer networks. Moreover, we illustrate that such developer collaboration can assist bug triage through a case study on the bug repositories of Eclipse and Mozilla involving over 800,000 bug reports. Experiment results show that our approach can improve the\u00a0\u2026", "num_citations": "24\n", "authors": ["238"]}
{"title": "Domain adaptation for test report classification in crowdsourced testing\n", "abstract": " In crowdsourced testing, it is beneficial to automatically classify the test reports that actually reveal a fault - a true fault, from the large number of test reports submitted by crowd workers. Most of the existing approaches toward this task simply leverage historical data to train a machine learning classifier and classify the new incoming reports. However, our observation on real industrial data reveals that projects under crowdsourced testing come from various domains, and the submitted reports usually contain different technical terms to describe the software behavior for each domain. The different data distribution across domains could significantly degrade the performance of classification models when utilized for cross-domain report classification. To build an effective cross-domain classification model, we leverage deep learning to discover the intermediate representation that is shared across domains, through the co\u00a0\u2026", "num_citations": "22\n", "authors": ["238"]}
{"title": "Heterogeneous network analysis of developer contribution in bug repositories\n", "abstract": " Using a bug repository, developers contribute to improve the quality of software incrementally by creating and updating bug reports. All the software artifacts in bug repositories are derived from developer contribution. Most prior studies on developer contribution in bug repositories bias on one particular form, e.g., commenting bug reports. However, in real practice of bug repositories, developers participate in and contribute to software projects via multiple ways, e.g., reporting new bugs, reopening incorrectly fixed bugs, commenting unfixed bug reports, and fixing unsolved bugs. In this paper, we exploit recent advances in analysis of heterogeneous network to avoid biased aspects in measuring developer contribution and explore multiple types of developer contribution in bug repositories. Further, we consider leveraging such multiple types of developer contribution to assist a typical prediction problem in bug\u00a0\u2026", "num_citations": "22\n", "authors": ["238"]}
{"title": "Images Don't Lie: Duplicate Crowdtesting Reports Detection With Screenshot Information\n", "abstract": " Context: Crowdtesting is effective especially when it comes to the feedback on GUI systems, or subjective opinions about features. Despite of this, we find crowdtesting reports are highly duplicated, i.e., 82% of them are duplicates of others. Most of the existing approaches mainly adopted textual information for duplicate detection, and suffered from low accuracy because of the lexical gap. Our observation on real industrial crowdtesting data found that when dealing with crowdtesting reports of GUI systems, the reports would be accompanied with images, i.e., the screenshots of the tested app. We assume the screenshot to be valuable for duplicate crowdtesting report detection because it reflects the real context of the bug and is not affected by the variety of natural languages.Objective: We aim at automatically detecting duplicate crowdtesting reports that could help reduce triaging effort.Method: In this work, we\u00a0\u2026", "num_citations": "21\n", "authors": ["238"]}
{"title": "Is there a\" golden\" feature set for static warning identification? an experimental evaluation\n", "abstract": " Background: The most important challenge regarding the use of static analysis tools (eg, FindBugs) is that there are a large number of warnings that are not acted on by developers. Many features have been proposed to build classification models for the automatic identification of actionable warnings. Through analyzing these features and related studies, we observe several limitations that make the users lack practical guides to apply these features.Aims: This work aims at conducting a systematic experimental evaluation of all the public available features, and exploring whether there is a golden feature set for actionable warning identification.Method: We first conduct a systematic literature review to collect all public available features for warning identification. We employ 12 projects with totally 60 revisions as our subject projects. We then implement a tool to extract the values of all features for each project revision\u00a0\u2026", "num_citations": "20\n", "authors": ["238"]}
{"title": "Multi-Objective Crowd Worker Selection in Crowdsourced Testing\n", "abstract": " Crowdsourced testing is an emerging trend in software testing, which relies on crowd workers to accomplish test tasks. Typically, a crowdsourced testing task aims to detect as many bugs as possible within a limited budget. For a specific test task, not all crowd workers are qualified to perform it, and different test tasks require crowd workers to have different experiences, domain knowledge, etc. Inappropriate workers may miss true bugs, introduce false bugs, or report duplicated bugs, which could not only decrease the quality of test outcomes, but also increase the cost of hiring workers. Thus, how to select the appropriate crowd workers for specific test tasks is a challenge in crowdsourced testing.This paper proposes a Multi-Objective crowd wOrker SE-lection approach (MOOSE), which includes three objectives: maximizing the coverage of test requirement, minimizing the cost, and maximizing bug-detection experience of the selected crowd workers. Specifically, MOOSE leverages NSGA-II, a widely used multi-objective evolutionary algorithm, to optimize the three objectives when selecting workers. We evaluate MOOSE on 42 test tasks (involve 844 crowd workers and 3,984 test reports) from one of the largest crowdsourced testing platforms in China, and the experimental results show MOOSE could improve the best baseline by 17% on average in bug detection rate.", "num_citations": "19\n", "authors": ["238"]}
{"title": "Characterizing Crowds to Better Optimize Worker Recommendation in Crowdsourced Testing\n", "abstract": " Crowdsourced testing is an emerging trend, in which test tasks are entrusted to the online crowd workers. Typically, a crowdsourced test task aims to detect as many bugs as possible within a limited budget. However not all crowd workers are equally skilled at finding bugs; Inappropriate workers may miss bugs, or report duplicate bugs, while hiring them requires nontrivial budget. Therefore, it is of great value to recommend a set of appropriate crowd workers for a test task so that more software bugs can be detected with fewer workers. This paper first presents a new characterization of crowd workers and characterizes them with testing context, capability, and domain knowledge. Based on the characterization, we then propose Multi-Objective Crowd wOrker recoMmendation approach (MOCOM), which aims at recommending a minimum number of crowd workers who could detect the maximum number of bugs for a\u00a0\u2026", "num_citations": "13\n", "authors": ["238"]}
{"title": "Will this bug-fixing change break regression testing?\n", "abstract": " Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing\u00a0\u2026", "num_citations": "13\n", "authors": ["238"]}
{"title": "BAHA: A novel approach to automatic bug report assignment with topic modeling and heterogeneous network analysis\n", "abstract": " We propose an approach called Bug report assignment with topic modeling and heterogeneous network analysis (BAHA) to automatically assign bug reports to developers. Existing studies adopt social network analysis to characterize the collaboration of developers. The networks used in these studies are all homogenous. In real practice of bug resolution, different developers collaborate on different bug reports that makes the homogenous network unable to capture this information. We use heterogeneous network to describe the relations between reporters, bug reports and developers to characterize developers' collaboration. Experiments on Eclipse JDT project show that BAHA outperforms the state of art methods on automatic bug report assignment.", "num_citations": "8\n", "authors": ["238"]}
{"title": "Machine Learning-based Water Level Prediction in Lake Erie\n", "abstract": " Predicting water levels of Lake Erie is important in water resource management as well as navigation since water level significantly impacts cargo transport options as well as personal choices of recreational activities. In this paper, machine learning (ML) algorithms including Gaussian process (GP), multiple linear regression (MLR), multilayer perceptron (MLP), M5P model tree, random forest (RF), and k-nearest neighbor (KNN) are applied to predict the water level in Lake Erie. From 2002 to 2014, meteorological data and one-day-ahead observed water level are the independent variables, and the daily water level is the dependent variable. The predictive results show that MLR and M5P have the highest accuracy regarding root mean square error (RMSE) and mean absolute error (MAE). The performance of ML models has also been compared against the performance of the process-based advanced hydrologic prediction system (AHPS), and the results indicate that ML models are superior in predictive accuracy compared to AHPS. Together with their time-saving advantage, this study shows that ML models, especially MLR and M5P, can be used for forecasting Lake Erie water levels and informing future water resources management. View Full-Text", "num_citations": "5\n", "authors": ["238"]}
{"title": "Context-aware In-process Crowdworker Recommendation\n", "abstract": " Identifying and optimizing open participation is essential to the success of open software development. Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to detect more bugs with fewer workers. However, these studies mainly focus on one-time recommendations with respect to the initial context at the beginning of a new task. This paper argues the need for in-process crowdtesting worker recommendation. We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, ie, no new bugs are revealed in consecutive test reports during the process of a crowdtesting task. This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened.", "num_citations": "5\n", "authors": ["238"]}
{"title": "Hiprank: Ranking nodes by influence propagation based on authority and hub\n", "abstract": " Traditional centrality measures such as degree, betweenness, closeness and eigenvector ignore the intrinsic impacts of a node on other nodes. This paper proposes a new algorithm, called HIPRank, to rank nodes based on their influences in the network. HIPRank includes two sub-procedures: one is to predefine the importance of an arbitrary small number of nodes with users\u2019 preferences, and the other one is to propagate the influences of nodes with respect to authority and hub to other nodes based on HIP propagation model. Experiments on DBLP citation network (over 1.5\u2009million nodes and 2.1\u2009million edges) demonstrate that on the one hand, HIPRank can prioritize the nodes having close relation to the user-preferred nodes with higher ranking than other nodes, and on the other hand, HIPRank can retrieve the authoritative nodes (with authority) and directive nodes (with hub) from the network\u00a0\u2026", "num_citations": "3\n", "authors": ["238"]}
{"title": "Application of SQL RAT Translation a Statement of RQP/RMP with an Object-oriented Solution\n", "abstract": " Since we have already designed a flexible form of representing the Relational Algebra Tree (RAT) translated by the SQL parser, the application of this kind of objectoriented representation should be explored. In this paper, we will show you how to apply this technique to complicated scenarios. The application of Reverse Query Processing and Reverse Manipulate Processing related to this issue will be discussed.", "num_citations": "3\n", "authors": ["238"]}
{"title": "Cutting away the confusion from crowdtesting\n", "abstract": " Crowdtesting is effective especially when it comes to the feedback on GUI systems, or subjective opinions about features. Despite of this, we find crowdtesting reports are highly replicated, i.e., 82% of them are replicates of others. Hence automatically detecting replicate reports could help reduce triaging efforts. Most of the existing approaches mainly adopted textual information for replicate detection, and suffered from low accuracy because of the expression gap. Our observation on real industrial crowdtesting data found that when dealing with crowdtesting reports of GUI systems, the reports would accompanied with images, i.e., the screenshots of the app. We assume the screenshot to be valuable for replicate crowdtesting report detection because it reflects the real scenario of the failure and is not affected by the variety of natural languages. In this work, we propose a replicate detection approach, TSDetector, which combines information from the screenshots and the textual descriptions to detect replicate crowdtesting reports. We extract four types of features to characterize the screenshots and the textual descriptions, and design an algorithm to detect replicates based on four similarity scores derived from the four different features respectively. We investigate the effectiveness and advantage of TSDetector on 15 commercial projects with 4,172 reports from one of the Chinese largest crowdtesting platforms.Results show that TSDetector can outperform existing state-of-the-art approaches significantly. In addition, we also evaluate its usefulness using real-world case studies. The feedback from real-world testers demonstrates its practical value", "num_citations": "2\n", "authors": ["238"]}
{"title": "Quest for the Golden Approach: An Experimental Evaluation of Duplicate Crowdtesting Reports Detection\n", "abstract": " Background: Given the invisibility and unpredictability of distributed crowdtesting processes, there is a large number of duplicate reports, and detecting these duplicate reports is an important task to help save testing effort. Although, many approaches have been proposed to automatically detect the duplicates, the comparison among them and the practical guidelines to adopt these approaches in crowdtesting remain vague.Aims: We aim at conducting the first experimental evaluation of the commonly-used and state-of-the-art approaches for duplicate detection in crowdtesting reports, and exploring which is the golden approach.Method: We begin with a systematic review of approaches for duplicate detection, and select ten state-of-the-art approaches for our experimental evaluation. We conduct duplicate detection with each approach on 414 crowdtesting projects with 59,289 reports collected from one of the largest\u00a0\u2026", "num_citations": "1\n", "authors": ["238"]}
{"title": "Leveraging Machine Learning to Improve Software Reliability\n", "abstract": " Finding software faults is a critical task during the lifecycle of a software system. While traditional software quality control practices such as statistical defect  prediction, static bug detection, regression test, and code review are often inefficient and time-consuming, which cannot keep up with the increasing complexity of modern software systems. We argue that machine learning with its capability in knowledge representation, learning, natural language processing, classification, etc., can be used to extract invaluable information from software artifacts that may be difficult to obtain with other research methodologies to improve existing software reliability practices such as statistical defect prediction, static bug detection, regression test, and code review.   This thesis presents a suite of machine learning based novel techniques to improve existing software reliability practices for helping developers find software bugs more effective and efficient. First, it introduces a deep learning based defect  prediction technique to improve existing statistical defect prediction models. To build accurate prediction models, previous studies focused on manually designing features that encode the statistical characteristics of programs. However, these features often fail to capture the semantic difference of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this thesis leverages deep learning techniques to learn a semantic representation of programs automatically from source code and further build and train defect prediction models by using these semantic features\u00a0\u2026", "num_citations": "1\n", "authors": ["238"]}
{"title": "Implementation technology study of distributed automated software testing\n", "abstract": " In order to study the distributed and automated testing of large-scale software, a distributed automated software testing platform (DASTP) was presented, which was based on the analysis of an existing software testing framework and the idea of distributed continuous software quality assurance. The prototype system was implemented. The platform integrated a series of tools required in the software testing, and could use free resources in Internet to complete the continuous integration and testing of large-scale software. A task scheduling algorithm based on part-time constraint and an improved ACO algorithm for set partitioning problem were proposed. These two algorithms can partition testing task into subtasks and then schedule these subtasks automatically. By running MySQL testing in the prototype system, the feasibility of the platform architecture and the effectiveness of the algorithms were verified.", "num_citations": "1\n", "authors": ["238"]}