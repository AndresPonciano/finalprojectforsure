{"title": "Building Watson: An overview of the DeepQA project\n", "abstract": " IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.", "num_citations": "1838\n", "authors": ["2083"]}
{"title": "Evaluating ontological decisions with OntoClean\n", "abstract": " Explosing common misuses of the subsumption relationship and the formal basis for why they are wrong.", "num_citations": "1190\n", "authors": ["2083"]}
{"title": "An overview of OntoClean\n", "abstract": " OntoClean is a methodology for validating the ontological adequacy of taxonomic relationships. It is based on highly general ontological notions drawn from philosophy, like essence, identity, and unity,which are used to characterize relevant aspects of the intended meaning of the properties, classes, and relations that make up an ontology. These aspects are represented by formal metaproperties, which impose several constraints on the taxonomic structure of an ontology. The analysis of these constraints helps in evaluating and validating the choices made. In this chapter we present an informal overview of the philosophical notions involved and their role in OntoClean, review some common ontological pitfalls, and walk through the example that has appeared in pieces in previous papers and has been the basis of numerous tutorials and talks.", "num_citations": "770\n", "authors": ["2083"]}
{"title": "A formal ontology of properties\n", "abstract": " A common problem of ontologies is that their taxonomic structure is often poor and confusing. This is typically exemplified by the unrestrained use of subsumption to accomplish a variety of tasks. In this paper we show how a formal ontology of unary properties can help using the subsumption relation in a disciplined way. This formal ontology is based on some meta-properties built around the fundamental philosophical notions of identity, unity, essence, and dependence. These meta-properties impose some constraints on the subsumption relation that clarify many misconceptions about taxonomies, facilitating their understanding, comparison and integration.", "num_citations": "656\n", "authors": ["2083"]}
{"title": "Supporting ontological analysis of taxonomic relationships\n", "abstract": " Taxonomies are an important part of conceptual modeling. They provide substantial structural information, and are typically the key elements in integration efforts, however there has been little guidance as to what makes a proper taxonomy. We have adopted several notions from the philosophical practice of formal ontology, and adapted them for use in information systems. These tools, identity, essence, unity, and dependence, provide a solid logical framework within which the properties that form a taxonomy can be analyzed. This analysis helps make intended meaning more explicit, improving human understanding and reducing the cost of integration.", "num_citations": "586\n", "authors": ["2083"]}
{"title": "Ontology: Towards a new synthesis\n", "abstract": " Systems presents a brief history of ontology as a discipline spanning the boundaries of philosophy and information science. We sketch some of the reasons for the growth of ontology in the information science field, and offer a preliminary stocktaking of how the term \u2018ontology\u2019is currently used. We conclude by suggesting some grounds for optimism as concerns the fhture collaboration between philosophical ontologists and information scientists.Philosophical ontology is the science of what is, of the kinds and structures of objects, properties, events, processes and relations in every area of reality. Philosophical ontology takes many forms, horn the metaphysics of Aristotle to the object-theory of Alexius Meinong. The term \u2018ontology\u2019(or ontologia) was itself coined in 1613, independently, by two philosophers, Rudolf Giickel (Goclenius), in his Lexiconphilosophicum and Jacob Lorhard (Lorhardus), in his Theatrum\u00a0\u2026", "num_citations": "426\n", "authors": ["2083"]}
{"title": "Owl web ontology language guide, w3c recommendation\n", "abstract": " CiNii \u8ad6\u6587 - OWL Web Ontology Language Guide, W3C Recommendation CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 OWL Web Ontology Language Guide, W3C Recommendation SMITH MK \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SMITH MK \u53ce\u9332\u520a\u884c\u7269 http://www.w3.org/TR/2004/REC-owl-guide-20040210/ http://www.w3.org/TR/2004/REC-owl-guide-20040210/, 2004 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30d5\u30a1\u30b8\u30a3\u8a18\u8ff0\u8ad6\u7406(<\u7279\u96c6>\u73fe\u5b9f\u30c6\u30ad\u30b9\u30c8\u304b\u3089\u306e\u77e5\u8b58\u8868\u73fe\u3068\u63a8\u8ad6\u306e\u5b9f\u73fe) \u517c\u5ca9 \u61b2 , Ken Kaneiwa \u4eba\u5de5\u77e5\u80fd\u5b66\u4f1a\u8a8c = Journal of Japanese Society for Artificial Intelligence 22(5), 588-596, 2007-09-01 \u53c2\u8003\u6587\u732e27\u4ef6 \u5927\u5b66\u9662\u8aac\u660e\u4f1a Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) \u2026", "num_citations": "419\n", "authors": ["2083"]}
{"title": "Ontological analysis of taxonomic relationships\n", "abstract": " Taxonomies based on a partial-ordering relation commonly known as is-a, class inclusion or subsumption have become an important tool in conceptual modeling. A well-formed taxonomy has significant implications for understanding, reuse, and integration, however the intuitive simplicity of taxonomic relations has led to widespread misuse, making clear the need for rigorous analysis techniques. Where previous work has focused largely on the semantics of the is-a relation itself, we concentrate here on the ontological nature of the arguments of this relation, in order to be able to tell whether a single is-a link is ontologically well-founded. For this purpose, we discuss techniques based on the philosophical notions of identity, unity, essence, and dependence, which have been adapted to the needs of information systems design. We demonstrate the effectiveness of these techniques by taking real examples of\u00a0\u2026", "num_citations": "288\n", "authors": ["2083"]}
{"title": "A reusable ontology for fluents in OWL\n", "abstract": " A critical problem for practical KR is dealing with relationships that change over time. This problem is compounded by representation languages such as OWL that are biased towards binary relations; even when the relationships that vary with time are binary, the time the relationship holds typically requires a third argument. We discuss the standard approach to this problem, and contrast it to a new alternative based on a four-dimensionalist (perdurantist) ontology, which allows us to use more of the expressive power of description logics. The technique is usable in other logics as well.", "num_citations": "284\n", "authors": ["2083"]}
{"title": "Identity, unity, and individuality: Towards a formal toolkit for ontological analysis\n", "abstract": " We introduce here the notions of identity and unity as they have been discussed in Philosophy, and then provide additional clarifications needed to use these notions as fundamental tools in a methodology for ontology-driven conceptual analysis. We show how identity and unity complement each other under a general notion of individuality, and conclude with an example of how these tools can be used in analysis to help check the ontological consistency of taxonomies.", "num_citations": "205\n", "authors": ["2083"]}
{"title": "Identity and subsumption\n", "abstract": " The intuitive simplicity of the so-called is-a (or subsumption) relationship has led to widespread ontological misuse. Where previous work has focused largely on the semantics of the relationship itself, we concentrate here on the ontological nature of its arguments, in order to tell whether a single is-a link is ontologically well-founded. For this purpose, we introduce some techniques based on the philosophical notions of identity, unity, and essence, which have been adapted to the needs of taxonomy design. We demonstrate the effectiveness of these techniques by taking real examples of poorly structured taxonomies and revealing cases of invalid generalization.", "num_citations": "146\n", "authors": ["2083"]}
{"title": "Towards a methodology for ontology-based model engineering\n", "abstract": " The philosophical discipline of Ontology is evolving towards an engineering discipline, and in this evolution the need for a principled methodology has clearly arisen. In this paper, we briefly discuss our recent work towards developing a methodology for ontology-based model engineering. This methodology builds on previous methodology efforts, and is founded on important analytic notions that have been drawn from Philosophy and adapted to Engineering: identity, unity, rigidity, and dependence. We demonstrate how these techniques can be used to analyze properties, which clarifies many misconceptions about taxonomies and helps bring substantial order to ontologies.", "num_citations": "132\n", "authors": ["2083"]}
{"title": "A framework for merging and ranking of answers in DeepQA\n", "abstract": " The final stage in the IBM DeepQA pipeline involves ranking all candidate answers according to their evidence scores and judging the likelihood that each candidate answer is correct. In DeepQA, this is done using a machine learning framework that is phase-based, providing capabilities for manipulating the data and applying machine learning in successive applications. We show how this design can be used to implement solutions to particular challenges that arise in applying machine learning for evidence-based hypothesis evaluation. Our approach facilitates an agile development environment for DeepQA; evidence scoring strategies can be easily introduced, revised, and reconfigured without the need for error-prone manual effort to determine how to combine the various evidence scores. We describe the framework, explain the challenges, and evaluate the gain over a baseline machine learning approach.", "num_citations": "125\n", "authors": ["2083"]}
{"title": "A multi-strategy and multi-source approach to question answering\n", "abstract": " We are interested in improving the performance of QA systems by breaking away from the strict pipeline architecture. In addition, we require an architecture that allows for hybridization at low development cost and facilitates experimentation with different instantiations of system components. Our resulting architecture is one that is modular and easily extensible, and allows for multiple answering agents to address the same question in parallel and for their results to be combined. Our new question answering system, PIQUANT, adopts this flexible architecture. The answering agents currently implemented in PIQUANT vary both in terms of the strategies used and the knowledge sources consulted. For example, an answering agent may employ statistical methods for extracting answers to questions from a large corpus, while another answering agent may transform select natural language questions into logical forms and query structured knowledge sources for answers. In this paper, we first describe the architecture on which PIQUANT is based. We then describe the answering agents currently implemented within the PIQUANT system, and how they were configured for our TREC2002 runs. Finally, we show that significant performance improvement was achieved by our multi-agent architecture by comparing our TREC2002 results against individual answering agent performance.Descriptors:", "num_citations": "115\n", "authors": ["2083"]}
{"title": "Ontology research\n", "abstract": " In this issue, I have collected a fairly broad, although by no means exhaustive, sampling of work in the field of ontology research. To define a field is often quite difficult; it is more a collection of people and ideas than it is a specific technology. To represent our field, I present six articles that cover several of the major thrusts of ontology research from the past decade.", "num_citations": "114\n", "authors": ["2083"]}
{"title": "Structured data and inference in DeepQA\n", "abstract": " Although the majority of evidence analysis in DeepQA is focused on unstructured information (e.g., natural-language documents), several components in the DeepQA system use structured data (e.g., databases, knowledge bases, and ontologies) to generate potential candidate answers or find additional evidence. Structured data analytics are a natural complement to unstructured methods in that they typically cover a narrower range of questions but are more precise within that range. Moreover, structured data that has formal semantics is amenable to logical reasoning techniques that can be used to provide implicit evidence. The DeepQA system does not contain a single monolithic structured data module; instead, it allows for different components to use and integrate structured and semistructured data, with varying degrees of expressivity and formal specificity. This paper is a survey of DeepQA components that\u00a0\u2026", "num_citations": "113\n", "authors": ["2083"]}
{"title": "Learning to predict readability using diverse linguistic features\n", "abstract": " In this paper we consider the problem of building a system to predict readability of natural-language documents. Our system is trained using diverse features based on syntax and language models which are generally indicative of readability. The experimental results on a dataset of documents from a mix of genres show that the predictions of the learned system are more accurate than the predictions of naive human judges when compared against the predictions of linguistically-trained expert human judges. The experiments also compare the performances of different learning algorithms and different types of feature sets when used for predicting readability.", "num_citations": "111\n", "authors": ["2083"]}
{"title": "Finding needles in the haystack: Search and candidate generation\n", "abstract": " A key phase in the DeepQA architecture is Hypothesis Generation, in which candidate system responses are generated for downstream scoring and ranking. In the IBM Watson\u2122 system, these hypotheses are potential answers to Jeopardy!\u2122 questions and are generated by two components: search and candidate generation. The search component retrieves content relevant to a given question from Watson's knowledge resources. The candidate generation component identifies potential answers to the question from the retrieved content. In this paper, we present strategies developed to use characteristics of Watson's different knowledge sources and to formulate effective search queries against those sources. We further discuss a suite of candidate generation strategies that use various kinds of metadata, such as document titles or anchor texts in hyperlinked documents. We demonstrate that a combination of these\u00a0\u2026", "num_citations": "87\n", "authors": ["2083"]}
{"title": "OWL: a description logic based ontology language for the semantic web\n", "abstract": " It has long been realized that the web could benefit from having its content understandable and available in a machine processable form. The Semantic Web aims to achieve this via annotations that use terms defined in ontologies to give well defined meaning to Web accessible information and services. OWL, the ontology language recommended by the W3C for this purpose, was heavily influenced by Description Logic research. In this chapter we review briefly some early efforts that combine Description Logics and the Web, including predecessors of OWL such as OIL and DAML+ OIL. We then go on to describe OWL in some detail, including the various influences on its design, its relationship with RDFS, its syntax and semantics, and a range of tools and applications.", "num_citations": "81\n", "authors": ["2083"]}
{"title": "Relation extraction and scoring in DeepQA\n", "abstract": " Detecting semantic relations in text is an active problem area in natural-language processing and information retrieval. For question answering, there are many advantages of detecting relations in the question text because it allows background relational knowledge to be used to generate potential answers or find additional evidence to score supporting passages. This paper presents two approaches to broad-domain relation extraction and scoring in the DeepQA question-answering framework, i.e., one based on manual pattern specification and the other relying on statistical methods for pattern elicitation, which uses a novel transfer learning technique, i.e., relation topics. These two approaches are complementary; the rule-based approach is more precise and is used by several DeepQA components, but it requires manual effort, which allows for coverage on only a small targeted set of relations (approximately 30\u00a0\u2026", "num_citations": "80\n", "authors": ["2083"]}
{"title": "WatsonPaths: scenario-based question answering and inference over unstructured information\n", "abstract": " We present WatsonPaths, a novel system that can answer scenario-based questions. These include medical questions that present a patient summary and ask for the most likely diagnosis or most appropriate treatment. WatsonPaths builds on the IBM Watson question answering system. WatsonPaths breaks down the input scenario into individual pieces of information, asks relevant subquestions of Watson to conclude new information, and represents these results in a graphical model. Probabilistic inference is performed over the graph to conclude the answer. On a set of medical test preparation questions, WatsonPaths shows a significant improvement in accuracy over multiple baselines.", "num_citations": "79\n", "authors": ["2083"]}
{"title": "Using the right tools: enhancing retrieval from marked-up documents\n", "abstract": " We are experimenting with the representation of a DTD and associated documents (i.e., documents conformant to the DTD) in a knowledge representation (KR) system, in order to provide more sophisticated query and retrieval from TEI documents than current systems provide. We are using CLASSIC, a frame-based representation system developed at AT&T Bell Laboratories. Like many KR systems, CLASSIC enables the definition of structured concepts/frames, their organization into taxonomies, the creation and manipulation of individual instances of such concepts, and inference such as inheritance, relation transitivity, inverses, etc. In addition, CLASSIC provides for the key inferences of subsumption and classification. By representing a document as an individual instance of a hierarchy of concepts derived from the DTD, and by allowing the creation of additional user-defined concepts and relations\u00a0\u2026", "num_citations": "78\n", "authors": ["2083"]}
{"title": "Typing candidate answers using type coercion\n", "abstract": " Many questions explicitly indicate the type of answer required. One popular approach to answering those questions is to develop recognizers to identify instances of common answer types (e.g., countries, animals, and food) and consider only answers on those lists. Such a strategy is poorly suited to answering questions from the Jeopardy!\u2122 television quiz show. Jeopardy! questions have an extremely broad range of types of answers, and the most frequently occurring types cover only a small fraction of all answers. We present an alternative approach to dealing with answer types. We generate candidate answers without regard to type, and for each candidate, we employ a variety of sources and strategies to judge whether the candidate has the desired type. These sources and strategies provide a set of type coercion scores for each candidate answer. We use these scores to give preference to answers with more\u00a0\u2026", "num_citations": "69\n", "authors": ["2083"]}
{"title": "Formal ontology for subject\n", "abstract": " Subject-based classification is an important part of information retrieval, and has a long history in libraries, where a subject taxonomy was used to determine the location of books on the shelves. We have been studying the notion of subject itself, in order to determine a formal ontology of subjects for a large-scale digital library card catalog system. Deep analysis reveals a lot of ambiguity regarding the usage of subjects in existing systems and terminology, and we attempt to formalize these notions into a single framework for representing it.", "num_citations": "68\n", "authors": ["2083"]}
{"title": "A model driven approach for building OWL DL and OWL full ontologies\n", "abstract": " This paper presents an approach for visually modeling OWL DL and OWL Full ontologies based on the well-established visual modeling language UML. We discuss a metamodel for OWL based on the Meta-Object Facility, an associated UML profile as visual syntax, and transformations between both. The work we present supports model-driven development of OWL ontologies and is currently undergoing the standardization process of the Object Management Group. After describing our approach, we present the implementation of our approach and an example, showing how the metamodel and UML profile can be used to improve developing Semantic Web applications.", "num_citations": "67\n", "authors": ["2083"]}
{"title": "An integrated representation for software development and discovery\n", "abstract": " This thesis presents research that is focused on making software systems more understandable at the code-level, the level of specification provided by programming languages and at which software is typically maintained in practice. The goal has been to make a software system easier to maintain by presenting a better represented and more understandable code-level. Previous efforts to achieve this goal in the area of Software Information Systems have been successful in limited ways. The main emphasis of this research has been to extend the underlying representations used by Software Information Systems and demonstrate how these extensions better serve the goal.", "num_citations": "65\n", "authors": ["2083"]}
{"title": "Towards the open advancement of question answering systems\n", "abstract": " On February 27-28, 2008, a group of researchers from industry and academia met to discuss the state of the Question Answering (QA) field. The discussion focused on recent experiences from funded research programs (eg AQUAINT, HALO) and open evaluations (eg TREC, NTCIR). The group acknowledged that funded research programs and evaluations have been instrumental in establishing fundamental QA research. However, major advances in the field of QA are yet to be realized. Advances that can openly accelerate progress and greatly generalize QA technologies to produce scalable, more adaptable methodologies and business applications are within our reach. Although there are deep technical challenges, we believe these challenges can be effectively addressed by open collaboration in the open-source development of integrated QA technologies aimed at well-chosen sets of QA applications.It is\u00a0\u2026", "num_citations": "64\n", "authors": ["2083"]}
{"title": "IBM's PIQUANT in TREC2003\n", "abstract": " For the most part, the system we used for TREC2003 was a smooth evolution of the one we ran in TREC2002. We continued to use our multi-source and multi-agent architecture. For Factoid questions we used all of our previous answering agents with an additional pattern-based agent, an enhanced answer resolution algorithm, and increased coverage of the Cyc sanity checker. We will devote a portion of this paper to performing a post-mortem of our experiences with Cyc this year. For List questions, which we did not attempt previously, we ran our Factoid system with different parameters. For Definition questions we took an entirely new approach, which we call QA-by-Dossier, and which will be the other focus of this paper. While we think that our system performed reasonably well in this subtask, the NIST evaluation results do not reflect this, raising some questions about the Definition subtask specification and evaluation.Descriptors:", "num_citations": "57\n", "authors": ["2083"]}
{"title": "Towards an interoperability standard for text and multi-modal analytics\n", "abstract": " This report motivates and proposes elements of an architecture specification for creating and composing text and multi-modal analytics for processing unstructured information, based on the UIMA project originated at IBM Research.", "num_citations": "54\n", "authors": ["2083"]}
{"title": "Method for processing natural language questions and apparatus thereof\n", "abstract": " A method and an apparatus for selecting an answer to a natural language question. The method includes: detecting a named entity in the natural language question; extracting information related to an answer from the natural language question; searching in linked data according to the detected named entity; generating a candidate answer according to a search result; parsing the candidate answer according to the information related to the answer; and obtaining a value of a feature of the candidate answer; and evaluating each candidate answer by synthesizing the value of the feature of the candidate answer.", "num_citations": "51\n", "authors": ["2083"]}
{"title": "The ontological nature of subject taxonomies\n", "abstract": " Subject based classification is an important part of information retrieval, and has a long history in libraries, where a subject taxonomy was used to determine the location of books on the shelves. We have been studying the notion of subject itself, in order to determine a formal ontology of subject for a large scale digital library card catalog system. Deep analysis reveals a lot of ambiguity regarding the usage of subjects in existing systems and terminology, and we attempt to formalize these notions into a single framework for representing it.", "num_citations": "50\n", "authors": ["2083"]}
{"title": "Augmenting abstract syntax trees for program understanding\n", "abstract": " Program understanding efforts by individual maintainers are dominated by a process known as discovery, which is characterized by low-level searches through the source code and documentation to obtain information that is important to the maintenance task. Discovery is complicated by the delocalization of information in the source code, and can consume from 40-60% of a maintainer's time. This paper presents an ontology for representing code-level knowledge based on abstract syntax trees, that was developed in the context of studying maintenance problems in a small software company. The ontology enables the utilization of automated reasoning to counter delocalization, and thus to speed up discovery.", "num_citations": "49\n", "authors": ["2083"]}
{"title": "What\u2019s in an instance\n", "abstract": " The notion of an instance is ubiquitous in knowledge representations for domain modeling. Most languages used for domain modeling offer syntactic or semantic restrictions on specific language constructs that distinguish individuals and classes in the application domain. The use, however, of instances and classes to represent domain entities has been driven by concerns that range from the strictly practical (eg the exploitation of inheritance) to the vaguely philosophical (eg intuitive notions of intension and extension). We demonstrate the importance of establishing a clear ontological distinction between instances and classes, and then show modeling scenarios where a single object may best be viewed as a class and an instance. To avoid ambiguous interpretations of such objects, it is necessary to introduce separate universes of discourse in which the same object exists in different forms. We show that a limited facility to support this notion exists in modeling languages like Smalltalk and CLOS, and argue that a more general facility should be made explicit in modeling languages.", "num_citations": "49\n", "authors": ["2083"]}
{"title": "Towards knowledge acquisition from information extraction\n", "abstract": " In our research to use information extraction to help populate the semantic web, we have encountered significant obstacles to interoperability between the technologies. We believe these obstacles to be endemic to the basic paradigms, and not quirks of the specific implementations we have worked with. In particular, we identify five dimensions of interoperability that must be addressed to successfully populate semantic web knowledge bases from information extraction systems that are suitable for reasoning. We call the task of transforming IE data into knowledge-bases knowledge integration, and briefly present a framework called KITE in which we are exploring these dimensions. Finally, we report on the initial results of an experiment in which the knowledge integration process uses the deeper semantics of OWL ontologies to improve the precision of relation extraction from text.", "num_citations": "45\n", "authors": ["2083"]}
{"title": "The object management group ontology definition metamodel\n", "abstract": " The Object Management Group (OMG) is a consortium which develops standards for various aspects of software engineering which are widely used in industry, including UML (Unified Modeling Language). With the advent of the Semantic Web movement [1] and the consequent development of ontology modeling languages like OWL by the World-Wide Web Consortium (W3C), the development of ontologies has become mainstream. Consequently, in 2003 the OMG issued a Request for Proposal for an Ontology Development Metamodel, for a Meta-Object Facility (MOF-2) metamodel intended to support:", "num_citations": "44\n", "authors": ["2083"]}
{"title": "Towards ontoclean 2.0: A framework for rigidity\n", "abstract": " The OntoClean methodology was based on a set of formal meta-properties whose semantics were specified in S5 modal logic. One of these metaproperties, Rigidity, has come under more focused scrutiny by the ontology community, and several problems with the formalization have been discussed along with several solutions. In this paper, we attempt to reconcile these results in a larger framework that exposes different kinds of rigidity, as well as two new metaproperties, actuality and permanence, which deal more specifically with the behavior of properties with respect to time and existence.", "num_citations": "40\n", "authors": ["2083"]}
{"title": "Providing question and answers with deferred type evaluation using text with limited structure\n", "abstract": " A system, method and computer program product for conducting questions and answers with deferred type evaluation based on any corpus of data. The method includes processing a query including waiting until a \u201cType\u201d(ie a descriptor) is determined AND a candidate answer is provided. Then, a search is conducted to look (search) for evidence that the candidate answer has the required Lexical Answer Type (eg, as determined by a matching function that can leverage a parser, a semantic interpreter and/or a simple pattern matcher). Prior to or during candidate answer evaluation, a process is provided for extracting and storing collections of entity-type pairs from semi-structured text documents. During QA processing and candidate answer scoring, a process is implemented to match the query LAT against the lexical type of each provided candidate answer and generate a score judging a degree of match.", "num_citations": "39\n", "authors": ["2083"]}
{"title": "Explaining conclusions from diverse knowledge sources\n", "abstract": " The ubiquitous non-semantic web includes a vast array of unstructured information such as HTML documents. The semantic web provides more structured knowledge such as hand-built ontologies and semantically aware databases. To leverage the full power of both the semantic and non-semantic portions of the web, software systems need to be able to reason over both kinds of information. Systems that use both structured and unstructured information face a significant challenge when trying to convince a user to believe their results: the sources and the kinds of reasoning that are applied to the sources are radically different in their nature and their reliability. Our work aims at explaining conclusions derived from a combination of structured and unstructured sources. We present our solution that provides an infrastructure capable of encoding justifications for conclusions in a single format. This integration\u00a0\u2026", "num_citations": "37\n", "authors": ["2083"]}
{"title": "Leveraging community-built knowledge for type coercion in question answering\n", "abstract": " Watson, the winner of the Jeopardy! challenge, is a state-of-the-art open-domain Question Answering system that tackles the fundamental issue of answer typing by using a novel type coercion (TyCor) framework, where candidate answers are initially produced without considering type information, and subsequent stages check whether the candidate can be coerced into the expected answer type. In this paper, we provide a high-level overview of the TyCor framework and discuss how it is integrated in Watson, focusing on and evaluating three TyCor components that leverage the community built semi-structured and structured knowledge resources \u2013 DBpedia (in conjunction with the YAGO ontology), Wikipedia Categories and Lists. These resources complement each other well in terms of precision and granularity of type information, and through links to Wikipedia, provide coverage for a large set of instances.", "num_citations": "36\n", "authors": ["2083"]}
{"title": "Digital libraries and web-based information systems\n", "abstract": " It has long been realized that the web could benefit from having its content understandable and available in a machine processable form, and it is widely agreed that ontologies will play a key role in providing much enabling infrastructure to achieve this goal. In this chapter we review briefly a selected history of Description Logics in web-based information systems, and the more recent developments related to OIL, DAML+ OIL and the Semantic Web. OIL and DAML+ OIL are ontology languages specifically designed for use on the web; they exploit existing web standards (XML, RDF and RDFS), adding the formal rigor of a Description Logic and the ontological primitives of object-oriented and frame-based systems.", "num_citations": "32\n", "authors": ["2083"]}
{"title": "Using ontological information in open domain type coercion\n", "abstract": " A computer-implemented system, method and program product generates answers to questions in an input query text string. The method includes determining, by a programmed processor unit, a lexical answer type (LAT) string associated with an input query; automatically obtaining a candidate answer string to the input query from a data corpus; mapping the query LAT string to a first type string in a structured resource; mapping the candidate answer string to a second type string in the structured resource; and determining if the first type string and the second type string are disjointed; and scoring the candidate answer string based on the determination of the types being disjointed wherein the structured resource includes a semantic database providing ontological content.", "num_citations": "31\n", "authors": ["2083"]}
{"title": "Extraction of semantic relations using distributional relation detection\n", "abstract": " According to an aspect, a pair of related entities that includes a first entity and a second entity is received. Distributional relations are detected between the first entity and the second entity. The detecting includes identifying two sets of entities in a corpus, the first set including the first entity and at least one other entity that is semantically similar to the first entity, and the second set including the second entity and at least one other entity that is semantically similar to the second entity. Semantic relations are detected between entities in the first set and entities in the second set. A relation classifier is trained using the pair of related entities and detected seman tic relations. The relation classifier model is applied to a new pair of entities to determine a likelihood of a semantic relation between the entities in the new pair of entities. 5 Claims, 7 Drawing Sheets", "num_citations": "27\n", "authors": ["2083"]}
{"title": "OntOWLClean: Cleaning OWL ontologies with OWL.\n", "abstract": " OWL is now very widely used for ontology development and several attempts have been made at incorporating OntoClean analysis into an OWL-based tool. I present here an OWL ontology representing the basic OntoClean distinctions, and a tool and methodology for applying it to OWL ontologies. I briefly touch on the semantic issues implied by using OWL Full syntax to characterize the OntoClean meta-properties as properties of OWL Classes, and how that was solved to employ an off-the-shelf OWL DL reasoner to check the OntoClean constraints on the taxonomy.", "num_citations": "27\n", "authors": ["2083"]}
{"title": "A comparison of hard filters and soft evidence for answer typing in watson\n", "abstract": " Questions often explicitly request a particular type of answer. One popular approach to answering natural language questions involves filtering candidate answers based on precompiled lists of instances of common answer types (e.g., countries, animals, foods, etc.). Such a strategy is poorly suited to an open domain in which there is an extremely broad range of types of answers, and the most frequently occurring types cover only a small fraction of all answers. In this paper we present an alternative approach called TyCor, that employs soft filtering of candidates using multiple strategies and sources. We find that TyCor significantly outperforms a single-source, single-strategy hard filtering approach, demonstrating both that multi-source multi-strategy outperforms a single source, single strategy, and that its fault tolerance yields significantly better performance than a hard filter.", "num_citations": "26\n", "authors": ["2083"]}
{"title": "When did that happen?\u2014linking events and relations to timestamps\n", "abstract": " We present work on linking events and fluents (ie, relations that hold for certain periods of time) to temporal information in text, which is an important enabler for many applications such as timelines and reasoning. Previous research has mainly focused on temporal links for events, and we extend that work to include fluents as well, presenting a common methodology for linking both events and relations to timestamps within the same sentence. Our approach combines tree kernels with classical feature-based learning to exploit context and achieves competitive F1-scores on event-time linking, and comparable F1-scores for fluents. Our best systems achieve F1-scores of 0.76 on events and 0.72 on fluents.", "num_citations": "26\n", "authors": ["2083"]}
{"title": "Large scale relation detection\n", "abstract": " We present a technique for reading sentences and producing sets of hypothetical relations that the sentence may be expressing. The technique uses large amounts of instance-level background knowledge about the relations in order to gather statistics on the various ways the relation may be expressed in language, and was inspired by the observation that half of the linguistic forms used to express relations occur very infrequently and are simply not considered by systems that use too few seed examples. Some very early experiments are presented that show promising results.", "num_citations": "25\n", "authors": ["2083"]}
{"title": "A formal ontology for re-use of software architecture documents\n", "abstract": " Software architecture has been established as a viable level of representation for reuse in practical software engineering efforts. The main reason for this is that an architectural view of software is sufficiently abstract to have many instantiations. Even with technologies such as CORBA and JavaBeans, which emphasize reuse of components, the realization of widespread reuse has been severely limited. While architectural reuse has been successful, it has thus far suffered from an ad-hoc semantics, and even savvy architecture practitioners are unsure precisely what is being reused. We have been engaged in research into reuse of software documents, such as design documents, statements of work, contracts, etc., that capture and reuse architectural level knowledge of software solutions. We have found that, given a sufficiently robust knowledge based tool for maintaining documents, a formal ontology or meta\u00a0\u2026", "num_citations": "25\n", "authors": ["2083"]}
{"title": "Towards a semantics for the web\n", "abstract": " The popularity and press surrounding the release of XML has created widespread interest in standards within particular communities that focus on representing content. The dream is that these standards will enable consumers and B2B systems to more accurately search information on the Web within these communities. We believe the expansiveness and diversity of the Web creates a need for a small set of standard semantic primitives that have the same meaning and interpretation across communities. Such a standard set of primitives should take into account existing efforts in ontology, and in e-commerce content standards. We are investigating existing content standards proposals for the Web, and present some basic motivations and very preliminary ideas regarding what such a standard set of semantic primitives could be. I begin with some quotes from the workshop so far, and then present some examples of work in the library and e-commerce domains, and how they might be harmonized.", "num_citations": "21\n", "authors": ["2083"]}
{"title": "Intelligent assistance for navigating the web\n", "abstract": " The Untangle Project is an attempt to apply KR&R techniques to the problem of finding information on the ever-expanding World Wide Web. There are two key enabling technologies that allow for Untangle to work: a deep ontology of the kind of information that can be found on the web. and an HTML interface that provides on the fly access to the knowledge-base. The interface allows for queries formulated in the underlying representation language (Classic), which provides a far more expressive facility for searching than is currently available in any web navigation tools. Subject Areas: Ontologies, Electronic and On-Line Information, KR&R User Interfaces, Representing large domains.", "num_citations": "21\n", "authors": ["2083"]}
{"title": "Evaluating ontological analysis\n", "abstract": " Ontologies have often been proposed as a solution to the semantic integration problem, relying on the premise that a clear, high-quality ontology can act as an interlingua in which mappings between systems can unambiguously be expressed [Smith and Welty, 2001]. While this approach has not been realized in practice, one recent development in ontology research has been the specification of a formal methodology for ontological analysis, OntoClean [Guarino and Welty, 2002], that addresses the problem of defining just what\" high quality\" is for ontologies. Following this definition and approach, a high quality foundational ontology, Dolce, is being developed [Gangemi, et al, 2002]. While OntoClean appears to be a widely accepted analysis tool in the scientific community, there is still only a little evidence that it can have impact on semantic integration*. In fact, there appears to be a significant obstacle in understanding the methodology, and even without this\" learning curve\", significant manual effort must be expended to employ the methodology to develop actual\" clean\" ontologies. Finally, there has been no clear argument that such an expenditure will pay for itself in the long run. Indeed,\" Why does it matter?\" has been the most frequent criticism of the OntoClean approach.", "num_citations": "20\n", "authors": ["2083"]}
{"title": "A View of OWL From the Field: Use-cases and Experiences.\n", "abstract": " In this paper, we describe our experiences with Semantic Web applications from the domain of life sciences, text mining and software engineering. In these domains, the state-of-the-art is limited to the use of simple taxonomies. This is partly because a sufficient set of use cases has not yet been developed to demonstrate the value of using more expressive languages (such as OWL) to add value in these domains. We are starting to catalog a set of such use cases, and we describe three concrete use cases in this paper.", "num_citations": "19\n", "authors": ["2083"]}
{"title": "Tracking information extraction from intelligence documents\n", "abstract": " We describe here some of the research underlying the development of KANI (Knowledge Associates for Novel Intelligence), a hybrid system that combines large scale information extraction (IE) with knowledge representation (KR). The combination of these two technologies raises numerous research problems, such as an evaluation and understanding of the requirements that KR puts on IE and vice-versa, the identification of useful intermediate results in the process of transforming information from unstructured and massive collections into small and precise chunks suitable for automated reasoning, and how extend traditional explanation techniques to provide the ability to trace provenance of information through these transformations.", "num_citations": "17\n", "authors": ["2083"]}
{"title": "Evaluating ontology cleaning\n", "abstract": " Ontology as a discipline of Computer Science has made many claims about its usefulness, however to date there has been very little evaluation of those claims. We present the results of an experiment using a hybrid search system with a significant knowledge-based component to measure, using precision and recall, the impact of improving the quality of an ontology on overall performance. We demonstrate that improving the ontology using OntoClean (Guarino and Welty, 2002), does positively impact performance, and that having knowledge of the search domain is more effective than domain-knowledge-free search techniques such as link analysis.", "num_citations": "17\n", "authors": ["2083"]}
{"title": "Introduction to the special issue on question answering\n", "abstract": " This special issue issue of AI Magazine presents six articles on some of the most interesting question answering systems in development today. Included are articles on Project, the Semantic Research, Watson, True Knowledge, and TextRunner (University of Washington\u2019s clever use of statistical NL techniques to answer questions across the open web).", "num_citations": "15\n", "authors": ["2083"]}
{"title": "Rif primer\n", "abstract": " This document is a primer on the Rule Interchange Format (RIF). The primer provides a practical introduction to specifying declarative rules and production rules in RIF, in particular for the RIF BLD and PRD dialects. Examples of RIF specifications are developed in a stepwise manner.", "num_citations": "15\n", "authors": ["2083"]}
{"title": "Hybridization in Question Answering Systems.\n", "abstract": " Question answering systems can benefit from the incorporation of a broad range of technologies, including natural language processing, machine learning, information retrieval, knowledge representation, and automated reasoning. We have designed an architecture that identifies the essential roles of components in a question answering system. This architecture greatly facilitates experimentation by enabling comparisons between different choices for filling the component roles, and also provides a framework for exploring hybridization of techniques\u2013that is, combining different approaches to question answering. We present results from an initial experiment that illustrate substantial performance improvement by combining statistical and linguistic approaches to question answering. We also present preliminary and encouraging results involving the incorporation of a large knowledge base.", "num_citations": "14\n", "authors": ["2083"]}
{"title": "An HTML Interface for Classic.\n", "abstract": " Several significant problems exist when applying knowledge representation systems to real problems. In particular, the obscurity of LISP, the resource consumption, garbage collection, and the single user nature of most KR systems can prevent the technology from being accepted in many non-research environments. This paper briefly presents one way to address these problems with a web interface, and outlines some of the general principles that guided the implementation of a web interface for Classic.", "num_citations": "14\n", "authors": ["2083"]}
{"title": "Long-distance time-event relation extraction\n", "abstract": " This paper proposes state-of-the-art models for time-event relation extraction (TERE). The models are specifically designed to work effectively with relations that span multiple sentences and paragraphs, ie, inter-sentence TERE. Our main idea is:(i) to build a computational representation of the context of the two target relation arguments, and (ii) to encode it as structural features in Support Vector Machines using tree kernels. Results on two data sets\u2013Machine Reading and TimeBank\u2013with 3-fold crossvalidation show that the combination of traditional feature vectors and the new structural features improves on the state of the art for inter-sentence TERE by about 20%, achieving a 30.2 F1 score on intersentence TERE alone, and 47.2 F1 for all TERE (inter and intra sentence combined).", "num_citations": "13\n", "authors": ["2083"]}
{"title": "Context slices: representing contexts in OWL\n", "abstract": " This ontology pattern can be used to represent and reason about contextualized statements using standard OWL dialects. The simple idea is to bundle the notion of context into certain nodes in the graph, rather than the more typical treatment of contexts as a property of the statements themselves.", "num_citations": "13\n", "authors": ["2083"]}
{"title": "Type evaluation in a question-answering system\n", "abstract": " A system and method for automatically mapping LATs and candidate answers to multiple taxonomies without a need to merge these taxonomies. The method includes using a syntactic analysis of a corpus to extract all type instances of the LAT. The extracted instances are then mapped to a given taxonomy and clustered in a set of supertypes. Each supertype receives a score based on the coverage of LAT instances in the corpus. The method includes mapping the candidate answer to the same taxonomy to determine if the candidate answer is an instance of a significant supertype. Then the score of a candidate answer is obtained by aggregating or taking a maximum of the score of the matched significant supertypes. This score evaluates the type match between the LAT and candidate answer for a taxonomy. Multiple taxonomies can be used to increase the chance of LAT and candidate answer mapping.", "num_citations": "11\n", "authors": ["2083"]}
{"title": "Coreference resolution on rdf graphs generated from information extraction: first results\n", "abstract": " In our research on the use of information extraction to help populate the semantic web, we have encountered significant obstacles to interoperability. One such obstacle is cross-document coreference resolution. In this paper we describe an effort to improve coreference resolution on RDF graphs generated by text analytics. In addition to driving knowledge-base population, our goal is to demonstrate that successfully combining semantic web and natural language processing technologies can offer advantages over either in isolation, and motivates overcoming the obstacles to interoperability. We present some early results that show improvement of coreference resolution using graph-matching algorithms over RDF.", "num_citations": "11\n", "authors": ["2083"]}
{"title": "Encoding extraction as inferences.\n", "abstract": " The analysis of natural-language text involves many different kinds of processes that might be described in multiple ways. One way to describe these processes is in terms of the semantics of their requirements and results. Such a description makes it possible to view these processes as analogous to inference rules in a theorem-proving system. This analogy is useful for metacognition because there is existing theory and infrastructure for manipulating inference rules. This paper presents a representational framework for text analysis processes. We describe a taxonomy of text extraction tasks that we have represented as inference rules. We also describe a working system that encodes the behavior of text analysis components as a graph of inferences. This representation is used to present browsable explanations of text extraction; in future work, we expect to perform additional automated reasoning over this encoding of text analysis processes.", "num_citations": "11\n", "authors": ["2083"]}
{"title": "Ontology-driven conceptual modeling\n", "abstract": " \u2022 If all instances of a property \u03c6 are wholes under the same relation, \u03c6 carries unity (+ U)\u2022 When at least one instance of \u03c6 is not a whole, or when two instances of \u03c6 are wholes under different relations, \u03c6 does not carry unity (-U)\u2022 When no instance of \u03c6 is a whole, \u03c6 carries anti-unity (~ U)", "num_citations": "11\n", "authors": ["2083"]}
{"title": "Towards an epistemology for software representations\n", "abstract": " The KBSE community is actively engaged in finding ways to represent software and the activities that relate to various stages in its lifecycle. While the wealth of modeling activities have, necessarily, been founded on first order logic based representations, the paper reports on research into software information systems that has found the domain of software knowledge to be inherently second order. A facility for accurately representing second order constructs such as are found in the software domain is also presented.", "num_citations": "11\n", "authors": ["2083"]}
{"title": "Using Part-Of relations for discovering causality\n", "abstract": " Historically, causal markers, syntactic structures and connectives have been the sole identifying features for automatically extracting causal relations in natural language discourse. However various connectives such as \u201cand,\u201d prepositions such as \u201cas\u201d and other syntactic structures are highly ambiguous in nature, and it is clear that one cannot solely rely on lexico-syntactic markers for detection of causal phenomenon in discourse. This paper introduces the theory of granularity and describes different approaches to identify granularity in natural language. As causality is often granular in nature, we use granularity relations to discover and infer the presence of causal relations in text. We compare this with causal relations identified using just causal markers. We achieve a precision of 0.91 and a recall of 0.79 using granularity for causal relation detection, as compared to a precision of 0.79 and a recall of 0.44 using pure causal markers for causality detection.", "num_citations": "10\n", "authors": ["2083"]}
{"title": "Untangle: a new ontology for card catalog systems\n", "abstract": " The ontology used by most card catalog and bibliographic systems is based on a now outdated assumption that users of the systems would be looking for books on shelves, and therefore only books were first-class objects, with people, organizations, etc. as simple attributes. This limited the ability of a user to browse. A new ontology for card catalog systems is proposed that suggests that persons, organizations, conferences, etc., should be first-class objects with attributes and relations of their own, creating a rich space of background information that helps users find what they are looking for. This new ontology has been implemented in a knowledge-based system called Untangle, which demonstrates two key advantages of this rich information space: it enables automatic augmentation of the data through reasoning, and it enables a new paradigm for search that combines querying and browsing.", "num_citations": "10\n", "authors": ["2083"]}
{"title": "Instances and classes in software engineering\n", "abstract": " Within the past 10 years, object-ori-ented analysis and modeling has led to significant improvements in productivity in the earlier phases of software engineering (such as requirements, design, and implementation). These improvements have been facilitated in part by the emergence of tools to support the process and languages such as UML for supporting partly standardized diagrams.The foundations of object-oriented modeling derive from philosophy through artificial intelligence to software engineering. In philosophy, the fields of epistemology and ontology have been considering for centuries what is needed to represent things in the world and what it means to do so. Within the past century, the development of first-order logic and, later, computers has significantly sped up and formalized many of these ideas. Knowledge representation (KR) is one of the original subfields of AI, and the study of modeling\u00a0\u2026", "num_citations": "10\n", "authors": ["2083"]}
{"title": "DLs for DLs: Description Logics for Digital Libraries.\n", "abstract": " We are working on several aspects of using description logics for digital libraries. Our main goal is to enable robust retrieval of information stored in a digital library. We have found that the use of description logics has also served to assist in ensuring the integrity of the data. In addtion, the recent release of XML has made us aware of an enormous opportunity to leverage description logic technology, in reality against the widely recognized problems with web searching.", "num_citations": "10\n", "authors": ["2083"]}
{"title": "Representing TEI documents in the CLASSIC knowledge representation system\n", "abstract": " The development of the Text Encoding Initiative (TEI) Guidelines enables the encoding of a wide variety of textual phenomena to any desired level of fine-grainedness and complexity, relevant to a broad range of applications and scholary interests. The ability to encode complex phenomena has, in turn, created a demand for adequate means to manipulate the text once it has been marked up according to the user's interests and needs. One obvious and immediate need for users of the TEI scheme is a flexible means to query and retrieve from an encoded text, which does not require deep knowledge of the structure of the text by the user. There has been some work in this area [1], although so far most systems require that the user know the structure of the document as defined by the DTD.Beyond the need to query and retrieve based on tags which exist in a TEI document, a means to manipulate and query classes of objects is also desirable. The TEI DTD uses SGML entity definitions to create\" classes\" of elements and attributes, in particular, for groups of elements with common structural properties (eg, all elements that can appear between paragraphs), groups of attributes which apply to certain classes of elements (eg, attributes for pointer elements), etc. In addition to grouping together elements and attributes with common structural properties, the definition of such classes recognizes common semantic properties among elements and attributes. However, the SGML entity definition mechanism provides only for string substitution within the DTD itself, thereby enabling easy reference to these classes in later element definitions; the common\u00a0\u2026", "num_citations": "10\n", "authors": ["2083"]}
{"title": "A Description Logic-based configurator on the Web\n", "abstract": " Description logics have a history of success in configuration applications in major companies including AT&T (mentioned in this paper) and the Ford Motor Company. While we have produced a number of commercial configurators, we find a demonstration application to be the best expository tool for describing how description logics can be leveraged effectively in tasks such as configuration.", "num_citations": "9\n", "authors": ["2083"]}
{"title": "What is fair? exploring pareto-efficiency for fairness constrained classifiers\n", "abstract": " The potential for learned models to amplify existing societal biases has been broadly recognized. Fairness-aware classifier constraints, which apply equality metrics of performance across subgroups defined on sensitive attributes such as race and gender, seek to rectify inequity but can yield non-uniform degradation in performance for skewed datasets. In certain domains, imbalanced degradation of performance can yield another form of unintentional bias. In the spirit of constructing fairness-aware algorithms as societal imperative, we explore an alternative: Pareto-Efficient Fairness (PEF). Theoretically, we prove that PEF identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane, maximizing multiple subgroup accuracy. Empirically we demonstrate that PEF outperforms by achieving Pareto levels in accuracy for all subgroups compared to strict fairness constraints in several UCI datasets.", "num_citations": "8\n", "authors": ["2083"]}
{"title": "Extraction of inference rules from heterogeneous graphs\n", "abstract": " According to an aspect, a heterogeneous graph in a data store is accessed. The heterogeneous graph includes a plurality of nodes having a plurality of node types. The nodes are connected by edges having a plurality of relation types. One or more intermediary graphs are created based on the heterogeneous graph. The intermediary graphs include intermediary nodes that are the relation types of the edges of the heterogeneous graph and include intermediary links between the intermediary nodes based on shared instances of the nodes between relation types in the heterogeneous graph. The intermediary graphs are traversed to find sets of relations based on intermediary links according to a template. An inference rule is extracted from the heterogeneous graph based on finding sets of relations in the intermediary graphs. The inference rule defines an inferred relation type between at least two of the nodes of the\u00a0\u2026", "num_citations": "8\n", "authors": ["2083"]}
{"title": "Using granularity concepts for discovering causal relations\n", "abstract": " Historically, causal markers, syntactic structures and connectives have been the sole identifying features for automatically extracting causal relations in natural language discourse. However various connectives such as \u201cand\u201d, prepositions such as \u201cas\u201d and other syntactic structures are highly ambiguous in nature, and it is clear that one cannot solely rely on lexico-syntactic markers for detection of causal phenomenon in discourse. This paper introduces the theory of granularity and describes different approaches to identify granularity in natural language. As causality is often granular in nature (Mazlack 2004), we use granularity relations to discover and infer the presence of causal relations in text. We compare this with causal relations identified using just causal markers. We achieve a precision of 0.91 and a recall of 0.79 using granularity for causal relation detection, as compared to a precision of 0.79 and a recall of 0.44 using pure causal markers for causality detection.", "num_citations": "8\n", "authors": ["2083"]}
{"title": "Endurantism and perdurantism: An ongoing debate\n", "abstract": " The following is a dialog about how to model entities and change with respect to time. These issues have practical application in temporal reasoning, natural language understanding and robot world modeling. This discussion assumes a context of the need to formalize these models in logic, either first order logic (FOL), or possibly FOL with some extensions.The two broad positions explored in this debate are a model in which entities that are situated in space and time are modeled as persisting over time and undergoing change without losing their identity. This position has been termed endurantism or \u201c3-D\u201d modeling. The opposing position advocates a model in which entities do not change. Unchanging entities are situated in space and time and are related to their different manifestations at different points in time. These temporal slices of an entity, when taken together, form a history of the existence of an object\u00a0\u2026", "num_citations": "8\n", "authors": ["2083"]}
{"title": "Towards Ontology-based harmonization of Web content standards\n", "abstract": " The popularity and press surrounding the release of XML has created widespread interest in standards within particular communities that focus on representing content. The dream is that these standards will enable consumers and B2B systems to more accurately search information on the Web within these communities. We believe the expansiveness and diversity of the Web creates a need for a small set of standard semantic primitives that have the same meaning and interpretation across communities. Such a standard set of primitives should take into account existing efforts in ontology, and in e-commerce content standards. We are investigating existing content standards proposals for the Web, and present here a large, but by no means complete, list of these standards efforts classified by their ontological sophistication and their intent. We then propose some very preliminary notions of how these\u00a0\u2026", "num_citations": "8\n", "authors": ["2083"]}
{"title": "Artificial intelligence and software engineering: breaking the toy mold\n", "abstract": " The application of AI techniques to software engineering has suffered, from the perspective of practising software engineers, due to a tradition of testing ideas and theories on small, toy domains. At the IJCAI-95 Workshop on AI and Software Engineering we focused onthis issue, and here we discuss some of the results of that workshop, identifying the major weaknesses in AI&SE research, and offer some insight into the future of the field.", "num_citations": "8\n", "authors": ["2083"]}
{"title": "Obtaining Formal Knowledge from Informal Text Analysis\n", "abstract": " Populating formal knowledge-bases from natural-language text is a long-standing objective in computer science. Recent advancements in both ontology research and information extraction research are making this objective increasingly obtainable. However, there are still serious obstacles to performing automated reasoning over the contents of text documents. This paper focuses on one of those obstacles: differences between the formal ontologies used by reasoning systems and the informal ontologies used by extraction systems. We describe a framework for automating translation from extracted information to formal knowledge, and we describe a complex, implemented system that uses this framework. We also describe results from this system applied to a moderately large (approximately 75 MB) text corpus.", "num_citations": "7\n", "authors": ["2083"]}
{"title": "Medical Concept Resolution.\n", "abstract": " In this paper, we present a problem that we refer to as Medical Concept Resolution for finding concept identifiers in a large knowledge base, given medical terms mentioned in a text. We define the problem with its unique features and novel algorithms to address it. We compare performance to MetaMap and find distinct and complementary behavior.", "num_citations": "6\n", "authors": ["2083"]}
{"title": "The ai behind watson\u2014the technical article\n", "abstract": " IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV quiz show, Jeopardy. The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After three years of intense research and development by a core team of about 20 researchers, Watson is performing at human expert levels in terms of precision, confidence, and speed at the Jeopardy quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that can be used as a foundation for combining, deploying, evaluating, and advancing a wide range of algorithmic techniques to rapidly advance the field of question answering (QA).", "num_citations": "6\n", "authors": ["2083"]}
{"title": "Combining different type coercion components for deferred type evaluation\n", "abstract": " In a method of answering questions, a question is received, a question LAT is determined, and a candidate answer to the question is identified. Preliminary types for the candidate answer are determined using first components to produce the preliminary types. Each of the first components produces a preliminary type using different methods. A first type-score representing a degree of match between the preliminary type and the question LAT is produced. Each preliminary type and each first type-score is evaluated using second components. Each of the second components produces a second score based on a combination of the first type-score and a measure of degree that the preliminary type matches the question LAT. The second components use different methods to produce the second score. A final score representing a degree of confidence that the candidate answer matches the question LAT is calculated\u00a0\u2026", "num_citations": "5\n", "authors": ["2083"]}
{"title": "Overview of component services for knowledge integration in uima (aka suki)\n", "abstract": " This white paper describes ongoing research work in the Semantic Analysis and Integration department at IBM Watson Research Center related to bridging unstructured information analysis with formal knowledge representations.", "num_citations": "5\n", "authors": ["2083"]}
{"title": "AAAI 2002 Workshops\n", "abstract": " The Association for the Advancement of Artificial Intelligence (AAAI) presented the AAAI-02 Workshop Program on Sunday and Monday, 28-29 July 2002 at the Shaw Convention Center in Edmonton, Alberta, Canada. The AAAI-02 workshop program included 18 workshops covering a wide range of topics in AI. The workshops were Agent-Based Technologies for B2B Electronic-Commerce; Automation as a Caregiver: The Role of Intelligent Technology in Elder Care; Autonomy, Delegation, and Control: From Interagent to Groups; Coalition Formation in Dynamic Multiagent Environments; Cognitive Robotics; Game-Theoretic and Decision-Theoretic Agents; Intelligent Service Integration; Intelligent Situation-Aware Media and Presentations; Meaning Negotiation; Multiagent Modeling and Simulation of Economic Systems; Ontologies and the Semantic Web; Planning with and for Multiagent Systems; Preferences in AI and CP: Symbolic Approaches; Probabilistic Approaches in Search; Real-Time Decision Support and Diagnosis Systems; Semantic Web Meets Language Resources; and Spatial and Temporal Reasoning.", "num_citations": "4\n", "authors": ["2083"]}
{"title": "Pareto-Efficient Fairness for Skewed Subgroup Data\n", "abstract": " As awareness of the potential for learned models to amplify existing societal biases increases, the field of ML fairness has developed mitigation techniques. A prevalent method applies constraints, including equality of performance, with respect to subgroups defined over the intersection of sensitive attributes such as race and gender. Enforcing such constraints when the subgroup populations are considerably skewed with respect to a target can lead to unintentional degradation in performance, without benefiting any individual subgroup, counter to the United Nations Sustainable Development goals of reducing inequalities and promoting growth. In order to avoid such performance degradation while ensuring equitable treatment to all groups, we propose Pareto-Efficient Fairness (PEF), which identifies the operating point on the Pareto curve of subgroup performances closest to the fairness hyperplane. Specifically, PEF finds a Pareto Optimal point which maximizes multiple subgroup accuracy measures. The algorithm* scalarizes* using the adaptive weighted metric norm by iteratively searching the Pareto region of all models enforcing the fairness constraint. PEF is backed by strong theoretical results on discoverability and provides domain practitioners finer control in navigating both convex and non-convex accuracyfairness trade-offs. Empirically, we show that PEF increases performance of all subgroups in skewed synthetic data and UCI datasets.", "num_citations": "3\n", "authors": ["2083"]}
{"title": "Semantic web and best practice in watson\n", "abstract": " IBM's revolutionary Watson system has successfully beaten human Jeopardy champions, and is now being extended and used in other domains, such as healthcare question answering, and financial data analysis. It is a common misconception that Watson is a through-and-through formally semantic system, which translates questions into formal language queries, and returns answers by executing those queries over a large knowledge base. In actual fact, Watson uses a variety of technologies to produce candidate answers to each question, and semantic technologies are primarily used in the subsequent candidate answer ranking components. In particular, linked data sources are used to provide typing evidence for candidate answers, but also several other answer ranking components rely more or less on semantics and linked data.", "num_citations": "3\n", "authors": ["2083"]}
{"title": "Representing Classes As Property Values on the Semantic Web\n", "abstract": " This document addresses the issue of using classes as property values in OWL and RDF Schema. It is often convenient to put a class (eg, Animal) as a property value (eg, topic or book subject) when building an ontology. While OWL Full and RDF Schema do not put any restriction on using classes as property values, in OWL DL and OWL Lite most properties cannot have classes as their values. We illustrate the direct approach for representing classes as property values in OWL-Full and RDF Schema. We present various alternative mechanisms for representing the required information in OWL DL and OWL Lite. For each approach, we discuss various considerations that the users should keep in mind when choosing the best approach for their purposes.", "num_citations": "3\n", "authors": ["2083"]}
{"title": "COALA: A Tool for Inter-document Coreference Resolution Evaluation.\n", "abstract": " A significant obstacle to scientific progress in machine reading is an objective evaluation method. Precision and recall, while for the most part quantitative, are often measured with respect to some \u201cgold standard\u201d or \u201cground truth\u201d\u2013itself typically a human annotated corpus. For more complex tasks, such as interdocument coreference resolution, or open ended tasks such as machine reading, relying on a ground truth is often (if not always) impractical. Yet a data-driven approach still requires techniques for evaluation. To address this, we present here a new approach to evaluation of linguistic analysis implemented in a tool we have developed called COALA. The approach basically requires establishing a baseline system that produces some results, and evaluations are performed by incrementally changing that system and comparing the results manually. In order to reduce the load on the human evaluator, our tool implements basically an intelligent and task-specific \u201cdiff\u201d between the two results, allowing the evaluator to focus only on the changes and evaluate them.", "num_citations": "3\n", "authors": ["2083"]}
{"title": "Embedding Semantic Taxonomies\n", "abstract": " A common step in developing an understanding of a vertical domain, eg shopping, dining, movies, medicine, etc., is curating a taxonomy of categories specific to the domain. These human created artifacts have been the subject of research in embeddings that attempt to encode aspects of the partial ordering property of taxonomies. We compare Box Embeddings, a natural containment representation of category taxonomies, to partial-order embeddings and a baseline Bayes Net, in the context of representing the Medical Subject Headings (MeSH) taxonomy given a set of 300K PubMed articles with subject labels from MeSH. We deeply explore the experimental properties of training box embeddings, including preparation of the training data, sampling ratios and class balance, initialization strategies, and propose a fix to the original box objective. We then present first results in using these techniques for representing a bipartite learning problem (ie collaborative filtering) in the presence of taxonomic relations within each partition, inferring disease (anatomical) locations from their use as subject labels in journal articles. Our box model substantially outperforms all baselines for taxonomic reconstruction and bipartite relationship experiments. This performance improvement is observed both in overall accuracy and the weighted spread by true taxonomic depth.", "num_citations": "2\n", "authors": ["2083"]}
{"title": "Discovering user bias in ordinal voting systems\n", "abstract": " Crowdsourcing systems increasingly rely on users to provide more subjective ground truth for intelligent systems-eg ratings, aspect of quality and perspectives on how expensive or lively a place feels, etc. We focus on the ubiquitous implementation of online user ordinal voting (eg 1-5, 1 star-4 stars) on some aspect of an entity, to extract a relative truth, measured by a selected metric such as vote plurality or mean. We argue that this methodology can aggregate results that yield little information to the end user. In particular, ordinal user rankings often converge to a indistinguishable rating. This is demonstrated by the trend in certain cities for the majority of restaurants to all have a 4 star rating. Similarly, the rating of an establishment can be significantly affected by a few users [10]. User bias in voting is not spam, but rather a preference that can be harnessed to provide more information to users. We explore notions of\u00a0\u2026", "num_citations": "2\n", "authors": ["2083"]}
{"title": "Taxonomy Embeddings on PubMed Article Subject Headings.\n", "abstract": " Machine learning approaches for hierarchical partial-orders, such as taxonomies, are of increasing interest in the research community, though practical applications have not yet emerged. The basic intuition of hierarchical embeddings is that some signal from taxonomic knowledge can be harnessed in broader machine learning problems; when we learn similarity of words using word embeddings, the similarity of lion and tiger are indistinguishable from the similarity of lion and animal. The ability to tease apart these two kinds of similarities in a machine learning setting yields improvements in quality as well as enabling the exploitation of the numerous human-curated taxonomies available across domains, while at the same time improving upon known taxonomic organization problems, such as partial or conditional membership. We explore some of the practical problems in learning taxonomies using Bayesian Networks, partial order embeddings, and box lattice embeddings, where box containment represents category containment. Using open data from PubMed articles with human assigned MeSH labels, we investigate the impact of taxonomic information, negative sampling, instance sampling, and objective functions to improve performance on the taxonomy learning problem. We discovered a particular problem for learning box embeddings for taxonomies we called the box crossing problem, and developed strategies to overcome it. Finally we make some initial contributions to using taxonomy embeddings to improve another learning problem: inferring disease (anatomical) locations from their use as subject labels in journal articles. In most\u00a0\u2026", "num_citations": "2\n", "authors": ["2083"]}
{"title": "KR Proceedings, Tenth International Conference on Principles of Knowledge Representation and Reasoning\n", "abstract": " Proceedings, Tenth International Conference on Principles of Knowledge Representation and Reasoning Page 1 2006 Page 2 Proceedings, Tenth International Conference on Principles of Knowledge Representation and Reasoning Edited by Patrick Doherty, John Mylopoulos, and Christopher Welty Menlo Park, California AAAI Press Page 3 Copyright \u00a9 2006, American Association for Artificial Intelligence AAAI Press 445 Burgess Drive Menlo Park, California 94025 All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher. ISBN 978-1-57735-271-6 Printed on acid free paper. Manufactured in the United States of America The Tenth International Conference on Principles of Knowledge Representation and Reasoning was held in the Lake \u2026", "num_citations": "2\n", "authors": ["2083"]}
{"title": "Semantic Web Ontologies\n", "abstract": " Semantic Web Ontologies Page 1 Semantic Web Ontologies Chris Welty IBM Research (also W3C Semantic Web Best Practices WG) Page 2 RDF to RDFS \u2022 RDF \u2013 triples as relations \u2022 <Chris Father William> \u2013 Interpreted as a graph \u2013 What do they mean? \u2022 <Person Father Person> \u2022 RDFS \u2013 Data types: class \u2013 Specialization: subclass Chris William Father Page 3 The groundwork \u2022 RDF + RDFS driven by multiple influences \u2013 Database \u2013 Knowledge Representation \u2013 Un/Semi Structured data \u2013 ERP \u2013 \u2026 \u2022 Provides a foundation to expand into these areas Page 4 OWL \u2022 Adds entailments to RDFS \u2013 IF Author-of(x,y) AND Author-of(z,y) THEN Collaborator(x,z) \u2022 Three levels \u2013 Lite: easier to implement Description Logic \u2013 DL: based on latest Description Logic \u2013 Full: First order with reified predicates Page 5 Entailment \u2022 A \u2192 B (\u201cA implies B\u201d) \u2013 \u2264x P(x) \u2192 Q(x) \u2227 R(x) \u2013 P(A) \u2192 Q(A) \u2227 R(A) \u2022 A \u00d1 B (\u201cA entails B\u201d) \u2013 More \u2026", "num_citations": "2\n", "authors": ["2083"]}
{"title": "Ontology Maintenance: Support\n", "abstract": " Ontology Maintenance Support Page 1 Ontology Maintenance Support Text, Tools, and Theories Chris Welty IBM Research Page 2 Outline \u2022 Opening joke \u2022 Motivation \u2022 Maintenance \u2022 Support \u2013 Tools \u2013 Theories \u2013 Text Analysis Page 3 Motivation \u2022 Given: Ontologies matter \u2013 Does quality matter? Page 4 Does quality matter? \u2022 Good quality ontologies cost more \u2013 Coverage, correctness, richness, commitment [Kashyap, 2003] \u2013 Organization, meta-level consistency [Guarino & Welty, 2000] [Rector, 2002] \u2013 Required for some applications \u2022 Improvements in quality can improve performance [Welty, et al, 2004] \u2013 18% f-improvement in search \u2013 Cleanup cost ~1mw/3000 classes \u2013 BUT \u2026 low quality ontology still improved base Page 5 Motivation \u2022 Given: Ontologies matter \u2013 Does quality matter? Sometimes \u2022 Problem: How to create them \u2022 Bigger problem: how to maintain them \u2013 From SE: 80% of the cost is maintenance [\u2026", "num_citations": "2\n", "authors": ["2083"]}
{"title": "The AI Bookie: Place Your Bets: Adversarial Collaboration for Scientific Advancement\n", "abstract": " The AI Bookie column documents highlights from AI Bets, an online forum for the creation of adjudicatable predictions and bets about the future of AI. While it is easy to make a prediction about the future, this forum was created to help researchers craft predictions whose accuracy can be clearly and unambiguously judged when they come due. The bets will be documented on line, and regularly in this publication in The AI Bookie. We encourage bets that are rigorously and scientifically argued. We discourage bets that are too general to be evaluated, or too specific to an institution or individual. The goal is not to continue to feed the media frenzy and pundit predictions about AI, but rather to curate and promote bets whose outcomes will provide useful feedback to the scientific community", "num_citations": "1\n", "authors": ["2083"]}
{"title": "The Semantics of Multiple Annotations\n", "abstract": " In the context of a project to populate knowledge-bases from large-scale unstructured information, we have encountered numerous problems integrating the extracted information and the knowledge-base, due to mismatches in semantics between the two. Semantic integration in general is quite hard, however we have found that many of the integration problems stem from unclear or ambiguous semantics on the extraction side. We have begun to catalog and document these ambiguities and report here on one in particular: the use and intended meaning of multiple annotations on a span of text. Multiple annotations are a representation structure used in at least seven semantically distinct ways by existing information extraction systems. This overloading makes the representation ambiguous, and creates an obstacle for integration. We focus on these different semantic interpretations and propose a more expressive representation that allows their differences to be expressed.", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Backtracking: the demise of \u201c!\u201d\n", "abstract": " When I was a kid in school learning about English grammar, spelling, and punctuation, I remember being taught about the importance of the exclamation point. This are, granted, one of the feuw things I does remember. Together with the question mark, these were the only two punctuation marks that could change the meaning of a sentence; more so for the exclamation point, because it was a stress mark. It was strong punctuation. It meant the previous sentence was important, and if uttered, should be uttered loud and clear.My eighth grade English teacher was an enormous hulk of a man, a Korean War veteran who broke up a fight between two middle-school combatants by picking them up, tucking one under each arm, and carrying them to the principal's office. He read manly books by Hemingway and watched John Wayne movies, and he believed you should consider pounding your fist on some flat surface when\u00a0\u2026", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Formal Ontology in Information Systems (FOIS)\n", "abstract": " Researchers in areas such as artificial intelligence, formal and computational linguistics, biomedical informatics, conceptual modeling, knowledge engineering and information retrieval have come to realise that a solid foundation for their research calls for serious work in ontology, understood as a general theory of the types of entities and relations that make up their respective domains of inquiry. In all these areas, attention is now being focused on the content of information rather than on just the formats and languages used to represent information. The clearest example of this development is provided by the many initiatives growing up around the project of the Semantic Web. And, as the need for integrating research in these different fields arises, so does the realisation that strong principles for building well-founded ontologies might provide significant advantages over ad hoc, case-based solutions. The tools of formal ontology address precisely these needs, but a real effort is required in order to apply such philosophical tools to the domain of information systems. Reciprocally, research in the information sciences raises specific ontological questions which call for further philosophical investigations. The purpose of FOIS is to provide a forum for genuine interdisciplinary exchange in the spirit of a unified effort towards solving the problems of ontology, with an eye to both theoretical issues and concrete applications. This book contains a wide range of areas, all of which are important to the development of formal ontologies.", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Backtracking: Garbage collection\n", "abstract": " Garbage Collection (GC). Ten years ago, that would have been unambiguous and clearly understandable as an attribute of Lisp. Lisp was initially small and elegant, and it became the de facto language of Artificial Intelligence. Developed in large part at MIT (see John McCarthy's Lisp 1.5 Programmers Manual, MIT Press, 1965), at Xerox PARC (Warren Teitelman, et al., Interlisp Reference Manual, Xerox PARC Tech Report, 1978) and by Guy Steele Jr.(see Common LISP, The Language, Digital Press, 1984), Lisp ran ever so nicely with a wonderful development and debugging environment on such special-purpose (and expensive) machines as Symbolics and LMI. Now it runs on typical UNIX boxes and PCs. But where is it today and what have we learned along the way? Lisp differs so much from C++ and its lack, among others things, of GC. Java doesn't come with Emacs but it does offer Java Doc. But let's not\u00a0\u2026", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Conferences. Report on the 1998 International Workshop on Description Logics (DL'98)\n", "abstract": " The main effort of research in knowledge representation is providing theories and systems for expressing structured knowledge and for accessing and reasoning with it in a principled way. Description Logics are a powerful class of knowledge representation languages which are presently considered to be the most important formalism unifying and giving a logical basis to the well known traditions of Frame-based systems, Semantic Networks and KL-ONE-like languages, Object-Oriented representations, Semantic data models, and Type systems. The 1998 International Workshop on Description Logics (DL\u201998) continued the tradition of international workshops devoted to discussing developments and applications of knowledge representation formalisms based on Description Logics. DL\u201998 was held in Trento, Italy, June 6\u20138 1998, immediately after KR\u201998 (Sixth International Conference on Principles of Knowledge\u00a0\u2026", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Towards an Ontology for Library Modalities\n", "abstract": " The AXIOM project is a collaboration between researchers in knowledge representation (KR), markup languages such as XML, linguistic analysis, text encoding, databases, humanities computing, and other related fields, whose goal is to develop technologies that will enable a large scale digital library. The central theme of the project is the exploitation of\" intelligent tools\" and\" intelligent texts.\" The project also includes several large text encoding projects associated with existing libraries that have been diligently encoding\" humanities\" data over the past ten years.", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Teaching Bottom-up AI from the Top Down.\n", "abstract": " For many reasons, it is desirable to use robots in courses such as introductory computer science, artificial intelligence, and cognitive science, yet the knowledge normally required by students to make effective use of these tools is often prohibitive in such courses with well established curricula. We have developed a user interface that allows students with no prior experience or training in robotics to experiment with behavior networks in real robots, and then brings them down through the software and then the hardware involved. The interface is still in the early stages of development, and has been tested somewhat in a Cognitive Science course, but more widespread use is expected.", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Desert Island Column\n", "abstract": " Day I. I woke to blinding sunlight. I moved my hand to shield my eyes and was immediately made aware of excruciating pain all over my body. I somehow crawled, without even realizing what I was doing, up the beach and into the shade. I collapsed again in some soft, cool leaves, and had a chance to examine myself. In several places where my clothing had torn, my skin was extremely sunburned. Great, I thought, I\u2019ll have skin cancer now before I\u2019m fifty. I sank back into the leaves with a big sigh, and wondered if perhaps I might not have a more pressing concern. Slowly, very slowly, the events of the previous night began to unfold in my recently activated consciousness.", "num_citations": "1\n", "authors": ["2083"]}
{"title": "Backtracking\n", "abstract": " Even if you are a limited artificial intelligence agent, you have probably noticed that this issue of the Bulletin is devoted to the special topic of Agents. Anyone knowledgeable about the history of AI cannot help but recognize that yet another neat little technology is being hyped nearly to death, and anyone knowledgeable about \"agents\" can not help but recognize that, as an artificial intelligence technology, there is typically very little actual intelligence involved. In some sense, Agents can be defined as small scale AI that works. This was particularly evident at the First International Conference on Autonomous Agents (Agents-97) in Marina Del Rey this past February. There is a lot of activity and real research going on related to Agents that is fascinating and occasionally entertaining. Agents-97 attendees certainly noticed the most popular presentations were those given by or relating to the entertainment industry.", "num_citations": "1\n", "authors": ["2083"]}