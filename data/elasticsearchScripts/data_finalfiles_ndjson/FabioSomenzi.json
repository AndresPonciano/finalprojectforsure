{"title": "Algebric decision diagrams and their applications\n", "abstract": " In this paper we present theory and experimental results on Algebraic Decision Diagrams. These diagrams extend BDDs by allowing values from an arbitrary finite domain to be associated with the terminal nodes of the diagram. We present a treatment founded in Boolean algebras and discuss algorithms and results in several areas of application: Matrix multiplication, shortest path algorithms, and direct methods for numerical linear algebra. Although we report an essentially negative result for Gaussian elimination per se, we propose a modified form of ADDs which appears to circumvent the difficulties in some cases. We discuss the relevance of our findings and point to directions for future work.", "num_citations": "1081\n", "authors": ["1596"]}
{"title": "Logic synthesis and verification algorithms\n", "abstract": " Logic Synthesis and Verification Algorithms is a textbook designed for courses on VLSI Logic Synthesis and Verification, Design Automation, CAD and advanced level discrete mathematics. It also serves as a basic reference work in design automation for both professionals and students. Logic Synthesis and Verification Algorithms is about the theoretical underpinnings of VLSI (Very Large Scale Integrated Circuits). It combines and integrates modern developments in logic synthesis and formal verification with the more traditional matter of Switching and Finite Automata Theory. The book also provides background material on Boolean algebra and discrete mathematics. A unique feature of this text is the large collection of solved problems. Throughout the text the algorithms covered are the subject of one or more problems based on the use of available synthesis programs.", "num_citations": "745\n", "authors": ["1596"]}
{"title": "CUDD: CU decision diagram package release 2.3. 1\n", "abstract": " CiNii \u8ad6\u6587 - CUDD : CU Decision Diagram Package Release 2.3.1 CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092 \u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 CUDD : CU Decision Diagram Package Release 2.3.1 SOMENZI Fabio \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI Fabio \u53ce\u9332\u520a\u884c\u7269 http://vlsi.colorado.edu/fabio/CUDD/cuddIntro.html http://vlsi.colorado.edu/fabio/CUDD/cuddIntro.html \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u5c02\u7528 \u30d7\u30ed\u30bb\u30c3\u30b5\u7528\u547d\u4ee4\u30bb\u30c3\u30c8\u306e\u81ea\u52d5\u751f\u6210\u624b\u6cd5\u306e\u63d0\u6848\u3068\u5b9f\u88c5 \u5c0f\u5cf6 \u6176\u4e45 , \u9ed2\u7fbd \u6bc5 , \u85e4\u7530 \u660c\u5b8f \u60c5\u5831\u51e6\u7406 \u5b66\u4f1a\u7814\u7a76\u5831\u544a. SLDM, [\u30b7\u30b9\u30c6\u30e0LSI\u8a2d\u8a08\u6280\u8853] 106, 49-54, 2002-05-23 \u53c2\u8003\u6587\u732e5\u4ef6 Tweet \u5404\u7a2e \u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10012619719 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 \u2026", "num_citations": "705\n", "authors": ["1596"]}
{"title": "High-level power modeling, estimation, and optimization\n", "abstract": " Silicon area, performance, and testability have been, so far, the major design constraints to be met during the development of digital very-large-scale-integration (VLSI) systems. In recent years, however, things have changed; increasingly, power has been given weight comparable to the other design parameters. This is primarily due to the remarkable success of personal computing devices and wireless communication systems, which demand high-speed computations with low power consumption. In addition, there exists a strong pressure for manufacturers of high-end products to keep power under control, due to the increased costs of packaging and cooling this type of device. Last, the need of ensuring high circuit reliability has turned out to be more stringent. The availability of tools for the automatic design of low-power VLSI systems has thus become necessary. More specifically, following a natural trend, the\u00a0\u2026", "num_citations": "420\n", "authors": ["1596"]}
{"title": "CUDD: CU decision diagram package\n", "abstract": " CiNii \u8ad6\u6587 - CUDD : CU Decision Diagram Package CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 \u3010\u5ef6\u9577\u3011\u65b0\u578b \u30b3\u30ed\u30ca\u30a6\u30a4\u30eb\u30b9\u611f\u67d3\u62e1\u5927\u9632\u6b62\u306b\u4f34\u3046CiNii\u7a93\u53e3\u696d\u52d9\u306e\u4e00\u90e8\u7e2e\u9000\u306b\u3064\u3044\u3066 CUDD : CU Decision Diagram Package SOMENZI F. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI F. \u53ce\u9332\u520a\u884c\u7269 http://vlsi.colorado.edu/\u301cfabio/CUDD/ http://vlsi.colorado.edu/\u301cfabio/CUDD/, 1997 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 BDD/ZDD\u306e \u6280\u6cd5\u3068\u96e2\u6563\u69cb\u9020\u51e6\u7406\u7cfb(<\u7279\u96c6>\u96e2\u6563\u69cb\u9020\u51e6\u7406\u7cfb-\u77e5\u80fd\u60c5\u5831\u51e6\u7406\u3092\u652f\u3048\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u306e\u6280\u6cd5) \u6e4a \u771f\u4e00 , Shin-ichi Minato \u4eba\u5de5\u77e5\u80fd\u5b66\u4f1a\u8a8c = Journal of Japanese Society for Artificial Intelligence 27(3), 232-238, 2012-05-01 \u53c2\u8003\u6587\u732e40\u4ef6 \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240\uff08NII\uff09\u30a4\u30d9\u30f3\u30c8 Tweet \u5404\u7a2e\u2026", "num_citations": "415\n", "authors": ["1596"]}
{"title": "Who are the variables in your neighbourhood\n", "abstract": " Dynamic reordering techniques have had considerable success in reducing the impact of the initial variable order on the size of decision diagrams. Sifting, in particular, has emerged as a very good compromise between low CPU time requirements and high quality of results. Sifting, however, has the absolute position of a variable as the primary objective, and only considers the relative positions of groups of variables indirectly. In this paper we propose an extension to sifting that may move groups of variables simultaneously to produce better results. Variables are aggregated by checking whether they have a strong affinity to their neighbors. (Hence the title.) Our experiments show an average improvement in size of 11%. This improvement, coupled with the greater robustness of the algorithm, more than offsets the modest increase in CPU time that is sometimes incurred.", "num_citations": "270\n", "authors": ["1596"]}
{"title": "Symmetry detection and dynamic variable ordering of decision diagrams\n", "abstract": " Knowing that some variables are symmetric in a function has numerous applications; in particular, it can help produce better variable orders for Binary Decision Diagrams (BDDs) and related data structures (eg, Algebraic Decision Diagrams). It has been observed that there often exists an optimum order for a BDD wherein symmetric variables are contiguous. We propose a new algorithm for the detection of symmetries, based on dynamic reordering, and we study its interaction with the reordering algorithm itself. We show that combining sifting with an e cient symmetry check for contiguous variables results in the fastest symmetry detection algorithm reported to date and produces better variable orders for many BDDs. The overhead on the sifting algorithm is negligible.", "num_citations": "220\n", "authors": ["1596"]}
{"title": "High-density reachability analysis\n", "abstract": " We address the problem of reachability analysis for large finite state systems. Symbolic techniques have revolutionized reachability analysis but still have limitations in traversing large systems. We present techniques to improve the symbolic breadth-first traversal and compute a lower bound on the reachable states. We identify the problem as one of density during traversal and our techniques seek to improve the same. Our results show a marked improvement on the existing breadth-first traversal methods.", "num_citations": "219\n", "authors": ["1596"]}
{"title": "Markovian analysis of large finite state machines\n", "abstract": " Regarding finite state machines as Markov chains facilitates the application of probabilistic methods to very large logic synthesis and formal verification problems. In this paper we present symbolic algorithms to compute the steady-state probabilities for very large finite state machines (up to 10/sup 27/ states). These algorithms, based on Algebraic Decision Diagrams (ADD's)-an extension of BDD's that allows arbitrary values to be associated with the terminal nodes of the diagrams-determine the steady-state probabilities by regarding finite state machines as homogeneous, discrete-parameter Markov chains with finite state spaces, and by solving the corresponding Chapman-Kolmogorov equations. We first consider finite state machines with state graphs composed of a single terminal strongly connected component; for this type of system we have implemented two solution techniques: One is based on the Gauss\u00a0\u2026", "num_citations": "193\n", "authors": ["1596"]}
{"title": "Minimal assignments for bounded model checking\n", "abstract": " A traditional counterexample to a linear-time safety property shows the values of all signals at all times prior to the error. However, some signals may not be critical to causing the failure. A succinct explanation may help human understanding as well as speed up algorithms that have to analyze many such traces. In Bounded Model Checking (BMC), a counterexample is constructed from a satisfying assignment to a Boolean formula, typically in CNF. Modern SAT solvers usually assign values to all variables when the input formula is satisfiable. Deriving minimal satisfying assignments from such complete assignments does not lead to concise explanations of counterexamples because of how CNF formulae are derived from the models. Hence, we formulate the extraction of a succinct counterexample as the problem of finding a minimal assignment that, together with the Boolean formula describing the model\u00a0\u2026", "num_citations": "159\n", "authors": ["1596"]}
{"title": "Redundancy identification/removal and test generation for sequential circuits using implicit state enumeration\n", "abstract": " Finite state machine (FSM) verification based on implicit state enumeration can be extended to test generation and redundancy identification. The extended method constructs the product machine of two FSMs to be compared, and reachability analysis is performed by traversing the product machine to find any difference in I/O behavior. When an output difference is detected, the information obtained by reachability analysis is used to generate a test sequence. This method is complete, and it generates one of the shortest possible test sequences for a given fault. However, applying this method indiscriminately for all faults may result in unnecessary waste of computer resources. An efficient method based on reachability analysis of the fault-free machine (three-phase ATPG) in addition to the powerful but more resource-demanding product machine traversal is presented. The application of these algorithms to the\u00a0\u2026", "num_citations": "155\n", "authors": ["1596"]}
{"title": "An exact minimizer for Boolean relations\n", "abstract": " Boolean relations are a generalization of incompletely specified logic functions. The authors give a procedure, similar to the Quine-McCluskey procedure, for finding the global optimum sum-of-product representation for a Boolean relation. This is formulated as a binate covering problem, ie as a generalization of the ordinary (unate) covering problem. They give an algorithm for it and review the relation of binate covering to tautology checking. The procedure has been implemented and results are presented.<>", "num_citations": "138\n", "authors": ["1596"]}
{"title": "Algorithms for approximate FSM traversal\n", "abstract": " In this paper we present algorithms for approximate FSM traversal based on state space decomposition. The original FSM is partitioned in sub-machines, and each of them is traversed sepa~ ately; the~ eszdt is an over-estimation of the set of reachable states. Several traversal strategies are discussed. Good partitioning is important for the performance of the traversal techniques; a method to heuristicallyjind an appropriate decomposition, based on the exploration of the FSM latch connection graph, is proposed. Applications of the approximate traversal methods to sequential optimization and behavioral verification of FSM\u2019S are described; ex-perimental results for such applications, together with data concerning pure traversal, are reported.", "num_citations": "130\n", "authors": ["1596"]}
{"title": "Re-encoding sequential circuits to reduce power dissipation\n", "abstract": " In cloud computing, data are managed by different entities, not only by the actual data owner but also by many cloud providers. Sophisticated clouds collaboration scenarios may require that the data objects are distributed at cloud providers and accessed remotely, while still being under the control of the data owners. This brings security challenges for distributed authorization and trust management that existing proposed schemes have not fully solved. In this paper, we propose a Dynamic Trust Establishment approach which can be incorporated into cloud services provisioning life-cycles for the multi-provider Intercloud environment. It relies on attribute-based policies as the mechanism for trust evaluation and delegation. The paper proposes a practical implementation approach for attribute-based policies evaluation using Multi-type Interval Decision Diagrams extended from Integer Decision Diagrams which is\u00a0\u2026", "num_citations": "123\n", "authors": ["1596"]}
{"title": "Binary decision diagrams\n", "abstract": " We review Binary Decision Diagrams presenting the properties and algorithms that are most relevant to their application to the verification of sequential systems.", "num_citations": "118\n", "authors": ["1596"]}
{"title": "To split or to conjoin: The question in image computation\n", "abstract": " Image computation is the key step in fixpoint computations that are extensively used in model checking. Two techniques have been used for this step: one based on conjunction of the terms of the transition relation, and the other based on recursive case splitting. We discuss when one technique outperforms the other, and consequently formulate a hybrid approach to image computation. Experimental results show that the hybrid algorithm is much more robust than the \u201cpure\u201d algorithms and outperforms both of them in most cases. Our findings also shed light on the remark of several researchers that splitting is especially effective in approximate reachability analysis.", "num_citations": "109\n", "authors": ["1596"]}
{"title": "Exact and heuristic algorithms for the minimization of incompletely specified state machines\n", "abstract": " In this paper we present two exact algorithms for state minimization of FSM's. Our results prove that exact state minimization is feasible for a large class of practical examples, certainly including most hand-designed FSM's. We also present heuristic algorithms, that can handle large, machine-generated, FSM's. The possibly many different reduced machines with the same number of states have different implementation costs. We discuss two steps of the minimization procedure, called state mapping and solution shrinking, that have received little prior attention to the literature, though they play a significant role in delivering an optimally implemented reduced machine. We also introduce an algorithm whose main virtue is the ability to cope with very general cost functions, while providing high performance.< >", "num_citations": "109\n", "authors": ["1596"]}
{"title": "Tearing based automatic abstraction for CTL model checking\n", "abstract": " In this paper we present the tearing paradigm as a way to automatically abstract behavior to obtain upper and lower bound approximations of a reactive system. We present algorithms that exploit the bounds to perform conservative ECTL and ACTL model checking. We also give an algorithm for false negative (or false positive) resolution for verification based on a theory of a lattice of approximations. We show that there exists a bipartition of the lattice set based on positive versus negative verification results. Our resolution methods are based on determining a pseudo-optimal shortest path from a given, possibly coarse but tractable approximation, to a nearest point on the contour separating one set of the bipartition from the other.", "num_citations": "105\n", "authors": ["1596"]}
{"title": "Minimization of symbolic relations\n", "abstract": " The problem of minimizing symbolic relations is addressed. The relevance of this problem in the field of optimal encoding is shown by examples. A binate covering formulation of the optimization problems involved is given, for which several algorithms are available. A novel method is proposed which is based on binary decision diagrams (BDDs) and the authors show how the covering problem can be solved in linear time in that case.<>", "num_citations": "104\n", "authors": ["1596"]}
{"title": "Synchronizing sequences and symbolic traversal techniques in test generation\n", "abstract": " Asynchronizing sequence drives a circuit from an arbitrary power-up state into a unique state. Test generation on a circuit without a reset state can be much simplified if the circuit has a synchronizing sequence. In this article, a framework and algorithms for test generation based on themultiple observation time strategy are developed by taking advantage of synchronizing sequences. Though it has been shown that the multiple observation time strategy can provide a higher fault coverage than the conventional single observation time strategy, until now the multiple observation time strategy has required a very complex tester operation model (referred asMultiple Observation time-Multiple Reference strategy (MOMR) in the sequel) over the conventional tester operation model. The overhead of MOMR, exponential in the worst case, has prevented widespread use of the method. However, when a circuit is\u00a0\u2026", "num_citations": "102\n", "authors": ["1596"]}
{"title": "Efficient manipulation of decision diagrams\n", "abstract": " Over the last decade significant progress has been made in the efficient implementation of algorithms for the manipulation of decision diagrams. We review the main issues involved in designing a decision diagram package, with special emphasis on the basic data structures and their management, on fast variable reordering, and on the collection of statistics to guide the tuning of the algorithms. Our analysis focuses on depth-first manipulation of reduced, ordered binary decision diagrams.", "num_citations": "100\n", "authors": ["1596"]}
{"title": "Border-block triangular form and conjunction schedule in image computation\n", "abstract": " Conjunction scheduling in image computation consists of clustering the parts of a transition relation and ordering the clusters, so that the size of the BDDs for the intermediate results of image computation stay small. We present an approach based on the analysis and permutation of the dependence matrix of the transition relation. Our algorithm computes a bordered-block lower triangular form of the matrix that heuristically minimizes the active lifetime of variables, that is, the number of conjunctions in which the variables participate. The ordering procedure guides a clustering algorithm based on the affinity of the transition relation parts. The ordering procedure is then applied again to define the cluster conjunction schedule.Our experimental results show the effectiveness of the new algorithm.", "num_citations": "94\n", "authors": ["1596"]}
{"title": "Probabilistic analysis of large finite state machines\n", "abstract": " Regarding finite state machines as Markov chains facilitates the application of probabilistic methods to very large logic synthesis and formal verification problems. Recently, we have shown how symbolic algorithms based on Algebraic Decision Diagrams may be used to calculate the steady-state probabilities of finite state machines with more than 108 states. These algorithms treated machines with state graphs composed of a single terminal strongly connected component. In this paper we consider the most general case of systems which can be modeled as state machines with arbitrary transition structures. The proposed approach exploits structural information to decompose and simplify the state graph of the machine.", "num_citations": "94\n", "authors": ["1596"]}
{"title": "Approximation and decomposition of binary decision diagrams\n", "abstract": " Efficient techniques for the manipulation of Binary Decision Diagrams (BDDs) are key to the success of formal verification tools. Recent advances in reachability analysis and model checking algorithms have emphasized the need for efficient algorithms for the approximation and decomposition of BDDs. In this paper we present a new algorithm for approximation and analyze its performance in comparison with existing techniques. We also introduce a new decomposition algorithm that produces balanced partitions. The effectiveness of our contributions is demonstrated by improved results in reachability analysis for some hard problem instances.", "num_citations": "89\n", "authors": ["1596"]}
{"title": "Using lower bounds during dynamic BDD minimization\n", "abstract": " Ordered binary decision diagrams (BDDs) are a data structure for the representation and manipulation of Boolean functions, often applied in very large scale integration (VLSI) computer-aided design (CAD). The choice of variable ordering largely influences the size of the BDD; its size may vary from linear to exponential. The most successful methods to find good orderings are based on dynamic variable reordering, i.e., exchanging neighboring variables. This basic operation has been used in various variants, like sifting and window permutation. In this paper, we show that lower bounds computed during the minimization process can speed up the computation significantly. First, lower bounds are studied from a theoretical point of view. Then these techniques are incorporated in dynamic minimization algorithms. By the computation of good lower bounds, large parts of the search space can be pruned, resulting in\u00a0\u2026", "num_citations": "88\n", "authors": ["1596"]}
{"title": "Algorithms for approximate FSM traversal based on state space decomposition\n", "abstract": " This paper presents algorithms for approximate finite state machine traversal based on state space decomposition. The original finite state machine is partitioned in component submachines, and each of them is traversed separately; the result of the computation is an over-estimation of the set of reachable states of the original machine. Different traversal strategies, which reduce the effects of the degrees of freedom introduced by the decomposition, are discussed. Efficient partitioning is a key point for the performance of the traversal techniques; a method to heuristically find a good decomposition of the overall finite state machine, based on the exploration of its state variable dependency graph, is proposed. Applications of the approximate traversal methods to logic optimization of sequential circuits and behavioral verification of finite state machines are described; experimental results for such applications, together\u00a0\u2026", "num_citations": "87\n", "authors": ["1596"]}
{"title": "Variable ordering and selection of FSM traversal\n", "abstract": " The authors consider the problem of variable ordering in algorithms for verification of finite state machines (FSMs) for which the traversal is based on BDD (binary decision diagram) representation and image computation via implicit enumeration. They treat two separate BDD ordering problems:(1) minimization of the representation of the next state function and the representation of the set of reachable states, and (2) a selection heuristic to reduce the complexity of the image computation problem by dynamic selection of the implicit enumeration splitting variables. In both problems they present theoretical results based on the algebraic structure of the next state functions, heuristic ordering methods, and favorable experimental results for problems with significant algebraic structure.<>", "num_citations": "83\n", "authors": ["1596"]}
{"title": "A symbolic algorithms for maximum flow in 0-1 networks\n", "abstract": " We present an algorithm for finding the maximum flow in a 0-1 network. The algorithm is symbolic and does not require explicit enumeration of the nodes and edges of the network. Therefore, it can handle much larger graphs than it was previously possible (more than 1036 edges). The main idea is to trace (implicitly) sets of edge-disjoint augmenting paths. Disjointness is enforced by solving an edge matching problem for each layer of the network with the help of newly defined priority functions.", "num_citations": "79\n", "authors": ["1596"]}
{"title": "Vacuum cleaning CTL formulae\n", "abstract": " Vacuity detection in model checking looks for properties that hold in a model, and can be strengthened without causing them to fail. Such properties often signal problems in the model, its environment, or the properties themselves. The seminal paper of Beer et al. [1] proposed an efficient algorithm applicable to a restricted set of properties. Subsequently, Kupferman and Vardi [15] extended vacuity detection to more expressive specification mechanisms. They advocated a more minute examination of temporal logic formulae than the one adopted in [1]. However, they did not address the issues of practicality and usefulness of this more scrupulous inspection. In this paper we discuss efficient algorithms for the detection of vacuous passes of temporal logic formulae, showing that a thorough vacuity check for CTL formulae can be carried out with very small overhead, and even, occasionally, in less time than\u00a0\u2026", "num_citations": "76\n", "authors": ["1596"]}
{"title": "Fate and FreeWill in error traces\n", "abstract": " The ability to generate counterexamples for failing properties is often cited as one of the strengths of model checking. However, it is often difficult to interpret long error traces in which many variables appear. Further, a traditional error trace presents only one possible behavior of the system causing the failure, with no further annotation. Our objective is to \u201ccapture more of the error\u201d in an error trace to make debugging easier.We present an enhanced error trace in an alternation of fated (forced) and free segments. The fated segments showunavoidable progress towards the error while the free segments show choices that, if avoided, may have prevented the error. This segmentation raises the questions of whether the fated segment should indeed be inevitable and whether the free segments are critical in causing the error. Addressing these questions may help the user analyze the error better.", "num_citations": "76\n", "authors": ["1596"]}
{"title": "Exact and heuristic algorithms for the minimization of incompletely specified state machines\n", "abstract": " Presents two exact algorithms for state minimization of FSM's. The authors' results prove that exact state minimization is feasible for a large class of practical examples, certainly including most hand-designed FSM's. They also present heuristic algorithms, that can handle large, machine-generated, FSM's. They argue that the true objective of state reduction should be reduction toward maximal encodability. The state mapping problem has received almost no prior attention in the literature. They show that mapping plays a significant role in delivering an optimally implemented reduced machine. They also introduce an algorithm whose main virtue is the ability to cope with very general cost functions, while providing very high performance.< >", "num_citations": "75\n", "authors": ["1596"]}
{"title": "Hints to accelerate symbolic traversal\n", "abstract": " Symbolic model checking is an increasingly popular debugging tool based on Binary Decision Diagrams (BDDs). The size of the diagrams, however, often prevents its application to large designs. The lack of flexibility of the conventional breadth-first approach to state search is often responsible for the excessive growth of the BDDs. In this paper we show that the use of hints to guide the exploration of the state space may result in orders-of-magnitude reductions in time and space requirements. We apply hints to invariant checking. The hints address the problems posed by difficult image computations, and are effective in both proving and refuting invariants. We show that good hints can often be found with the help of simple heuristics by someone who understands the circuit well enough to devise simulation stimuli or verification properties for it. We present an algorithm for guided traversal and discuss its\u00a0\u2026", "num_citations": "73\n", "authors": ["1596"]}
{"title": "Omega-regular objectives in model-free reinforcement learning\n", "abstract": " We provide the first solution for model-free reinforcement learning of {\\omega}-regular objectives for Markov decision processes (MDPs). We present a constructive reduction from the almost-sure satisfaction of {\\omega}-regular objectives to an almost- sure reachability problem and extend this technique to learning how to control an unknown model so that the chance of satisfying the objective is maximized. A key feature of our technique is the compilation of {\\omega}-regular properties into limit- deterministic Buechi automata instead of the traditional Rabin automata; this choice sidesteps difficulties that have marred previous proposals. Our approach allows us to apply model-free, off-the-shelf reinforcement learning algorithms to compute optimal strategies from the observations of the MDP. We present an experimental evaluation of our technique on benchmark learning problems.", "num_citations": "72\n", "authors": ["1596"]}
{"title": "MOZART: a concurrent multilevel simulator\n", "abstract": " MOZART, a concurrent fault simulator for large circuits described at the register-transfer, functional, gate, and switch levels, is described. The requirements of multilevel simulation have guided the definition of MOZART's syntax, value set, delay model, and algorithms. Performance is improved by reducing unnecessary activity. Two such techniques are levelized: two-pass simulation, which minimizes the number of events and evaluations, and list event scheduling, which allows optimized processing of simultaneous (fraternal) events for concurrent machines. Moreover, efficient handling of abnormally large or active fault machines can improve fault-simulator performance by several orders of magnitude. These and related issues are discussed; both analytical and experimental evidence is provided for the effectiveness of the solutions adopted in MOZART. A performance metric is introduced for fault simulation, based\u00a0\u2026", "num_citations": "70\n", "authors": ["1596"]}
{"title": "Fast sequential ATPG based on implicit state enumeration\n", "abstract": " The knowledge of the State Transition Graph (STG) of a sequential circuit helps in generating test sequences. For instance, by determining that a set of states is not reachable from the reset state, it is possible to identify a certain type of sequentially untestable faults. However, until recently, the ability of algorithms to store the STG of a sequential circuit has been limited to small instances. Recent advances in sequential circuit verification, based on the use of binary decision diagrams and new powerful implicit enumeration algorithms, have dramatically improved our ability to deal with large numbers of states. In this paper we report on the application of these algorithms to the problems of generating justification sequences, identifying redundancies, and dealing with hard-to-detect faults. Our experiments show substantial improvements over previously published results.", "num_citations": "69\n", "authors": ["1596"]}
{"title": "Extended BDD's: Trading off canonicity for structure in verification algorithms\n", "abstract": " The authors present an extension to binary decision diagrams (BDDs) that exploits the information contained in the structure of the circuit to produce a compact, semicanonical representation. The extended BDDs (XBDDs) retain many of the advantages of BDDs while at the same time allowing one to deal with larger circuits. Using XBDDs, it is possible to verify circuits for which the BDDs could not be built in the same amount of space. Results of the application of XBDDs to combinational multipliers are presented.<>", "num_citations": "67\n", "authors": ["1596"]}
{"title": "Linear sifting of decision diagrams\n", "abstract": " We propose a new algorithm, called linear sifting, for theoptimization of decision diagrams that combines the efficiency of sifting and the power of linear transformations. We show that the new algorithm is applicable to large examples, and that inmany cases it leads to substantiallymore compact diagrams when compared to simple variablereordering. We show inwhat sense linear transformationscomplement variable reordering, and we discuss applications of the new technique to synthesis and verification.", "num_citations": "61\n", "authors": ["1596"]}
{"title": "Better generalization in IC3\n", "abstract": " An improved clause generalization procedure for IC3 is presented. Whereas standard generalization extracts a relatively inductive clause from a single state, called a counterexample to induction (CTI), the new procedure also extracts such clauses from other states, called counterexamples to generalization (CTG), that interfere with the primary generalization attempt. The motivation is to enable IC3 to explore states farther from the error states than are CTIs while remaining property-focused. CTGs are strong candidates for being farther but still backward reachable. Significant reductions in the maximum depth reached by IC3's priority queue-directed explicit backward search indicate that this intention is achieved in practice. The effectiveness of the new procedure is established in two independent implementations of IC3, which demonstrate an increase of 17 and 27, respectively, in the number of solved HWMCC\u00a0\u2026", "num_citations": "59\n", "authors": ["1596"]}
{"title": "On-the-fly clause improvement\n", "abstract": " Most current propositional SAT solvers apply resolution at various stages to derive new clauses or simplify existing ones. The former happens during conflict analysis, while the latter is usually done during preprocessing. We show how subsumption of the operands by the resolvent can be inexpensively detected during resolution; we then show how this detection is used to improve three stages of the SAT solver: variable elimination, clause distillation, and conflict analysis. The \u201con-the-fly\u201d subsumption check is easily integrated in a SAT solver. In particular, it is compatible with the strong conflict analysis and the generation of unsatisfiability proofs. Experiments show the effectiveness of this technique and illustrate an interesting synergy between preprocessing and the DPLL procedure.", "num_citations": "59\n", "authors": ["1596"]}
{"title": "CUDD: CU decision diagram package\n", "abstract": " CiNii \u8ad6\u6587 - Cudd : Cu decision diagram package CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf [\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b \u306b\u3064\u3044\u3066 Cudd : Cu decision diagram package SOMENZI F. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI F. \u53ce\u9332\u520a\u884c\u7269 ftp://vlsi.colorado.edu/pub/ ftp://vlsi.colorado.edu/pub/, 1995 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u9806\u5e8f\u56de\u8def\u306e\u72b6\u614b\u63a2\u7d22\u5411\u3051BDD\u306e\u52d5\u7684\u5909\u6570\u9806\u5e8f\u3065\u3051\u624b\u6cd5 \u6a0b\u53e3 \u535a\u4e4b , Somenzi Fabio \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. CPSY, \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30b7\u30b9\u30c6\u30e0 99(531), 25-32, 2000-01-11 \u53c2\u8003 \u6587\u732e17\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10022196367 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 Refer/BiblX\u3067\u8868\u793a \u2026", "num_citations": "59\n", "authors": ["1596"]}
{"title": "An incremental algorithm to check satisfiability for bounded model checking\n", "abstract": " In Bounded Model Checking (BMC), the search for counterexamples of increasing lengths is translated into a sequence of satisfiability (SAT) checks. It is natural to try to exploit the similarity of these SAT instances by forwarding clauses learned during conflict analysis from one instance to the next. The methods proposed to identify clauses that remain valid fall into two categories: Those that are oblivious to the mechanism that generates the sequence of SAT instances and those that rely on it. In the case of a BMC run, it was observed by Strichman [O. Shtrichman. Pruning techniques for the SAT-based bounded model checking problem. In Correct Hardware Design and Verification Methods (CHARME 2001), pages 58\u201370, Livingston, Scotland, Sept. 2001. Springer. LNCS 2144] that those clauses learned during one SAT check that only depend on the structure of the model remain valid when checking for longer\u00a0\u2026", "num_citations": "58\n", "authors": ["1596"]}
{"title": "A structural approach to state space decomposition for approximate reachability analysis\n", "abstract": " Exploiting circuit structure is a key issue in the implementation of algorithms for state space decomposition when the target is approximate FSM traversal. Given the gate-level description of a sequential circuit, the information about its structure can be captured by evaluating the affinity between pairs or groups of latches. Two main factors have to be considered in carrying out the structural analysis of a sequential circuit: latch connectivity and latch correlation. We estimate the affinity of two latches by combining these two factors, and we use this measure to translate the state space decomposition problem into a graph partitioning problem. Traversal results obtained on the largest ISCAS'89 benchmarks show the effectiveness of the method.< >", "num_citations": "58\n", "authors": ["1596"]}
{"title": "Don't care sequences and the optimization of interacting finite state machines\n", "abstract": " We explore the nature of incomplete specification in sequential circuits. We compare it to the case of combinational circuits and propose new definitions and algorithms. We extend the existing algorithms for input don't care sequences and provide a new theory for output don't care sequences, based on the concept of information lossyness. The implementation of the proposed techniques in a program called SEQUOIA (sequential optimization of interacting automata) shows that our approach is viable and effective.< >", "num_citations": "58\n", "authors": ["1596"]}
{"title": "An incremental approach to model checking progress properties\n", "abstract": " An incremental algorithm for model checking progress properties is proposed. It follows from the following insight: any SCC-closed region of a system's state graph can be represented by a sequence of inductive assertions. Each iteration of the algorithm selects a set of states, called a skeleton, that together satisfy all fairness conditions; it then applies safety model checkers to attempt to connect the states into a reachable fair cycle. If this attempt fails, the resulting learned lemma takes one of two forms: an inductive reachability assertion that shows that at least one state of the skeleton is unreachable, or an inductive wall that defines two SCC-closed regions of the state graph. Subsequent skeletons must be chosen entirely from one side of the wall. Because a lemma often applies more generally than to the one skeleton from which it was derived, property-directed abstraction is achieved. The algorithm is highly\u00a0\u2026", "num_citations": "57\n", "authors": ["1596"]}
{"title": "Efficient conflict analysis for finding all satisfying assignments of a Boolean circuit\n", "abstract": " Finding all satisfying assignments of a propositional formula has many applications to the synthesis and verification of hardware and software. An approach to this problem that has recently emerged augments a clause-recording propositional satisfiability solver with the ability to add \u201cblocking clauses.\u201d One generates a blocking clause from a satisfying assignment by taking its complement. The resulting clause prevents the solver from visiting the same solution again. Every time a blocking clause is added the search is resumed until the instance becomes unsatisfiable. Various optimization techniques are applied to get smaller blocking clauses, since enumerating each satisfying assignment would be very inefficient.               In this paper, we present an improved algorithm for finding all satisfying assignments for a generic Boolean circuit. Our work is based on a hybrid SAT solver that can apply conflict\u00a0\u2026", "num_citations": "57\n", "authors": ["1596"]}
{"title": "A symbolic method to reduce power consumption of circuits containing false paths\n", "abstract": " Most digital circuits synthesized from a finite-state machine contain idle functional units (modules) at some time during operation in the sense that they do not contain any valid data. In a synchronous circuit with an on-chip synthesized controller which explicitly determines the f? ow of data, the idle finctional units are known in advance for each FSM stage. A case study of the implementation of an on-line built-in self-test that exploits the idle modules is presented in this paper. A concurrent test data-path which cycles test vectors through the idle functional units and produces a response signature is synthesized along with the original algorithm, avoiding test-time overheads.", "num_citations": "55\n", "authors": ["1596"]}
{"title": "Guiding simulation with increasingly refined abstract traces\n", "abstract": " We combine abstraction refinement and simulation to provide a more efficient approach to checking invariant properties whose only counterexamples are very long traces. We allow each transition of an abstract error trace to map to multiple transitions of the concrete error trace and simulate pseudorandom vectors to build segments of the concrete trace. This approach addresses the capacity limitation of the formal verification engine as well as the short-sightedness of the simulator, thus providing a more effective technique for deep, subtle bugs.", "num_citations": "51\n", "authors": ["1596"]}
{"title": "Do's and don'ts of CTL state coverage estimation\n", "abstract": " Coverage estimation for model checking quantifies the completeness of a set of properties. We present an improved version of the algorithm of Hoskote et al. (1999) that applies to a larger subset of CTL; we prove properties of the algorithm and apply it to three case studies. From these case studies we derive recommendations for an effective use of coverage estimation.", "num_citations": "51\n", "authors": ["1596"]}
{"title": "Computing the maximum power cycles of a sequential circuit\n", "abstract": " This paper studies the problem of estimating worst case power dissipation in a sequential circuit. We approach this problem by finding the maximum average weight cycles in a weighted directed graph. In order to handle practical sized examples, we use symbolic methods, based on Algebraic Decision Diagrams (ADDs), for computing the maximum average length cycles as well as the number of gate transitions in the circuit, which is necessary to construct the weighted directed graph.", "num_citations": "50\n", "authors": ["1596"]}
{"title": "A new algorithm for the binate covering problem and its application to the minimization of boolean relations\n", "abstract": " The binate covering problem (BCP) is the problem of finding a minimum cost assignment to variables that is a solution of a Boolean equation f=1. It is a generalization of the set covering (or unate covering) problem, where f is positive unate, and is generally given as a table with rows corresponding to the set elements and the columns corresponding to the subsets. Previous methods have considered the case when f is given as a product-of-sum formula or as a binary decision diagram (BDD). A branch-and-bound algorithm for the BCP that assumes f is expressed as the conjunction of multiple BDDs is presented. The BCP solver that has been implemented can be applied to several problems, including exact minimization of Boolean relations, for which results are presented. It has been possible to solve large, difficult problems (up to 4692 variables) which could not be solved by the product of sum based method.< >", "num_citations": "50\n", "authors": ["1596"]}
{"title": "Automatic state space decomposition for approximate FSM traversal based on circuit analysis\n", "abstract": " Exploiting circuit structure is a key issue in the implementation of algorithms for state space decomposition when the target is approximate FSM traversal. Given the gate-level description of a sequential circuit, the information about its structure can be captured by evaluating the affinity between pairs or groups of latches. Two main factors have to be considered in carrying out the structural analysis of a sequential circuit: latch connectivity and latch correlation. The first one takes into account the mutual dependency of each memory element on the others; the second one tells us how related are the functions realized by the logic feeding each latch. In this paper we estimate the affinity of two latches by combining these two factors, and we use this measure to formulate the state space decomposition problem as a graph partitioning problem. We propose an algorithm to automatically determine \"good\" partitions of the latch\u00a0\u2026", "num_citations": "49\n", "authors": ["1596"]}
{"title": "Incremental, inductive CTL model checking\n", "abstract": " A SAT-based incremental, inductive algorithm for model checking CTL properties is proposed. As in classic CTL model checking, the parse graph of the property shapes the analysis. However, in the proposed algorithm, called IICTL, the analysis is directed by task states that are pushed down the parse tree. To each node is associated over- and under-approximations to the set of states satisfying that node\u2019s property; these approximations are refined until a proof that the property does or does not hold is obtained. Each CTL operator corresponds naturally to an incremental sub-query: given a task state, an EX node executes a SAT query; an EU node applies IC3; and an EG node applies FAIR. In each case, the query result provides more general information than necessary to satisfy the task. When a query is satisfiable, the returned trace is generalized using forall-exists reasoning, during which IC3 is applied\u00a0\u2026", "num_citations": "48\n", "authors": ["1596"]}
{"title": "Minimum length synchronizing sequences of finite state machine\n", "abstract": " Computing synchronizing sequences is an important step in test generation for circuits without external reset [CJSP93]. An efficient heuristic algorithm has been recently proposed [PJH92]. However, the only algorithms known so far for the minimum-length synchronizing sequence have been applicable only to small finite state machines, because they required excessive enumeration. In this paper we present a new technique to calculate the minimum length synchronizing sequence of a finite state machine, based on the BDD model of an iterative system. It implicitly calculates all possible minimum length sequences and corresponding reset states by deciding the satisfiability of a boolean formula. We also present a procedure to obtain the synchronizing sequences of a class of sequential iterative systems in constant time regardless of the number of cells. We present experimental results that show the viability of the\u00a0\u2026", "num_citations": "47\n", "authors": ["1596"]}
{"title": "Improving Ariadne's bundle by following multiple threads in abstraction refinement\n", "abstract": " The authors propose a scalable abstraction-refinement method for model checking invariant properties on large sequential circuits, which is based on fine-grain abstraction and simultaneous analysis of all abstract counterexamples of the shortest length. Abstraction efficiency is introduced to measure for a given abstraction-refinement algorithm how much of the concrete model is required to make the decision. The fully automatic techniques presented in this paper can efficiently reach or come near to the maximal abstraction efficiency. First, a fine-grain abstraction approach is given to keep the abstraction granularity small by breaking down large combinational logic cones with Boolean network variables (BNVs) and then treating both state variables and BNVs as atoms in abstraction. Second, a refinement algorithm is proposed based on an improved Ariadne's bundle In the legend of Theseus, Ariadne's bundle\u00a0\u2026", "num_citations": "45\n", "authors": ["1596"]}
{"title": "Linear sifting of decision diagrams and its application in synthesis\n", "abstract": " We propose a new algorithm, called linear sifting, for the optimization of decision diagrams that combines the efficiency of sifting and the power of linear transformations. The new algorithm is applicable to large examples, and in many cases leads to substantially more compact diagrams when compared to simple variable reordering. We also show in what sense linear transformations complement variable reordering and how the technique can be applied to verification issues. Going a step further, we discuss a synthesis scenario where-due to the complexity of the target function-it is inevitable to decompose the function in a preprocessing step. By using linear sifting it is possible to extract a linear filter and, hence, to achieve the necessary decomposition. Using this method we were able to synthesize functions with standard tools which fail otherwise.", "num_citations": "45\n", "authors": ["1596"]}
{"title": "Timing analysis of combinational circuits using ADDs\n", "abstract": " This paper presents a symbolic algorithm to perform timing analysis of combinational circuits which takes advantage of the high compactness of representation of the Algebraic Decision Diagrams (ADDs). The procedure we propose, implemented as on extension of the SIS synthesis system, is able to provide more accurate timing information than any other method presented so far; in particular, it is able to compute and store the true delay of the gate-level representation of the circuit for all possible input vectors, as opposed to the traditional methods which consider only the worst-case primary inputs combination. Furthermore, the approach does not require any explicit false path elimination. The information calculated by the timing analyzer has several practical applications such as determining the sets of critical input vectors, critical gates, and critical paths of the circuit, which may be efficiently used in the process of\u00a0\u2026", "num_citations": "45\n", "authors": ["1596"]}
{"title": "CirCUs: A satisfiability solver geared towards bounded model checking\n", "abstract": " CirCUs is a satisfiability solver that works on a combination of And-Inverter-Graph, CNF clauses, and BDDs. It has been designed to work well with bounded model checking. It takes as inputs a Boolean circuit (e.g., the model unrolled k times) and an optional set of additional constraints (for instance, requesting that a solution correspond to a simple path) in the form of CNF clauses or BDDs. The algorithms in CirCUs take advantage of the mixed representation by applying powerful BDD-based implication algorithms, and decision heuristics that are objective-driven. CirCUs supports incremental SAT solving, early termination checks, and other analyses of the model that translate into SAT. Experimental results demonstrate CirCUs\u2019s efficiency.", "num_citations": "44\n", "authors": ["1596"]}
{"title": "CirCUs: A hybrid satisfiability solver\n", "abstract": " CirCUs is a satisfiability solver that works on a combination of an And-Inverter-Graph (AIG), Conjunctive Normal Form (CNF) clauses, and Binary Decision Diagrams (BDDs). We show how BDDs are used by CirCUs to help in the solution of SAT instances given in CNF. Specifically, the clauses are sorted by solving a hypergraph linear arrangement problem. Then they are clustered by an algorithm that strives to avoid explosion in the resulting BDD sizes. If clustering results in a single diagram, the SAT instance is solved directly. Otherwise, search for a satisfying assignment is conducted on the original clauses, enhanced with information extracted from the BDDs. We also describe a new decision variable selection heuristic that is based on recognizing that the variables involved in a conflict clause are often best treated as a related group. We present experimental results that demonstrate CirCUs\u2019s efficiency\u00a0\u2026", "num_citations": "44\n", "authors": ["1596"]}
{"title": "Approximate reachability don't cares for CTL model checking\n", "abstract": " RDCS (Reachability Don\u2019t Cares) can have a dramatic impact on the cost of~ model checking [18]. Unfortunately, RDCS, being a global property, are often much more difficult to compute than the satisfying set of typical CTL formulas. We address this problem through the use of Approximate Reachabili~ Don\u2019t Cares (ARDCS), computd with the algorithms developed for the VERITAS sequential synthesis package [4, 5]. Approximate Reachable states represent an upper bound on the set of true reachable states, and thus a lower bound on the set of unreachable (Don\u2019t Care) states. ARDCS can be 10X to IOOX (or much more for very large circuits) cheaper to compute than RDCS, and in some cases have the same dramatic effect on CTL model checking as the real RDCS. We also discuss the application of ARDCS to the problem of exact computation of the RDCSthemselves. Experiments on industrial benchmarks\u00a0\u2026", "num_citations": "44\n", "authors": ["1596"]}
{"title": "Abstraction refinement for large scale model checking\n", "abstract": " Abstraction Refinement for Large Scale Model Checking summarizes recent research on abstraction techniques for model checking large digital system. Considering both the size of today's digital systems and the capacity of state-of-the-art verification algorithms, abstraction is the only viable solution for the successful application of model checking techniques to industrial-scale designs. This book describes recent research developments in automatic abstraction refinement techniques. The suite of algorithms presented in this book has demonstrated significant improvement over prior art; some of them have already been adopted by the EDA companies in their commercial/in-house verification tools.", "num_citations": "42\n", "authors": ["1596"]}
{"title": "Low power TLB design for high performance microprocessors\n", "abstract": " This paper proposes a banked associative design for TLBs (BA-TLB), and presents results for the two approaches for both single process and multi-process environments. Banked associative designs consume less power than fully-associative TLBs (FA-TLB) since only half the CAM entries are looked up on each access to the TLB. Experiments show that BA-TLBs perform as well as FA-TLBs. Also, given the same, fixed TLB power budget for BA-TLB and FA-TLB designs, the BA design outperforms the FA design by an average of 83%.", "num_citations": "42\n", "authors": ["1596"]}
{"title": "Fault detection in programmable logic arrays\n", "abstract": " When designing fault-tolerant systems including programmable logic arrays (PLAs), the various aspects of these circuits concerning fault diagnosis have to be taken into account. The peculiarity of these aspects, ranging from fault models to test generation algorithms and to self-checking structures, is due to the regularity of PLAs. The fault model generally accepted for PLAs is the crosspoint defect; it is employed by dedicated test generation algorithms, based on the fact that PLAs implement a two-level combinational function. The problem of accessing inputs and outputs of the PLA can be alleviated by augmenting the PLA itself so as to simplify the test vectors to be applied, making them function independent in the limit. A further step consists in the addition of the circuitry required to generate test vectors and to evaluate the answer, thus obtaining a built-in self-test (BIST) architecture. Finally, high reliability can be\u00a0\u2026", "num_citations": "42\n", "authors": ["1596"]}
{"title": "Alembic: An efficient algorithm for CNF preprocessing\n", "abstract": " Satisfiability (SAT) solvers often benefit from a preprocessing of the formula to be decided. For formulae in conjunctive normal form (CNF), subsumed clauses may be removed or partial resolution may be applied. Preprocessing aims at simplifying the formula and at increasing the deductive power of the solver. These two objectives are sometimes competing. We present a new algorithm that combines simplification and increase of deductive power and we show its effectiveness in speeding up SAT solvers.", "num_citations": "41\n", "authors": ["1596"]}
{"title": "Least fixpoint approximations for reachability analysis\n", "abstract": " The knowledge of the reachable states of a sequential circuit can dramatically speed up optimization and model checking. However, since exact reachability analysis may be intractable, approximate techniques are often preferable. H. Cho et al. (1996) presented the machine-by-machine (MBM) and frame-by-frame (FBF) methods to perform approximate finite state machine (FSM) traversal. FBF produces tighter upper bounds than MBM; however, it usually takes much more time and it may have convergence problems. In this paper, we show that there exists a class of methods-least fixpoint approximations-that compute the same results as RFBF (\"reached FBF\", one of the FBF methods). We show that one member of this class, which we call \"least fixpoint MBM\" (LMBM), is as efficient as MBM, but provably more accurate. Therefore, the trade-off that existed between MBM and RFBF has been eliminated. LMBM can\u00a0\u2026", "num_citations": "41\n", "authors": ["1596"]}
{"title": "Advances in concurrent multilevel simulation\n", "abstract": " Fault simulation of circuits described at multiple levels of abstraction (RT, gate, switch) is a major problem in the area of CAD and testing. Although the concurrent paradigm is generally acknowledged as the most efficient, several techniques are crucial to successfully extend it to multilevel simulation of large circuits. In particular, based on multilist traversal, fraternal event processing, list events, and levelizing, advances are presented here in simulation speed, accuracy, and generality. For zero-delay elements, the simulation of irrelevant activity is avoided, but the accuracy of structural (interconnect) logic simulation is maintained. What is described here has been implemented in MOZART, and detailed experimental results are reported. Relative to the good machine, the average faulty machine is simulated 900 to 17 000 times faster. The approach presented is not restricted to fault simulation, and is thus applicable to\u00a0\u2026", "num_citations": "41\n", "authors": ["1596"]}
{"title": "Prime clauses for fast enumeration of satisfying assignments to boolean circuits\n", "abstract": " Finding all satisfying assignments of a propositional formula has many applications in the design of hardware and software. An approach to this problem augments a clause-recording propositional satisfiability solver with the ability to add blocking clauses, which prevent the solver from visiting the same solution more than once. One generates a blocking clause from a satisfying assignment by taking its complement. In this paper, we present an improved algorithm for finding all satisfying assignments for a generic Boolean circuit. An optimization based on lifting - which generates minimal satisfying assignments - yields prime blocking clauses. Thanks to the primality of the blocking clauses, the derived conflict clauses usually prune both satisfiable and unsatisfiable points at once. The efficiency of our new algorithm is demonstrated by our preliminary results on SAT-based unbounded model checking.", "num_citations": "40\n", "authors": ["1596"]}
{"title": "Safety first: A two-stage algorithm for LTL games\n", "abstract": " In the game theoretic approach to the synthesis of reactive systems, specifications are often given as a conjunction of linear time properties. An implementation can be obtained from a winning strategy derived from a suitable generalized parity game in which each property produces a parity acceptance condition. Safety and persistence properties usually make up the majority of the specification. We show how this can be exploited to play the game in two stages and substantially speed up synthesis without sacrificing generality and conciseness of specification.", "num_citations": "39\n", "authors": ["1596"]}
{"title": "Symbolic algorithms for layout-oriented synthesis of pass transistor logic circuits\n", "abstract": " This paper presents a nouel methodology for synthesizing PTL circuifg, whose disfincfiue feafures are the use of a symbolic algorifhm for the covem. ng of fhe initial network in ferms of PTL cells, and the eqloifation of layout-level ama and delay models dun. ng fhe selection of fhe be~ t couem. ng solufion. The results produced by the synthesis procedure on the full guite of fhe kcas\u2019 85 combinational circuifs are very encouraging.", "num_citations": "39\n", "authors": ["1596"]}
{"title": "Fate and free will in error traces\n", "abstract": " The ability to generate counterexamples for failing properties is often cited as one of the strengths of model checking. However, it is often difficult to interpret long error traces in which many variables appear. Besides, a traditional error trace presents only one possible behavior of the system causing the failure, with no further annotation. Our objective is to identify some structure in the error trace to make debugging easier. We present an enhanced error trace as an alternation of fated (forced) and free segments. The fated segments show unavoidable progress toward the error while the free segments show choices that, if avoided, may have prevented the error. Hence, the demarcation into segments tends to highlight critical events. The segmentation of a trace raises the questions of whether the fated segment should indeed be inevitable and whether the free segments are critical in causing the error\u00a0\u2026", "num_citations": "38\n", "authors": ["1596"]}
{"title": "CU Decision Diagram Package Release 2.3. 0\n", "abstract": " CiNii \u8ad6\u6587 - CU Decision Diagram Package Release 2.3.0 CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3 \u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 CU Decision Diagram Package Release 2.3.0 SOMENZI Fabio \u88ab\u5f15\u7528 \u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI Fabio \u53ce\u9332\u520a\u884c\u7269 http://vlsi.colorado.edu/\u301cfabio/ http://vlsi.colorado.edu/\u301cfabio/, 1998 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 CCTomato : CMOS\u8ad6\u7406\u30bb\u30eb\u306e\u7279\u6027\u62bd\u51fa\u30c4\u30fc\u30eb\u306e\u958b\u767a \u5409\u51a8 \u5c06 , \u5c0f\u5ddd \u516c\u88d5 , \u68ee\u6c38 \u8aa0\u4e5f , \u6728\u6751 \u664b\u4e8c \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. DC, \u30c7\u30a3\u30da\u30f3\u30c0\u30d6\u30eb \u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0 102(479), 71-78, 2002-11-21 \u53c2\u8003\u6587\u732e8\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10020989626 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 \u2026", "num_citations": "38\n", "authors": ["1596"]}
{"title": "Proving more properties with bounded model checking\n", "abstract": " Bounded Model Checking, although complete in theory, has been thus far limited in practice to falsification of properties that were not invariants. In this paper we propose a termination criterion for all of LTL, and we show its effectiveness through experiments. Our approach is based on converting the LTL formula to a B\u00fcchi automaton so as to reduce model checking to the verification of a fairness constraint. This reduction leads to one termination criterion that applies to all formulae. We also discuss cases for which a dedicated termination test improves bounded model checking efficiency.", "num_citations": "37\n", "authors": ["1596"]}
{"title": "Refining the SAT decision ordering for bounded model checking\n", "abstract": " Bounded Model Checking (BMC) relies on solving a sequence of highly correlated Boolean satisfiability (SAT) problems, each of which checks for the existence of counter-examples of a bounded length. The performance of SAT search depends heavily on the variable decision ordering. We propose an algorithm to exploit the correlation among different SAT problems in BMC, by predicting and successively refining a partial variable ordering. This ordering is based on the analysis of all previous unsatisfiable instances, and is combined with the SAT solver's existing decision heuristic to determine the final variable decision ordering. Experiments on real designs from industry show that our new method improves the performance of SAT-based BMC significantly.", "num_citations": "37\n", "authors": ["1596"]}
{"title": "Symbolic algorithms to calculate steady-state probabilities of a finite state machine\n", "abstract": " In this paper we present two symbolic algorithms to compute the steady-state probabilities for very large finite state machines. These algorithms, based on Algebraic Decision Diagrams (ADD's)/spl minus/an extension of BDDs that allows arbitrary values to be associated with the terminal nodes of the diagrams/spl minus/determine the steady-state probabilities by regarding finite state machines as homogeneous, discrete-parameter Markov chains with finite state spaces, and by solving the corresponding Chapman-Kolmogorov equations. We have implemented two solution techniques: one is based on the Gauss-Jacobi iteration, and the other one on simple matrix multiplication, we report the experimental results obtained for problems with over 10/sup 8/ unknowns in irreducible form.< >", "num_citations": "37\n", "authors": ["1596"]}
{"title": "IC3: where monolithic and incremental meet\n", "abstract": " IC3 is an approach to the verification of safety properties based on relative induction. It is incremental in the sense that instead of focusing on proving one assertion, it builds a sequence of small, relatively easy lemmas. These lemmas are in the form of clauses that are derived from counterexamples to induction and that are inductive relative to reachability assumptions. At the same time, IC3 progressively refines approximations of the states reachable in given numbers of steps. These approximations, also made up of clauses, are among the assumptions used to support the inductive reasoning, while their strengthening relies on the inductive clauses themselves. This interplay of the incremental and monolithic approaches lends IC3 efficiency and flexibility and produces high-quality property-driven abstractions. In contrast to other SAT-based approaches, IC3 performs very many, very inexpensive queries. This is\u00a0\u2026", "num_citations": "34\n", "authors": ["1596"]}
{"title": "On the optimization power of retiming and resynthesis transformations\n", "abstract": " Retiming and resynthesis transformations can be used for optimizing the area, power, and delay of sequential circuits. Even though this technique has been known for more than a decade, its exact optimization capability has not been formally established. We show that retiming and resynthesis can exactly implement 1-step equivalent state transition graph transformations. This result is the strongest to date. We also show how the notions of retiming and resynthesis can be moderately extended to achieve more powerful state transition graph transformations. Our work will provide theoretical foundation for practical retiming and resynthesis based optimization and verification.", "num_citations": "32\n", "authors": ["1596"]}
{"title": "Lazy group sifting for efficient symbolic state traversal of FSMs\n", "abstract": " Proposes lazy group sifting for dynamic variable reordering during state traversal of finite state machines (FSMs). The proposed method relaxes the idea of pairwise grouping of the present state variables and their corresponding next state variables. This is done to produce better variable orderings during image computation without causing BDD (binary decision diagram) size blowup in the substitution of next state variables with present state variables at the end of image computation. Experimental results show that our approach is more robust in state traversal than the approaches that either unconditionally group variable pairs or never group them.", "num_citations": "31\n", "authors": ["1596"]}
{"title": "Minimization of Boolean relations\n", "abstract": " The authors present a Quine-McCluskey-like method for the minimization of Boolean relations. They develop the notion of prime implications for Boolean relations and give a procedure for generating all primes. Using these primes, the minimization problem is formulated as a linear integer (0-1) program with a special structure. The authors show that this can be solved as a binate covering problem. The notions of essential and of row and column dominance are presented as bounding techniques for solving this covering problem using a branch-and-bound method.< >", "num_citations": "31\n", "authors": ["1596"]}
{"title": "Cudd: Colorado university decision diagram package, release 2.3. 1\n", "abstract": " CiNii \u8ad6\u6587 - CUDD : Colorado university decision diagram package, release 2.3.1 CiNii \u56fd\u7acb \u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e \u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 CUDD : Colorado university decision diagram package, release 2.3.1 SOMENZI F. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI F. \u53ce\u9332\u520a\u884c\u7269 http://vlsi.colorado.edu/\u301cfabio/CUDD/ http://vlsi.colorado.edu/\u301cfabio/CUDD/ \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 An Integrated Approach of Variable Ordering and Logic Mapping into LUT-Array-Based PLD(<Special Section>Selected Papers from the 17th Workshop on Circuits and Systems in Karuizawa) IZUMI Tomonori , KOUYAMA Shinichi , OCHI Hiroyuki , NAKAMURA Yukihiro IEICE \u2026", "num_citations": "30\n", "authors": ["1596"]}
{"title": "Automatic generation of network invariants for the verification of iterative sequential systems\n", "abstract": " In this paper we present new results on the verification of iterative sequential systems. We address bilateral interconnections and circular arrangements of cells, and extend our previous treatment [13] of unilateral systems. Our approach is based on the new definition of boundedness, given in terms of regularity of the union of an infinite number of languages. The definition of boundedness provides sufficient conditions for the equivalence of iterative systems to be decidable. It also provides network invariants for inductive proofs. This new framework allows the derivation of some previously known results as well as the new ones presented here.", "num_citations": "30\n", "authors": ["1596"]}
{"title": "Safety first: a two-stage algorithm for the synthesis of reactive systems\n", "abstract": " In the game-theoretic approach to the synthesis of reactive systems, specifications are often expressed as \u03c9-regular languages. Computing a winning strategy to an infinite game whose winning condition is an \u03c9-regular language is then the main step in obtaining an implementation. Conjoining all the properties of a specification to obtain a monolithic game suffers from the doubly exponential determinization that is required. Despite the success of symbolic algorithms, the monolithic approach is not practical. Existing techniques achieve efficiency by imposing restrictions on the \u03c9-regular languages they deal with. In contrast, we present an approach that achieves improvement in performance through the decomposition of the problem while still accepting the full set of \u03c9-regular languages. Each property is translated into a deterministic \u03c9-regular automaton explicitly while the two-player game defined by the\u00a0\u2026", "num_citations": "29\n", "authors": ["1596"]}
{"title": "Using combinational verification for sequential circuits\n", "abstract": " Retiming combined with combinational optimization is a powerful sequential synthesis method. However, this methodology has not found wide application because formal sequential verification is not practical and current simulation methodology requires the correspondence of latches disallowing any movement of latches. We present a practical verification technique which permits such sequential synthesis for a class of circuits. In particular, we require certain constraints to be met on the feedback paths of the latches involved in the retiming process. For a general circuit, we can satisfy these constraints by fixing the location of some latches, eg, by making them observable. We show that equivalence checking after performing repeated retiming and synthesis on this class of circuit reduces to a combinational verification problem. We also demonstrate that our methodology covers a large class of circuits by applying it\u00a0\u2026", "num_citations": "29\n", "authors": ["1596"]}
{"title": "A state space decomposition algorithm for approximate FSM traversal\n", "abstract": " Approximate FSM traversal is very useful in many applications of interest, when the size of the problem to be solved is too large to be handled by exact methods. The quality of the approximation strongly depends on how state variables are partitioned to decompose the state space, and exploiting circuit structure appears to be the most promising technique to determine a good state variable partition. In this paper we formulate the state space decomposition problem as a graph problem to be solved on the latch connection graph the FSM under investigation. Structural analysis on that graph is used to determine an initial partition; breaking and aggregation, based on seed generation, clustering, and iterative refinement of the current result, are then performed on the initial solution. Approximate FSM traversal results obtained on the largest ISCAS89 examples show the effectiveness of the proposed algorithm.< >", "num_citations": "29\n", "authors": ["1596"]}
{"title": "Variable ordering for binary decision diagrams\n", "abstract": " Considers the problem of variable ordering in binary decision diagrams (BDD's). The authors present several heuristics for finding a good variable ordering based on the algebraic structure of the functions. They provide a non-interleaving theorem and an accurate cost formula for the optimal ordering. They treat the output ordering problem when a given circuit has multiple outputs and propose new heuristics for the problem and experimental results which enables a quick comparison of some existing heuristics and the proposed heuristics.<>", "num_citations": "28\n", "authors": ["1596"]}
{"title": "Redundancy identification and removal based on implicit state enumeration\n", "abstract": " The knowledge of the state transition graph (STG) of a sequential circuit helps in generating test sequences and identifying redundancies. The application of algorithms to the identification and removal of redundancies is reported. This strategy is based on traversing the STG of the given circuit and then performing redundancy identification using the reachability information calculated by the traversal. This method considers one candidate redundancy at a time, in an order that tries to minimize the total processing time. Substantial area and delay reductions are achieved. Experiments show that for many circuits 100% of the sequentially redundant faults can be eliminated in very reasonable amounts of time.<>", "num_citations": "28\n", "authors": ["1596"]}
{"title": "Boolean techniques for low power driven re-synthesis\n", "abstract": " We present a boolean technique to reduce power consumption of combinational circuits that have already been optimized for area and delay and then mapped onto a library of gates. In order to achieve a better optimization, we cluster gates by collapsing two or more levels of gates into a single node. When optimizing each cluster, our method extends the algorithms used in ESPRESSO, by adding heuristics that bias the minimization toward lowering the power dissipation in the circuit. The results of our method, on a number of benchmark circuits, show an average of 11% improvement in power savings compared to existing boolean techniques.", "num_citations": "27\n", "authors": ["1596"]}
{"title": "Abstraction refinement in symbolic model checking using satisfiability as the only decision procedure\n", "abstract": " We present an abstraction refinement algorithm for model checking of safety properties that relies exclusively on a SAT solver for checking the abstract model, testing abstract counterexamples on the concrete model, and refinement. Model checking of the abstractions is based on bounded model checking extended with checks for the existence of simple paths that help in deciding passing properties. All minimum-length spurious counterexamples are eliminated in one refinement step by an incremental procedure that combines the analysis of the conflict dependency graph produced by the SAT solver while looking for concrete counterexamples with an effective refinement minimization procedure.", "num_citations": "26\n", "authors": ["1596"]}
{"title": "Fine-grain conjunction scheduling for symbolic reachability analysis\n", "abstract": " In symbolic model checking, image computation is the process of computing the successors of a set of states. Containing the cost of image computation depends critically on controlling the number of variables that appear in the functions being manipulated; this in turn depends on the order in which the basic operations of image computation-conjunctions and quantifications-are performed. In this paper we propose an approach to this ordering problem-the conjunction scheduling problem-that is especially suited to the case in which the transition relation is specified as the composition of many small relations. (This is the norm in hardware verification.) Our fine-grain approach leads to the formulation of conjunction scheduling in terms of minimum max-cut linear arrangement, an NP-complete problem for which efficient heuristics have been developed. The cut whose width is minimized is related to the number\u00a0\u2026", "num_citations": "26\n", "authors": ["1596"]}
{"title": "Fundamental CAD algorithms\n", "abstract": " Computer-aided design (CAD) tools are now making it possible to automate many aspects of the design process. This has mainly been made possible by the use of effective and efficient algorithms and corresponding software structures. The very large scale integration (VLSI) design process is extremely complex, and even after breaking the entire process into several conceptually easier steps, it has been shown that each step is still computationally hard. To researchers, the goal of understanding the fundamental structure of the problem is often as important as producing a solution of immediate applicability. Despite this emphasis, it turns out that results that might first appear to be only of theoretical value are sometimes of profound relevance to practical problems, VLSI CAD is a dynamic area where problem definitions are continually changing due to complexity, technology and design methodology. In this paper\u00a0\u2026", "num_citations": "26\n", "authors": ["1596"]}
{"title": "Efficient abstraction refinement in interpolation-based unbounded model checking\n", "abstract": " It has been pointed out by McMillan that modern satisfiability (SAT) solvers have the ability to perform on-the-fly model abstraction when examining it for the existence of paths satisfying certain conditions. The issue has therefore been raised of whether explicit abstraction refinement schemes still have a role to play in SAT-based model checking. Recent work by Gupta and Strichman has addressed this issue for bounded model checking (BMC), while in this paper we consider unbounded model checking based on interpolation. We show that for passing properties abstraction refinement leads to proofs that often require examination of shorter paths. On the other hand, there is significant overhead involved in computing efficient abstractions. We describe the techniques we have developed to minimize such overhead to the point that even for failing properties the abstraction refinement scheme remains\u00a0\u2026", "num_citations": "25\n", "authors": ["1596"]}
{"title": "CUDD decision diagram package\n", "abstract": " CiNii \u8ad6\u6587 - CUDD decision diagram package CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3 ] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7 \u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 CUDD decision diagram package SOMENZI F. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI F. \u53ce\u9332\u520a\u884c\u7269 http://bessie.colorado.edu\u301c/fabio/CUDD http://bessie.colorado.edu\u301c/fabio/CUDD \u88ab\u5f15\u7528\u6587\u732e : 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 XML Framework for Various Types of Decision Diagrams for Discrete Functions STANKOVIC Stanislav , ASTOLA Jaakko IEICE transactions on information and systems 90(11), 1731-1740, 2007-11-01 \u53c2\u8003\u6587\u732e23\u4ef6 \u5927\u5b66\u5171\u540c\u5229\u7528\u6a5f\u95a2\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10026185087 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 \u2026", "num_citations": "25\n", "authors": ["1596"]}
{"title": "A hybrid algorithm for LTL games\n", "abstract": " In the game theoretic approach to the synthesis of reactive systems, specifications are often given in linear time logic (LTL). Computing a winning strategy to an infinite game whose winning condition is the set of LTL properties is the main step in obtaining an implementation. We present a practical hybrid algorithm\u2014a combination of symbolic and explicit algorithm\u2014for the computation of winning strategies for unrestricted LTL games that we have successfully applied to synthesize reactive systems with up to 1011 states.", "num_citations": "24\n", "authors": ["1596"]}
{"title": "Strong conflict analysis for propositional satisfiability\n", "abstract": " We present a new approach to conflict analysis for propositional satisfiability solvers based on the DPLL procedure and clause recording. When conditions warrant it, we generate a supplemental clause from a conflict. This clause does not contain a unique implication point, and therefore cannot replace the standard conflict clause. However, it is very effective at reducing excessive depth in the implication graphs and at preventing repeated conflicts on the same clause. Experimental results show consistent improvements over state-of-the-art solvers and confirm our analysis of why the new technique works", "num_citations": "23\n", "authors": ["1596"]}
{"title": "Power optimisation of FPGA-based designs without rewiring\n", "abstract": " A new technique is proposed to perform power-oriented reconfiguration of combinational circuits implemented using look-up table (LUT)-based FPGAs. The main features of this approach are: exploitation of functional flexibilities, concurrent optimisation of multiple LUTs based on Boolean relations, and in-place reprogramming without replacement and rewiring. The tool optimises the combinational component of the configurable logic blocks (CLBs) after layout, and does not necessitate any rerouting or rewiring. Hence, delay and CLB usage are left unchanged, while power is minimised. As the algorithm operates locally on the various LUT clusters of the network, it is applicable and best performs on large examples as demonstrated by our experimental results: an average power reduction of 11.5% has been obtained on standard benchmark circuits.", "num_citations": "23\n", "authors": ["1596"]}
{"title": "CUDD package, Release 2.3. 1\n", "abstract": " CiNii \u8ad6\u6587 - CUDD package, Release 2.3.1 CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3 ] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9 \u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 CiNii Research\u30d7\u30ec\u7248\u306e\u516c\u958b\u306b\u3064\u3044\u3066 CUDD package, Release 2.3.1 SOMENZI F. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI F. \u53ce\u9332\u520a\u884c\u7269 http://vlsi.Colorado.EDU/\u301cfabio/CUDD/cuddIntro.html http://vlsi.Colorado.EDU/\u301cfabio/CUDD/cuddIntro.html \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30ec\u30a4\u30eb\u6570\u306b\u5236\u9650\u306e\u3042\u308bLUT\u30ab\u30b9\u30b1\u30fc\u30c9\u306e\u8ad6\u7406\u5408\u6210\u6cd5 : \u591a\u51fa\u529b \u8ad6\u7406\u95a2\u6570\u306e\u76f4\u63a5\u5b9f\u73fe \u30df\u30b7\u30e5\u30c1\u30a7\u30f3\u30b3 \u30a2\u30e9\u30f3 , \u7b39\u5c3e \u52e4 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. VLD, VLSI\u8a2d\u8a08 \u6280\u8853 102(476), 103-108, 2002-11-21 \u53c2\u8003\u6587\u732e17\u4ef6 \u88ab\u5f15\u7528\u6587\u732e5\u4ef6 \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u8a2d\u7acb20\u2026", "num_citations": "23\n", "authors": ["1596"]}
{"title": "Termination criteria for bounded model checking: Extensions and comparison\n", "abstract": " Increasing attention has been paid recently to criteria that allow one to conclude that a structure models a linear-time property from the knowledge that no counterexamples exist up to a certain length. These termination criteria effectively turn Bounded Model Checking into a full-fledged verification technique and sometimes result in considerable time savings. In [M. Awedh and F. Somenzi. Proving more properties with bounded model checking. In R. Alur and D. Peled, editors, Sixteenth Conference on Computer Aided Verification (CAV'04), pages 96\u2013108. Springer-Verlag, Berlin, July 2004. LNCS 3114] we presented a criterion based on the translation of the linear-time specification into a B\u00fcchi automaton. BMC can be terminated if no fair cycle is found up to a given length, and one can prove that no fair cycle exists beyond that length. The maximum length for which counterexamples are explicitly checked is called\u00a0\u2026", "num_citations": "22\n", "authors": ["1596"]}
{"title": "In-place power optimization for LUT-based FPGAs\n", "abstract": " This paper presents a new technique to perform power-oriented re-configuration of a system implemented using LUT FPGAs. The main features of our approach are: Accurate exploitation of degrees of freedom, concurrent optimization of multiple LUTs based on Boolean relations, and in-place re-programming without re-routing. Our tool optimizes the combinational component of the CLBs after layout, and does not require any re-wiring. Hence, delay and CLB usage are left unchanged, while power is minimized. As the algorithm operates locally on the various LUT clusters, it best performs on large examples as demonstrated by our experimental results: An average power reduction of 20.6% has been obtained on standard benchmarks.", "num_citations": "22\n", "authors": ["1596"]}
{"title": "A new algorithm for 0-1 programming based on binary decision diagrams\n", "abstract": " The Binate Covering Problem (BCP) is the problem of finding a minimum cost assignment to variables that is a solution of a boolean equation f = 1. It is a generalization of the set covering (or unate covering) problem, where f is positive unate, and is generally given as a table with rows corresponding to the set elements and the columns corresponding to the subsets.             Previous methods have considered the case when f is given as a product-of-sum formula or as a binary decision diagram (BDD). In this paper we present a new branchand-bound algorithm for the BCP, that assumes f is expressed as the conjunction of multiple BDD\u2019s.             In general all 0-1 integer linear programs can be translated into a binate covering problem. However, if the characteristic function is represented as a product of sums, the number of clauses may exceed the number of linear constraints by so far as to render the method\u00a0\u2026", "num_citations": "22\n", "authors": ["1596"]}
{"title": "Verification of systems containing counters\n", "abstract": " It is pointed out that systems containing counters have very large and deep state spaces, and the verification of properties on these systems can be very expensive in terms of memory space and computation time. A technique for automatically reducing the state space associated with the system on which some properties that can express both safeness and fairness constraints have to be proved is presented. In particular, a set of conditions upon which some counters can be reduced to three-state, nondeterministic machines is given. The controllers can be simplified by removing the redundancy induced by their interaction with the counter, so that the verification tasks can be more easily performed.< >", "num_citations": "22\n", "authors": ["1596"]}
{"title": "Fast and coherent simulation with zero delay elements\n", "abstract": " This paper addresses the problem of event-directed logic simulation with part of the elements having zero delay. Incoherences arising from spikes having null duration (i.e., multiple transitions of a signal at a given simulation time, due to the simulation algorithm) are solved by a Two-Pass procedure, combining levelizing and event-driven simulation and yielding Ordered Activity Propagation. To overcome the speed degradation with respect to One-Pass simulation, a variation of the usual Two-Pass technique, termed Predictor/Corrector, is introduced at the gate level. Ordered Activity Propagation is also beneficial to concurrent multilevel simulation.", "num_citations": "22\n", "authors": ["1596"]}
{"title": "Symbolic computation of logic implications for technology-dependent low-power synthesis\n", "abstract": " This paper presents a novel technique for re-synthesizing circuits for low-power dissipation. Power consumption is reduced through redundancy addition and removal by using learning to identify indirect logic implications within a circuit. Such implications are exploited by adding gates and connections to the circuit without altering its overall behavior and thereby enabling us to eliminate other, high power dissipating, nodes. We propose a new BDD-based method for computing indirect implications in a logic network; furthermore, we present heuristic techniques to perform redundancy addition and removal without destroying the topology of the mapped circuit. Experimental results show the effectiveness of the proposed technique in reducing power while keeping within delay and area constraints.", "num_citations": "21\n", "authors": ["1596"]}
{"title": "Testing strategy and technique for macro-based circuits\n", "abstract": " The increasing complexity of VLSI systems demands structured approaches to reduce both design time and test generation effort. PLA's and scan paths have both been widely reported to be efficient in this sense. This correspondence presents an easily testable structure and its related testing strategies. The circuits are assumed to be based on the interconnection of combinatorial macros, mostly implemented by PLA's; tests are generated locally, considering the involved macro as an isolated item, and then are expressed in terms of primary inputs and outputs using a topological approach as general strategy and algebraic techniques for the propagation of signals through macros. Propagation is dealt with by new algorithms. Since the problem of test generation is NP-hard, a set of heuristics is introduced to keep the amount of computation reasonable; several implementation issues are finally investigated.", "num_citations": "21\n", "authors": ["1596"]}
{"title": "Efficient Term-ITE conversion for satisfiability modulo theories\n", "abstract": " This paper describes how term-if-then-else (term-ITE) is handled in Satisfiability Modulo Theories (SMT) and to decide Linear Arithmetic Logic (LA) in particular. Term-ITEs allow one to conveniently express verification conditions; hence, they are very common in practice. However, the theory provers of SMT solvers are usually designed to work on conjunctions of literals; therefore, the input formulae are rewritten so as to eliminate term-ITEs. The challenge in rewriting is to avoid introducing too many new variables, while avoiding as often as possible the exponential explosion that is frequent when a naive approach is applied. We propose a solution that is based on cofactoring and theory propagation, which often produces orders-of-magnitude speedups in several SMT solvers for LA problems.", "num_citations": "20\n", "authors": ["1596"]}
{"title": "Power and delay reduction via simultaneous logic and placement optimization in FPGAs\n", "abstract": " Traditional FPGA design flows have treated logic synthesis and physical design as separate steps. With the recent advances in technology, the lack of information on the physical implementation during logic synthesis has caused mismatches between the final circuit characteristics (delay, power and area) and those predicted by logic synthesis. In this paper, we present a technique that tightly links the logic and physical domains\u2014we combine logic and placement optimization in a single step. The combined algorithm is based on simulated annealing and hence, very amenable to new optimization goals or constraints. Two types of moves, directed towards global reduction in the cost function (linear congestion), are accepted by the simulated annealing algorithm:(1) logic optimization steps consisting of removing or replacing redundant wires in a circuit using functional flexibilities derived from SPFDs [12] and (2) the\u00a0\u2026", "num_citations": "20\n", "authors": ["1596"]}
{"title": "Efficient fixpoint computation for invariant checking\n", "abstract": " Techniques for the computation of fixpoints are key to the success of many formal verification algorithms. To be efficient, these techniques must take into account how sets of states are represented. When BDDs are used, this means controlling, directly or indirectly, the size of the BDDs. Traditional fixpoint computations do little to keep BDD sizes small, apart from reordering variables. In this paper, we present a new strategy that attempts to keep the size of the BDDs under control at every stage of the computation. Our contribution includes also new techniques to compute partial images, and to speed up and test convergence. We present experimental results that prove the effectiveness of our strategy by demonstrating up to 40 orders of magnitude improvement in the number of states computed.", "num_citations": "19\n", "authors": ["1596"]}
{"title": "Don't cares and global flow analysis of Boolean networks\n", "abstract": " External, intermediate, and fan-out don't care sets have been used to describe information about network structure required to optimize a node of a Boolean network locally. Another method to optimize a network has been called global flow analysis. The authors relate these approaches, generalize global flow to arbitrary Boolean networks, and suggest new algorithms for these problems.<>", "num_citations": "19\n", "authors": ["1596"]}
{"title": "Formal controller synthesis for continuous-space MDPs via model-free reinforcement learning\n", "abstract": " A novel reinforcement learning scheme to synthesize policies for continuous-space Markov decision processes (MDPs) is proposed. This scheme enables one to apply model-free, off-the- shelf reinforcement learning algorithms for finite MDPs to compute optimal strategies for the corresponding continuous-space MDPs without explicitly constructing the finite-state abstraction. The proposed approach is based on abstracting the system with a finite MDP (without constructing it explicitly) with unknown transition probabilities, synthesizing strategies over the abstract MDP, and then mapping the results back over the concrete continuous-space MDP with approximate optimality guarantees. The properties of interest for the system belong to a fragment of linear temporal logic, known as syntactically co-safe linear temporal logic (scLTL), and the synthesis requirement is to maximize the probability of satisfaction within a\u00a0\u2026", "num_citations": "18\n", "authors": ["1596"]}
{"title": "The performance of the concurrent fault simulation algorithms in MOZART\n", "abstract": " MOZART is a concurrent fault simulator for large circuits described in the RT, functional gate, and switch levels. Performance is gained by means of techniques aimed at the reduction of unnecessary activity. Two such techniques are levelized two-pass simulation, which minimizes the number of events and evaluations, and list event scheduling, which allows optimized processing of simultaneous (fraternal) events for concurrent machines. Moreover, efficient handling of abnormally large or active faulty machines can dramatically improve fault simulator performance. These and related issues are discussed, and analytical and experimental evidence is provided for the effectiveness of the solutions adopted in MOZART. A novel performance metric is introduced for fault simulation that is based on comparison with the serial algorithm and is more accurate than those currently used.< >", "num_citations": "18\n", "authors": ["1596"]}
{"title": "PART: Programmable ARray Testing based on a PARTitioning algorithm\n", "abstract": " PART is a system for PLA testing and design verification, intended to be properly interfaced with other existing tools to generate a comprehensive design environment. To this purpose, it provides several facilities, among which the capability of generating a fault population on the basis of layout information. PART aims at producing a very compact test set for all detectable crosspoint defects, using limited amounts of run time and storage. This is achieved by means of an efficient partitioning algorithm together with powerful heuristics. Test minimality is ensured by a simple procedure. In the present paper these are discussed, experimental results are given and a comparison with competing strategies is made.", "num_citations": "18\n", "authors": ["1596"]}
{"title": "Application of formal word-level analysis to constrained random simulation\n", "abstract": " Constrained random simulation is supported by constraint solvers integrated within simulators. These constraint solvers need to be fast and memory efficient to maintain simulation performance. Binary Decision Diagrams (BDDs) have been successfully applied to represent constraints in this context. However, BDDs are vulnerable to size explosions depending on the constraints they are representing and the number of Boolean variables appearing in them. In this paper, we present a word-level analysis tool DomRed to reduce the number of Boolean variables required to represent constraints by reducing the domain of constraint variables. DomRed employs static analysis techniques to obtain these reductions. We present experimental results to illustrate the impact of this tool.", "num_citations": "17\n", "authors": ["1596"]}
{"title": "Optimizing sequential verification by retiming transformations\n", "abstract": " Sequential verification methods based on reachability analysis are still limited by the size of the BDDs involved in computations. Extending their applicability to larger and real circuits is still a key issue.", "num_citations": "17\n", "authors": ["1596"]}
{"title": "A symbolic algorithm for maximum flow in 0-1 networks\n", "abstract": " We present an algorithm for finding the maximum flow in a 0-1 network. The algorithm is symbolic and avoids explicit enumeration of the nodes and edges of the network. Therefore, it can handle much larger graphs than it was previously possible (more than 10/sup 36/ edges). The main idea is to trace (implicitly) sets of edge-disjoint augmenting paths. Disjointness is enforced by solving an edge matching problem for each layer of the network with the help of newly defined priority functions.", "num_citations": "17\n", "authors": ["1596"]}
{"title": "An iterative algorithm for the binate covering problem\n", "abstract": " Many problems of interest in the field of electronic design automation can be formulated as binate covering problems. This occurs whenever the solution sought for is the minimum cost assignment satisfying a Boolean formula in conjunctive form. Practical problems may have hundreds of variables and thousands of clauses; hence efficient techniques are important. The paper proposes a new iterative scheme that is based on solving a sequence of sub-problems that normally lead to the solution of the original problem in a fraction of the time.<>", "num_citations": "17\n", "authors": ["1596"]}
{"title": "Automatic invariant strengthening to prove properties in bounded model checking\n", "abstract": " In this paper, we present a method that helps improve the performance of bounded model checking by automatically strengthening invariants so that the termination proof may be obtained by analyzing shorter paths. The strengthening technique identifies sets of states as byproducts of the termination checks. It then uses SAT-based preimage computations to extend those sets. Our approach may substantially speed up the verification of both failing and passing properties. We present experimental results showing that our new method improves the performance of BMC significantly", "num_citations": "16\n", "authors": ["1596"]}
{"title": "A satisfiability-based approach to abstraction refinement in model checking\n", "abstract": " We present an abstraction refinement algorithm for model checking of safety properties that relies exclusively on a SAT solver for checking the abstract model, testing abstract counterexamples on the concrete model, and refinement. Model checking of the abstractions is based on bounded model checking extended with checks for the existence of simple paths that help in deciding passing properties. All minimum-length spurious counterexamples are eliminated in one refinement step by a procedure that combines the analysis of the conflict dependency graph produced by the SAT solver while looking for concrete counterexamples with an effective abstraction minimization procedure.", "num_citations": "16\n", "authors": ["1596"]}
{"title": "Modular verification of multipliers\n", "abstract": " We present a new method for the efficient verification of multipliers and other arithmetic circuits. It is based on modular arithmetic like Kimura's approach, and on composition, like Hamaguchi's approach. It differs from both in several important respects, which make it more robust. The technique builds the residue Algebraic Decision Diagram (ADD) of as many variables as the number of outputs in the multiplier and composes the implementation circuit from the outputs to the inputs into the residue. Finally, the residue ADD is checked against the specification.", "num_citations": "16\n", "authors": ["1596"]}
{"title": "Multiple observation time single reference test generation using synchronizing sequences\n", "abstract": " A synchronizing sequence drives a circuit from an arbitrary power-up state into a unique state. A framework and algorithms for test generation based on the multiple observation time strategy are developed by taking advantage of synchronizing sequences. Though it has been shown that the multiple observation time strategy can provide a higher fault coverage than the conventional single observation time strategy, until now the multiple observation time strategy has required a very complex tester operation model and its overhead. However, when a circuit is synchronizable, test generation can employ the multiple observation time strategy and provide better fault coverages, while using the conventional tester operation model. It is shown that the same fault coverage can be achieved in both tester operation models if the circuit under test generation is synchronizable. The authors investigate how a synchronizing\u00a0\u2026", "num_citations": "16\n", "authors": ["1596"]}
{"title": "Inductive verification of iterative systems\n", "abstract": " Recent advances in binary decision diagram (BDD)-based algorithms have brought much larger circuits than before within the reach of verification programs. The authors show how inductive proof procedures can derive information on regular circuits in optimal time, e.g. they can perform reachability analysis in linear time or check the equivalence of two iterative circuits in time independent of the length of the two arrays. The algorithms presented rely on the canonicity of BDDs to make the inductive steps efficient. The authors have experimented with the techniques described and compared them with the conventional techniques on a few examples.< >", "num_citations": "16\n", "authors": ["1596"]}
{"title": "Arithmetic boolean expression manipulator using BDDs\n", "abstract": " Recently, there has been a lot of works on LSI design systems using Binary Decision Diagrams (BDDs), which are efficient representations of Boolean functions. We previously developed a Boolean expression manipulator, that can quickly calculate Boolean expressions by using BDD techniques. It has greatly assisted us in developing VLSI design systems and solving combinatorial problems.               In this paper, we present an Arithmetic Boolean Expression Manipulator (BEM-II), that is also based on BDD techniques. BEM-II calculates Boolean expressions that contain arithmetic operations, such as addition, subtraction, multiplication and comparison, and then displays the results in various formats. It can solve problems represented by a set of equalities and inequalities, which are dealt with in 0-1 linear programming. We discuss the algorithm and data structure used for manipulating arithmetic Boolean\u00a0\u2026", "num_citations": "15\n", "authors": ["1596"]}
{"title": "Symbolic timing analysis and resynthesis for low power of combinational circuits containing false paths\n", "abstract": " This paper presents applications of algebraic decision diagrams (ADDs) to timing analysis and resynthesis for low power of combinational CMOS circuits. We first propose a symbolic algorithm to perform true delay calculation of a technology mapped network; the procedure we propose, implemented as an extension of the SIS synthesis system, is able to provide more accurate timing information than any other method presented so far; in particular, it is able to compute and store the arrival times of all the gates of the circuit for all possible input vectors, as opposed to the traditional methods which consider only the worst case primary inputs combination. Furthermore, the approach does not require any explicit false path elimination. We then extend our timing analysis tool to the symbolic calculation of required times and slacks, and we use this information to perform resynthesis for low power of the circuit by gate\u00a0\u2026", "num_citations": "14\n", "authors": ["1596"]}
{"title": "Remembrance of things past: Locality and memory in BDDs\n", "abstract": " Binary Decision Diagrams (BDDs) are efficient at manipulating large sets in a compact manner. BDDs, however, are inefficient at utilizing the memory hierarchy ofthe computer. Recent work addresses this problem by manipulating the BDDsin breath-first manner (BFS). BFS processing is quite successful at reducing the number of page faults when the BDDs do not fit in the available physical memory. When pagingdoes not take place, it is much less clear which paradigmleads to the better performance. In this paper, we perform adetailed analysis of BFS and DFS packages using simulationand direct performance monitoring ofthe memory hierarchy. We show that there is very little difference in TLB and cachemiss rates for DFS and BFS paradigms. We also show thatdifferences in execution time between carefully tuned BFSand DFS implementations are primarily a function of thelossless computed table used in BFS\u00a0\u2026", "num_citations": "14\n", "authors": ["1596"]}
{"title": "Making deduction more effective in SAT solvers\n", "abstract": " Satisfiability (SAT) solvers often benefit from transformations of the formula to be decided that allow them to do more through deduction and decrease their reliance on enumeration. For formulae in conjunctive normal form, subsumed clauses may be removed or partial resolution may be applied. The objectives of simplifying the formula and speeding up the solver are sometimes competing. We characterize existing transformations in terms of their impact on the deductive power of the formula and their effects on the sizes of the implication graphs. For example, we show that variable elimination works by improving implication graphs. We also present two new techniques that try to increase deductive power. The first is a check performed during the computation of resolvents. The second is a new preprocessing algorithm based on distillation that combines simplification and increase of deductive power. Most current\u00a0\u2026", "num_citations": "13\n", "authors": ["1596"]}
{"title": "The compositional far side of image computation\n", "abstract": " Symbolic image computation is the most fundamental computation in BDD-based sequential system optimization and formal verification. In this paper, we explore the use of over-approximation and BDD minimization with don't cares during image computation. Our new method, based on the partitioned representation of the transition relation, consists of three phases: First, the model is treated as a set of loosely coupled components, and over-approximate images are computed to minimize the transition relation of each component. A refined overall image is then computed using the simplified transition relation. Finally, the exact image is obtained by a clipping operation that recovers all previous over-approximations. Since BDD minimization employs constraints on the next-state variables of the transition relation, instead of the customary constraints on the present-state variables, we call the resulting method far side\u00a0\u2026", "num_citations": "13\n", "authors": ["1596"]}
{"title": "Sequential logic optimization based on state space decomposition\n", "abstract": " Binary decision diagrams (BDDs) and implicit state enumeration have provided remarkable improvements to sequential logic synthesis, testing, and verification in recent years. However, their inability to deal with large circuits has been the major limitation of the methods based on them. A method to compute a subset of unreachable states using implicit state enumeration, which can be applied to large circuits where the current exact methods fail is presented. Since it is guaranteed that the computed unreachable state set is contained in the exact unreachable state set, this set can be used as invalid state don't cares in sequential logic optimization. Traversal and optimization results on benchmark examples are provided.< >", "num_citations": "13\n", "authors": ["1596"]}
{"title": "Decomposing image computation for symbolic reachability analysis using control flow information\n", "abstract": " The main challenge in BDD-based symbolic reachability analysis is represented by the sizes of the intermediate decision diagrams obtained during image computations. Methods proposed to mitigate this problem fall broadly into two categories: Search strategies that depart from breadth-first search, and efficient techniques for image computation. In this paper we present an algorithm that belongs to the latter category. It exploits define-use information along executable paths extracted from the control-flow graph of the model being analyzed; this information enables an effective constraining of the transition relation and a decomposition of the image computation process that often leads to much smaller intermediate BDDs. Our experiments confirm that this reduction in the size of the representation of state sets translates in significant decreases in CPU and memory requirements.", "num_citations": "12\n", "authors": ["1596"]}
{"title": "An ADD-based algorithm for shortest path back-tracing of large graphs\n", "abstract": " Symbolic computation techniques play a fundamental role in logic synthesis and formal hardware verification algorithms. Recently, Algebraic Decision Diagrams, i.e., BDDs with a set of constant values different to the set /spl lcub/0,1/spl rcub/, have been used to solve general purpose problems, such as matrix multiplication, shortest path calculation, and solution of linear systems, as well as logic synthesis and formal verification problems, such as timing analysis, probabilistic analysis of finite state machines, and state space decomposition for approximate finite state machine traversal. ADD-based procedures for single-source and all-pairs shortest path weight calculation have appeared to be very effective for the manipulation of large graphs (over 10/sup 27/ vertices and 10/sup 36/ edges). However, for those procedures to be applicable to real problems, for example flow network problems, computing only shortest\u00a0\u2026", "num_citations": "12\n", "authors": ["1596"]}
{"title": "An application of ADD-based timing analysis to combinational low power re-synthesis\n", "abstract": " Power dissipation in technology mapped circuits can be reduced by performing gate re-sizing, that is, slowing down gates without decreasing the speed of the logic network. Recently we have proposed a symbolic procedure which exploits the compactness of the ADD data structure to accurately calculate the arrival times at each node of a circuit for any primary input vector. In this paper we extend our timing analysis tool to the symbolic calculation of required times and slacks, and we use this information to identify gates of the circuit that can be re-sized. The nice feature of our approach is that it takes into account the presence of false paths naturally, that is, false paths in the network do not have to be handled by ad-hoc techniques. As shown by the experimental results, circuits re-synthesized with the technique we present in this paper are guaranteed to be at least as fast as the original implementations, but smaller and substantially less power-consuming. Our methods inproves...", "num_citations": "12\n", "authors": ["1596"]}
{"title": "Periodic signal suppression in a concurrent fault simulator\n", "abstract": " Clock suppression has been proposed to take advantage of the periodic signals such as the clock present in synchronous designs. In clock suppression, no events due to the clock input are generated, but the information can be reconstructed as needed. In this paper, the authors present periodic signal suppression, which is a generalized form of clock suppression, as a means to suppress predictable events of all periodic signals throughout the circuit. To do this, a special signal state labeled P is introduced. P states indicate that signals are periodic, but cause no unnecessary activity in an event driven simulator. At any time, the original waveform can be reconstructed from the periodic signal. This general implementation allows clock suppression to work on divided or gated clocks and on signals that may alternate periodic and non-periodic behavior in addition to the clock tree. Moreover, concurrent simulation of\u00a0\u2026", "num_citations": "12\n", "authors": ["1596"]}
{"title": "Fault simulation for general FCMOS ICs\n", "abstract": " This work presents a technique to correctly deal with non-stuck-at faults in FCMOS circuits making use of complex macrogates. This method can be applied to any gate-level fault simulator providing, for each line of the circuit, the observability status that is directly related to that of individual devices in the actual macrogate implementation. Conductance conflicts are correctly solved to detect bridgings and transistors stuck-on. Fault coverage results are presented and discussed for two typical FCMOS circuits. Results obtained on all ISCAS benchmarks show that the time required for the fault simulation of CMOS faults is comparable to that of stuck-ats.", "num_citations": "11\n", "authors": ["1596"]}
{"title": "Good-for-MDPs automata for probabilistic analysis and reinforcement learning\n", "abstract": " We characterize the class of nondeterministic -automata that can be used for the analysis of finite Markov decision processes (MDPs). We call these automata `good-for-MDPs' (GFM). We show that GFM automata are closed under classic simulation as well as under more powerful simulation relations that leverage properties of optimal control strategies for MDPs. This closure enables us to exploit state-space reduction techniques, such as those based on direct and delayed simulation, that guarantee simulation equivalence. We demonstrate the promise of GFM automata by defining a new class of automata with favorable properties - they are B\\\"uchi automata with low branching degree obtained through a simple construction - and show that going beyond limit-deterministic automata may significantly benefit reinforcement learning.", "num_citations": "10\n", "authors": ["1596"]}
{"title": "Finite instantiations for integer difference logic\n", "abstract": " The last few years have seen the advent of a new breed of decision procedures for various fragments of first-order logic based on propositional abstraction. A lazy satisfiability checker for a given fragment of first-order logic invokes a theory-specific decision procedure (a theory solver) on (partial) satisfying assignments for the abstraction. If the assignment is found to be consistent in the given theory, then a model for the original formula has been found. Otherwise, a refinement of the propositional abstraction is extracted from the proof of inconsistency and the search is resumed. We describe a theory solver for integer difference logic that is effective when the formula to be decided contains equality and disequality (negated equality) constraints so that the decision problem partakes of the nature of the pigeonhole problem. We propose a reduction of the problem to propositional satisfiability by computing bounds on a\u00a0\u2026", "num_citations": "10\n", "authors": ["1596"]}
{"title": "Efficient computation of small abstraction refinements\n", "abstract": " In the abstraction refinement approach to model checking, the discovery of spurious counterexamples in the current abstract model triggers its refinement. The proof - produced by a SAT solver - that the abstract counterexamples cannot be concretized can be used to identify the circuit elements or predicates that should be added to the model. It is common, however, for the refinements thus computed to be highly redundant. A costly minimization phase is therefore often needed to prevent excessive growth of the abstract model. In This work we show how to modify the search strategy of a SAT solver so that it generates refinements that are close to minimal, thus greatly reducing the time required for their minimization.", "num_citations": "10\n", "authors": ["1596"]}
{"title": "Fine-grain abstraction and sequential do not cares for large scale model checking\n", "abstract": " Abstraction refinement is a key technique for applying model checking to the verification of real-world digital systems. In previous work, the abstraction granularity is often limited at the state variable level, which is too coarse for verifying industrial-scale designs. In this paper, we propose a finer grain abstraction in which intermediate variables are selectively inserted to partition large combinational logic cones into smaller pieces; these intermediate variables, together with the state variables, are then treated as \"atoms\" in abstraction refinement. With this fine-grain approach, refinement is conducted in two different directions, sequential and Boolean. We propose a SAT-based method for predicting the appropriate refinement direction, and apply greedy minimization in both directions to keep the refinement set small. We also explore the use of approximate reachable states of the remaining submodules to help verifying\u00a0\u2026", "num_citations": "10\n", "authors": ["1596"]}
{"title": "Extended BDDs: Trading of canonicity for structure in verification algorithms\n", "abstract": " We present an extension to binary decision diagrams (BDDs) that exploits the information contained in the structure of a given circuit to produce a compact,semicanonical, representation. The resulting XBDDs (extended BDDs) retain many of the advantages of BDDs, while at the same time allowing one to deal with larger circuits.               We propose algorithms for verification of combinational circuits based on XBDDs that overcome the exponential growth in the number of nodes in the BDDs for some specific circuits such as the multipliers. While the approach remains cpu-time intensive, we believe it is the first to \u201cexactly\u201d verify the most difficult (median) output of a 16-bit multiplier. Experimental results are presented to support our claim that the XBDD approach is the \u201cbest\u201d for multiplier verification.", "num_citations": "9\n", "authors": ["1596"]}
{"title": "Clause simplification through dominator analysis\n", "abstract": " Satisfiability (SAT) solvers often benefit from clauses learned by the DPLL procedure, even though they are by definition redundant. In addition to those derived from conflicts, the clauses learned by dominator analysis during the deduction procedure tend to produce smaller implication graphs and sometimes increase the deductive power of the input CNF formula. We extend dominator analysis with an efficient self-subsumption check. We also show how the information collected by dominator analysis can be used to detect redundancies in the satisfied clauses and, more importantly, how it can be used to produce supplemental conflict clauses. We characterize these transformations in terms of deductive power and proof conciseness. Experiments show that the main advantage of dominator analysis and its extensions lies in improving proof conciseness.", "num_citations": "8\n", "authors": ["1596"]}
{"title": "Automatic generation of hints for symbolic traversal\n", "abstract": " Recent work in avoiding the state explosion problem in hardware verification during breath-first symbolic traversal (BFST) based on Binary Decision Diagrams (BDDs) applies hints to constrain the transition relation of the circuit being verified [14]. Hints are expressed as constraints on the primary inputs and states of a circuit modeled as a finite transition system and can often be found with the help of simple heuristics by someone who understands the circuit well enough to devise simulation stimuli or verification properties for it. However, finding good hints requires one to constrain the transition system so that small intermediate BDDs arise during image computations that produce large numbers of reachable states. Thus, the ease of finding good hints is limited by the user\u2019s ability to predict their usefulness. In this paper we present a method to statically and automatically determine good hints. Working on the\u00a0\u2026", "num_citations": "8\n", "authors": ["1596"]}
{"title": "Increasing the robustness of bounded model checking by computing lower bounds on the reachable states\n", "abstract": " Most symbolic model checkers are based on either Binary Decision Diagrams (BDDs), which may grow exponentially large, or Satisfiability (SAT) solvers, whose time requirements rapidly increase with the sequential depth of the circuit. We investigate the integration of BDD-based methods with SAT to speed up the verification of safety properties of the form G                      f, where f is either propositional or contains only the next-time temporal operator X. We use BDD-based reachability analysis to find lower bounds on the reachable states and the states that reach the bad states. Then, we use these lower bounds to shorten the counterexample or reduce the depth of the induction step (termination depth). We present experimental results that compare our method to a pure BDD-based method and a pure SAT-based method. Our method can prove properties that are hard for both the BDD-based and the SAT\u00a0\u2026", "num_citations": "8\n", "authors": ["1596"]}
{"title": "Least fixpoint MBM: Improved technique for approximate reachability\n", "abstract": " Reachability don\u2019t cares (RDCs) can have a dramatic impact on sequential optimization and CTL model checking. However, since the computation of RDCs is often intractable, approximate reachability don\u2019t cares (ARDCs) are often preferable. The challenge in computing approximations of the reachable states is to obtain the best accuracy within given time and memory limits. Cho et al. presented the Machine-By-Machine (MBM) and Frame-By-Frame (FBF) methods to perform approximate FSM traversal. FBF produces tighter upper bounds than MBM; however, it usually takes much more time and it may have convergence problems. In this paper, we present a new method that produces the same upper bounds as RFBF (Reached FBF, one of the FBF methods), and is as fast and efficient as MBM, but more accurate. Since the original MBM is a greatest fixpoint computation and the new method is a least fixpoint MBM, we call the new method LMBM (Least fixpoint MBM).", "num_citations": "8\n", "authors": ["1596"]}
{"title": "Who are the Variables in Your Neighbourhood\n", "abstract": " Dynamic reordering techniques have had considerable suc-cess in reducing the impact of the initial variable order on the size of decision diagrams. Sifting, in particular, has emerged as a very good compromise between low CPU time requirements and high quality of results. Sifting, however, has the absolute position of a variable as the primary objective, and only considers the relative positions of groups of variables indirectly. In this paper we propose an extension to sifting that may move groups of variables simultaneously to produce better results. Variables are aggregated by checking whether they have a strong affinity to their neighbors.(Hence the title.) Our experiments show an average im-provement in size of 11%. This improvement, coupled with the greater robustness of the algorithm, more than offsets the modest increase in CPU time that is sometimes in. curred.", "num_citations": "8\n", "authors": ["1596"]}
{"title": "Formal verification-prove it or pitch it\n", "abstract": " Despite a number of solid advances in simulation and verification techniques over the last twenty years, semiconductor chip designs continue to see large increases in the cost of verification-both in terms of human resources and time. Most of these increases are due to the growing size and complexity of the chip designs. Many of these designs are complete systems in their own right thus enlarging the scope of the verification problem. Formal verification has held out the most promise for reducing the magnitude of the verification task. Indeed, most major microprocessor teams-at IBM, Intel and Motorola-have routinely hosted formal verification experts since the early'90s. ASIC vendors and their tool providers have been closely following these developments into a number of initiatives and new startup companies driven by that very promise of formal verification. Despite these developments, simulation continues to be\u00a0\u2026", "num_citations": "7\n", "authors": ["1596"]}
{"title": "Function decomposition and synthesis using linear sifting\n", "abstract": " In order to simplify a synthesis task for particularly hard functions it is sometimes inevitable to decompose the function in a preprocessing step. We propose a new algorithm for automatically decomposing a target function by extracting a linear filter within the synthesis process. The algorithm is an application of the Linear Sifting algorithm which has been proposed in Meinel et al. (1996). Using this method we were able to synthesize functions with standard tools which fail otherwise.", "num_citations": "7\n", "authors": ["1596"]}
{"title": "Fault simulation in a multilevel environment: the MOZART approach\n", "abstract": " MOZART is a concurrent fault simulator for large circuits described at the RT, functional, gate, and switch levels. Performance is gained by means of techniques aimed at the reduction of unnecessary activity. Two such techniques are levelized two-pass simulation, which minimizes the number of events and evaluations, and list event scheduling, which allows optimized processing of simultaneous (fraternal) events for concurrent machines. Both analytical and experimental evidence is provided for the effectiveness of the solutions adopted in MOZART. A performance metric is introduced for fault simulation that is based on comparison with the serial algorithm and is more accurate than those used up till now. Possible tradeoffs between the speeds of fault-free and fault simulations are discussed.<>", "num_citations": "7\n", "authors": ["1596"]}
{"title": "A study of sweeping algorithms in the context of model checking\n", "abstract": " Combinational simplification techniques have proved their usefulness in both industrial and academic model checkers. Several combinational simplification algorithms have been proposed in the past that vary in efficiency and effectiveness. In this paper, we report our experience with three algorithms that fall in the combinational equivalence checking (sweeping) category. We propose an improvement to one of these algorithms. We have conducted an empirical study to identify the strengths and weaknesses of each of the algorithms and how they can be synergistically combined, as well as to understand how they interact with ic3 [1].", "num_citations": "6\n", "authors": ["1596"]}
{"title": "Symbolic state exploration\n", "abstract": " The exploration of the state space of the model is at the heart of model checking. Symbolic algorithms often use Binary Decision Diagrams for the representation of sets of states, and have to pay attention to the size of the BDDs. We review the techniques that have been proposed for this task in the framework of closed sequences of monotonic functions.", "num_citations": "6\n", "authors": ["1596"]}
{"title": "Formal verification of digital systems by automatic reduction of data paths\n", "abstract": " Verification of properties (tasks) on a system P containing data paths may require too many resources (memory space and/or computation time) because such systems have very large and deep state spaces. As pointed out by Kurshan, what is needed is a reduced system P' which behaves exactly as P with respect to the properties that must be proved, but more compact than P, so that the verification can be easily performed. The process of finding P' from P is called reduction. P is specified by a network of interacting finite-state machines for data paths and controllers, and tasks are specified by finite-state automate. The verification of a task T on P is performed by the language containment check L(P)/spl sube/L(T), where L(P) is the language generated by P and L(T) is the language accepted by T. It has been shown that, under appropriate conditions, the system P can be reduced to P' and the task T to T' such that L\u00a0\u2026", "num_citations": "6\n", "authors": ["1596"]}
{"title": "The role of prime compatibles in the minimization of finite state machines\n", "abstract": " A. Grasselli and F. Luccio (1965) proved that a minimum state cover of an incompletely specified finite-state machine could be found by only considering prime compatibles. It was conjectured that in practice one could restrict even further the set of compatibles being considered to the set of maximal compatibles. The conditions under which a solution formed of maximal compatibles was guaranteed to be exact were determined, but the question of the practical relevance of prime compatibles remained open. It is shown here that state minimization problems which require the full generality afforded by prime compatibles for the solution to be optimal are actually found in practice. The proof relies on the concept of analogous machines-essentially machines that pose the same minimization problem. The main result is that for any incompletely specified machine there is an analogous machine that has to be minimized in\u00a0\u2026", "num_citations": "6\n", "authors": ["1596"]}
{"title": "Faithful and Effective Reward Schemes for Model-Free Reinforcement Learning of Omega-Regular Objectives\n", "abstract": " Omega-regular properties\u2014specified using linear time temporal logic or various forms of omega-automata\u2014find increasing use in specifying the objectives of reinforcement learning (RL). The key problem that arises is that of faithful and effective translation of the objective into a scalar reward for model-free RL. A recent approach exploits B\u00fcchi automata with restricted nondeterminism to reduce the search for an optimal policy for an-regular property to that for a simple reachability objective. A possible drawback of this translation is that reachability rewards are sparse, being reaped only at the end of each episode. Another approach reduces the search for an optimal policy to an optimization problem with two interdependent discount parameters. While this approach provides denser rewards than the reduction to reachability, it is not easily mapped to off-the-shelf RL algorithms. We propose a reward scheme that\u00a0\u2026", "num_citations": "5\n", "authors": ["1596"]}
{"title": "Model-free reinforcement learning for stochastic parity games\n", "abstract": " This paper investigates the use of model-free reinforcement learning to compute the optimal value in two-player stochastic games with parity objectives. In this setting, two decision makers, player Min and player Max, compete on a finite game arena-a stochastic game graph with unknown but fixed probability distributions-to minimize and maximize, respectively, the probability of satisfying a parity objective. We give a reduction from stochastic parity games to a family of stochastic reachability games with a parameter \u03b5, such that the value of a stochastic parity game equals the limit of the values of the corresponding simple stochastic games as the parameter \u03b5 tends to 0. Since this reduction does not require the knowledge of the probabilistic transition structure of the underlying game arena, model-free reinforcement learning algorithms, such as minimax Q-learning, can be used to approximate the value and mutual best-response strategies for both players in the underlying stochastic parity game. We also present a streamlined reduction from 1 1/2-player parity games to reachability games that avoids recourse to nondeterminism. Finally, we report on the experimental evaluations of both reductions.", "num_citations": "5\n", "authors": ["1596"]}
{"title": "CUDD: CU decision diagram package. Public Software\n", "abstract": " CiNii \u8ad6\u6587 - CUDD : CU decision diagram package. Public Software CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092 \u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9\u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 CiNii Research\u30d7\u30ec \u7248\u306e\u516c\u958b\u306b\u3064\u3044\u3066 CUDD : CU decision diagram package. Public Software SOMENZI F. \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 SOMENZI F. \u53ce\u9332\u520a\u884c\u7269 http://vlsi.colorad.edu/\u301cfabio/CUDD http://vlsi.colorad.edu/\u301cfabio/CUDD \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u8ad6\u7406\u5408\u6210\u6280\u8853 \u6e4a \u771f\u4e00 \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u7814\u7a76\u5831\u544a. SLDM, [\u30b7\u30b9\u30c6\u30e0 LSI\u8a2d\u8a08\u6280\u8853] 102, 33-45, 2001-11-20 \u53c2\u8003\u6587\u732e62\u4ef6 CiNii\u5229\u7528\u8005\u30a2\u30f3\u30b1\u30fc\u30c8 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10020870928 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 \u2026", "num_citations": "5\n", "authors": ["1596"]}
{"title": "Proving parameterized systems safe by generalizing clausal proofs of small instances\n", "abstract": " We describe an approach to proving safety properties of parameterized reactive systems. Clausal inductive proofs for small instances are generalized to quantified formulae, which are then checked against the whole family of systems. Clausal proofs are generated at the bit-level by the IC3 algorithm. The clauses are partitioned into blocks, each of which is represented by a quantified implication formula, whose antecedent is a conjunction of modular linear arithmetic constraints.                 Each quantified formula approximates the set of clauses it represents; good approximations are computed through a process of proof saturation, and through the computation of convex hulls. Candidate proofs are conjunctions of quantified lemmas. For systems with a small-model bound, the proof can often be shown valid for all values of the parameter. When the candidate proof cannot be shown valid, it can still be used to\u00a0\u2026", "num_citations": "4\n", "authors": ["1596"]}
{"title": "Symbolic model checking\n", "abstract": " Model checking [CE81, QS81] is an algorithmic method for proving that a digital system satisfies a user-defined specification. Both the system and the specification must be formally specified: The model of the system must have a finite number of states; the specification, or property, is often expressed in temporal logics. In the model checking literature, the model and the property are often represented by the Kripke structure and a temporal logic formula, respectively. Given a model K and a property 0, model checking is used to check whether K models 0, denoted hy K\\=-cj). For properties specified in Computational Tree Logic (CTL [CE81, EH83]), the model checking problem can be solved by a set of least and/or greatest fixpoint computations [CES86]. For properties specified in Linear Time Logic (LTL [Pnu77]), model checking is often transformed into language emptiness checking in a generalized Blichi automaton\u00a0\u2026", "num_citations": "4\n", "authors": ["1596"]}
{"title": "CirCUs 2.0-SAT competition 2009 edition\n", "abstract": " CirCUs is a SAT solver based on the DPLL procedure and conflict clause recording [7, 5, 2]. CirCUs includes most current popular techniques such as two-watched literals scheme for Boolean Constraint Propagation (BCP), activity-based decision heuristics, clause deletion strategies, restarting heuristics, and first UIP-based clause learning. In this submission we focus on the search for a balance between the ability of a technique to detect implications (the deductive power of [3]) and its cost. This version of CirCUs adopts strong conflict analysis [4], one that often learns better clauses than those of the first Unique Implication Point (UIP). Just as different clauses may be derived from the same implication graph, different implication graphs may be obtained from the same sequence of decisions, depending on the order in which implications are propagated. Strong conflict analysis is a method to get more compact conflict clauses by scrutinizing the implication graph. It attempts to make future implication graphs simpler without expending effort on reshaping the current implication graph. The clauses generated by strong conflict analysis tend to be effective particularly in escaping hot spots, which are regions of the search space where the solver lingers for a long time. A new restarting heuristics added in this version can also help the SAT solver to handle hot spots. This new scheme is an ongoing work. Detecting whether the resolvent of two clauses subsumes either operand is easy and inexpensive. Therefore, checking on-the-fly for subsumption can be added with almost no penalty to those operations of SAT solvers that are based on resolution. This\u00a0\u2026", "num_citations": "3\n", "authors": ["1596"]}
{"title": "Constraints in one-to-many concretization for abstraction refinement\n", "abstract": " In one-to-many concretization for model checking based on abstraction refinement, constraints on input vectors that are pseudo-randomly generated are often essential to the success of the procedure. These constraints have to do with both primary inputs and invisible state variables. We discuss algorithms that address both types and we show their effectiveness through experiments.", "num_citations": "3\n", "authors": ["1596"]}
{"title": "Improved visibility in one-to-many trace concretization\n", "abstract": " We present an improved algorithm for concretization of abstract error traces in abstraction refinement-based invariant checking. The proposed algorithm maps each transition of the abstract error trace to one or more transitions in the concrete model by using a combination of simulation and satisfiability checking. Prior simulation-based approaches were hindered by limited visibility, which often resulted in excessive backtracking or refinements. The proposed technique addresses this issue in three ways: By identifying variables whose addition to the abstract trace significantly improves its predictive power at a low computational cost; by combining SAT checks with pseudo-random simulation in the construction of the concrete trace; and by a more flexible budgeting of simulation vectors that accounts for the progress made in concretization.", "num_citations": "3\n", "authors": ["1596"]}
{"title": "Disequality management in integer difference logic via finite instantiations\n", "abstract": " The last few years have seen the advent of a new breed of decision procedures for various fragments of first-order logic based on propositional abstraction. A lazy satisfiability checker for a given fragment of first-order logic invokes a theory-specific decision procedure (a theory solver) on a (partial) model for the abstraction. If the model is found to be consistent in the given theory, then a model for the original formula has been found. Otherwise, a refinement of the propositional abstraction is extracted from the proof of inconsistency and the search is resumed. We describe a theory solver for integer difference logic that is effective when the formula to be decided contains equality and disequality (negated equality) constraints so that the decision problem partakes of the nature of the pigeonhole problem. We propose a reduction of the problem to propositional satisfiability by computing bounds on a sufficient subset of\u00a0\u2026", "num_citations": "3\n", "authors": ["1596"]}
{"title": "Reinforcement learning and formal requirements\n", "abstract": " Reinforcement learning is an approach to controller synthesis where agents rely on reward signals to choose actions in order to satisfy the requirements implicit in reward signals. Oftentimes non-experts have to come up with the requirements and their translation to rewards under significant time pressure, even though manual translation is time consuming and error prone. For safety-critical applications of reinforcement learning a rigorous design methodology is needed and, in particular, a principled approach to requirement specification and to the translation of objectives into the form required by reinforcement learning algorithms.                 Formal logic provides a foundation for the rigorous and unambiguous requirement specification of learning objectives. However, reinforcement learning algorithms require requirements to be expressed as scalar reward signals. We discuss a recent technique, called\u00a0\u2026", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Equivalence checking\n", "abstract": " In the early 1980s, IBM\u2019s mainframe design ow introduced automatic logic synthesis that largely replaced the manual design step. is did not diminish the importance of equivalence checking that became instrumental in verifying the absence of functional discrepancies that", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Almost-sure reachability in stochastic multi-mode system\n", "abstract": " A constant-rate multi-mode system is a hybrid system that can switch freely among a finite set of modes, and whose dynamics is specified by a finite number of real-valued variables with mode-dependent constant rates. We introduce and study a stochastic extension of a constant-rate multi-mode system where the dynamics is specified by mode-dependent compactly supported probability distributions over a set of constant rate vectors. Given a tolerance , the almost-sure reachability problem for stochastic multi-mode systems is to decide the existence of a control strategy that steers the system almost-surely from an arbitrary start state to an -neighborhood of an arbitrary target state while staying inside a pre-specified safety set. We prove a necessary and sufficient condition to decide almost-sure reachability and, using this condition, we show that almost-sure reachability can be decided in polynomial time. Our algorithm can be used as a path-following algorithm in combination with any off-the-shelf path-planning algorithm to make a robot or an autonomous vehicle with noisy low-level controllers follow a given path with arbitrary precision.", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Using abstraction to guide the search for long error traces\n", "abstract": " Model checking is a formal method for verifying whether the system satisfies a user-defined specification. Compared to simulation, model checking is restricted in capacity. On the other hand, simulation is weak in detecting bugs that require long and complex sequences of events to be exposed. This paper combines model checking and simulation in an abstraction-refinement scheme to mitigate the problems of both methods. Abstraction refinement iteratively constructs a simplified model to verify the original model. While a simplified model mitigates the weakness of model checking, the set of simplified error traces model helps guide simulation toward deep bugs. In abstraction refinement, concretization-a process of deriving an error trace in the original model from the abstract ones-is used to invalidate spurious abstract error traces or to refute a property. In this paper, we describe a novel concretization algorithm\u00a0\u2026", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Sateen: Sat Enumeration Engine for SMT-COMP\u201908\n", "abstract": " Sateen deals with Integer Difference Logic (IDL), in which arithmetic atomic formulae constrain the difference between the values of pairs of integer variables. It is sufficient to rewrite each equality constraint (of the form x\u2212 y= n) as the conjunction of two inequalities. However, if an equality constraint is negated, then the conjunction turns into a disjunction, which requires case splitting in the enumeration of the propositional solutions. In contrast, Sateen is based on the approach of [3], which does not decompose equalities and their negations; rather, it converts the problem of checking satisfiability of a conjunction of arithmetic atomic formulae into a set of propositional satisfiability checks\u2014whose cardinality is bounded by the number of strongly connected components (SCC) of a suitable constraint graph. Sateen has been upgraded to the new version of the propositional solver, CirCUs [2]. Sateen generates solutions to the Boolean variables and the numerical variables in the original formula if the problem is satisfiable. Sateen incrementally checks infeasibility of IDL constraints. If the partial assignments from propositional solver are satisfiable, Sateen finds implied atomic variables through theory propagation. In contrast to other solvers, Sateen performs equality theory propagation that searchs for implied atomic variabls from zero-slack SCCs. To check the feasibility of IDL constraints that contain disequality constraints, it generates a maximal clique with bounds for each variable. In the final step, finite instantiation is performed for complete assignments. UNSAT Core is returned as a proof of infeasibility if the encoded SAT instance is unsatisfiable.", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Equivalence checking\n", "abstract": " 4.1 Introduction 4-1 4.2 Equivalence Checking Problem 4-3 4.3 Boolean Reasoning 4-5", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Abstraction\n", "abstract": " The concrete model is considered as a formal description of the complete behavior of the system. Abstraction preserves only part of the behavior that is relevant to the verification of the given property, in the hope that the simplified model is easier to verify. The definition of simplified model depends on the type of algorithms used in the verification process. For explicit state traversal algorithms whose complexity depends on the number of states, the simplification often aims at reducing the size of the state space. The complexity of symbolic state space traversal algorithms depends on the size of the symbolic representation, and therefore the abstract model must be simplified to provide a more compact BDD representation of the transition relation and the sets of states.The properties under verification must be at least partially preserved during abstraction. Based on how well they preserve the properties, abstraction\u00a0\u2026", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Simulation-based re-synthesis of sequential circuits for peak sustainable power reduction\n", "abstract": " The life-cycle and the reliability of an electronic circuit are related to the maximum power the circuit can dissipate. The development of automatic tools for the estimation and the minimization of the peak power is thus becoming a central issue in the EDA community. While the estimation problem has been tackled with some success in the recent past, especially in the case of combinational networks, no attempt has been made to come up with peak power optimization procedures. In this paper, we propose a re-synthesis technique whose ultimate target is the minimization of the peak sustainable power of a sequential circuit. The method consists of two main steps: First, the maximum average power cycles (MPCs) of the circuit are identi ed. Second, the states in such cycles are re-encoded and the logic re-synthesized, so as to decrease the existing MPCs. The approach we follow is simulation-based; it relies on the assumption that only the behaviors of the circuit which are exercised during the simulation of a long trace of typical usage patterns are considered as representative of the circuit. This line of thinking has been successfully applied in other contexts of the EDA world, and it seems to be one of the few solutions that computer-aided design tools can exploit to cope with the size and the complexity of modern digital ICs.", "num_citations": "2\n", "authors": ["1596"]}
{"title": "Mungojerrie: Reinforcement Learning of Linear-Time Objectives\n", "abstract": " Reinforcement learning synthesizes controllers without prior knowledge of the system. At each timestep, a reward is given. The controllers optimize the discounted sum of these rewards. Applying this class of algorithms requires designing a reward scheme, which is typically done manually. The designer must ensure that their intent is accurately captured. This may not be trivial, and is prone to error. An alternative to this manual programming, akin to programming directly in assembly, is to specify the objective in a formal language and have it \"compiled\" to a reward scheme. Mungojerrie ($\\href{https://plv.colorado.edu/mungojerrie/}{plv.colorado.edu/mungojerrie}$) is a tool for testing reward schemes for $\\omega$-regular objectives on finite models. The tool contains reinforcement learning algorithms and a probabilistic model checker. Mungojerrie supports models specified in PRISM and $\\omega$-automata specified in HOA.", "num_citations": "1\n", "authors": ["1596"]}
{"title": "Reward Shaping for Reinforcement Learning with Omega-regular Objectives\n", "abstract": " Recently, successful approaches have been made to exploit good-for-MDPs automata (B\\\"uchi automata with a restricted form of nondeterminism) for model free reinforcement learning, a class of automata that subsumes good for games automata and the most widespread class of limit deterministic automata. The foundation of using these B\\\"uchi automata is that the B\\\"uchi condition can, for good-for-MDP automata, be translated to reachability. The drawback of this translation is that the rewards are, on average, reaped very late, which requires long episodes during the learning process. We devise a new reward shaping approach that overcomes this issue. We show that the resulting model is equivalent to a discounted payoff objective with a biased discount that simplifies and improves on prior work in this direction.", "num_citations": "1\n", "authors": ["1596"]}
{"title": "The reach-avoid problem for constant-rate multi-mode systems\n", "abstract": " A constant-rate multi-mode system is a hybrid system that can switch freely among a finite set of modes, and whose dynamics is specified by a finite number of real-valued variables with mode-dependent constant rates. Alur, Wojtczak, and Trivedi have shown that reachability problems for constant-rate multi-mode systems for open and convex safety sets can be solved in polynomial time. In this paper we study the reachability problem for non-convex state spaces, and show that this problem is in general undecidable. We recover decidability by making certain assumptions about the safety set. We present a new algorithm to solve this problem and compare its performance with the popular sampling based algorithm rapidly-exploring random tree (RRT) as implemented in the Open Motion Planning Library (OMPL).", "num_citations": "1\n", "authors": ["1596"]}
{"title": "The Charme of Abstract Entities\n", "abstract": " Abstraction is fundamental in combating the state explosion problem in model checking. Automatic techniques have been developed that eliminate presumed irrelevant detail from a model and then refine the abstraction until it is accurate enough to prove the given property. This abstraction refinement approach, initially proposed by Kurshan, has received great impulse from the use of efficient satisfiability solvers in the check for the existence of error traces in the concrete model. Today it is widely applied to the verification of both hardware and software. For complex proofs, the challenge is to keep the abstract model small while carrying out most of the work on it. We review and contrast several refinement techniques that have been developed with this objective. These techniques differ in aspects that range from the choice of decision procedures for the various tasks, to the recourse to syntactic or semantic\u00a0\u2026", "num_citations": "1\n", "authors": ["1596"]}
{"title": "An exact algorithm for FPGA rectification\n", "abstract": " FPGAs are fast and easy prototyping tools in many applications. It is desirable for lookup table type FPGAs to be able to change only the functionality while keeping the netlist identical. A redesign method satisfying the above design goal for FPGAs has been proposed in KF92]. In this paper, we present a recti cation method of lookup table type FPGAs that is comparable in speed to the heuristic method of KF92] but is exact. Our method is based on solving a satis ability problem that produces a cover for a boolean relation from a restricted set of compatible primes. We show that the problem can be formulated as a modi ed boolean relation minimization problem and show an algorithm based on restricting the set of compatible primes so as to preserve the netlist structure.", "num_citations": "1\n", "authors": ["1596"]}
{"title": "CMOS dynamic power estimation based on collapsible current source transistor modeling\n", "abstract": " When estimating the dynamic power dissipated by a circuit different methods ranging from numeric analog simulation to event-driven logic simulation have been proposed. However, as the technology reaches the deep sub-micron range, additional effects as the short circuit current, partial voltage swings, transient behavior between clock cycles and leak current are becoming more relevant to achieve an accurate estimation of the consumption. In this paper we present Meiga, an event-driven simulation-based power estimator that accounts for power dissipation due to short circuit current, partial swings and transient behavior. Experiments have shown that circuits in the order of several thousands of transistors are simulated in seconds of CPU per input vector.", "num_citations": "1\n", "authors": ["1596"]}
{"title": "Zero delay elements in logic simulation\n", "abstract": " This paper addresses the problem of event-directedd simulation with zero delay elements. Incoherence arising from spikes having null duration (due to the simulation algorithm) are solved by a two-pass procedure. An additional benefit is the increased simulation speed.", "num_citations": "1\n", "authors": ["1596"]}
{"title": "A new integrated system for PLA testing and verification\n", "abstract": " PART is a system for PLA testing and verification, intended to be properly interfaced with other existing tools to generate a comprehensive design environment. To this purpose, it provides several facilities, among which the capability of generating fault population on the basis of layout information. PART aims at producing a very compact test set for all detectable crosspoint defects, using limited amounts of run time and storage. This is achieved by means of an efficient partitioning algorithm together with powerful heuristics. Test minimality is ensured by a simple procedure. In the present paper these are discussed, experimental results are given and a comparison with competing strategies is made.", "num_citations": "1\n", "authors": ["1596"]}