{"title": "Constructing test suites for interaction testing\n", "abstract": " Software system faults are often caused by unexpected interactions among components. Yet the size of a test suite required to test all possible combinations of interactions can be prohibitive in even a moderately sized project. Instead, we may use pairwise or t-way testing to provide a guarantee that all pairs or t-way combinations of components are tested together This concept draws on methods used in statistical testing for manufacturing and has been extended to software system testing. A covering array, CA(N; t, k, v), is an N/spl times/k array on v symbols such that every N x t sub-array contains all ordered subsets from v symbols of size t at least once. The properties of these objects, however do not necessarily satisfy real software testing needs. Instead we examine a less studied object, the mixed level covering array and propose a new object, the variable strength covering array, which provides a more robust\u00a0\u2026", "num_citations": "497\n", "authors": ["325"]}
{"title": "Constructing interaction test suites for highly-configurable systems in the presence of constraints: A greedy approach\n", "abstract": " Researchers have explored the application of combinatorial interaction testing (CIT) methods to construct samples to drive systematic testing of software system configurations. Applying CIT to highly-configurable software systems is complicated by the fact that, in many such systems, there are constraints between specific configuration parameters that render certain combinations invalid. Many CIT algorithms lack a mechanism to avoid these. In recent work, automated constraint solving methods have been combined with search-based CIT construction methods to address the constraint problem with promising results. However, these techniques can incur a non-trivial overhead. In this paper, we build upon our previous work to develop a family of greedy CIT sample generation algorithms that exploit calculations made by modern Boolean satisfiability (SAT) solvers to prune the search space of the CIT problem. We\u00a0\u2026", "num_citations": "358\n", "authors": ["325"]}
{"title": "Interaction testing of highly-configurable systems in the presence of constraints\n", "abstract": " Combinatorial interaction testing (CIT) is a method to sample configurations of a software system systematically for testing. Many algorithms have been developed that create CIT samples, however few have considered the practical concerns that arise when adding constraints between combinations of options. In this paper, we survey constraint handling techniques in existing algorithms and discuss the challenges that they present. We examine two highly-configurable software systems to quantify the nature of constraints in real systems. We then present a general constraint representation and solving technique that can be integrated with existing CIT algorithms and compare two constraint-enhanced algorithm implementations with existing CIT tools to demonstrate feasibility.", "num_citations": "298\n", "authors": ["325"]}
{"title": "Covering arrays for efficient fault characterization in complex configuration spaces\n", "abstract": " Many modern software systems are designed to be highly configurable so they can run on and be optimized for a wide variety of platforms and usage scenarios. Testing such systems is difficult because, in effect, you are testing a multitude of systems, not just one. Moreover, bugs can and do appear in some configurations, but not in others. Our research focuses on a subset of these bugs that are \"option-related\"-those that manifest with high probability only when specific configuration options take on specific settings. Our goal is not only to detect these bugs, but also to automatically characterize the configuration subspaces (i.e., the options and their settings) in which they manifest. To improve efficiency, our process tests only a sample of the configuration space, which we obtain from mathematical objects called covering arrays. This paper compares two different kinds of covering arrays for this purpose and assesses\u00a0\u2026", "num_citations": "290\n", "authors": ["325"]}
{"title": "Evaluating improvements to a meta-heuristic search for constrained interaction testing\n", "abstract": " Combinatorial interaction testing (CIT) is a cost-effective sampling technique for discovering interaction faults in highly-configurable systems. Constrained CIT extends the technique to situations where some features cannot coexist in a configuration, and is therefore more applicable to real-world software. Recent work on greedy algorithms to build CIT samples now efficiently supports these feature constraints. But when testing a single system configuration is expensive, greedy techniques perform worse than meta-heuristic algorithms, because greedy algorithms generally need larger samples to exercise the same set of interactions. On the other hand, current meta-heuristic algorithms have long run times when feature constraints are present. Neither class of algorithm is suitable when both constraints and the cost of testing configurations are important factors. Therefore, we reformulate one meta-heuristic\u00a0\u2026", "num_citations": "238\n", "authors": ["325"]}
{"title": "Configuration-aware regression testing: an empirical study of sampling and prioritization\n", "abstract": " Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Researchers have created techniques for testing configurable software, but to date, only a little research has addressed the problems of regression testing configurable systems as they evolve. Whereas problems such as selective retesting and test prioritization at the test case level have been extensively researched, these problems have rarely been considered at the configuration level. In this paper we address the problem of providing configuration-aware regression testing for evolving software systems. We use combinatorial interaction testing techniques to model and generate configuration samples for use in regression testing. We conduct an empirical study on a non-trivial evolving software system to measure the impact of configurations on testing effectiveness, and to compare the effectiveness of\u00a0\u2026", "num_citations": "222\n", "authors": ["325"]}
{"title": "Combinatorial interaction regression testing: A study of test case generation and prioritization\n", "abstract": " Regression testing is an expensive part of the software maintenance process. Effective regression testing techniques select and order (or prioritize) test cases between successive releases of a program. However, selection and prioritization are dependent on the quality of the initial test suite. An effective and cost efficient test generation technique is combinatorial interaction testing, CIT, which systematically samples all t-way combinations of input parameters. Research on CIT, to date, has focused on single version software systems. There has been little work that empirically assesses the use of CIT test generation as the basis for selection or prioritization. In this paper we examine the effectiveness of CIT across multiple versions of two software subjects. Our results show that CIT performs well in finding seeded faults when compared with an exhaustive test set. We examine several CIT prioritization techniques and\u00a0\u2026", "num_citations": "185\n", "authors": ["325"]}
{"title": "A deterministic density algorithm for pairwise interaction coverage.\n", "abstract": " Pairwise coverage of factors affecting software has been proposed to screen for potential errors. Techniques to generate test suites for pairwise coverage are evaluated according to many criteria. A small number of tests is a main criterion, as this dictates the time for test execution. Randomness has been exploited to search for small test suites, but variation occurs in the test suite produced. A worst-case guarantee on test suite size is desired; repeatable generation is often necessary. The time to construct the test suite is also important. Finally, testers must be able to include certain tests, and to exclude others. The main approaches to generating test suites for pairwise coverage are examined; these are exemplified by AETG, IPO, TCG, TConfig, simulated annealing, and combinatorial design techniques. A greedy variant of AETG and TCG is developed. It is deterministic, guaranteeing reproducibility. It generates only one candidate test at a time, providing faster test suite development. It is shown to provide a logarithmic worst-case guarantee on the test suite size. It permits users to \u201cseed\u201d the test suite with specified tests. Finally, comparisons with other greedy approaches demonstrate that it often yields the smallest test suite.", "num_citations": "169\n", "authors": ["325"]}
{"title": "A framework of greedy methods for constructing interaction test suites\n", "abstract": " Greedy algorithms for the construction of software interaction test suites are studied. A framework is developed to evaluate a large class of greedy methods that build suites one test at a time. Within this framework are many instantiations of greedy methods generalizing those in the literature. Greedy algorithms are popular when the time for test suite construction is of paramount concern. We focus on the size of the test suite produced by each instantiation. Experiments are analyzed using statistical techniques to determine the importance of the implementation decisions within the framework. This framework provides a platform for optimizing the accuracy and speed of\" one-test-at-a-time\" greedy methods.", "num_citations": "166\n", "authors": ["325"]}
{"title": "Coverage and adequacy in software product line testing\n", "abstract": " Software product line modeling has received a great deal of attention for its potential in fostering reuse of software artifacts across development phases. Research on the testing phase, has focused on identifying the potential for reuse of test cases across product line instances. While this offers potential reductions in test development effort for a given product line instance, it does not focus on and leverage the fundamental abstraction that is inherent in software product lines-variability. In this paper, we illustrate how rich software product line modeling notations can be mapped onto an underlying relational model that captures variability in the feasible product line instances. This relational model serves as the semantic basis for defining a family of coverage criteria for testing of a product line. These criteria make it possible to accumulate test coverage information for the product line itself over the course of multiple\u00a0\u2026", "num_citations": "162\n", "authors": ["325"]}
{"title": "A variable strength interaction testing of components\n", "abstract": " Complete interaction testing of components is too costly in all but the smallest systems. Yet component interactions are likely to cause unexpected faults. Recently, design of experiment techniques have been applied to software testing to guarantee a minimum coverage of all t-way interactions across components. However, t is always fixed. This paper examines the need to vary the size of t in an individual test suite and defines a new object, the variable strength covering array that has this property. We present some computational methods to find variable strength arrays and provide initial bounds for a group of these objects.", "num_citations": "161\n", "authors": ["325"]}
{"title": "Augmenting simulated annealing to build interaction test suites\n", "abstract": " Component based software development is prone to unexpected interaction faults. The goal is to test as many-potential interactions as is feasible within time and budget constraints. Two combinatorial objects, the orthogonal array and the covering array, can be used to generate test suites that provide a guarantee for coverage of all t-sets of component interactions in the case when the testing of all interactions is not possible. Methods for construction of these types of test suites have focused on two main areas. The first is finding new algebraic constructions that produce smaller test suites. The second is refining computational search algorithms to find smaller test suites more quickly. In this paper we explore one method for constructing covering arrays of strength three that combines algebraic constructions with computational search. This method leverages the computational efficiency and optimality of size obtained\u00a0\u2026", "num_citations": "156\n", "authors": ["325"]}
{"title": "Constructing strength three covering arrays with augmented annealing\n", "abstract": " Abstract A covering array CA (N; t, k, v) is an N\u00d7 k array such that every N\u00d7 t sub-array contains all t-tuples from v symbols at least once, where t is the strength of the array. One application of these objects is to generate software test suites to cover all t-sets of component interactions. Methods for construction of covering arrays for software testing have focused on two main areas. The first is finding new algebraic and combinatorial constructions that produce smaller covering arrays. The second is refining computational search algorithms to find smaller covering arrays more quickly. In this paper, we examine some new cut-and-paste techniques for strength three covering arrays that combine recursive combinatorial constructions with computational search; when simulated annealing is the base method, this is augmented annealing. This method leverages the computational efficiency and optimality of size obtained\u00a0\u2026", "num_citations": "146\n", "authors": ["325"]}
{"title": "Designing Test Suites for Software Interactions Testing\n", "abstract": " Testing is an expensive but essential part of any software project. Having the right methods to detect faults is a primary factor for success in the software industry. Component based systems are problematic because they are prone to unexpected interaction faults, yet these may be left undetected by traditional testing techniques. In all but the smallest of systems, it is not possible to test every component interaction. One can use a reduced test suite that guarantees to include a defined subset of interactions instead. A well studied combinatorial object, the covering array, can be used to achieve this goal. Constructing covering arrays for a specific software system is not always simple and the resulting object may not closely mirror the real test environment. Not only are new methods for building covering arrays needed, but new tools to support these are required as well. Our aim is to develop methods for building smaller test suites that provide stronger interaction coverage, while retaining the flexibility required in a practical test suite. We combine ideas from combinatorial design theory, computational search, statistical design of experiments and software engineering. We begin with a description of a framework for greedy algorithms that has formed the basis for several published methods and a widely used commercial tool. We compare this with a meta-heuristic search algorithm, simulated annealing. The results suggest that simulated annealing is more effective at finding smaller test suites, and in some cases improves on combinatorial methods as well. We then develop a mathematical model for variable strength interaction testing. This allows us to\u00a0\u2026", "num_citations": "133\n", "authors": ["325"]}
{"title": "Directed test suite augmentation: techniques and tradeoffs\n", "abstract": " Test suite augmentation techniques are used in regression testing to identify code elements affected by changes and to generate test cases to cover those elements. Our preliminary work suggests that several factors influence the cost and effectiveness of test suite augmentation techniques. These include the order in which affected elements are considered while generating test cases, the manner in which existing regression test cases and newly generated test cases are used, and the algorithm used to generate test cases. In this work, we present the results of an empirical study examining these factors, considering two test case generation algorithms (concolic and genetic). The results of our experiment show that the primary factor affecting augmentation is the test case generation algorithm utilized; this affects both cost and effectiveness. The manner in which existing and newly generated test cases are utilized\u00a0\u2026", "num_citations": "114\n", "authors": ["325"]}
{"title": "An improved meta-heuristic search for constrained interaction testing\n", "abstract": " Combinatorial interaction testing (CIT) is a cost-effective sampling technique for discovering interaction faults in highly configurable systems. Recent work with greedy CIT algorithms efficiently supports constraints on the features that can coexist in a configuration. But when testing a single system configuration is expensive, greedy techniques perform worse than meta-heuristic algorithms because they produce larger samples. Unfortunately, current meta-heuristic algorithms are inefficient when constraints are present. We investigate the sources of inefficiency, focusing on simulated annealing, a well-studied meta-heuristic algorithm. From our findings we propose changes to improve performance, including a reorganized search space based on the CIT problem structure. Our empirical evaluation demonstrates that the optimizations reduce run-time by three orders of magnitude and yield smaller samples. Moreover\u00a0\u2026", "num_citations": "107\n", "authors": ["325"]}
{"title": "Covering arrays for efficient fault characterization in complex configuration spaces\n", "abstract": " Testing systems with large configurations spaces that change often is a challenging problem. The cost and complexity of QA explodes because often there isn't just one system, but a multitude of related systems. Bugs may appear in certain configurations, but not in others.The Skoll system and process has been developed to test these types of systems through distributed, continuous quality assurance, leveraging user resources around-the-world, around-the-clock. It has been shown to be effective in automatically characterizing configurations in which failures manifest. The derived information helps developers quickly narrow down the cause of failures which then improves turn around time for fixes. However, this method does not scale well. It requires one to exhaustively test each configuration in the configuration space.In this paper we examine an alternative approach. The idea is to systematically sample the\u00a0\u2026", "num_citations": "106\n", "authors": ["325"]}
{"title": "Moving forward with combinatorial interaction testing\n", "abstract": " Combinatorial interaction testing (CIT) is an effective failure detection method for many types of software systems. This review discusses the current approaches CIT uses in detecting parameter interactions, the difficulties of applying it in practice, recent advances, and opportunities for future research.", "num_citations": "82\n", "authors": ["325"]}
{"title": "Feature interaction faults revisited: An exploratory study\n", "abstract": " While a large body of research is dedicated to testing for feature interactions in configurable software, there has been little work that examines what constitutes such a fault at the code level. In consequence, we do not know how prevalent real interaction faults are in practice, what a typical interaction fault looks like in code, how to seed interaction faults, or whether current interaction testing techniques are effective at finding the faults they aim to detect. We make a first step in this direction, by deriving a white box criterion for an interaction fault. Armed with this criterion, we perform an exploratory study on hundreds of faults from the field in two open source systems. We find that only three of the 28 which appear to be interaction faults are in fact due to features' interactions. We investigate the remaining 25 and find that, although they could have been detected without interaction testing, varying the system configuration\u00a0\u2026", "num_citations": "73\n", "authors": ["325"]}
{"title": "Incremental covering array failure characterization in large configuration spaces\n", "abstract": " The increasing complexity of configurable software systems has created a need for more intelligent sampling mechanisms to detect and characterize failure-inducing dependencies between configurations. Prior work-in idealized environments-has shown that test schedules based on a mathematical object, called a covering array, in combination with classification techniques, can meet this need. Applying this approach in practice, however, is tricky because testing time and resource availability are unpredictable, and because failure characteristics can change from release to release. With current approaches developers must set a key covering array parameter (its strength) based on estimated release times and failure characterizations. This will influence the outcome of their results.", "num_citations": "69\n", "authors": ["325"]}
{"title": "Predicting survival, length of stay, and cost in the surgical intensive care unit: APACHE II versus ICISS\n", "abstract": " BackgroundRisk stratification of patients in the intensive care unit (ICU) is an important tool because it permits comparison of patient populations for research and quality control. Unfortunately, currently available scoring systems were developed primarily in medical ICUs and have only mediocre performance in surgical ICUs. Moreover, they are very expensive to purchase and use. We conceived a simple risk-stratification tool for the surgical ICU that uses readily available International Classification of Diseases, Ninth Revision, codes to predict outcome. Called ICISS (International Classification of Disease Illness Severity Score), our score is the product of the survival risk ratios (obtained from an independent data set) for all International Classification of Diseases, Ninth Revision, diagnosis codes.MethodsA total of 5,322 noncardiac patients admitted to a surgical ICU during an 8-year period had their Acute\u00a0\u2026", "num_citations": "69\n", "authors": ["325"]}
{"title": "Configurations everywhere: Implications for testing and debugging in practice\n", "abstract": " Many industrial systems are highly-configurable, complicating the testing and debugging process. While researchers have developed techniques to statically extract, quantify and manipulate the valid system configurations, we conjecture that many of these techniques will fail in practice. In this paper we analyze a highly-configurable industrial application and two open source applications in order to quantify the true challenges that configurability creates for software testing and debugging. We find that (1) all three applications consist of multiple programming languages, hence static analyses need to cross programming language barriers to work,(2) there are many access points and methods to modify configurations, implying that practitioners need configuration traceability and should gather and merge metadata from more than one source and (3) the configuration state of an application on failure cannot be reliably\u00a0\u2026", "num_citations": "63\n", "authors": ["325"]}
{"title": "Reducing field failures in system configurable software: Cost-based prioritization\n", "abstract": " System testing of configurable software is an expensive and resource constrained process. Insufficient testing often leads to escaped faults in the field where failures impact customers and are costly to repair. Prior work has shown that it is possible to efficiently sample configurations for testing using combinatorial interaction testing, and to prioritize these configurations to increase the rate of early fault detection. The underlying assumption to date has been that there is no added complexity to configuring a system level environment over a user configurable one; i.e. the time required to setup and test each individual configuration is nominal. In this paper we examine prioritization of system configurable software driven not only by fault detection but also by the cost of configuration and setup time that moving between different configurations incurs. We present a case study on two releases of an enterprise software\u00a0\u2026", "num_citations": "60\n", "authors": ["325"]}
{"title": "Trauma registry injury coding is superfluous: a comparison of outcome prediction based on trauma registry International Classification of Diseases-Ninth Revision (ICD-9) and\u00a0\u2026\n", "abstract": " BackgroundTrauma registries are an essential but expensive tool for monitoring trauma system performance. The time required to catalog patients' injuries is the source of much of this expense. Typically, 15 minutes of chart review per patient are required, which in a busy trauma center may represent 25% of a full-time employee. We hypothesized that International Classification of Disease-Ninth Revision (ICD-9) codes generated by the hospital information system (HI) would be similar to those coded by a dedicated trauma registrar (TR) and would be as accurate as TR ICD-9 codes in predicting outcome.MethodsOne thousand eight hundred twelve patients admitted to a Level I trauma center during 2 years had International Classification of Disease Injury Severity Scores (ICISS) calculated based on HI and TR ICD-9 codes. The relative predictive powers of these two ICISSs were then compared for every patient\u00a0\u2026", "num_citations": "56\n", "authors": ["325"]}
{"title": "Feedback driven adaptive combinatorial testing\n", "abstract": " The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations. The basic justification for CIT approaches is that they can cost-effectively exercise all system behaviors caused by the settings of t or fewer options. We conjecture, however, that in practice many such behaviors are not actually tested because of masking effects--failures that perturb execution so as to prevent some behaviors from being exercised. In this work we present a feedback-driven, adaptive, combinatorial testing approach aimed at detecting and working around masking effects. At each iteration we detect potential masking effects, heuristically isolate their likely causes, and then generate new covering arrays that allow previously masked combinations to be\u00a0\u2026", "num_citations": "55\n", "authors": ["325"]}
{"title": "Integration testing of software product lines using compositional symbolic execution\n", "abstract": " Software product lines are families of products defined by feature commonality and variability, with a well-managed asset base. Recent work in testing of software product lines has exploited similarities across development phases to reuse shared assets and reduce test effort. The use of feature dependence graphs has also been employed to reduce testing effort, but little work has focused on code level analysis of dataflow between features. In this paper we present a compositional symbolic execution technique that works in concert with a feature dependence graph to extract the set of possible interaction trees in a product family. It composes these to incrementally and symbolically analyze feature interactions. We experiment with two product lines and determine that our technique can reduce the overall number of interactions that must be considered during testing, and requires less time to run than a\u00a0\u2026", "num_citations": "50\n", "authors": ["325"]}
{"title": "Reducing masking effects in combinatorialinteraction testing: A feedback drivenadaptive approach\n", "abstract": " The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations. The basic justification for CIT approaches is that they can cost-effectively exercise all system behaviors caused by the settings of t or fewer options. We conjecture, however, that in practice some of these behaviors are not actually tested because of unanticipated masking effects - test case failures that perturb system execution so as to prevent some behaviors from being exercised. While prior research has identified this problem, most solutions require knowing the masking effects a priori. In practice this is impractical, if not impossible. In this work, we reduce the harmful consequences of masking effects. First we define a novel interaction testing criterion, which aims\u00a0\u2026", "num_citations": "45\n", "authors": ["325"]}
{"title": "Financial aspects of providing trauma care at the extremes of life\n", "abstract": " BackgroundChildren and the elderly are more likely to be underinsured compared with the general population of trauma patients. We performed financial analysis on all trauma patients admitted during an 18-month period to a Level I adult and pediatric trauma center to evaluate the financial impact of providing trauma care for children and the elderly.MethodsPatients were categorized by age: PEDI< 17 years, GERI> 64 years and MID= 17 to 64 years. Reimbursement ratio (RR= reimbursement/cost; RR> 1= profit, RR< 1= loss), length of stay (LOS), and Injury Severity Score (ISS) were calculated for each age group.ResultsRR for GERI (RR= 0.99) was significantly lower than for PEDI (RR= 1.15) and MID (RR= 1.16). There was no difference in ISS, but the LOS of GERI was greater than that of PEDI and MID (p< 0.05). Cost per patient and LOS were less in PEDI versus MID and GERI (p< 0.05).ConclusionTrauma care\u00a0\u2026", "num_citations": "43\n", "authors": ["325"]}
{"title": "Exploiting constraint solving history to construct interaction test suites\n", "abstract": " Researchers have explored the application of combinatorial interaction testing (CIT) methods to construct samples to drive systematic testing of software system configurations. Applying CIT to highly-configurable software systems is complicated by the fact that, in many such systems, there are constraints between specific configuration parameters that render certain combinations invalid. In recent work, automated constraint solving methods have been combined with search-based CIT methods to address this problem with promising results. In this paper, we observe that the pattern of computation in greedy CIT algorithms leads to sequences of constraint solving problems that are closely related to one another. We propose two techniques for exploiting the history of constraint solving: (1) using incremental algorithms that are present within available constraint solvers and (2) mining constraint solver data structures to\u00a0\u2026", "num_citations": "42\n", "authors": ["325"]}
{"title": "Beyond the rainbow: self-adaptive failure avoidance in configurable systems\n", "abstract": " Self-adaptive software systems monitor their state and then adapt when certain conditions are met, guided by a global utility function. In prior work we developed algorithms and conducted a post-hoc analysis demonstrating the possibility of adapting to software failures by judiciously changing configurations. In this paper we present the REFRACT framework that realizes this idea in practice by building on the self-adaptive Rainbow architecture. REFRACT extends Rainbow with new components and algorithms targeting failure avoidance. We use REFRACT in a case study running four independently executing Firefox clients with 36 passing test cases and 7 seeded faults. The study show that workarounds for all but one of the seeded faults are found and the one that is not found never fails--it is guarded from failing by a related workaround. Moreover, REFRACT finds workarounds for eight configuration-related\u00a0\u2026", "num_citations": "40\n", "authors": ["325"]}
{"title": "Factors affecting the use of genetic algorithms in test suite augmentation\n", "abstract": " Test suite augmentation techniques are used in regression testing to help engineers identify code elements affected by changes, and generate test cases to cover those elements. Researchers have created various approaches to identify affected code elements, but only recently have they considered integrating, with this task, approaches for generating test cases. In this paper we explore the use of genetic algorithms in test suite augmentation. We identify several factors that impact the effectiveness of this approach, and we present the results of a case study exploring the effects of one of these factors: the manner in which existing and newly generated test cases are utilized by the genetic algorithm. Our results reveal several ways in which this factor can influence augmentation results, and reveal open problems that researchers must address if they wish to create augmentation techniques that make use of genetic\u00a0\u2026", "num_citations": "39\n", "authors": ["325"]}
{"title": "A study in prioritization for higher strength combinatorial testing\n", "abstract": " Recent studies have shown that combinatorial interaction testing (CIT) is an effective fault detection technique and that early fault detection can be improved by ordering test suites by interaction based prioritization approaches. Despite research that has shown that higher strength CIT improves fault detection, there have been fewer studies that aim to understand the impact of prioritization based on higher strength criteria. In this paper, we aim to understand how interaction based prioritization techniques perform, in terms of early fault detection when we prioritize based on 3-way interactions. We generalize prior work on prioritizing using 2-way interactions to t-way prioritization, and empirically evaluate this on three open source subjects, across multiple versions of each. We examine techniques that prioritize both existing CIT suites as well as generate new ones in prioritized order. We find that early fault detection\u00a0\u2026", "num_citations": "36\n", "authors": ["325"]}
{"title": "PrefFinder: getting the right preference in configurable software systems\n", "abstract": " Highly configurable software, such as web browsers, databases or office applications, have a large number of preferences that the user can customize, but documentation of them may be scarce or distributed. A user, tester or service technician may have to search through hundreds or thousands of choices in multiple documents when trying to identify which preference will modify a particular system behavior. In this paper we present PrefFinder, a natural language framework that finds (and changes) user preferences. It is tied into an application's preference system and static documentation. We have instantiated PrefFinder as a plugin on two open source applications, and as a stand-alone GUI for an industrial application. PrefFinder finds thecorrect answer between 76-96% of the time on more than 175 queries. When compared to asking questions on a help forum or through the company's service center, we can\u00a0\u2026", "num_citations": "35\n", "authors": ["325"]}
{"title": "Clustering the heap in multi-threaded applications for improved garbage collection\n", "abstract": " Garbage collection can be a performance bottleneck in large distributed, multi-threaded applications. Applications may produce millions of objects during their lifetimes and may invoke hundreds or thousands of threads. When using a single shared heap, each time a garbage collection phase occurs all threads must be stopped, essentially halting all other processing. Attempts to fix this bottleneck include creating a single heap per thread, however this may not scale to large thread intensive applications. In this paper we explore the potential of clustering threads into related sub-heaps. We hypothesize that this will lead to a smaller shared heap, while maintaining good garbage collection parallelism. We leverage results from software module clustering to achieve this goal. Our results show that we can significantly reduce the number of sub-heaps created and reduce the number of objects in the shared heap in a\u00a0\u2026", "num_citations": "35\n", "authors": ["325"]}
{"title": "Interaction coverage meets path coverage by SMT constraint solving\n", "abstract": " We present a novel approach for generating interaction combinations based on SMT constraint resolution. Our approach can generate maximal interaction coverage in the presence of general constraints as supported by the underlying solver. It supports seeding with general predicates, which allows us to combine it with path exploration such that both interaction and path coverage goals can be met. Our approach is motivated by the application to behavioral model-based testing in the Spec Explorer tool, where parameter combinations must be generated such that all path conditions of a model action have at least one combination which enables the path. It is applied in a large-scale project for model-based quality assurance of interoperability documentation at Microsoft.", "num_citations": "34\n", "authors": ["325"]}
{"title": "Continuous test suite augmentation in software product lines\n", "abstract": " Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, to generate test cases for individual products in product lines more efficiently. In this paper we propose an approach, CONTESA, for generating test cases for SPLs using test suite augmentation. Instead of generating test cases for products independently, our approach generates new test cases for products in an order that allows it to build on test cases created for products tested earlier. In this work, we use a genetic algorithm to generate test cases, targeting branches not yet covered in each product, although other algorithms and coverage criteria could be utilized. We have evaluated CONTESA on two non-trivial SPLs, and have shown that CONTESA is more efficient and effective than an approach that generates test cases for products independently. A further\u00a0\u2026", "num_citations": "32\n", "authors": ["325"]}
{"title": "Using feature locality: can we leverage history to avoid failures during reconfiguration?\n", "abstract": " Despite the best efforts of software engineers, faults still escape into deployed software. Developers need time to prepare and distribute fixes, and in the interim deployments must either tolerate or avoid failures. Self-adaptive systems, systems that adapt to meet changing requirements in a dynamic environment, have a daunting task if their reconfiguration involves adding or removing functional features, because configurable software is known to suffer from failures that appear only under certain feature combinations.", "num_citations": "30\n", "authors": ["325"]}
{"title": "Towards incremental adaptive covering arrays\n", "abstract": " The increasing complexity of configurable software systems creates a need for more intelligent sampling mechanisms to detect and locate failure-inducing dependencies between configurations. Prior work shows that test schedules based on a mathematical object, called a covering array, can be used to detect and locate failures in combination with a classification tree analysis. This paper addresses limitations of the earlier approach. First, the previous work requires developers to choose the covering array's strength, even though there is no scientific or historical basis for doing so. Second, if a single covering array is insufficient to classify specific failures, the entire process must be rerun from scratch. To address these issues, our new approach incrementally and adaptively builds covering array schedules. It begins with a low strength, and continually increases this as resources allow, or poor classification results\u00a0\u2026", "num_citations": "30\n", "authors": ["325"]}
{"title": "Regression testing in software as a service: An industrial case study\n", "abstract": " Many organizations are moving towards a business model of Software as a Service (SaaS), where customers select and pay for services dynamically via the web. In SaaS, service providers face the challenge of delivering and maintaining high quality software solutions which must continue to work under an enormous number of scenarios; customers can easily subscribe and unsubscribe from services at any point. To date, there has been little research on unique approaches for regression test methodologies for testing in a SaaS environment. In this paper, we present an industrial case study of a regression testing approach to improve test effectiveness and efficiency in SaaS. We model service level use cases from field failures as abstract events and then generate sequences of these for testing to provide a broad coverage of the possible use cases. In subsequent releases of the system we prioritize the tests to\u00a0\u2026", "num_citations": "28\n", "authors": ["325"]}
{"title": "Financial outcome of treating trauma in a rural environment\n", "abstract": " The financial plight of the urban trauma center is well documented. However, the financial status of the rural trauma center is largely unknown. We hypothesized that our rural trauma center with a high number of blunt trauma patients, a wide spectrum of injury severity, and a large percentage of insured patients would prove to be financially advantageous to the institution. From January 1994 to June 1995, 1,119 consecutive trauma admissions had a complete financial profile compiled including actual costs, reimbursements, and reimbursement ratio (RR= reimbursement/actual costs).", "num_citations": "28\n", "authors": ["325"]}
{"title": "Users beware: Preference inconsistencies ahead\n", "abstract": " The structure of preferences for modern highly-configurable software systems has become extremely complex, usually consisting of multiple layers of access that go from the user interface down to the lowest levels of the source code. This complexity can lead to inconsistencies between layers, especially during software evolution. For example, there may be preferences that users can change through the GUI, but that have no effect on the actual behavior of the system because the related source code is not present or has been removed going from one version to the next. These inconsistencies may result in unexpected program behaviors, which range in severity from mild annoyances to more critical security or performance problems. To address this problem, we present SCIC (Software Configuration Inconsistency Checker), a static analysis technique that can automatically detect these kinds of inconsistencies\u00a0\u2026", "num_citations": "27\n", "authors": ["325"]}
{"title": "Probe distribution techniques to profile events in deployed software\n", "abstract": " Profiling deployed software provides valuable insights for quality improvement activities. The probes required for profiling, however, can cause an unacceptable performance overhead for users. In previous work we have shown that significant overhead reduction can be achieved, with limited information loss, through the distribution of probes across deployed instances. However, existing techniques for probe distribution are designed to profile simple events. In this paper we present a set of techniques for probe distribution to enable the profiling of complex events that require multiple probes and that share probes with other events. Our evaluation of the new techniques indicates that, for tight overhead bounds, techniques that produce a balanced event allocation can retain significantly more field information", "num_citations": "25\n", "authors": ["325"]}
{"title": "Test case prioritization of build acceptance tests for an enterprise cloud application: An industrial case study\n", "abstract": " The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since BAT contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be\u00a0\u2026", "num_citations": "23\n", "authors": ["325"]}
{"title": "Hybrid directed test suite augmentation: An interleaving framework\n", "abstract": " Test suite augmentation techniques generate test cases to cover code missed by existing regression test suites. Various augmentation techniques have been proposed, utilizing several test case generation algorithms. Research has shown that different algorithms have different strengths, and that combining them into a single hybrid approach may be cost-effective. In this paper we present a framework for hybrid test suite augmentation that allows test case generation algorithms to be interleaved dynamically and that can easily incorporate new algorithms, interleaving strategies, and choices of other parameters that influence algorithm performance. We empirically study an implementation of this framework in which we use two test case generation algorithms and several algorithm interleavings. Our results show that specific instantiations of our framework can produce augmentation techniques that are more cost\u00a0\u2026", "num_citations": "23\n", "authors": ["325"]}
{"title": "Optimal and pessimal orderings of Steiner triple systems in disk arrays\n", "abstract": " Steiner triple systems are well studied combinatorial designs that have been shown to possess properties desirable for the construction of multiple erasure codes in RAID architectures. The ordering of the columns in the parity check matrices of these codes affects system performance. Combinatorial problems involved in the generation of good and bad column orderings are defined, and examined for small numbers of accesses to consecutive data blocks in the disk array.", "num_citations": "23\n", "authors": ["325"]}
{"title": "Kirkman triple systems of order 21 with nontrivial automorphism group\n", "abstract": " There are 50,024 Kirkman triple systems of order 21 admitting an automorphism of order 2. There are 13,280 Kirkman triple systems of order 21 admitting an automorphism of order 3. Together with the 192 known systems and some simple exchange operations, this leads to a collection of 63,745 nonisomorphic Kirkman triple systems of order 21. This includes all KTS (21) s having a nontrivial automorphism group. None of these is doubly resolvable. Four are quadrilateral-free, providing the first examples of such a KTS (21).", "num_citations": "23\n", "authors": ["325"]}
{"title": "Directed test suite augmentation: an empirical investigation\n", "abstract": " Test suite augmentation techniques are used in regression testing to identify code elements in a modified program that are not adequately tested and to generate test cases to cover those elements. A defining feature of test suite augmentation techniques is the potential for reusing existing regression test suites. Our preliminary work suggests that several factors influence the efficiency and effectiveness of augmentation techniques that perform such reuse. These include the order in which target code elements are considered while generating test cases, the manner in which existing regression test cases and newly generated test cases are used, and the algorithm used to generate test cases. In this work, we present the results of two empirical studies examining these factors, considering two test case generation algorithms (concolic and genetic). The results of our studies show that the primary factor affecting\u00a0\u2026", "num_citations": "20\n", "authors": ["325"]}
{"title": "Human performance regression testing\n", "abstract": " As software systems evolve, new interface features such as keyboard shortcuts and toolbars are introduced. While it is common to regression test the new features for functional correctness, there has been less focus on systematic regression testing for usability, due to the effort and time involved in human studies. Cognitive modeling tools such as CogTool provide some help by computing predictions of user performance, but they still require manual effort to describe the user interface and tasks, limiting regression testing efforts. In recent work, we developed CogTool-Helper to reduce the effort required to generate human performance models of existing systems. We build on this work by providing task specific test case generation and present our vision for human performance regression testing (HPRT) that generates large numbers of test cases and evaluates a range of human performance predictions for the same\u00a0\u2026", "num_citations": "19\n", "authors": ["325"]}
{"title": "Failure avoidance in configurable systems through feature locality\n", "abstract": " Despite the best efforts of software engineers, faults still escape into deployed software. Developers need time to prepare and distribute fixes, and in the interim, deployments must either avoid failures or endure their consequences. Self-adaptive systems\u2013systems that adapt to changes internally, in requirements, and in a dynamic environment\u2013 can often handle these challenges automatically, depending on the nature of the failures.               Those self-adaptive systems where functional features can be added or removed also constitute configurable systems. Configurable software is known to suffer from failures that appear only under certain feature combinations, and these failures are particularly challenging for testers, who must find suitable configurations as well as inputs to detect them. However, these elusive failures seem well suited for avoidance by self-adaptation. We need only find an alternative\u00a0\u2026", "num_citations": "18\n", "authors": ["325"]}
{"title": "Navigating the maze: the impact of configurability in bioinformatics software\n", "abstract": " The bioinformatics software domain contains thousands of applications for automating tasks such as the pairwise alignment of DNA sequences, building and reasoning about metabolic models or simulating growth of an organism. Its end users range from sophisticated developers to those with little computational experience. In response to their needs, developers provide many options to customize the way their algorithms are tuned. Yet there is little or no automated help for the user in determining the consequences or impact of the options they choose. In this paper we describe our experience working with configurable bioinformatics tools. We find limited documentation and help for combining and selecting options along with variation in both functionality and performance. We also find previously undetected faults. We summarize our findings with a set of lessons learned, and present a roadmap for creating\u00a0\u2026", "num_citations": "15\n", "authors": ["325"]}
{"title": "Simlatte: A framework to support testing for worst-case interrupt latencies in embedded software\n", "abstract": " Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is\u00a0\u2026", "num_citations": "14\n", "authors": ["325"]}
{"title": "Ladder orderings of pairs and RAID performance\n", "abstract": " In a systematic erasure code for the correction of two simultaneous erasures, each information symbol must have two associated parity symbols. When implemented in a redundant array of independent disks (RAID), performance requirements on the update penalty necessitate that each information symbol be associated with no more parity symbols than the two required. This leads to a simple graph model of the erasure codes, with parity symbols as vertices and information symbols as edges. Based on simulations of RAID performance, an ordering of the edges in which every sequence of three consecutive edges in the order induces as few vertices as possible is found to optimize access performance of the disk array. The ladder orderings to optimize performance are shown to exist for the complete graph Kn, except possibly when n\u2208{15,18,22}.", "num_citations": "14\n", "authors": ["325"]}
{"title": "Cluttered orderings for the complete graph\n", "abstract": " In a systematic erasure code for the correction of two simultaneous erasures, each information symbol must have two associated parity symbols. When implemented in a redundant array of independent disks (RAID), performance requirements on the update penalty necessitate that each information symbol be associated with no more parity symbols than the two required. This leads to a simple graph model of the erasure codes, with parity symbols as vertices and information symbols as edges. Ordering the edges so that no more than f check disks (vertices) appear amongan y set of d consecutive edges is found to optimize access performance of the disk array when d is maximized. These cluttered orderings are examined for the complete graph K                         n. The maximum number d of edges is determined precisely when f \u2264 5 and when f = n - 1, and bounds are derived in the remainingcases.", "num_citations": "13\n", "authors": ["325"]}
{"title": "End-to-end molecular communication channels in cell metabolism: an information theoretic study\n", "abstract": " The opportunity to control and fine-tune the behavior of biological cells is a fascinating possibility for many diverse disciplines, ranging from medicine and ecology, to chemical industry and space exploration. While synthetic biology is providing novel tools to reprogram cell behavior from their genetic code, many challenges need to be solved before it can become a true engineering discipline, such as reliability, safety assurance, reproducibility and stability. This paper aims to understand the limits in the controllability of the behavior of a natural (non-engineered) biological cell. In particular, the focus is on cell metabolism, and its natural regulation mechanisms, and their ability to react and change according to the chemical characteristics of the external environment. To understand the aforementioned limits of this ability, molecular communication is used to abstract biological cells into a series of channels that\u00a0\u2026", "num_citations": "12\n", "authors": ["325"]}
{"title": "Easing the generation of predictive human performance models from legacy systems\n", "abstract": " With the rise of tools for predictive human performance modeling in HCI comes a need to model legacy applications. Models of legacy systems are used to compare products to competitors, or new proposed design ideas to the existing version of an application. We present CogTool-Helper, an exemplar of a tool that results from joining this HCI need to research in automatic GUI testing from the Software Engineering testing community. CogTool-Helper uses automatic UI-model extraction and test case generation to automatically create CogTool storyboards and models and infer methods to accomplish tasks beyond what the UI designer has specified. A design walkthrough with experienced CogTool users reveal that CogTool-Helper resonates with a\" pain point\" of real-world modeling and provide suggestions for future work.", "num_citations": "12\n", "authors": ["325"]}
{"title": "Steiner triple systems as multiple erasure correcting codes in large disk arrays\n", "abstract": " Desirable RAID architectures provide fast performance with minimal overhead costs and short reconstruction times. Increasing the size of a disk array decreases its reliability. Developing new architectures that guarantee no information is lost in the presence of multiple disk failures, therefore, becomes important as disk arrays grow large. One class of 3-erasure correcting codes which have minimal check disk overhead and update penalty arise from a class of well studied combinatorial designs, the Steiner triple systems. They provide data recovery for a large number of 4-erasures as well. A preliminary performance analysis using Steiner triple systems of order 15 as mappings for RAID is performed. The ordering of triples within a system appears to have the greatest impact on system performance.", "num_citations": "11\n", "authors": ["325"]}
{"title": "Modeling and testing a family of surgical robots: an experience report\n", "abstract": " Safety-critical applications often use dependability cases to validate that specified properties are invariant, or to demonstrate a counter example showing how that property might be violated. However, most dependability cases are written with a single product in mind. At the same time, software product lines (families of related software products) have been studied with the goal of modeling variability and commonality, and building family based techniques for both analysis and testing. However, there has been little work on building an end to end dependability case for a software product line (where a property is modeled, a counter example is found and then validated as a true positive via testing), and none that we know of in an emerging safety-critical domain, that of robotic surgery. In this paper, we study a family of surgical robots, that combine hardware and software, and are highly configurable, representing over\u00a0\u2026", "num_citations": "10\n", "authors": ["325"]}
{"title": "EventFlowSlicer: A tool for generating realistic goal-driven GUI tests\n", "abstract": " Most automated testing techniques for graphical user interfaces (GUIs) produce test cases that are only concerned with covering the elements (widgets, menus, etc.) on the interface, or the underlying program code, with little consideration of test case semantics. This is effective for functional testing where the aim is to find as many faults as possible. However, when one wants to mimic a real user for evaluating usability, or when it is necessary to extensively test important end-user tasks of a system, or to generate examples of how to use an interface, this generation approach fails. Capture and replay techniques can be used, however there are often multiple ways to achieve a particular goal, and capturing all of these is usually too time consuming and unrealistic. Prior work on human performance regression testing introduced a constraint based method to filter test cases created by a functional test case generator, however that work did not capture the specifications, or directly generate only the required tests and considered only a single type of test goal. In this paper we present EventFlowSlicer, a tool that allows the GUI tester to specify and generate all realistic test cases relevant to achieve a stated goal. The user first captures relevant events on the interface, then adds constraints to provide restrictions on the task. An event flow graph is extracted containing only the widgets of interest for that goal. Next all test cases are generated for edges in the graph which respect the constraints. The test cases can then be replayed using a modified version of GUITAR.", "num_citations": "9\n", "authors": ["325"]}
{"title": "Profiling deployed software: Strategic probe placement\n", "abstract": " Profiling deployed software provides valuable insights for quality improvement activities. The probes required for profiling, however, can cause an unacceptable performance overhead for users. In previous work we have shown that significant overhead reduction can be achieved, with limited information loss, through the distribution of probes across deployed instances. However, existing strategies for probe distribution do not account for several relevant factors: acceptable overheads may vary, the distributions to be deployed may be limited, profiled events may have different likelihoods, and the user pool composition may be unknown. This paper evaluates strategies for probe distribution while manipulating these factors through an empirical study. Our findings indicate that for tight overhead bounds: 1) deploying numerous versions with complementary probe distributions can compensate for the reduction of probes per deployed instance, and 2) techniques that balance probe allocation, consider groupings and prior-allocations, can generate distributions that retain significantly more field information.", "num_citations": "8\n", "authors": ["325"]}
{"title": "Combinatorial test design in practice\n", "abstract": " Combinatorial testing is a specification based sampling technique that provides a systematic way to select combinations of program inputs or features for testing. It has been applied over the years to test input data, configurations, web forms, protocols, graphical user interfaces and for testing software product lines. This tutorial introduces the fundamentals of combinatorial testing, including both practical and theoretical foundations, to provide a comprehensive introduction that is relevant to both test practitioners and software engineering researchers. The tutorial will present an overview of Combinatorial Test Design (CTD) and describe some state of the art research advances and domains where CTD has been applied. It will present the theoretical underpinnings of CTD and explain a few algorithmic techniques used to generate CTD samples, as well as describe recent work on practical extensions to these\u00a0\u2026", "num_citations": "7\n", "authors": ["325"]}
{"title": "Ordering disks for double erasure codes\n", "abstract": " Dish arrays have been designed with two competing goals in mind, the ability to reconstruct erased disks (reliability), and the speed with which information can be read, written, and reconstructed (performance). The substantial loss in performance of write operations as reliability requirements increase has resulted in an emphasis on performance at the expense of reliability. This has proved acceptable given the relatively small members of disks in current disk arrays. We develop a method for improving the performance of write operations in disk arrays capable of correcting any double erasure, by ordering the columns of the erasure code to minimize the amount of parity information that requires updating. For large disk arrays, this affords a method to support the reliability needed without the generally accepted loss of performance.", "num_citations": "7\n", "authors": ["325"]}
{"title": "Scaling up the Fitness Function for Reverse Engineering Feature Models\n", "abstract": " Recent research on software product line engineering has led to several search-based frameworks for reverse engineering feature models. The most common fitness function utilized maximizes the number of matched products with an oracle set of products. However, to calculate this fitness each product defined by the chromosome has to be enumerated using a SAT solver and this limits scalability to product lines with fewer than 30 features. In this paper we propose , a fitness function that simulates validity by computing the difference between constraints in the chromosome and oracle. In an empirical study on 101 feature models comparing  with two existing fitness functions that use the enumeration technique we find that  shows a significant improvement over one, and no significant difference with the other one. We also find that  requires only 7\u00a0% of the runtime on average\u00a0\u2026", "num_citations": "6\n", "authors": ["325"]}
{"title": "GitSonifier: using sound to portray developer conflict history\n", "abstract": " There are many tools that help software engineers analyze data about their software, projects, and teams. These tools primarily use visualizations to portray data in a concise and understandable way. However, software engineering tasks are often multi-dimensional and temporal, making some visualizations difficult to understand. An alternative for representing data, which can easily incorporate higher dimensionality and temporal information, is the use of sound. In this paper we propose the use of sonification to help portray collaborative development history. Our approach, GitSonifier, combines sound primitives to represent developers, days, and conflicts over the history of a program's development. In a formative user study on an open source project's data, we find that users can easily extract meaningful information from sound clips and differentiate users, passage of time, and development conflicts, suggesting\u00a0\u2026", "num_citations": "6\n", "authors": ["325"]}
{"title": "DNA as features: organic software product lines\n", "abstract": " Software product line engineering is a best practice for managing reuse in families of software systems. In this work, we explore the use of product line engineering in the emerging programming domain of synthetic biology. In synthetic biology, living organisms are programmed to perform new functions or improve existing functions. These programs are designed and constructed using small building blocks made out of DNA. We conjecture that there are families of products that consist of common and variable DNA parts, and we can leverage product line engineering to help synthetic biologists build, evolve, and reuse these programs. As a first step towards this goal, we perform a domain engineering case study that leverages an open-source repository of more than 45,000 reusable DNA parts. We are able to identify features and their related artifacts, all of which can be composed to make different programs. We\u00a0\u2026", "num_citations": "5\n", "authors": ["325"]}
{"title": "The Assurance Recipe: Facilitating Assurance Patterns\n", "abstract": " As assurance cases have grown in popularity for safety-critical systems, so too has their complexity and thus the need for methods to systematically build them. Assurance cases can grow too large and too abstract for anyone but the original builders to understand, making reuse difficult. Reuse is important because different systems might have identical or similar components, and a good solution for one system should be applicable to similar systems. Prior research has shown engineers can alleviate some of the complexity issues through modularity and identifying common patterns which are more easily understood for reuse across different systems. However, we believe these patterns are too complicated for users who lack expertise in software engineering or assurance cases. This paper suggests the concept of lower-level patterns which we call recipes. We use the safety-critical field of synthetic\u00a0\u2026", "num_citations": "5\n", "authors": ["325"]}
{"title": "An evolutionary approach for analyzing Alloy specifications\n", "abstract": " Formal methods use mathematical notations and logical reasoning to precisely define a program's specifications, from which we can instantiate valid instances of a system. With these techniques we can perform a multitude of tasks to check system dependability. Despite the existence of many automated tools including ones considered lightweight, they still lack a strong adoption in practice. At the crux of this problem, is scalability and applicability to large real world applications. In this paper we show how to relax the completeness guarantee without much loss, since soundness is maintained. We have extended a popular lightweight analysis, Alloy, with a genetic algorithm. Our new tool, EvoAlloy, works at the level of finite relations generated by Kodkod and evolves the chromosomes based on the failed constraints. In a feasibility study we demonstrate that we can find solutions to a set of specifications beyond the\u00a0\u2026", "num_citations": "5\n", "authors": ["325"]}
{"title": "Understanding Git history: a multi-sense view\n", "abstract": " Version control systems archive data about the development history of a project, which can be used to analyze and understand different facets of a software project. The project history can be used to evaluate the development process of a team, as an aid in bug fixing, or to help new members get on track with development. However, state of the art techniques for analyzing version control data provide only partial views into this information, and lack an easy way to present all the dimensions of the data. In this paper we present GitVS, a hybrid view that incorporates visualization and sonification to represent the multiple dimensions of version control data-development time line, conflicts, etc. In a formative user study comparing the GitHub Network Graph, GitVS, and a version of GitVS without sound, we show GitVS improves over the GitHub Network Graph and that while sound makes it easier to correctly understand\u00a0\u2026", "num_citations": "5\n", "authors": ["325"]}
{"title": "Splrevo: optimizing complex feature models in search based reverse engineering of software product lines\n", "abstract": " Recent work to reverse engineer feature models from a set of products leverages a genetic algorithm. This idea has been realized as a framework called ETHOM, however, it suffers from two limitations. First, the existing fitness functions either do not penalize models that contain products which are not in the original dataset, or create models which are missing too many valid products. This results in models that greatly over or under-approximate the feature space. Second, the ETHOM framework only supports models with simple cross-tree constraints in the form of binary excludes or requires, meaning that feature models with complex cross-tree constraints may not be captured precisely. In this work, we present an improved fitness function that more effectively guides the search to balance both additional and missing products. In addition, we have created a new framework, called SPLRevO, containing a chromosome and additional mutation and crossover operators to support complex cross-tree constraints. In a study spanning 99 feature models of various sizes, both with and without complex cross-tree constraints, we show that our new fitness within the ETHOM framework improves the F-measure over the existing fitness functions from 7 to 50%. When the new fitness is used within SPLRevO, we increase the F-measure over ETHOM by 20 to 63%.", "num_citations": "5\n", "authors": ["325"]}
{"title": "Vdtest: an automated framework to support testing for virtual devices\n", "abstract": " The use of virtual devices in place of physical hardware is increasing in activities such as design, testing and debugging. Yet virtual devices are simply software applications, and like all software they are prone to faults. A full system simulator (FSS), is a class of virtual machine that includes a large set of virtual devices--enough to run the full target software stack. Defects in an FSS virtual device may have cascading effects as the incorrect behavior can be propagated forward to many different platforms as well as to guest programs. In this work we present VDTest, a novel framework for testing virtual devices within an FSS. VDTest begins by generating a test specification obtained through static analysis. It then employs a two-phase testing approach to test virtual components both individually and in combination. It leverages a differential oracle strategy, taking advantage of the existence of a physical or golden device\u00a0\u2026", "num_citations": "4\n", "authors": ["325"]}
{"title": "Guided test generation for finding worst-case stack usage in embedded systems\n", "abstract": " Embedded systems are challenging to program correctly, because they use an interrupt programming paradigm and run in resource constrained environments. This leads to a class of faults for which we need customized verification techniques. One such class of faults, stack overflows, are caused when the combination of active methods and interrupt invocations on the stack grows too large, and these can lead to data loss and other significant device failures. Developers need to estimate the worst-case stack usage (WCSU) during system design, but determining the actual maximum value is known to be a hard problem. The state of the art for calculating WCSU uses static analysis, however this has a tendency to over approximate the potential stack which can lead to wasted resources. Dynamic techniques such as random testing often under approximate the WCSU. In this paper, we present SIMSTACK, a\u00a0\u2026", "num_citations": "4\n", "authors": ["325"]}
{"title": "Linking graphical user interface testing tools and human performance modeling to enable usability assessment\n", "abstract": " Methods of providing usability predictions are set forth herein. A method includes obtaining test cases for analyzing usability of a graphical user interface of a given software application. The method further includes automatically generating, using a processor, a human performance model based on the test cases. The method also includes generating the usability predictions based on the human performance model. At least some of the test cases are generated automatically from an analysis of the given software application.", "num_citations": "4\n", "authors": ["325"]}
{"title": "The Maturation of Search-Based Software Testing: Successes and Challenges\n", "abstract": " In this paper we revisit the field of search-based software testing (SBST) in the context of its technological maturity. We highlight some successes with respect to tools, hybrid approaches, extensions and industry adoption. We then discuss some open challenges that remain for SBST including the need for new approaches to system testing, automated oracle generation, incorporating humans into the search process, and leveraging learning through hyper-heuristic search.", "num_citations": "3\n", "authors": ["325"]}
{"title": "BioSIMP: using software testing techniques for sampling and inference in biological organisms\n", "abstract": " Years of research in software engineering have given us novel ways to reason about, test, and predict the behavior of complex software systems that contain hundreds of thousands of lines of code. Many of these techniques have been inspired by nature such as genetic algorithms, swarm intelligence, and ant colony optimization. In this paper we reverse the direction and present BioSIMP, a process that models and predicts the behavior of biological organisms to aid in the emerging field of systems biology. It utilizes techniques from testing and modeling of highly-configurable software systems. Using both experimental and simulation data we show that BioSIMP can find important environmental factors in two microbial organisms. However, we learn that in order to fully reason about the complexity of biological systems, we will need to extend existing or create new software engineering techniques.", "num_citations": "3\n", "authors": ["325"]}
{"title": "EventFlowSlicer: Goal based test generation for graphical user interfaces\n", "abstract": " Automated test generation techniques for graphical user interfaces include model-based approaches that generate tests from a graph or state machine model, capture-replay methods that require the user to demonstrate each test case, and pattern-based approaches that provide templates for abstract test cases. There has been little work, however, in automated goal-based testing, where the goal is a realistic user task, a function, or an abstract behavior. Recent work in human performance regression testing has shown that there is a need for generating multiple test cases that execute the same user task in different ways, however that work does not have an efficient way to generate tests and only a single type of goal has been considered.", "num_citations": "3\n", "authors": ["325"]}
{"title": "The Assurance Timeline: Building Assurance Cases for Synthetic Biology\n", "abstract": " Recent research advances in modifying and controlling DNA have created a booming field of biological engineering called synthetic biology. In synthetic biology engineers manipulate and modify living organisms to change (and produce entirely novel) functionality, which has led to new fuel sources or the ability to mitigate pollution. Synthetic biology research is also expected to lead to methods of intelligent drug delivery. In synthetic biology, designs are first built using biological programming languages and then implemented in a laboratory. These synthetic organisms can be considered living programs that will sense, respond and interact with humans while they persist in the natural environment. We argue that we should view these as safety critical devices which can be both regulated and certified. Since the synthetically engineered organisms follow a regular cycle of reproduction and replication that\u00a0\u2026", "num_citations": "3\n", "authors": ["325"]}
{"title": "Partial Specifications for Program Repair\n", "abstract": " In this paper we argue for using many partial test suites instead of one full test suite during program repair. This may provide a pool of simpler, yet correct patches, addressing both the overfitting and poor repair quality problem. To support this idea, we present some insight obtained running APR partial test suites on the well studied triangle program.", "num_citations": "2\n", "authors": ["325"]}
{"title": "CRNRepair: Automated Program Repair of Chemical Reaction Networks\n", "abstract": " Chemical reaction networks (CRNs) are abstractions of distributed networks that form the foundations of many natural phenomena such as biological processes. These can be encoded and/or compiled into DNA and have been shown to be Turing complete. Before CRNs are implemented in a physical environment, they are often simulated in programming environments. Researchers have recently designed a software testing framework for CRNs, however, repairing CRN programs is still a manual task. Finding and repairing the faults can be difficult without automated support. In this paper we present CRNRepair, a program repair framework for CRN programs. We built our framework on top of an existing APR framework and use a testing infrastructure built in the Matlab SimBiology package. We adapt it to use the SBML representation for its abstract syntax tree. In a case study on 19 mutant versions of 2 programs\u00a0\u2026", "num_citations": "2\n", "authors": ["325"]}
{"title": "Metabolic Feedback Inhibition Influences Metabolite Secretion by the Human Gut Symbiont Bacteroides thetaiotaomicron\n", "abstract": " Microbial metabolism and trophic interactions between microbes give rise to complex multispecies communities in microbe-host systems. Bacteroides thetaiotaomicron (B. theta) is a human gut symbiont thought to play an important role in maintaining host health. Untargeted nuclear magnetic resonance metabolomics revealed B. theta secretes specific organic acids and amino acids in defined minimal medium. Physiological concentrations of acetate and formate found in the human intestinal tract were shown to cause dose-dependent changes in secretion of metabolites known to play roles in host nutrition and pathogenesis. While secretion fluxes varied, biomass yield was unchanged, suggesting feedback inhibition does not affect metabolic bioenergetics but instead redirects carbon and energy to CO2 and H2. Flux balance analysis modeling showed increased flux through CO2-producing reactions under\u00a0\u2026", "num_citations": "2\n", "authors": ["325"]}
{"title": "Using a Genetic Algorithm to Optimize Configurations in a Data-Driven Application\n", "abstract": " Users of highly-configurable software systems often want to optimize a particular objective such as improving a functional outcome or increasing system performance. One approach is to use an evolutionary algorithm. However, many applications today are data-driven, meaning they depend on inputs or data which can be complex and varied. Hence, a search needs to be run (and re-run) for all inputs, making optimization a heavy-weight and potentially impractical process. In this paper, we explore this issue on a data-driven highly-configurable scientific application. We build an exhaustive database containing 3,000 configurations and 10,000 inputs, leading to almost 100 million records as our oracle, and then run a genetic algorithm individually on each of the 10,000 inputs. We ask if (1) a genetic algorithm can find configurations to improve functional objectives; (2) whether patterns of best configurations over all\u00a0\u2026", "num_citations": "2\n", "authors": ["325"]}
{"title": "ChemTest: An Automated Software Testing Framework for an Emerging Paradigm\n", "abstract": " In recent years the use of non-traditional computing mechanisms has grown rapidly. One paradigm uses chemical reaction networks (CRNs) to compute via chemical interactions. CRNs are used to prototype molecular devices at the nanoscale such as intelligent drug therapeutics. In practice, these programs are first written and simulated in environments such as MatLab and later compiled into physical molecules such as DNA strands. However, techniques for testing the correctness of CRNs are lacking. Current methods of validating CRNs include model checking and theorem proving, but these are limited in scalability. In this paper we present the first (to the best of our knowledge) testing framework for CRNs, ChemTest. ChemTest evaluates test oracles on individual simulation traces and supports functional, metamorphic, internal and hyper test cases. It also allows for flakiness and programs that are probabilistic\u00a0\u2026", "num_citations": "2\n", "authors": ["325"]}
{"title": "EvoIsolator: Evolving Program Slices for Hardware Isolation Based Security\n", "abstract": " To provide strong security support for today\u2019s applications, microprocessor manufacturers have introduced hardware isolation, an on-chip mechanism that provides secure accesses to sensitive data. Currently, hardware isolation is still difficult to use by software developers because the process to identify access points to sensitive data is error-prone and can lead to under and over protection of sensitive data. Under protection can lead to security vulnerabilities. Over protection can lead to an increased attack surface and excessive communication overhead. In this paper we describe EvoIsolator, a search-based framework to (i) automatically generate executable minimal slices that include all access points to a set of specified sensitive data; and (ii) automatically optimize (for small code block size and low communication overhead) the code modules for hardware isolation. We demonstrate, through a small\u00a0\u2026", "num_citations": "2\n", "authors": ["325"]}
{"title": "The evolutionary landscape of SBST: A 10 year perspective\n", "abstract": " A key indicator of the health and quality of any evolutionary algorithm is the landscape of its search. By analyzing the landscape one can determine the peaks (local maxima) where significant solutions exist. In this paper we examine the landscape for the history of the International Workshop on Search-Based Software Testing (SBST) within the context of the broader field of search-based software testing. We study the evolution of the field, highlighting key advances during three phases of its ten year history. In 2008 the focus of SBST was inner looking, with advances in existing search techniques, improvements to individual generation techniques, and methods to transform the problem space for search effectiveness. However, diverse seeds of new ideas (such as automated program repair) were already being injected into the population. A few SBST tools existed, but the engineer still required skill and expertise to\u00a0\u2026", "num_citations": "2\n", "authors": ["325"]}
{"title": "A self-adjusting code cache manager to balance start-up time and memory usage\n", "abstract": " In virtual machines for embedded devices that use just-in-time compilation, the management of the code cache can significantly impact performance in terms of both memory usage and start-up time. Although improving memory usage has been a common focus for system designers, start-up time is often overlooked. In systems with constrained resources, however, these two performance metrics are often at odds and must be considered together. In this paper, we present an adaptive self-adjusting code cache manager to improve performance with respect to both start-up time and memory usage. It balances these concerns by detecting changes in method compilation rates, resizing the cache after each pitching event. We conduct experiments to validate our proposed system and quantify the impacts that different code cache management techniques have on memory usage and start-up time through two oracle\u00a0\u2026", "num_citations": "2\n", "authors": ["325"]}
{"title": "A hybrid approach to testing for nonfunctional faults in embedded systems using genetic algorithms\n", "abstract": " Embedded systems are challenging to program correctly, because they use an interrupt\u2010driven programming paradigm and run in resource\u2010constrained environments. This leads to various classes of nonfunctional faults that can be detected only by customized verification techniques. These nonfunctional faults are specifically related to usage of resources such as time and memory. For example, the presence of interrupts can induce delays in interrupt servicing and in system execution time. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. As another example, stack overflows are caused when the combination of active methods and interrupt invocations on the stack grows too large, and these can lead to data loss and other significant device failures. To detect these types of nonfunctional faults, developers need to estimate\u00a0\u2026", "num_citations": "1\n", "authors": ["325"]}
{"title": "Budgeted testing through an algorithmic lens\n", "abstract": " Automated testing has been a focus of research for a long time. As such, we tend to think about this in a coverage centric manner. Testing budgets have also driven research such as prioritization and test selection, but as a secondary concern. As our systems get larger, are more dynamic, and impact more people with each change, we argue that we should switch from a coverage centric view to a budgeted testing centric view. Researchers in other fields have designed approximation algorithms for such budgeted scenarios and these are often simple to implement and run. In this paper we present an exemplar study on combinatorial interaction testing (CIT) to show that a budgeted greedy algorithm, when adapted to our problem for various budgets, does almost as well coverage-wise as a state of the art greedy CIT algorithm, better in some cases than a state of the art simulated annealing, and always improves over\u00a0\u2026", "num_citations": "1\n", "authors": ["325"]}