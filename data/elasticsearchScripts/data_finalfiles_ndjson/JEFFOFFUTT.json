{"title": "Constraint-based automatic test data generation\n", "abstract": " This paper presents a new technique for automatically generating test data. The technique is based on mutation analysis and creates test data that approximates relative-adequacy. The technique is a fault-based technique that uses algebraic constraints to describe test cases designed to find particular types of faults. A set of tools, collectively called Godzilla, has been implemented that automatically generates constraints and solves them to create test cases for unit and module testing. Godzilla has been integrated with the Mothra testing system and has been used as an effective way to generate test data that kills program mutants.", "num_citations": "1149\n", "authors": ["53"]}
{"title": "MuJava: an automated class mutation system\n", "abstract": " Several module and class testing techniques have been applied to object\u0393\u00c7\u00c9oriented (OO) programs, but researchers have only recently begun developing test criteria that evaluate the use of key OO features such as inheritance, polymorphism, and encapsulation. Mutation testing is a powerful testing technique for generating software tests and evaluating the quality of software. However, the cost of mutation testing has traditionally been so high that it cannot be applied without full automated tool support. This paper presents a method to reduce the execution cost of mutation testing for OO programs by using two key technologies, mutant schemata generation (MSG) and bytecode translation. This method adapts the existing MSG method for mutants that change the program behaviour and uses bytecode translation for mutants that change the program structure. A key advantage is in performance: only two compilations\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "851\n", "authors": ["53"]}
{"title": "Generating tests from UML specifications\n", "abstract": " Although most industry testing of complex software is conducted at the system level, most formal research has focused on the unit level. As a result, most system level testing techniques are only described informally. This paper presents a novel technique that adapts pre-defined state-based specification test data generation criteria to generate test cases from UML statecharts. UML statecharts provide a solid basis for test generation in a form that can be easily manipulated. This technique includes coverage criteria that enable highly effective tests to be developed. To demonstrate this technique, a tool has been developed that uses UML statecharts produced by Rational Software Corporation\u0393\u00c7\u00d6s Rational Rose tool to generate test data. Experimental results from using this tool are presented.", "num_citations": "657\n", "authors": ["53"]}
{"title": "Combination testing strategies: a survey\n", "abstract": " Combination strategies are test case selection methods that identify test cases by combining values of the different test object input parameters based on some combinatorial strategy. This survey presents 16 different combination strategies, covering more than 40 papers that focus on one or several combination strategies. This collection represents most of the existing work performed on combination strategies. This survey describes the basic algorithms used by the combination strategies. Some properties of combination strategies, including coverage criteria and theoretical bounds on the size of test suites, are also included in this description. This survey paper also includes a subsumption hierarchy that attempts to relate the various coverage criteria associated with the identified combination strategies. Copyright \u252c\u2310 2005 John Wiley & Sons, Ltd.", "num_citations": "575\n", "authors": ["53"]}
{"title": "Quality attributes of web software applications\n", "abstract": " Web applications have very high requirements for numerous quality attributes. This article discusses some of the technological challenges of building today's complex Web software applications, their unique quality requirements, and how to achieve them.", "num_citations": "540\n", "authors": ["53"]}
{"title": "A fortran language system for mutation\u0393\u00c7\u00c9based software testing\n", "abstract": " Mutation analysis is a powerful technique for testing software systems. The Mothra software testing project uses mutation analysis as the basis for an integrated software testing environment. Mutation analysis requires executing many slightly differing versions of the same program to evaluate the quality of the data used to test the program. The current version of Mothra includes a complete language system that translates a program to be tested into intermediate code so that it and its mutated versions can be executed by an interpreter. In this paper, we discuss some of the unique requirements of a language system used in a mutation\u0393\u00c7\u00c9based testing environment. We then describe how these requirements affected the design and implementation of the Fortran 77 version of the Mothra system. We also describe the intermediate language used by Mothra and the features of the language system that are needed for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "520\n", "authors": ["53"]}
{"title": "Mutation 2000: Uniting the orthogonal\n", "abstract": " Mutation testing is a powerful, but computationally expensive, technique for unit testing software. This expense has prevented mutation from becoming widely used in practical situations, but recent engineering advances have given us techniques and algorithms for significantly reducing the cost of mutation testing. These techniques include a new algorithmic execution technique called schema-based mutation, an approximation technique called weak mutation, a reduction technique called selective mutation, heuristics for detecting equivalent mutants, and algorithms for automatic test data generation. This paper reviews experimentation with these advances and outlines a design for a system that will approximate mutation, but in a way that will be accessible to everyday programmers. We envision a system to which a programmer can submit a program unit and get back a set of input/output pairs that are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "504\n", "authors": ["53"]}
{"title": "Testing web applications by modeling with FSMs\n", "abstract": " Researchers and practitioners are still trying to find effective ways to model and test Web applications. This paper proposes a system-level testing technique that combines test generation based on finite state machines with constraints. We use a hierarchical approach to model potentially large Web applications. The approach builds hierarchies of Finite State Machines (FSMs) that model subsystems of the Web applications, and then generates test requirements as subsequences of states in the FSMs. These subsequences are then combined and refined to form complete executable tests. The constraints are used to select a reduced set of inputs with the goal of reducing the state space explosion otherwise inherent in using FSMs. The paper illustrates the technique with a running example of a Web-based course student information system and introduces a prototype implementation to support the technique.", "num_citations": "487\n", "authors": ["53"]}
{"title": "Investigations of the software testing coupling effect\n", "abstract": " Fault-based testing strategies test software by focusing on specific, common types of faults. The coupling effect hypothesizes that test data sets that detect simple types of faults are sensitive enough to detect more complex types of faults. This paper describes empirical investigations into the coupling effect over a specific class of software faults. All of the results from this investigation support the validity of the coupling effect. The major conclusion from this investigation is the fact that by explicitly testing for simple faults, we are also implicitly testing for more complicated faults, giving us confidence that fault-based testing is an effective way to test software.", "num_citations": "477\n", "authors": ["53"]}
{"title": "Generating test data from state\u0393\u00c7\u00c9based specifications\n", "abstract": " Although the majority of software testing in industry is conducted at the system level, most formal research has focused on the unit level. As a result, most system\u0393\u00c7\u00c9level testing techniques are only described informally. This paper presents formal testing criteria for system level testing that are based on formal specifications of the software. Software testing can only be formalized and quantified when a solid basis for test generation can be defined. Formal specifications represent a significant opportunity for testing because they precisely describe what functions the software is supposed to provide in a form that can be automatically manipulated. This paper presents general criteria for generating test inputs from state\u0393\u00c7\u00c9based specifications. The criteria include techniques for generating tests at several levels of abstraction for specifications (transition predicates, transitions, pairs of transitions and sequences of transitions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "475\n", "authors": ["53"]}
{"title": "Automatically detecting equivalent mutants and infeasible paths\n", "abstract": " Mutation testing is a technique for testing software units that has great potential for improving the quality of testing, and thereby increasing the ability to assure the high reliability of critical software. It will be shown that recent advances in mutation research have brought a practical mutation testing system closer to reality. One recent advance is a partial solution to the problem of automatically detecting equivalent mutant programs. Equivalent mutants are currently detected by hand, which makes it very expensive and time\u0393\u00c7\u00c9consuming. The problem of detecting equivalent mutants is a specific instance of a more general problem, commonly called the feasible path problem, which says that for certain structural testing criteria some of the test requirements are infeasible in the sense that the semantics of the program imply that no test case satisfies the test requirements. Equivalent mutants, unreachable statements in path\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "387\n", "authors": ["53"]}
{"title": "An extended overview of the Mothra software testing environment\n", "abstract": " The authors give a brief introduction to mutation analysis. They they discuss Mothra, emphasizing how it interacts with the tester. The authors present some major problems with using mutation analysis and discuss possible solutions. They conclude with a solution to one of these problems-a method of automatically generating mutation-adequate data.<>", "num_citations": "387\n", "authors": ["53"]}
{"title": "Using UML collaboration diagrams for static checking and test generation\n", "abstract": " Software testing can only be formalized and quantified when a solid basis for test generation can be defined. Tests are commonly generated from program source code, graphical models of software (such as control flow graphs), and specifications/requirements. UML collaboration diagrams represent a significant opportunity for testing because they precisely describe how the functions the software provides are connected in a form that can be easily manipulated by automated means. This paper presents novel test criteria that are based on UML collaboration diagrams. The most novel aspect of this is that tests can be generated automatically from the software design, rather than the code or the specifications. Criteria are defined for both static and dynamic testing of specification-level and instance-level collaboration diagrams. These criteria allow a formal integration tests to be based on high level design\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "355\n", "authors": ["53"]}
{"title": "Inter-class mutation operators for Java\n", "abstract": " The effectiveness of mutation testing depends heavily on the types of faults that the mutation operators are designed to represent. Therefore, the quality of the mutation operators is key to mutation testing. Mutation testing has traditionally been applied to procedural-based languages, and mutation operators have been developed to support most of their language features. Object-oriented programming languages contain new language features, most notably inheritance, polymorphism, and dynamic binding. Not surprisingly; these language features allow new kinds of faults, some of which are not modeled by traditional mutation operators. Although mutation operators for OO languages have previously been suggested, our work in OO faults indicate that the previous operators are insufficient to test these OO language features, particularly at the class testing level. This paper introduces a new set of class mutation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "317\n", "authors": ["53"]}
{"title": "Generating test cases for web services using data perturbation\n", "abstract": " Web services have the potential to dramatically reduce the complexities and costs of software integration projects. The most obvious and perhaps most significant difference between Web services and traditional applications is that Web services use a common communication infrastructure, XML and SOAP, to communicate through the Internet. The method of communication introduces complexities to the problems of verifying and validating Web services that do not exist in traditional software. This paper presents a new approach to testing Web services based on data perturbation. Existing XML messages are modified based on rules defined on the message grammars, and then used as tests. Data perturbation uses two methods to test Web services: data value perturbation and interaction perturbation. Data value perturbation modifies values according to the data type. Interaction perturbation classifies the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "275\n", "authors": ["53"]}
{"title": "An empirical evaluation of weak mutation\n", "abstract": " Mutation testing is a fault-based technique for unit-level software testing. Weak mutation was proposed as a way to reduce the expense of mutation testing. Unfortunately, weak mutation is also expected to provide a weaker test of the software than mutation testing does. This paper presents results from an implementation of weak mutation, which we used to evaluate the effectiveness versus the efficiency of weak mutation. Additionally, we examined several options in an attempt to find the most appropriate way to implement weak mutation. Our results indicate that weak mutation can be applied in a manner that is almost as effective as mutation testing, and with significant computational savings.< >", "num_citations": "265\n", "authors": ["53"]}
{"title": "An experimental evaluation of data flow and mutation testing\n", "abstract": " Two experimental comparisons of data flow and mutation testing are presented. These techniques are widely considered to be effective for unit\u0393\u00c7\u00c9level software testing, but can only be analytically compared to a limited extent. We compare the techniques by evaluating the effectiveness of test data developed for each. We develop ten independent sets of test data for a number of programs: five to satisfy the mutation criterion and five to satisfy the all\u0393\u00c7\u00c9uses data\u0393\u00c7\u00c9flow criterion. These test sets are developed using automated tools, in a manner consistent with the way a test engineer might be expected to generate test data in practice. We use these test sets in two separate experiments. First we measure the effectiveness of the test data that was developed for one technique in terms of the other. Second, we investigate the ability of the test sets to find faults. We place a number of faults into each of our subject programs, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "258\n", "authors": ["53"]}
{"title": "MuJava: a mutation system for Java\n", "abstract": " Mutation testing is a valuable experimental research technique that has been used in many studies. It has been experimentally compared with other test criteria, and also used to support experimental comparisons of other test criteria, by using mutants as a method to create faults. In effect, mutation is often used as a``gold standard''for experimental evaluations of test methods. Although mutation testing is powerful, it is a complicated and computationally expensive testing method. Therefore, automated tool support is indispensable for conducting mutation testing. This demo presents a publicly available mutation system for Java that supports both method-level mutants and class-level mutants. MuJava can be freely downloaded and installed with relative ease under both Unix and Windows. MuJava is offered as a free service to the community and we hope that it will promote the use of mutation analysis for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "256\n", "authors": ["53"]}
{"title": "The dynamic domain reduction procedure for test data generation\n", "abstract": " Test data generation is one of the most technically challenging steps of testing software, but most commercial systems currently incorporate very little automation for this step. This paper presents results from a project that is trying to find ways to incorporate test data generation into practical test processes. The results include a new procedure for automatically generating test data that incorporates ideas from symbolic evaluation, constraint\u0393\u00c7\u00c9based testing, and dynamic test data generation. It takes an initial set of values for each input, and dynamically \u0393\u00c7\u00ffpushes\u0393\u00c7\u00d6 the values through the control\u0393\u00c7\u00c9flow graph of the program, modifying the sets of values as branches in the program are taken. The result is usually a set of values for each input parameter that has the property that any choice from the sets will cause the path to be traversed. This procedure uses new analysis techniques, offers improvements over previous research\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "253\n", "authors": ["53"]}
{"title": "Abstract\n", "abstract": " Object-oriented design has caused a shift in focus from software units to the way software classes and components are connected. Thus, we are finding that we need less emphasis on unit testing and more on integration testing. The compositional relationships of inheritance and aggregation, especially when combined with polymorphism, introduce new kinds of integration faults, which cannot be covered unless testers use test criteria that specifically evaluate inheritance and polymorphism. This paper results from a set of experiments on the relative effectiveness of several coupling-based OO testing criteria and branch coverage. Tests developed according to the criteria were evaluated on programs seeded with faults that are specific to object-oriented programs. The paper uses a statistical analysis technique, log-linear analysis, that has not been widely used in software engineering but that is more applicable than analysis of variance in situations where the sample space is not well understood (such as with software). The data indicate that the OO criteria are all much more effective at detecting faults due to the misuse of inheritance and polymorphism than branch coverage.", "num_citations": "243\n", "authors": ["53"]}
{"title": "Procedures for reducing the size of coverage-based test sets\n", "abstract": " This paper addresses the problem of reducing the size of test sets for regression testing and test output inspection. Since regression testing requires the execution of some, and in the worst case, all test cases, reducing the number of tests can have a large benefit. Additionally, testers generally have to examine the output of each test case, both during initial and regression testing. Since this is done by hand, reducing the number of outputs that need to be examined can reduce the cost of testing. We observe that for mutation-based test sets, the order in which the test cases are executed impacts the size of the test sets. This paper presents several strategies for selecting a smaller number of test cases by reordering the test tests. We illustrate our technique using a proof-of-concept empirically study using mutation testing, achieving approximately a 33 reduction in size, and a corresponding reduction in the cost of regression testing, with a cost of only one extra run of the test case set. We suggest that these results should be extendable to apply to any test strategy that includes a quantifiable measure of test case effectiveness, such as data flow testing and branch testing, and try it with statement coverage with positive results.", "num_citations": "238\n", "authors": ["53"]}
{"title": "A semantic model of program faults\n", "abstract": " Program faults are artifacts that are widely studied, but there are many aspects of faults that we still do not understand. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas in testing. These include fault-based testing, testability, mutation testing, and the comparative evaluation of testing strategies. In this workshop paper, we explore the fundamental nature of faults by looking at the differences between a syntactic and semantic characterization of faults. We offer definitions of these characteristics and explore the differentiation. Specifically, we discuss the concept of \"size\" of program faults --- the measurement of size provides interesting and useful distinctions between the syntactic and semantic characterization of faults. We use the fault size observations to make several\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "228\n", "authors": ["53"]}
{"title": "Criteria for generating specification-based tests\n", "abstract": " This paper presents general criteria for generating test inputs from state-based specifications. Software testing can only be formalized and quantified when a solid basis for test generation can be defined. Formal specifications of complex systems represent a significant opportunity for testing because they precisely describe what functions the software is supposed to provide in a form that can easily be manipulated. These techniques provide coverage criteria that are based on the specifications, and are made up of several parts, including test prefixes that contain inputs necessary to put the software into the appropriate state for the test values. The test generation process includes several steps for transforming specifications to tests. Empirical results from a comparative case study application of these criteria are presented.", "num_citations": "215\n", "authors": ["53"]}
{"title": "Using compiler optimization techniques to detect equivalent mutants\n", "abstract": " Mutation analysis is a software testing technique that requires the tester to generate test data that will find specific, well\u0393\u00c7\u00c9defined errors. Mutation testing executes many slightly differing versions, called mutants, of the same program to evaluate the quality of the data used to test the program. Although these mutants are generated and executed efficiently by automated methods, many of the mutants are functionally equivalent to the original program and are not useful for testing. Recognizing and eliminating equivalent mutants is currently done by hand, a time\u0393\u00c7\u00c9consuming and arduous task. This problem is currently a major obstacle to the practical application of mutation testing. This paper presents extensions to previous work in detecting equivalent mutants; specifically, algorithms for determining several classes of equivalent mutants are presented, an implementation of these algorithms is discussed, and results from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "208\n", "authors": ["53"]}
{"title": "Experimental results from an automatic test case generator\n", "abstract": " Constraint-based testing is a novel way of generating test data to detect specific types of common programming faults. The conditions under which faults will be detected are encoded as mathematical systems of constraints in terms of program symbols. A set of tools, collectively called Godzilla, has been implemented that automatically generates constraint systems and solves them to create test cases for use by the Mothra testing system. Experimental results from using Godzilla show that the technique can produce test data that is very close in terms of mutation adequacy to test data that is produced manually, and at substantially reduced cost. Additionally, these experiments have suggested a new procedure for unit testing, where test cases are viewed as throw-away items rather than scarce resources.", "num_citations": "192\n", "authors": ["53"]}
{"title": "Testing web services by XML perturbation\n", "abstract": " The eXtensible Markup Language (XML) is widely used to transmit data across the Internet. XML schemas are used to defile the syntax of XML messages. XML-based applications can receive messages from arbitrary applications, as long as they follow the protocol defined by the schema. A receiving application must either validate XML messages, process the data in the XML message without validation, or modify the XML message to ensure that it conforms to the XML schema. A problem for developers is how well the application performs the validation, data processing, and, when necessary, transformation. This paper describes and gives examples of a method to generate tests for XML-based communication by modifying and then instantiating XML schemas. The modified schemas are based on precisely defined schema primitive perturbation operators", "num_citations": "186\n", "authors": ["53"]}
{"title": "Using formal methods to derive test frames in category-partition testing\n", "abstract": " Testing is a standard method of assuring that software performs as intended. We extend the category-partition method, which is a specification-based testing method. An important aspect of category-partition testing is the construction of test specifications as an intermediate between functional specifications and actual tests. We define a minimal coverage criterion for category-partition test specifications identify a mechanical process to produce a test specification that satisfies the criterion, and discuss the problem of resolving infeasible combinations of choices for categories. Our method uses formal schema-based functional specifications and is shown to be feasible with an example study of a simple file system.< >", "num_citations": "185\n", "authors": ["53"]}
{"title": "Modeling presentation layers of web applications for testing\n", "abstract": " Web software applications have become complex, sophisticated programs that are based on novel computing technologies. Their most essential characteristic is that they represent a different kind of software deployment\u0393\u00c7\u00f6most of the software is never delivered to customers\u0393\u00c7\u00d6 computers, but remains on servers, allowing customers to run the software across the web. Although powerful, this deployment model brings new challenges to developers and testers. Checking static HTML links is no longer sufficient; web applications must be evaluated as complex software products. This paper focuses on three aspects of web applications that are unique to this type of deployment: (1) an extremely loose form of coupling that features distributed integration, (2) the ability that users have to directly change the potential flow of execution, and (3) the dynamic creation of HTML forms. Taken together, these aspects allow the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "177\n", "authors": ["53"]}
{"title": "Establishing theoretical minimal sets of mutants\n", "abstract": " Mutation analysis generates tests that distinguish variations, or mutants, of an artifact from the original. Mutation analysis is widely considered to be a powerful approach to testing, and hence is often used to evaluate other test criteria in terms of mutation score, which is the fraction of mutants that are killed by a test set. But mutation analysis is also known to provide large numbers of redundant mutants, and these mutants can inflate the mutation score. While mutation approaches broadly characterized as reduced mutation try to eliminate redundant mutants, the literature lacks a theoretical result that articulates just how many mutants are needed in any given situation. Hence, there is, at present, no way to characterize the contribution of, for example, a particular approach to reduced mutation with respect to any theoretical minimal set of mutants. This paper's contribution is to provide such a theoretical foundation for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "176\n", "authors": ["53"]}
{"title": "Correcton to SOFL: A formal engineering methodology for industrial applicatoins\n", "abstract": " Shaoying Liu holds a BSc, and an MSc degree in computer science from Xi\u0393\u00c7\u00d6an Jiaotong University, the People\u0393\u00c7\u00d6s Republic of China, and a PhD in formal methods from the University of Manchester, United Kingdom. He is an associate professor in the Computer Science Department at Hiroshima City University. Dr. Liu worked as an assistant lecturer and a lecturer at Xi\u0393\u00c7\u00d6an Jiaotong University; a research associate at the University of York; and a research assistant at the Royal Holloway and Bedford New College of the University of London, respectively, before 1994. He was a visiting research fellow, by invitation, at The Queen\u0393\u00c7\u00d6s University of Belfast from December 1994 to February 1995 His research interests include formal methods, software development methodology, software evolution, software testing, software engineering environments, formal languages, and safety critical systems. Dr. Liu received an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "176\n", "authors": ["53"]}
{"title": "A software metric system for module coupling\n", "abstract": " Low module coupling is considered to be a desirable quality for modular programs to have. Previously, coupling has been defined subjectively and not quantified, making it difficult to use in practice. In this article, we extend previous work to reflect newer programming languages and quantify coupling by developing a general software metric system that allows us to automatically measure coupling. We have precisely defined the levels of coupling so that they can be determined algorithmically, incorporated the notion of direction into the coupling levels, and accounted for different types of nonlocal variables present in modern programming languages. With our system, we can measure the coupling between all pairs of modules in a system, measure the coupling of a particular module with all other modules in a system, and measure the coupling of an entire system. We have implemented our metric system so that it\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "163\n", "authors": ["53"]}
{"title": "UML-based integration testing for component-based software\n", "abstract": " Component-based software engineering is increasingly being adopted for software development. Currently, components delivered by component providers only include specifications of the interfaces. This imposes significant dificulties on adequate testing of an integrated component-based system. Without source code, many testing techniques will not be applicable. The Unified Modeling Language (UML) has been widely adopted in component-based software development processes. Many of its useful tools, such as interaction diagrams, statechart diagrams, and component diagrams, characterize the behavior of components in various aspects, and thus can be used to help test componentbased systems. In this paper, we first analyze different test elements that are critical to test component-based software, then we propose a group of UML-based test adequacy criteria that can be used to test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "162\n", "authors": ["53"]}
{"title": "An experimental comparison of four unit test criteria: Mutation, edge-pair, all-uses and prime path coverage\n", "abstract": " With recent increased expectations for quality, and the growth of agile processes and test driven development, developers are expected to do more and more effective unit testing. Yet, our knowledge of when to use the various unit level test criteria is incomplete. The paper presents results from a comparison of four unit level software testing criteria. Mutation testing, prime path coverage, edge pair coverage, and all-uses testing were compared on two bases: the number of seeded faults found and the number of tests needed to satisfy the criteria. The comparison used a collection of Java classes taken from various sources and hand-seeded faults. Tests were designed and generated mostly by hand with help from tools that compute test requirements and muJava. The findings are that mutation tests detected more faults and the other three criteria were very similar. The paper also presents a secondary measure, a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "161\n", "authors": ["53"]}
{"title": "The coupling effect: fact or fiction\n", "abstract": " Fault-based testing strategies test software by focusing on specific, common types of errors. The coupling effect states that test data sets that detect simple types of faults are sensitive enough to detect more complex types of faults. This paper describes empirical investigations into the coupling effect over a specific domain of software faults. All the results from this investigation support the validity of the coupling effect. The major conclusion from this investigation is that by explicitly testing for simple faults, we are also implicitly testing for more complicated faults. This gives us confidence that fault-based testing is an effective means of testing software.", "num_citations": "156\n", "authors": ["53"]}
{"title": "An evaluation of combination strategies for test case selection\n", "abstract": " This paper presents results from a comparative evaluation of five combination strategies. Combination strategies are test case selection methods that combine \u0393\u00c7\u00a3interesting\u0393\u00c7\u00a5 values of the input parameters of a test subject to form test cases. This research comparatively evaluated five combination strategies; the All Combination strategy (AC), the Each Choice strategy (EC), the Base Choice strategy (BC), Orthogonal Arrays (OA) and the algorithm from the Automatic Efficient Test Generator (AETG). AC satisfies n-wise coverage, EC and BC satisfy 1-wise coverage, and OA and AETG satisfy pair-wise coverage. The All Combinations strategy was used as a \u0393\u00c7\u00a3gold standard\u0393\u00c7\u00a5 strategy; it subsumes the others but is usually too expensive for practical use. The others were used in an experiment that used five programs seeded with 128 faults. The combination strategies were evaluated with respect to the number of test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "151\n", "authors": ["53"]}
{"title": "Maintainability of the Linux kernel\n", "abstract": " The authors have examined 365 versions of Linux. For every version, they counted the number of instances of common (global) coupling between each of the 17 kernel modules and all the other modules in that version of Linux. They found that the number of instances of common coupling grows exponentially with the version number. This result is significant at the 99.99% level, and no additional variables are needed to explain this increase. On the other hand, the number of lines of code in each kernel module grows only linearly with the version number. They conclude that, unless Linux is restructured with a bare minimum of common coupling, the dependencies induced by common coupling will, at some future date, make Linux exceedingly hard to maintain without inducing regression faults.", "num_citations": "149\n", "authors": ["53"]}
{"title": "Generating test data from SOFL specifications\n", "abstract": " Software testing can only be formalized and quantified when a solid basis for test generation can be defined. Tests are commonly generated from the source code, control flow graphs, design representations, and specifications/requirements. Formal specifications represent a significant opportunity for testing because they precisely describe what functions the software is supposed to provide in a form that can be easily manipulated. This paper presents a new method for generating tests from formal specifications. This method is comprehensive in specification coverage, applies at several levels of abstraction, and can be highly automated. The paper applies the method to SOFL specifications, describes the technique, and demonstrates the application on a case study. A preliminary evaluation using a code-level coverage criterion (mutation testing), indicates that the method can result in very effective tests.", "num_citations": "145\n", "authors": ["53"]}
{"title": "Coverage criteria for logical expressions\n", "abstract": " A large number of coverage criteria to generate tests from logical expressions have been proposed. Although there have been large variations in the terminology, the articulation of the criteria and the original source of the expressions, many of these criteria are fundamentally the same. The most commonly known and widely used criterion is that of modified condition decision coverage (MCDC), but some articulations of MCDC have had some ambiguities. This has led to confusion on the part of testers, students, and tool developers on how best to implement these test criteria. This paper presents a complete comprehensive set of criteria that incorporate all the existing criteria, and eliminates the ambiguities by introducing precise definitions of the various possibilities.", "num_citations": "136\n", "authors": ["53"]}
{"title": "Bypass testing of web applications\n", "abstract": " Web software applications are increasingly being deployed in sensitive situations. Web applications are used to transmit, accept and store data that is personal, company confidential and sensitive. Input validation testing (IVT) checks user inputs to ensure that they conform to the program's requirements, which is particularly important for software that relies on user inputs, including Web applications. A common technique in Web applications is to perform input validation on the client with scripting languages such as JavaScript. An insidious problem with client-side input validation is that end users can bypass this validation. Bypassing validation can cause failures in the software, and can also break the security on Web applications, leading to unauthorized access to data, system failures, invalid purchases and entry of bogus data. We are developing a strategy called bypass testing to create client-side tests for Web\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "133\n", "authors": ["53"]}
{"title": "Coupling\u0393\u00c7\u00c9based criteria for integration testing\n", "abstract": " Integration testing is an important part of the testing process, but few integration testing techniques have been systematically studied or defined. The goal of this research is to develop practical, effective, formalizable, automatable techniques for testing of connections between components during software integration. This paper presents an integration testing technique that is based on couplings between software components. This technique can be used to support integration testing of software components, and satisfies part of the USA's Federal Aviation Authority's requirements for structural coverage analysis of software. The coupling\u0393\u00c7\u00c9based testing technique is described, and the coverage criteria for three types of couplings are defined. Techniques and algorithms for developing coverage analysers to measure the extent to which a test set satisfies the criteria are presented, and results from a comparative case\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "133\n", "authors": ["53"]}
{"title": "A fault model for subtype inheritance and polymorphism\n", "abstract": " Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about OO software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fault types. This paper presents a model for the appearance and realization of OO faults and defines and discusses specific categories of inheritance and polymorphic faults. The model and categories can be used to support empirical investigations of object-oriented testing techniques, to inspire further research into object-oriented testing and analysis, and to help improve design and development of object-oriented software.", "num_citations": "126\n", "authors": ["53"]}
{"title": "Detecting equivalent mutants and the feasible path problem\n", "abstract": " Mutation testing is a technique for testing software units that has great potential for improving the quality of testing, and thereby increasing our ability to assure the high reliability of critical software. The paper presents a technique that uses mathematical constraints to automatically detect equivalent mutant programs. The paper also describes how the approach is used for the feasible path problem. The paper describes how test criteria are formalized as mathematical constraint systems, how equivalent mutants are represented as infeasible constraints, and how infeasible constraints are detected. A proof of concept implementation has been developed to demonstrate this technique, and experimental results from using this tool are presented. Limitations of the system and the method are described, and proposals for improvements are made.", "num_citations": "125\n", "authors": ["53"]}
{"title": "A practical system for mutation testing: help for the common programmer\n", "abstract": " Mutation testing is a technique for unit testing software that, although powerful, is computationally expensive. Recent engineering advances have given us techniques and algorithms for significantly reducing the cost of mutation testing. These techniques include a new algorithmic execution technique called schema-based mutation, an approximation technique called weak mutation, a reduction technique called selective mutation, and algorithms for automatic test data generation. This paper outlines a design for a system that will approximate mutation, but in a way that will be accessible to everyday programmers. We envisage a system to which a programmer can submit a program unit, and get back a set of input/output pairs that are guaranteed to form an effective test of the unit by being close to mutation adequate.", "num_citations": "124\n", "authors": ["53"]}
{"title": "Mutation operators for Ada\n", "abstract": " Mutation analysis is a method for testing software. It provides a method for assessing the adequacy of test data. This report describes the mutation operators defined for the Ada programming language. The mutation operators are categorized using syntactic criteria, in a form suitable for an implementor of a mutation-based system, or a tester wishing to understand how mutation analysis can be used to test Ada programs.Each mutation operator is carefully defined, and when appropriate, implementation notes and suggestions are provided. We include operators for all syntactic elements of Ada, including exception handling, generics, and tasking. A summary table listing all operators for Ada, and compared with C and Fortran operators is also provided. The design described here is the result of deliberations among the authors in which all aspects of the Ada language and software development in Ada were considered. These operators can also be viewed as the culmination of previous mutation operator definitions for other languages. This report is intended to serve as a manual for the Ada mutation operators.", "num_citations": "121\n", "authors": ["53"]}
{"title": "The class-level mutants of MuJava\n", "abstract": " This paper presents results from empirical studies of object-oriented, class level mutation operators, using the automated analysis and testing tool MuJava. Class mutation operators modify OO programming language features such as inheritance, polymorphism, dynamic binding and encapsulation. This paper presents data from 866 classes in six open-source programs. Several new class-level mutation operators are defined in this paper and an analysis of the number of mutants generated is provided. Techniques for eliminating some equivalent mutants are described and data from an automated tool are provided. One important result is that class-level mutation operators yield far more equivalent mutants than traditional, statement-level, operators. Another is that there are far fewer class-level mutants than statement-level mutants. Together, these data suggest that mutation for inter-class testing can be practically\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "119\n", "authors": ["53"]}
{"title": "Generating test cases for XML-based Web component interactions using mutation analysis\n", "abstract": " Web software systems are built using heterogeneous software components. They interact by passing messages that exchange data and activity state information. Such heterogeneous message transfers can be structured using the eXtensible Markup Language (XML), which allows a flexible common data exchange. Parsers have been developed to check the syntax of component interactions, but there are as yet no techniques for checking the semantic correctness of the interactions. The paper presents a technique for using mutation analysis to test the semantic correctness of XML-based component interactions. The Web software interactions are specified using an Interaction Specification Model (ISM) that consists of document type definitions, messaging specifications, and a set of constraints. Test cases are XML messages that are passed between the Web software components. Classes of interaction-specific\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "117\n", "authors": ["53"]}
{"title": "A mutation carol: Past, present and future\n", "abstract": " ContextThe field of mutation analysis has been growing, both in the number of published papers and the number of active researchers. This special issue provides a sampling of recent advances and ideas. But do all the new researchers know where we started?ObjectiveTo imagine where we are going, we must first know where we are. To understand where we are, we must know where we have been. This paper reviews past mutation analysis research, considers the present, then imagines possible future directions.MethodA retrospective study of past trends lets us the ability to see the current state of mutation research in a clear context, allowing us to imagine and then create future vectors.ResultsThe value of mutation has greatly expanded since the early view of mutation as an expensive way to unit test subroutines. Our understanding of what mutation is and how it can help has become much deeper and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "100\n", "authors": ["53"]}
{"title": "Determining the distribution of maintenance categories: Survey versus measurement\n", "abstract": " In 1978, Lientz, Swanson, and Tompkins published the results of a survey on software maintenance. They found that 17.4% of maintenance effort was categorized as corrective in nature, 18.2% as adaptive, 60.3% as perfective, and 4.1% was categorized as other. We refer to this as the \u0393\u00c7\u00a3LST\u0393\u00c7\u00a5 result. We contrast this survey-based result with our empirical results from the analysis of data for the repeated maintenance of three software products: a commercial real-time product, the Linux kernel, and GCC. For all three products and at both levels of granularity we considered, our observed distributions of maintenance categories were statistically very highly significantly different from LST. In particular, corrective maintenance was always more than twice the LST value. For the summed data, the percentage of corrective maintenance was more than three times the LST value. We suggest various explanations for the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "100\n", "authors": ["53"]}
{"title": "Change impact analysis of object-oriented software\n", "abstract": " As the software industry has matured, we have shifted our resources from being devoted to developing new software systems to making modifications in evolving software systems. A major problem for developers in an evolutionary environment is that seemingly small changes can ripple throughout the system to cause major unintended impacts elsewhere. As such, software developers need mechanisms to understand how a change to a software system will impact the rest of the system. Although the effects of changes in object-oriented software can be restricted, they are also more subtle and more difficult to detect. Maintaining the current object-oriented systems is more of an art, similar to where we were 15 years ago with procedural systems, than an engineering skill. We are beginning to see \u0393\u00c7\u00a3legacy\u0393\u00c7\u00a5 object-oriented systems in industry. A difficult problem is how to maintain these objects in large, complex systems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "99\n", "authors": ["53"]}
{"title": "Empirical evaluation of the statement deletion mutation operator\n", "abstract": " Mutation analysis is widely considered to be an exceptionally effective criterion for designing tests. It is also widely considered to be expensive in terms of the number of test requirements and in the amount of execution needed to create a good test suite. This paper posits that simply deleting statements, implemented with the statement deletion (SDL) mutation operators in Mothra, is enough to get very good tests. A version of the SDL operator for Java was designed and implemented inside the muJava mutation system. The SDL operator was applied to 40 separate Java classes, tests were designed to kill the non-equivalent SDL mutants, and then run against all mutants.", "num_citations": "98\n", "authors": ["53"]}
{"title": "The Mothra tool set (software testing)\n", "abstract": " Mothra is a software test environment that supports mutation-based testing of software systems. Mutation analysis is a powerful software testing technique that evaluates the adequacy of test data based on its ability to differentiate between the program under test and its mutants, where mutants are constructed by inserting single, simple errors into the program under test. This evaluation process also provides guidance in the creation of new test cases to provide more adequate testing. Mothra consists of a collection of individual tools, each of which implements a separate, independent function for the testing system. The initial Mothra tool set, for the most part, duplicates functionality existing in previous mutation analysis systems. Current efforts are concentrated on extending this basic tool set to include capabilities previously unavailable to the software testing community. The authors describe Mothra tool set and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "98\n", "authors": ["53"]}
{"title": "An experimental mutation system for Java\n", "abstract": " Mutation is a powerful but complicated and computationally expensive testing method. Mutation is also a valuable experimental research technique that has been used in many studies. Mutation has been experimentally compared with other test criteria, and also used to support experimental comparisons of other test criteria, by using mutants as a method to create faults. In effect, mutation is often used as a \"gold standard\" for experimental evaluations of test methods. This paper presents a publicly available mutation system for Java that supports both traditional statement-level mutants and newer inter-class mutants. MUJAVA can be freely downloaded and installed with relative ease under both Unix and Windows. MUJAVA is offered as a free service to the community and we hope that it will promote the use of mutation analysis for experimental research in software testing.", "num_citations": "90\n", "authors": ["53"]}
{"title": "Estimation and enhancement of real-time software reliability through mutation analysis\n", "abstract": " A simulation-based method for obtaining numerical estimates of the reliability of N-version, real-time software is proposed. An extended stochastic Petri net is used to represent the synchronization structure of N versions of the software, where dependencies among versions are modeled through correlated sampling of module execution times. The distributions of execution times are derived from automatically generated test cases that are based on mutation testing. Since these test cases are designed to reveal software faults, the associated execution times and reliability estimates are likely to be conservative. Experimental results using speci cations for NASA's planetary lander control software suggest that mutation-based testing could hold greater potential for enhancing reliability than the desirable but perhaps unachievable goal of independence among N versions. Nevertheless, some support for N-version enhancement of high quality, mutation-tested code is also o ered. Experimental results on data diversity, in which retry with a mutation-directed variation in input is attempted after system failure, suggest that mutation analysis could also be valuable in the design of fault-tolerant software systems.", "num_citations": "85\n", "authors": ["53"]}
{"title": "Categorization of common coupling and its application to the maintainability of the Linux kernel\n", "abstract": " Data coupling between modules, especially common coupling, has long been considered a source of concern in software design, but the issue is somewhat more complicated for products that are comprised of kernel modules together with optional nonkernel modules. This paper presents a refined categorization of common coupling based on definitions and uses between kernel and nonkernel modules and applies the categorization to a case study. Common coupling is usually avoided when possible because of the potential for introducing risky dependencies among software modules. The relative risk of these dependencies is strongly related to the specific definition-use relationships. In a previous paper, we presented results from a longitudinal analysis of multiple versions of the open-source operating system Linux. This paper applies the new common coupling categorization to version 2.4.20 of Linux, counting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "83\n", "authors": ["53"]}
{"title": "Mutation operators for testing Android apps\n", "abstract": " Context: Due to the widespread use of Android devices, Android applications (apps) have more releases, purchases, and downloads than apps for any other mobile devices. The sheer volume of code in these apps creates significant concerns about the quality of the software. However, testing Android apps is different from testing traditional Java programs due to the unique program structure and new features of apps. Simple testing coverage criteria such as statement coverage are insufficient to assure high quality of Android apps. While researchers show significant interest in finding better Android testing approaches, there is still a lack of effective and usable techniques to evaluate their proposed test selection strategies, and to ensure a reasonable number of effective tests.Objective: As mutation analysis has been found to be an effective way to design tests in other software domains, we hypothesize that it is also\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["53"]}
{"title": "Criteria for testing polymorphic relationships\n", "abstract": " The emphasis in object oriented programs is on defining abstractions that have both state and behavior. This emphasis causes a shift in focus from software units to the way software components are connected. Thus, we are finding that we need less emphasis on unit testing and more on integration testing. The compositional relationships of inheritance and aggregation, especially when combined with polymorphism, introduce new kinds of integration faults. The paper presents results from an ongoing research project that has the goal of improving the quality of object oriented software. New testing criteria are introduced that take the effects of inheritance and polymorphism into account. These criteria are based on the new analysis technique of quasi-interprocedural data flow analysis. These testing criteria can improve the quality of object oriented software by ensuring that integration tests are high quality.", "num_citations": "82\n", "authors": ["53"]}
{"title": "Subsumption of condition coverage techniques by mutation testing\n", "abstract": " Condition coverage testing is a family of testing techniques that are based on the logical flow of control through a program. The condition coverage techniques include a variety of requirements, including that each statement in the program is executed and that each branch is executed. Mutation testing is a fault-based testing technique that is widely considered to be very powerful, and that imposes requirements on testing that include, and go beyond, many other techniques. In this paper, we consider the six common condition coverage techniques, and formally show that these techniques are subsumed by mutation testing, in the sense that if mutation testing is satisfied, then the condition coverage techniques are also satisfied. The fact that condition coverage techniques are subsumed by mutation has immediate practical significance because the extensive research that has already been done for mutation can be used to support condition coverage techniques, including automated tools for performing mutation testing and generating test cases.", "num_citations": "81\n", "authors": ["53"]}
{"title": "Mutation Testing of Software Using MIMD Computer.\n", "abstract": " Abstract {Mutation testing is a fault-based method for testing software that is computationally expensive. Mothra is an interpreter-based mutation testing system that is centered around an interpreter. This paper presents a parallel implementation of Mothra's interpreter on a MIMD machine. The parallel interpreter, HyperMothra, is implemented on a sixteen processor Intel iPSC/2 hypercube. Our goal was to demonstrate that the expense of software testing schemes such as mutation can be reduced by using parallel processing, and we demonstrate this by measuring the performance gains of the parallel interpreter over the Mothra interpreter. Results are presented using ten test programs, three different static work distribution schemes, and various numbers of processors. On our test programs, we found that our parallel interpreter achieved almost linear speedup over Mothra's sequential interpreter. With larger, faster high-performance computers available, mutation testing can be done at significantly less expense.", "num_citations": "80\n", "authors": ["53"]}
{"title": "Improving logic-based testing\n", "abstract": " Logic-based testers design tests from logical expressions that appear in software artifacts such as source code, design models, and requirements specifications. This paper presents three improvements to logic-based test design. First, in the context of mutation testing, we present fault hierarchies for the six relational operators. Applying the ROR mutation operator causes each relational operator to generate seven mutants per clause. The fault hierarchies show that only three of these seven mutants are needed. Second, we show how to bring the power of the ROR operator to logic-based test criteria such as the widely used Multiple Condition-Decision Coverage (MCDC) test criterion. Third, we present theoretical results supported by empirical data that show that the more recent coverage criterion of minimal-MUMCUT can find significantly more faults than MCDC. The paper has three specific recommendations: (1\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "79\n", "authors": ["53"]}
{"title": "Automatic test data generation\n", "abstract": " Degree: Ph. D.DegreeYear: 1988Institute: Georgia Institute of TechnologyAdviser: RA DeMillo.This dissertation presents a completely automatable test data generation technique, constraint-based testing (CBT), that is based on test case adequacy. CBT has been implemented as part of the Mothra mutation system.", "num_citations": "79\n", "authors": ["53"]}
{"title": "SE 2014: Curriculum guidelines for undergraduate degree programs in software engineering\n", "abstract": " The SE 2004 guidelines were assembled by an ACM/IEEE Computer Society task force led by Rich LeBlanc and Ann Sobel. However, because software engineering knowledge continues to grow and evolve, the societies created a new task force in 2010 (see \u0393\u00c7\u00a3Software Engineering 2014 Curriculum Guidelines Task Forces\u0393\u00c7\u00a5 sidebar), asking members to consider whether revisions were needed and, if so, how extensive they should be. The group duly sought feedback by running events at major conferences; consulting with individuals; and organizing an online survey, which received 477 completed returns from 42 countries. In addition to these efforts, the task force looked at activities related to the Guide to the Software Engineering Body of Knowledge (SWEBOK; www. computer. org/web/swebok), such as the Software En gineering Competency Model (SWECOM; www. computer. org/web/peb/swecom), and related\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "78\n", "authors": ["53"]}
{"title": "Deriving tests from software architectures\n", "abstract": " Software architectures are intended to describe essential high level structural and behavioral characteristics of a system. Architecture Description Languages (ADLs) describe these characteristics in ways that can be analyzed and manipulated algorithmically. This provides a unique opportunity for deriving tests at the system level. The paper defines formal testing criteria based on architecture relations, which are paths that architectural components use to communicate. The criteria have been applied to a specific ADL. Results from a comparative empirical study on industrial software are presented.", "num_citations": "78\n", "authors": ["53"]}
{"title": "Mutant subsumption graphs\n", "abstract": " Mutation testing researchers have long known that many generated mutants are not needed. This paper develops a graph model to describe redundancy among mutations. We define \"true\" subsumption, a relation that practicing test engineers would like to have, but cannot due to issues of computability. We also define dynamic subsumption and static subsumption as approximations of \"true\" subsumption. We explore the properties of the approximate subsumption relations in the context of a small example. We suggest possible uses for subsumption graphs.", "num_citations": "77\n", "authors": ["53"]}
{"title": "Integration testing of object\u0393\u00c7\u00c9oriented components using finite state machines\n", "abstract": " In object\u0393\u00c7\u00c9oriented terms, one of the goals of integration testing is to ensure that messages from objects in one class or component are sent and received in the proper order and have the intended effect on the state of the objects that receive the messages. This research extends an existing single\u0393\u00c7\u00c9class testing technique to integration testing of multiple classes. The single\u0393\u00c7\u00c9class technique models the behaviour of a single class as a finite state machine, transforms the representation into a data flow graph that explicitly identifies the definitions and uses of each state variable of the class, and then applies conventional data flow testing to produce test case specifications that can be used to test the class. This paper extends those ideas to inter\u0393\u00c7\u00c9class testing by developing flow graphs, finding paths between pairs of definitions and uses, detecting some infeasible paths and automatically generating tests for an arbitrary\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "73\n", "authors": ["53"]}
{"title": "Test case generation for mutation-based testing of timeliness\n", "abstract": " Temporal correctness is crucial for real-time systems. Few methods exist to test temporal correctness and most methods used in practice are ad-hoc. A problem with testing real-time applications is the response-time dependency on the execution order of concurrent tasks. Execution order in turn depends on execution environment properties such as scheduling protocols, use of mutual exclusive resources as well as the point in time when stimuli is injected. Model based mutation testing has previously been proposed to determine the execution orders that need to be verified to increase confidence in timeliness. An effective way to automatically generate such test cases for dynamic real-time systems is still needed. This paper presents a method using heuristic-driven simulation to generate test cases.", "num_citations": "73\n", "authors": ["53"]}
{"title": "Analyzing the validity of selective mutation with dominator mutants\n", "abstract": " Various forms of selective mutation testing have long been accepted as valid approximations to full mutation testing. This paper presents counterevidence to traditional selective mutation. The recent development of dominator mutants and minimal mutation analysis lets us analyze selective mutation without the noise introduced by the redundancy inherent in traditional mutation. We then exhaustively evaluate all small sets of mutation operators for the Proteum mutation system and determine dominator mutation scores and required work for each of these sets on an empirical test bed. The results show that all possible selective mutation approaches have poor dominator mutation scores on at least some of these programs. This suggests that to achieve high performance with respect to full mutation analysis, selective approaches will have to become more sophisticated, possibly by choosing mutants based on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["53"]}
{"title": "Open-source change logs\n", "abstract": " A recent editorial in Empirical Software Engineering suggested that open-source software projects offer a great deal of data that can be used for experimentation. These data not only include source code, but also artifacts such as defect reports and update logs. A common type of update log that experimenters may wish to investigate is the ChangeLog, which lists changes and the reasons for which they were made. ChangeLog files are created to support the development of software rather than for the needs of researchers, so questions need to be asked about the limitations of using them to support research. This paper presents evidence that the ChangeLog files provided at three open-source web sites were incomplete. We examined at least three ChangeLog files for each of three different open-source software products, namely, GNUJSP, GCC-g++, and Jikes. We developed a method for counting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["53"]}
{"title": "An integrated automatic test data generation system\n", "abstract": " The Godzilla automatic test data generator is an integrated collection of tools that implements a relatively new test data generation method\u0393\u00c7\u00f6constraint-based testing\u0393\u00c7\u00f6that is based on mutation analysis. Constraint-based testing integrates mutation analysis with several other testing techniques, including statement coverage, branch coverage, domain perturbation, and symbolic evaluation. Because Godzilla uses a rule-based approach to generate test data, it is easily extendible to allow new testing techniques to be integrated into the current system. This article describes the system that has been built to implement constraint-based testing Godzilla\u0393\u00c7\u00d6s design emphasizes orthogonality and modularity, allowing relatively easy extensions. Godzilla\u0393\u00c7\u00d6s internal structure and algorithms are described with emphasis on internal structures of the system and the engineering problems that were solved during the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "72\n", "authors": ["53"]}
{"title": "Applying mutation testing to web applications\n", "abstract": " As our awareness of the complexities inherent in web applications grows, we find an increasing need for more sophisticated ways to test them. Many web application faults are a result of how web software components interact; sometimes client-server and sometimes server-server. This paper presents a novel solution to the problem of integration testing of web applications by using mutation analysis. New mutation operators are defined, a tool (webMuJava) that implements these operators is presented, and results from a case study applying the tool to test a small web application are presented. The results show that mutation analysis can help create tests that are effective at finding web application faults, as well as indicating several directions for improvement.", "num_citations": "69\n", "authors": ["53"]}
{"title": "Description of method-level mutation operators for java\n", "abstract": " This document provides a brief description of method-level mutation operators for Java used by muJava. When designing method-level mutation operators for Java, we followed the selective approach [1]. The selective results found that the traditional operatorso of modifying operands and statements give little effectiveness to mutation testing. Therefore, we only consider mutation operators that modify expression by replacing, deleting, and inserting primitive operators. muJava provides six kinds of primitive operators;(1) arithmetic operator,(2) relational operator,(3) conditional operator,(4) shift operator,(5) logical operator, and (6) assignment. For some of them, muJava provides short-cut operators. This section presents designs of mutation operators for those six kinds of primitive operators. We try to design mutation operators that replace, insert, and delete the primitive operators. We defined total 12 method-level\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "69\n", "authors": ["53"]}
{"title": "How strong is weak mutation?\n", "abstract": " Mutation testing is a fault-based technique for unit level testing of software. Weak mutation was proposed as a way to reduce the expense of mutation testing. Unfortunately, weak mutation is also expected to provide a weaker test of the software than mutation testing. This paper presents results from an implementation of weak mutation, where we compared the effectiveness versus the efficiency of weak mutation. Additionally, we examined several options in an attempt to find the most appropriate way to implement weak mutation. Our results indicate that weak mutation can be applied in a manner that is almost as effective as mutation testing, and with significant computational savings.", "num_citations": "68\n", "authors": ["53"]}
{"title": "Input parameter modeling for combination strategies\n", "abstract": " Combination strategies are test methods that generate test cases based on input parameter models. This paper suggests a structured modeling method used to translate requirements expressed in a general format into an input parameter model suitable for combination strategies. This paper also describes results from two initial experiments exploring the efficiency and effectiveness of the modeling method. These results indicate that the resulting models may contain enough information to detect the vast majority of faults in the system under test. Further, results indicate that the modeling method is simple enough to use in practical testing.", "num_citations": "66\n", "authors": ["53"]}
{"title": "Maintaining Evolving Component-Based Software with UML.\n", "abstract": " Component-based software engineering is increasingly being adopted for software development. This approach relies on using reusable components as the building blocks for constructing software. On the one hand, this helps improve software quality and productivity; on the other hand, it necessitates frequent maintenance activities. The cost of maintenance for conventional software can account for as much as two-thirds of the total cost, and it is likely to be more for component-based software.This paper presents a UML-based technique that attempts to help resolve difficulties introduced by the implementation transparent characteristics of componentbased software systems. This technique can also be useful for other maintenance activities. For corrective maintenance activities, the technique starts with UML diagrams that represent changes to a component, and uses them to support regression testing. To accommodate this approach for perfective maintenance activities, more challenges are encountered. We provide a UML-based framework to evaluate the similarities of the old and new components, and corresponding retesting strategies are provided.", "num_citations": "64\n", "authors": ["53"]}
{"title": "Evaluation of three specification-based testing criteria\n", "abstract": " This paper compares three specification-based testing criteria using Mathur and Wong's PROBSUBSUMES measure. The three criteria are specification-mutation coverage, full predicate coverage, and transition-pair coverage. A novel aspect of the work is that each criterion is encoded in a model checker, and the model checker is used first to generate test sets for each criterion and then to evaluate test sets against alternate criteria. Significantly, the use of the model checker for generation of test sets eliminates human bias from this phase of the experiment. The strengths and weaknesses of the criteria are discussed.", "num_citations": "62\n", "authors": ["53"]}
{"title": "An approach to fault modeling and fault seeding using the program dependence graph\n", "abstract": " We present a fault-classification scheme and a fault-seeding method that are based on the manifestation of faults in the program dependence graph (PDG). We enhance the domain/computation fault classification scheme developed by Howden to further characterize faults as structural and statement-level depending on the differences between the PDG for the original program and the PDG for the faulty program. We perform transformations on the PDG to produce the different types of faults described in our PDG-based fault-classification scheme. To demonstrate the usefulness of our technique, we implemented a fault seeder to embed faults in C programs. Our fault seeder makes controlled fault transformations to the PDG for a C program, and generates C code from the transformed PDG. The current version of the fault seeder creates multiple fault-seeded versions of the original program, each with one known fault\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "58\n", "authors": ["53"]}
{"title": "Towards mutation analysis of android apps\n", "abstract": " Android applications (apps) have the highest number of releases, purchases, and downloads among mobile apps. However, quality is a known problem, and hence there is significant research interest in better methods for testing Android apps. We identify three reasons to extend mutation testing to Android apps. First, current testing approaches for Android apps use simple coverage criteria such as statement coverage; extending mutation coverage to Android apps promises more sophisticated testing. Second, testing researchers inventing other test methods for Android apps need to evaluate the quality of their test selection strategies, which mutation excels at. Finally, some approaches to test generation for Android apps, specifically combinatorial testing approaches, generate very large numbers of tests. This is particularly problematic because running Android tests is slow. For these reasons, this paper proposes\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "57\n", "authors": ["53"]}
{"title": "Testing object-oriented software using the category-partition method\n", "abstract": " When migrating from conventional to object-oriented programming, developers face di cult decisions in modifying their development process to best use the new technology. In particular, ensuring that the software is highly reliable in this new environment poses different challenges and developers need to understand effective ways to test the software. Much previous work in testing OO software has focused on developing new techniques and procedures. We ask whether existing techniques can work, and present empirical data that show that the existing technique of category-partition testing can effectively find faults in object-oriented software, and new techniques may not be needed. For this study, we identified types of faults that are common to C++ software and inserted faults of these types into two C++ programs. Test cases generated using the category-partition method were used to test the programs. A fault was considered detected if it caused the program to terminate abnormally or if the output was different from the output of the original program. The results show that the combination of the category-partition method and", "num_citations": "57\n", "authors": ["53"]}
{"title": "Designing deletion mutation operators\n", "abstract": " As a test criterion, mutation analysis is known for yielding very effective tests. It is also known for creating many test requirements, each of which is represented by a \"mutant\" that must be \"killed.\" In recent years, researchers have found that these test requirements have a lot of duplication, in that many test requirements yield the same tests. Put another way, hundreds of mutants can usually be killed by only a few dozen tests. If we could reduce this duplication without reducing mutation's effectiveness, mutation testing could become more cost-effective. One avenue of this research has been to use only one type of mutant, the statement deletion mutation operator. Researchers have found that statement deletion mutation has relatively few mutants, but yields tests that are almost as effective as using all mutants, with the significant benefit that fewer equivalent mutants are generated. This paper extends this idea by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "55\n", "authors": ["53"]}
{"title": "Static analysis of mutant subsumption\n", "abstract": " Mutation analysis generates a large set of variants, or mutants, and then demands a test set that distinguishes each variant from the original artifact. It has long been apparent that many mutants contribute little, if anything, to the subsequent test set. Researchers have developed various approaches to separate valuable mutants from redundant mutants. The notion of subsumption underlies several such approaches. Informally, one mutant subsumes another if tests that kill the first also kill the second. Computing subsumption relations is, not surprisingly, undecidable. Recent work formalized the notion of a mutant subsumption graph (MSG) and showed that root nodes in the MSG precisely identify mutants that are not redundant. To address the decidability issue, we first defined the dynamic subsumption graph as an approximation to the MSG. This paper continues by showing how symbolic execution can be used to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["53"]}
{"title": "Test oracle strategies for model-based testing\n", "abstract": " Testers use model-based testing to design abstract tests from models of the system's behavior. Testers instantiate the abstract tests into concrete tests with test input values and test oracles that check the results. Given the same test inputs, more elaborate test oracles have the potential to reveal more failures, but may also be more costly. This research investigates the ability for test oracles to reveal failures. We define ten new test oracle strategies that vary in amount and frequency of program state checked. We empirically compared them with two baseline test oracle strategies. The paper presents several main findings. (1) Test oracles must check more than runtime exceptions because checking exceptions alone is not effective at revealing failures. (2) Test oracles do not need to check the entire output state because checking partial states reveals nearly as many failures as checking entire states. (3) Test oracles do\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["53"]}
{"title": "On the testing maturity of software producing organizations\n", "abstract": " This paper presents data from a study of the current state of practice of software testing. Test managers from twelve different software organizations were interviewed. The interviews focused on the amount of resources spent on testing, how the testing is conducted, and the knowledge of the personnel in the test organizations. The data indicate that the overall test maturity is low. Test managers are aware of this but have trouble improving. One problem is that the organizations are commercially successful, suggesting that products must already be \"good enough\". Also, the current lack of structured testing in practice makes it difficult to quantify the current level of maturity and thereby articulate the potential gain from increasing testing maturity to upper management and developers", "num_citations": "53\n", "authors": ["53"]}
{"title": "Experimental evaluation of SDL and one-op mutation for C\n", "abstract": " Mutation analysis modifies a program by applying syntactic rules, called mutation operators, systematically to create many versions of the program (mutants) that differ in small ways. Testers then design tests to cause the mutants to behave differently from the original program. Mutation testing is widely considered to result in very effective tests, however, it is also quite costly. Cost comes from the many mutants that are created, the number of tests that are needed to kill the mutants, and the difficulty of deciding whether mutants behave equivalently to the original program. One-op mutation theorizes that cost can be reduced by using a single, very powerful, mutation operator that leads to tests that are almost as effective as if all operators are used. Previous research proposed the statement deletion operator (SDL) and found promising results. This paper investigates the use of SDL-mutation in a new context, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["53"]}
{"title": "Mutation testing implements grammar-based testing\n", "abstract": " This paper presents an abstract view of mutation analysis. Mutation was originally thought of as making changes to program source, but similar kinds of changes have been applied to other artifacts, including program specifications, XML, and input languages. This paper argues that mutation analysis is actually a way to modify any software artifact based on its syntactic description, and is in the same family of test generation methods that create inputs from syntactic descriptions. The essential characteristic of mutation is that a syntactic description such as a grammar is used to create tests. We call this abstract view grammar-based testing, and view it as an interface, which mutation analysis implements. This shift in view allows mutation to be defined in a general way, yielding three benefits. First, it provides a simpler way to understand mutation. Second, it makes it easier to develop future applications of mutation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["53"]}
{"title": "A Fortran 77 interpreter for mutation analysis\n", "abstract": " Mutation analysis is a powerful technique for testing software systems. In the Mothra project, conducted at Georgia Tech's Software Engineering Research Center, mutation analysis is used as a basis for building an integrated software testing environment. Mutation analysis requires the execution of many slightly differing versions of the same program to evaluate the quality of the data used to test the program. In the current version of the Mothra system, a program to be tested is translated to intermediate code, where it and its mutated versions are executed by an interpreter. In this paper, we discuss some of the unique requirements of an interpreter used in a mutation-based testing environment. We then describe how these requirements affected the design and implementation of the Fortran 77 version of the Mothra interpreter. Other topics covered include the architecture of the interpreter and many of the design\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["53"]}
{"title": "Mutation-based software testing using program schemata\n", "abstract": " Mutation analysis is a powerful technique for assessing the quality of test data used in unit testing software. Unfortunately, current automated mutation analysis systems suffer from severe performance problems. In this paper the principles of mutation analysis are reviewed, current automation approaches are described, and a new method of performing mutation analysis is outlined. Performance improvements of over 300% are reported and other advantages of this new method are highlighted.", "num_citations": "50\n", "authors": ["53"]}
{"title": "A comparative evaluation of tests generated from different UML diagrams\n", "abstract": " This paper presents a single project experiment on the fault revealing capabilities of model-based test sets. The tests are generated from UML statecharts and UML sequence diagrams. This experiment found that the statechart test sets did better at revealing unit level faults than the sequence diagram test sets, and the sequence diagram test sets did better at revealing integration level faults than the statechart test sets. The statecharts also resulted in more test cases than the sequence diagrams. The results show that model-based testing can be used to systematically generate test data and indicates that different UML models can play different roles in testing.", "num_citations": "49\n", "authors": ["53"]}
{"title": "Using coupling-based weights for the class integration and test order problem\n", "abstract": " During component-based and object-oriented software development, software classes exhibit relationships that complicate integration, including method calls, inheritance and aggregation. Classes are integrated and tested in specific orders, where each class is added and tested one by one to see if it integrates successfully. A difficulty arises when cyclic dependencies exist\u0393\u00c7\u00f6the functionality that is used by the first class to be tested must be mimicked by creating \u0393\u00c7\u00ffstubs\u0393\u00c7\u00d6 (sometimes called \u0393\u00c7\u00ffmock objects\u0393\u00c7\u00d6), an expensive and error-prone operation. This problem is generally called the class integration and test order (CITO) problem, and solutions must fully be automated for integration and testing to proceed smoothly and efficiently. This paper describes new techniques and algorithms to solve the CITO problem. New results include improved edge weights to more precisely model the cost of stubbing, and the use of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["53"]}
{"title": "Barriers to usable security? Three organizational case studies\n", "abstract": " Usable security assumes that when security functions are more usable, people are more likely to use them, leading to an improvement in overall security. Existing software design and engineering processes provide little guidance for leveraging this in the development of applications. Three case studies explore organizational attempts to provide usable security products.", "num_citations": "45\n", "authors": ["53"]}
{"title": "Quantitatively measuring object-oriented couplings\n", "abstract": " One key to several quality factors of software is the way components are connected. Software coupling can be used to estimate a number of quality factors, including maintainability, complexity, and reliability. Object-oriented languages are designed to reduce the number of dependencies among classes, which encourages separation of concerns and should reduce the amount of coupling. At the same time, the object-oriented language features change the way the connections are made, how they must be analyzed, and how they are measured. This paper discusses software couplings based on object-oriented relationships between classes, specifically focusing on types of couplings that are not available until after the implementation is completed, and presents a static analysis tool that measures couplings among classes in Java packages. Data from evaluating the tool on several open-source projects are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "43\n", "authors": ["53"]}
{"title": "Recognizing authors: an examination of the consistent programmer hypothesis\n", "abstract": " Software developers have individual styles of programming. This paper empirically examines the validity of the  consistent programmer hypothesis: that a facet or set of facets exist that can be used to recognize the author of a given program based on programming style. The paper further postulates that the programming style means that different test strategies work better for some programmers (or programming styles) than for others. For example, all\u0393\u00c7\u00c9edges adequate tests may detect faults for programs written by Programmer A better than for those written by Programmer B. This has several useful applications: to help detect plagiarism/copyright violation of source code, to help improve the practical application of software testing, and to help pursue specific rogue programmers of malicious code and source code viruses. This paper investigates this concept by experimentally examining whether particular facets of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["53"]}
{"title": "Generating test cases from UML activity diagrams using the Condition-Classification Tree Method\n", "abstract": " A key technical challenge in software testing is the design of useful test cases. Test design can be based on a variety of software artifacts, including requirements, designs, or even the implementation. The Unified Modeling Language (UML) is now widely used to describe object-oriented designs. This paper focuses on one UML diagram, the activity diagram, which is used to model software behavior. This paper proposes the Condition-Classification Tree Method for generating test cases from activity diagrams. Activity diagrams are used to generate condition-classification trees, which are then used to create test case tables and test cases. The paper presents experimental data that show the proposed method can help generate a relatively small number of test cases at reasonable cost, early in development.", "num_citations": "42\n", "authors": ["53"]}
{"title": "Scalability issues with using FSMWeb to test web applications\n", "abstract": " Web applications are fast becoming more widespread, larger, more interactive, and more essential to the international use of computers. It is well understood that web applications must be highly dependable, and as a field we are just now beginning to understand how to model and test Web applications. One straightforward technique is to model Web applications as finite state machines. However, large numbers of input fields, input choices and the ability to enter values in any order combine to create a state space explosion problem. This paper evaluates a solution that uses constraints on the inputs to reduce the number of transitions, thus compressing the FSM. The paper presents an analysis of the potential savings of the compression technique and reports actual savings from two case studies.", "num_citations": "39\n", "authors": ["53"]}
{"title": "Coupling-based class integration and test order\n", "abstract": " During component-based and object-oriented software development, software classes exhibit relationships that complicate integration, including method calls, inheritance, and aggregation. When classes are integrated and tested, an order of integration must be established. The difficulty arises when cyclic dependencies exist-the functionality that is used by the first class to be tested must be mimicked by creating\" stubs\"(sometimes called\" mocks\"), an expensive and error-prone operation. This problem is generally called the class integration and test order (CITO) problem, and solutions must be fully automated for integration and testing to proceed smoothly and efficiently. This paper describes new techniques and algorithms to solve the CITO problem. New results include improved edge weights that are derived from quantitative coupling measures to more precisely model the cost of stubbing, and the use of weights\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["53"]}
{"title": "Handling constraints in the input space when using combination strategies for software testing\n", "abstract": " This study compares seven different methods for handling constraints in input parameter models when using combination strategies to select test cases. Combination strategies are used to select test cases based on input parameter models. An input parameter model is a representation of the input space of the system under test via a set of parameters and values for these parameters. A test case is one specific combination of values for all the parameters. Sometimes the input parameter model may contain parameters that are not independent. Some sub-combinations of values of the dependent parameters may not be valid, ie, these subcombinations do not make sense. Combination strategies, in their basic forms, do not take into account any semantic information. Thus, invalid sub-combinations may be included in test cases in the test suite.This paper proposes four new constraint handling methods and compares these with three existing methods in an experiment in which the seven constraint handling methods are used to handle a number of different constraints in different sized input parameter models under three different coverage criteria. All in all, 2568 test suites with a total of 634263 test cases have been generated within the scope of this experiment.", "num_citations": "39\n", "authors": ["53"]}
{"title": "A systematic literature review of techniques and metrics to reduce the cost of mutation testing\n", "abstract": " Historically, researchers have proposed and applied many techniques to reduce the cost of mutation testing. It has become difficult to find all techniques and to understand the cost-benefit tradeoffs among them, which is critical to transitioning this technology to practice. This paper extends a prior workshop paper to summarize and analyze the current knowledge about reducing the cost of mutation testing through a systematic literature review. We selected 175 peer-reviewed studies, from which 153 present either original or updated contributions. Our analysis resulted in six main goals for cost reduction and 21 techniques. In the last decade, a growing number of studies explored techniques such as selective mutation, evolutionary algorithms, control-flow analysis, and higher-order mutation. Furthermore, we characterized 18 metrics, with particular interest in the number of mutants to be executed, test cases required\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["53"]}
{"title": "Better predicate testing\n", "abstract": " Mutation testing is widely recognized as being extremely powerful, but is considered difficult to automate enough for practical use. This paper theoretically addresses two possible reasons for this: the generation of redundant mutants and the lack of integration of mutation analysis with other test criteria. By addressing these two issues, this paper brings an important mutation operator, relational-operator-replacement (ROR), closer to practical use. First, we develop fault hierarchies for the six relational operators, each of which generates seven mutants per clause. These hierarchies show that, for any given clause, only three mutants are necessary. This theoretical result can be integrated easily into mutation analysis tools, thereby eliminating generation of 57% of the ROR mutants. Second, we show how to bring the power of the ROR operator to the widely used Multiple Condition-Decision Coverage (MCDC) test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["53"]}
{"title": "Comparison of unit-level automated test generation tools\n", "abstract": " Data from projects worldwide show that many software projects fail and most are completed late or over budget. Unit testing is a simple but effective technique to improve software in terms of quality, flexibility, and time-to-market. A key idea of unit testing is that each piece of code needs its own tests and the best person to design those tests is the developer who wrote the software. However, generating tests for each unit by hand is very expensive, possibly prohibitively so. Automatic test data generation is essential to support unit testing and as unit testing is achieving more attention, developers have a greater need for automated unit test data generation tools. However, developers have very little information about which tools are effective. This experiment compared three well-known public-accessible unit test data generation tools, JCrasher, TestGen4j, and JUB. We applied them to Java classes and evaluated them\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["53"]}
{"title": "Maintainability of the kernels of open-source operating systems: A comparison of Linux with FreeBSD, NetBSD, and OpenBSD\n", "abstract": " We compared and contrasted the maintainability of four open-source operating systems: Linux, FreeBSD, NetBSD, and OpenBSD. We used our categorization of common coupling in kernel-based software to highlight future maintenance problems. An unsafe definition is a definition of a global variable that can affect a kernel module if that definition is changed. For each operating system we determined a number of measures, including the number of global variables, the number of instances of global variables in the kernel and overall, as well as the number of unsafe definitions in the kernel and overall. We also computed the value of each our measures per kernel KLOC and per KLOC overall. For every measure and every ratio, Linux compared unfavorably with FreeBSD, NetBSD, and OpenBSD. Accordingly, we are concerned about the future maintainability of Linux.", "num_citations": "38\n", "authors": ["53"]}
{"title": "Analysis techniques for testing polymorphic relationships\n", "abstract": " As we move from developing procedure oriented to object oriented programs, the complexity traditionally found in functions and procedures is moving to the connections among components. More faults occur as components are integrated to form higher level aggregates of behavior and state. Consequently, we need to place more effort on testing the connections among components. Although object oriented technology provides abstraction mechanisms to build components to integrate, it also adds new compositional relations that can contain faults, which must be found during integration testing. The paper describes new techniques for analyzing and testing the polymorphic relationships that occur in object oriented software. The application of these techniques can result in an increased ability to find faults and overall higher quality software.", "num_citations": "38\n", "authors": ["53"]}
{"title": "Integration testing based on software couplings\n", "abstract": " Integration testing is an important part of the testing process, but few integration testing techniques have been systematically studied or defined. This paper presents an integration testing technique based on couplings between software components. The coupling-based testing technique is described, and 12 coverage criteria are defined. The coupling-based technique is also compared with the category-partition method on a case study, which found that the coupling-based technique detected more faults with fewer test cases than category-partition. This modest result indicates that the coupling-based testing approach can benefit practitioners who are performing integration testing on software. While it is our intention to develop algorithms to fully automate this technique, it is relatively easy to apply by hand.", "num_citations": "38\n", "authors": ["53"]}
{"title": "Quality impacts of clandestine common coupling\n", "abstract": " The increase in maintenance of software and the increased amounts of reuse are having major positive impacts on the quality of software, but are also introducing some rather subtle negative impacts on the quality. Instead of talking about existing problems (faults), developers now discuss \u0393\u00c7\u00a3potential problems,\u0393\u00c7\u00a5 that is, aspects of the program that do not affect the quality initially, but could have deleterious consequences when the software goes through some maintenance or reuse. One type of potential problem is that of common coupling, which unlike other types of coupling can be clandestine. That is, the number of instances of common coupling between a module M and the other modules can be changed without any explicit change to M. This paper presents results from a study of clandestine common coupling in 391 versions of Linux. Specifically, the common coupling between each of 5332 kernel\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["53"]}
{"title": "Better algorithms to minimize the cost of test paths\n", "abstract": " Model-based testing creates tests from abstract models of the software. These models are often described as graphs, and test requirements are defined as sub paths in the graphs. As a step toward creating concrete tests, complete (test) paths that include the sub paths through the graph are generated. Each test path is then transformed into a test. If we can generate fewer and shorter test paths, the cost of testing can be reduced. The minimum cost test paths problem is finding the test paths that satisfy all test requirements with the minimum cost. This paper presents new algorithms to solve the problem, and then presents data from an empirical comparison. The algorithms adapt approximation algorithms for the shortest super string problem. The comparison is with an existing tool that uses a brute force approach to extend each sub path to a complete path. One new algorithm is based on the greedy set-covering\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["53"]}
{"title": "Mutation at system and functional levels\n", "abstract": " Mutation analysis has been applied to many testing problems, including functional programs in numerous languages, specifications, network protocols, web services, and security policies. Program mutation, where mutation analysis is applied to programs, has been applied to the unit level (functions and methods), integration of pairs of functions, and individual classes. However, program mutation has not been applied to the problem of testing multiple classes or entire software programs, that is, there is no system level mutation. This paper introduces a project on the problem of multi-class and system level mutation testing. The technical differences between using mutation to test single classes and multiple classes are explored, and new system level mutation operators are defined. A new execution style for detecting killed mutants, Flexible Weak Mutation, is introduced. A support tool, Bacterio, is currently under\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["53"]}
{"title": "Input validation analysis and testing\n", "abstract": " This research addresses the problem of statically analyzing input command syntax as defined in interface and requirements specifications and then generating test cases for dynamic input validation testing. The IVAT (Input Validation Analysis and Testing) technique has been developed, a proof-of-concept tool (MICASA) has been implemented, and a case study validation has been performed. Empirical validation on large-scale industrial software (from the Tomahawk Cruise Missile) shows that as compared with senior, experienced analysts and testers, MICASA found more syntactic requirement specification defects, generated test cases with higher syntactic coverage, and found additional defects. The experienced analysts found more semantic defects than MICASA, and the experienced testers\u0393\u00c7\u00d6 cases found 7.4 defects per test case as opposed to an average of 4.6 defects found by MICASA test cases\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["53"]}
{"title": "Generating test cases from UML specifications\n", "abstract": " 6 CONCLUSIONS AND RECOMMENDATIONS 89 6.1 Conclusions.................................... 89 6.2 Recommendations................................ 90 6.3 Future Work................................... 91", "num_citations": "36\n", "authors": ["53"]}
{"title": "Mutation-based testing criteria for timeliness\n", "abstract": " Temporal correctness is crucial to the dependability of real-time systems. Few methods exist to test for temporal correctness and most existing methods are ad-hoc. A problem with testing real-time applications is the dependency on the execution time and execution order of individual tasks. Thus, the response times for the tasks may be non-deterministic with respect to inputs. Conventional test coverage criteria ignore task interleaving and tinting and, thus do not help determine which execution orders need to be exercised to test for temporal correctness. This paper presents test criteria based on mutation to test timeliness. We also show how previously proposed methods in specification based testing, can be applied to testing real-time systems", "num_citations": "33\n", "authors": ["53"]}
{"title": "Increased software reliability through input validation analysis and testing\n", "abstract": " The input validation testing (IVT) technique has been developed to address the problem of statically analyzing input command syntax as defined in an English textual interface and requirements specifications and then generating test cases for input validation testing. The technique does not require design or code, so it can be applied early in the life cycle. A proof-of-concept tool has been implemented and validation has been performed. Empirical validation on industrial software shows that the IVT method found more requirements specification defects than senior testers, generated test cases with higher syntactic coverage than senior testers, and found defects that were not found by the test cases of senior testers. Additionally, the tool performed at a much-reduced cost.", "num_citations": "33\n", "authors": ["53"]}
{"title": "An analysis of OO mutation operators\n", "abstract": " This paper presents results from empirical studies using object-oriented, class-level mutation operators. Class mutation operators modify OO programming language features such as inheritance, polymorphism, dynamic binding and encapsulation. Most previous empirical studies of mutation operators used statement-level operators, this study asked questions about the static and dynamic nature of class-level mutation operators. Results include statistics on the various types of mutants, how many are equivalent, new rules for avoiding creation of equivalent mutants, the difficulty of killing individual mutants, and the difficulty of killing mutants from the various operators. The paper draws conclusions about which mutation operators are more or less useful, leading to recommendations about how future OO mutation systems should be built.", "num_citations": "32\n", "authors": ["53"]}
{"title": "Managing conflicts when using combination strategies to test software\n", "abstract": " Testers often represent systems under test in input parameter models. These contain parameters with associated values. Combinations of parameter values, with one value for each parameter, are potential test cases. In most models, some values of two or more parameters cannot be combined. Testers must then detect and avoid or remove these conflicts. This paper proposes two new methods for automatically handling such conflicts and compares these with two existing methods, based on the sizes of the final conflict-free test suites. A test suite reduction method, usable with three of the four investigated methods is also included in the study, resulting in seven studied conflict handling methods. In the experiment, the number and types of conflicts, as well as the size of the input parameter model and the coverage criterion used, are varied. All in all, 3854 test suites with a total of 929,158 test cases were generated\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["53"]}
{"title": "Are we there yet? How redundant and equivalent mutants affect determination of test completeness\n", "abstract": " Mutation score has long been used in research as a metric to measure the effectiveness of testing strategies. This paper presents evidence that mutation score lacks the desired accuracy to determine the completeness of a test suite due to noise introduced by the redundancy inherent in traditional mutation, and that dominator mutation score is a superior metric for this purpose. We evaluate the impact of different levels of redundant and equivalent mutants on mutation score and the ability to determine completeness in developing a mutation-adequate test suite. We conclude that, in the context of our model, redundant mutants make it very difficult to accurately assess test completeness. Equivalent mutants, on the other hand, have little effect on determining completeness. Based on this information, we suggest limits to redundancy and equivalency that mutation tools must achieve to be practical for general use in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["53"]}
{"title": "A logic mutation approach to selective mutation for programs and queries\n", "abstract": " ContextProgram mutation testing is a technique for measuring and generating high quality test data. However, traditional mutation operators are not necessarily efficient or effective. We address three specific issues. One, test data that kills all mutants generated by current mutation tools can still miss detection of some common logic faults because such tools lack appropriate logic mutation operators. Two, the number of mutants generated is often unnecessarily large. Three, many equivalent mutants can be generated and these can be difficult to eliminate.ObjectiveThis paper explores the idea of addressing these issues by selectively generating only specially engineered subsuming higher order logic mutants. However, such an approach is only useful if a test set that kills all such mutants also kills a high percentage of general mutants.MethodAn empirical study was conducted using a tool that generates only\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["53"]}
{"title": "Is bytecode instrumentation as good as source code instrumentation: An empirical study with industrial tools (experience report)\n", "abstract": " Branch coverage (BC) is a widely used test criterion that is supported by many tools. Although textbooks and the research literature agree on a standard definition for BC tools measure BC in different ways. The general strategy is to \u0393\u00c7\u00a3instrument\u0393\u00c7\u00a5 the program by adding statements that count how many times each branch is taken. But the details for how this is done can influence the measurement for whether a set of tests have satisfied BC. For example, the standard definition is based on program source, yet some tools instrument the bytecode to reduce computation cost. A crucial question for the validity of these tools is whether bytecode instrumentation gives results that are the same as, or at least comparable to, source code instrumentation. An answer to this question will help testers decide which tool to use. This research looked at 31 code coverage tools, finding four that support branch coverage. We chose one\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["53"]}
{"title": "An analysis tool for coupling-based integration testing\n", "abstract": " This research is part of a project to develop practical, effective, formalizable, automatable techniques for integration testing. Integration testing is an important part of the testing process, but few integration testing techniques have been systematically studied or defined. This paper discusses the design and implementation of an analysis tool for measuring the amount of coverage achieved by a set of test data according to a set of previously defined coupling criteria. This tool can be used to support integration testing of software components. The coupling-based testing technique, which has been described elsewhere, is summarized, and coverage algorithms are discussed. The focus of this paper is on the instrumentation techniques and an analysis tool built for Java programs. It was built in Java using the general Java parser JavaCC and the Java Tree Builder (JTB). We are currently using this tool to gather\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["53"]}
{"title": "An empirical analysis of test oracle strategies for model-based testing\n", "abstract": " Model-based testing is a technique to design abstract tests from models that partially describe the system's behaviour. Abstract tests are transformed into concrete tests, which include test input values, expected outputs, and test oracles. Although test oracles require significant investment and are crucial to the success of the testing, we have few empirical results about how to write them. With the same test inputs, test oracles that check more of the program state have the potential to reveal more failures, but may also cost more to design and create. This research defines six new test oracle strategies that check different parts of the program state different numbers of times. The experiment compared the six test oracle strategies with two baseline test oracle strategies. The null test oracle strategy just checks whether the program crashes and the state invariant test oracle strategy checks the state invariants in the model\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["53"]}
{"title": "Coupling-based Testing of OO Programs.\n", "abstract": " As we move from developing procedure-oriented to OO programs, the complexity traditionally found in functions and procedures is moving to the connections among components. More faults occur as components are integrated to form higher level aggregates. Consequently, we need to place more effort on testing the connections among components. Although OO technology provides abstraction mechanisms to build components to integrate, it also adds new compositional relations that can contain faults, which must be found during integration testing. This paper describes new techniques for analyzing and testing the polymorphic relationships that occur in OO software. The application of these techniques can result in an increased ability to find faults and overall higher quality software.", "num_citations": "27\n", "authors": ["53"]}
{"title": "An industrial case study of structural testing applied to safety-critical embedded software\n", "abstract": " Effective testing of safety-critical real-time embedded software is difficult and expensive. Many companies are hesitant about the cost of formalized criteria-based testing and are not convinced of the benefits. This paper presents the results of an industrial case study that compared the normal testing at a company (manual functional testing) with testing based on the logic-based criterion of correlated active clause coverage (CACC). The evaluation was performed during the testing of embedded, real-time control software that has been deployed in a safety-critical application in the transportation industry. We found in our study that the test cases generated to satisfy the CACC criterion detected major safety-critical faults that were not detected by functional testing. We also found that the cost required for CACC testing was not necessarily higher than the cost of functional testing. There were also several faults that were\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "26\n", "authors": ["53"]}
{"title": "Web software applications quality attributes\n", "abstract": " In only four or five years, the World Wide Web has changed from a static collection of HTML web pages to a dynamic engine that powers e-commerce, collaborative work, and distribution of information and entertainment. These exciting changes have been fueled by changes in software technology, the software development process, and how software is deployed. Although the word \u0393\u00c7\u00a3heterogeneous\u0393\u00c7\u00a5 is commonly used to describe web software, we might easily forget to notice in how many ways it can be applied. In fact, the synonymous term \u0393\u00c7\u00a3diverse\u0393\u00c7\u00a5 is more general and familiar, and may be more appropriate. Web software applications use diverse types of hardware, they include a diverse collection of types of implementation languages (including traditional programs, HTML, interpreted scripts, and databases), they are composed of software written in diverse languages, and they are built by collections of people with very diverse sets of skills.Although these changes in how web applications are built are interesting and fascinating, one of the most unique aspects of web software applications is in terms of the needs they must satisfy. Web applications have very high requirements for a number of quality attributes. Some of these quality attributes have been important in other (mostly relatively small) segments of the industry, but some of them are relatively new. This paper discusses some of the unique technological aspects of building web software applications, the unique requirements of quality attributes, and how they can be achieved.", "num_citations": "26\n", "authors": ["53"]}
{"title": "Mutation at the multi-class and system levels\n", "abstract": " Mutation analysis has been applied to many testing problems, including numerous programming languages, specifications, network protocols, web services, and security policies. Program mutation, where mutation analysis is applied to programs, has been applied to the unit level (functions and methods), integration of pairs of functions, and individual classes. However, program mutation has not been applied to the problem of integration testing of multiple classes or entire software programs; thus, there is no system level mutation. This paper introduces a project on the problem of integration testing of multiple classes (multi-class) and system level mutation testing. The technical differences between using mutation to test single classes and multiple classes are explored, and new system level mutation operators are defined. A new execution style for detecting killed mutants, flexible weak mutation, is introduced. A\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["53"]}
{"title": "Putting the engineering into software engineering education\n", "abstract": " Based on over 20 years of teaching and research experience, the author provides his assessment of software engineering education. He then builds on the analysis to provide recommendations on how we need to diverge from computer science to increase our impact, gain credibility, and ultimately ensure the success and recognition of our young discipline. A key behind the author's message is that we need to become a true engineering discipline.", "num_citations": "24\n", "authors": ["53"]}
{"title": "Description of class mutation mutation operators for java\n", "abstract": " This document provides a brief description of the muJava class mutation operators, which were updated currently for version II of the tool. The class mutation operators are classified into four groups, based on the language features that are affected. The first three groups are based on language features that are common to all OO languages. The last group includes OO features that are Java-specific.", "num_citations": "24\n", "authors": ["53"]}
{"title": "A controlled experimental evaluation of test cases generated from UML diagrams\n", "abstract": " This paper presents a single project experiment on the fault revealing capabilities of test sets that are generated from UML statecharts and sequence diagrams. The results of this experiment show that the statechart test sets do better at revealing unit level faults than the sequence diagram test sets, and the sequence diagram test sets do better at revealing integration level faults than the statechart test sets. The experimental data also show that the statecharts result in more test cases than the sequence diagrams. The experiment showed that UML diagrams can be used in generating test data systematically, and different UML diagrams can play different roles in testing.", "num_citations": "24\n", "authors": ["53"]}
{"title": "A computer algebra system for nonassociative identities\n", "abstract": " In recent years there has been an emergence of general purpose computer algebra systems such as Maple, Macsyma, Mathematica, and Reduce. These systems provide a wealth of facilities for performing symbolic algebraic computation. Many are capable of computing a Gr\u251c\u2562bner bases of the ideal generated by a set of multivariate polynomials. Such systems work well at manipulating polynomials in variables that commute and associate. The reader is referred to Caviness [3] for more on these computer systems. To our knowledge, however, little work has been done toward an interactive computer system specifically for performing nonassociative algebraic computation. The purpose of this paper is to describe an initial attempt at building a specialized, but useful, system for doing nonassociative algebra. We should remark that the application of computers to nonassociative algebra is not new. An example of very early work is by Kleinfeld [10], and two books [2, 12] contain many papers on such work. However these computer solutions have usually been aimed at solving a unique problem with a very particular solution. Most often, the resulting program could not be easily reused for other similar problems. Our system is called Albert. In this paper we will describe its capabilities, briefly discuss its implementation, and then discuss some typical computations that Albert has performed.", "num_citations": "24\n", "authors": ["53"]}
{"title": "Test Sequence Generation For Integration Testing Of Component Software\n", "abstract": " Ensuring high object interoperability is a goal of integration testing for object-oriented (OO) software. When messages are sent, objects that receive them should respond as intended. Ensuring this is especially difficult when software uses components that are developed by different vendors, in different languages, and the implementation sources are not all available. A finite state machines model of inter-operating OO classes was presented in a previous paper. The previous paper presented details of the method and empirical results from an automatic tool. This paper presents additional details about the tool itself, including how test sequences are generated, how several difficult problems were solved and the introduction of new capabilities to help automate the transformation of test specifications into executable test cases. Although the test method is not 100% automated, it represents a fresh approach to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["53"]}
{"title": "Coverage criteria for state based specifications\n", "abstract": " Test engineers often face the task of developing a set of test cases that are appropriate for a given software artefact. The software testing literature is replete with testing methods tailored to the various specification, design, and implementation methods used in software engineering. This chapter takes a novel inverted view. Instead of starting with the specific artefact at hand, we identify two general sets of coverage criteria \u0393\u00c7\u00f4 one based on graphs and the other based on predicates. We then ask two questions with respect to the specific artefact under test: (1) What graphs are suitable abstractions of the artefact for the purpose of testing? (2) What predicates should be extracted from this artefact for the purpose of testing? Combining the answers to these two questions with the standard graph-based and logic-based coverage criteria yields test requirements. The test engineer can then proceed to identify test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["53"]}
{"title": "On the nonmaintainability of open-source software\n", "abstract": " A major strength of open-source software is that the source code is open to scrutiny by anyone who chooses to examine it. Accordingly, it is reasonable to assume that the quality of opensource software will be higher than that of closed-source software. After all, closed-source software is examined by only a limited number of individuals, all of whom are paid to do so. It seems equally reasonable to conclude that open-source software is superior to closed-source software in other ways as well, including maintainability. Again, the argument is that the scrutiny by a large number of volunteers leads to a better product. On the other hand, the fact that open-source software is a product of an amorphous group of individuals, rather than a hierarchical development team, means that there is no single person who is in charge of an open-source software product. As a result, modifications can be made to an individual module that could have a deleterious effect on the maintainability of the opensource software product as a whole. An example of this is the introduction of common coupling into an open-source software product.The coupling between two units of a software product is a measure of the degree of interaction between those units [1\u0393\u00c7\u00f43] and, hence, of the dependency between the units. Modules X and Y are common (global) coupled if X and Y share references to the same global variable. It has been shown [4] that coupling is related to fault-proneness. Coupling has not yet been explicitly shown to be related to maintainability. On the other hand, there is as yet no precise definition of maintainability, and therefore there are no generally accepted\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["53"]}
{"title": "A model-based testing technique for component-based real-time embedded systems\n", "abstract": " The growing complexity of modern real-time embedded systems is leading to increased use of component-based software engineering (CBSE) technology. Although many ideas have been proposed for building component-based real-time embedded software, techniques for testing component-based realtime systems are scarce. The challenges not only come from the integration of different types of components through their interfaces but also include the composition of extra-functional properties. In an embedded system, extra-functional requirements are as important as functional requirements. A real-time embedded system needs to achieve its functionality under the constraints caused by its extra-functional properties. Correct functional behavior with regard to timing properties is essential to real-time embedded system. This paper focuses on real-time requirements instead of all extra-functional requirements\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["53"]}
{"title": "Automated testing of timeliness: A case study\n", "abstract": " A problem with testing timeliness of real-time applications is the response-time dependency on the execution order of concurrent tasks. Conventional test methods ignore task interleaving and timing and thus do not help determine which execution orders need to be exercised to test temporal correctness. Model based mutation testing has been proposed to generate inputs and determine the execution orders that need to be verified to increase confidence in timeliness. This paper evaluate a mutation-based framework for automated testing of timeliness by applying it on a small control system running on Linux/RTAI. The experiments presented in this paper indicate that mutation-based test cases are more effective than random and stress tests in finding both naturally occurring and randomly seeded timeliness faults.", "num_citations": "20\n", "authors": ["53"]}
{"title": "Syntactic fault patterns in oo programs\n", "abstract": " Although program faults are widely studied, there are many aspects of faults that we still do not understand, particularly about OO software. In addition to the simple fact that one important goal during testing is to cause failures and thereby detect faults, a full understanding of the characteristics of faults is crucial to several research areas. The power that inheritance and polymorphism brings to the expressiveness of programming languages also brings a number of new anomalies and fault types. In prior work we presented a fault model for the appearance and realization of OO faults that are specific to the use of inheritance and polymorphism. Many of these faults cannot appear unless certain syntactic patterns are used. The patterns are based on language constructs, such as overriding methods that directly define inherited state variables and non-inherited methods that call inherited methods. If one of these syntactic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "20\n", "authors": ["53"]}
{"title": "An experimental evaluation of web mutation operators\n", "abstract": " While modern web development technologies enhance the capabilities of web applications, they introduce challenges for testers. This paper introduces, evaluates, and refines web mutation operators that target interaction faults in web applications. An experimental study is conducted on 11 subject web applications using 15 web mutation operators. The effectiveness of 12 independently developed test sets are analyzed in terms of how well they kill web mutants. Web mutation adequate tests are compared with independently created test sets to evaluate the web mutation operators. Tests designed to satisfy the web mutation testing criterion provide 100% coverage while the tests designed to satisfy traditional testing criteria provide, on average, 47%coverage. The paper also analyzes which mutants and mutation operators the traditional tests had difficulty killing. We found that some types of mutants that are not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["53"]}
{"title": "Impact of release intervals on empirical research into software evolution, with application to the maintainability of Linux\n", "abstract": " In most empirical research on software evolution, analysis of the data is performed with respect to the release sequence number (RSN), rather than the release date. This distinction is important when the intervals between release dates vary widely, as is generally the case with open-source software. A widely cited study on the maintainability of Linux was published in this journal in 2002. The study showed that, whereas the size of the Linux kernel grew linearly with respect to the RSN, the amount of common coupling grew exponentially. In view of the adverse effect of common coupling on maintainability, the conclusion drawn there was that Linux needed to be refactored with minimal common coupling. Here, it is shown that, if the same data are analysed with respect to the release date, the amount of common coupling grows linearly; hence, there is no need to refactor Linux to promote maintainability. The authors\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["53"]}
{"title": "An integrated system for automatically generating test data\n", "abstract": " A description is given of the Godzilla automatic test data generator, an integrated set of tools that implements a new test data generation method, constraint-based testing, that is based on mutation analysis. Constraint-based testing integrates mutation analysis with several other testing techniques, such as statement analysis, branch coverage, and data flow analysis. Because Godzilla uses a rule-based approach to generate adequate test data, it is easily extendible to allow new testing techniques to be integrated into the current system. The system that has been built to implement constraint-based testing is described. Godzilla was designed in an object-oriented fashion that emphasized orthogonality and modularity. Godzilla is described from a practical viewpoint with emphasis on the internal structure of the system and the engineering problems that were solved during the implementation.< >", "num_citations": "19\n", "authors": ["53"]}
{"title": "Anatomy of a software engineering project\n", "abstract": " This paper discusses a complete software development project carried out in a one quarter undergraduate software engineering course. The project was the design and implementation of a complete system by 25 students. They worked in smaller groups on four functionally separate subsystems that were successfully integrated into a complete system. This was accomplished by using five advanced students to manage the groups, real users to criticize each step of the process, and UNIX tools to implement the subsystems. This paper describes the project, presents the methodologies used, and discusses both the positive and negative aspects of this course. It concludes by presenting a set of recommendations based on our experience with this project.", "num_citations": "19\n", "authors": ["53"]}
{"title": "A scalable big data test framework\n", "abstract": " This paper identifies three problems when testing software that uses Hadoop-based big data techniques. First, processing big data takes a long time. Second, big data is transferred and transformed among many services. Do we need to validate the data at every transition point? Third, how should we validate the transferred and transformed data? We are developing a novel big data test framework to address these problems. The test framework generates a small and representative data set from an original large data set using input space partition testing. Using this data set for development and testing would not hinder the continuous integration and delivery when using agile processes. The test framework also accesses and validates data at various transition points when data is transferred and transformed.", "num_citations": "18\n", "authors": ["53"]}
{"title": "Automatically testing interacting software components\n", "abstract": " One goal of integration testing for object-oriented software is to ensure high object interoperability. Sent messages should have the intended effects on the states and subsequent actions of the receiving objects. This is especially difficult when software is comprised of components developed by different vendors, with different languages, and the implementation sources are not all available. A previous paper presented a model of inter-operating OO classes based on finite state machines. It addresses methods for identifying the relevant actions of a test component to be integrated into the system, transforms the finite state specification into a control and data flow graph, labels the graph with all defs and uses of class variables, and presents an algorithm to generate test specifications as specific paths through the directed graph. It also presents empirical results from an automatic tool that was built to support this test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["53"]}
{"title": "Albert user's guide\n", "abstract": " Albert is an interactive research tool to assist the specialist in the study of nonassociative algebras. This document serves as a technical guide to Albert. We refer the reader to [JMO] for a more casual tutorial\u0393\u00c7\u00e1. The main problem addressed by Albert is the recognition of polynomial identities. Roughly, Albert works in the following way. Suppose a user wishes to study alternative algebras. These are algebras defined by the two polynomial identities (yx) x\u0393\u00ea\u00c6 y (xx) and (xx) y\u0393\u00ea\u00c6 x (xy), known respectively as the right and left alternative laws. In particular, the user wishes to know if, in the presence of the right and left alternative laws,(a, b, c)\u0393\u00f9\u00aa[a, b] is also an identity. Here (a, b, c) denotes (ab) c\u0393\u00ea\u00c6 a (bc),[a, b] denotes ab\u0393\u00ea\u00c6 ba, and x\u0393\u00f9\u00aa y denotes xy+ yx. The user first supplies Albert with the right and left alternative laws, using the identity command. Next, the user supplies the problem type. This refers to the number and degree of letters in the target polynomial. For example, in this problem, each term of the target polynomial has two a\u0393\u00c7\u00d6s, two b\u0393\u00c7\u00d6, and one c, and so the problem type is 2a2b1c. This is entered using the generators command. It may be that over certain fields of scalars the polynomial is an identity, but over others it is not an identity. Albert allows the user to supply the field of scalars, but currently the user must select either a Galois field Z/pZ in which p is a prime less than 263\u0393\u00ea\u00c6 1, or the field of rational numbers Q\u0393\u00c7\u00ed. This is done using the field command. If no field is entered, the default field Z/251Z is chosen.In deciding whether a given polynomial is an identity or not, Albert internally constructs a certain homomorphic image of the free algebra. It is not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "17\n", "authors": ["53"]}
{"title": "Using symbolic execution to aid automatic test data generation\n", "abstract": " It is shown how symbolic execution is used to solve the internal variable problem in the Godzilla test data generator. In constraint-based testing, which is used by the system, the internal variable problem appears when the constraints that specify the test cases contain internal variables. The necessary background is developed by describing the constraint systems used by Godzilla and by discussing symbolic execution in general terms. The application of symbolic execution to the internal variable problem is presented. The discussion focuses on the software used, including algorithmic details. Following this, a practical example of using this system to detect a fault in a small program is presented.< >", "num_citations": "17\n", "authors": ["53"]}
{"title": "A systematic review of cost reduction techniques for mutation testing: preliminary results\n", "abstract": " This paper reports on results from a systematic review that characterizes the state-of-the-art on cost reduction for mutation testing. It analyzes the evolution of research on this topic and its underlying goals and techniques, and identifies metrics used to measure cost reduction. The mixed search strategy used automatic search, snowballing, and a survey of authors of primary studies. The analysis is based on a set of 165 peer-reviewed studies, from which 146 present either original or updated approaches and results for cost reduction of mutation testing. A list of 6 main goals for cost reduction is presented, and 22 techniques were identified. Historically, 18 metrics have been used to measure the gains and losses observed in experimental studies. In the last decade, substantial growth in the number of published studies was observed, particularly among techniques such as selective mutation, evolutionary algorithms\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["53"]}
{"title": "Evasive bots masquerading as human beings on the web\n", "abstract": " Web bots such as crawlers are widely used to automate various online tasks over the Internet. In addition to the conventional approach of human interactive proofs such as CAPTCHAs, a more recent approach of human observational proofs (HOP) has been developed to automatically distinguish web bots from human users. Its design rationale is that web bots behave intrinsically differently from human beings, allowing them to be detected. This paper escalates the battle against web bots by exploring the limits of current HOP-based bot detection systems. We develop an evasive web bot system based on human behavioral patterns. Then we prototype a general web bot framework and a set of flexible de-classifier plugins, primarily based on application-level event evasion. We further abstract and define a set of benchmarks for measuring our system's evasion performance on contemporary web applications\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["53"]}
{"title": "Testing coupling relationships in object\u0393\u00c7\u00c9oriented programs\n", "abstract": " As we move toward developing object\u0393\u00c7\u00c9oriented (OO) programs, the complexity traditionally found in functions and procedures is moving to the connections among components. Different faults occur when components are integrated to form higher\u0393\u00c7\u00c9level structures that aggregate the behavior and state. Consequently, we need to place more effort on testing the connections among components. Although OO technologies provide abstraction mechanisms for building components that can then be integrated to form applications, it also adds new compositional relations that can contain faults. This paper describes techniques for analyzing and testing the polymorphic relationships that occur in OO software. The techniques adapt traditional data flow coverage criteria to consider definitions and uses among state variables of classes, particularly in the presence of inheritance, dynamic binding, and polymorphic overriding of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["53"]}
{"title": "Introduction to software testing\n", "abstract": " Extensively class tested, this text takes an innovative approach to explaining the process of software testing: it defines testing as the process of applying a few well-defined, general-purpose test criteria to a structure or model of the software. The structure of the text directly reflects the pedagogical approach and incorporates the latest innovations in testing, including techniques to test modern types of software such as OO, web applications, and embedded software.", "num_citations": "16\n", "authors": ["53"]}
{"title": "Coupling-based integration testing\n", "abstract": " Integration testing is an important part of the testing process, but few integration testing techniques have been systematically studied or defined. This paper presents an integration testing technique based on couplings between software components. The coupling-based testing technique is described, and coverage criteria for three types of 12 coupling levels are defined. This technique can be used to support integration testing of software components, and satisfies part of the FAA's requirements for structural coverage analysis of software.", "num_citations": "16\n", "authors": ["53"]}
{"title": "Growing a reduced set of mutation operators\n", "abstract": " Although widely considered to be quite powerful, mutation testing is also known for its expense. Three fundamental (and related) sources for much of the expense are (1) the number of mutants, (2) the number of equivalent mutants, and (3) the number of test cases needed to kill the mutants. Recent results have shown that mutation systems create a significant number of mutants that are killed by the same tests. These mutants can be considered to be \u0393\u00c7\u00a3redundant,\u0393\u00c7\u00a5 in the sense that if N mutants are killed by the same test, only one of those mutants is truly needed. Selective mutation, one-op mutation, and random mutant selection are ways to choose a \u0393\u00c7\u00a3reduced\u0393\u00c7\u00a5 set of mutation operators that will help testers design tests that are almost as effective, as measured by running the tests against the complete set of mutants. This paper presents a novel procedure for choosing a reduced set of mutation operators based on a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["53"]}
{"title": "Web application bypass testing\n", "abstract": " Input validation refers to checking user inputs to a program to ensure that they conform to expectations of the program. Input validation is used to check the format of numbers and strings, check the length of strings, and to ensure that strings do not contain invalid characters. Input validation testing (IVT) is particularly important for software that has a heavy reliance on user inputs, including Web applications. A common technique in Web applications is to perform input validation on the client by using HTML attributes and scripting languages such as JavaScript. An insidious problem with performing input validation on the client is that end users have the ability to bypass this validation. Bypass testing is a unique and novel way to create test cases that is available only because of the unusual mix of client-server, HTML GUI, and JavaScript technologies that are used in Web applications. This workshop paper presents the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["53"]}
{"title": "Analyzing software architecture descriptions to generate system-level tests\n", "abstract": " As the size and complexity of software systems increase, problems stemming from the design and integration of overall system structure become more significant than problems stemming from the choice of algorithms and data structures. This is due not only to the increased amount of code, but also to the need to distribute the parts of the application and have them interact in complex and potentially novel ways. Future software systems can be expected to continue to grow in size and complexity, which will greatly strain our already questionable ability to develop software that is both functional and reliable. At least for the foreseeable future, software architecture and object-oriented design will continue to be used to facilitate this growth. Software architecture allows developers to abstract away the details of the individual components of an application, allowing them to be viewed as sets of components with associated connectors that describe the interactions between these components. One product of software architecture research has been a set of formal and semi-formal languages that provide behavioral descriptions of the components and connectors. These architecture description languages (ADLs) represent a significant opportunity for dealing with the issue of scale with respect to testing and analysis of software systems. Other researchers have looked into aspects of analysis and testing of software architectures, including developing integration test plans by using test criteria based on CHAM [3], dependence analysis techniques called chaining [6], and applying dependence analysis to problems in software maintenance [5]. We are currently\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["53"]}
{"title": "Is mutation analysis effective at testing android apps?\n", "abstract": " Not only is Android the most widely used mobile operating system, more apps have been released and downloaded for Android than for any other OS. However, quality is an ongoing problem, with many apps being released with faults, sometimes serious faults. Because the structure of mobile app software differs from other types of software, testing is difficult and traditional methods do not work. Thus we need different approaches to test mobile apps. In this paper, we identify challenges in testing Android apps, and categorize common faults according to fault studies. Then, we present a way to apply mutation testing to Android apps. Additionally, this paper presents results from two empirical studies on fault detection effectiveness using open-source Android applications: one for Android mutation testing, and another for four existing Android testing techniques. The studies use naturally occurring faults as well as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["53"]}
{"title": "Toward harnessing high-level language virtual machines for further speeding up weak mutation testing\n", "abstract": " High-level language virtual machines (HLL VMs) are now widely used to implement high-level programming languages. To a certain extent, their widespread adoption is due to the software engineering benefits provided by these managed execution environments, for example, garbage collection (GC) and cross-platform portability. Although HLL VMs are widely used, most research has concentrated on high-end optimizations such as dynamic compilation and advanced GC techniques. Few efforts have focused on introducing features that automate or facilitate certain software engineering activities, including software testing. This paper suggests that HLL VMs provide a reasonable basis for building an integrated software testing environment. As a proof-of-concept, we have augmented a Java virtual machine (JVM) to support weak mutation analysis. Our mutation-aware HLL VM capitalizes on the relationship\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["53"]}
{"title": "An industrial case study of bypass testing on web applications\n", "abstract": " Web applications are interactive programs that are deployed on the world wide Web. Their execution is usually controlled very heavily by user choices and user data. This makes them vulnerable to abnormal behavior from invalid inputs as well as security attacks. Thus, Web applications invest heavily in validating user inputs according to defined constraints on the values. This work focuses on validation done on the client, which uses two types of technologies; restrictions in HTML form fields and scripts that check values. Unfortunately users have the ability to subvert or skip client-side validation. Bypass testing has been developed to test the behavior of Web applications when client-side validation is skipped. This paper presents results from an industry case study of bypass testing applied to a project from Avaya Research Labs, NPP. The paper presents a process for designing, implementing, automating and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["53"]}
{"title": "An empirical comparison of modularity of procedural and object-oriented software\n", "abstract": " A commonly held belief is that applications written in object-oriented languages are more modular than those written in procedural languages. This paper presents results from an experiment that examines this hypothesis. Open source and industrial program modules written in the procedural languages of Fortran and C were compared with open source program modules written in the object-oriented languages of C++ and Java. The metrics examined in this study were lines of code per module and number of parameters per module. The results of the investigation support the hypothesis. The modules of the object-oriented programs were found to be half the size of those of the procedural programs and the average number of parameters per module for the object-oriented programs was approximately half that of the procedural programs. Thus the object-oriented programs were twice as modular as the procedural\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["53"]}
{"title": "Finding redundancy in web mutation operators\n", "abstract": " New web development technologies enhance functionality of web applications but also introduce challenges in testing the software. As mutation analysis has been shown to be effective at designing tests for traditional software, we previously proposed web mutation testing. However, applying mutation analysis can be computationally expensive due to an extensive number of test requirements. This paper introduces, evaluates, and refines web mutation operators with the goal of reducing the number of test requirements. This paper evaluates redundancy among web mutation operators by analyzing the types of mutants that can be killed by tests generated specifically to kill other types of mutants and the types of mutants (and the operators that generate them) that can be excluded from testing while maintaining the same level of fault detection. An experimental study was conducted on 12 subject web applications\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["53"]}
{"title": "Using mutation to design tests for aspect-oriented models\n", "abstract": " Context: Testing for properties such as robustness or security is complicated because their concerns are often repeated in many locations and muddled with the normal code. Such \u0393\u00c7\u00a3cross-cutting concerns\u0393\u00c7\u00a5 include things like interrupt events, exception handling, and security protocols. Aspect-oriented (AO) modeling allows developers to model the cross-cutting behavior independently of the normal behavior, thus supporting model-based testing of cross-cutting concerns. However, mutation operators defined for AO programs (source code) are usually not applicable to AO models (AOMs) and operators defined for models do not target the AO features.Objective: We present a method to design abstract tests at the aspect-oriented model level. We define mutation operators for aspect-oriented models and evaluate the generated mutants for an example system.Method: AOMs are mutated with novel operators that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["53"]}
{"title": "Mutating aspect-oriented models to test cross-cutting concerns\n", "abstract": " Aspect-oriented (AO) modeling is used to separate normal behaviors of software from specific behaviors that affect many parts of the software. These are called \u0393\u00c7\u00a3cross-cutting concerns,\u0393\u00c7\u00a5 and include things such as interrupt events, exception handling, and security protocols. AO modeling allow developers to model the behaviors of cross-cutting concerns independently of the normal behavior. Aspect-oriented models (AOM) are then transformed into code by \u0393\u00c7\u00a3weaving\u0393\u00c7\u00a5 the aspects (modeling the cross-cutting concerns) into all locations in the code where they are needed. Testing at this level is unnecessarily complicated because the concerns are often repeated in many locations and because the concerns are muddled with the normal code. This paper presents a method to design robustness tests at the abstract, or model, level. The models are mutated with novel operators that specifically target the features of AOM\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["53"]}
{"title": "A test automation language framework for behavioral models\n", "abstract": " Model-based testers design tests in terms of models, such as paths in graphs. This results in abstract tests, which have to be converted to concrete tests because the abstract tests use names and events that exist in the model, but not the implementation. Model elements often appear in many abstract tests, so testers write the same redundant code many times. However, many existing model-based testing techniques are very complicated to use in practice, especially in agile software development. Thus, testers usually have to transform abstract tests to concrete tests by hand. This is time-consuming, labor-intensive, and error-prone. This paper presents a language to automate the creation of mappings from abstract tests to concrete tests. Three issues are addressed: (1) creating mappings and generating test values, (2) transforming graphs and using coverage criteria to generate test paths, and (3) solving constraints\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["53"]}
{"title": "Adding criteria-based tests to test driven development\n", "abstract": " Test driven development (TDD) is the practice of writing unit tests before writing the source. TDD practitioners typically start with example-based unit tests to verify an understanding of the software's intended functionality and to drive software design decisions. Hence, the typical role of test cases in TDD leans more towards specifying and documenting expected behavior, and less towards detecting faults. Conversely, traditional criteria-based test coverage ignores functionality in favor of tests that thoroughly exercise the software. This paper examines whether it is possible to combine both approaches. Specifically, can additional criteria based tests improve the quality of TDD test suites without disrupting the TDD development process? This paper presents the results of an observational study that generated additional criteria-based tests as part of a TDD exercise. The criterion was mutation analysis and the additional\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["53"]}
{"title": "On the testing maturity of software producing organizations: Detailed data\n", "abstract": " This paper presents data from a study of the current state of practice of software testing. Test managers from twelve different software organizations were interviewed. The interviews", "num_citations": "12\n", "authors": ["53"]}
{"title": "What to expect of predicates: An empirical analysis of predicates in real world programs\n", "abstract": " One source of complexity in programs is logic expressions, i.e., predicates. Predicates define much of the functional behavior of the software. Many logic-based test criteria have been developed, including the active clause coverage (ACC) criteria and the modified condition/decision coverage (MCDC). The MCDC/ACC criteria is viewed as being expensive, which motivated us to evaluate the cost of applying these criteria using a basic proxy: the number of clauses. We looked at the frequency and percentage of predicates in 63 Java programs. Moreover, we also compared these Java programs with three programs in the safety-critical domain, in which logic-basic testing is often used. Although around 99% of the predicates within Java programs contain at most three clauses, there is a positive linear correlation between overall measures of size and the number of predicates that have more than three clauses\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["53"]}
{"title": "Description of mujava\u0393\u00c7\u00d6s method-level mutation operators\n", "abstract": " This document provides a brief description of method-level mutation operators for Java used by muJava. When designing method-level mutation operators for Java, we followed the selective approach [3]. The selective results found that the traditional operatorso of modifying operands and statements give little effectiveness to mutation testing. Therefore, we only consider mutation operators that modify expression by replacing, deleting, and inserting primitive operators. muJava provides six kinds of primitive operators;(1) arithmetic operator,(2) relational operator,(3) conditional operator,(4) shift operator,(5) logical operator, and (6) assignment. For some of them, muJava provides short-cut operators. This section presents designs of mutation operators for those six kinds of primitive operators. We try to design mutation operators that replace, insert, and delete the primitive operators. We defined total 12 method-level operators in Table 1. The detailed description for the operators are described in the following subsections, according to each primitive operator.", "num_citations": "10\n", "authors": ["53"]}
{"title": "Increasing Class-Component Testability.\n", "abstract": " Testability has many effects on software. In general, increasing testability makes detecting faults easier. However, increasing testability of third party software components is difficult because the source is usually not available. This paper introduces a method to increase component testability. This method helps a user test when the component is reused during integration. First, we analyze a component to gather definition and use information about method and class variables. Then, this information is used to increase component testability to support component testing. Increased testability helps to detect errors, and helps testers observe state variables and generate inputs for testing. This paper uses an example to report the effort (in terms of test cases) and effectiveness (in terms of killed mutants).", "num_citations": "10\n", "authors": ["53"]}
{"title": "Using mutant stubbornness to create minimal and prioritized test sets\n", "abstract": " In testing, engineers want to run the most useful tests early (prioritization). When tests are run hundreds or thousands of times, minimizing a test set can result in significant savings (minimization). This paper proposes a new analysis technique to address both the minimal test set and the test case prioritization problems. This paper precisely defines the concept of mutant stubbornness, which is the basis for our analysis technique. We empirically compare our technique with other test case minimization and prioritization techniques in terms of the size of the minimized test sets and how quickly mutants are killed. We used seven C language subjects from the Siemens Repository, specifically the test sets and the killing matrices from a previous study. We used 30 different orders for each set and ran every technique 100 times over each set. Results show that our analysis technique performed significantly better than prior\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["53"]}
{"title": "A novel self-paced model for teaching programming\n", "abstract": " The Self-Paced Learning Increases Retention and Capacity (SPARC) project is responding to the well-documented surge in CS enrollment by creating a self-paced learning environment that blends online learning, automated assessment, collaborative practice, and peer-supported learning. SPARC delivers educational material online, encourages students to practice programming in groups, frees them to learn material at their own pace, and allows them to demonstrate proficiency at any time. This model contrasts with traditional course offerings, which impose a single schedule of due dates and exams for all students. SPARC allows students to complete courses faster or slower at a pace tailored to the individual, thereby allowing universities to teach more students with the same or fewer resources. This paper describes the goals and elements of the SPARC model as applied to CS1. We present results so far and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["53"]}
{"title": "A case study on bypass testing of web applications\n", "abstract": " Society\u0393\u00c7\u00d6s increasing reliance on services provided by web applications places a high demand on their reliability. The flow of control through web applications heavily depends on user inputs and interactions, so user inputs should be thoroughly validated before being passed to the back-end software. Although several techniques are used to validate inputs on the client, users can easily bypass this validation and submit arbitrary data to the server. This can cause unexpected behavior, and even allow unauthorized access. A test technique called bypass testing intentionally sends invalid data to the server by bypassing client-side validation. This paper reports results from a comprehensive case study on 16 deployed, widely used, commercial web applications. As part of this project, the theory behind bypass testing was extended and an automated tool, AutoBypass, was built. The case study found failures in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["53"]}
{"title": "Integrating research, reuse, and integration into software engineering courses\n", "abstract": " This paper discusses a method for incorporating several important software engineering concepts that have been traditionally hard to teach into courses at both the undergraduate and graduate level. We have created a project template that can be instantiated in many ways to be tailored to the level of a particular course, the number of students, the quality of students, and the goals of the course. We consider a \u0393\u00c7\u00a3large\u0393\u00c7\u00a5 software project to be one in which each programmer's contribution represents a small part of the overall project (less than 10%). Our project template is a completed software system, which, although too large for a semester project in its complete form, can be easily divided into coherent subsystems. The students are provided with some subsystems, and asked to derive requirements for, design, implement, and test the remaining subsystems. This approach allows the students to work in a large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["53"]}
{"title": "An experimental comparison of edge, edge-pair, and prime path criteria\n", "abstract": " BackgroundMany criteria have been proposed to generate test inputs. Criteria are usually compared in terms of subsumption: if a criterion C1 subsumes C2, it is guaranteed that every test set that satisfies C1 will also satisfy C2. An implication of this notion of subsumption is that C1-adequate tests tend to find more faults than C2-adequate tests, but C1-adequate tests tend to be larger. Thus, while useful, the idea of subsumption does not elaborate on some practical properties of expensive criteria as, for instance, how many more faults a C1-adequate test set will find? More generally, what is the return on investment for choosing more expensive criteria?MethodTo provide a more accurate idea of the fault finding ability and cost of several criteria, we set out to compare three structural graph coverage criteria: edge coverage (EC), edge-pair coverage (EPC), and prime path coverage (PPC). PPC and EPC subsume EC\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["53"]}
{"title": "An industrial study of applying input space partitioning to test financial calculation engines\n", "abstract": " This paper presents results from an industrial study that applied input space partitioning and semi-automated requirements modeling to large-scale industrial software, specifically financial calculation engines. Calculation engines are used in financial service applications such as banking, mortgage, insurance, and trading to compute complex, multi-conditional formulas to make high risk financial decisions. They form the heart of financial applications, and can cause severe economic harm if incorrect. Controllability and observability of these calculation engines are low, so robust and sophisticated test methods are needed to ensure the results are valid. However, the industry norm is to use pure human-based, requirements-driven test design, usually with very little automation. The Federal Home Loan Mortgage Corporation (FHLMC), commonly known as Freddie Mac, concerned that these test design\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["53"]}
{"title": "Assessing the influence of multiple test case selection on mutation experiments\n", "abstract": " Mutation testing is widely used in experiments. Some papers experiment with mutation directly, while others use it to introduce faults to measure the effectiveness of tests created by other methods. There is some random variation in the mutation score depending on the specific test values used. When generating tests to use in experiments, a common, although not universal practice, is to generate multiple sets of tests to satisfy the same criterion or according to the same procedure, and then to compute their average performance. Averaging over multiple test sets is thought to reduce the variation in the mutation score. This practice is extremely expensive when tests are generated by hand (as is common) and as the number of programs increase (a current positive trend in software engineering experimentation). The research reported in this short paper asks a simple and direct question: do we need to generate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["53"]}
{"title": "Using abstraction and Web applications to teach criteria-based test design\n", "abstract": " The need for better software continues to rise, as do expectations. This, in turn, puts more emphasis on finding problems before software is released. Industry is responding by testing more, but many test engineers in industry lack a practical, yet theoretically sound, understanding of testing. Software engineering educators must respond by teaching students to test better. An essential testing skill is designing tests, and an efficient way to design high quality tests is to use an engineering approach: test criteria. To achieve the maximum benefit, criteria should be used during unit (developer) testing, as well as integration and system testing. This paper presents an in-depth teaching experience report on how we successfully teach criteria-based test design using abstraction and publicly accessible web applications. Our teaching materials are freely available online or upon request.", "num_citations": "8\n", "authors": ["53"]}
{"title": "Software testing: from theory to practice\n", "abstract": " This paper is about the disparity between what is known and what is being learned in academia and what is being used in industry. The author believes there are many reasons. Some of these represent accidental and some essential problems. Most are part of a general issue of quality in the software products. Most problems that make it more difficult to apply testing techniques are part of a larger problem which make them more cost-effective to produce high quality software. The author presents a list of reasons industry does not use the highly advanced, and in some cases, highly developed software testing techniques that are available. The problems are divided into three broad categories: problems in industry; problems in academic research and education; and problems in the interface between the two.", "num_citations": "8\n", "authors": ["53"]}
{"title": "Transformation rules for platform independent testing: An empirical study\n", "abstract": " Most Model-Driven Development projects focus on model-level functional testing. However, our recent study found an average of 67% additional logic-based test requirements from the code compared to the design model. The fact that full coverage at the design model level does not guarantee full coverage at the code level indicates that there are semantic behaviors in the model that model-based tests might miss, e.g., conditional behaviors that are not explicitly expressed as predicates and therefore not tested by logic-based coverage criteria. Avionics standards require that the structure of safety critical software is covered according to logic-based coverage criteria, including MCDC for the highest safety level. However, the standards also require that each test must be derived from the requirements. This combination makes designing tests hard, time consuming and expensive to design. This paper defines a new\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["53"]}
{"title": "Web Application Testing Challenges\n", "abstract": " A website is a static collection of HTML files that are linked together through tags on the World Wide Web. A web application, however, is an arbitrarily complex program that happens to be deployed on the World Wide Web. Web applications use new technologies that change on a regular basis, are almost always distributed, often employ concurrency, are inherently component-based, usually built with diverse languages, and use control and state behaviors that are not available to traditional software. This paper explores the technological-based differences between web software applications and traditional applications, with a specific emphasis on how these differences affect testing. Both applied and research issues are considered.", "num_citations": "7\n", "authors": ["53"]}
{"title": "The journal impact factor\n", "abstract": " This issue contains three divergent papers. The first, Modular formal verification of specifications of concurrent systems, by Gradara, Santone, Vaglini, and Villani, proposes a bottom-up approach to verifying modular systems. Properties of components are first verified, then emergent properties of the system as a whole are verified. The approach is applied to a web service. The second paper, Simulated time for host-based testing with TTCN-3, by Blom, Deiss, Kontio, Rennoch, and Sidorova, describes a method to test real-time embedded software. When real-time software is tested in a development environment where the timing characteristics do not match the target environment, this research proposes using simulated time to test the real-time properties. The third paper, IPOG/IPOG-D: Efficient test generation for multi-way combinatorial testing, by Lei, Kacker, Kuhn, Okun, and Lawrence, presents two new strategies\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["53"]}
{"title": "Why don't we publish more TDD research papers?\n", "abstract": " One of the most important changes in the software engineering industry over the last decade has been the emergence and growth of agile processes. The agile process that has the most effect on our field is test\u0393\u00c7\u00c9driven development (TDD), which is being adopted by more companies every week. TDD puts testing \u0393\u00c7\u00a3front and center,\u0393\u00c7\u00a5 by using automated tests to replace functional requirements. TDD asks the engineer to define initial behavior of software with a test case. Each automated TDD test includes input values and a desired response encoded in an assertion. The desired response, or behavior, replaces the test oracle in more traditional tests. Since a TDD test expresses desired behavior, it initially fails. Then the engineer writes just enough software to allow the test to pass; that is, the engineer implements the desired behavior. After the latest test passes, the engineer should refactor the software by cleaning up\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["53"]}
{"title": "Introduction to Software Testing Edition 2\n", "abstract": " This document contains the work-in-progress solutions for the second edition of the text. The goal is to keep the solution manuals synchronized with the textbook so that there are no \u0393\u00c7\u00a3TBD\u0393\u00c7\u00a5 solutions, as persisted in the first edition for many years. We distinguish between \u0393\u00c7\u00a3student solutions\u0393\u00c7\u00a5 and \u0393\u00c7\u00a3instructor only\u0393\u00c7\u00a5 for the convenience of both. Students can work homeworks then check their own answers. Instructors can assign homeworks with some confidence that students will do their own work instead of looking up the answer in the manual.", "num_citations": "6\n", "authors": ["53"]}
{"title": "A test automation language for behavioral models\n", "abstract": " Model-based testers design tests in terms of models, such as paths in graphs. Abstract tests cannot be run directly because they use names and events that exist in the model, but not the implementation. Testers usually solve this mapping problem by hand. Model elements often appear in many abstract tests, so testers write the same redundant code many times. This is time-consuming, labor-intensive, and error-prone. This paper presents a language to automate the creation of mappings from abstract tests to concrete tests. Three issues are addressed:(1) creating mappings and generating test values,(2) transforming graphs and using coverage criteria to generate test paths, and (3) solving constraints and generating concrete test.The paper also presents results from an empirical comparison of testers using the mapping language with manual mapping on 17 open source and example programs. We found that the automated test generation method took a fraction of the time the manual method took, and the manual tests contained numerous errors in which concrete tests did not match their abstract tests.", "num_citations": "6\n", "authors": ["53"]}
{"title": "Model transformation impact on test artifacts: An empirical study\n", "abstract": " Development environments that support Model-Driven Development often focus on model-level functional testing, enabling verification of design models against their specifications. However, developers of safety-critical software systems are also required to show that tests cover the structure of the implementation. Unfortunately, the implementation structure can diverge from the model depending on choices such as the model compiler or target language. Therefore, structural coverage at the model level may not guarantee coverage of the implementation.", "num_citations": "6\n", "authors": ["53"]}
{"title": "Using formal methods to mechanize category-partition testing\n", "abstract": " We extend the category-partition method, a specification-based method for testing software. Previous work in category-partition has focused on developing structured test specifications that describe software tests. We offer guidance in making the important decisions involved in transforming test specifications to actual test cases. We present a structured approach to making those decisions, including a mechanical procedure for test derivation. With this procedure, we suggest a heuristic for choosing a minimal coverage of the categories identified in the test specifications, suggest parts of the process that can be automated, and offer a solution to the problem of identifying infeasible combinations of test case values. Our method uses formal schema-based functional specifications and is illustrated with an example of a simple file system. We conclude that our approach eases test case creation and leads to effective tests of software. We also note that by basing this procedure on formal specifications, we can identify anomalies in the functional specifications.", "num_citations": "6\n", "authors": ["53"]}
{"title": "Testing concurrent user behavior of synchronous web applications with Petri nets\n", "abstract": " Web applications are now used in every aspect of our lives to manage work, provide products and services, read email, and provide entertainment. The software technologies used to build web applications provide features that help designers provide flexible functionality, but that are challenging to model and test. In particular, the network-based request-response model of programming means that web applications are inherently \u0393\u00c7\u00a3stateless\u0393\u00c7\u00a5 and implicitly concurrent. They are stateless because a new network connection is made for each request (for example, when a user clicks a submit button). Thus, the server does not, by default, recognize multiple requests from the same user. Web applications are also concurrent because multiple users can use the same web application at the same time, creating contention for the same resources. Unfortunately, most web application testing does not adequately\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["53"]}
{"title": "Exoneration-based fault localization for SQL predicates\n", "abstract": " Spectrum-based fault localization (SFL) techniques automatically localize faults in program entities (statements, predicates, SQL clauses, etc.) by analyzing information collected from test executions. One application of SFL techniques is to find faulty SQL statements in database applications. However, prior techniques treated each SQL statement as one program entity, thus they could not find faulty elements inside SQL statements. Since SQL statements can be complex, identifying the faulty element within a faulty SQL statement is still time-consuming.In our previous paper, we developed a novel fault localization method based on row-based dynamic slicing and delta debugging techniques that can localize faults in individual clauses within SQL predicates. We call this technique exoneration-based fault localization because it can exonerate \u0393\u00c7\u00a3innocent\u0393\u00c7\u00a5 elements and precisely identify the faulty element, whereas\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["53"]}
{"title": "You should (and absolutely can) keep diversity in sharp focus during the enrollment surge\n", "abstract": " Call it a surge, call it a bubble, just don't call it business as usual. It is no secret that enrollments in college and university computer science (CS) classes are growing rapidly and faculty are under pressure to teach more classes and significantly larger classes. They need new and creative ways to accommodate as many students as possible while maintaining excellent pedagogy. Not only that, we don't want to repeat history and worsen already poor diversity statistics with enrollment management strategies that shut out or discourage women and other under-represented groups.", "num_citations": "5\n", "authors": ["53"]}
{"title": "The globalization of software engineering\n", "abstract": " Cotroneo, Orlando, Pietrantuono and Russo, presents a practical exploration of the issue of aging software. Software is modeled as \u0393\u00c7\u00ffaging\u0393\u00c7\u00d6by considering such things as the long-term depletion of resources from the operating system, incremental corruption of data and accumulation of numerical errors. This paper analyses the JVM\u0393\u00c7\u00d6s aging characteristics.(Recommended by Michael Lyu.)", "num_citations": "5\n", "authors": ["53"]}
{"title": "What is the purpose of publishing?\n", "abstract": " Gallardo, and Merino, shows how to automatically extract PROMELA models from C programs and shows results from model checking real-time avionics software. The second, Improved code defect detection with fault links, by Hayes, Chemannoor, and Holbrook, presents results from an experimental study of fault links, or relationships between types of code faults, and the types of components where the faults are located.I recently had an interesting conversation with a younger colleague. The conversation started with deciding where to submit a paper, but quickly became more general. I eventually asked her:\u0393\u00c7\u00ffWhy do you want to publish this paper?\u0393\u00c7\u00d6We came up with some interesting possible answers.", "num_citations": "5\n", "authors": ["53"]}
{"title": "Programmers Ain\u0393\u00c7\u00d6t Mathematicians, and Neither Are Testers\n", "abstract": " Formal methods have been developed for decades. An early promise was that we could use formal methods to \u0393\u00c7\u00a5prove\u0393\u00c7\u00a5 our programs correct.We have also tried to use formal methods to completely specify functional behavior of programs and to partially specify specific aspects of software behavior. Research into formal methods have led to weaker techniques to \u0393\u00c7\u00a5model\u0393\u00c7\u00a5 functional behavior, less completely and less precisely, but in ways that are easier to use. Despite these years of activity, formal methods are still seldom used in industry. As software engineering researchers, we are compelled to take the view that there must be a path from our research to actual use in industry, where real software developers use our ideas to help create real, and better, software. Thus we must ask, are formal methods a solution in search of a problem?               In this talk, I will draw a distinction between mathematical thinking\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["53"]}
{"title": "Testability of dynamic real-time systems: An empirical study of constrained execution environment implications\n", "abstract": " Real-time systems must respond to events in a timely fashion; in hard real-time systems the penalty for a missed deadline is high. It is therefore necessary to design hard real-time systems so that the timing behavior of the tasks can be predicted. Static real-time systems have prior knowledge of the worst-case arrival patterns and resource usage. Therefore, a schedule can be calculated off-line and tasks can be guaranteed to have sufficient resources to complete (resource adequacy). Dynamic real-time systems, on the other hand, do not have such prior knowledge, and therefore must react to events when they occur. They also must adapt to changes in the urgencies of various tasks, and fairly allocate resources among the tasks. A disadvantage of static real-time systems is that a requirement on resource adequacy makes them expensive and often impractical. Dynamic realtime systems, on the other hand, have the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["53"]}
{"title": "Class-component testability analysis\n", "abstract": " Testability is a quality factor used to predict the amount of effort required for software testing and to indicate the difficulty of revealing faults. This paper presents a quantitative testability analysis method for a software component that can be used when the source program is not available, but the bytecode is (as in Java. class files). This process analyzes the testability of each location to evaluate the component testability. The testability of a location is analyzed by computing the probability that the location will be executed and, if the location contains a fault, the execution will cause the fault to be revealed as a failure. This analysis process helps developers measure component testability and determine whether the component testability should be increased before the component is reused. In addition, low testability locations are identified.", "num_citations": "5\n", "authors": ["53"]}
{"title": "A comparative evaluation of tests generated from different UML diagrams: diagrams and data\n", "abstract": " This paper presents a single project experiment on the fault revealing capabilities of model-based test sets. The tests are generated from UML statecharts and UML sequence diagrams. This experiment found that the statechart test sets did better at revealing unit level faults than the sequence diagram test sets, and the sequence diagram test sets did better at revealing integration level faults than the statechart test sets. The statecharts also resulted in more test cases than the sequence diagrams. The experiment showed that model-based testing can be used to systematically generate test data and indicates that different UML models can play different roles in testing. This technical report version includes appendices with data that does not fit in a published paper.", "num_citations": "5\n", "authors": ["53"]}
{"title": "An empirical analysis of blind tests\n", "abstract": " Modern software engineers automate as many tests as possible. Test automation allows tests to be run hundreds or thousands of times: hourly, daily, and sometimes continuously. This saves time and money, ensures reproducibility, and ultimately leads to software that is better and cheaper. Automated tests must include code to check that the output of the program on the test matches expected behavior. This code is called the test oracle and is typically implemented in assertions that flag the test as passing if the assertion evaluates to true and failing if not. Since automated tests require programming, many problems can occur. Some lead to false positives, where incorrect behavior is marked as correct, and others to false negatives, where correct behavior is marked as incorrect. This paper identifies and studies a common problem where test assertions are written incorrectly, leading to incorrect behavior that is not\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "Dazed droids: A longitudinal study of android inter-app vulnerabilities\n", "abstract": " Android devices are an integral part of modern life from phone to media boxes to smart home appliances and cameras. With 38.9% of market share, Android is now the most used operating system not just in terms of mobile devices but considering all OSes. As applications' complexity and features increased, Android relied more heavily on code and data sharing among apps for faster response times and richer user experience. To achieve that, Android apps reuse functionality and data by means of inter-app message passing where each app defines the messages it expects to receive. In this paper, we analyze the proliferation of exploitable inter-app communication vulnerabilities using a rich corpus of 1) a representative sample of 32 Android devices, 2) 59 official Google Android versions, and 3) the top 18,583 apps from 2016 to 2017. This corpus covers  Android builds from version 4.4 to present. To the best of our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "Using Petri nets to test concurrent behavior of web applications\n", "abstract": " Web applications are used by companies across the world to deploy their products and services. Because of the technologies used to build web applications, they are by nature concurrent, for example, multiple users can have problems when accessing the same limited resources. The combination of the stateless nature of web applications and concurrent behavior creates unique challenges. Models have traditionally been used to test specific aspects of systems. However, existing web application models do not effectively address the combination of concurrent behavior and stateless protocol. This research project is using a novel Petri net-based model for web applications. This paper defines a novel way to design model-based coverage criteria tests that address concurrent behavior involving HTTP browser-based sessions. A tool that extracts the Petri net model of a web application has been developed and used\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "Plagiarism is for losers\n", "abstract": " This editorial explores different forms of plagiarism, discusses why people plagiarize, and offers strategies for avoiding unintentional plagiarism. The amount of plagiarism detected by STVR\u0393\u00c7\u00d6s editorial board has increased significantly in recent years. This increase might be due to multiple factors. Like most journals, STVR now uses an automated plagiarism detector that searches tens of thousands of papers for similarities. The increase might also be partly due to the general globalization of SWE research [1]. Still, another factor is that STVR receives more submissions than in the past. Regardless of why, we now desk-reject almost a dozen submissions to STVR every year. This editorial discusses plagiarism to help potential authors understand what plagiarism is and how to avoid it.Let\u0393\u00c7\u00d6s start with definitions. The Oxford Dictionary defines plagiarism as \u0393\u00c7\u00fftaking someone else\u0393\u00c7\u00d6s work or ideas and passing them off as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "Globalization\u0393\u00c7\u00f6standards for research quality\n", "abstract": " This issue features three interesting papers, all of which offer real solutions to real problems, and demonstrate their success on industrial software. The first,\u0393\u00c7\u00ffA practical model-based statistical approach for generating functional test cases: application in the automotive industry\u0393\u00c7\u00d6, by Awedikian and Yannou, presents a new way to generate tests from models. The results include a tool that selects test inputs (the test generation problem), predicts the expected results (the oracle problem), and suggests when testing can stop (the stopping problem). They have demonstrated their approach on automotive software.(Recommended by Hong Zhu.) The second,\u0393\u00c7\u00ffA novel approach to software quality risk management\u0393\u00c7\u00d6, by Bubevski, offers an advance in managing the risk of software. Bubevski\u0393\u00c7\u00d6s technique uses Six Sigma and Monte Carlo simulation and has been successfully used on industrial software.(Recommended by Min\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "globalization\u0393\u00c7\u00f6language and dialects\n", "abstract": " This issue has two excellent papers. The first, Testing and verification in service-oriented architecture: A survey, by Bozkurt, Harman and Hassoun, is another detailed survey from the Centre for Research in Evolution, Search and Testing at University College London. The authors do a wonderful job summarizing testing research on service-oriented architecture, covering no less than262 papers.(Recommended by Atif Memon.) The second, Parallel mutation testing, by Mateo andUsaola, presents new ideas that dramatically increase the speed at which we can perform mutation analysis. The paper presents a new tool, Bacterio, and results from three studies on five algorithms to execute mutants in parallel.(Recommended by Byoungju Choi.) Note that because of previous papers co-authored with the authors of these papers, Co-EiC Rob Hierons was not involved with the handling of the survey paper, and Offutt was\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "SEEWeb: making experimental artifacts available\n", "abstract": " This position paper suggests that some of the technical and methodological challenges facing software testing researchers can be addressed by establishing a repository of experimental software artifacts, in particular, artifacts that are related to software testing empirical research. We introduce the Software Engineering Experiments on the Web (SEEWEB) project, a Web site that is created to be a convenient and usable infrastructure for gathering, organizing, and distributing experimental software artifacts. A common problem in designing software engineering and software testing experiments is finding experimental artifacts that are appropriate for the experiment, convenient to gather and use, and will fit with other experimental artifacts. SEEWEB was initially funded by the NSF and is offered as a service to the community and provides access to experimental artifacts through an interface that allows browsing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "Using an existing suite of test objects: Experience from a testing experiment\n", "abstract": " This workshop paper presents lessons learned from a recent experiment to compare several test strategies. The test strategies were compared in terms of the number of tests needed to satisfy them and in terms of faults found. The experimental design and conduct are discussed, and frank assessments of the decisions that were made are provided. The paper closes with a summary of the lessons that were learned.", "num_citations": "4\n", "authors": ["53"]}
{"title": "Open source software engineering\n", "abstract": " The term Open Source Software (OSS) first appeared in 1998 in an effort to change the popular understanding of what had to that point been called \u0393\u00c7\u00a3free software\u0393\u00c7\u00a5-products like GNU/Linux, Apache, Perl and BIND. In the periods both immediately before and after the coining of the term, many insightful essays appeared both online and in print, attempting to better explain this novel approach to the problems of software development. However, as Eric S. Raymond, President of the Open Source Initiative, puts it,\u0393\u00c7\u00a3all the attempts at a really comprehensive description of the phenomenon had come from open-source hackers like myself, theorists operating from within the culture we were describing. We had the advantage of knowing our ground, but the disadvantage of knowing it perhaps too well--there are undoubtedly good questions we would never have thought to ask. That's why I've hoped from the beginning that an\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "Input validation testing: A requirements-driven, system level, early lifecycle technique\n", "abstract": " This paper addresses the problem of statically analyzing input command syntax as defined in interface and requirements specifications and then generating test cases for input validation testing. The IVT (Input Validation Testing) technique has been developed, a proof-of-concept tool (MICASA) has been implemented, and validation has been performed. Empirical validation on actual industrial software (for the Tomahawk Cruise Missile) shows that as compared with senior, experienced testers, MICASA found more requirement specification defects, generated test cases with higher syntactic coverage, and found additional defects. Additionally, the tool performed at significantly less cost.", "num_citations": "4\n", "authors": ["53"]}
{"title": "Version 3.0. Albert\u0393\u00c7\u00d6s User Guide\n", "abstract": " Albert is an interactive research tool to assist the specialist in the study of nonassociative algebra. This document serves as a technical guide to Albert. We refer the reader to [1] for a more casual tutorial. The main problem addressed by Albert is the recognition of polynomial identities. Roughly, Albert works in the following way. Suppose a user wishes to study alternative algebras. These are algebras dened by the two polynomial identities (yx) xy (xx) and (xx) yx (xy), known respectively as the right and left alternative laws. In particular, the user wishes to know if, in the presence of the right and left alternative laws,(a; b; c)[a; b] is also an identity. Here (a; b; c) denotes (ab) ca (bc),[a; b] denotes ab ba, and xy denotes xy+ yx. The user rst supplies Albert with the right and left alternative laws, using the identity command. Next, the user supplies the problem type. This refers to the number and degree of letters in the target\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["53"]}
{"title": "The Effectiveness of Category-Partition Testing of Object-Oriented Software\n", "abstract": " When migrating from conventional to object-oriented programming, developers face difficult decisions in modifying their development process to best use the new technology. In particular, ensuring that the software is highly reliable in this new environment poses different challenges. Developers need understanding of effective ways to test the software. This paper presents empirical data that show that the existing technique of category-partition testing can effectively find faults in object-oriented software, and new techniques are not necessarily needed. For this study, we identified types of faults that are common to C++ software and inserted faults of these types into two C++ programs. Test cases generated using the category-partition method were used to test the programs. A fault was considered detected if it caused the program to terminate abnormally or if the output was different from the output of the original program. The results show that the combination of the categ...", "num_citations": "4\n", "authors": ["53"]}
{"title": "What is the value of the peer\u0393\u00c7\u00c9reviewing system?\n", "abstract": " Scientists have been publishing research papers in scholarly journals since the 17th century [1]. And the peer review system, flawed as it is, has been used the entire time. I've written about peer reviewing before, including how to do it [2], why should we do it [3], how not to do it [4], and the benefits to reviewers [5]. Here I ask an existential question: What is the value of peer reviewing? As a journal editor, I see many papers that our readers never do. Papers that are plagiarised, papers that have no scientific content, papers that have very little scientific content, and papers that are written so badly they are incomprehensible. We desk\u0393\u00c7\u00c9reject almost half of STVR's submissions. We even have template emails for the different cases.I also see papers that look like good scientific papers, but are significantly flawed. The experiment was poorly designed, the ideas do not work, somebody else already had the same idea, or the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["53"]}
{"title": "Automatically Repairing SQL Faults\n", "abstract": " SQL is the standard database language, yet SQL statements can be complex and expensive to debug by hand. Automatic program repair techniques have the potential to reduce cost significantly. A previous attempt to repair SQL faults automatically used a decision tree (DT) algorithm that succeeded in some cases, but also generated many patches that passed the automated tests but that were not acceptable to the engineers. This paper proposes a novel fault localization and repair technique to repair faulty SQL statements. It targets faults in two common SQL constructs, JOIN and WHERE. It identifies the fault location and type precisely, and then creates a patch to fix the fault. We implemented this technique in a tool, and evaluated it on five medium to large-scale databases using 825 faulty queries with various complexity and faulty types. Experimental results showed that this technique can identify and repair JOIN\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["53"]}
{"title": "Globalization\u0393\u00c7\u00f6references and citations\n", "abstract": " This issue has three exciting papers that show how test tools can help in areas ranging from modelbased testing to embedded software to Web application software. The first, Tool Support for the Test Template Framework, by Cristi\u251c\u00ed, Albertengo, Frydman, Pl\u251c\u255dss, and Rodr\u251c\u00a1guez Monetti, describes a new tool to support model-based testing with Z specifications. Their tool, Fastest, is open-source and available online.(Recommended by Paul Strooper.) The second, Model Checking Trampoline OS: A Case Study on Safety Analysis for Automotive Software, by Choi, presents a study of the use of model checking to check safety properties in automotive operating systems. The author was able to find evidence of safety problems in the Trampoline operating system.(Recommended by", "num_citations": "3\n", "authors": ["53"]}
{"title": "Testing polymorphic relationships\n", "abstract": " As we move from developing procedure-oriented to object-oriented programs, the complexity traditionally found in functions and procedures is moving to the connections among components. Different faults occur when components are integrated to form higher level structures that aggregate the behavior and state of the components. Consequently, we need to place more effort on testing the connections among components. Although object-oriented technologies provide abstraction mechanisms for building components that can then be integrated to form applications, it also adds new compositional relations that can contain faults, which are most properly found during integration testing. This paper describes techniques for analyzing and testing the polymorphic relationships that occur in object-oriented software. The techniques adapt traditional data flow coverage criteria to consider definitions and uses among state variables of classes, particularly in the presence of inheritance and polymorphic overriding of state variables and methods. The application of these techniques can result in an increased ability to find faults and to create overall higher quality software.", "num_citations": "3\n", "authors": ["53"]}
{"title": "Designing an IT college\n", "abstract": " The University of the United Arab Emirates (UAEU) commissioned an international panel of experts to devise a model curriculum for their new College of Information Technology. The model has a university core in the freshman year, an IT core in the second year, and a professional concentration in one of seven degree programs in the third and fourth year. The seven degrees are computer science, computer systems engineering, software engineering, information systems, network engineering, information security, e-commerce, and educational technology. The curriculum is structured around a progressive-competence model, whose graduates meet recognised standards for entry-level professionals.", "num_citations": "3\n", "authors": ["53"]}
{"title": "TUMS: testing using mutant schemata\n", "abstract": " Mutation analysis is a way of quantifying the quality of a test set used in unit testing software. Unfortunately, mutation analysis performed using the conventional interpretive method is very slow. A new non-interpretive method, the MSG method, that has the promise of much higher performance was recently advanced. In this paper we describe TUMS, a prototype MSG-based implementation of a mutation analysis system. We also describe several experiments using this prototype that compare the performance of mutation analysis using the MSG method with mutation analysis using the conventional method. Our experiments strongly suggest that using the new MSG method is significantly faster than using the conventional method, with speed-ups as high as an order-of-magnitude observed.", "num_citations": "3\n", "authors": ["53"]}
{"title": "Efficiently Finding Data Flow Subsumptions\n", "abstract": " Data flow testing creates test requirements as definition-use (DU) associations, where a definition is a program location that assigns a value to a variable and a use is a location where that value is accessed. Data flow testing is expensive, largely because of the number of test requirements. Luckily, many DU-associations are redundant in the sense that if one test requirement (e.g., node, edge, DU-association) is covered, other DU-associations are guaranteed to also be covered. This relationship is called subsumption. Thus, testers can save resources by only covering DU-associations that are not subsumed by other testing requirements. Although this has the potential to significantly decrease the cost of data flow testing, finding subsumption among DU-associations is quite difficult. Previous solutions are costly and contain subtle flaws that sometimes lead to incorrect results. We model the data flow testing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "SiMut: exploring program similarity to support the cost reduction of mutation testing\n", "abstract": " Scientists have created many cost reduction techniques for mutation testing, and most of them reduce cost with minor losses of effectiveness. However, many of these techniques are difficult to generalize, difficult to scale, or both. Published results are usually limited to a modest collection of programs. Therefore, an open question is whether the results of a given cost reduction technique on programs studied in the paper will hold true for other programs. This paper introduces a conceptual framework, named SiMut, to support the cost reduction of mutation testing based on historical data and program similarity. Given a new, untested program u, the central idea is applying to u the same cost reduction strategy applied to a group G of programs that are similar to u and have already been tested with mutation, and check for consistency of results in terms of reduced costs and quality of test sets. SiMut includes activities to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "Experimental evaluation of redundancy in android mutation testing\n", "abstract": " Because of the widespread usage of Android devices, the Android ecosystem has the highest numbers of users, developers, and app downloads. Researchers find that many Android apps are not sufficiently tested, which may lead to crashes, incorrect behaviors, and security vulnerabilities. Mutation testing is a syntax-based software testing technique that is very effective at designing high-quality tests and evaluating pre-existing tests. Our prior research designed and implemented Android mutation testing technique, and then used experiments to assess its strength. However, the high computational cost of Android mutation testing possibly limits its industrial application. This paper presents an experimental evaluation that investigates redundant mutation operators in Android mutation analysis. While maintaining the test quality, our goal is to reduce the cost by excluding redundant mutation operators or improving\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "What is a facade journal?\n", "abstract": " Scientific journals have helped scientists advance research for centuries. The peer\u0393\u00c7\u00c9review system [1] lets journals filter for papers that advance our scientific knowledge and also helps authors improve their papers. This system has contributed to unbelievable scientific progress and incredible progress in human civilization.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Identifying useful mutants to test time properties\n", "abstract": " Real-time systems have to be verified and tested for timely behavior as well as functional behavior. Thus, time is an extra dimension that adds to the complexity of software testing. A timed automata model with a model-checker can be used to generate timed test traces. To properly test the timely behavior, the set of test traces should challenge the different time constraints in the model. This paper describes and adapts mutation operators that target such time constraints in timed automata models. Time mutation operators apply a delta to the time constraints to help testers design tests that exceed the time constraints. We suggest that the size of this delta determines how easy the mutant is to kill and that the optimal delta varies by the program, mutation operator, and the individual mutant. To avoid trivial and equivalent time mutants, the delta should be set individually for each mutant. We discuss mutant subsumption\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "How to revise a research paper\n", "abstract": " Ruiz and P\u251c\u2310rez, presents a way to integrate verification into a GUI during execution. The runtime verifier reads verification rules from files created by the engineers and checks the state of the GUI for violations while running (recommended by Peter Mueller). The second, Model-Based", "num_citations": "2\n", "authors": ["53"]}
{"title": "Who Is An Author?\n", "abstract": " Techniques and benefits, by Farjo, Assi, and Masri, presents results of analysis of execution profiles. They invented six ways to reduce the size of execution profiles and empirically measured the effect on the quality of analysis after reducing the profiles.(Recommended by TY Chen.) The third paper, Automated metamorphic testing of variability analysis tools, by Segura, Dur\u251c\u00edn, S\u251c\u00ednchez, Le Berre, Lonca, and Ruiz-Cort\u251c\u2310s, invent a technique for solving the oracle problem when testing tools that analyze the variability of software.(Recommended by TH Tse.) Combined, these three papers have a whopping 14 co-authors, which leads in perfectly to the subject of this editorial: determining authorship.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Globalization\u0393\u00c7\u00f4ethics and plagiarism\n", "abstract": " Compilation with Preservation of Structural Code Coverage Metrics to Support Software Testing, by Kirner and Hass, presents results on mapping coverage computed at the source level to coverage at the executable level. Their suggestion is to modify compilers to add additional information to the executable version of the software to make it possible to back-calculate coverage measured on the executable source.(Recommended by Jose Maldonado.) The second, A Hitchhiker\u0393\u00c7\u00d6s Guide to Statistical Tests for Assessing Randomized Algorithms in Software Engineering, by Arcuri and Briand, presents guidelines on using statistical tests in experiments involving randomized algorithms. The authors make the point that randomized algorithms have different characteristics than other kinds of experimental research, which means different statistical analysis techniques are needed. The paper analyzes several recent\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "A model IT curriculum for the UAE University\n", "abstract": " ObjectiveThe University of the United Arab Emirates (UAEU) is establishing a college of information technology, which they intend to be among the best in the world. To accomplish this, they have requested assistance in designing a forward-looking, innovative curriculum, and approaches to engaging with IT research and with professional and entrepreneurial practice.The IT curriculum task force is charged to produce an innovative model curriculum to serve as a model for the new IT college and to recommend ways of integrating curriculum with research and with professional practice. The curriculum model should incorporate innovations and best practices but need not be constrained by traditions that no longer make sense. The curricula of degree programs for which there is a duly constituted accreditation body should be accreditable. The College will set up external advisory panels to assist in the ongoing review of the other degree programs.", "num_citations": "2\n", "authors": ["53"]}
{"title": "An Evaluation of the Minimal-MUMCUT Logic Criterion and Prime Path Coverage.\n", "abstract": " This paper presents comparisons of the Minimal-MUMCUT logic criterion and prime path coverage. A theoretical comparison of the two criteria is performed in terms of (1) how well tests satisfying one criterion satisfy the other and (2) fault detection. We then compare the criteria experimentally. For 22 programs, we develop tests to satisfy Minimal-MUMCUT and prime path coverage. We use these tests in two separate experiments. First we measure the effectiveness of the tests developed for one criterion in terms of the other. Next we investigate the ability of the test sets to find actual faults. Faults are seeded via a mutation tool and then supplemented with mutants created by DNF logic mutation operators. We then measure the number of non-equivalent mutants killed by each test set. Results indicate that while prime path-adequate test sets are closer to satisfying Minimal-MUMCUT than vice versa, the criteria had similar fault detection and Minimal-MUMCUT required fewer tests.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Location Consistency model revisited: Problem, solution and prospects\n", "abstract": " Location consistency (LC) is a weak memory consistency model which is defined entirely on partial order execution semantics of parallel programs. Compared with sequential consistency (SC), LC is scalable and provides ample theoretical parallelism. This makes LC an interesting memory model in the upcoming many-core parallel processing era. Previous work has pointed out that LC does not guarantee SC execution behavior for all data race free programs. In this paper, we compare the semantics of LC with PRAM consistency and memory coherence, and prove that LC is strictly weaker than PRAM consistency. For data race free programs, we prove that the semantics of LC is equivalent to memory coherence. In addition, by introducing memory ordering semantics into LC judiciously, we prove that the enhanced model is equivalent to SC for data race free programs. Finally, we discuss possible solutions for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "Standards for reviewing papers\n", "abstract": " When I started submitting papers, I observed that reviewers were sometimes very unscientific in their evaluations. They assumed that authors intentionally wrote stupid things instead of simply making mistakes, assumed that something they didn\u0393\u00c7\u00d6t understand must be wrong, would criticize any paper that omitted references to their own work, and expected theoretical papers to be empirical and empirical papers to be theoretical.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Six issues in testing event-triggered real-time systems\n", "abstract": " Verification of real-time systems is a complex task, with problems coming from issues like concurrency. A previous paper suggested dealing with these problems by using a time-triggered design, which gives good support both for testing and formal analysis. However, a time-triggered solution is not always feasible and an event-triggered design is needed. Event-triggered systems are far more difficult to test than time-triggered systems.This paper revisits previously identified testing problems from a new perspective and identifies additional problems for event-triggered systems. The paper also presents an approach to deal with these problems. The TETReS project assumes a model-driven development process. We combine research within three different fields:(i) transformation of rule sets between timed automata specifications and ECA rules with maintained semantics,(ii) increasing testability in event-triggered system, and (iii) development of test case generation methods for event-triggered systems.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Using Combination Strategies for Software Testing in Practice-A proof-of-concept\n", "abstract": " In this thesis, the overall conclusion is that combination strategies,(ie, test case selection methods that manage the combinatorial explosion of possible things to test), can improve the software testing in most organizations. The research underlying this thesis emphasizes relevance by working in close relationship with industry.Input parameter models of test objects play a crucial role for combination strategies. These models consist of parameters with corresponding parameter values and represent the input space and possibly other properties, such as state, of the test object. Test case selection is then defined as the selection of combinations of parameter values from these models.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Test case generation for testing of timeliness: Extended version\n", "abstract": " Temporal correctness is crucial for real-time systems. There are few methods to test temporal correctness and most methods used in practice are ad-hoc. A problem with testing real-time applications is the response-time dependency on the execution order of concurrent tasks. Execution orders in turn depends on scheduling protocols, task execution times, and use of mutual exclusive resources apart from the points in time when stimuli is injected. Model-based mutation testing has previously been proposed to determine the execution orders that need to be tested to increase confidence in timeliness. An effective way to automatically generating such test cases for dynamic real-time systems is still needed. This paper presents a method using heuristic-driven simulation for generation of test cases.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Modeling and testing web-based applications\n", "abstract": " Modeling and Testing Web- based Applications Page 1 Common Ground Research Day 2002 November 6, 2002 Common Ground Research Day 2002 1 University Logo Modeling and Testing Webbased Applications Jeff Offutt George Mason University Joint work with Ye Wu November 6, 2002 J. Offutt, George Mason University 2 Thesis Statement \u0393\u00c7\u00f3 Web applications are heterogeneous, dynamic and must satisfy very high quality attributes \u0393\u00c7\u00f3 Use of the Web is hindered by low quality Web sites and applications \u0393\u00c7\u00f3 Web applications need to be built better and tested more Page 2 Common Ground Research Day 2002 November 6, 2002 Common Ground Research Day 2002 2 November 6, 2002 J. Offutt, George Mason University 3 Research in a Nutshell \u0393\u00c7\u00f3 Web applications control flow cannot be determined statically \u0393\u00c7\u00f3 Define atomic sections that have structural information and dynamic content \u0393\u00c7\u00f3 Model is composition of \u0393\u00c7\u00f3 , \u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "Applying a semantic fault model to the empirical study of corrective maintenance\n", "abstract": " A full understanding of the characteristics of faults is crucial to several important research areas in testing and software maintenance. Applicable maintenance research areas include change impact analysis, maintainability, regression testing, and comparative evaluation of maintenance techniques. We explore the fundamental nature of faults by looking at the differences between a syntactic and semantic characterization of faults. We offer definitions of these characteristics and explore the differentiation. We discuss the concept of \u0393\u00c7\u00a3size\u0393\u00c7\u00a5 of program faults. This model is then directly applied to changes in corrective maintenance. The measurement of fault size provides interesting and useful distinctions between the syntactic and semantic characterization of changes.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Mutation Testing Tool For Java\n", "abstract": " Mutation testing takes a different approach to testing by asking questions about the efficacy of test cases. The test cases are tested by introducing bugs in the code. The test cases are then run on the original program and all the mutants. The effectiveness of a test case is determined by the percentage of mutants it kills. This report describes a mutation tool for Java programs.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Integrating testing with the software development process\n", "abstract": " Software testing is usually postponed to the end of the development, after coding has started, or even after it has ended. By waiting until this late in the process, testing winds up being compressed {there are not enough resources (time and budget) remaining, problems with previous stages have been solved by taking time and dollars from testing, and there is insu cient time to adequately plan for testing. In this paper, we present an integrated approach to testing software, where testing activities begin as soon as development activities begin, and are carried out in parallel with the development activities. We present speci c activities {including planning, active testing, and development in uencing activities {that are associated with each of the traditional lifecycle phases. These activities can be carried out by the developers or separate test engineers, and can be associated with development activities within the con nes of any development process. These activities allows the tester to detect and prevent throughout the software development process, leading to more reliable and higher quality software.", "num_citations": "2\n", "authors": ["53"]}
{"title": "Unit Testing Versus Integration Testing\n", "abstract": " Industry typically leaves unit testing to the individual programmers, who are given little or no formal training or test tools. The few tools that are available are usually test support tools such as drivers or test case managers, rather than tools that solve the hard problems of generating test cases to satisfy formal criteria. In extreme cases, the testing problem is solved by a huge investment in labor. It is common, when developing mission critical software, to use as many as one tester for every programmer. Testing researchers, on the other hand, try to develop testing techniques that can be utilized to test a wide variety of software. Formal testing techniques such as statement coverage, branch coverage, mutation analysis, and data flow coverage were developed to test software units, and there is little or no reason to believe that they can be applied to complete programs.There are good reasons for both groups\u0393\u00c7\u00d6 view of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["53"]}
{"title": "A data flow analysis framework for data flow subsumption\n", "abstract": " Data flow testing creates test requirements as definition-use (DU) associations, where a definition is a program location that assigns a value to a variable and a use is a location where that value is accessed. Data flow testing is expensive, largely because of the number of test requirements. Luckily, many DU-associations are redundant in the sense that if one test requirement (e.g., node, edge, DU-association) is covered, other DU-associations are guaranteed to also be covered. This relationship is called subsumption. Thus, testers can save resources by only covering DU-associations that are not subsumed by other testing requirements. In this work, we formally describe the Data Flow Subsumption Framework (DSF) conceived to tackle the data flow subsumption problem. We show that DFS is a distributive data flow analysis framework which allows efficient iterative algorithms to find the Meet-Over-All-Paths (MOP) solution for DSF transfer functions. The MOP solution implies that the results at a point  are valid for all paths that reach . We also present an algorithm, called Subsumption Algorithm (SA), that uses DSF transfer functions and iterative algorithms to find the local DU-associations-node subsumption; that is, the set of DU-associations that are covered whenever a node  is toured by a test. A proof of SA's correctness is presented and its complexity is analyzed.", "num_citations": "1\n", "authors": ["53"]}
{"title": "How can we recognize facade journals?\n", "abstract": " This paper contains two terrific papers on testing. A hybrid approach to testing for nonfunctional faults in embedded systems using genetic algorithms, by Tingting Yu, Witawas Srisa-an, Myra B. Cohen, and Gregg Rothermel, investigates challenging aspects of testing for nonfunctional faults in embedded software.(Recommended by Gordon Fraser.) Approaches for computing test-case-aware covering arrays, by Ugur Koc and Cemal Yilmaz, presents several novel approaches to correctly compute test-case-aware covering arrays.(Recommended by Mauro Pezz\u251c\u00bf.) This editorial continues a series about what I call \u0393\u00c7\u00a3facade journals.\u0393\u00c7\u00a5 In the 28 (5) issue, I pointed out that peer reviews are the most important mechanism research journals use to ensure scientific quality of published papers [1]. Then in the 28 (6) issue, I defined \u0393\u00c7\u00a3facade journals\u0393\u00c7\u00a5 to be journals that publish papers that look like research but that do not truly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}
{"title": "Self\u0393\u00c7\u00c9plagiarism is not a thing\n", "abstract": " I've been ranting on ethics and plagiarism for awhile, both here and elsewhere [1, 2]. I have been plagiarised [3, 4], I have been accused of plagiarising (although I did not!), and I deal with plagiarism cases regularly as reviewer and editor. I've also been asked to speak about ethics in publishing to PhD students and young faculty at my university. This is a short rant in the other direction\u0393\u00c7\u00f6there is no such thing as self plagiarism! I recently joined a Facebook discussion on this topic when one of my colleagues complained about a reviewer who wanted to reject a paper because it contained \u0393\u00c7\u00a3self plagiarism.\u0393\u00c7\u00a5 Since I was originally educated as a mathematician, I did what all mathematicians do first: started with definitions. Below are three definitions for plagiarism (all properly quoted and with sources given, by the way):", "num_citations": "1\n", "authors": ["53"]}
{"title": "Reducing the Cost of Android Mutation Testing.\n", "abstract": " Due to the high market share of Android mobile devices, Android apps dominate the global market in terms of users, developers, and app releases. However, the quality of Android apps is a significant problem. Previously, we developed a mutation analysis-based approach to testing Android apps and showed it to be very effective. However, the computational cost of Android mutation testing is very high, possibly limiting its practical use. This paper presents a cost-reduction approach based on identifying redundancy among mutation operators used in Android mutation analysis. Excluding them can reduce cost without affecting the test quality. We consider a mutation operator to be redundant if tests designed to kill other types of mutants can also kill all or most of the mutants of this operator. We conducted an empirical study with selected open source Android apps. The results of our study show that three operators are redundant and can be excluded from Android mutation analysis. We also suggest updating one operator\u0393\u00c7\u00d6s implementation to stop generating trivial mutants. Additionally, we identity subsumption relationships among operators so that the operators subsumed by others can be skipped in Android mutation analysis.", "num_citations": "1\n", "authors": ["53"]}
{"title": "Do we need to teach ethics to PhD students?\n", "abstract": " This issue contains two excellent papers that address modern and important problems.\u0393\u00c7\u00a3Why Does the Orientation Change Mess Up My Android Application? From GUI Failures to Code Faults,\u0393\u00c7\u00a5 by Domenico Amalfitano, Vincenzo Riccio, Ana CR Paiva, and Anna Rita Fasolino, investigates the common problem of mobile apps that fail when we turn our devices sideways.(Recommended by Marcio Delamaro.)\u0393\u00c7\u00a3CoopREP: Cooperative Record and Replay of Concurrency Bugs,\u0393\u00c7\u00a5 by Nuno Machado, Paolo Romano, and Lu\u251c\u00a1s Rodrigues, presents a system that addresses the difficult problem of replicating concurrent faults, which often appear to be random.(Recommended by Paul Strooper.) I was in a faculty meeting last week where we were discussing a new course proposal on the research process for new PhD students. One topic in the proposal was integrity and ethics.", "num_citations": "1\n", "authors": ["53"]}
{"title": "Beware of predatory journals\n", "abstract": " This issue contains two papers. Debugging\u0393\u00c7\u00c9Workflow\u0393\u00c7\u00c9aware Software Reliability Growth Analysis, by Cinque, Cotroneo, Pecchia, Pietrantuono, and Russo, presents a reliability growth model that is based on data drawn from a project's bug tracking systems.(Recommended by Michael Lyu.) Prioritizing MCDC Test Cases by Spectral Analysis of Boolean Functions, by Ayav, proposes a method to prioritize the order in which we run tests by using an analysis based on logic test criteria (MCDC). This is less expensive than prioritization strategies based on mutation and does not lose significant accuracy.(Recommended by Shaoying Liu.)New journals have proliferated in recent years, as has the models for publication. When I was a student, we had about a dozen software engineering research journals, and all followed the same process. Authors submitted papers (on paper!), an editor sent copies to reviewers, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}
{"title": "How to extend a conference paper to a journal paper\n", "abstract": " In a previous editorial, I set out STVR\u0393\u00c7\u00d6s policy on extending conference papers to journal submissions [1]. The major requirements are that the journal version must have at least 30% new material, the journal version must include a citation to the conference paper, and the journal version must discuss the conference paper and summarize the new material. Here I suggest some practical guidelines for how to do the extension.", "num_citations": "1\n", "authors": ["53"]}
{"title": "STVR policy on extending conference papers to journal submissions\n", "abstract": " This issue presents three novel solutions to quite difficult problems. Stochastic modelling and simulation approaches to analysing enhanced fault tolerance on service-based software systems, by Kuan-Li Peng and Chin-Yu Huang, presents two approaches to improve the effectiveness of fault tolerant analysis.(Recommended by Min Xie.) Exhaustive test sets for algebraic specifications, by Marc Aiguier, Agn\u251c\u00bfs Arnould, Pascale Le Gall, and Delphine Longuet, presents a way to generate test sets from formal specifications that are not only complete, but exhaustive.(Recommended by Lu Zhang.) Using combinatorial testing to build navigation graphs for dynamic web applications, Wenhua Wang, Sreedevi Sampath, Yu Lei, Raghu Kacker, Richard Kuhn, and James Lawrence, presents a new technique to address the very difficult problem of finding navigation structures within web applications. This is difficult because\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}
{"title": "How to write an effective \u0393\u00c7\u00a3Response to Reviewers\u0393\u00c7\u00a5 letter\n", "abstract": " Czemerinski, Victor Braberman, and Sebastian Uchitel, invents new criteria to test whether APIs are used correctly. Specifically, the criteria measure the extent to which software that uses the APIs conforms to the expected protocol such as whether the methods are called in valid orders.(Recommended by Hasan Ural.) The third paper in this issue addresses the difficult oracle problem.", "num_citations": "1\n", "authors": ["53"]}
{"title": "An Evaluation of the Effectiveness of the Atomic Section Model\n", "abstract": " Society increasingly depends on web applications for business and pleasure. As the use of web applications continues to increase, the number of failures, some minor and some major, continues to grow. A significant problem is that we still have relatively weak abilities to test web applications. Traditional testing techniques do not adequately model or test these novel technologies. The atomic section model (ASM), models web applications to support design, analysis, and testing. This paper presents an empirical study to evaluate the effectiveness of the ASM. The model was implemented into a tool, WASP, which extracts the ASM from the implementation and supports various test criteria. We studied ten web applications, totaling 156 components and 11,829 lines of code. Using WASP, we generated 207 tests, which revealed 31 faults.Seventeen of those faults exposed internal information about the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}
{"title": "How to get your paper rejected from STVR\n", "abstract": " This editorial is based on a talk I gave at the ICST PhD symposium in April 2014. I had fun giving the talk and hope you enjoy reading this summary. First, I want to make it clear that I am highly qualified to give advice on getting papers rejected. I have well over 100 rejections in my time and may well be the most rejected software testing researcher of all time.", "num_citations": "1\n", "authors": ["53"]}
{"title": "What I have learned from usability\n", "abstract": " I have recently been working on a project about usable security. An interesting thing I have observed is the many similarities between usability and testing. To start with, they both address emergent properties; that is, they address aspects of the software that do not exist until the entire system is completed. Of course, we can (and should!) test software piecewise, but some faults are not visible until the system is entirely integrated, and we certainly cannot measure reliability until the entire system is present. Usability is even more emergent\u0393\u00c7\u00f6there is really no sign of usability until we get the user interface of the system in place. Many engineers and educators view usability and testing in the same way: as being unimportant to the real work of building the backend software. Or worse, usability and testing may get in the way of the real work.I think that the most important way they are similar is in their historical progression\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}
{"title": "The h\u0393\u00c7\u00c9index beats the impact factor\n", "abstract": " This issue has two papers with results on real industrial software projects. The first, On the testing of user-configurable software systems using firewalls, by Robinson and White, presents the \u0393\u00c7\u00ffjust-intime\u0393\u00c7\u00d6testing strategy for user-configurable software and includes results on commercial software.", "num_citations": "1\n", "authors": ["53"]}
{"title": "OOP and discrete math\n", "abstract": " This issue has three terrific papers. The first, Verification of real-time systems design, by M. Emilia Cambronero, Valent\u2500\u2592n Valero, and Gregorio D\u2500\u2592az, presents a way to verify aspects of software during design. The second, A systematic representation of path constraints for implicit path enumeration technique, by Tai Hyo Kim, Ho Jung Bang, and Sung Deok Cha, proposes to encode path constraints into \u0393\u00c7\u00ffflow facts\u0393\u00c7\u00d6 to improve the accuracy of implicit path enumeration. The third, Fault localization using a model checker, by Andreas Griesmayer, Stefan Staber, and Roderick Bloem, gives a method for using model checking information to not only identify software failures, but to help find where the corresponding faults may be located.I learned a new word last year. A retronym is a descriptive word that was not needed before a new word came into being. When we started writing object-oriented programs, we needed a new\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}
{"title": "Service Oriented Architecture Empirical Study.\n", "abstract": " Service Oriented Architecture (SOA) has been proposed as model for distributed software development that surpasses the traditional Distributed Object Architecture (DOA) practices in many areas. However, no empirical studies have been conducted to verify the claimed benefits. This study is a first attempt at presenting empirical evidence regarding the benefits of SOA. It is a comparison between traditional DOA and SOA. The two technologies were compared on the basis of code size and development time. The results show that, as a whole, the subject application was faster to develop using the SOA method. However, the application size was larger than that developed in the DOA method.", "num_citations": "1\n", "authors": ["53"]}
{"title": "Analysis for class-component testability.\n", "abstract": " Testability is a quality factor used to predict the amount of effort required for software testing and to indicate the difficulty of revealing faults. Also, it can be used to estimate the difficulty of software testing. This paper presents a quantitative testability analysis method for a software component that can be used when the source program is not available, but the bytecode is (as in Java. class files). This process analyzes the testability of each location to evaluate the component testability. The testability of a location is analyzed by computing the probability that the location will be executed and, if the location contains a fault, the execution will cause the fault to be revealed as a failure. This analysis process helps developers measure component testability and determine whether the component testability should be increased before the component is reused. In addition, low testability locations are identified. This paper uses an example to evaluate the ability of this process to indicate component testability.", "num_citations": "1\n", "authors": ["53"]}
{"title": "Test case generation for testing of timeliness\n", "abstract": " In complex distributed real-time systems the temporal correctness is imperative for dependability. Industrial practice has few methods for testing of temporal correctness and the methods that exist are often ad-hoc. A problem associated with testing real-time is that their timeliness depends on the execution order of tasks. This is particularly problematic for eventtriggered real-time systems where the system continuously is notified of events that influence the execution order. Further, real-time systems may behave differently depending not only on time dependencies in program logic but also on differences in execution times. This paper investigates how existing test-case generation methods take these factors into consideration and explains the opportunity to construct better methods and metrics for their evaluation. An analysis of current methods for generating test cases for testing temporal properties of real-time systems is presented. The methods are classified according to their characteristics and analyzed in relation to the requirements of an event-triggered system model. The aim of the classification is to detect similarities between different test-case generation methods so that the characteristics that have an impact on the applicability of the method when testing timeliness can be determined. We conclude that existing work only consider a subset of the factors that influence timeliness, and therefore propose research activities that take these factors into consideration during test-case generation.", "num_citations": "1\n", "authors": ["53"]}
{"title": "Bytecode-based analysis for increasing class-component testability\n", "abstract": " Software testing attempts to reveal software faults by executing the program on input values and comparing the outputs from the execution with expected outputs. Testing software is easier when testability is high, so increasing testability allows faults to be", "num_citations": "1\n", "authors": ["53"]}
{"title": "Modeling and Testing of Dynamic Integration Aspects of Web-based Applications\n", "abstract": " Web software applications have become complex, sophisticated programs that are based on novel computing technologies. Although powerful, these technologies bring new challenges to developers and testers. Checking static HTML links is no longer sufficient; web applications must be evaluated as complex software products. This paper focuses on two unique aspects of web applications, an extremely loose form of coupling and dynamic integration, and the ability that users have to directly change the potential flow of execution. Taken together, these allow the potential control flow to vary with each execution, and means the possible control flows cannot be determined statically. Thus we cannot perform standard analysis techniques that are fundamental to many software engineering activities. This paper presents a way to model the new couplings and the dynamic flow of control of web applications, supplying the equivalent of a control flow graph for web applications. The model is used to propose new test criteria for web applications, and results are shown from a case study on a moderate-size application.", "num_citations": "1\n", "authors": ["53"]}
{"title": "Maintaining knowledge currency in the 21st century\n", "abstract": " Software engineering is a rapidly changing discipline, and will continue to be so for the foreseeable future. This pace of change brings both problems and opportunities to universities that teach software engineering. Engineers ore no longer satisfied with one or two initial university education experiences, but by necessity are becoming lifetime learners, with frequent trips back to educational providers. This recurring education is needed to update engineers' knowledge with new ideas and concepts, and to update engineers' skills. In this paper, we take the position that universities can and should respond to this situation with a new model for graduate software engineering education, which we call professional currency certificates. These courses should offer the depth of knowledge and university academic credit that traditional academic courses offer, but with the convenience and practical nature of corporate\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}
{"title": "Integrating research, reuse, and integration into Software Engineering course\n", "abstract": " Integrating Research, Reuse, and Integration into Software Engineering Course | Proceedings of the SEI Conference on Software Engineering Education ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the SEI Conference on Software Engineering EducationIntegrating Research, Reuse, and Integration into Software Engineering Course Article Integrating Research, Reuse, and Integration into Software Engineering Course Share on Authors: Roland H. Untch View Profile , A. Jefferson Offutt View Profile Authors Info & Affiliations Proceedings of the SEI Conference on Software Engineering EducationJanuary :\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["53"]}