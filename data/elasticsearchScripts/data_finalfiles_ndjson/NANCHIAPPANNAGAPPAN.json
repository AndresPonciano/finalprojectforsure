{"title": "Understanding network failures in data centers: measurement, analysis, and implications\n", "abstract": " We present the first large-scale analysis of failures in a data center network. Through our analysis, we seek to answer several fundamental questions: which devices/links are most unreliable, what causes failures, how do failures impact network traffic and how effective is network redundancy? We answer these questions using multiple data sources commonly collected by network operators. The key findings of our study are that (1) data center networks show high reliability,(2) commodity switches such as ToRs and AggS are highly reliable,(3) load balancers dominate in terms of failure occurrences with many short-lived software related faults,(4) failures have potential to cause loss of many small packets such as keep alive messages and ACKs, and (5) network redundancy is only 40% effective in reducing the median impact of failure.", "num_citations": "857\n", "authors": ["223"]}
{"title": "Characterizing cloud computing hardware reliability\n", "abstract": " Modern day datacenters host hundreds of thousands of servers that coordinate tasks in order to deliver highly available cloud computing services. These servers consist of multiple hard disks, memory modules, network cards, processors etc., each of which while carefully engineered are capable of failing. While the probability of seeing any such failure in the lifetime (typically 3-5 years in industry) of a server can be somewhat small, these numbers get magnified across all devices hosted in a datacenter. At such a large scale, hardware component failure is the norm rather than an exception.", "num_citations": "608\n", "authors": ["223"]}
{"title": "Usage and perceptions of agile software development in an industrial context: An exploratory study\n", "abstract": " Agile development methodologies have been gaining acceptance in the mainstream software development community. While there are numerous studies of agile development in academic and educational settings, there has been little detailed reporting of the usage, penetration and success of agile methodologies in traditional, professional software development organizations. We report on the results of an empirical study conducted at Microsoft to learn about agile development and its perception by people in development, testing, and management. We found that one-third of the study respondents use agile methodologies to varying degrees, and most view it favorably due to improved communication between team members, quick releases and the increased flexibility of agile designs. The scrum variant of agile methodologies is by far the most popular at Microsoft. Our findings also indicate that developers are\u00a0\u2026", "num_citations": "339\n", "authors": ["223"]}
{"title": "Evaluating the efficacy of test-driven development: industrial case studies\n", "abstract": " This paper discusses software development using the Test Driven Development (TDD) methodology in two different environments (Windows and MSN divisions) at Microsoft. In both these case studies we measure the various context, product and outcome measures to compare and evaluate the efficacy of TDD. We observed a significant increase in quality of the code (greater than two times) for projects developed using TDD compared to similar projects developed in the same organization in a non-TDD fashion. The projects also took at least 15% extra upfront time for writing the tests. Additionally, the unit tests have served as auto documentation for the code when libraries/APIs had to be used as well as for code maintenance.", "num_citations": "234\n", "authors": ["223"]}
{"title": "Pair programming: what's in it for me?\n", "abstract": " Pair programming is a practice in which two programmers work collaboratively at one computer on the same design, algorithm, or code. Prior research on pair programming has primarily focused on its evaluation in academic settings. There has been limited evidence on the use, problems and benefits, partner selection, and the general perceptions towards pair programming in industrial settings. In this paper we report on a longitudinal evaluation of pair programming at Microsoft Corporation. We find from the results of a survey sent to a randomly selected 10% of engineers at Microsoft that 22% pair program or have pair programmed in the past. Using qualitative analysis, we performed a large-scale card sort to group the various benefits and problems of pair programming. The biggest perceived benefits of pair programming were the introduction of fewer bugs, spreading code understanding, and producing overall\u00a0\u2026", "num_citations": "163\n", "authors": ["223"]}
{"title": "Test coverage and post-verification defects: A multiple case study\n", "abstract": " Test coverage is a promising measure of test effectiveness and development organizations are interested in cost-effective levels of coverage that provide sufficient fault removal with contained testing effort. We have conducted a multiple-case study on two dissimilar industrial software projects to investigate if test coverage reflects test effectiveness and to find the relationship between test effort and the level of test coverage. We find that in both projects the increase in test coverage is associated with decrease in field reported problems when adjusted for the number of prerelease changes. A qualitative investigation revealed several potential explanations, including code complexity, developer experience, the type of functionality, and remote development teams. All these factors were related to the level of coverage and quality, with coverage having an effect even after these adjustments. We also find that the test effort\u00a0\u2026", "num_citations": "132\n", "authors": ["223"]}
{"title": "Global software development: Who does it?\n", "abstract": " In today's world, software development is increasingly spread across national and geographic boundaries. There is limited empirical evidence about the number and distribution of people in a large software company who have to deal with global software development (GSD). Is GSD restricted to a select few in a company? How many time zones do engineers have to deal with? Do managers have to deal with GSD more than individual engineers? What are the benefits and problems that engineers see with GSD? How have they tried to improve GSD coordination? These are interesting questions to be addressed in an empirical context. In this paper, we report on the results of a large-scale survey of software engineers at Microsoft Corporation. We found that a very high proportion of engineers are directly involved with GSD. In addition, more than 50% of the respondents regularly collaborate with people more than\u00a0\u2026", "num_citations": "82\n", "authors": ["223"]}
{"title": "Crane: Failure prediction, change analysis and test prioritization in practice--experiences from windows\n", "abstract": " Building large software systems is difficult. Maintaining large systems is equally hard. Making post-release changes requires not only thorough understanding of the architecture of a software component about to be changed but also its dependencies and interactions with other components in the system. Testing such changes in reasonable time and at a reasonable cost is a difficult problem as infinitely many test cases can be executed for any modification. It is important to obtain a risk assessment of impact of such post-release change fixes. Further, testing of such changes is complicated by the fact that they are applicable to hundreds of millions of users, even the smallest mistakes can translate to a very costly failure and re-work. There has been significant amount of research in the software engineering community on failure prediction, change analysis and test prioritization. Unfortunately, there is little evidence on\u00a0\u2026", "num_citations": "80\n", "authors": ["223"]}
{"title": "Codemine: Building a software development data analytics platform at microsoft\n", "abstract": " The scale and speed of today's software development efforts impose unprecedented constraints on the pace and quality of decisions made during planning, implementation, and postrelease maintenance and support for software. Decisions during the planning process include level of staffing and choosing a development model given the scope of a project and timelines. Tracking progress, course correcting, and identifying and mitigating risks are key in the development phase, as are monitoring aspects of and improving overall customer satisfaction in the maintenance and support phase. Availability of relevant data can greatly increase both the speed and likelihood of making a decision that leads to a successful software system. This article outlines the process Microsoft has gone through developing CODEMINE--a software development data analytics platform for collecting and analyzing engineering process data\u00a0\u2026", "num_citations": "73\n", "authors": ["223"]}
{"title": "Empirically detecting false test alarms using association rules\n", "abstract": " Applying code changes to software systems and testing these code changes can be a complex task that involves many different types of software testing strategies, e.g. system and integration tests. However, not all test failures reported during code integration are hinting towards code defects. Testing large systems such as the Microsoft Windows operating system requires complex test infrastructures, which may lead to test failures caused by faulty tests and test infrastructure issues. Such false test alarms are particular annoying as they raise engineer attention and require manual inspection without providing any benefit. The goal of this work is to use empirical data to minimize the number of false test alarms reported during system and integration testing. To achieve this goal, we use association rule learning to identify patterns among failing test steps that are typically for false test alarms and can be used to\u00a0\u2026", "num_citations": "68\n", "authors": ["223"]}
{"title": "Data analytics for game development: NIER track\n", "abstract": " The software engineering community has had seminal papers on data analysis for software productivity, quality, reliability, performance etc. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. Little work has been done on the emerging digital game industry. In this paper we explore how data can drive game design and production decisions in game development. We define a mixture of qualitative and quantitative data sources, broken down into three broad categories: internal testing, external testing, and subjective evaluations. We present preliminary results of a case study of how data collected from users of a released game can inform subsequent development.", "num_citations": "48\n", "authors": ["223"]}
{"title": "Who? where? what? examining distributed development in two large open source projects\n", "abstract": " To date, a large body of knowledge has been built up around understanding open source software development. However, there is limited research on examining levels of geographic and organizational distribution within open source software projects, despite many studies examining these same aspects in commercial contexts. We set out to fill this gap in OSS knowledge by manually collecting data for two large, mature, successful projects in an effort to assess how distributed they are, both geographically and organizationally. Both Firefox and Eclipse have been the subject of many studies and are ubiquitous in the areas of software development and internet usage respectively. We identified the top contributors that made 95% of the changes over multiple major releases of Firefox and Eclipse and determined their geographic locations and organizational affiliations. We examine the distribution in each project's\u00a0\u2026", "num_citations": "47\n", "authors": ["223"]}
{"title": "Toward a software testing and reliability early warning metric suite\n", "abstract": " The field reliability is measured too late for affordably guiding corrective action to improve the quality of the software. Software developers can benefit from an early warning of their reliability while they can still affordably react. This early warning can be built from a collection of internal metrics. An internal metric, such as the number of lines of code, is a measure derived from the product itself. An external measure is a measure of a product derived from assessment of the behavior of the system. For example, the number of defects found in test is an external measure. The ISO/IEC standard states that [i]nternal metrics are of little value unless there is evidence that they are related to external quality. Internal metrics can be collected in-process and more easily than external metrics. Additionally, internal metrics have been shown to be useful as early indicators of externally-visible product quality. For these early indicators\u00a0\u2026", "num_citations": "46\n", "authors": ["223"]}
{"title": "Transition from centralized to decentralized version control systems: A case study on reasons, barriers, and outcomes\n", "abstract": " In recent years, software development has started to transition from centralized version control systems (CVCSs) to decentralized version control systems (DVCSs). Although CVCSs and DVCSs have been studied extensively, there has been little research on the transition across these systems. This paper investigates the transition process, from the developer\u2019s view, in a large company. The paper captures the transition reasons, barriers, and outcomes through 10 developer interviews, and investigates these findings through a survey, participated by 70 developers. The paper identifies that the majority of the developers need to work incrementally and offline, and manage multiple contexts efficiently. DVCSs fulfill these developer needs; however the transition comes with a cost depending on the previous development workflow. The paper discusses the transition reasons, barriers and outcomes, and provides\u00a0\u2026", "num_citations": "43\n", "authors": ["223"]}
{"title": "A software testing and reliability early warning (STREW) metric suite\n", "abstract": " The demand for quality in software applications has grown, and awareness of software testing-related issues plays an important role towards that. Unfortunately in industrial practice, information on software field quality of a product tends to become available too late in the software lifecycle to affordably guide corrective actions. An important step towards remediation of this problem lies in the ability to provide an early estimation of post-release field quality.", "num_citations": "38\n", "authors": ["223"]}
{"title": "Empirical analysis of user data in game software development\n", "abstract": " For several years empirical studies have spanned the spectrum of research from software productivity, quality, reliability, performance to human computer interaction. Analyses have involved software systems ranging from desktop software to telecommunication switching systems. But surprising there has been little work done on the emerging digital game industry, one of the fastest growing domains today. To the best of our knowledge, our work is one of the first empirical analysis of a large commercially successful game system. In this paper, we introduce an analysis of the significant user data generated in the gaming industry by using a successful game: Project Gotham Racing 4. More specifically, due to the increasing ubiquity of constantly connected high-speed internet connections for game consoles, developers are able to collect extensive amounts of data about their games following release. The challenge\u00a0\u2026", "num_citations": "34\n", "authors": ["223"]}
{"title": "Characterizing the differences between pre-and post-release versions of software\n", "abstract": " Many software producers utilize beta programs to predict post-release quality and to ensure that their products meet quality expectations of users. Prior work indicates that software producers need to adjust predictions to account for usage environments and usage scenarios differences between beta populations and post-release populations. However, little is known about how usage characteristics relate to field quality and how usage characteristics differ between beta and post-release. In this study, we examine application crash, application hang, system crash, and usage information from millions of Windows\u00ae users to 1) examine the effects of usage characteristics differences on field quality (e.g. which usage characteristics impact quality), 2) examine usage characteristics differences between beta and post-release (e.g. do impactful usage characteristics differ), and 3) report experiences adjusting field quality\u00a0\u2026", "num_citations": "30\n", "authors": ["223"]}
{"title": "Programming element modification recommendation\n", "abstract": " Techniques described herein help determine dependencies and associations between CPEs in a computing system. These techniques track previous check-ins over a period of time in order to learn the dependencies and associations between CPEs. The previous check-ins are performed by a plurality of different computer programmers. In some embodiments, in response to receiving an indication that a CPE has either already been modified or is about to be modified by a computer programmer, the techniques provide the computer programmer with a recommendation indicating CPEs that are associated with the CPE being modified. This recommendation is based on the dependencies and associations determined from the previous check-ins performed by the plurality of different computer programmers.", "num_citations": "28\n", "authors": ["223"]}
{"title": "Code coverage and postrelease defects: A large-scale study on open source projects\n", "abstract": " Testing is a pivotal activity in ensuring the quality of software. Code coverage is a common metric used as a yardstick to measure the efficacy and adequacy of testing. However, does higher coverage actually lead to a decline in postrelease bugs? Do files that have higher test coverage actually have fewer bug reports? The direct relationship between code coverage and actual bug reports has not yet been analyzed via a comprehensive empirical study on real bugs. Past studies only involve a few software systems or artificially injected bugs (mutants). In this empirical study, we examine these questions in the context of open-source software projects based on their actual reported bugs. We analyze 100 large open-source Java projects and measure the code coverage of the test cases that come along with these projects. We collect real bugs logged in the issue tracking system after the release of the software and\u00a0\u2026", "num_citations": "25\n", "authors": ["223"]}
{"title": "Profiling the operational behavior of OS device drivers\n", "abstract": " As the complexity of modern Operating Systems (OS) increases, testing key OS components such as device drivers (DD) becomes increasingly complex given the multitude of possible DD interactions. Currently, DD testing entails a broad spectrum of techniques, where static (requiring source code) and dynamic (requiring the executable image) and static-dynamic testing combinations are employed. Despite the sustained and improving test efforts in the field of driver development, DDs still represent a significant cause of system outages as the coverage is invariably limited by test resources and release time considerations. The basic factor is the inability to exhaustively assess and then cover the operational states, leading to releases of inadequately tested DDs. Consequently, if representative operational activity profiles of DDs within an OS could be obtained, these could significantly improve the\u00a0\u2026", "num_citations": "23\n", "authors": ["223"]}
{"title": "Forking and the Sustainability of the Developer Community Participation--An Empirical Investigation on Outcomes and Reasons\n", "abstract": " A majority of OSS projects fails due to their inability to garner significant and sustained developer community participation. The problem proliferates when competing projects emerge from the source code of an existing project, a phenomenon called forking of the original project, claiming existing and potential developer community participation. In this study, we empirically analyze the influence of forking on the sustainability of the developer community participation in the original project. Further, we try to explain the observed behavior in terms of the characteristics of the project observed at the time of forking. A large-scale study of 2,217 projects hosted on GitHub shows that 1 in every 5 original projects observes a decline in the sustainability of the developer community participation after forking. We find that the negative effect is more pronounced in projects ported to GitHub from other platforms (\u2248 20%), compared to\u00a0\u2026", "num_citations": "22\n", "authors": ["223"]}
{"title": "Code dependency calculation\n", "abstract": " Generation of a dependency graph for code that includes code portions such as resources or functions or both. For some or all of the nodes, the dependency is calculated by determining that the given node, a depending node, depends on an affecting node. The dependency is recorded so as to be associated with the node. Furthermore, the dependency calculation method is recorded so as to be associated with the dependency. The code may perhaps include portions within two different domains, in which the mechanism for calculating dependencies may differ. In some cases, the dependency graph may be constructed in stages, and perhaps additional properties may be associated with the node, and metadata of the properties may also be recorded.", "num_citations": "20\n", "authors": ["223"]}
{"title": "Technologies for code failure proneness estimation\n", "abstract": " The present examples provide technologies for estimating code failure proneness probabilities for a code set and/or the files that make up the set. The code set being evaluated is typically comprised of binary and/or source files that embody the software for which the estimates are desired. The estimates are typically based on a set of selected code metrics, the code metrics typically selected based on corresponding failures of a previous version of the software. A historically variant metric feedback factor may also be calculated and code metric values classified relative to a baseline code set embodying the previous version of the software.", "num_citations": "20\n", "authors": ["223"]}
{"title": "On the personality traits of github contributors\n", "abstract": " People's personality has the potential to explain the behavior in different situations. This understanding of the behavior is required to understand the intricacies of the team, which can then be used to optimize work performance. As an initial step towards optimizing work performance, in this paper, we explore the inferential power of the personality traits in explaining the behavior of contributors in various contexts of software development in GitHub. Analyses of 243 actively discussed projects showed that the contributors with extreme (high or low) levels of contributions are more neurotic compared to the contributors with medium-level of contributions. Analyses of 423 active contributors showed that contributors evolve as more conscientious, more extrovert and less agreeable over the years of participation. The findings of this study match our ideas and are promising for further explorations.", "num_citations": "18\n", "authors": ["223"]}
{"title": "Automatically extracting coupling metrics from compiled code\n", "abstract": " Code coupling metrics are extracted from compiled code rather than from source code or software specifications. Examples of compiled code include binary machine code and machine-independent intermediate code that is convertible into binary machine code by a just-in-time compiler. The compiled code may be compiled from source code written in an object-oriented programming language, or from source code written in a procedural programming language, or from any combination thereof. A coupling metrics system includes a reader to access compiled code and its symbol table information, and a coupling metrics extraction component to calculate coupling metrics from the compiled code and its symbol table information. The coupling metrics system may be part of an integrated development environment (IDE) system.", "num_citations": "17\n", "authors": ["223"]}
{"title": "The impact of test ownership and team structure on the reliability and effectiveness of quality test runs\n", "abstract": " Context: Software testing is a crucial step in most software development processes. Testing software is a key component to manage and assess the risk of shipping quality products to customers. But testing is also an expensive process and changes to the system need to be tested thoroughly which may take time. Thus, the quality of a software product depends on the quality of its underlying testing process and on the effectiveness and reliability of individual test cases.Goal: In this paper, we investigate the impact of the organizational structure of test owners on the reliability and effectiveness of the corresponding test cases. Prior empirical research on organizational structure has focused only on developer activity. We expand the scope of empirical knowledge by assessing the impact of organizational structure on testing activities.Method: We performed an empirical study on the Windows build verification test suites\u00a0\u2026", "num_citations": "16\n", "authors": ["223"]}
{"title": "Predicting pull request completion time: a case study on large scale cloud services\n", "abstract": " Effort estimation models have been long studied in software engineering research. Effort estimation models help organizations and individuals plan and track progress of their software projects and individual tasks to help plan delivery milestones better. Towards this end, there is a large body of work that has been done on effort estimation for projects but little work on an individual checkin (Pull Request) level. In this paper we present a methodology that provides effort estimates on individual developer check-ins which is displayed to developers to help them track their work items. Given the cloud development infrastructure pervasive in companies, it has enabled us to deploy our Pull Request Lifetime prediction system to several thousand developers across multiple software families. We observe from our deployment that the pull request lifetime prediction system conservatively helps save 44.61% of the developer\u00a0\u2026", "num_citations": "15\n", "authors": ["223"]}
{"title": "Intermediate Code Metrics\n", "abstract": " Metrics may be determined from intermediate computer code by reading and analyzing an entire application using intermediate code, including any linked portions. The metrics may include cyclomatic complexity, estimated or actual number of lines of code, depth of inheritance, type coupling, and other metrics. The metrics may be combined into a quantifiable metric for the code.", "num_citations": "12\n", "authors": ["223"]}
{"title": "Building scalable failure-proneness models using complexity metrics for large scale software systems\n", "abstract": " Building statistical models for estimating failure-proneness of systems can help software organizations make early decisions on the quality of their systems. Such early estimates can be used to help inform decisions on testing, refactoring, code inspections, design rework etc. This paper demonstrates the efficacy of building scalable failure-proneness models based on code complexity metrics across the Microsoft Windows operating system code base. We show the ability of such models to estimate failure-proneness and provide feedback on the complexity metrics to help guide refactoring and the design rework effort.", "num_citations": "12\n", "authors": ["223"]}
{"title": "FastLane: Test minimization for rapidly deployed large-scale online services\n", "abstract": " Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases. This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that\u00a0\u2026", "num_citations": "6\n", "authors": ["223"]}
{"title": "Potential of Open Source Systems as Project Repositories for Empirical Studies Working Group Results\n", "abstract": " This paper is a report on the working group discussion on the potential of open source systems as project repositories for empirical studies. The discussions of this working group focused more generally on the typical questions that can be answered using the open source data, the challenging issues that can be addressed in future, and a discussion on the results to date.", "num_citations": "6\n", "authors": ["223"]}
{"title": "Method of detecting false test alarms using test step failure analysis\n", "abstract": " Identifying false test alarms to a developer. A code build is executed in a test system that includes computing functionality and computing infrastructure that is able to execute the build. Executing the code build includes running a plurality of system and integration tests on the code build. As a result of executing the code build, a system and integration test failure is identified. One or more characteristics of the system and integration test failure are identified. The characteristics of the system and integration test failure are compared to characteristics of a set of historical previous known false test alarms. False test alarms are failures caused by a factor other than a factor for which a test is being run. Based on the act of comparing, information is provided to a developer with respect to if the system and integration test failure is potentially a false test alarm.", "num_citations": "5\n", "authors": ["223"]}
{"title": "Transition from centralized to distributed vcs: A microsoft case study on reasons, barriers, and outcomes\n", "abstract": " In recent years, software development has started to transition from centralized version control systems (CVCSs) to decentralized version control systems (DVCSs). Although CVCSs and DVCSs have been studied extensively, there has been little research on the transition across these systems.This paper investigates the transition process, from the developer\u2019s view, in a large company. The paper captures the transition reasons, barriers, and outcomes through 10 developer interviews, and investigates these findings through a survey, participated by 70 developers. The paper identifies that the majority of the developers need to work incrementally and offline, and manage multiple contexts efficiently. DVCSs fulfill these developer needs; however the transition comes with a cost depending on the previous development workflow. The paper discusses the transition reasons, barriers and outcomes, and provides recommendations for teams planning such a transition. The paper shows that lightweight branches, and local and incremental commits were the main reasons for developers wanting to move to a DVCS. Further, the paper identifies the main problems with the transition process as: steep DVCS learning curve; incomplete DVCS integration with the rest of the development workflow; and DVCS scaling issues.", "num_citations": "5\n", "authors": ["223"]}
{"title": "Codemine: Building a software analytics platform for collecting and analyzing engineering process data at microsoft\n", "abstract": " The scale and speed of today\u2019s software development efforts imposes unprecedented constraints on the pace and quality of decisions made during planning, implementation, and post-release maintenance and support of software. Decisions during the planning process include, level of staffing and development model given the scope of a project and timelines. Tracking progress and course correcting, identifying and mitigating risks are the key in the development phase. As are monitoring aspects of and improving overall customer satisfaction in the maintenance and support phase. Availability of relevant data can greatly increase both the speed as well as likelihood of making a decision that leads to a successful software system.", "num_citations": "3\n", "authors": ["223"]}
{"title": "Coordination in Large-Scale So ware Development: Helpful and Unhelpful Behaviors\n", "abstract": " Large-scale software development requires coordination within and between very large engineering teams which may be located in different buildings, on different company campuses, and in different time zones. At Microsoft Corporation, we studied a 3-year-old, 300-person software application team based in Redmond, WA to learn how they coordinate with three intra-organization, physically distributed dependencies: a platform library team also in Redmond; a team three time zones away in Boston, MA; and a team in Hyderabad, India. Thirty-one interviews with 26 team members revealed that coordination was most impacted by issues of communication, capacity and cooperation. Distributed teams faced additional challenges due to time zone and cultural differences between the team members. We support our findings with a survey of 775 engineers across Microsoft who described their experiences managing coordination in their own software products. We suggest new processes and tools to improve team coordination.", "num_citations": "3\n", "authors": ["223"]}
{"title": "Empirical analyses of software contributor productivity\n", "abstract": " Software is everywhere and touches all aspects of human lives in one form or the other. The relevance of software in today\u2019s world can be simply gauged by its current and ever increasing market size. According to the industry analyst Gartner, worldwide software market size in 2016 is expected to be US $326 billion1 with a 5.3% growth from the previous year 2015. The relevance of the industry and its tremendous impact makes it important to optimize software productivity. Software productivity is impacted by three main dimensions-process, people and tools/technology [1]. Much of the earlier works on productivity improvement focused on process and tools/technology. Studies on process focused on individual processes [2], overall process of a project or an organization for improvements [3]. Studies on tools/technology evaluated the usefulness of tools/technology for improving software productivity [4]. Relatively less focus was given to people aspects. People are important assets in software projects as software productivity depends on the productivity of the contributors. This realization dates back to 1970s when Fred Brooks in \u2018The Mythical Man-Month\u2019argued that technical issues are subordinate to human issues while explaining the quality of software development [5]. Further, Gerald Weinberg in \u2018The Psychology of Computer Programming\u2019addressed \u201cprogramming as human performance and social activity\u201d[6]. However, there is not much information to explain how software contributors perform their tasks in workplace [7]. Recently, the studies in software engineering\u2013an intensely people-oriented activity, observed a rise in explaining software\u00a0\u2026", "num_citations": "2\n", "authors": ["223"]}
{"title": "Finding Dependencies from Defect History\n", "abstract": " Finding Dependencies from Defect History Page 1 Finding Dependencies from Defect History Rajiv Das, Wipro Technologies Jacek Czerwonka, Microsoft Corporation Nachiappan Nagappan, Microsoft Corporation Copyright ISSRE 2009 Page 2 Context \u2013 Windows Development \u2022 Size and scope \u2013 40+ MLOC \u2013 Development team spread all over the world \u2013 1B+ users \u2013 400,000 supported devices \u2013 6,000,000 apps running on Windows \u2013 Up to 10 years of servicing \u2022 Challenges \u2013 Large, complex codebase with millions of tests \u2013 Diverse customer base \u2013 Time and resource constraints \u2013 Diverse test execution \u2013 Very low tolerance to failure Page 3 Problems \u2022 Unknown Dependencies \u2013 Static, Dynamic Analysis does not find everything \u2022 Large number of Dependencies \u2013 How to prioritize Integration Testing when changes are routine and costs involved are high? Page 4 Motivation Graphics Driver crashes whenever \u2018\u2019 , \\\u2026", "num_citations": "2\n", "authors": ["223"]}
{"title": "Tempest: Towards early identification of failure-prone binaries\n", "abstract": " Early estimates of failure-proneness can be used to help inform decisions on testing, refactoring, design rework etc. Often such early estimates are based on code metrics like churn and complexity. But such estimates of software quality rarely make their way into a mainstream tool and find industrial deployment. In this paper we discuss about the Tempest tool that uses statistical failure-proneness models based on code complexity and churn metrics across the Microsoft Windows code base to identify failure-prone binaries early in the development process. We also present the tool architecture and its usage as of date at Microsoft.", "num_citations": "2\n", "authors": ["223"]}
{"title": "SoftNER: Mining Knowledge Graphs From Cloud Incidents\n", "abstract": " The move from boxed products to services and the widespread adoption of cloud computing has had a huge impact on the software development life cycle and DevOps processes. Particularly, incident management has become critical for developing and operating large-scale services. Prior work on incident management has heavily focused on the challenges with incident triaging and de-duplication. In this work, we address the fundamental problem of structured knowledge extraction from service incidents. We have built SoftNER, a framework for mining Knowledge Graphs from incident reports. First, we build a novel multi-task learning based BiLSTM-CRF model which leverages not just the semantic context but also the data-types for extracting factual information in the form of named entities. Next, we present an approach to mine relations between the named entities for automatically constructing knowledge graphs. We have deployed SoftNER at Microsoft, a major cloud service provider and have evaluated it on more than 2 months of cloud incidents. We show that the unsupervised machine learning pipeline has a high precision of 0.96. Our multi-task learning based deep learning model also outperforms the state-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER, we are able to build accurate models for applications such as incident triaging and recommending entities based on their relevance to incident titles.", "num_citations": "1\n", "authors": ["223"]}
{"title": "Summary of the National Research Council report on \u201creliability growth: enhancing Defense system reliability\u201d\n", "abstract": " This paper, extracted from National Research Council (2015), summarizes the findings and recommendations from a recent report from the Panel on Reliability Growth Methods for Defense Systems, operating under the auspices of the Committee on National Statistics (CNSTAT) within the National Research Council (NRC). The report offers recommendations to improve defense system reliability throughout the sequence of stages that comprise U.S. Department of Defense (DoD) acquisition processes \u2013 beginning with the articulation of requirements for new systems and ending with feedback mechanisms that document the reliability experience of deployed systems. A number of these recommendations are partially or fully embraced by current DoD directives and practice, particularly with the advent of recent DoD initiatives that elevate the importance of design for reliability techniques, reliability growth testing, and formal reliability growth modeling. The report supports the many recent steps taken by DoD, building on these while addressing associated engineering and statistical issues. The report provides a self-contained rendition of reliability enhancement proposals, recognizing that current DoD guides and directives have not been fully absorbed or consistently applied and are subject to change.", "num_citations": "1\n", "authors": ["223"]}
{"title": "On equivalence partitioning of code paths inside os kernel components\n", "abstract": " Commercial-off-the-shelf operating systems (COTS OSs) are increasingly chosen as key building blocks in embedded system design due to their rich feature-set available at low costs. Unfortunately, as the complexity of such OSs increases, testing key OS components such as device drivers (DD) to ensure continuous service provision becomes increasingly challenging. Despite the improving test efforts targeting DDs, they still represent a significant cause of system outages as the test coverage is invariably limited by the inability to exhaustively assess and cover the operational states. Consequently, if representative operational execution profiles of DDs within an OS could be obtained, these could significantly improve the understanding of the actual operational DD state space and help focus the test efforts onto the execution hotspots. Focusing on characterizing DD operational activities while assuming no access\u00a0\u2026", "num_citations": "1\n", "authors": ["223"]}
{"title": "OS Driver Test Effort Reduction via Operational Profiling\n", "abstract": " Operating Systems (OS\u2019s) constitute the operational core for computing devices, and consequently, the OS\u2019s ability to sustain operations determines the dependability level of the provided system services. In order to facilitate their applicability to a variety of hardware platforms, OS\u2019s have evolved into complex, componentized software entities whose key function is to provide applications access to the system\u2019s hardware resources. Within the OS, the key components dominating the cause of OS failures are the device drivers (DDs), precisely the OS parts designed to enhance the OS\u2019s support for hardware. Unfortunately, despite intensive efforts to elevate DD\u2019s robustness levels by employing varied test paradigms, the existing DDs still exhibit very high failure rates. Obviously, testing the complete state space of a DD is neither technically or economically viable. Based on our empirical DD evaluations, we conjecture that the testing deficiencies are the consequence of missing key parts of the DD\u2019s operational states in the process of testing, a situation illustrated in Fig. 1.", "num_citations": "1\n", "authors": ["223"]}