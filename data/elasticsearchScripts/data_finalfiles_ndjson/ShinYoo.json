{"title": "Regression testing minimization, selection and prioritization: a survey\n", "abstract": " Regression testing is a testing activity that is performed to provide confidence that changes do not harm the existing behaviour of the software. Test suites tend to grow in size as software evolves, often making it too costly to execute entire test suites. A number of different approaches have been studied to maximize the value of the accrued test suite: minimization, selection and prioritization. Test suite minimization seeks to eliminate redundant test cases in order to reduce the number of tests to run. Test case selection seeks to identify the test cases that are relevant to some set of recent changes. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. This paper surveys each area of minimization, selection and prioritization technique and discusses open problems and potential directions for future research. Copyright \u00a9 2010 John Wiley & Sons, Ltd.", "num_citations": "1515\n", "authors": ["546"]}
{"title": "Ask the mutants: Mutating faulty programs for fault localization\n", "abstract": " We present MUSE (MUtation-baSEd fault localization technique), a new fault localization technique based on mutation analysis. A key idea of MUSE is to identify a faulty statement by utilizing different characteristics of two groups of mutants-one that mutates a faulty statement and the other that mutates a correct statement. We also propose a new evaluation metric for fault localization techniques based on information theory, called Locality Information Loss (LIL): it can measure the aptitude of a localization technique for automated fault repair systems as well as human debuggers. The empirical evaluation using 14 faulty versions of the five real-world programs shows that MUSE localizes a fault after reviewing 7.4 statements on average, which is about 25 times more precise than the state-of-the-art SBFL technique Op2.", "num_citations": "188\n", "authors": ["546"]}
{"title": "Using hybrid algorithm for Pareto efficient multi-objective test suite minimisation\n", "abstract": " Test suite minimisation techniques seek to reduce the effort required for regression testing by selecting a subset of test suites. In previous work, the problem has been considered as a single-objective optimisation problem. However, real world regression testing can be a complex process in which multiple testing criteria and constraints are involved. This paper presents the concept of Pareto efficiency for the test suite minimisation problem. The Pareto-efficient approach is inherently capable of dealing with multiple objectives, providing the decision maker with a group of solutions that are not dominated by each other. The paper illustrates the benefits of Pareto efficient multi-objective test suite minimisation with empirical studies of two and three objective formulations, in which multiple objectives such as coverage and past fault-detection history are considered. The paper utilises a hybrid, multi-objective genetic\u00a0\u2026", "num_citations": "156\n", "authors": ["546"]}
{"title": "Evolving human competitive spectra-based fault localisation techniques\n", "abstract": " Spectra-Based Fault Localisation (SBFL) aims to assist debugging by applying risk evaluation formul\u00e6\u00a0(sometimes called suspiciousness metrics) to program spectra and ranking statements according to the predicted risk. Designing a risk evaluation formula is often an intuitive process done by human software engineer. This paper presents a Genetic Programming (GP) approach for evolving risk assessment formul\u00e6. The empirical evaluation using 92 faults from four Unix utilities produces promising results. Equations evolved by Genetic Programming can consistently outperform many of the human-designed formul\u00e6, such as Tarantula, Ochiai, Jaccard, Ample, and Wong1/2, up to 6 times. More importantly, they can perform equally as well as Op2, which was recently proved to be optimal against If-Then-Else-2 (ITE2) structure, or even outperform it against other program structures.", "num_citations": "111\n", "authors": ["546"]}
{"title": "Fault Localization Prioritization: Comparing Information Theoretic and Coverage Based Approaches\n", "abstract": " Test case prioritization techniques seek to maximize early fault detection. Fault localization seeks to use test cases already executed to help find the fault location. There is a natural interplay between the two techniques; once a fault is detected, we often switch focus to fault fixing, for which localization may be a first step. In this article we introduce the Fault Localization Prioritization (FLP) problem, which combines prioritization and localization. We evaluate three techniques: a novel FLP technique based on information theory, FLINT (Fault Localization using INformation Theory), that we introduce in this article, a standard Test Case Prioritization (TCP) technique, and a \u201ctest similarity technique\u201d used in previous work. Our evaluation uses five different releases of four software systems. The results indicate that FLP and TCP can statistically significantly reduce fault localization costs for 73% and 76% of cases\u00a0\u2026", "num_citations": "105\n", "authors": ["546"]}
{"title": "Highly scalable multi objective test suite minimisation using graphics cards\n", "abstract": " Despite claims of \u201cembarrassing parallelism\u201d for many optimisation algorithms, there has been very little work on exploiting parallelism as a route for SBSE scalability. This is an important oversight because scalability is so often a critical success factor for Software Engineering work. This paper shows how relatively inexpensive General Purpose computing on Graphical Processing Units (GPGPU) can be used to run suitably adapted optimisation algorithms, opening up the possibility of cheap scalability. The paper develops a search based optimisation approach for multi objective regression test optimisation, evaluating it on benchmark problems as well as larger real world problems. The results indicate that speed\u2013ups of over 25x are possible using widely available standard GPUs. It is also encouraging that the results reveal a statistically strong correlation between larger problem instances and the degree\u00a0\u2026", "num_citations": "84\n", "authors": ["546"]}
{"title": "Pandemic programming\n", "abstract": " MethodA questionnaire survey was created mainly from existing, validated scales and translated into 12 languages. The data was analyzed using non-parametric inferential statistics and structural equation modeling.ResultsThe questionnaire received 2225 usable responses from 53 countries. Factor analysis supported the validity of the scales and the structural model achieved a good fit (CFI= 0.961, RMSEA= 0.051, SRMR= 0.067). Confirmatory results include:(1) the pandemic has had a negative effect on developers\u2019 wellbeing and productivity;(2) productivity and wellbeing are closely related;(3) disaster preparedness, fear related to the pandemic and home office ergonomics all affect wellbeing or productivity. Exploratory analysis suggests that:(1) women, parents and people with disabilities may be disproportionately affected;(2) different people need different kinds of support.ConclusionsTo improve employee\u00a0\u2026", "num_citations": "77\n", "authors": ["546"]}
{"title": "Test data regeneration: Generating new test data from existing test data\n", "abstract": " Existing automated test data generation techniques tend to start from scratch, implicitly assuming that no pre\u2010existing test data are available. However, this assumption may not always hold, and where it does not, there may be a missed opportunity; perhaps the pre\u2010existing test cases could be used to assist the automated generation of additional test cases. This paper introduces search\u2010based test data regeneration, a technique that can generate additional test data from existing test data using a meta\u2010heuristic search algorithm. The proposed technique is compared to a widely studied test data generation approach in terms of both efficiency and effectiveness. The empirical evaluation shows that test data regeneration can be up to 2 orders of magnitude more efficient than existing test data generation techniques, while achieving comparable effectiveness in terms of structural coverage and mutation score\u00a0\u2026", "num_citations": "66\n", "authors": ["546"]}
{"title": "Faster fault finding at Google using multi objective regression test optimisation\n", "abstract": " Companies such as Google tend to develop products from one continually evolving core of code. Software is neither shipped, nor released in the traditional sense. It is simply made available, with dramatically compressed release cycles regression testing. This large scale rapid release environment creates challenges for the application of regression test optimisation techniques. This paper reports initial results from a partnership between Google and the CREST centre at UCL aimed at transferring techniques from the regression test optimisation literature into industrial practice. The results illustrate the industrial potential for these techniques: regression test time can be reduced by between 33%\u201382%, while retaining fault detection capability. Our experience also highlights the importance of a multi objective approach: optimising for coverage and time alone is insufficient; we have, at least, to additionally prioritise historical fault revelation.", "num_citations": "57\n", "authors": ["546"]}
{"title": "GPGPU test suite minimisation: search based software engineering performance improvement using graphics cards\n", "abstract": " It has often been claimed that SBSE uses so-called \u2018embarrassingly parallel\u2019 algorithms that will imbue SBSE applications with easy routes to dramatic performance improvements. However, despite recent advances in multicore computation, this claim remains largely theoretical; there are few reports of performance improvements using multicore SBSE. This paper shows how inexpensive General Purpose computing on Graphical Processing Units (GPGPU) can be used to massively parallelise suitably adapted SBSE algorithms, thereby making progress towards cheap, easy and useful SBSE parallelism. The paper presents results for three different algorithms: NSGA2, SPEA2, and the Two Archive Evolutionary Algorithm, all three of which are adapted for multi-objective regression test selection and minimization. The results show that all three algorithms achieved performance improvements up to 25 times\u00a0\u2026", "num_citations": "40\n", "authors": ["546"]}
{"title": "Metamorphic testing of stochastic optimisation\n", "abstract": " Testing stochastic optimisation algorithms presents an unique challenge because of two reasons. First, these algorithms are non-testable programs, i.e. if the test oracle was known, there wouldn't have been the need for those algorithms in the first place. Second, their performance can vary depending on the problem instances they are used to solve. This paper applies the statistical metamorphic testing approach to stochastic optimisation algorithms and investigates the impact that different problem instances have on testing optimisation algorithms. The paper presents an empirical evaluation of the approach using instances of Next Release Problem (NRP). The effectiveness of the testing method is evaluated using mutation testing. The result shows that, despite the challenges from the stochastic nature of the optimisation algorithm, metamorphic testing can be effective in testing them.", "num_citations": "30\n", "authors": ["546"]}
{"title": "A novel mask-coding representation for set cover problems with applications in test suite minimisation\n", "abstract": " Multi-Objective Set Cover problem forms the basis of many optimisation problems in software testing because the concept of code coverage is based on the set theory. This paper presents Mask-Coding, a novel representation of solutions for set cover optimisation problems that explores the problem space rather than the solution space. The new representation is empirically evaluated with set cover problems formulated from real code coverage data. The results show that Mask-Coding representation can improve both the convergence and diversity of the Pareto-efficient solution set of the multi-objective set cover optimisation.", "num_citations": "16\n", "authors": ["546"]}
{"title": "Precise Learn-to-Rank Fault Localization Using Dynamic and Static Features of Target Programs\n", "abstract": " Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information\u00a0\u2026", "num_citations": "14\n", "authors": ["546"]}
{"title": "PyGGI 2.0: language independent genetic improvement framework\n", "abstract": " PyGGI is a research tool for Genetic Improvement (GI), that is designed to be versatile and easy to use. We present version 2.0 of PyGGI, the main feature of which is an XML-based intermediate program representation. It allows users to easily define GI operators and algorithms that can be reused with multiple target languages. Using the new version of PyGGI, we present two case studies. First, we conduct an Automated Program Repair (APR) experiment with the QuixBugs benchmark, one that contains defective programs in both Python and Java. Second, we replicate an existing work on runtime improvement through program specialisation for the MiniSAT satisfiability solver. PyGGI 2.0 was able to generate a patch for a bug not previously fixed by any APR tool. It was also able to achieve 14% runtime improvement in the case of MiniSAT. The presented results show the applicability and the expressiveness of the\u00a0\u2026", "num_citations": "13\n", "authors": ["546"]}
{"title": "Test data augmentation: generating new test data from existing test data\n", "abstract": " Existing automated test data generation techniques tend to start from scratch, implicitly assuming no pre-existing test data are available. However, this assumption may not always hold, and where it does not, there may be a missed opportunity; perhaps the pre-existing test cases could be used to assist the automated generation of additional test cases. This paper introduces search-based test data augmentation, a technique that can generate additional test data from existing test data using a meta-heuristic search algorithm. The proposed technique is compared to a widely studied test data generation approach in terms of both efficiency and effectiveness. The empirical evaluation shows that test data augmentation can be up to two orders of magnitude more efficient than existing test data generation techniques, while achieving comparable effectiveness in terms of structural coverage and mutation score.", "num_citations": "13\n", "authors": ["546"]}
{"title": "Classifying False Positive Static Checker Alarms in Continuous Integration Using Convolutional Neural Networks\n", "abstract": " Static code analysis in Continuous Integration (CI) environment can significantly improve the quality of a software system because it enables early detection of defects without any test executions or user interactions. However, being a conservative over-approximation of system behaviours, static analysis also produces a large number of false positive alarms, identification of which takes up valuable developer time. We present an automated classifier based on Convolutional Neural Networks (CNNs). We hypothesise that many false positive alarms can be classified by identifying specific lexical patterns in the parts of the code that raised the alarm: human engineers adopt a similar tactic. We train a CNN based classifier to learn and detect these lexical patterns, using a total of about 10K historical static analysis alarms generated by six static analysis checkers for over 27 million LOC, and their labels assigned by actual\u00a0\u2026", "num_citations": "12\n", "authors": ["546"]}
{"title": "Generating test input with deep reinforcement learning\n", "abstract": " Test data generation is a tedious and laborious process. Search-based Software Testing (SBST) automatically generates test data optimising structural test criteria using metaheuristic algorithms. In essence, metaheuristic algorithms are systematic trial-and-error based on the feedback of fitness function. This is similar to an agent of reinforcement learning which iteratively decides an action based on the current state to maximise the cumulative reward. Inspired by this analogy, this paper investigates the feasibility of employing reinforcement learning in SBST to replace human designed metaheuristic algorithms. We reformulate the software under test (SUT) as an environment of reinforcement learning. At the same time, we present GunPowder, a novel framework for SBST which extends SUT to the environment. We train a Double Deep Q-Networks (DDQN) agent with deep neural network and evaluate the\u00a0\u2026", "num_citations": "11\n", "authors": ["546"]}
{"title": "Field Report: Applying Monte Carlo Tree Search for Program Synthesis\n", "abstract": " Program synthesis aims to automatically generate an executable segment of code that satisfies a given set of criteria. Genetic programming has been widely studied for program synthesis. However, it has drawbacks such as code bloats and the difficulty in finer control over the growth of programs. This paper explores the possibility of applying Monte Carlo Tree Search (MCTS) technique to general purpose program synthesis. The exploratory study applies MCTS to synthesis of six small benchmarks using Java Bytecode instructions, and compares the results to those of genetic programming. The paper discusses the major challenges and outlines the future work.", "num_citations": "8\n", "authors": ["546"]}
{"title": "Amortised Optimisation of Non-functional Properties in Production Environments\n", "abstract": " Search Based Software Engineering has high potential for optimising non-functional properties such as execution time or power consumption. However, many non-functional properties are dependent not only on the software system under consideration but also the environment that surrounds the system. This necessitates a support for online, in situ optimisation. This paper introduces the novel concept of amortised optimisation which allows such online optimisation. The paper also presents two case studies: one that seeks to optimise JIT compilation, and another to optimise a hardware dependent algorithm. The results show that, by using the open source libraries we provide, developers can improve the speed of their Python script by up\u00a0to 8.6\u00a0% with virtually no extra effort, and adapt a hardware dependent algorithm automatically for unseen CPUs.", "num_citations": "7\n", "authors": ["546"]}
{"title": "SBST in the Age of Machine Learning Systems-Challenges Ahead\n", "abstract": " Machine Learning, and especially Deep Neural Network (DNN), is being rapidly adopted by various software systems, including applications in safety-critical systems such as autonomous driving and medical imaging. This calls for an urgent need to test these AI/ML techniques as part of larger systems. However, this task can be very different from testing of traditional software systems. We will briefly examine the fundamentals of software testing as well as the state of the art in Search Based Software Testing (SBST), and try to outline the challenges ahead while highlighting areas where SBST can shine.", "num_citations": "6\n", "authors": ["546"]}
{"title": "Search-Based Approaches for Software Module Clustering Based on Multiple Relationship Factors\n", "abstract": " Software remodularization seeks to cluster software modules with high cohesion and low coupling: such a structure can help the comprehension and maintenance of complex systems. The modularization quality is usually captured using either structural, semantic, or history-based factors. All existing techniques apply a single factor to the entire system, which raises the following issues. First, a single factor may fail to capture the quality across the entire project: some modules may form semantic bondings, while others may form more structural ones. Second, the user of the technique has to choose a factor without knowing which one would perform the best. To resolve these issues, we propose a multi-factor module clustering, in which module clusters can be formed based on different factors. Our technique not only allows module clusters of different natures, but also relieve users from having to select a single factor\u00a0\u2026", "num_citations": "6\n", "authors": ["546"]}
{"title": "Evaluating Surprise Adequacy for Question Answering\n", "abstract": " With the wide and rapid adoption of Deep Neural Networks (DNNs) in various domains, an urgent need to validate their behaviour has risen, resulting in various test adequacy metrics for DNNs. One of the metrics, Surprise Adequacy (SA), aims to measure how surprising a new input is based on the similarity to the data used for training. While SA has been evaluated to be effective for image classifiers based on Convolutional Neural Networks (CNNs), it has not been studied for the Natural Language Processing (NLP) domain. This paper applies SA to NLP, in particular to the question answering task: the aim is to investigate whether SA correlates well with the correctness of answers. An empirical evaluation using the widely used Stanford Question Answering Dataset (SQuAD) shows that SA can work well as a test adequacy metric for the question answering task.", "num_citations": "5\n", "authors": ["546"]}
{"title": "Why train-and-select when you can use them all?: ensemble model for fault localisation\n", "abstract": " Learn-to-rank techniques have been successfully applied to fault localisation to produce ranking models that place faulty program elements at or near the top. Genetic Programming has been successfully used as a learning mechanism to produce highly effective ranking models for fault localisation. However, the inherent stochastic nature of GP forces its users to learn multiple ranking models and choose the best performing one for the actual use. This train-and-select approach means that the absolute majority of the computational resources that go into the evolution of ranking models are eventually wasted. We introduce Ensemble Model for Fault Localisation (EMF), which is a learn-to-rank fault localisation technique that utilises all trained models to improve the accuracy of localisation even further. EMF ranks program elements using a lightweight, voting-based ensemble of ranking models. We evaluate EMF using\u00a0\u2026", "num_citations": "5\n", "authors": ["546"]}
{"title": "Pareto Efficient Multi-Objective Regression Test Suite Prioritisation\n", "abstract": " Test suite prioritisation seeks a test case ordering that maximises the likelihood of early fault revelation. Previous prioritisation techniques have tended to be single objective, for which the additional greedy algorithm is the current state-of-the-art. We study multi objective test suite prioritisation, evaluating it on multiple versions of five widely-used benchmark programs and a much larger real world system of over 1MLoC. Our multi objective algorithms find faults significantly faster and with large effect size for 20 of the 22 versions. We also introduce a nonlossy coverage compact algorithm that dramatically scales the performance of all algorithms studied by between 2 and 4 orders of magnitude, making prioritisation practical for even very demanding problems.", "num_citations": "3\n", "authors": ["546"]}
{"title": "NIA3CIN: Non-Invasive Autonomous and Amortised Adaptivity Code Injection\n", "abstract": " Search Based Software Engineering has high potential for optimising non-functional properties such as execution time or power consumption. However, many non-functional properties are dependent not only on the software system under consideration but also the environment that surrounds the system. This results in two problems. First, systems optimised in offline environment may not perform as well online. Second, the system needs to be taken offline and re-optimised when the environment changes. This paper introduces the novel concept of amortised optimisation to solve both problems, and presents an open source implementation. We evaluate the framework to optimise block matrix multiplication algorithm.", "num_citations": "3\n", "authors": ["546"]}
{"title": "FLINT: Fault Localisation using Information Theory\n", "abstract": " Test case prioritisation techniques aim to maximise the chance of fault detection as early in testing as possible. This is most commonly achieved by prioritising the tests according to a surrogate measure that is thought to correspond to fault detection capabilities, such as code coverage. However, once the prioritised test suite indeed detects a fault, the original prioritisation may become obsolete. Rather, from the point of the first fault detection, the aim of the prioritisation should be that it should maximise the chance of locating the detected fault. This paper introduced a novel dynamic test prioritisation technique that is based on Shannon\u2019s entropy. Fault localisation is formulated as a process of decreasing the entropy calculated over the test information. The dynamic test case prioritisation uses both the coverage information from the previous testing and the results from the current testing and selects the next test that is most likely to reduce the entropy of information regarding the locality of the fault, maximising the chance of identifying the location of the fault whenever the testing is terminated.", "num_citations": "3\n", "authors": ["546"]}
{"title": "The use of a novel semi-exhaustive search algorithm for the analysis of data sensitivity in a feature subset selection problem\n", "abstract": " This project focuses on the issues of data sensitivity in the feature selection problem. While it is known that the meta heuristic search techniques can outperform the human experts significantly, the search results are based on the estimated data about the features. From the decision maker\u2019s point of view, it is therefore important to know how sensitive these estimations are to changes and possible errors. The project exploits the discreteness of the given data in such a way that the global optima can be identified, which enables us to measure the exact changes that any error in the estimation brings about. This method is presented as the semi-exhaustive search. Using this algorithm, the effects of the errors of varying degree in the initial estimation were measured. The observation from these experiments enables us to identify the parts of the data which are more sensitive to errors than others. In addition to the data sensitivity analysis, the algorithm was further developed so that it can consider the dependency relations between the software features.", "num_citations": "3\n", "authors": ["546"]}
{"title": "Embedding genetic improvement into programming languages\n", "abstract": " We present a vision of genetic improvement firmly embedded in, and supported by, programming languages. Genetic improvement has already been envisioned as the next compiler, which would take human written programs as input and return versions optimised with respect to various objectives. As an intermediate stage, or perhaps to complement the fully automated vision, we imagine genetic improvement processes that are hinted at and directed by humans but understood and undertaken by programming languages and their runtimes, via interactions through the source code. We examine existing similar ideas and examine the benefits of embedding them within programming languages.", "num_citations": "2\n", "authors": ["546"]}
{"title": "Search-Based Software Engineering: 6th International Symposium, SSBSE 2014, Fortaleza, Brazil, August 26-29, 2014, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 6th International Symposium on Search-Based Software Engineering, SSBSE 2014, held in Fortaleza, Brazil. The 14 revised full papers presented together with 2 keynote addresses, 1 invited talk, 1 short paper, 3 papers of the graduate track, and 4 challenge track papers were carefully reviewed and selected from 51 submissions. Search Based Software Engineering (SBSE) studies the application of meta-heuristic optimization techniques to various software engineering problems, ranging from requirements engineering to software testing and maintenance.", "num_citations": "2\n", "authors": ["546"]}
{"title": "Hybrid-MUSE: mutating faulty programs for precise fault localization\n", "abstract": " This paper presents Hybrid-MUSE, a new fault localization technique that combines MUtation-baSEd fault localization (MUSE) and Spectrum-Based Fault Localization (SBFL) technique. The core component of Hybrid-MUSE, MUSE, identifies a faulty statement by utilizing different characteristics of two groups of mutants\u2013one that mutates a faulty statement and the other that mutates a correct statement. This paper also proposes a new evaluation metric for fault localization techniques based on information theory, called Locality Information Loss (LIL): it can measure the aptitude of a localization technique for automated fault repair systems as well as human debuggers. The empirical evaluation using 51 faulty versions of the five real-world programs shows that Hybrid-MUSE outperforms the state-of-art fault localization technique significantly. For example, Hybrid-MUSE localizes a fault after reviewing 1.65% of executed statements on average, which is around 5.6 times more precise than the state-of-the-art SBFL technique, Op2.", "num_citations": "2\n", "authors": ["546"]}
{"title": "SBSE As Gaming\n", "abstract": " One way to understand meta-heuristic optimisation process is to look at it as an automated trial-and-error problem solving technique. This view provides interesting parallels to video gaming, which often requires the gamers to solve various problems using trial-and-error approaches. This paper considers the possibility of formulating optimisation problems in SBSE as video games and discusses challenges that are anticipated. The paper concludes with a demonstration of a proofof-concept implementation that casts test data generation problem as a spatial puzzle.", "num_citations": "1\n", "authors": ["546"]}