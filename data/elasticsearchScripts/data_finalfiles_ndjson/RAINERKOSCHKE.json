{"title": "Locating features in source code\n", "abstract": " Understanding the implementation of a certain feature of a system requires identification of the computational units of the system that contribute to this feature. In many cases, the mapping of features to the source code is poorly documented. In this paper, we present a semiautomatic technique that reconstructs the mapping for features that are triggered by the user and exhibit an observable behavior. The mapping is in general not injective; that is, a computational unit may contribute to several features. Our technique allows for the distinction between general and specific computational units with respect to a given set of features. For a set of features, it also identifies jointly and distinctly required computational units. The presented technique combines dynamic and static analyses to rapidly focus on the system's parts that relate to a specific set of features. Dynamic information is gathered based on a set of scenarios\u00a0\u2026", "num_citations": "713\n", "authors": ["266"]}
{"title": "Clone detection using abstract syntax suffix trees\n", "abstract": " Reusing software through copying and pasting is a continuous plague in software development despite the fact that it creates serious maintenance problems. Various techniques have been proposed to find duplicated redundant code (also known as software clones). A recent study has compared these techniques and shown that token-based clone detection based on suffix trees is extremely fast but yields clone candidates that are often no syntactic units. Current techniques based on abstract syntax trees-on the other hand-find syntactic clones but are considerably less efficient. This paper describes how we can make use of suffix trees to find clones in abstract syntax trees. This new approach is able to find syntactic clones in linear time and space. The paper reports the results of several large case studies in which we empirically compare the new technique to other techniques using the Bellon benchmark for clone\u00a0\u2026", "num_citations": "413\n", "authors": ["266"]}
{"title": "Survey of research on software clones\n", "abstract": " This report summarizes my overview talk on software clone detection research. It first discusses the notion of software redundancy, cloning, duplication, and similarity. Then, it describes various categorizations of clone types, empirical studies on the root causes for cloning, current opinions and wisdom of consequences of cloning, empirical studies on the evolution of clones, ways to remove, to avoid, and to detect them, empirical evaluations of existing automatic clone detector performance (such as recall, precision, time and space consumption) and their fitness for a particular purpose, benchmarks for clone detector evaluations, presentation issues, and last but not least application of clone detection in other related fields. After each summary of a subarea, I am listing open research questions.", "num_citations": "371\n", "authors": ["266"]}
{"title": "Software visualization in software maintenance, reverse engineering, and re\u2010engineering: a research survey\n", "abstract": " Software visualization is concerned with the static visualization as well as the animation of software artifacts, such as source code, executable programs, and the data they manipulate, and their attributes, such as size, complexity, or dependencies. Software visualization techniques are widely used in the areas of software maintenance, reverse engineering, and re\u2010engineering, where typically large amounts of complex data need to be understood and a high degree of interaction between software engineers and automatic analyses is required. This paper reports the results of a survey on the perspectives of 82 researchers in software maintenance, reverse engineering, and re\u2010engineering on software visualization. It describes to which degree the researchers are involved in software visualization themselves, what is visualized and how, whether animation is frequently used, whether the researchers believe\u00a0\u2026", "num_citations": "270\n", "authors": ["266"]}
{"title": "Incremental clone detection\n", "abstract": " Finding, understanding and managing software clones -- passages of duplicated source code - is of large interest in research and practice. There is an abundance of techniques to detect clones. However, all these techniques are limited to a single revision of a program. When the code changes, the analysis must be run again from scratch even though only small parts may have changed. In this paper, we present an incremental clone detection algorithm, which detects clones based on the results of the previous revision's analysis. Moreover, it creates a mapping between clones of one revision to the next, supplying information about the addition and deletion of clones. Our empirical results demonstrate that the incremental technique requires considerably less time than a non-incremental approach if the changes do not exceed a certain fraction of the source code. An incremental analysis is useful for on-the-fly\u00a0\u2026", "num_citations": "252\n", "authors": ["266"]}
{"title": "Atomic architectural component recovery for program understanding and evolution\n", "abstract": " Component recovery and remodularization is a means to get back control on large and complex legacy systems suffering from ad-hoc changes by recovering logical components and restructuring the physical components accordingly to decrease coupling among components and increase cohesion of components. This thesis is on unifying, quantitatively and qualitatively evaluating, improving, and integrating automatic and semi-automatic methods for component recovery.", "num_citations": "200\n", "authors": ["266"]}
{"title": "Aiding program comprehension by static and dynamic feature analysis\n", "abstract": " Understanding a system's implementation without prior knowledge is a hard task for reengineers in general. However, some degree of automatic aid is possible. The authors present a technique for building a mapping between the system's externally visible behavior and the relevant parts of the source code. The technique combines dynamic and static analyses to rapidly focus on the system's parts urgently required for a goal-directed process of program understanding.", "num_citations": "197\n", "authors": ["266"]}
{"title": "Effort-aware defect prediction models\n", "abstract": " Defect Prediction Models aim at identifying error-prone modules of a software system to guide quality assurance activities such as tests or code reviews. Such models have been actively researched for more than a decade, with more than 100 published research papers. However, most of the models proposed so far have assumed that the cost of applying quality assurance activities is the same for each module. In a recent paper, we have shown that this fact can be exploited by a trivial classifier ordering files just by their size: such a classifier performs surprisingly good, at least when effort is ignored during the evaluation. When effort is considered, many classifiers perform not significantly better than a random selection of modules. In this paper, we compare two different strategies to include treatment effort into the prediction process, and evaluate the predictive power of such models. Both models perform\u00a0\u2026", "num_citations": "184\n", "authors": ["266"]}
{"title": "Frequency and risks of changes to clones\n", "abstract": " Code Clones-duplicated source fragments-are said to increase maintenance effort and to facilitate problems caused by inconsistent changes to identical parts. While this is certainly true for some clones and certainly not true for others, it is unclear how many clones are real threats to the system's quality and need to be taken care of. Our analysis of clone evolution in mature software projects shows that most clones are rarely changed and the number of unintentional inconsistent changes to clones is small. We thus have to carefully select the clones to be managed to avoid unnecessary effort managing clones with no risk potential.", "num_citations": "172\n", "authors": ["266"]}
{"title": "Revisiting the evaluation of defect prediction models\n", "abstract": " Defect Prediction Models aim at identifying error-prone parts of a software system as early as possible. Many such models have been proposed, their evaluation, however, is still an open question, as recent publications show.", "num_citations": "170\n", "authors": ["266"]}
{"title": "Hierarchical Reflexion Models.\n", "abstract": " The reflexion model originally proposed by Murphy and Notkin allows one to structurally validate a descriptive or prescriptive architecture model against a source model. First, the entities in the source model are mapped onto the architectural model, then discrepancies between the architecture model and source model are computed automatically. The original reflexion model allows an analyst to specify only non-hierarchical architecture models, which is insufficient for larger systems that are decomposed into hierarchical subsystems. This paper extends the original reflexion model to hierarchical architecture models, describes a method to apply this technique, and reports on case studies conducted on two large-scale and complex applications (namely, the C compiler sdcc for 8-bit microprocessors and the GNU C compiler gcc).", "num_citations": "157\n", "authors": ["266"]}
{"title": "A framework for experimental evaluation of clustering techniques\n", "abstract": " Experimental evaluation of clustering techniques for component recovery is necessary in order to analyze their strengths and weaknesses in comparison to other techniques. For comparable evaluations of automatic clustering techniques, a common reference corpus of freely available systems is needed for which the actual components are known. The reference corpus is used to measure recall and precision of automatic techniques. For this measurement, a standard scheme for comparing the components recovered by a clustering technique to components in the reference corpus is required. This paper describes both the process of setting up reference corpora and ways of measuring recall and precision of automatic clustering techniques. For methods with human intervention, controlled experiments should be conducted. This paper additionally proposes a controlled experiment as a standard for evaluating\u00a0\u2026", "num_citations": "131\n", "authors": ["266"]}
{"title": "Finding components in a hierarchy of modules: a step towards architectural understanding\n", "abstract": " This paper presents a method to view a system as a hierarchy of modules according to information hiding concepts and to identify architectural component candidates in this hierarchy. The result of the method eases the understanding of a system's underlying software architecture. A prototype tool implementing this method was applied to three systems written in C (each over 30 Kloc). For one of these systems, an author of the system created an architectural description. The components generated by our method correspond to those of this architectural description in almost all cases. For the other two systems, most of the components resulting from the method correspond to meaningful system abstractions", "num_citations": "109\n", "authors": ["266"]}
{"title": "Empirical evaluation of clone detection using syntax suffix trees\n", "abstract": " Reusing software through copying and pasting is a continuous plague in software development despite the fact that it creates serious maintenance problems. Various techniques have been proposed to find duplicated redundant code (also known as software clones). A recent study has compared these techniques and shown that token-based clone detection based on suffix trees is fast but yields clone candidates that are often not syntactic units. Current techniques based on abstract syntax trees\u2014on the other hand\u2014find syntactic clones but are considerably less efficient. This paper describes how we can make use of suffix trees to find syntactic clones in abstract syntax trees. This new approach is able to find syntactic clones in linear time and space. The paper reports the results of a large case study in which we empirically compare the new technique to other techniques using the Bellon benchmark for\u00a0\u2026", "num_citations": "106\n", "authors": ["266"]}
{"title": "Frontiers of software clone management\n", "abstract": " Ad-hoc reuse through copy-and-paste occurs frequently in practice affecting the evolvability of software. This paper summarizes the state of the art in software clone management (detection, tracking, presentation, assessing, removal, changing) and the empirical knowledge we have gained so far. In the course of the summary, the paper identifies further research potential.", "num_citations": "89\n", "authors": ["266"]}
{"title": "An intermediate representation for integrating reverse engineering analyses\n", "abstract": " Intermediate representations (IR) are a key issue both for compilers as well as for reverse engineering tools to enable efficient analyses. Research in the field of compilers has proposed many sophisticated IRs that can be used in the domain of reverse engineering, especially in the case of deep analyses, but reverse engineering has also its own requirements for intermediate representations not covered by traditional compiler technology. This paper discusses requirements of IRs for reverse engineering. It shows then how most of these requirements can be met by extending and integrating existing IRs. These extensions include a generalized AST and a mechanism supporting multiple views on programs. Moreover the paper shows how these views can efficiently be implemented.", "num_citations": "89\n", "authors": ["266"]}
{"title": "Supporting the grow-and-prune model in software product lines evolution using clone detection\n", "abstract": " Software product lines (SPL) can be used to create and maintain different variants of software-intensive systems by explicitly managing variability. Often, SPLs are organized as an SPL core, common to all products, upon which product-specific components are built. Following the so called grow-and-prune model, SPLs may be evolved by copy&paste at large scale. New products are created from existing ones and existing products are enhanced with functionalities specific to other products by copying and pasting code between product-specific code. To regain control of this unmanaged growth, such code may be pruned, that is, identified and refactored into core components upon success. This paper describes tool support for the grow-and- prune model in the evolution of software product lines by identifying similar functions which can be moved to the core. These functions are identified in two steps. First, token\u00a0\u2026", "num_citations": "87\n", "authors": ["266"]}
{"title": "Large-scale inter-system clone detection using suffix trees\n", "abstract": " Detecting license violations of source code requires to compare a suspected system against a very large corpus of source code, for instance, the Debian source distribution. Thus, techniques detecting suspiciously similar code must scale in terms of resources needed. In addition to that, high precision of the detection is necessary because a human needs to inspect the results. The current approaches to address the resource challenge is to create an index for the corpus to which the suspected source code is compared. The index creation, however, is very costly. If the analysis is done only once, it may not be worth the effort. This paper demonstrates how suffix trees can be used to obtain a scalable comparison. Our evaluation shows that this approach is faster than current index-based techniques. In addition to that, this paper proposes a method to improve precision through user feedback and automated data mining.", "num_citations": "85\n", "authors": ["266"]}
{"title": "Extending the reflexion method for consolidating software variants into product lines\n", "abstract": " Software variants emerge from ad-hoc copying in-the-large with adaptations to a specific context. As the number of variants increases, maintaining such software variants becomes more and more difficult and expensive. In contrast to such ad-hoc reuse, software product lines offer organized ways of reuse, taking advantage of similarities of different products. To re-gain control, software variants may be consolidated as organized software product lines. In this paper, we describe a method and supporting tools to compare software variants at the architectural level extending the reflexion method to software variants. Murphy\u2019s reflexion method allows one to reconstruct the module view, a static architectural view describing the static components, their interfaces and dependencies and their grouping as layers and subsystems. The method consists of the specification of the module view and the mapping of\u00a0\u2026", "num_citations": "83\n", "authors": ["266"]}
{"title": "Equipping the reflexion method with automated clustering\n", "abstract": " A significant aspect in applying the reflexion method is the mapping of components found in the source code onto the conceptual components defined in the hypothesized architecture. To date, this mapping is established manually, which requires a lot of work for large software systems. In this paper, we present a new approach, in which clustering techniques are applied to support the user in the mapping activity. The result is a semi-automated mapping technique that accommodates the automatic clustering of the source model with the user's hypothesized knowledge about the system's architecture. This paper describes also a case study in which our semi-automated mapping technique has been applied successfully to extend a partial map of a real-world software application", "num_citations": "80\n", "authors": ["266"]}
{"title": "Identifying and removing software clones\n", "abstract": " Ad-hoc reuse through copy-and-paste occurs frequently in practice affecting the evolvability of software. Researchers have investigated ways to locate and remove duplicated code. Empirical studies have explored the root causes and effects of duplicated code and the evolution of duplicated code. This chapter summarizes the state of the art in detecting, managing, and removing software redundancy. It describes consequences, pros and cons of copying and pasting code.", "num_citations": "78\n", "authors": ["266"]}
{"title": "An evaluation of code similarity identification for the grow\u2010and\u2010prune model\n", "abstract": " In case new functionality is required, which is similar to the existing one, developers often copy the code that implements the existing functionality and adjust the copy to the new requirements. The result of the copying is code growth. If developers face maintenance problems, because of the need to make changes multiple times for the original and all its copies, they may decide to merge the original and its copies again; that is, they prune the code. This approach was named the grow\u2010and\u2010prune model by Faust and Verhoef. This paper describes tool support for the grow\u2010and\u2010prune model in the evolution of software by identifying similar functions that may be merged. These functions are identified in two steps. First, token\u2010based clone detection is used to detect pairs of functions sharing code. Second, Levenshtein distance (LD) measures the textual similarity among these functions. Sufficient similarity at function\u00a0\u2026", "num_citations": "76\n", "authors": ["266"]}
{"title": "Studying clone evolution using incremental clone detection\n", "abstract": " Finding, understanding and managing software clones\u2014passages of duplicated source code\u2014is of large interest in research and practice. Analyzing the evolution of clones across multiple versions of a program adds value to both applications. Although there is an abundance of techniques to detect clones, current approaches are limited to a single version of a program. The current techniques to track clones utilize these single\u2010version approaches and map clones of consecutive versions retroactively. This causes an unnecessary overhead in runtime and may lead to an incorrect mapping due to ambiguity. In this paper, we present an incremental clone detection algorithm, which detects clones based on the results of the previous version's analysis. It creates a mapping between clones of consecutive versions along with the detection. We evaluated our incremental approach regarding its advantage in runtime as\u00a0\u2026", "num_citations": "66\n", "authors": ["266"]}
{"title": "Feature-driven program understanding using concept analysis of execution traces\n", "abstract": " The first task of a programmer who wants to understand how a certain feature is implemented is to localize the implementation of the feature in the code. If the implementations of a set of related features are to be understood, a programmer is interested in their commonalities and variabilities. For large and badly documented programs, localizing features in code and identifying the commonalities and variabilities of components and features can be difficult and time-consuming. It is useful to derive this information automatically. The feature-component correspondence describes which components are needed to implement a set of features and what are the respective commonalities and variabilities of those features and components. This paper describes a new technique to derive the feature-component correspondence utilizing dynamic information and concept analysis. The method is simple to apply, cost-effective\u00a0\u2026", "num_citations": "66\n", "authors": ["266"]}
{"title": "Architecture reconstruction\n", "abstract": " Software architectures are described by different views which depend upon the concerns of the respective stakeholders. Far too often, software architectures are not documented sufficiently. In such cases, an architecture description must be reconstructed when changes to the system are to be made.               This article summarizes the current state of the art of techniques and methods for software architecture reconstruction and relates them to the viewpoints that have been proposed in architecture design. The article identifies research opportunities based on the comparison.", "num_citations": "59\n", "authors": ["266"]}
{"title": "Extending the reflexion method for consolidating software variants into product lines\n", "abstract": " Software variants emerge from ad-hoc copying in- the-large with adaptations to a specific context. As the number of variants increases, maintaining such software variants becomes more and more difficult and expensive. In contrast to such ad-hoc reuse, software product lines offer organized ways of reuse, taking advantage of similarities of different products. To re-gain control, software variants may be consolidated as organized software product lines. In this paper, we describe a method and supporting tools to compare software variants at the architectural level extending the reflexion method to software variants. Murphy's reflexion method allows one to reconstruct the module view, a static architectural view describing the static components, their interfaces and dependencies and their grouping as layers and subsystems. The method consists of the specification of the module view and the mapping of\u00a0\u2026", "num_citations": "58\n", "authors": ["266"]}
{"title": "A metric-based approach to detect abstract data types and state encapsulations\n", "abstract": " This article presents an approach to identify abstract data types (ADT) and abstract state encapsulations (ASE, also called abstract objects) in source code. This approach, named similarity clustering, groups together functions, types, and variables into ADT and ASE candidates according to the proportion of features they share. The set of features considered includes the context of these elements, the relationships to their environment, and informal information. A prototype tool has been implemented to support this approach. It has been applied to three C systems (each between 30\u201338 Kloc). The ADTs and ASEs identified by the approach are compared to those identified by software engineers who did not know the proposed approach or other automatic approaches. Within this case study, this approach has been shown to have a higher detection quality and to identify, in most of the cases, more ADTs and\u00a0\u2026", "num_citations": "54\n", "authors": ["266"]}
{"title": "Software visualization for reverse engineering\n", "abstract": " This article describes the Bauhaus tool suite as a concrete example for software visualization in reverse engineering, re-engineering, andsoft ware maintenance. Results from a recent survey on software visualization in these domains are reported. According to this survey, Bauhaus can indeed be considered a typical representative of these domains regarding the way software artifacts are visualized. Specific requirements for software visualizations are drawn from both the specific example and the survey.", "num_citations": "53\n", "authors": ["266"]}
{"title": "Automated clustering to support the reflexion method\n", "abstract": " A significant aspect in applying the Reflexion Method is the mapping of components found in the source code onto the conceptual components defined in the hypothesized architecture. To date, this mapping is established manually, which requires a lot of work for large software systems. In this paper, we present a new approach, in which clustering techniques are applied to support the user in the mapping activity. The result is a semi-automated mapping technique that accommodates the automatic clustering of the source model with the user\u2019s hypothesized knowledge about the system\u2019s architecture.This paper describes three case studies in which the semi-automated mapping technique, called HuGMe, has been applied successfully to extend a partial map of real-world software applications. In addition, the results of another case study from an earlier publication are summarized, which lead to comparable results\u00a0\u2026", "num_citations": "52\n", "authors": ["266"]}
{"title": "Automatically extracting threats from extended data flow diagrams\n", "abstract": " Architectural risk analysis is an important aspect of developing software that is free of security flaws. Knowledge on architectural flaws, however, is sparse, in particular in small or medium-sized enterprises. In this paper, we propose a practical approach to architectural risk analysis that leverages Microsoft\u2019s threat modeling. Our technique decouples the creation of a system\u2019s architecture from the process of detecting and collecting architectural flaws. This way, our approach allows an software architect to automatically detect vulnerabilities in software architectures by using a security knowledge base. We evaluated our approach with real-world case studies, focusing on logistics applications. The evaluation uncovered several flaws with a major impact on the security of the software.", "num_citations": "50\n", "authors": ["266"]}
{"title": "Derivation of feature component maps by means of concept analysis\n", "abstract": " Feature component maps describe which software components are needed to implement a particular feature, and they are used early in processes to develop a product line based on existing assets. This paper describes a new technique to derive the feature component map and additional dependencies utilizing dynamic information and concept analysis. The method is simple to apply, cost-effective, largely language-independent, and can yield results quickly and very early in the process.", "num_citations": "46\n", "authors": ["266"]}
{"title": "An assessment of type-3 clones as detected by state-of-the-art tools\n", "abstract": " Code reuse through copying and pasting leads to so-called software clones. These clones can be roughly categorized into identical fragments (type-1 clones), fragments with parameter substitution (type-2 clones), and similar fragments that differ through modified,deleted, or added statements (type-3 clones). Although there has been extensive research on detecting clones, detection of type-3 clones is still an open research issue due to the inherent vaguenessin their definition. In this paper, we analyze type-3 clones detected by state-of-the-art tools and investigate type-3 clones in terms of their syntactic differences. Then, we derive their underlying semantic abstractions from their syntactic differences. Finally, we investigate whether there are any additional code characteristics that indicate that a tool-suggested clone candidate is a real type-3 clone from a human's perspective. Our findings can help developers of\u00a0\u2026", "num_citations": "44\n", "authors": ["266"]}
{"title": "Reverse engineering variability in source code using clone detection: A case study for linux variants of consumer electronic devices\n", "abstract": " The Consumer Electronics Working Group (CEWG) in the Linux Foundation has identified several problems in the re-use process of embedded Linux software for consumer electronic devices. Among these is the increasing fragmentation of Linux derivatives. Vendors of electronic devices copy the Linux sources and make their modifications to adapt it to their own devices, but fail to back port their modifications to the mainstream Linux sources. Likewise, later improvements of the Linux sources are not integrated into the vendors' variants. CEWG launched the Long Term Support Initiative (LTSI) for an industry-managed tree of the Linux sources, maintained by CEWG, that is based on the long-term stable kernel tree annually updated with the latest mainstream kernel version to address their needs. In order to justify this initiative, CEWG asked us to investigate whether and if so how much non-upstream code can be\u00a0\u2026", "num_citations": "43\n", "authors": ["266"]}
{"title": "Organizing security patterns related to security and pattern recognition requirements\n", "abstract": " Software security is an emerging area in software development. More and more vulnerabilities are published and highlight the endangerment of systems. Hence, software designers and programmers are increasingly faced with the need to apply security solutions to software systems. Security patterns are best practices to handle recurring security problems. The abundance of documented security patterns calls for meaningful classifications to ease searching and assessing the right pattern for a security problem at hand. Existing classifications for security patterns consider only a small number of patterns and their purpose is often focused on implementation issues. Therefore, we identify missing aspects in existing classifications and the similarities between design and security pattern classifications. Based on that, we introduce two new classification schemes. The first is based on application domains formed by a literature survey on security patterns published in the period of 1997 to mid-2012 to cover the whole bandwidth of existing security patterns. The second is based on a subset of the collected patterns that are concerned with software and combines pattern-recognition needs and security aspects.", "num_citations": "41\n", "authors": ["266"]}
{"title": "An incremental semi-automatic method for component recovery\n", "abstract": " Atomic components are sets of related variables, types, and subprograms, e.g., abstract data types and objects. Many techniques exist to detect them automatically. However, as an evaluation has shown, none of them has the precision needed (J.F. Girand and R. Koshke, 1999). One approach to achieve a higher precision is to integrate the user into the detection cycle. The paper describes a method in which computer and human work together to find atomic components. Furthermore, it discusses how the techniques can be enhanced to work incrementally which is needed if they are to be integrated with this method. Moreover it proposes ways of combining the techniques within this interactive method.", "num_citations": "41\n", "authors": ["266"]}
{"title": "Large\u2010scale inter\u2010system clone detection using suffix trees and hashing\n", "abstract": " Detecting a similar code between two systems has various applications such as comparing two software variants or versions or finding potential license violations. Techniques detecting suspiciously similar code must scale in terms of resources needed to very large code corpora and need to have high precision because a human needs to inspect the results. This paper demonstrates how suffix trees can be used to obtain a scalable comparison. The evaluation is carried out for very large code corpora. Our evaluation shows that our approach is faster than index\u2010based techniques when the analysis is run only once. If the analysis is to be conducted multiple times, creating an index pays off. We report how much code can be filtered out from the analysis using an index\u2010based filter. In addition to that, this paper proposes a method to improve precision through user feedback. A user validates a sample of the found\u00a0\u2026", "num_citations": "39\n", "authors": ["266"]}
{"title": "Extracting and analyzing the implemented security architecture of business applications\n", "abstract": " Security is getting more and more important for the software development process as the advent of more complex, connected and extensible software entails new risks. In particular, multi-tier business applications, e.g., based on the Service-Oriented Architecture (SOA), are vulnerable to new attacks, which may endanger the business processes of an organization. These applications consist often of legacy code, which is now exported via Web Services, although it has originally been developed for internal use only. The last years showed great progress in the area of static code analysis for the detection of common low level security bugs, such as buffer overflows and cross-site scripting vulnerabilities. However, there is still a lack of tools that allow an analyst to assess the implemented security architecture of an application. In this paper, we propose a technique that automatically extracts the implemented security\u00a0\u2026", "num_citations": "38\n", "authors": ["266"]}
{"title": "A comparison of abstract data types and objects recovery techniques\n", "abstract": " In the context of the authors\u2019 research on architectural features recovery, abstract data types (ADT) and abstract data objects (ADO, also called objects) have been identified as two of the smallest components which are useful for building a significant architectural overview of the system. The authors have named these the atomic components (AC) of an architecture. This article compares six published techniques which extract ADTs and ADOs from source code without extensive data flow analysis. A prototype tool implementing each technique has been developed and applied to three medium-sized and systems written in C (each over 30 Kloc). The results from each approach are compared with the atomic components identified by hand by a group of software engineers. This article extends previous papers by discussing how the software engineers\u2019 AC identification was validated and by analyzing the false positives\u00a0\u2026", "num_citations": "36\n", "authors": ["266"]}
{"title": "Program complexity metrics and programmer opinions\n", "abstract": " Various program complexity measures have been proposed to assess maintainability. Only relatively few empirical studies have been conducted to back up these assessments through empirical evidence. Researchers have mostly conducted controlled experiments or correlated metrics with indirect maintainability indicators such as defects or change frequency. This paper uses a different approach. We investigate whether metrics agree with complexity as perceived by programmers. We show that, first, programmers' opinions are quite similar and, second, only few metrics and in only few cases reproduce complexity rankings similar to human raters. Data-flow metrics seem to better match the viewpoint of programmers than control-flow metrics, but even they are only loosely correlated. Moreover we show that a foolish metric has similar or sometimes even better correlation than other evaluated metrics, which raises\u00a0\u2026", "num_citations": "33\n", "authors": ["266"]}
{"title": "Static object trace extraction for programs with pointers\n", "abstract": " A trace is a record of the execution of a computer program, showing the sequence of operations executed. Dynamic traces are obtained by executing the program and depend upon the input. Static traces, on the other hand, describe potential sequences of operations extracted statically from the source code. Static traces offer the advantage that they do not depend upon input data.This paper describes a new automatic technique to extract static traces for individual stack and heap objects. The extracted static traces can be used in many ways, such as protocol recovery and validation in particular and program understanding in general.In addition, this article describes four case studies we conducted to explore the efficiency of our algorithm, the size of the resulting static traces, and the influence of the underlying points-to analysis on this size.", "num_citations": "33\n", "authors": ["266"]}
{"title": "Static trace extraction\n", "abstract": " A trace is a record of the execution of a computer program, showing the sequence of operations executed. Dynamic traces are obtained by executing the program and depend upon the input. Static traces, on the other hand, describe potential sequences of operations extracted statically from the source code. Static traces offer the advantage that they do not depend upon input data. This paper describes a new automatic technique to extract static traces for individual stack and heap objects. The extracted static traces can be used in many ways, such as protocol recovery and validation in particular and program understanding in general.", "num_citations": "33\n", "authors": ["266"]}
{"title": "An extended assessment of type-3 clones as detected by state-of-the-art tools\n", "abstract": " Code reuse through copying and pasting leads to so-called software clones. These clones can be roughly categorized into identical fragments (type-1 clones), fragments with parameter substitution (type-2 clones), and similar fragments that differ through modified, deleted, or added statements (type-3 clones). Although there has been extensive research on detecting clones, detection of type-3 clones is still an open research issue due to the inherent vagueness in their definition. In this paper, we analyze type-3 clones detected by state-of-the-art tools and investigate type-3 clones in terms of their syntactic differences. Then, we derive their underlying semantic abstractions from their syntactic differences. Finally, we investigate whether there are code characteristics that indicate that a tool-suggested clone candidate is a real type-3 clone from a human\u2019s perspective. Our findings can help developers of clone\u00a0\u2026", "num_citations": "30\n", "authors": ["266"]}
{"title": "Comparison of abstract data type and abstract state encapsulation detection techniques for architectural understanding\n", "abstract": " In the context of the authors' research on architectural feature recovery, abstract data type (ADT) and abstract state encapsulation (ASE, also called abstract object) have been identified as two of the smallest components which are useful to build a significant architectural overview of the system. The authors have named these the atomic components of an architecture. The paper compares five published techniques which extract ADT and ASE from source code. A prototype tool implementing each technique has been developed and applied to three medium-size systems written in C (each over 30 Kloc). The results from each approach are compared with the atomic components identified by hand by a group of software engineers. These people did not know the automatic techniques which were going to be applied to the systems.", "num_citations": "30\n", "authors": ["266"]}
{"title": "Incremental location of combined features for large-scale programs\n", "abstract": " The need for changing a program frequently confronts maintainers with the reality that no valid architectural description is at hand. To solve that problem, we presented at ICSM 2001 a language-independent and easy to use technique for opportunistic and demand driven location of features in source code based on static and dynamic analysis and concept analysis. In order to further validate the technique, we performed an industrial case study on a 1.2 million LOC production system. The experiences we made during that case study showed two problems of our approach: the growing complexity of concept lattices for large systems with many features and the need for handling compositions of features. This paper extends our technique to solve these problems. We show how this method allows incremental exploration of features while preserving the \"mental map\" the maintainer has gained through the analysis\u00a0\u2026", "num_citations": "29\n", "authors": ["266"]}
{"title": "WoSEF: Workshop on standard exchange format\n", "abstract": " A workshop was held at ICSE 2000 in Limerick, Ireland to further efforts in the development of a standard exchange format (SEF) for data extracted from and about source code. WoSEF (Workshop on Standard Exchange Format) brought together people with expertise in a variety of formats, such as RSF, TA, GraX, FAMIX, XML, and XMI, from across the software engineering discipline. We had five sessions consisting of a presentation and discussion period and a working session with three subgroups. The five sessions were: 1) Survey and Overview, 2) Language-level schemas and APIs, 3) High-level schemas, 4) MOF/XMI/UML and CDIF, and 5) Meta schemas and Typed Graphs. During that time we reviewed previous work and debated a number of important issues. This report includes descriptions of the presentations made during these sessions. The main result of the workshop is the agreement of the majority of\u00a0\u2026", "num_citations": "27\n", "authors": ["266"]}
{"title": "Evaluating defect prediction models for a large evolving software system\n", "abstract": " A plethora of defect prediction models has been proposed and empirically evaluated, often using standard classification performance measures. In this paper, we explore defect prediction models for a large, multi-release software system from the telecommunications domain. A history of roughly 3 years is analyzed to extract process and static code metrics that are used to build several defect prediction models with random forests. The performance of the resulting models is comparable to previously published work. Furthermore, we develop a new evaluation measure based on the comparison to an optimal model.", "num_citations": "25\n", "authors": ["266"]}
{"title": "Revisiting the \u0394IC approach to component recovery\n", "abstract": " Component recovery supports program understanding, architecture recovery, and re-use. Among the best known techniques for detection of re-usable objects (related global variables and their accessor functions) is \u0394 IC (the improvement in internal connectivity). This paper re-visits the original approach and extends it in different ways. It describes a variant of \u0394 IC suitable for reverse engineering that omits the slicing step of the original approach. The underlying metric of \u0394 IC is extended toward types integrating ideas of the Internal Access technique such that abstract data types can also be detected. Furthermore, the connectivity metric of \u0394 IC is combined with a cohesion metric based on vertex connectivity. The new metrics and the new algorithm for reverse engineering are evaluated and compared to other techniques quantitatively. The new contributions of this paper over the conference paper are the analysis of\u00a0\u2026", "num_citations": "25\n", "authors": ["266"]}
{"title": "Data exchange in Bauhaus\n", "abstract": " In the context of the Bauhaus project, reengineering environments to support program understanding of legacy code are being developed. Bauhaus defines two formats to represent information that has been extracted from source code. One of these formats, RG, is suitable as an exchange format. This paper introduces RG, describes how it is represented as an exchanged format, and discusses schema conversions in RG.", "num_citations": "24\n", "authors": ["266"]}
{"title": "Software-clone rates in open-source programs written in C or C++\n", "abstract": " It is often claimed that duplicated code, also known as software clones, occurs frequently. Different researchers have reported clone rates in the range of 19 and 28%, in extreme cases even 59% for particular systems. It is not clear, however, whether those systems are just outliers. In this paper, we analyze about 7,800 open-source projects written in C or C++, summing up to 240 MSLOC, and measure their clone rates. We use statistical analysis to estimate the means of clone rates in open-source projects. Based on our findings, we could not confirm the high clone rates reported in previous studies as expected averages. Except for small projects including a few copied and modified files, we found rather low clone rates compared to previous studies. For instance, if a minimal clone length of 100 tokens (roughly 16 LOC) is requested, we found an average rate of duplicated type-2 clones of about 12%. Fortype-1\u00a0\u2026", "num_citations": "23\n", "authors": ["266"]}
{"title": "Incremental reflexion analysis\n", "abstract": " Architecture conformance checking is implemented in many commercial and research tools. These tools typically implement the reflexion analysis originally proposed by Murphy, Notkin, and Sullivan. This analysis allows for structural validation of an architecture model against a source model connected by a mapping from source entities onto architecture entities. Given this mapping, the reflexion analysis computes the discrepancies between the architecture model and source model automatically. The mapping process is usually highly interactive and the most time\u2010consuming activity in the reflexion analysis. In current tools, the reflexion analysis must be repeated completely whenever the underlying source or architecture models or the mapping changes. In large systems, the recomputation can hinder interactive use as users expect an immediate response to their changes. This paper describes an incremental\u00a0\u2026", "num_citations": "21\n", "authors": ["266"]}
{"title": "The second international workshop on detection of software clones: workshop report\n", "abstract": " This report is intended to summarize the proceedings of the Second International Workshop on Detection of Software Clones (IWDSC'2003). The aim of the workshop was to bring together researchers within the field of clone detection to critically assess the current state of research, and to establish new directions and partnerships for research. There were at least 30 people in attendance. Five position papers were presented and discussed. In addition, an index card-based brainstorming technique was used to focus discussion on assessing the current state of clone analysis and detection. A report and analysis of the results of this brainstorming session is the main content of this report. We recommend another international workshop on clones and clone detection be set to be held alongside another conference sometime in 2004.", "num_citations": "20\n", "authors": ["266"]}
{"title": "An empirical study of clone removals\n", "abstract": " It is often claimed that duplicated source code is a threat to the maintainability of a software system and that developers should manage code duplication. A previous study analyzed the evolution of four software systems and found a remarkable discrepancy between code clones detected by a state-of-the-art clone detector and those deliberately removed by developers as the scope of the clones hardly ever matched. However, the results are based on a relatively small amount of data and need to be validated by a more extensive analysis. In this paper, we present an extension of this study by analyzing deliberate as well as accidental removals of code duplication in the evolution of eleven systems. Based on our findings, we could confirm the results of the previous study. Beyond that we found that accidental removals of cloned code occur slightly more often than deliberate removals and that many clone removals\u00a0\u2026", "num_citations": "19\n", "authors": ["266"]}
{"title": "A survey on goal-oriented visualization of clone data\n", "abstract": " Comprehending software clones is necessary for a number of activities in software development. The comprehension of software clones is challenged by the sheer volume of data and the complexity of the information content in that data. Visualization, or visual data analysis, takes advantage of human cognitive skills to discover unstructured insights from the visual presentations of complex and voluminous data. In this paper, we survey the existing literature on visualization of software clones. We gather the insights provided, and put that information in context of actual information needs systematically derived from the clone management goals. This framework allows us to better understand the role a visualization may play in achieving a specific user goal, identify potential gaps between existing types of visualization and information needs, and find complementary non-redundant subsets of visualizations for each\u00a0\u2026", "num_citations": "17\n", "authors": ["266"]}
{"title": "A controlled experiment on spatial orientation in vr-based software cities\n", "abstract": " Multiple authors have proposed a city metaphor for visualizing software. While early approaches have used three-dimensional rendering on standard two-dimensional displays, recently researchers have started to use head-mounted displays to visualize software cities in immersive virtual reality systems (IVRS). For IVRS of a higher order it is claimed that they offer a higher degree of engagement and immersion as well as more intuitive interaction. On the other hand, spatial orientation may be a challenge in IVRS as already reported by studies on the use of IVRS in domains outside of software engineering such as gaming, education, training, and mechanical engineering or maintenance tasks. This might be even more true for the city metaphor for visualizing software. Software is immaterial and, hence, has no natural appearance. Only a limited number of abstract aspects of software are mapped onto visual\u00a0\u2026", "num_citations": "16\n", "authors": ["266"]}
{"title": "Duplication, Redundancy, and Similarity in Software\n", "abstract": " CiNii \u8ad6\u6587 - Duplication, Redundancy, and Similarity in Software CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853 \u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3 \u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Duplication, Redundancy, and Similarity in Software KOSCHKE R. Eds. \u88ab \u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 KOSCHKE R. Eds. \u53ce\u9332\u520a\u884c\u7269 Dagstuhl Seminar Proc., 2007 Dagstuhl Seminar Proc., 2007, 2007 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30b3\u30fc\u30c9\u30af\u30ed\u30fc\u30f3\u691c\u51fa\u6280\u8853\u306e\u5c55\u958b \u795e\u8c37 \u5e74\u6d0b , \u80a5\u5f8c \u82b3\u6a39 , \u5409\u7530 \u5247\u88d5 \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2 28(3), 29-42, 2011-07-26 \u53c2\u8003\u6587\u732e121\u4ef6 \u88ab \u5f15\u7528\u6587\u732e1\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10029650952 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 Refer/\u3067| \u2026", "num_citations": "16\n", "authors": ["266"]}
{"title": "Revisiting the delta ic approach to component recovery\n", "abstract": " Component recovery supports program understanding, architecture recovery and reuse. Among the best known techniques for detection of reusable objects (related global variables and their accessor functions) is Delta-IC (Canfora et al., 1996). This paper re-visits the original approach and extends it in different ways. It describes a variant of Delta-IC suitable for reverse engineering that omits the slicing step of the original approach. The underlying metric of Delta-IC is extended toward types integrating ideas of the Internal Access technique (Yeh et al., 1995) such that abstract data types can also be detected. Furthermore, the connectivity metric of Delta-IC is combined with a cohesion metric based on vertex connectivity. The new metrics and the new algorithm for reverse engineering are quantitatively evaluated using the framework proposed in (Koschke and Eisenbarth, 2000) as a standard evaluation of clustering\u00a0\u2026", "num_citations": "16\n", "authors": ["266"]}
{"title": "Analyzing xfig using the Bauhaus tool\n", "abstract": " The Bauhaus project researches reengineering techniques to help program understanding of legacy code. Part of the program understanding task is the discovery of the system's architecture, which consists of components, subsystems, connectors, and constraints. Information about the system is exclusively extracted from the source code (because this is the only reliable source of information), in a semi-automatic way that involves the Bauhaus user (presumably a software maintainer or auditor). The authors used the Bauhaus tool to analyze the xfig program. The architecture of xfig was recovered and all of the (hypothetical) maintenance tasks described in the Developer Handbook were performed.", "num_citations": "16\n", "authors": ["266"]}
{"title": "What architects should know about reverse engineering and rengineering\n", "abstract": " Architecture reconstruction is a form of reverse engineering that reconstructs architectural views from an existing system. It is often necessary because a complete and authentic architectural description is not available. This paper puts forward the goals of architecture reconstruction, revisits the technical difficulties we are facing in architecture reconstruction, and presents a summary of a literature survey about the types of architectural viewpoints addressed in reverse engineering research.", "num_citations": "15\n", "authors": ["266"]}
{"title": "Approximate code search in program histories\n", "abstract": " Very often a defect must be corrected not only in the current version of a program at one particular place but in many places and many other versions -- possibly even in different development branches. Consequently, we need a technique to efficiently locate all approximate matches of an arbitrary defective code fragment in the program's history as they may need to be fixed as well. This paper presents an approximate whole-program code search in multiple releases and branches. We evaluate this technique for real-world defects of various large and realistic programs having multiple releases and branches. We report runtime measurements and recall using varying levels of allowable differences of the approximate search.", "num_citations": "13\n", "authors": ["266"]}
{"title": "Rekonstruktion von software-architekturen\n", "abstract": " Software-Architekturen werden aus verschiedenen Sichten beschrieben, die abh\u00e4ngig von den Anliegen sind, die mit der Beschreibung verfolgt werden. In vielen F\u00e4llen ist die Software-Architektur eines Systems nicht ausreichend beschrieben. Sichten m\u00fcssen dann rekonstruiert werden, um f\u00fcr Planung von \u00c4nderungen zur Verf\u00fcgung zu stehen.               Dieser Artikel fasst die in der wissenschaftlichen Literatur vorgeschlagenen Techniken zur Rekonstruktion von Software-Architektursichten zusammen und stellt sie den Sichten des Architekturentwurfs gegen\u00fcber. Anhand der Gegen\u00fcberstellung identifiziert er Forschungsbedarf.", "num_citations": "13\n", "authors": ["266"]}
{"title": "Application-domain classification for security patterns\n", "abstract": " Security patterns are best practices to handle recurring security problems. Existing classifications for security patterns consider only a small number of patterns, and their purpose is often focused on implementations issues. Therefore we identify missing aspects in existing classifications and introduce a new classification scheme based on application domains. This scheme is based on a literature survey on security patterns published in the period of 1997 to 2010 to cover the whole bandwidth of exiting security pattern.", "num_citations": "10\n", "authors": ["266"]}
{"title": "Encapsulating targeted component abstractions using software reflexion modelling\n", "abstract": " Design abstractions such as components, modules, subsystems or packages are often not made explicit in the implementation of legacy systems. Indeed, often the abstractions that are made explicit turn out to be inappropriate for future evolution agendas. This can make the maintenance, evolution and refactoring of these systems difficult. In this publication, we carry out a fine\u2010grained evaluation of Reflexion Modelling as a technique for encapsulating user\u2010targeted components. This process is a prelude to component recovery, reuse and refactoring. The evaluation takes the form of two in vivo case studies, where two professional software developers encapsulate components in a large, commercial software system. The studies demonstrate the validity of this approach and offer several best\u2010use guidelines. Specifically, they argue that users benefit from having a strong mental model of the system in advance of\u00a0\u2026", "num_citations": "10\n", "authors": ["266"]}
{"title": "Ans\u00e4tze des Programmverstehens\n", "abstract": " Programmverstehen ist der Proze\u00df des Wissenserwerbs \u00fcber ein Computerprogramm. Es ist die Voraussetzung f\u00fcr Fehlersuche, Erweiterung, Wiederverwendung und Dokumentation. Eine Reihe von Ans\u00e4tzen versucht, das Programmverstehen maschinell zu unterst\u00fctzen. Die gegenw\u00e4rtigen Ans\u00e4tze werden in diesem Artikel klassifiziert in grundlegende und wissensbasierte Analysen. Grundlegende Analysen verfugen \u00fcber kein Wissen \u00fcber die Anwendung und allgemeine Programmierung; sie basieren lediglich auf Programmiersprachensyntax und -Semantik. Grundlegende Analysen lassen sich weiter gliedern in grundlegende statische und grundlegende dynamische Analysen, abh\u00e4ngig davon, ob sie zur \u00dcbersetzungsszeit oder zur Laufzeit vorgenommen werden. Wissensbasierte Analysen verf\u00fcgen \u00fcber Anwendungswissen und allgemeines Programmierwissen. Wissensbasierte Analysen lassen\u00a0\u2026", "num_citations": "10\n", "authors": ["266"]}
{"title": "Comparing the EvoStreets Visualization Technique in Two-and Three-Dimensional Environments A Controlled Experiment\n", "abstract": " Analyzing and maintaining large software systems is a challenging task due to the sheer amount of information contained therein. To overcome this problem, Steinbr\u00fcckner developed a visualization technique named EvoStreets. Utilizing the city metaphor, EvoStreets are well suited to visualize the hierarchical structure of a software as well as hotspots regarding certain aspects. Early implementations of this approach use three-dimensional rendering on regular two-dimensional displays. Recently, though, researchers have begun to visualize EvoStreets in virtual reality using head-mounted displays, claiming that this environment enhances user experience. Yet, there is little research on comparing the differences of EvoStreets visualized in virtual reality with EvoStreets visualized in conventional environments. This paper presents a controlled experiment, involving 34 participants, in which we compared the\u00a0\u2026", "num_citations": "8\n", "authors": ["266"]}
{"title": "A formal method for the analysis of product maps\n", "abstract": " During the initiation and evolution of a software product line, developers make use of product maps [3] for scoping and requirements engineering. We present how to utilize a formal mathematical method for analyzing the product maps for an anticipated software product line. The result of the method reveals variabilities and commonalities of products, as well as dependencies between products.", "num_citations": "8\n", "authors": ["266"]}
{"title": "Zehn Jahre WSR\u2013Zw\u00f6lf Jahre Bauhaus\n", "abstract": " Das zehnj\u00e4hrige Jubil\u00e4um des deutschsprachigen Reengineering-Workshops WSR in Bad Honnef gibt Anlass, auch auf die eigene Arbeit zuru \u0308ck zu blicken, die thematisch mit dem WSR so eng verbunden ist. Unsere Forschergruppe Bauhaus besch\u00e4ftigt sich schon seit zw\u00f6lf Jahren mit dem Thema Software-Reengineering und verwandten Themen. Und seit Bestehen des WSR sind wir regelm\u00e4\u00dfige Teilnehmer am WSR. Dieser Artikel gibt einen historischen R\u00fcckblick auf unsere zw\u00f6lfj\u00e4hrige Arbeit und fasst unsere Forschungsarbeiten auf diesem Gebiet zusammen.", "num_citations": "7\n", "authors": ["266"]}
{"title": "Component recovery, protocol recovery and validation in bauhaus\n", "abstract": " Bauhaus is a research collaboration between the department for programming languages and compilers at the University of Stuttgart and the Fraunhofer institute for experimental software engineering in Kaiserslautern. At last year\u2019s Bad Honnef workshop [2], we have outlined future research topics of Stuttgart\u2019s Bauhaus group. This year, we summarize the achievements of the last 12 months and elaborate our research directions in more detail. This paper specifically addresses continued research in component recovery based on previous work [7] that additionally leverages our new infrastructure for control and data flow analyses. The paper introduces also relatively new research to recover protocols for the identified components.", "num_citations": "6\n", "authors": ["266"]}
{"title": "On the utility of a defect prediction model during hw/sw integration testing: A retrospective case study\n", "abstract": " Testing is an important and cost-intensive part of the software development life cycle. Defect prediction models try to identify error-prone components, so that these can be tested earlier or more in-depth, and thus improve the cost-effectiveness during testing. Such models have been researched extensively, but whether and when they are applicable in practice is still debated. The applicability depends on many factors, and we argue that it cannot be analyzed without a specific scenario in mind. In this paper, we therefore present an analysis of the utility for one case study, based on data collected during the hardware/software integration test of a system from the avionic domain. An analysis of all defects found during this phase reveals that more than half of them are not identifiable by a code-based defect prediction model. We then investigate the predictive performance of different prediction models for the remaining\u00a0\u2026", "num_citations": "5\n", "authors": ["266"]}
{"title": "Addendum to\" Locating features in source code\"\n", "abstract": " For original paper by T. Eisenbarth et al. see ibid., vol.29, no.3, p.210-24 (2003). We compare three approaches that apply formal concept analysis on execution profiles. This survey extends the discourse of related research by Bojic and Velasevic (2000).", "num_citations": "5\n", "authors": ["266"]}
{"title": "Effect of clone information on the performance of developers fixing cloned bugs\n", "abstract": " Duplicated source code -- clones -- is known to occur frequently in software systems and bears the risk of inconsistent updates of the code. The impact of clones has been investigated mostly by retrospective analysis of software systems. Only little effort has been spent to investigate human interaction when dealing with clones. A previous study by Chatterji and colleagues found that cloned defects are removed significantly more accurately when clone information is provided to the programmers. We conducted a controlled experiment to extend the previous study on the use of clone information by investigating the effect of clone information on the performance of developers in common bug-fixing tasks. The experiment shows that developers are quite capable to compensate missing clone information through testing to provide correct solutions. Clone information does help to detect cloned defects faster, although\u00a0\u2026", "num_citations": "4\n", "authors": ["266"]}
{"title": "Vorlesungen zum Thema Software-Reengineering\n", "abstract": " Department Programming Languages and Compilers | Publications : Bibliography 2000 BibTeX Bild von Institut mit Unilogo home uni uni suche suche sitemap sitemap kontakt kontakt unilogo Universit\u00e4t Stuttgart Faculty | Institute | Department | General | Teaching | Research | News Programming Languages and Compilers : Publications Bibliography 2000 BibTeX germanicon @inproceedings {INPROC-2000-56, author = {Rainer Koschke}, title = {{Vorlesungen zum Thema Software-Reengineering}}, booktitle = {2. Workshop Software-Reengineering, Bad Honnef 2000}, publisher = {Universit{\\\"a}t Koblenz-Landau}, institution = {University of Stuttgart, Faculty of Computer Science, Electrical Engineering, and Information Technology, Germany}, pages = {3--7}, type = {Workshop Paper}, month = {May}, year = {2000}, keywords = {Vorlesungen; Software Reengineering}, language = {German}, cr-category = {D.2 Software }, \u2026", "num_citations": "4\n", "authors": ["266"]}
{"title": "Software-Reengineering\n", "abstract": " Diese Aufgabe ist die zweite der Aufgaben f\u00fcr den Schein. In dieser Aufgabe geht es um die semantische Analyse, die Metrikberechnung und die Architekturextraktion. Sie baut auf dem bisherigen Rahmen f\u00fcr die einfache Programmiersprache SAT auf, die bei der letzten Aufgabe eingef\u00fchrt wurde.", "num_citations": "3\n", "authors": ["266"]}
{"title": "Clustering paths with dynamic time warping\n", "abstract": " Studying software visualization often includes the evaluation of paths collected from participants of a study (e.g., eye tracking or movements in virtual worlds). In this paper, we explore clustering techniques to automate the process of grouping similar paths. The heart of the evaluated approach is a distance metric between paths that is based on dynamic time warping (DTW). DTW aligns two paths based on any given distance metric between their data points so as to minimize the distance between those paths\u2014alignment may stretch or compress time for best fit. With a data set of 127 paths of professional software developers exploring code cities in virtual reality, we evaluate the clustering based on objective quality indices and manual inspection.", "num_citations": "2\n", "authors": ["266"]}
{"title": "How EvoStreets Are Observed in Three-Dimensional and Virtual Reality Environments\n", "abstract": " When analyzing software systems, a large amount of data accumulates. In order to assist developers in the preparation, evaluation, and understanding of findings, different visualization techniques have been developed. Due to recent progress in immersive virtual reality, existing visualization tools were ported to this environment. However, three-dimensional and virtual reality environments have different advantages and disadvantages, and by transferring concepts, such as layout algorithms and user interaction mechanisms, more or less one-to-one, the characteristics of these environments are neglected. In order to develop techniques adapting to the circumstance of a particular environment, more research in this field is necessary. In previously conducted case studies, we compared EvoStreets deployed in three different environments: 2D, 2.5D, and virtual reality. We found evidence that movement patterns-path\u00a0\u2026", "num_citations": "2\n", "authors": ["266"]}
{"title": "Improving Clone Detection Precision Using Machine Learning Techniques\n", "abstract": " Code clones or similar segments of code in a software project can be detected by using a clone detection tool. Due to modifications applied after copying and pasting of the cloned code, the current code clone detection tools face challenges to accurately detect clones with heavy modifications (i.e., Type-3 clones or clones with added/deleted/modified statements). One challenge is because the clone results contain several false positives. In this paper, we propose an approach for increasing the precision of code clone detection using machine learning techniques. By training a decision tree on 19 clone class metrics, we use the trained decision tree as a clone filter by placing it in the last step in the clone detection pipeline. This aims to remove false positive clone classes reported by a clone detection tool. We found that the decision tree clone filter is helpful for decreasing the number of false positive clone classes in\u00a0\u2026", "num_citations": "2\n", "authors": ["266"]}
{"title": "The Architectural Security Tool Suite\u2014ARCHSEC\n", "abstract": " Architectural risk analysis is a risk management process for identifying security flaws at the level of software architectures and is used by large software vendors, to secure their products. We present our architectural security environ- ment (ARCHSEC) that has been developed at our institute during the past eight years in several research projects. ARCHSEC aims to simplify architectural risk analysis, making it easier for small and mid-sized companies to get started. With ARCHSEC, it is possible to graphically model or to reverse engineer software security architectures. The regained software architectures can then be inspected manually or au- tomatically analyzed w.r.t. security flaws, resulting in a threat model, which serves as a base for discussion between software and security experts to improve the overall security of the software system in question, beyond the level of implementation bugs. In the evaluation part\u00a0\u2026", "num_citations": "2\n", "authors": ["266"]}
{"title": "Industrial experience on code clean-up using architectural conformance checking\n", "abstract": " This paper reports experiences in using the reflexion method to reverse engineer the architecture of an industrial Java application and to specify the target architecture for an architectural refactoring in order to steer the refactoring of the code and to measure progress. The goal in this industrial case study was to clean up obsolete code after a larger migration and to provide an architectural documentation for new developers. The distinctiveness of this study is that it was conducted by the author who is both an academic researcher on architecture erosion and conformance as well as a professional developer who is also among the two original developers of the Java program that was refactored.", "num_citations": "2\n", "authors": ["266"]}
{"title": "Guest editor\u2019s introduction to the special section on the 2009 international conference on program comprehension (ICPC 2009)\n", "abstract": " Program comprehension is a vital blend of software engineering activities that support reuse, inspection, maintenance, evolution, migration, reverse engineering, and reengineering of existing software systems. The International Conference on Program Comprehension (ICPC) is the principal venue for work in the area of program comprehension as well as a leading venue for work in the areas of software analysis, reverse engineering, software evolution, and software visualization. ICPC 2009 took place during May 17\u201319, 2009, in Vancouver, British Columbia, Canada, co-located with the International Conference on Software Engineering (ICSE \u201809). ICPC 2009 received a record number of technical paper submissions (74), which allowed us to assemble an excellent program that continues ICPC\u2019s tradition of providing a high-quality venue for sharing the latest advances in program comprehension. The program\u00a0\u2026", "num_citations": "2\n", "authors": ["266"]}
{"title": "Visualisierung von software-klonerkennung\n", "abstract": " Kapitel 1. Einleitung allem eine Reihe von Techniken zur Klonerkennung und darauf aufbauende Erkennungs-Systeme. Diese beschr\u00e4nken sich jedoch fast immer bei der Pr\u00e4sentation der gewonnen Daten auf eine rein textuelle Darstellung der Ergebnisse. In den meisten F\u00e4llen stellt es sich aber als schwieriges Unterfangen heraus, in solchen Listen einen \u00dcberblick \u00fcber die selektierten Daten zu gewinnen oder die Auspr\u00e4gung eines bestimmten Merkmales zu erkennen. Gleichzeitig ist die Auswertung der Daten langwierig und es besteht auf Grund der schlechte \u00dcbersicht au\u00dferdem die Gefahr falscher Schlussfolgerungen [3, 4].", "num_citations": "2\n", "authors": ["266"]}
{"title": "Wiedergewinnung von Architekturinformationen: Ausblicke\n", "abstract": " Im Rahmen des Bauhaus-Projektes wurde der bereits vorhandene Rigi-Editor [7] um Analysen zur Komponentenerkennung erweitert [3].", "num_citations": "2\n", "authors": ["266"]}
{"title": "A metric-based approach to detect abstract data types and state encapsulations\n", "abstract": " This article presents an approach to identify abstract data types (ADT) and abstract state encapsulations (ASE, also called abstract objects) in source code. This approach, named similarity clustering, groups together functions, types, and variables into ADT and ASE candidates according to the proportion of features they share. The set of features considered includes the context of these elements, the relationships to their environment, and informal information.A prototype tool has been implemented to support this approach. It has been applied to three C systems (each between 30-38 Kloc). The ADTs and ASEs identified by the approach are compared to those identified by software engineers who did not know the proposed approach or other automatic approaches. Within this case study, this approach has been shown to have a higher detection quality and to identify, in most of the cases, more ADTs and ASEs than the other techniques. In all other cases its detection quality is second best. NB This article reports work in progress on this approach which has evolved since it was presented in the original ASE97 conference paper.", "num_citations": "2\n", "authors": ["266"]}
{"title": "Javadoc Violations and Their Evolution in Open-Source Software\n", "abstract": " Software quality comprises different and interrelated aspects. One of them is maintainability, which in turn is made up of measurable attributes. Previous studies have shown that documentation, by contributing to the comprehensibility of software, may have a positive effect on maintainability and, hence, software quality. This paper presents a study in which we analyzed Javadoc comments from 163 different open-source projects. Javadoc is the de facto standard for documenting source code files in Java projects, and although its syntax is less strict than in other (programming) languages, documentation written with Javadoc may contain violations. Our study focuses on the detection of different types of Javadoc violations as well as the source code elements affected by them. Also, by utilizing software repository mining techniques, we examined the history of the subject systems to gain further insights into the evolution\u00a0\u2026", "num_citations": "1\n", "authors": ["266"]}
{"title": "Generated code in studies on clone rates\n", "abstract": " Various earlier studies have measured clone rates for diverse projects. One of the reasons for exceptionally high clone rates for individual source files was found to be auto-generated code. Automatically generated code is generally not maintained and, hence, should be excluded from clone-rate measurements. This kind of code might even introduce a bias to clone rates of projects when there is a large amount of generated code and clone rates for generated files generally deviate from the average clone rate for handwritten code. While some generated files stuck out with clone rates above the average in earlier studies, we do not know whether this is generally the case and how much code is actually generated automatically. This paper investigates the amount of generated files in projects, whether clone rates for generated files really differ from handwritten code, and - overall - whether generated code in fact\u00a0\u2026", "num_citations": "1\n", "authors": ["266"]}
{"title": "From preprocessor-constrained parse graphs to preprocessor-constrained control flow\n", "abstract": " Preprocessor-aware static analysis tools are needed for C Code to gain sound knowledge about the interference among all conditionally compiled program parts. We provide formal descriptions and algorithms to construct a preprocessor-aware control flow graph from preprocessor-aware parse graphs of SuperC. Based on the structure of parse graphs capturing the syntax nodes constrained by preprocessor constraints, we show how to model, formalize, and compute preprocessor-aware intra-procedural control-flow graphs. Such preprocessor-aware control-flow graphs may serve as the basis for subsequent preprocessor-aware control and data flow analyses.", "num_citations": "1\n", "authors": ["266"]}
{"title": "Object-based dynamic protocol recovery for multi-threading programs\n", "abstract": " A protocol defines the sequencing constraints for the operations that can be applied to an object. Quante introduced a protocol recovery technique that is able to extract protocols from existing software by means of dynamic analysis. This approach represents the behavior as object process graphs (OPG). OPGs are a projection of the control flow graph reduced to the operations relevant to an individual object. The protocol is inferred from a set of OPGs. The extraction was designed to handle sequential programs only. As multi-core architectures and, hence, multi-threading becomes more and more common in nowadays programming, it is necessary to extend reverse engineering techniques for multi-threaded programs. In this paper, we extend Quante's approach to protocol reconstruction for programs with multiple threads. We are formalizing this process using concepts from automata theory, namely, product and\u00a0\u2026", "num_citations": "1\n", "authors": ["266"]}
{"title": "Haben wir Programmverstehen schon ganz verstanden?\n", "abstract": " Langlebige Systeme m\u00fcssen kontinuierlich von Entwicklern angepasst werden, wenn sie nicht an Wert verlieren sollen. Ohne ausreichendes Verst\u00e4ndnis des Anderungswunsches und des zu \u00e4ndernden Gegenstands kann die Anpassung nicht effizient und effektiv vorgenommen werden. Deshalb ist es wichtig, das System so zu strukturieren, dass Entwickler es leicht verstehen k\u00f6nnen. Methoden und Werkzeuge m\u00fcssen bereitgestellt werden, um die Aktivit\u00e4ten bei der Anderung zu unterst\u00fctzen. Dazu ist ein umfassendes Verst\u00e4ndnis notwendig, wie \u00fcberhaupt Entwickler Programme verstehen.", "num_citations": "1\n", "authors": ["266"]}
{"title": "Empirische Grundlagen f\u00fcr das Klonmanagement\n", "abstract": " Software-Systeme enthalten in der Praxis h\u00e4ufig einen hohen Grad redundanten Quelltextes - so genannte Klone. Von solcher Software-Redundanz wird angenommen, dass sie bei der Entwicklung und Wartung von Software zu zus\u00e4tzlichem Aufwand und Problemen f\u00fchrt. Ein Ziel der Klonforschung besteht darin, Methoden und Werkzeuge f\u00fcr den Umgang mit Klonen oder deren Vermeidung zu entwickeln. F\u00fcr das so genannte Klonmanagement fehlen jedoch noch wichtige empirische Grundlagen, die Aufschluss \u00fcber die Ursachen und Auswirkungen von Klonen geben und eine informierte Basis f\u00fcr die Entwicklung von Methoden und Werkzeugen bieten. In diesem Positionspapier nennen wir, ausgehend vom Stand der Forschung, offene Forschungsfragen, deren Beantwortung wesentliche Grundlagen f\u00fcr das Klonmanagement liefern wird.", "num_citations": "1\n", "authors": ["266"]}
{"title": "06301 Summary--Duplication, Redundancy, and Similarity in Software\n", "abstract": " This paper summarizes the proceedings and outcomes of the Dagstuhl Seminar 06301. The purpose of the seminar was to bring together a broad selection of experts on duplication, redundancy, and similarity in software in order to: synthesize a comprehensive understanding of the topic area, appreciate the diversity in the topic, and to critically evaluate current knowledge. The structure of the seminar was specifically formulated to evoke such a synthesis and evaluation. We report here the success of this seminar and summarize its results, much of which is a record of working groups charged with discussing the topics of interest.", "num_citations": "1\n", "authors": ["266"]}
{"title": "Software-Projekt\n", "abstract": " Wann und wie oft die Anforderungsanalyse durchgef\u00fchrt wird, h\u00e4ngt ebenso stark vom Vorgehensmodell zur Softwareentwicklung ab. Beim Wasserfallmodell wird die Anforderungsanalyse nur einmal und ganz zu Anfang durchgef\u00fchrt. Beim inkrementellen Vorgehen, bei dem die Entwicklung in kleinen abgeschlossenen Inkrementen des zu entwickelnden Systems erfolgt, wird sie jedes Mal wieder vor der Entwicklung jedes Inkrements durchgef\u00fchrt. Beim Extreme-Programming hat man durch die Pr\u00e4senz eines Kundenvertreters vor Ort eine fast kontinuierlich stattfindende Anforderungsanalyse.", "num_citations": "1\n", "authors": ["266"]}
{"title": "Vorlesung Software-Reengineering\n", "abstract": " Erreichbar: OAS, Telefon 218-9671, koschke@ tzi. de Sprechstunde nach Vereinbarung Video im Netz http://mlecture. uni-bremen. de bitte bei Stud. IP anmelden unter https://elearning. uni-bremen. de/Literatur: Folien zur Vorlesung und verwendete Artikel http://www. informatik. uni-bremen. de/st/lehredetails. php? id= &lehre_id= 308", "num_citations": "1\n", "authors": ["266"]}
{"title": "A concept analsis primer\n", "abstract": " Formal concept analysis is a mathematical technique to analyze binary relations. It has been successfully applied to various software engineering problems. This paper gives a brief introduction to concept analysis.", "num_citations": "1\n", "authors": ["266"]}
{"title": "15 Symphony Fallstudie: Hierarchische Reflexion Modelle\n", "abstract": " Reflexion Modell, das von Murphy and Notkin [5, 6, 7] vorgeschlagen wurde, erlaubt es dem Reengineer, ein abgeleitetes oder vorgeschriebenes Architekturmodell eine Softwaresystems gegen ein automatisch aus den Quelltexten abgeleitetes Modell zu validieren. Dabei werden zun\u00e4chst die Entit\u00e4ten im Quellmodell auf das hypothetische Architekturmodell abgebildet. Im Anschluss daran werden die Diskrepanzen zwischen Quell-und Architekturmodell automatisch berechnet und k\u00f6nnen dem Benutzer in geeigneter Weise pr\u00e4sentiert werden. Da das Modell keine hierarchischen Architekturen unterst\u00fctzt, ist es f\u00fcr die Untersuchung gro\u00dfer Systeme nur bedingt geeignet. In [4] haben wir das Modell von Notkin ua um die M\u00f6glichkeit erweitert, auch mit hierarchischen Architekturen umzugehen.", "num_citations": "1\n", "authors": ["266"]}
{"title": "Component Recovery, Protocol Recovery and Validation}}\n", "abstract": " Department Programming Languages and Compilers | Publications : Bibliography 2001 BibTeX Bild von Institut mit Unilogo home uni uni suche suche sitemap sitemap kontakt kontakt unilogo Universit\u00e4t Stuttgart Faculty | Institute | Department | General | Teaching | Research | News Programming Languages and Compilers : Publications Bibliography 2001 BibTeX germanicon @inproceedings {INPROC-2001-88, author = {Rainer Koschke and Yan Zhang}, title = {{Component Recovery, Protocol Recovery and Validation}}, booktitle = {3. Workshop Software-Reengineering}, publisher = {Universit{\\\"a}t Koblenz-Landau}, institution = {University of Stuttgart, Faculty of Computer Science, Electrical Engineering, and Information Technology, Germany}, pages = {73--76}, type = {Workshop Paper}, month = {May}, year = {2001}, language = {English}, cr-category = {D.2.7 Software Engineering Distribution, Maintenance, and }, = \u2026", "num_citations": "1\n", "authors": ["266"]}
{"title": "Workshop on standard exchange format (WoSEF)(workshop session)\n", "abstract": " A common exchange format for sharing data extracted from source code is necessary to advance the state of the art in many branches of software engineering, such as reverse engineering, software visualization, metrics, program comprehension, and testing. With a common exchange format researchers can more easily leverage each others' tools and take a \u201cbest of breed\u201d approach when solving problems. To take an example from reverse engineering, one would be able to select the best parsing and analysis tools, send the output to a clustering program, display the results using a visualization tool from another group, and use the results to effect changes to the software. A standard exchange format would allow researchers to build a repository of example systems, or\" guinea pigs\", and compare or combine results from various tools.There have been many efforts to create a standard data exchange format. Some\u00a0\u2026", "num_citations": "1\n", "authors": ["266"]}
{"title": "An Intermediate Representation for Integrating Reverse Engineering Analyses\n", "abstract": " Intermediate representations (IR) are a key issue both for compilers as well as for reverse engineering tools to enable efficient analyses. Research in the field of compilers has proposed many sophisticated IRs that can be used in the domain of reverse engineering, especially in the case of deep analyses, but reverse engineering has also its own requirements for intermediate representations not covered by traditional compiler technology. This paper discusses requirements of IRs for reverse engineering. It shows then how most of these requirements can be met by extending and integrating existing IRs. These extensions include a generalized AST and a mechanism supporting multiple views on programs. Moreover, the paper shows how these views can efficiently be implemented.", "num_citations": "1\n", "authors": ["266"]}
{"title": "Experiences in adjusting a compiler toolkit to generate Ada 95 code\n", "abstract": " Ada 95 code is generated using a compiler toolkit called Cocktail. The object-oriented features of the compiler was used to map data structures and algorithms to Ada 95. A problem between dispatching and locality that seemed to have no real solution in the current Ada language definition was encountered but was solved by making a trade off between tedious and inefficient nested if-statements to implement additional user-defined operations. Differences between the form of compilation and run-time efficiency and code size of Ada 83 and Ada 95 solutions were evaluated.", "num_citations": "1\n", "authors": ["266"]}
{"title": "Feature-driven program understanding using concept analysis of execution\n", "abstract": " The first task of a programmer who wants to under-stand how a certain feature is implemented is to localize the implementation of the feature in the code. If the imple-mentations of a set of related features are to be understood, a programmer is interested in their commonalities and variabilities. For large and badly documented programs, localizing features in code and identifying commonalities and variabilities of components and features can be difficult and time-consuming. It is useful to derive this information automatically. The feature-component correspondence describes which components are needed to implement a set offeatures and what are the respective commonalities and vari-abilities offeatures and components. This paper describes a new technique to derive the feature-component correspondence utilizing dynamic information and concept analysis. The method is simple to apply, cost-effective, largely\u00a0\u2026", "num_citations": "1\n", "authors": ["266"]}