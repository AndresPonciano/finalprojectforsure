{"title": "Transactional memory: Architectural support for lock-free data structures\n", "abstract": " A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional\u00a0\u2026", "num_citations": "3297\n", "authors": ["1872"]}
{"title": "The DaCapo benchmarks: Java benchmarking development and analysis\n", "abstract": " Since benchmarks drive computer science research and industry product development, which ones we use and how we evaluate them are key questions for the community. Despite complex runtime tradeoffs due to dynamic compilation and garbage collection required for Java programs, many evaluations still use methodologies developed for C, C++, and Fortran. SPEC, the dominant purveyor of benchmarks, compounded this problem by institutionalizing these methodologies for their Java benchmark suite. This paper recommends benchmarking selection and evaluation methodologies, and introduces the DaCapo benchmarks, a set of open source, client-side Java benchmarks. We demonstrate that the complex interactions of (1) architecture,(2) compiler,(3) virtual machine,(4) memory management, and (5) application require more extensive evaluation than C, C++, and Fortran which stress (4) much less, and do\u00a0\u2026", "num_citations": "1765\n", "authors": ["1872"]}
{"title": "Nested Transactions: An Approach to Reliable Distributed Computing.\n", "abstract": " We examine how to program a system so that the software continues to work in the face of a variety of failures of parts of the system. The design presented uses the concept of transactions collections of primitive actions that are indivisible. The indivisibility of transactions insures that consistent results are obtained even when requests are processed concurrently or failures occur during a request. Our design permits transactions to be nested. Nested transactions provide nested universes of synchronization and recovery from failures. The advantages of nested transactions over single-level transactions are that they provide concurrency control within transactions by serializing subtransactions appropriately, and that they permit parts of a transaction to fail without necessarily aborting the entire transaction. The method for implementing nested transactions described in this report is novel in that it uses locking for concurrency control. We present the necessary algorithms for locking, recovery, distributed commitment, and distributed deadlock detection for a nested transaction system. While the design has not been implemented, it has been simulated. The algorithms are described in a formal notation in an appendix, as well as narratively in the body of the report.Descriptors:", "num_citations": "566\n", "authors": ["1872"]}
{"title": "System for achieving atomic non-sequential multi-word operations in shared memory\n", "abstract": " A computer system provides transactional memory operations, in which a selected data item in a shared memory is referenced by a CPU in local storage (such as a write-back cache). The CPU performs some operation to alter or use the data item while it is in local memory, and meanwhile monitors the bus to the shared memory to see if another processor references the selected location (as by a snoop mechanism); if so, a status bit is toggled to indicate that the transaction must be scrubbed. When the operation has been completed by the CPU, it attempts to\" commit\" the transaction, and this includes checking the status bit; if the bit has been toggled, the transaction aborts, the data item is invalidated in local memory, and the selected location in shared memory is not affected. If the status bit has not been toggled, the transaction is committed and the altered data item becomes visible to the other processors, and may\u00a0\u2026", "num_citations": "277\n", "authors": ["1872"]}
{"title": "Working with persistent objects: To swizzle or not to swizzle\n", "abstract": " Pointer swizzling\" is the conversion of database objects between an external form (object identifiers) and an internal form (direct memory pointers). Swizzling is used in some object-oriented databases, persistent object stores, and persistent and database programming language implementations to speed manipulation of memory resident data. Here we describe a simplifying model of application behavior, revealing those aspects where swizzling is most relevant in both benefits and costs. The model has a number of parameters, which we have measured for a particular instance of the Mneme persistent ohject store, varying the swizzling technique used. The results confirm most of the intuitive, qualitative trade-offs, with the quantitative data showing that some performance differences between schemes are smaller than might be expected. However, there are some interesting effects that run counter to naive intuition\u00a0\u2026", "num_citations": "269\n", "authors": ["1872"]}
{"title": "Open nesting in software transactional memory\n", "abstract": " Transactional memory (TM) promises to simplify concurrent programming while providing scalability competitive to fine-grained locking. Language-based constructs allow programmers to denote atomic regions declaratively and to rely on the underlying system to provide transactional guarantees along with concurrency. In contrast with fine-grained locking, TM allows programmers to write simpler programs that are composable and deadlock-free.", "num_citations": "237\n", "authors": ["1872"]}
{"title": "Type-based alias analysis\n", "abstract": " This paper evaluates three alias analyses based on programming language types. The first analysis uses type compatibility to determine aliases. The second extends the first by using additional high-level information such as field names. The third extends the second with a flow-insensitive analysis. Although other researchers suggests using types to disambiguate memory references, none evaluates its effectiveness. We perform both static and dynamic evaluations of type-based alias analyses for Modula-3, a statically-typed type-safe language. The static analysis reveals that type compatibility alone yields a very imprecise alias analysis, but the other two analyses significantly improve alias precision. We use redundant load elimination (RLE) to demonstrate the effectiveness of the three alias algorithms in terms of the opportunities for optimization, the impact on simulated execution times, and to compute an upper\u00a0\u2026", "num_citations": "226\n", "authors": ["1872"]}
{"title": "Efficient packet demultiplexing for multiple endpoints and large messages\n", "abstract": " \"Efficient Packet Demultiplexing for Multiple Endpoints and Large Messages\" Masanobu Yuhara Brian N. Bershad Fujitsu Laboratories Ltd. Department of Computer Science 1015 Kamikodanaka and Engineering FR-35 Nakahara-ku University of Washington Kawasaki 211, Japan Seattle, WA 98195 Chris Maeda J. Eliot B. Moss School of Computer Science Dept. of Computer Science Carnegie Mellon University University of Massachusetts 5000 Forbes Ave. Amherst, MA 01003 Pittsburgh, PA 15213 {This research was sponsored in part by The Advanced Research Projects Agency, Information Science and Technology Office, under the title ``Research on Parallel Computing'', ARPA Order No. 7330, issued by DARPA/CMO under Contract MDA972-90-C-0035, by the Advanced Research Projects Agency, CSTO, under the title ``The Fox Project: Advanced Development of Systems Software'', ARPA Order No. 8313, \u2026", "num_citations": "203\n", "authors": ["1872"]}
{"title": "Incremental collection of mature objects\n", "abstract": " We present a garbage collection algorithm that extends generational scavenging to collect large older generations (mature objects) non-disruptively. The algorithm's approach is to process bounded-size pieces of mature object space at each collection; the subtleties lie in guaranteeing that it eventually collects any and all garbage. The algorithm does not assume any special hardware or operating system support, e.g., for forwarding pointers or protection traps. The algorithm copies objects, so it naturally supports compaction and reclustering.", "num_citations": "191\n", "authors": ["1872"]}
{"title": "The garbage collection advantage: improving program locality\n", "abstract": " As improvements in processor speed continue to outpace improvements in cache and memory speed, poor locality increasingly degrades performance. Because copying garbage collectors move objects, they have an opportunity to improve locality. However, no static copying order is guaranteed to match program traversal orders. This paper introduces online object reordering (OOR) which includes a new dynamic, online class analysis for Java that detects program traversal patterns and exploits them in a copying collector. OOR uses run-time method sampling that drives just-in-time (JIT) compilation. For each hot (frequently executed) method, OOR analysis identifies the hot field accesses. At garbage collection time, the OOR collector then copies referents of hot fields together with their parent. Enhancements include static analysis to exclude accesses in cold basic blocks, heuristics that decay heat to respond to\u00a0\u2026", "num_citations": "190\n", "authors": ["1872"]}
{"title": "Nested transactional memory: model and architecture sketches\n", "abstract": " We offer a reference model for nested transactions at the level of memory accesses, and sketch possible hardware architecture designs that implement that model. We describe both closed and open nesting. The model is abstract in that it does not relate to hardware, such as caches, but describes memory as seen by each transaction, memory access conflicts, and the effects of commits and aborts. The hardware sketches describe approaches to implementing the model using bounded size caches in a processor with overflows to memory. In addition to a model that will support concurrency within a transaction, we describe a simpler model that we call linear nesting. Linear nesting supports only a single thread of execution in a transaction nest, but may be easier to implement. While we hope that the model is a good target to which to compile transactions from source languages, the mapping from source constructs to\u00a0\u2026", "num_citations": "180\n", "authors": ["1872"]}
{"title": "Design of the Mneme persistent object store\n", "abstract": " The Mneme project is an investigation of techniques for integrating programming language and database features to provide better support for cooperative, information-intensive tasks such as computer-aided software engineering. The project strategy is to implement efficient, distributed, persistent programming languages. We report here on the Mneme persistent object store, a fundamental component of the project, discussing its design and initial prototype. Mneme stores objects in a simple and general format, preserving object identity and object interrelationships. Specific goals for the store include portability, extensibility (especially with respect to object management policies), and performance. The model of memory that the store aims at is a single, cooperatively-shared heap, distributed across a collection of networked computers. The initial prototype is intended mainly to explore performance issues and to\u00a0\u2026", "num_citations": "164\n", "authors": ["1872"]}
{"title": "A comparative performance evaluation of write barrier implementations\n", "abstract": " Generational garbage collectors are able to achieve very small pause times by concentrating on the youngest (most recently allocated) objects when collecting, since objects have been observed to die young in many systems. Generational collectors must keep track of all pointers from older to younger generations, by \u201cmonitoring\u201d all stores into the heap. This write barrier has been implemented in a number of ways, varying essentially in the granularity of the information observed and stored. Here we examine a range of write barrier implementations and evaluate their relative performance within a generation scavenging garbage collector for Smalltalk.", "num_citations": "163\n", "authors": ["1872"]}
{"title": "Simple and effective analysis of statically-typed object-oriented programs\n", "abstract": " To use modern hardware effectively, compilers need extensive control-flow information. Unfortunately, the frequent method invocations in object-oriented languages obscure control flow. In this paper, we describe and evaluate a range of analysis techniques to convert method invocations into direct calls for statically-typed object-oriented languages and thus improve control-flow information in object-oriented languages. We present simple algorithms for type hierarchy analysis, aggregate analysis, and interprocedural and intraprocedural type propagation. These algorithms are also fast, O(|procedures| * \u2211pprocedure np * vp) worst case time (linear in practice) for our slowest analysis, where np is the size of procedure p and vp is the number of variables in procedure p, and are thus practical for use in a compiler. When they fail, we introduce cause analysis to reveal the source of imprecision and suggest where more\u00a0\u2026", "num_citations": "145\n", "authors": ["1872"]}
{"title": "CRAMM: Virtual memory support for garbage-collected applications\n", "abstract": " Existing virtual memory systems usually work well with applications written in C and C++, but they do not provide adequate support for garbage-collected applications. The performance of garbage-collected applications is sensitive to heap size. Larger heaps reduce the frequency of garbage collections, making them run several times faster. However, if the heap is too large to fit in the available RAM, garbage collection can trigger thrashing. Existing Java virtual machines attempt to adapt their application heap sizes to fit in RAM, but suffer performance degradations of up to 94% when subjected to bursts of memory pressure. We present CRAMM (Cooperative Robust Automatic Memory Management), a system that solves these problems. CRAMM consists of two parts:(1) a new virtual memory system that collects detailed reference information for (2) an analytical model tailored to the underlying garbage collection algorithm. The CRAMM virtual memory system tracks recent reference behavior with low overhead. The CRAMM heap sizing model uses this information to compute a heap size that maximizes throughput while minimizing paging. We present extensive empirical results demonstrating CRAMM\u2019s ability to maintain high performance in the face of changing application and system load.", "num_citations": "137\n", "authors": ["1872"]}
{"title": "Open nested transactions: Semantics and support\n", "abstract": " We describe semantics for serializable (safe) open nested transactions. Given these semantics, we then suggest hardware necessary to support them directly. We further consider some useful, but not serializable, applications for open nesting, and their hardware implications. We focus primarily on linear nesting, which we previously argued to be more amenable to hardware support than the general case.I. MOTIVATION There is increasing interest in adding some notion of transactions to general purpose programming languages such as Java (see [1] for example). Transactions promise to make it easier for ordinary programmers to write correct and efficient concurrent applications (at least of certain kinds), by solving a number of problems exhibited by locking, such as deadlock, priority inversion, and concurrency bottlenecks.", "num_citations": "133\n", "authors": ["1872"]}