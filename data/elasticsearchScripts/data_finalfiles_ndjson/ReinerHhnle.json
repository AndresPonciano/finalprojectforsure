{"title": "ABS: A core language for abstract behavioral specification\n", "abstract": " This paper presents ABS, an abstract behavioral specification language for designing executable models of distributed object-oriented systems. The language combines advanced concurrency and synchronization mechanisms for concurrent object groups with a functional language for modeling data. ABS uses asynchronous method calls, interfaces for encapsulation, and cooperative scheduling of method activations inside concurrent objects. This feature combination results in a concurrent object-oriented model which is inherently compositional. We discuss central design issues for ABS and formalize the type system and semantics of Core ABS, a calculus with the main features of ABS. For Core ABS, we prove a subject reduction property which shows that well-typedness is preserved during execution; in particular, \u201cmethod not understood\u201d errors do not occur at runtime for well-typed ABS models. Finally\u00a0\u2026", "num_citations": "404\n", "authors": ["665"]}
{"title": "Automated deduction in multiple-valued logics\n", "abstract": " La derniere chose qu\u2019on trouve en faisant un ouvrage est de savoir celle qu\u2019il faut mettre la premiere.", "num_citations": "277\n", "authors": ["665"]}
{"title": "A theorem proving approach to analysis of secure information flow\n", "abstract": " Most attempts at analysing secure information flow in programs are based on domain-specific logics. Though computationally feasible, these approaches suffer from the need for abstraction and the high cost of building dedicated tools for real programming languages. We recast the information flow problem in a general program logic rather than a problem-specific one. We investigate the feasibility of this approach by showing how a general purpose tool for software verification can be used to perform information flow analyses. We are able to prove security and insecurity of programs including advanced features such as method calls, loops, and object types for the target language Java Card. In addition, we can express declassification of information.", "num_citations": "261\n", "authors": ["665"]}
{"title": "Tableaux and related methods\n", "abstract": " CiteSeerX \u2014 Tableaux and Related Methods Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Tableaux and Related Methods (2001) Cached Download as a PDF Download Links [www.cs.chalmers.se] [www.math.chalmers.se] Save to List Add to Collection Correct Errors Monitor Changes by Reiner H\u00e4hnle Citations: 48 - 3 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The Pennsylvania State University \u2026", "num_citations": "214\n", "authors": ["665"]}
{"title": "Resource analysis of complex programs with cost equations\n", "abstract": " We present a novel static analysis for inferring precise complexity bounds of imperative and recursive programs. The analysis operates on cost equations. Therefore, it permits uniform treatment of loops and recursive procedures. The analysis is able to provide precise upper bounds for programs with complex execution flow and multi-dimensional ranking functions. In a first phase, a combination of control-flow refinement and invariant generation creates a representation of the possible behaviors of a (possibly inter-procedural) program in the form of a set of execution patterns. In a second phase, a cost upper bound of each pattern is obtained by combining individual costs of code fragments. Our technique is able to detect dependencies between different pieces of code and hence to compute a precise upper bounds for a given program. A prototype has been implemented and evaluated to demonstrate the\u00a0\u2026", "num_citations": "108\n", "authors": ["665"]}
{"title": "The liberalized \u03b4-rule in free variable semantic tableaux\n", "abstract": " In this paper we have a closer look at one of the rules of the tableau calculus presented by Fitting [4], called the \u03b4-rule. We prove that a modification of this rule, called the \u03b4+-rule, which uses fewer free variables, is also sound and complete. We examine the relationship between the \u03b4+-rule and variations of the \u03b4-rule presented by Smullyan [9]. This leads to a second proof of the soundness of the \u03b4+-rule. An example shows the relevance of this modification for building tableau-based theorem provers.", "num_citations": "103\n", "authors": ["665"]}
{"title": "A Survey of Active Object Languages\n", "abstract": " To program parallel systems efficiently and easily, a wide range of programming models have been proposed, each with different choices concerning synchronization and communication between parallel entities. Among them, the actor model is based on loosely coupled parallel entities that communicate by means of asynchronous messages and mailboxes. Some actor languages provide a strong integration with object-oriented concepts; these are often called active object languages. This article reviews four major actor and active object languages and compares them according to carefully chosen dimensions that cover central aspects of the programming paradigms and their implementation.", "num_citations": "99\n", "authors": ["665"]}
{"title": "Advanced many-valued logics\n", "abstract": " Let me begin with a brief discussion of the name of this chapter: the adjective \u201cadvanced\u201d in the title can only be understood in the temporal sense; the bulk of Urquhart\u2019s chapter in this Handbook was written for the first edition in the early 1980s and, therefore, does not cover recent results in depth. Perhaps \u201ccomplementary\u201d would be an altogether more fitting qualification for the present text. It is not required to have read \u201cBasic Many-Valued Logic\u201d in order to use my chapter. It is (I hope) not more difficult to read, either. On the other hand, you will find few overlaps and for sure some quite different points of view. Urquhart\u2019s chapter, for example, covers functional completeness, model theory, or theory of consequence relations very well, and I do not repeat this material.", "num_citations": "96\n", "authors": ["665"]}
{"title": "Uniform notation of tableau rules for multiple-valued logics\n", "abstract": " A framework for axiomatizing arbitrary finitely valued logics with minimal overhead compared to the classical case is presented. The main idea is to work with tableaux using generalized signs, which makes it possible to express complex assertions regarding the possible truth values of a formula. The class of regular logical connectives which, together with a suitable restriction on queries (ie allowed signs) to the system, allow a uniform notation style representation of multiple-valued propositional and first-order logics is introduced. It has been demonstrated that various systems differing in their allowed classes of connectives and complexity, of rules may be formulated. This allows the use of tools and methods that are close to the ones used in classical logic, both on the theoretical (uniform notation in definitions and proofs) and practical (use of classical theorem provers with few modifications) sides.<>", "num_citations": "94\n", "authors": ["665"]}
{"title": "Many-valued logic and mixed integer programming\n", "abstract": " We generalize prepositional semantic tableaux for classical and many-valued logics toconstraint tableaux. We show that this technique is a generalization of the standard translation from CNF formulas into integer programming. The main advantages are (i) a relatively efficient satisfiability checking procedure for classical, finitely-valued and, for the first time, for a wide range of infinitely-valued propositional logics; (ii) easy NP-containment proofs for many-valued logics. The standard translation of two-valued CNF formulas into integer programs and Tseitin's structure preserving clause form translation are obtained as a special case of our approach.", "num_citations": "88\n", "authors": ["665"]}
{"title": "Short conjunctive normal forms in finitely valued logics\n", "abstract": " New applications for many-valued theorem proving in various subfields, for example in the theory of error-correcting codes, in non-monotonic reasoning, and in formal software and hardware verification, demand efficient automatic proof procedures for many-valued logics. Many successful theorem-proving methods in two-valued logic, notably resolution, presume the existence of a conjunctive normal form (CNF). We present a general satisfiability preserving transformation of formulae from arbitrary finitely valued logics into a CNF which is based on signed atomic formulae. The transformation is always linear with respect to the length of the input, and we define a generalized concept of polarity in order to avoid the generation of redundant clauses. The transformation rules are based on the concept of \u2018sets-as-signs\u2019 developed earlier by the author in the context of tableau-based deduction in many-valued logics\u00a0\u2026", "num_citations": "87\n", "authors": ["665"]}
{"title": "Generating unit tests from formal proofs\n", "abstract": " We present a new automatic test generation method for Java Card based on attempts at formal verification of the implementation under test (IUT). Self-contained unit tests in JUnit format are generated automatically. The advantages of the approach are: (i)\u00a0it exploits the full information available in the IUT and in its formal model giving very good hybrid coverage; (ii)\u00a0a non-trivial formal model of the IUT is unnecessary; (iii)\u00a0it is adaptable to the skills that users may possess in formal methods.", "num_citations": "81\n", "authors": ["665"]}
{"title": "Towards an efficient tableau proof procedure for multiple-valued logics\n", "abstract": " One of the obstacles against the use of tableau-based theorem provers for non-standard logics is the inefficiency of tableau systems in practical applications, though they are highly intuitive and extremely flexible from a proof theoretical point of view. We present a method for increasing the efficiency of tableau systems in the case of multiple-valued logics by introducing a generalized notion of signed formulas and give sound and complete tableau systems for arbitrary propositional finite-valued logics.", "num_citations": "76\n", "authors": ["665"]}
{"title": "An authoring tool for informal and formal requirements specifications\n", "abstract": " We describe foundations and design principles of a tool that supports authoring of informal and formal software requirements specifications simultaneously and from a single source. The tool is an attempt to bridge the gap between completely informal requirements specifications (as found in practice) and formal ones (as needed in formal methods). The user is supported by an interactive syntax-directed editor, parsers and linearizers. As a formal specification language we realize the Object Constraint Language, a substandard of the UML, on the informal side a fragment of English. The implementation is based on the Grammatical Framework, a generic tool that combines linguistic and logical methods.", "num_citations": "74\n", "authors": ["665"]}
{"title": "Deduction in many-valued logics: a survey\n", "abstract": " Until the late 1980s research in many-valued logic (MVL) focussed on theoretical issues in proof theory, algebra, expressivity, axiomatizability and, on the applicative side, discrete function minimization and simplification. The first papers with practical implementation of deduction systems in mind came up in paraconsistent/annotated logic programming [18, 76] and in automated theorem proving [55, 108].A partial survey of the results up to 1993 is contained in [58]. In the past five years deduction methods for MVL got more and more refined. Recent results can match those in classical theorem proving with respect to depth and attention to detail. They are not confined to mimicking improvements of deduction invented in classical logic, rather, specifically non-classical strategies are started to being pursued. As can be seen from the references list of this article, there is considerable activity in MVL deduction which is why we felt justified in writing this survey. Needless to say, we cannot give a general introduction to MVL in the present context. For this, we have to refer to general treatments such as [152, 53, 93].", "num_citations": "67\n", "authors": ["665"]}
{"title": "Exploiting data dependencies in many-valued logics\n", "abstract": " The purpose of this paper is to make some pratically relevant results in automated theorem proving available to many-valued logics with suitable modifications. We are working with a notion of many-valued first-order clauses which any finitely- valued logic formula can be translated into and that has been used several times in the literature, but in an ad hoc way. We give a many-valued version of polarity which in turn leads to natural many-valued counterparts of Horn formulas, hyperresolution, and a Davis\u2014Putnam procedure. We show that the many-valued generalizations share many of the desirable properties of the classical versions. Our results justify and generalize several earlier results on theorem proving in many-valued logics.", "num_citations": "65\n", "authors": ["665"]}
{"title": "Formal modeling and analysis of resource management for cloud architectures: an industrial case study using Real-Time ABS\n", "abstract": " We demonstrate by a case study of an industrial distributed system how performance, resource consumption, and deployment on the cloud can be formally modeled and analyzed using the abstract behavioral specification language Real-Time ABS. These non-functional aspects of the system are integrated with an existing formal model of the functional system behavior, achieving a separation of concerns between the functional and non-functional aspects in the integrated model. The resource costs associated with execution in the system depend on the size of local data structures, which evolve over time; we derive corresponding worst-case cost estimations by static analysis techniques and integrate them into our resource-sensitive model. The model is further parameterized with respect to deployment scenarios which capture different application-level management policies for virtualized resources. The\u00a0\u2026", "num_citations": "63\n", "authors": ["665"]}
{"title": "Tableaux for many-valued logics\n", "abstract": " This article reports on research done in the intersection between many-valued logics and logical calculi related to tableaux. A lot of important issues in many-valued logic, such as algebras arising from many-valued logic, many-valued function minimization, philosophical topics, or applications are not discussed here; for these, we refer the reader to general monographs and overviews such as [Rosser and Turquette, 1952; Rescher, 1969; Urquhart, 1986; Bole and Borowik, 1992; Malinowski, 1993; H\u00e4hnle, 1994; Panti, to appear]. More questionable, perhaps, than the omissions is the need for a handbook chapter on tableaux for many-valued logics in the first place.", "num_citations": "60\n", "authors": ["665"]}
{"title": "The tableau-based theorem prover 3 T A P Version 4.0\n", "abstract": " Overview3774/9 is a tableau-based theorem prover for many-valued first-order logics with sorts (in the two-valued version with equality); it is implemented in Prolog. This paper gives an overview of the system with a special focus on the new features of 3TAP Version 4.0, including: efficient completion-based equality reasoning, methods for handling redundant axiom sets, utilization of pragmatic information contained in axioms to rearrange the search space, and a graphical user interface for controlling 3T4P and visualizing its output,. 37z4P has been developed at the University of Karlsruhe. In 1989 the project started in cooperation with the Institute for Knowledge Based Systems of IBM Germany. Since 1992 the system is maintained and improved as part of a new project at the University'of Ka. rlsruhe fimded by the Deutsche Forschungsgemeinschaft (DFG).", "num_citations": "59\n", "authors": ["665"]}
{"title": "The abstract behavioral specification language: a tutorial introduction\n", "abstract": " ABS (for abstract behavioral specification) is a novel language for modeling feature-rich, distributed, object-oriented systems at an abstract, yet precise level. ABS has a clear and simple concurrency model that permits synchronous as well as actor-style asynchronous communication. ABS abstracts away from specific datatype or I/O implementations, but is a fully executable language and has code generators for Java, Scala, and Maude. ABS goes beyond conventional programming languages in two important aspects: first, it embeds architectural concepts such as components or feature hierarchies and allows to connect features with their implementation in terms of product families. In contrast to standard OO languages, code reuse in ABS is feature-based instead of inheritance-based. Second, ABS has a formal semantics and has been designed with formal analyzability in mind. This paper gives a tutorial\u00a0\u2026", "num_citations": "56\n", "authors": ["665"]}
{"title": "Many-valued logic, partiality, and abstraction in formal specification languages\n", "abstract": " The purpose of this article is to clarify the role that many-valued logic can or should play in formal specification of software systems for modeling partiality. We analyse a representative set of specification languages. Our findings suggest that many-valued logic is less useful for modeling those aspects of partiality, for which it is traditionally intended: modeling non-termination and error values. On the other hand, many-valued logic is emerging as a mainstream tool in abstraction of formal analyses of various kinds, and we suggest that specification languages feature many-valued abstraction logics.", "num_citations": "53\n", "authors": ["665"]}
{"title": "A modular reduction of regular logic to classical logic\n", "abstract": " In this paper we first define a reduction /spl delta/ that transforms an instance /spl Gamma/ of Regular-SAT into a satisfiability equivalent instance /spl Gamma//sup /spl delta// of SAT. The reduction /spl delta/ has interesting properties: (i) the size of /spl Gamma//sup /spl delta// is linear in the size of /spl Gamma/, (ii) /spl delta/ transforms regular Horn formulas into Horn formulas, and (iii) /spl delta/ transforms regular 2-CNF formulas into 2-CNF formulas. Second, we describe a new satisfiability algorithm that determines the satisfiability of a regular 2-CNF formula /spl Gamma/ in time O(|/spl Gamma/|log|/spl Gamma/|); this algorithm is inspired by the reduction /spl delta/. Third, we introduce the concept of renamable-Horn regular CNF formula and define another reduction /spl delta/' that transforms a renamable-Horn instance /spl Gamma/ of Regular-SAT into a renamable-Horn instance /spl Gamma//sup /spl delta/'/ of SAT\u00a0\u2026", "num_citations": "48\n", "authors": ["665"]}
{"title": "Verification of switch-level designs with many-valued logic\n", "abstract": " This paper is an approach to automated verification of circuits represented as switch-level designs. Switch-level models (SLM) are a well-established framework for modelling low-level properties of circuits. We use many-valued prepositional logic to represent a suitable variant of SLM. Logical properties of circuits (gate-level) can be expressed in a standard way in the same logic. As a result we can express soundness of switch-level designs wrt to gate-level specifications as many-valued deduction problems. Recent advances in many-valued theorem proving indicate that it is possible to handle real life examples. We report first results obtained with an experimental theorem prover.", "num_citations": "47\n", "authors": ["665"]}
{"title": "Commodious axiomatization of quantifiers in multiple-valued logic\n", "abstract": " We provide tools for a concise axiomatization of a broad class of quantifiers in many-valued logic, so-called distribution quantifiers. Although sound and complete axiomatizations for such quantifiers exist, their size renders them virtually useless for practical purposes. We show that for quantifiers based on finite distributive lattices compact axiomatizations can be obtained schematically. This is achieved by providing a link between skolemized signed formulas and filters/ideals in Boolean set lattices. Then lattice theoretic tools such as Birkhoff's representation theorem for finite distributive lattices are used to derive tableau-style axiomatizations of distribution quantifiers.", "num_citations": "41\n", "authors": ["665"]}
{"title": "Proof theory of many-valued logic\u2014linear optimization\u2014logic design: Connections and interactions\n", "abstract": " In this paper proof theory of many-valued logic is connected with areas outside of logic, namely, linear optimization and computer aided logic design. By stating these not widely-known connections explicitly, I want to encourage interaction between the mentioned disciplines. Once familiar with the others\u2019 terminology, I believe that the respective communities can greatly benefit from each other.", "num_citations": "39\n", "authors": ["665"]}
{"title": "Deductive software verification: From pen-and-paper proofs to industrial tools\n", "abstract": " Deductive software verification aims at formally verifying that all possible behaviors of a given program satisfy formally defined, possibly complex properties, where the verification process is based on logical inference. We follow the trajectory of the field from its inception in the late 1960s via its current state to its promises for the future, from pen-and-paper proofs for programs written in small, idealized languages to highly automated proofs of complex library or system code written in mainstream languages. We take stock of the state-of-art and give a list of the most important challenges for the further development of the field of deductive software verification.", "num_citations": "34\n", "authors": ["665"]}
{"title": "Formal modeling of resource management for cloud architectures: An industrial case study\n", "abstract": " We show how aspects of performance, resource consumption, and deployment on the cloud can be formally modeled for an industrial case study of a distributed system, using the abstract behavioral specification language ABS. These non-functional aspects are integrated with an existing formal model of the functional system behavior, supporting a separation of concerns between the functional and non-functional aspects in the integrated model. The ABS model is parameterized with respect to deployment scenarios which capture different application-level management policies for virtualized resources. The model is validated against the existing system\u2019s performance characteristics and used to simulate and compare deployment scenarios on the cloud.", "num_citations": "33\n", "authors": ["665"]}
{"title": "Analytic tableaux\n", "abstract": " The aim of this chapter is twofold: first, introducing the basic concepts of analytic tableaux and, secondly, presenting state-of-the-art techniques for using non-clausal tableaux in automated deduction. An important point involves problems arising with implementing tableau calculi, in particular with designing a deterministic proof procedure (although no concrete implementation is presented). The most important optimizations of analytic tableaux are discussed; but there are too many to give a complete list here. Instead, we present examples for the important types of optimizations, and describe the general techniques for proving soundness and completeness of different tableau variants. In Section 2, we introduce tableaux for full first-order logic, including unifying notation, both ground and free variable versions of tableau rules, and the (non-deterministic) construction of tableau proofs. In Section 3, the semantics of tableaux is defined, which is used to prove soundness and completeness of free variable tableau in Section 4. Besides the completeness proof that uses the notion of Hintikka sets, an alternative proof is presented, based on an induction on the number of different symbols in the signature (Section 4.3). In Section 5, we discuss difficulties that emerge while resolving the inherent indeterminism of the tableau calculus and in defining a concrete, deterministic (and complete) procedure for systematic free variable tableau proof search. Finally, in Section 6 examples for optimizations of tableaux are presented, in particular those that are different from the corresponding refinements of tableaux for clause logic; it is shown how to adapt the two types\u00a0\u2026", "num_citations": "33\n", "authors": ["665"]}
{"title": "An integrated metamodel for OCL types\n", "abstract": " The main objectives of OCL are to restrict UML models by additional constraints and to clarify the denition of the UML meta model. For certain applications, however, it is crucial for the modeler to have a exible and precisely dened access mechanism to the meta level of UML models. In the present paper we sketch such a modeling scenario and we argue that the current denition of OclType is insucient. We propose an alternative denition based on metamodeling the type system of OCL in such a way that it is fully integrated with the UML meta model. This also claries some ambiguous issues in the OCL language specication and makes the reexion mechanisms in OCL explicit. 1 Introduction The main objectives of OCL are to restrict UML models by additional constraints and to clarify the denition of the UML meta model. OCL is a formal language in the sense that it has a formal syntax given as an EBNF context-free grammar (see the current OCL language specication [4, Section 7])...", "num_citations": "31\n", "authors": ["665"]}
{"title": "Common Syntax of the DFG-Schwerpunktprogramm Deduktion Version 1.5\n", "abstract": " A common exchange format for logic problems to be used by members of the DFG-Schwerpunktprogramm Deduktion is introduced. It is thought to be a format that can easily be parsed such that it forms a compromise between the needs of the different groups. The language is partly more general than other popular exchange formats such as Otter or TPTP in allowing non-clausal and sorted formulae, several proof formats as well as user-defined operators. The latter feature makes it also useful for non-classical logics.", "num_citations": "30\n", "authors": ["665"]}
{"title": "Tutorial: Complexity of many-valued logics\n", "abstract": " Like in the case of classical logic and other non-standard logics, a variety of complexity-related questions can be asked in the context of many-valued logic. Some questions, such as the complexity of the sets of satisfiable and valid formulas in various logics, are completely standard; others, such as the maximal size of representations of many-valued connectives, only make sense in a many-valued context. In this overview I concentrate mainly on two kinds of complexity problems related to many-valued logics: I discuss the complexity of the membership problem in various languages, such as the satisfiable, respectively, the valid formulas in some well-known logics. Two basic proof techniques an presented in some detail: a reduction of many-valued logic to mixed integer programming and a reduction to classical logic.", "num_citations": "29\n", "authors": ["665"]}
{"title": "Verification of safety properties in the presence of transactions\n", "abstract": " The JavaCard transaction mechanism can ensure that a sequence of statements either is executed to completion or is not executed at all. Transactions make verification of JavaCard programs considerably more difficult, because they cannot be formalised in a logic based on pre- and postconditions. The KeY system includes an interactive theorem prover for JavaCard source code that models the full JavaCard standard including transactions. Based on a case study of realistic size we show the practical difficulties encountered during verification of safety properties. We provide an assessment of current JavaCard source code verification, and we make concrete suggestions towards overcoming the difficulties by design for verification. The main conclusion is that largely automatic verification of realistic JavaCard software is possible provided that it is designed with verification in mind from the start.", "num_citations": "28\n", "authors": ["665"]}
{"title": "Symbolic fault injection\n", "abstract": " Fault tolerance mechanisms are a key ingredient of dependable systems. In particular, software-implemented hardware fault tolerance (SIHFT) is gaining in popularity, because of its cost efficiency and flexibility. Fault tolerance mechanisms are often validated using fault injection, comprising a variety of techniques for introducing faults into a system. Traditional fault injection techniques, however, lack coverage guarantees and may fail to activate enough injected faults. In this paper we present a new approach called symbolic fault injection which is targeted at validation of SIHFT mechanisms and is based on the concept of symbolic execution of programs. It can be seen as the extension of a formal technique for formal program verification that makes it possible to evaluate the consequences of all possible faults (of a certain kind) in given memory locations for all possible system inputs. This makes it possible to formally prove properties of fault tolerance mechanisms.", "num_citations": "27\n", "authors": ["665"]}
{"title": "Entwurfsmustergesteuerte Erzeugung von OCL-Constraints\n", "abstract": " Eine der gr\u00f6\u00dften H\u00fcrden auf dem Weg zur formalen Verifikation von Software ist die Erstellung und Validierung der hierf\u00fcr notwendigen formalen Spezifikation. Der Erfolg formaler Methoden bei der industriellen Softwareerstellung h\u00e4ngt also davon ab, ob es zum einen gelingt, die notwendigen Zugangsvoraussetzungen f\u00fcr die Erstellung und Verwendung formaler Objekte gering genug zu machen, und zum anderen davon, formale und informelle Softwaremodelle m\u00f6glichst eng zu integrieren. F\u00fcr letzteres bietet die weithin verwendete, objektorientierte Modellierungssprache Unified Modeling Language (UML) einen guten Ansatzpunkt durch ihre semi-formale Untersprache Object Constraint Language (OCL). In der vorliegenden Arbeit zeigen wir, da\u00df es auch f\u00fcr Programmierer ohne formalen Hintergrund prinzipiell m\u00f6glich ist, formale Teilspezifikationen in OCL zu erstellen. Dies erfolgt durch Auswahl\u00a0\u2026", "num_citations": "27\n", "authors": ["665"]}
{"title": "Uniform modeling of railway operations\n", "abstract": " We present a comprehensive model of railway operations written in the abstract behavioral specification (ABS) language. The model is based on specifications taken from the rulebooks of Deutsche Bahn AG. It is statically analyzable and executable, hence allows to use static and dynamic analysis within one and the same formalism. We are able to combine aspects of micro- and macroscopic modeling and provide a way to inspect changes in the rulebooks. We illustrate the static analysis capability by a safety analysis based on invariant reasoning that only relies on assumptions about the underlying railway infrastructure instead of explicitly exploring the state space. A concrete infrastructure layout and train schedule can be used as input to the model to examine dynamic properties such as delays. We illustrate the capability for dynamic analysis by demonstrating the effect that different ways of dealing with\u00a0\u2026", "num_citations": "26\n", "authors": ["665"]}
{"title": "The tableau-based theorem prover 3TAP for multiple-valued logics\n", "abstract": " 3T4P is an acronym for 3-valued tableau-based theorem prover. It is based on the method of analytic tableaux. 3TAP has been developed at the University of Karlsruhe in cooperation with the Institute for Knowledge Based Systems of IBM Germany in Heidelberg. Despite its name 3T4p is able to deal with\" classical\"--ie twovalued--first-order predicate logic as well as with any finite-valued first-order logic, provided the semantics is specified by truth-tables. Currently implemented versions are working for two-valued and for a certain three-valued first-order predicate logic, which is a variant of the strong Kleene logic, see [3]. The multiple-valued version implements the concept of generalized signs. These may be seen as sets of ordinary tableau signs or prefixes, see [6] and [7] for details. Without generalized signs one has to build a separate tableau for each non-designated sign to refute a formula. 3T4p needs to close\u00a0\u2026", "num_citations": "26\n", "authors": ["665"]}
{"title": "The 2-SAT problem of regular signed CNF formulas\n", "abstract": " Signed conjunctive normal form (signed CNF) is a classical conjunctive clause form using a generalized notion of literal, called signed atom. A signed atom is an expression of the form S:p, where p is a classical atom and S, its sign, is a subset of a domain N. The informal meaning is \"p rakes one of the values in S \" Applications for deduction in signed logics derive from those of annotated logic programming (e.g., mediated deductive databases), constraint programming (e.g., scheduling), and many-valued logics (e.g., natural language processing). The central role of signed CNF justifies a detailed study of its subclasses, including algorithms for and complexities of associated SAT problems. Continuing previous work (1999), in this paper we present new results on the complexity of the signed 2-SAT problem; i.e., the case in which all clauses of a signed CNF formula have at most two literals.", "num_citations": "25\n", "authors": ["665"]}
{"title": "Formal modeling and analysis of railway operations with active objects\n", "abstract": " We present a comprehensive model of railway operations written in the active object language ABS. The model is based on specifications taken from the rulebooks of Deutsche Bahn AG. It is statically analyzable and executable, hence allows to use static and dynamic analysis within one and the same formalism. We are able to combine aspects of micro- and macroscopic modeling and provide a way to inspect changes in the rulebooks. We illustrate the static analysis capability by a safety analysis based on invariant reasoning that only relies on assumptions about the underlying railway infrastructure instead of explicitly exploring the state space. A concrete infrastructure layout and train schedule can be used as input to the model to examine dynamic properties such as delays. We illustrate the capability for dynamic analysis by demonstrating the effect that different ways of dealing with faulty signals have on delays\u00a0\u2026", "num_citations": "24\n", "authors": ["665"]}
{"title": "History-based specification and verification of scalable concurrent and distributed systems\n", "abstract": " The ABS modelling language targets concurrent and distributed object-oriented systems. The language has been designed to enable scalable formal verification of detailed executable models. This paper provides evidence for that claim: it gives formal specifications of safety properties in terms of histories of observable communication for ABS models as well as formal proofs of those properties. We illustrate our approach with a case study of a Network-on-Chip packet switching platform. We provide an executable formal model in ABS of a generic  mesh chip with an unbounded number of packets and verify several crucial properties. Our concern is formal verification of unbounded concurrent systems. In this paper we show how scalable verification can be achieved by compositional and local reasoning about history-based specifications of observable behavior.", "num_citations": "24\n", "authors": ["665"]}
{"title": "An interactive verification system based on dynamic logic\n", "abstract": " An interactive verification system based on dynamic logic is presented. This approach allows to strengthen the role of \"dynamic reasoning\", i.e. reasoning in terms of state transitions caused by programs.             The advantages of the approach are: (i) dynamic logic is more expressive than HOARE's logic, e.g. termination and program implications can be expressed; (ii) user-defined rules enable reasoning in a very natural way; (iii) simpler verification conditions are obtained; (iv) many proofs can be performed schematically.             The problem of rule validation is discussed.             An example demonstrates the style of reasoning supported by the system.", "num_citations": "23\n", "authors": ["665"]}
{"title": "The KeY system 1.0 (deduction component)\n", "abstract": " The KeY system is a development of the ongoing KeY project, whose aim is to integrate formal specification and deductive verification into the industrial software engineering processes. The deductive component of the KeY system is a novel interactive /automated prover for first-order Dynamic Logic for Java. The KeY prover features a user-friendly graphical interface, a backtracking-free free-variable sequent calculus, a simple and powerful theory formalization language called \u201ctaclets,\u201d solution procedures for linear and non-linear integer arithmetic, external theorem prover integration, and facilities for proof reuse, among other aspects. The system is publicly available.", "num_citations": "22\n", "authors": ["665"]}
{"title": "Verifying OpenJDK's Sort Method for Generic Collections\n", "abstract": " TimSort is the main sorting algorithm provided by the Java standard library and many other programming frameworks. Our original goal was functional verification of TimSort with mechanical proofs. However, during our verification attempt we discovered a bug which causes the implementation to crash by an uncaught exception. In this paper, we identify conditions under which the bug occurs, and from this we derive a bug-free version that does not compromise performance. We formally specify the new version and verify termination and the absence of exceptions including the bug. This verification is carried out mechanically with KeY, a state-of-the-art interactive verification tool for Java. We provide a detailed description and analysis of the proofs. The complexity of the proofs required extensions and new capabilities in KeY, including symbolic state merging.", "num_citations": "21\n", "authors": ["665"]}
{"title": "Improving temporal logic tableaux using integer constraints\n", "abstract": " It is fairly easy, if somewhat tedious, to prove that our rules preserve satisfiability. Completeness, however, is a much tougher question, and the proof is quite involved. This is one of the reasons why the present note is merely a position paper. The other reason is that so far we did not show that the idea of incorporating linear constraints into temporal logic tableaux is in some sense a real improvement over the current state of the art. On the other hand, we would like to point out at least a few advantages that might be gained from it:                                                                Better performance if compared to conventional temporal tableaux. Admittedly, this is not obvious from the present appearance of the rules, but there is ample room for optimisation, for example, in the synchronisation rules.                                                     Implementing real-time logics like the ones suggested in [1].                                                     Extend\u00a0\u2026", "num_citations": "21\n", "authors": ["665"]}
{"title": "HATS-a formal software product line engineering methodology\n", "abstract": " Trust in software is typically achieved via stabilization efforts over long periods of use. Adaptation to changing circumstances, however, often requires substantial changes to the software. Changing a software system using standard manufacturing processes often results in quality regressions, invalidating trust. Formal methods provide a means for guaranteeing various properties of a software system that increase its trustworthiness. The HATS methodology aims to integrate formal methods for modeling changes of software systems in terms of variability and evolution, while preserving trustworthiness properties. This paper outlines how different formal methods are extended and integrated to build an industrially viable Software Product Line Engineering method for manufacturing highly adaptable and trustworthy software.", "num_citations": "19\n", "authors": ["665"]}
{"title": "Short CNF in finitely-valued logics\n", "abstract": " We present a transformation of formulae from arbitrary finitely-valued logics into a conjunctive normal form based on signed atomic formulae which can be used to syntactically characterize many-valued validity with a simple resolution rule very much like in classical logic. The transformation is always linear with relation to the size of the input, and we define a generalized concept of polarity in order to remove clauses which are not needed in the proof. The transformation rules are based on the concept of \u2019sets-as-signs\u2019 developed earlier by the author in the context of tableau-based deduction in many-valued logics. We claim that the approach presented here is much more efficient than existing approaches to many-valued resolution.", "num_citations": "19\n", "authors": ["665"]}
{"title": "Normal forms for knowledge compilation\n", "abstract": " A class of formulas called factored negation normal form is introduced. They are closely related to BDDs, but there is a DPLL-like tableau procedure for computing them that operates in PSPACE.Ordered factored negation normal form provides a canonical representation for any boolean function. Reduction strategies are developed that provide a unique reduced factored negation normal form. These compilation techniques work well with negated form as input, and it is shown that any logical formula can be translated into negated form in linear time.", "num_citations": "18\n", "authors": ["665"]}
{"title": "Engineering Virtualized Services\n", "abstract": " To foster the industrial adoption of virtualized services, it is necessary to address two important problems:(1) the efficient analysis, dynamic composition and deployment of services with qualitative and quantitative service levels and (2) the dynamic control of resources such as storage and processing capacities according to the internal policies of the services. The position supported in this paper is to overcome these problems by leveraging service-level agreements into software models and resource management into early phases of service design.", "num_citations": "17\n", "authors": ["665"]}
{"title": "Fast subsumption checks using anti-links\n", "abstract": " The concept of anti-link is defined (an anti-link consists of two occurrences of the same literal in a formula), and useful equivalence-preserving operations based on anti-links are introduced. These operations eliminate a potentially large number of subsumed paths in a negation normal form formula. Those anti-links that directly indicate the presence of subsumed paths are characterized. The operations have linear time complexity in the size of that part of the formula containing the anti-link.               The problem of removing all subsumed paths in an NNF formula is shown to be NP-hard, even though such formulas may be small relative to the size of their path sets. The general problem of determining whether there exists a pair of subsumed paths associated with an arbitrary anti-link is shown to be NP-complete. Additional techniques that generalize the concept of pure literals are introduced and are also\u00a0\u2026", "num_citations": "15\n", "authors": ["665"]}
{"title": "Semantic tableaux with ordering restrictions\n", "abstract": " The aim of this paper is to make restriction strategies based on orderings of the Herbrand universe available for semantic tableau-like calculi as well. A marriage of tableaux and ordering restriction strategies seems to be most promising in applications where generation of counter examples is required. In this paper, starting out from semantic trees, we develop a formal tool called refutation graphs, which (i) serves as a basis for completeness proofs of both resolution and tableaux, and (ii) is compatible with so-called A-ordering restrictions. The main result is a first-order ground tableau procedure complete for A-ordering restrictions.", "num_citations": "15\n", "authors": ["665"]}
{"title": "A new translation from deduction into integer programming\n", "abstract": " We generalize propositional analytic tableaux for classical and many-valued logics to constraint tableaux. We show that this technique provides a new translation from deduction into integer programming. The main advantages are (i) an efficient satisfiability checking procedure for classical and, for the first time, for a wide range of many-valued, including infinitely-valued propositional logics; (ii) a new point of view on classifying complexity of many-valued logics; (iii) easy NPcontainment proofs for many-valued logics.", "num_citations": "15\n", "authors": ["665"]}
{"title": "Resource-aware applications for the cloud\n", "abstract": " Resource-Aware Applications for the Cloud Page 1 Resource-Aware Applications for the Cloud Reiner H\u00e4hnle 1 and Einar Broch Johnsen 2 1 Technical University of Darmstadt, Germany haehnle@cs.tu-darmstadt.de 2 University of Oslo, Norway einarj@ifi.uio.no Making'full'usage'of'the'potential'of'virtualized'computation'requires'nothing'' less'than'to'rethink'the'way'in'which'we'design'and'develop'software.' ! Capitalizing*on*the*Cloud* The!planet\u2019s!data!storage!and!processing!is!about!to!move!into!the!clouds.!This!has! the!potential!to!revolutionize!how!we!will!interact!with!computers!in!the!future.!A! cloud!consists!of!virtual!computers!that!can!only!be!accessed!remotely.!It!is!not!a! physical!computer,!you!do!not!necessarily!know!where!it!is,!but!you!can!use!it!to! store!and!process!your!data!and!you!can!access!it!at!any!time!from!your!regular! computer.!If!you!still!have!an!old?fashioned!computer,!that!is.!You!might!as!well! \u2026", "num_citations": "14\n", "authors": ["665"]}
{"title": "A-ordered tableaux\n", "abstract": " In resolution proof procedures refinements based on A-orderings of literals have a long tradition and are well investigated. In tableau proof procedures such refinements were only recently introduced by the authors of the present paper. In this paper we prove the following results: we give a completeness proof of A-ordered ground clause tableaux which is a lot easier to follow than the one published previously. The technique used in the proof is extended to the non-clausal case as well as to the non-ground case and we introduce an ordered version of Hintikka sets that shares the model existence property of standard Hintikka sets. We show that A-ordered tableaux are a proof confluent refinement of tableaux and that A-ordered tableaux together with well-known connection refinements yield an incomplete proof procedure. We introduce A-ordered first-order NNF tableaux, prove their completeness, and we\u00a0\u2026", "num_citations": "14\n", "authors": ["665"]}
{"title": "Abstract Execution\n", "abstract": " We propose a new static software analysis principle called Abstract Execution, generalizing Symbolic Execution: While the latter analyzes all possible execution paths of a specific program, Abstract Execution analyzes a partially unspecified program by permitting abstract symbols representing unknown contexts. For each abstract symbol, we faithfully represent each possible concrete execution resulting from its substitution with concrete code. There is a wide range of applications of Abstract Execution, especially for verifying relational properties of schematic programs. We implemented Abstract Execution in a deductive verification framework and proved correctness of eight well-known statement-level refactoring rules, including two with loops. For each refactoring we characterize the preconditions that make it semantics-preserving. Most preconditions are not mentioned in the literature.", "num_citations": "13\n", "authors": ["665"]}
{"title": "Integration of a security type system into a program logic\n", "abstract": " Type systems and program logics are often conceived to be at opposing ends of the spectrum of formal software analyses. In this paper we show that a flow-sensitive type system ensuring non-interference in a simple while language can be expressed through specialised rules of a program logic. In our framework, the structure of non-interference proofs resembles the corresponding derivations in a recent security type system, meaning that the algorithmic version of the type system can be used as a proof procedure for the logic. We argue that this is important for obtaining uniform proof certificates in a proof-carrying code framework. We discuss in which cases the interleaving of approximative and precise reasoning allows us to deal with delimited information release. Finally, we present ideas on how our results can be extended to encompass features of realistic programming languages like Java.", "num_citations": "13\n", "authors": ["665"]}
{"title": "Efficient deduction in many-valued logics\n", "abstract": " This paper tries to identify the basic problems encountered in automated theorem proving in many-valued logics and demonstrates to which extent they can be currently solved. To this end a number of recently developed techniques are reviewed. We list the avenues of research in many-valued theorem proving that are in our eyes the most promising.< >", "num_citations": "13\n", "authors": ["665"]}
{"title": "Integration of a security type system into a program logic\n", "abstract": " Type systems and program logics are often thought to be at opposing ends of the spectrum of formal software analyses. In this paper we show that a flow-sensitive type system ensuring non-interference in a simple while-language can be expressed through specialised rules of a program logic. In our framework, the structure of non-interference proofs resembles the corresponding derivations in a state-of-the-art security type system, meaning that the algorithmic version of the type system can be used as a proof procedure for the logic. We argue that this is important for obtaining uniform proof certificates in a proof-carrying code framework. We discuss in which cases the interleaving of approximative and precise reasoning allows us to deal with delimited information release. Finally, we present ideas on how our results can be extended to encompass features of realistic programming languages such as Java.", "num_citations": "12\n", "authors": ["665"]}
{"title": "Rule-based simplification of OCL constraints\n", "abstract": " To help designers in writing OCL constraints, we have to construct systems that can generate some of these constraints. This might be done by instantiating templates, by combining prefabricated parts, or by more general computation. Such generated specifications will often contain redundancies that reduce their readability. In this paper, we explore the possibilities of simplifying OCL formulae through the repeated application of simple rules. We discuss the different kinds of rules that are needed, and we describe a prototypical implementation of the approach.", "num_citations": "12\n", "authors": ["665"]}
{"title": "Simplification of many-valued logic formulas using anti-links\n", "abstract": " The theoretical foundations of the many-valued generalization of a technique for simplifying large non-clausal formulas in propositional logic, called removal of anti-links are presented. Possible applications include computation of prime implicates of large non-clausal formulas as required, for example, in diagnosis. With the anti-link technique, one does not compute any normal form of a given formula; rather, one removes certain forms of redundancy from formulas in negation normal form (NNF). Its main advantage is that no clausal normal form has to be computed in order to remove redundant parts of a formula. An anti-link operation on a generic language is defined for expressing many-valued logic formulas called signed NNF and it is shown that all interesting properties of two-valued anti-links generalize to the many-valued setting, although in a non-trivial way.", "num_citations": "12\n", "authors": ["665"]}
{"title": "Locally Abstract, Globally Concrete Semantics of Concurrent Programming Languages\n", "abstract": " Language semantics that is formal and mathematically precise, is the essential prerequisite for the design of logics and calculi that permit automated reasoning about programs. The most popular approach to programming language semantics\u2014small step operational semantics (SOS)\u2014is not modular in the sense that it does not separate conceptual layers in the target language. SOS is also hard to relate formally to program logics and calculi. Minimalist semantic formalisms, such as automata, Petri nets, or -calculus are inadequate for rich programming languages. We propose a new formal trace semantics for a concurrent, active objects language. It is designed with the explicit aim of being compatible with a sequent calculus for a program logic and has a strong model theoretic flavor. Our semantics separates sequential and object-local from concurrent computation: the former yields abstract traces which\u00a0\u2026", "num_citations": "11\n", "authors": ["665"]}
{"title": "Automating verification of loops by parallelization\n", "abstract": " Loops are a major bottleneck in formal software verification, because they generally require user interaction: typically, induction hypotheses or invariants must be found or modified by hand. This involves expert knowledge of the underlying calculus and proof engine. We show that one can replace interactive proof techniques, such as induction, with automated first-order reasoning in order to deal with parallelizable loops, where a loop can be parallelized whenever it avoids dependence of the loop iterations from each other. We develop a dependence analysis that ensures parallelizability. It guarantees soundness of a proof rule that transforms a loop into a universally quantified update of the state change information represented by the loop body. This makes it possible to use automatic first order reasoning techniques to deal with loops. The method has been implemented in the KeY verification tool. We\u00a0\u2026", "num_citations": "11\n", "authors": ["665"]}
{"title": "Ordered tableaux: Extensions and applications\n", "abstract": " In this paper several conceptual extensions to the theory of order-restricted free variable clausal tableaux which was initiated in [9, 8] are presented: atom orderings are replaced by the more general concept of a selection function, the substitutivity condition required for lifting is for certain variants of the calculus replaced by a much weaker assumption, and a first version of order-restricted tableaux with theories is introduced. The resulting calculi are shown to be sound and complete. We report on first experiments made with a prototypical implementation and indicate for which classes of problems order-restricted tableaux calculi are likely to be beneficial.", "num_citations": "11\n", "authors": ["665"]}
{"title": "Formal Methods for Components and Objects: 11th International Symposium, FMCO 2012, Bertinoro, Italy, September 24-28, 2012, Revised Lectures\n", "abstract": " This book constitutes revised lectures from the 11th Symposium on Formal Methods for Components and Object, FMCO 2012, held in Bertinoro, Italy, in September 2012. The 8 lectures featured in this volume are by world-renowned experts within the area of formal models for objects and components. The book provides a unique combination of ideas on software engineering and formal methods which reflect the expanding body of knowledge on modern software systems.", "num_citations": "10\n", "authors": ["665"]}
{"title": "Linearity and regularity with negation normal form\n", "abstract": " Proving completeness of NC-resolution under a linear restriction has been elusive; it is proved here for formulas in negation normal form. The proof uses a generalization of the Anderson\u2013Bledsoe excess literal argument, which was developed for resolution. That result is extended to NC-resolution with partial replacement. A simple proof of the completeness of regular, connected tableaux for formulas in conjunctive normal form is also presented. These techniques are then used to establish the completeness of regular, connected tableaux for formulas in negation normal form.", "num_citations": "10\n", "authors": ["665"]}
{"title": "Machine Learning for Dynamic Software Analysis: Potentials and Limits (Dagstuhl Seminar 16172)\n", "abstract": " This report documents the program and the outcomes of Dagstuhl Seminar 16172\" Machine Learning for Dynamic Software Analysis: Potentials and Limits\". Machine learning is a powerful paradigm for software analysis that provides novel approaches to automating the generation of models and other essential artefacts. This Dagstuhl Seminar brought together top researchers active in the fields of machine learning and software analysis to have a better understanding of the synergies between these fields and suggest new directions and collaborations for future research.", "num_citations": "9\n", "authors": ["665"]}
{"title": "Tableaux+ constraints\n", "abstract": " There is an increasing number of publications in which the analytic tableaux calculus is combined with technology based on constraint solving. Although the details, as well as the purpose of these combinations vary widely, the results are invariably referred to as \u201cconstraint tableaux\u201d or sometimes \u201cconstrained tableaux\u201d. We review some of the combinations and propose a more differentiated nomenclature.", "num_citations": "9\n", "authors": ["665"]}
{"title": "Completeness for linear regular negation normal form inference systems\n", "abstract": " Completeness proofs that generalize the Anderson- Bledsoe excess literal argument are developed for calculi other than resolution. A simple proof of the completeness of regular, connected tableaux for formulas in conjunctive normal form (CNF) is presented. These techniques also provide completeness results for some inference mechanisms that do not rely on clause form. In particular, the completeness of regular, connected tableaux for formulas in negation normal form (NNF), and the completeness of NC-resolution under a linear restriction, are established.", "num_citations": "9\n", "authors": ["665"]}
{"title": "Deductive Verification of Railway Operations\n", "abstract": " We use deductive verification to show safety properties for the railway operations of Deutsche Bahn. We formalize and verify safety properties for a precise, comprehensive model of operational procedures as specified in the rule books, independently of the shape and size of the actual network layout and the number or schedule of trains. We decompose a global safety property into local properties as well as compositionality and well-formedness assumptions. Then we map local state-based safety properties into history-based properties that can be proven with a high degree of automation using deductive verification. We illustrate our methodology with the proof that for any well-formed infrastructure operating according to the regulations of Deutsche Bahn the following safety property holds: whenever a train leaves a station, the next section is free and no other train on the same line runs in the opposite\u00a0\u2026", "num_citations": "8\n", "authors": ["665"]}
{"title": "Automated Planning of ETCS Tracks\n", "abstract": " Planning of railway tracks at Deutsche Bahn (DB) so far is done manually by planning experts with the help of CAD tools. This incurs substantial cost and planning time which is exacerbated by the complex planning rules laid down in ETCS regulations mandatory for new tracks. In a project performed for DB Netz AG we explore the possibility of automating a large part of the ETCS rail track planning process. We report on our experience in building a prototypic automated ETCS planning tool. It takes a standardized object-oriented track model as input and provides output in the same format with all required ETCS track elements placed at their correct position. The tool can be integrated into manual planning processes and allows manual fine-tuning. Our approach uses algorithmic sequencing of formalized planning rules based on the knowledge and best practices obtained from experienced track planners\u00a0\u2026", "num_citations": "7\n", "authors": ["665"]}
{"title": "Modular, correct compilation with automatic soundness proofs\n", "abstract": " Formal verification of compiler correctness requires substantial effort. A particular challenge is lack of modularity and automation. Any change or update to the compiler can render existing proofs obsolete and cause considerable manual proof effort. We propose a framework for automatically proving the correctness of compilation rules based on simultaneous symbolic execution for the source and target language. The correctness of the whole system follows from the correctness of each compilation rule. To support a new source or target language it is sufficient to formalize that language in terms of symbolic execution, while the corresponding formalization of its counterpart can be re-used. The correctness of translation rules can be checked automatically. Our approach is based on a reduction of correctness assertions to formulas in a program logic capable of symbolic execution of abstract programs. We\u00a0\u2026", "num_citations": "7\n", "authors": ["665"]}
{"title": "Towards incremental validation of railway systems\n", "abstract": " We propose to formally model requirements and interoperability constraints among components of a railway system to enable automated, incremental analysis and validation mechanisms. The goal is to provide the basis for a technology that can drastically reduce the time and cost for certification by making it possible to trace changes from requirements via design to implementation.", "num_citations": "7\n", "authors": ["665"]}
{"title": "Using a software testing technique to improve theorem proving\n", "abstract": " Most efforts to combine formal methods and software testing go in the direction of exploiting formal methods to solve testing problems, most commonly test case generation. Here we take the reverse viewpoint and show how the technique of partition testing can be used to improve a formal proof technique (induction for correctness of loops). We first compute a partition of the domain of the induction variable, based on the branch predicates in the program code of the loop we wish to prove. Based on this partition we derive a partitioned induction rule, which is (hopefully) easier to use than the standard induction rule. In particular, with an induction rule that is tailored to the program to be verified, less user interaction can be expected to be required in the proof. We demonstrate with a number of examples the practical efficiency of our method.", "num_citations": "7\n", "authors": ["665"]}
{"title": "Tableau-based Theorem Proving\n", "abstract": " Tableau-based approaches to automated deduction have gained a lot of ground in recent years after many years during which they have been mainly considered for teaching and in proof theoretic studies. One reason for this development is that competitive implementations of rst-order logic tableau provers are now available, but another reason for the renewed interest is the large number of applications for deduction in non-classical in linguistics, intelligent agents, knowledge representation to name just a few elds. I will present various tableau-related calculi in a uniform, modern framework with new, concise completeness proofs. Important implementations as well as basic techniques for accommodating non-classical aspects are discussed as well. A brief introduction into the required concepts of computational logic is included.", "num_citations": "7\n", "authors": ["665"]}
{"title": "Prototyping Formal System Models with Active Objects\n", "abstract": " We propose active object languages as a development tool for formal system models of distributed systems. Additionally to a formalization based on a term rewriting system, we use established Software Engineering concepts, including software product lines and object orientation that come with extensive tool support. We illustrate our modeling approach by prototyping a weak memory model. The resulting executable model is modular and has clear interfaces between communicating participants through object-oriented modeling. Relaxations of the basic memory model are expressed as self-contained variants of a software product line. As a modeling language we use the formal active object language ABS which comes with an extensive tool set. This permits rapid formalization of core ideas, early validity checks in terms of formal invariant proofs, and debugging support by executing test runs. Hence, our approach supports the prototyping of formal system models with early feedback.", "num_citations": "6\n", "authors": ["665"]}
{"title": "Connecting OCL with the rest of the world\n", "abstract": " The paper addresses the problems of making program specifications written in OCL easier to read and to maintain. The solution proposed is to use the grammar formalism GF to define a high-level abstract grammar of specifications with translations into OCL, logic, and natural languages. A prototype of the solution is illustrated by an example.", "num_citations": "6\n", "authors": ["665"]}
{"title": "Model generation theorem proving with interval constraints\n", "abstract": " We investigate how the deduction paradigm of model generation theorem proving can be enhanced with interval-and extraval-based constraints leading to more efficient model generation in for some finite domain problems. 1 Model Generation Model generation (MG) is a sound and complete inference rule for first-order predicate logic for input in conjunctive normal form (CNF). One can view it as a positive literal restriction of clausal semantic tableaux. Manthey & Bry [9] gave a concise implementation of a variant of model generation in Prolog. In logic programming and deductive databases it is common to impose the range-restrictedness condition on first-order clauses: 1 Definition 1 A first-order clause C! D is range-restricted if all variables that occur in D occur also in C. Observe that a range-restricted positive clause must be ground. In the range-restricted case model generation can be defined as follows: 2 Definition 2 Let S= fC 1;:::; Cm g be a first-order CNF formula...", "num_citations": "6\n", "authors": ["665"]}
{"title": "On anti-links\n", "abstract": " The concept of anti-link is defined, and useful equivalence-preserving operations on propositional formulas based on anti-links are introduced. These operations eliminate a potentially large number of subsumed paths in a negation normal form formula. The operations have linear time complexity in the size of that part of the formula containing the anti-link.             These operations are useful for prime implicant/implicate algorithms because most of the computational effort in such algorithms is spent on subsumption checks.", "num_citations": "6\n", "authors": ["665"]}
{"title": "Asynchronous cooperative contracts for cooperative scheduling\n", "abstract": " Formal specification of multi-threaded programs is notoriously hard, because thread execution may be preempted at any point. In contrast, abstract concurrency models such as actors seriously restrict concurrency to obtain race-free programs. Languages with cooperative scheduling occupy a middle ground between these extremes by explicit scheduling points. They have been used to model complex, industrial concurrent systems. This paper introduces cooperative contracts, a contract-based specification approach for asynchronous method calls in presence of cooperative scheduling. It permits to specify complex concurrent behavior succinctly and intuitively. We design a compositional program logic to verify cooperative contracts and discuss how global analyses can be soundly integrated into the program logic.", "num_citations": "4\n", "authors": ["665"]}
{"title": "Array abstraction with symbolic pivots\n", "abstract": " In this paper we present a novel approach to automatically generate invariants for loops manipulating arrays. The intention is to achieve deductive program verification without the need for user-specified loop invariants. Many loops iterate and manipulate collections. Finding useful, ie, sufficiently precise invariants for those loops is a challenging task, in particular, if the iteration order is complex. Our approach partitions an array and provides an abstraction for each of these partitions. Symbolic pivot elements are used to compute the partitions. In addition we integrate a faithful and precise program logic for sequential (Java) programs with abstract interpretation using an extensible multi-layered framework to compute array invariants. The presented approach has been implemented.", "num_citations": "4\n", "authors": ["665"]}
{"title": "Verification of variable software: an experience report\n", "abstract": " We report on our experiences with formal specification and verification of variable and customizable software realized in a software product family architecture using the Java Modeling Language (JML) and the KeY verification system. Software product families can be adapted to different deployment scenarios and provide instantiable feature sets as requested by the customer. Along a small case study we explore how to generate JML specifications for/from a given feature configuration and report on verification attempts of selected methods of the derived product. We identify challenges that need to be solved to allow scalable specification and verification of variable software.", "num_citations": "4\n", "authors": ["665"]}
{"title": "Fair constraint merging tableaux in lazy functional programming style\n", "abstract": " Constraint merging tableaux maintain a system of all closing substitutions of all subtableau up to a certain depth, which is incrementally increased. This avoids backtracking as necessary in destructive first order free variable tableaux. The first successful implementation of this paradigm was given in an object-oriented style. We analyse the reasons why lazy functional implementations so far were problematic (although appealing), and we give a solution. The resulting implementation in Haskell is compact and modular.", "num_citations": "4\n", "authors": ["665"]}
{"title": "Ordered resolution vs. connection graph resolution\n", "abstract": " Connection graph resolution (cg-resolution) was introduced by Kowalski as a means of restricting the search space of resolution. Several researchers expected unrestricted connection graph (cg) resolution to be strongly complete until Eisinger proved that it was not. In this paper, ordered resolution is shown to be a special case of cg-resolution, and that relationship is used to prove that ordered cg-resolution is strongly complete. On the other hand, ordered resolution provides little insight about completeness of first order cg-resolution and little about the establishment of strong completeness from completeness. A first order version of Eisinger\u2019s cyclic example is presented, illustrating the difficulties with first order cg resolution. But resolution with selection functions does yield a simple proof of strong cg-completeness for the unit-refutable class.", "num_citations": "4\n", "authors": ["665"]}
{"title": "Safer parallelization\n", "abstract": " Adapting sequential legacy software to parallel environments can not only save time and money, but additionally avoids the loss of valuable domain knowledge hidden in existing code. A common parallelization approach is the use of standardized parallel design patterns, which allow making best use of parallel programming interfaces such as OpenMP. When such patterns cannot be implemented directly, it can be necessary to apply code transformations beforehand to suitably re-shape the input program. In this paper, we describe how we used Abstract Execution, a semi-automatic program proving technique for second-order program properties, to formally prove the conditional correctness of the restructuring techniques CU Repositioning, Loop Splitting and Geometric Decomposition\u2014for all input programs. The latter two techniques require an advanced modeling technique based on families of abstract location\u00a0\u2026", "num_citations": "3\n", "authors": ["665"]}
{"title": "Analysis of SLA Compliance in the Cloud--An Automated, Model-based Approach\n", "abstract": " Service Level Agreements (SLA) are commonly used to specify the quality attributes between cloud service providers and the customers. A violation of SLAs can result in high penalties. To allow the analysis of SLA compliance before the services are deployed, we describe in this paper an approach for SLA-aware deployment of services on the cloud, and illustrate its workflow by means of a case study. The approach is based on formal models combined with static analysis tools and generated runtime monitors. As such, it fits well within a methodology combining software development with information technology operations (DevOps).", "num_citations": "3\n", "authors": ["665"]}
{"title": "Model generation theorem proving with finite interval constraints\n", "abstract": " Model generation theorem proving (MGTP) is a class of deduction procedures for first-order logic that were successfully used to solve hard combinatorial problems. For some applications the representation of models in MGTP and its extension CMGTP is too redundant. Here we suggest to extend members of model candidates in such a way that a predicate p can have not only terms as arguments, but at certain places also subsets of totally ordered finite domains. The ensuing language and deduction system relies on constraints based on finite intervals in totally ordered sets and is called IV-MGTP. It is related to constraint programming and many-valued logic, but differs significantly from either. We show soundness and completeness of IV-MGTP. First results with our implementation show considerable potential of the method.", "num_citations": "3\n", "authors": ["665"]}
{"title": "Theorem Proving with Analytic Tableaux and Related Methods: 4th International Workshop, TABLEAUX-95, Schlo\u00df Rheinfels, St. Goar, Germany, May 7-10, 1995. Proceedings\n", "abstract": " This volume constitutes the proceedings of the 4th International Workshop on Theorem Proving with Analytic Tableaux and Related Methods, TABLEAU'95, held at Schlo\u00df Rheinfels, St. Goar, Germany in May 1995. Originally tableau calculi and their relatives were favored primarily as a pedagogical device because of their advantages at the presentation level. The 23 full revised papers in this book bear witness that these methods have now gained fundamental importance in theorem proving, particularly as competitors for resolution methods. The book is organized in sections on extensions, modal logic, intuitionistic logic, the connection method and model elimination, non-clausal proof procedures, linear logic, higher-order logic, and applications", "num_citations": "3\n", "authors": ["665"]}
{"title": "Automated Model Extraction: From Non-deterministic C Code to Active Objects\n", "abstract": " The C programming language is well-known to have a large amount of underspecified behavior that often results in non-determinism even of sequential programs. In many application areas, not necessarily safety-critical ones, this is highly undesirable. A number of approaches and tools that statically analyze such behavior have been suggested, but they suffer from a high number of false positives and negatives. We present a novel model-based approach to analyzing non-determinism that works by automatic extraction of a faithful model of a given C program in a concurrent active object language. The extracted model renders any non-deterministic behavior of the C program in terms of explicit concurrency. This opens the door to global, semantic analyses. We give a fully formal account of the model extraction process and present an experimental evaluation of its implementation in the model extraction tool C2ABS.", "num_citations": "2\n", "authors": ["665"]}
{"title": "The Trace Modality\n", "abstract": " We propose the trace modality, a concept to uniformly express a wide range of program verification problems. To demonstrate its usefulness, we formalize several program verification problems in it: Functional Verification, Information Flow Analysis, Temporal Model Checking, Program Synthesis, Correct Compilation, and Program Evolution. To reason about the trace modality, we translate programs and specifications to regular symbolic traces and construct simulation relations on first-order symbolic automata. The idea with this uniform representation is that it helps to identify synergy potential\u2014theoretically and practically\u2014between so far separate verification\u00a0approaches.", "num_citations": "2\n", "authors": ["665"]}
{"title": "Modeling Non-deterministic C Code with Active Objects\n", "abstract": " Cheap and ubiquitous availability of multi-processor hardware provides a strong incentive to parallelize existing software. We aim to annotate existing sequential applications written in C with OpenMP directives that can be processed by compilers on high performance parallel computers. We adopt a model-based approach, where from sequential C-code a software model is extracted in a largely automatic fashion. The target is the modeling language ABS (Abstract Behavioral Specification), an active objects-language with formal semantics. ABS has been designed to be statically analyzable. We focus on the first stages of model-based parallelization: model extraction and validation. We define a behavior-preserving, fully automatic translation of a large fragment of sequential C that explicitly renders all possible execution sequences, then use automated test case generation to produce validation test cases.", "num_citations": "2\n", "authors": ["665"]}
{"title": "Fundamental Approaches to Software Engineering: 22nd International Conference, FASE 2019, Held as Part of the European Joint Conferences on Theory and Practice of Software\u00a0\u2026\n", "abstract": " This book is Open Access under a CC BY licence. This book constitutes the proceedings of the 22nd International Conference on Fundamental Approaches to Software Engineering, FASE 2019, which took place in Prague, Czech Republic in April 2019, held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2019. The 24 papers presented in this volume were carefully reviewed and selected from 94 submissions. The papers are organized in topical sections named: software verification; model-driven development and model transformation; software evolution and requirements engineering; specification, design, and implementation of particular classes of systems; and software testing.", "num_citations": "2\n", "authors": ["665"]}
{"title": "24 Challenges in Deductive Software Verification\n", "abstract": " Deductive software verification aims at formally verifying that all possible behaviors of a given program satisfy formally defined, complex properties, where the verification process is based on logical inference. We list the most important challenges for the further development of the field.", "num_citations": "2\n", "authors": ["665"]}
{"title": "Quo Vadis Formal Verification?\n", "abstract": " The KeY System has been developed for over a decade. During this time, the field of Formal Methods as well as Computer Science in general has changed considerably. Based on an analysis of this trajectory of changes we argue why, after all these years, the project is still relevant and what the challenges in the coming years might be. At the same time we give a brief overview of the various tools based on KeY technology and explain their architecture.", "num_citations": "2\n", "authors": ["665"]}
{"title": "Tests and Proofs: Second International Conference, TAP 2008, Prato, Italy, April 9-11, 2008, Proceedings\n", "abstract": " This volume contains the research papers, invited papers, and abstracts of-torials presented at the Second International Conference on Tests and Proofs (TAP 2008) held April 9\u201311, 2008 in Prato, Italy. TAP was the second conference devoted to the convergence of proofs and tests. It combines ideas from both areasfor the advancement of softwarequality. To provethe correctnessof a programis to demonstrate, through impeccable mathematical techniques, that it has no bugs; to test a programis to run it with the expectation of discovering bugs. On the surface, the two techniques seem contradictory: if you have proved your program, it is fruitless to comb it for bugs; and if you are testing it, that is surely a sign that you have given up on anyhope of proving its correctness. Accordingly, proofs and tests have, since the onset of software engineering research, been pursued by distinct communities using rather di? erent techniques and tools. And yet the development of both approaches leads to the discovery of c-mon issues and to the realization that each may need the other. The emergence of model checking has been one of the? rst signs that contradiction may yield to complementarity, but in the past few years an increasing number of research e? orts have encountered the need for combining proofs and tests, dropping e-lier dogmatic views of their incompatibility and taking instead the best of what each of these software engineering domains has to o? er.", "num_citations": "2\n", "authors": ["665"]}
{"title": "Proof confluent tableau calculi\n", "abstract": " A tableau calculus is proof confluent if every partial tableau proof for an unsatisfiable formula can be extended to a closed tableau. A rule application may be redundant but it can never prevent the construction of a proof; there are no \u201cdead ends\u201d in the proof search. Proof confluence is a prerequisite of (a) backtracking-free proof search and (b) the generation of counter examples to non-theorems.", "num_citations": "2\n", "authors": ["665"]}
{"title": "Automated model analysis tools and techniques presented at FASE 2019\n", "abstract": " This special issue contains substantially revised and extended versions of some of the best papers presented at the 22nd International Conference on Fundamental Approaches to Software Engineering in 2019.  All papers share the common theme that they are either concerned with model-based analysis of systems or they develop methods in its service.", "num_citations": "1\n", "authors": ["665"]}
{"title": "An Architectural Pattern to Realize Multi Software Product Lines in Java\n", "abstract": " We present a realization of multi software product lines in the Java programming language that permits full interoperability and hierarchical dependencies among multiple product variants. This concept, called variability modules (VM), is implemented in terms of an architectural pattern in Java and does not require any pre-processing or language extension. It can be used with any Java development environment. The VM architectural pattern comes with a dedicated UML profile, which makes it possible to present variability to non-technical stakeholders. We evaluate our approach with the help of a real-world case study.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Certified Abstract Cost Analysis\n", "abstract": " A program containing placeholders for unspecified statements or expressions is called an abstract (or schematic) program. Placeholder symbols occur naturally in program transformation rules, as used in refactoring, compilation, optimization, or parallelization. We present a generalization of automated cost analysis that can handle abstract programs and, hence, can analyze the impact on the cost of program transformations. This kind of relational property requires provably precise cost bounds which are not always produced by cost analysis. Therefore, we certify by deductive verification that the inferred abstract cost bounds are correct and sufficiently precise. It is the first approach solving this problem. Both, abstract cost analysis and certification, are based on quantitative abstract execution (QAE) which in turn is a variation of abstract execution, a recently developed symbolic execution technique for abstract programs. To realize QAE the new concept of a cost invariant is introduced. QAE is implemented and runs fully automatically on a benchmark set consisting of representative optimization rules.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Who Carries the Burden of Modularity?\n", "abstract": " Modularity and compositionality in verification frameworks occur within different contexts: the model that is the verification target, the specification of the stipulated properties, and the employed verification principle. We give a representative overview of mechanisms to achieve modularity and compositionality along the three mentioned contexts and analyze how mechanisms in different contexts are related. In many verification frameworks one of the contexts carries the main burden. It is important to clarify these relations to understand the potential and limits of the different modularity mechanisms.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Behavioral Contracts for Cooperative Scheduling\n", "abstract": " Formal specification of multi-threaded programs is notoriously hard, because thread execution may be preempted at any point. In contrast, abstract concurrency models such as actors seriously restrict concurrency to obtain race-free programs. Languages with cooperative scheduling occupy a middle ground between these extremes by explicit scheduling points. We introduce cooperative contracts, a contract-based specification approach designed for cooperative scheduling. It permits to specify complex concurrent behavior succinctly. Cooperative contracts are formalized as behavioral contracts in a compositional behavioral program logic in which they can be formally verified.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Locally Static, Globally Dynamic Session Types for Active Objects\n", "abstract": " Active object languages offer an attractive trade-off between low-level, preemptive concurrency and fully distributed actors: syntactically identifiable atomic code segments and asynchronous calls are the basis of cooperative concurrency, still permitting interleaving, but nevertheless being mechanically analyzable. The challenge is to reconcile local static analysis of atomic segments with the global scheduling constraints it depends on. Here, we propose an approximate, hybrid approach; At compile-time we perform a local static analysis: later, any run not complying to a global specification is excluded via runtime checks. That specification is expressed in a type-theoretic language inspired by session types. The approach reverses the usual (first global, then local) order of analysis and, thereby, supports analysis of open distributed systems.", "num_citations": "1\n", "authors": ["665"]}
{"title": "A Model-Centric Approach to the Design of Resource-Aware Cloud Applications\n", "abstract": " The planet\u2019s data storage and processing are moving into the clouds. This has the potential to revolutionize how we interact with computers in the future. A cloud consists of virtual computers that can only be accessed remotely. It is not a physical computer, and you do not necessarily know where it is, but you can use it to store and process your data and can access it at any time from your regular computer. If you still have an old-fashioned computer, that is. You might as well access your data or applications through your mobile device, for example while sitting on the bus.Cloud-based data processing, or cloud computing, is more than just a convenient solution for individuals on the move. Even though privacy of data is still an issue, the cloud is already an economically attractive model for a startup, a small to medium enterprise (SME), or simply for a student who develops an app as a side project, due to an undeniable added value and compelling business drivers [1]. One such driver is elasticity: businesses pay for computing resources when these are needed, and avoid major upfront investments for resource provisioning. Additional resources such as processing power or memory can be added to a virtual computer on the fly, or an additional virtual computer can be provided to the client application. Going beyond shared storage, the main potential in cloud computing lies in its scalable virtualized framework for data processing, which becomes a shared computing facility for multiple devices. If a service uses cloudbased processing, its capacity can be adjusted automatically when new users arrive. Another driver isagility: new services can be\u00a0\u2026", "num_citations": "1\n", "authors": ["665"]}
{"title": "The NoC verification case study with KeY-ABS\n", "abstract": " We present a case study on scalable formal verification of distributed systems that involves a formal model of a Network-on-Chip (NoC) packet switching platform. We provide an executable model of a generic m\u00d7 n mesh chip with unbounded number of packets, the formal specification of certain safety properties, and formal proofs that the model fulfills these properties. The modeling has been done in ABS, a language that is intended to permit scalable verification of detailed, precisely modeled, executable, concurrent systems. Our paper shows that this is indeed possible and so advances the state-of-art verification of NoC systems. It also demonstrates that deductive verification is a viable alternative to model checking for the verification of unbounded concurrent systems that can effectively deal with state explosion.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Deductive verification of c programs in key\n", "abstract": " We present KeY-C: a tool for deductive verification of C programs. KeY-C allows verification of C programs wrt operation contracts and invariants. It is based on an earlier version of KeY that supports Java Card. In this paper we outline syntax, semantics, and calculus of C Dynamic Logic (CDL) that were adapted from their Java Card counterparts. Currently, the tool is in an early development stage. This paper is a further development of our work described in [10].", "num_citations": "1\n", "authors": ["665"]}
{"title": "Agile Formal Methods\n", "abstract": " Partial List of Agile Method Principles\u25ba Rapid, continuous delivery of useful and working software\u25ba Working software is the principal measure of progress\u25ba Even late changes in requirements are welcome\u25ba Regular adaptation to changing circumstances\u25ba Close, daily, cooperation between business people and developers\u25ba Continuous attention to technical excellence and good design\u25ba Simplicity", "num_citations": "1\n", "authors": ["665"]}
{"title": "A New Look at Formal Methods for Software Construction\n", "abstract": " Abstract                              This chapter sets the stage. We take stock of formal methods for software construction and sketch a path along which formal methods can be brought into mainstream applications. In addition, we provide an overview of the material covered in this book, so that the reader may make optimal use of it.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Symbolic fault injection\n", "abstract": " Computer systems that are dependable in the presence of faults are  increasingly in demand. Among available fault tolerance mechanisms,  software-implemented hardware fault tolerance (SIHFT) is constantly gaining  in popularity, because of its cost efficiency and flexibility.  Fault  tolerance mechanisms are often validated using fault injection, comprising a  variety of techniques for introducing faults into a system.  Traditional fault  injection techniques, however, suffer from a number of drawbacks, notably lack  of coverage (impossibility to exhaust all test cases) and the failure to activate enough injected faults. In this paper we present a new approach  called symbolic fault injection which is targeted at validation of SIHFT  mechanisms and is based on the concept of symbolic execution of programs. It  can be seen as the extension of a formal technique for formal program  verification that makes it possible to evaluate\u00a0\u2026", "num_citations": "1\n", "authors": ["665"]}
{"title": "\u6709\u9650\u533a\u9593\u5236\u7d04\u3092\u4ed8\u52a0\u3057\u305f\u30e2\u30c7\u30eb\u751f\u6210\u578b\u5b9a\u7406\u8a3c\u660e\u7cfb\u3068\u305d\u306e\u5fdc\u7528\n", "abstract": " \u8ad6\u6587\u6284\u9332\u30e2\u30c7\u30eb\u751f\u6210\u578b\u5b9a\u7406\u8a3c\u660e\u30b7\u30b9\u30c6\u30e0 MGTP (Model Generation Theorem Prover) \u306f, \u30dc\u30c8\u30e0\u30a2\u30c3\u30d7\u8a08\u7b97\u306b\u57fa\u3065\u304f\u4e00\u968e\u8ff0\u8a9e\u8ad6\u7406\u306e\u5b9a\u7406\u8a3c\u660e\u7cfb\u3067\u3042\u308b. MGTP \u306a\u3089\u3073\u306b\u305d\u306e\u62e1\u5f35\u5f62\u3067\u3042\u308b CMGTP \u306f, \u5236\u7d04\u5145\u8db3\u554f\u984c\u306b\u5bfe\u3057\u3066\u3082\u6709\u52b9\u306a\u624b\u7d9a\u304d\u3092\u63d0\u4f9b\u3057, \u6709\u9650\u4ee3\u6570\u5206\u91ce\u306b\u304a\u3044\u3066, \u672a\u89e3\u6c7a\u554f\u984c\u306e\u8a3c\u660e\u306b\u6210\u529f\u3059\u308b\u306a\u3069, \u9ad8\u52b9\u7387\u306a\u8a3c\u660e\u7cfb\u3067\u3042\u308b\u3053\u3068\u304c\u5b9f\u8a3c\u3055\u308c\u3066\u3044\u308b\u304c, \u534a\u9762, \u6574\u6570\u306b\u3088\u3063\u3066\u8868\u3055\u308c\u305f\u533a\u9593\u306a\u3069\u306e\u9806\u5e8f\u4ed8\u3051\u3089\u308c\u305f\u6709\u9650\u9818\u57df\u306b\u95a2\u3059\u308b\u5236\u7d04\u4f1d\u642c\u306b\u304a\u3044\u3066\u306f, \u305d\u306e\u8a18\u8ff0\u306e\u6c4e\u7528\u6027\u3086\u3048\u306b, \u5197\u9577\u306a\u30e2\u30c7\u30eb\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u304c\u660e\u3089\u304b\u3068\u306a\u3063\u305f. \u672c\u8ad6\u6587\u306b\u304a\u3044\u3066\u306f, \u3053\u3046\u3057\u305f\u554f\u984c\u70b9\u3092\u89e3\u6c7a\u3059\u308b\u305f\u3081\u306b, \u5168\u9806\u5e8f\u6709\u9650\u9818\u57df\u306e\u90e8\u5206\u96c6\u5408\u3092\u30e2\u30c7\u30eb\u5019\u88dc\u3068\u3057\u3066\u8a31\u3059\u65b0\u3057\u3044\u5b9a\u7406\u8a3c\u660e\u7cfb\u306e\u67a0\u7d44\u307f\u3092\u63d0\u6848\u3057, \u305d\u306e\u5f62\u5f0f\u7684\u610f\u5473\u8ad6\u3068\u6f14\u7e79\u624b\u7d9a\u304d\u3092\u660e\u3089\u304b\u306b\u3059\u308b. \u307e\u305f, \u3044\u304f\u3064\u304b\u306e\u554f\u984c\u306b\u5bfe\u3057\u3066\u9069\u7528\u3057\u305f\u5b9f\u9a13\u7d50\u679c\u3092\u793a\u3057, \u305d\u306e\u6709\u7528\u6027\u3092\u793a\u3059. \u6700\u5f8c\u306b, \u3053\u306e\u65b0\u3057\u3044\u67a0\u7d44\u307f\u3068\u5236\u7d04\u8ad6\u7406\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u3084\u591a\u5024\u8ad6\u7406, \u975e\u5358\u8abf\u8ad6\u7406\u3068\u306e\u95a2\u9023\u306b\u3064\u3044\u3066\u3082\u8a00\u53ca\u3059\u308b.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Some Remarks on Completeness, Connection Graph Resolution, and Link Deletion\n", "abstract": " A new completeness proof that generalizes the Anderson-Bledsoe excess literal argument is developed for connection-graph resolution. The technique also provides a simplified completeness proof for semantic resolution. Some observations about subsumption and about link deletion are made. Link deletion is the basis for connection graphs. Subsumption plays an important role in most resolution-based inference systems. In some settings\u2014for example, connection graphs in negation normal form\u2014both subsumption and link deletion can be quite tricky. Nevertheless, a completeness result that uses both is obtained in this setting.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Restart tableaux with selection function\n", "abstract": " Recently, several different sound and complete tableau calculi were introduced, all sharing the idea to use a selection function and so-called restart clauses: A-ordered tableaux, tableaux with selection function, and strict restart model elimination. We present two new sound and complete abstract tableau calculi which generalize these on the ground level. This makes differences and similarities between the calculi clearer and, in addition, gives insight into how properties of the calculi can be transferred among them. In particular, a precise borderline separating proof confluent from non-proof confluent variants is exhibited.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Deduction by combining semantic tableaux and integer programming\n", "abstract": " In this paper we propose to extend the current capabilities of automated reasoning systems by making use of techniques from integer programming. We describe the architecture of an automated reasoning system based on a Herbrand procedure (enumeration of formula instances) on clauses. The input are arbitrary sentences of first-order logic. The translation into clauses is done incrementally and is controlled by a semantic tableau procedure using unification. This amounts to an incremental polynomial CNF transformation which at the same time encodes part of the tableau structure and, therefore, tableau-specific refinements that reduce the search space. Checking propositional unsatisfiability of the resulting sequence of clauses can either be done with a symbolic inference system such as the Davis-Putnam procedure or it can be done using integer programming. If the latter is used a number of\u00a0\u2026", "num_citations": "1\n", "authors": ["665"]}
{"title": "Automated deduction and integer programming\n", "abstract": " We generalize propositional semantic tableaux for classical and many-valued logics to constraint tableaux. We show that this technique is a generalization of the standard translation from CNF formulas into integer programming. The main advantages are (i) a relatively efficient satisfiability checking procedure for classical, finitely-valued and, for the first time, for a wide range of infinitely-valued propositional logics; (ii) easy NP-containment proofs for many-valued logics. The standard translation of two-valued CNF formulas into integer programs and Tseitin\u2019s structure preserving clause form translation are obtained as a special case of our approach.", "num_citations": "1\n", "authors": ["665"]}
{"title": "Propositional non clausal deduction and diagnosis\n", "abstract": " In [11] Reiter laid the foundations of the formal theory of an approach to diagnosis known as diagnosis from first principles. His work was based on the work of many researchers, notably that of de Kleer [4] and Genesereth [5]. In diagnosis from first principles, we have a logic based description of some system (eg, a circuit) and an observation of the system\u2019s behavior. We then try to find a set of components in the system which, when assumed to be abnormal, explains the discrepancy between the intended behavior of the system and the observation. Usually the number of diagnoses will be exponential in the total number of components. Reiter uses minimal diagnosis to characterize all diagnoses, and gives a procedure (without an implementation) which can compute all minimal diagnoses. de Kleer [2] uses a less formal method, which requires the failure probabilities of the components to be known, in his diagnosis system. Mozetic and Holzbaur [6] describe an algorithm which uses Prolog for specifying the system description. Their technique is a purely symbolic method based on Reiter\u2019s theory. However, all these techniques require the system description to be in conjunctive normal form (CNF) or some form which is close to CNF. Therefore, any formula not in CNF must be converted to CNF before applying the diagnosis system. Relying on CNF or any other clause form may cause an exponential blow-up even before the diagnosis algorithms can be applied. Efficient clause form translations [8] commonly used in theorem provers cannot be used here because they do not preserve equivalence during transformation. The diagnosis systems of\u00a0\u2026", "num_citations": "1\n", "authors": ["665"]}
{"title": "KeY Quicktour\n", "abstract": " This document constitutes a tutorial introduction to the KeY-Tool. The KeY-Tool is an integrated environment for creating, analysing, and verifying UML/OCL models and their implementation. The main focus of the KeY-Tool are class diagrams. Other kinds of diagrams are currently not supported yet. The KeY-Tool is an extension of the commercial CASE tool Together ControlCenter1 (in the following referred to as TogetherCC). We assume that the reader is familiar with the CASE tool TogetherCC. Here we concentrate on the description of the KeY extensions. Furthermore, we assume that the KeY-Tool has been already installed successfully.The KeY-Tool is designed as an add-on to TogetherCC. Thus, all features offered by TogetherCC are available and the user can work with a powerful UML CASE tool in a familiar environment. The design philosophy of the KeY-Tool is to encourage but not to force users to take advantage of formal methods. Users are able to decide themselves at which point the KeY extensions are useful. For a longer discussion on the architecture, design philosophy, and theoretical underpinnings of the KeY-Tool please refer to [3]. The most recent version of the KeY-Tool can be downloaded from http://download. key-project. org.", "num_citations": "1\n", "authors": ["665"]}