{"title": "Estimating software project effort using analogies\n", "abstract": " Accurate project effort prediction is an important goal for the software engineering community. To date most work has focused upon building algorithmic models of effort, for example COCOMO. These can be calibrated to local environments. We describe an alternative approach to estimation based upon the use of analogies. The underlying principle is to characterize projects in terms of features (for example, the number of interfaces, the development method or the size of the functional requirements document). Completed projects are stored and then the problem becomes one of finding the most similar projects to the one for which a prediction is required. Similarity is defined as Euclidean distance in n-dimensional space where n is the number of project features. Each dimension is standardized so all dimensions have equal weight. The known effort values of the nearest neighbors to the new project are then used\u00a0\u2026", "num_citations": "1307\n", "authors": ["152"]}
{"title": "Effort estimation using analogy\n", "abstract": " The staff resources or effort required for a software project are notoriously difficult to estimate in advance. To date most work has focused upon algorithmic cost models such as COCOMO and Function Points. These can suffer from the disadvantage of the need to calibrate the model to each individual measurement environment coupled with very variable accuracy levels even after calibration. An alternative approach is to use analogy for estimation. We demonstrate that this method has considerable promise in that we show it to out perform traditional algorithmic methods for six different datasets. A disadvantage of estimation by analogy is that it requires a considerable amount of computation. The paper describes an automated environment known as ANGEL that supports the collection, storage and identification of the most analogous projects in order to estimate the effort for a new project. ANGEL is based upon the\u00a0\u2026", "num_citations": "407\n", "authors": ["152"]}
{"title": "Data quality: Some comments on the NASA software defect datasets\n", "abstract": " Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally\u00a0\u2026", "num_citations": "375\n", "authors": ["152"]}
{"title": "A general software defect-proneness prediction framework\n", "abstract": " BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and\u00a0\u2026", "num_citations": "372\n", "authors": ["152"]}
{"title": "An empirical investigation of an object-oriented software system\n", "abstract": " The paper describes an empirical investigation into an industrial object oriented (OO) system comprised of 133000 lines of C++. The system was a subsystem of a telecommunications product and was developed using the Shlaer-Mellor method (S. Shlaer and S.J. Mellor, 1988; 1992). From this study, we found that there was little use of OO constructs such as inheritance, and therefore polymorphism. It was also found that there was a significant difference in the defect densities between those classes that participated in inheritance structures and those that did not, with the former being approximately three times more defect-prone. We were able to construct useful prediction systems for size and number of defects based upon simple counts such as the number of states and events per class. Although these prediction systems are only likely to have local significance, there is a more general principle that software\u00a0\u2026", "num_citations": "343\n", "authors": ["152"]}
{"title": "Comparing software prediction techniques using simulation\n", "abstract": " The need for accurate software prediction systems increases as software becomes much larger and more complex. We believe that the underlying characteristics: size, number of features, type of distribution, etc., of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. It would also be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1000) validation cases. The authors compare four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently\u00a0\u2026", "num_citations": "325\n", "authors": ["152"]}
{"title": "Reliability and validity in comparative studies of software prediction models\n", "abstract": " Empirical studies on software prediction models do not converge with respect to the question \"which prediction model is best?\" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable\u00a0\u2026", "num_citations": "315\n", "authors": ["152"]}
{"title": "A critique of cyclomatic complexity as a software metric\n", "abstract": " McCabe's cyclomatic complexity metric is widely cited as a useful predictor of various software attributes such as reliability and development effort. This critique demonstrates that it is based upon poor theoretical foundations and an inadequate model of software development. The argument that the metric provides the developer with a useful engineering approximation is not borne out by the empirical evidence. Furthermore, it would appear that for a large class of software it is no more than a proxy for, and in many cases is outperformed by, lines of code.", "num_citations": "277\n", "authors": ["152"]}
{"title": "An investigation of machine learning based prediction systems\n", "abstract": " Traditionally, researchers have used either off-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to obtain software effort estimates. More recently, attention has turned to a variety of machine learning methods such as artificial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This paper outlines some comparative research into the use of these three machine learning methods to build software effort prediction systems. We briefly describe each method and then apply the techniques to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We compare the prediction systems in terms of three factors: accuracy, explanatory value and configurability. We show that ANN methods have superior accuracy and that RI methods are least accurate. However, this view is somewhat\u00a0\u2026", "num_citations": "258\n", "authors": ["152"]}
{"title": "Software Defect Association Mining and Defect Correction Effort Prediction\n", "abstract": " Much current software defect prediction work focuses on the number of defects remaining in a software system. In this paper, we present association rule mining based methods to predict defect associations and defect correction effort. This is to help developers detect software defects and assist project managers in allocating testing resources more effectively. We applied the proposed methods to the SEL defect data consisting of more than 200 projects over more than 15 years. The results show that, for defect association prediction, the accuracy is very high and the false-negative rate is very low. Likewise, for the defect correction effort prediction, the accuracy for both defect isolation effort prediction and defect correction effort prediction are also high. We compared the defect correction effort prediction method with other types of methods - PART, C4.5, and Naive Bayes - and show that accuracy has been improved\u00a0\u2026", "num_citations": "255\n", "authors": ["152"]}
{"title": "Comments on \"A metrics suite for object oriented design\"\n", "abstract": " A suite of object oriented software metrics has recently been proposed by S.R. Chidamber and C.F. Kemerer (see ibid., vol. 20, p. 476-94, 1994). While the authors have taken care to ensure their metrics have a sound measurement theoretical basis, we argue that is premature to begin applying such metrics while there remains uncertainty about the precise definitions of many of the quantities to be observed and their impact upon subsequent indirect metrics. In particular, we show some of the ambiguities associated with the seemingly simple concept of the number of methods per class. The usefulness of the proposed metrics, and others, would be greatly enhanced if clearer guidance concerning their application to specific languages were to be provided. Such empirical considerations are as important as the theoretical issues raised by the authors.< >", "num_citations": "210\n", "authors": ["152"]}
{"title": "Foundations of software measurement\n", "abstract": " Foundations of software measurement | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksFoundations of software measurement Export Citation Select Citation format Download citation Copy citation Categories Journals Magazines Books Proceedings SIGs Conferences Collections People About About ACM Digital Library Subscription Information Author Guidelines Using ACM Digital Library All Holdings within the ACM Digital Library ACM Computing Classification System Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin The ACM Digital Library \u2026", "num_citations": "159\n", "authors": ["152"]}
{"title": "Derivation and validation of software metrics\n", "abstract": " Derivation and validation of software metrics | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksDerivation and validation of software metrics Export Citation Select Citation format Download citation Copy citation Categories Journals Magazines Books Proceedings SIGs Conferences Collections People About About ACM Digital Library Subscription Information Author Guidelines Using ACM Digital Library All Holdings within the ACM Digital Library ACM Computing Classification System Join Join ACM Join SIGs Subscribe to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin The ACM is . \u2026", "num_citations": "155\n", "authors": ["152"]}
{"title": "Search heuristics, case-based reasoning and software project effort prediction\n", "abstract": " This paper reports on the use of search techniques to help optimise a case-based reasoning (CBR) system for predicting software project effort.  A major problem, common to ML techniques in general, has been dealing with large numbers of case features, some of which can hinder the prediction process. Unfortunately searching for the optimal feature subset is a combinatorial problem and therefore NP-hard. This paper examines the use of random searching, hill climbing and forward sequential selection (FSS) to tackle this problem.  Results from examining a set of real software project data show that even random searching was better than using all available for features (average error 35.6% rather than 50.8%).  Hill climbing and FSS both produced results substantially better than the random search (15.3 and 13.1% respectively), but FSS was more computationally efficient.  Providing a description of the fitness landscape of a problem along with search results is a step towards the classification of search problems and their assignment to optimum search techniques.  This paper attempts to describe the fitness landscape of this problem by combining the results from random searches and hill climbing, as well as using multi-dimensional scaling to aid visualisation.  Amongst other findings, the visualisation results suggest that some form of heuristic-based initialisation might prove useful for this problem.", "num_citations": "153\n", "authors": ["152"]}
{"title": "Experiences using case-based reasoning to predict software project effort\n", "abstract": " This paper explores some of the practical issues associated with the use of case-based reasoning (CBR) or estimation by analogy. We note that different research teams have reported widely differing results with this technology. Whilst we accept that underlying characteristics of the datasets being used play a major role we also argue that configuring a CBR system can also have an impact. We examine the impact of the choice of number of analogies when making predictions; we also look at different adaptation strategies. Our analysis is based on a dataset of software projects collected by a Canadian software house. Our results show that choosing analogies is important but adaptation strategy appears to be less so. These findings must be tempered, however, with the finding that it was difficult to show statistical significance for smaller datasets even when the accuracy indicators differed quite substantially. For this reason we urge some degree of caution when comparing competing prediction systems and only modest numbers of cases.", "num_citations": "145\n", "authors": ["152"]}
{"title": "A critique of three metrics\n", "abstract": " This article examines the metrics of the software science model, cyclomatic complexity, and an information flow metric of Henry and Kafura. These were selected on the basis of their popularity within the software engineering literature and the significance of the claims made by their progenitors. Claimed benefits are summarized. Each metric is then subjected to an in-depth critique. All are found wanting. We maintain that this is not due to mischance, but indicates deeper problems of methodology used in the field of software metrics. We conclude by summarizing these problems.", "num_citations": "140\n", "authors": ["152"]}
{"title": "Dealing with missing software project data\n", "abstract": " Whilst there is a general consensus that quantitative approaches are an important part of successful software project management, there has been relatively little research into many of the obstacles to data collection and analysis in the real world. One feature that characterises many of the data sets we deal with is missing or highly questionable values. Naturally this problem is not unique to software engineering, so we explore the application of two existing data imputation techniques that have been used to good effect elsewhere. In order to assess the potential value of imputation we use two industrial data sets. Both are quite problematic from an effort modelling perspective because they contain few cases, have a significant number of missing values and the projects are quite heterogeneous. We examine the quality of fit of effort models derived by stepwise regression on the raw data and data sets with values\u00a0\u2026", "num_citations": "126\n", "authors": ["152"]}
{"title": "Predicting with sparse data\n", "abstract": " It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe our sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity\u00a0\u2026", "num_citations": "124\n", "authors": ["152"]}
{"title": "Data sets and data quality in software engineering\n", "abstract": " OBJECTIVE-to assess the extent and types of techniques used to manage quality within software engineering data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets. METHOD-we perform a systematic review of available empirical software engineering studies. RESULTS-only 23 out of the many hundreds of studies assessed, explicitly considered data quality. CONCLUSIONS-first, the community needs to consider the quality and appropriateness of the data set being utilised; not all data sets are equal. Second, we need more research into means of identifying, and ideally repairing, noisy cases. Third, it should become routine to use sensitivity analysis to assess conclusion stability with respect to the assumptions that must be made concerning noise levels.", "num_citations": "123\n", "authors": ["152"]}
{"title": "Special issue on repeatable results in software engineering prediction\n", "abstract": " The goal of science is conclusion stability, ie to discover some effect X that holds in multiple situations. Sadly, there are all too few examples of stable conclusions in software engineering (SE). In fact, the typical result is conclusion instability where what is true for project one, does not hold for project two. We can find numerous studies of the following form: there is as much evidence for as against the argument that some aspect X adds value to a software project. Below are four examples of this type of problem which we believe to be endemic within SE.\u2013J\u00f8rgensen (2004) reviewed 15 studies comparing model-based to expert-based estimation. Five of those studies found in favor of expert-based methods, five found no difference, and five found in favor of model-based estimation.\u2013Mair and Shepperd (2005) compared regression to analogy methods for effort estimation and similarly found conflicting evidence. From a\u00a0\u2026", "num_citations": "119\n", "authors": ["152"]}
{"title": "Using simulation to evaluate prediction techniques [for software]\n", "abstract": " The need for accurate software prediction systems increases as software becomes larger and more complex. A variety of techniques have been proposed, but none has proved consistently accurate. The underlying characteristics of the data set influence the choice of the prediction system to be used. It has proved difficult to obtain significant results over small data sets; consequently, we required large validation data sets. Moreover, we wished to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system and data set characteristics. Our solution has been to simulate data, allowing both control and the possibility of large validation cases. We compared regression, rule induction and nearest neighbours (a form of case-based reasoning). The results suggest that there are significant differences depending upon the characteristics of the\u00a0\u2026", "num_citations": "115\n", "authors": ["152"]}
{"title": "Mining web browsing patterns for E-commerce\n", "abstract": " Web user clustering, Web page clustering, and frequent access path recognition are important issues in E-commerce. They can be used for the purposes of marketing strategies and product offerings, mass customization and personalization, and Web site adaptation. In this paper, we view the topology of a Web site as a directed graph, and use a user's access information on all URLs of a Web site as features to characterize the user and use all users\u2019 access information on a URL as features to characterize the URL. The user clusters and Web page clusters are discovered by both vector analysis and fuzzy set theory based methods. The frequent access paths are recognized based on Web page clusters and take into account the underlying structure of a Web site. Our method does not require the identification of user sessions from Web server logs, and both a user and a page can be assigned to more than one\u00a0\u2026", "num_citations": "113\n", "authors": ["152"]}
{"title": "A Comprehensive Investigation of the Role of Imbalanced Learning for Software Defect Prediction\n", "abstract": " Context: Software defect prediction (SDP) is an important challenge in the field of software engineering, hence much research work has been conducted, most notably through the use of machine learning algorithms. However, class-imbalance typified by few defective components and many non-defective ones is a common occurrence causing difficulties for these methods. Imbalanced learning aims to deal with this problem and has recently been deployed by some researchers, unfortunately with inconsistent results. Objective: We conduct a comprehensive experiment to explore (a) the basic characteristics of this problem; (b) the effect of imbalanced learning and its interactions with (i) data imbalance, (ii) type of classifier, (iii) input metrics and (iv) imbalanced learning method. Method: We systematically evaluate 27 data sets, 7 classifiers, 7 types of input metrics and 17 imbalanced learning methods (including doing\u00a0\u2026", "num_citations": "110\n", "authors": ["152"]}
{"title": "Predicting software project effort: A grey relational analysis based method\n", "abstract": " The inherent uncertainty of the software development process presents particular challenges for software effort prediction. We need to systematically address missing data values, outlier detection, feature subset selection and the continuous evolution of predictions as the project unfolds, and all of this in the context of data-starvation and noisy data. However, in this paper, we particularly focus on outlier detection, feature subset selection, and effort prediction at an early stage of a project. We propose a novel approach of using grey relational analysis (GRA) from grey system theory (GST), which is a recently developed system engineering theory based on the uncertainty of small samples. In this work we address some of the theoretical challenges in applying GRA to outlier detection, feature subset selection, and effort prediction, and then evaluate our approach on five publicly available industrial data sets using both\u00a0\u2026", "num_citations": "110\n", "authors": ["152"]}
{"title": "Using genetic programming to improve software effort estimation based on general data sets\n", "abstract": " This paper investigates the use of various techniques including genetic programming, with public data sets, to attempt to model and hence estimate software project effort. The main research question is whether genetic programs can offer \u2018better\u2019 solution search using public domain metrics rather than company specific ones. Unlike most previous research, a realistic approach is taken, whereby predictions are made on the basis of the data available at a given date. Experiments are reported, designed to assess the accuracy of estimates made using data within and beyond a specific company. This research also offers insights into genetic programming\u2019s performance, relative to alternative methods, as a problem solver in this domain. The results do not find a clear winner but, for this data, GP performs consistently well, but is harder to configure and produces more complex models. The evidence here agrees\u00a0\u2026", "num_citations": "107\n", "authors": ["152"]}
{"title": "Software project economics: a roadmap\n", "abstract": " The objective of this paper is to consider research progress in the field of software project economics with a view to identifying important challenges and promising research directions. I argue that this is an important sub-discipline since this will underpin any cost-benefit analysis used to justify the resourcing, or otherwise, of a software project. To accomplish this I conducted a bibliometric analysis of peer reviewed research articles to identify major areas of activity. My results indicate that the primary goal of more accurate cost prediction systems remains largely unachieved. However, there are a number of new and promising avenues of research including: how we can combine results from primary studies, integration of multiple predictions and applying greater emphasis upon the human aspects of prediction tasks. I conclude that the field is likely to remain very challenging due to the people-centric nature of software\u00a0\u2026", "num_citations": "106\n", "authors": ["152"]}
{"title": "The consistency of empirical comparisons of regression and analogy-based software project cost prediction\n", "abstract": " The objective is to determine the consistency within and between results in empirical studies of software engineering cost estimation. We focus on regression and analogy techniques as these are commonly used. We conducted an exhaustive literature search using predefined inclusion and exclusion criteria and identified 67 journal papers and 104 conference papers. From this sample we identified 11 journal papers and 9 conference papers that used both methods. Our analysis found that about 25% of studies were internally inconclusive. We also found that there is approximately equal evidence in favour of, and against analogy-based methods. We confirm the lack of consistency in the findings and argue that this inconsistent pattern from 20 different studies comparing regression and analogy is somewhat disturbing. It suggests that we need to ask more detailed questions than just: \"What is the best prediction\u00a0\u2026", "num_citations": "106\n", "authors": ["152"]}
{"title": "Quantitative analysis of static models of processes\n", "abstract": " The upstream activities of software development projects are often viewed as both the most important, the least understood, and hence the most problematic. This is particularly noticeable in terms of satisfying customer requirements. Business process modelling is one solution that is being increasingly used in conjunction with traditional software development, often feeding in to requirements and analysis activities. In addition, research in Systems Engineering for Business Process Change,1 highlights the importance of modelling business processes in evolving and maintaining legacy systems that support those processes. However, the major use of business process modelling, is to attempt to restructure the business process, in order to improve some given aspect, e.g., cost or time. This restructuring may be seen either as separate activity or as a pre-cursor to the development of systems to support the new or\u00a0\u2026", "num_citations": "103\n", "authors": ["152"]}
{"title": "A new imputation method for small software project data sets\n", "abstract": " Effort prediction is a very important issue for software project management. Historical project data sets are frequently used to support such prediction. But missing data are often contained in these data sets and this makes prediction more difficult. One common practice is to ignore the cases with missing data, but this makes the originally small software project database even smaller and can further decrease the accuracy of prediction. The alternative is missing data imputation. There are many imputation methods. Software data sets are frequently characterised by their small size but unfortunately sophisticated imputation methods prefer larger data sets. For this reason we explore using simple methods to impute missing data in small project effort data sets. We propose a class mean imputation (CMI) method based on the k-NN hot deck imputation method (MINI) to impute both continuous and nominal missing data in\u00a0\u2026", "num_citations": "98\n", "authors": ["152"]}
{"title": "Can k-NN imputation improve the performance of C4.5 with small software project data sets? A comparative evaluation\n", "abstract": " Missing data is a widespread problem that can affect the ability to use data to construct effective prediction systems. We investigate a common machine learning technique that can tolerate missing values, namely C4.5, to predict cost using six real world software project databases. We analyze the predictive performance after using the k-NN missing data imputation technique to see if it is better to tolerate missing data or to try to impute missing values and then apply the C4.5 algorithm. For the investigation, we simulated three missingness mechanisms, three missing data patterns, and five missing data percentages. We found that the k-NN imputation can improve the prediction accuracy of C4.5. At the same time, both C4.5 and k-NN are little affected by the missingness mechanism, but that the missing data pattern and the missing data percentage have a strong negative impact upon prediction (or imputation\u00a0\u2026", "num_citations": "79\n", "authors": ["152"]}
{"title": "Making inferences with small numbers of training sets\n", "abstract": " A potential methodological problem with empirical studies that assess project effort prediction system is discussed. Frequently, a hold-out strategy is deployed so that the data set is split into a training and a validation set. Inferences are then made concerning the relative accuracy of the different prediction techniques under examination. This is typically done on very small numbers of sampled training sets. It is shown that such studies can lead to almost random results (particularly where relatively small effects are being studied). To illustrate this problem, two data sets are analysed using a configuration problem for case-based prediction and results generated from 100 training sets. This enables results to be produced with quantified confidence limits. From this it is concluded that in both cases using less than five training sets leads to untrustworthy results, and ideally more than 20 sets should be deployed\u00a0\u2026", "num_citations": "79\n", "authors": ["152"]}
{"title": "Using grey relational analysis to predict software effort with small data sets\n", "abstract": " The inherent uncertainty of the software development process presents particular challenges for software effort prediction. We need to systematically address missing data values, feature subset selection and the continuous evolution of predictions as the project unfolds, and all of this in the context of data-starvation and noisy data. However, in this paper, we particularly focus on feature subset selection and effort prediction at an early stage of a project. We propose a novel approach of using grey relational analysis (GRA) of grey system theory (GST), which is a recently developed system engineering theory based on the uncertainty of small samples. In this work we address some of the theoretical challenges in applying GRA to feature subset selection and effort prediction, and then evaluate our approach on five publicly available industrial data sets using stepwise regression as a benchmark. The results are very\u00a0\u2026", "num_citations": "74\n", "authors": ["152"]}
{"title": "On building prediction systems for software engineers\n", "abstract": " Building and evaluating predictionsystems is an important activity for software engineering researchers.Increasing numbers of techniques and datasets are now being madeavailable. Unfortunately systematic comparison is hindered bythe use of different accuracy indicators and evaluation processes.We argue that these indicators are statistics that describe propertiesof the estimation errors or residuals and that the sensible choiceof indicator is largely governed by the goals of the estimator.For this reason it may be helpful for researchers to providea range of indicators. We also argue that it is useful to formallytest for significant differences between competing predictionsystems and note that where only a few cases are available thiscan be problematic, in other words the research instrument mayhave insufficient power. We demonstrate that this is the casefor a well known empirical study of cost models\u00a0\u2026", "num_citations": "74\n", "authors": ["152"]}
{"title": "Case and feature subset selection in case-based software project effort prediction\n", "abstract": " Prediction systems adopting a case-based reasoning (CBR) approach have been widely advocated. However, as with most machine learning techniques, feature and case subset selection can be extremely influential on the quality of the predictions generated. Unfortunately, both are NP-hard search problems which are intractable for non-trivial data sets. Using all features frequently leads to poor prediction accuracy and pre-processing methods (filters) have not generally been effective. In this paper we consider two different real world project effort data sets. We describe how using simple search techniques, such as hill climbing and sequential selection, can achieve major improvements in accuracy. We conclude that, for our data sets, forward sequential selection, for features, followed by backward sequential selection, for cases, is the most effective approach when exhaustive searching is not possible.", "num_citations": "69\n", "authors": ["152"]}
{"title": "Design metrics: an empirical analysis\n", "abstract": " The application of design metrics to software development. is considered. An empirical investigation is described, the goal of which is to identify design metrics that allow the software engineer to discriminate between designs and pinpoint design weaknesses, particularly with a view to minimising development effort. A design measure based upon information flows between modules is proposed and empirically validated by analysing 13 software systems. A highly significant correlation is found between this metric and development effort measured as computer connect time. By contrast, size-based design metrics are found to exhibit little association with effort.< >", "num_citations": "64\n", "authors": ["152"]}
{"title": "Case-based reasoning and software engineering\n", "abstract": " Case-based reasoning (CBR) is a technology that is based on the idea of analogy. Solutions from past problems (cases) can be retrieved and deployed, with adaptation where necessary, to solve new problems. It is argued that CBR as a technology has a number of strengths, since it deals well with poorly understood problem domains, does not require explicit knowledge elicitation and supports collaboration with users. This chapter provides some general background information on CBR and then considers how CBR has been deployed to solve problems in the domain of software engineering. These problems fall into two general categories, namely prediction and reuse. The main prediction problems are related to project characteristics such as effort and duration, whilst the chief reuse foci are related to learning from past experiences. The chapter concludes by identifying three research challenges. These\u00a0\u2026", "num_citations": "63\n", "authors": ["152"]}
{"title": "Towards a conceptual framework for object oriented software metrics\n", "abstract": " The development of software metrics for object oriented (OO) languages is receiving increasing attention. We examine the reasons why this is a much more challenging problem than for conventional languages. It seems premature to develop and apply OO metrics while there remains uncertainty not only about the precise definitions of many fundamental quantities and their subsequent impact on derived metrics, but also a lack of qualitative understanding of the structure and behaviour of OO systems. We argue that establishing a standard terminology and data model will help provide a framework for both theoretical and empirical work and increase the chances of early success. One potential benefit is improvement of the ability to perform independent validation of models and metrics. We propose a data model and terminology and illustrate the importance of such definitions by examining the seemingly\u00a0\u2026", "num_citations": "63\n", "authors": ["152"]}
{"title": "A review of experimental investigations into object-oriented technology\n", "abstract": " In recent years there has been a growing interest in empirically investigating object-oriented technology (OOT). Much of this empirical work has been experimental in nature. This paper reviews the published output of such experiments\u201418 in total\u2014with the twin aims of, first, assessing what has been learnt about OOT and, second, what has been learnt about conducting experimental work. We note that much work has focused upon evaluation of the inheritance mechanism. Whilst such experiments are of some interest, we observe that this may be of less significance to the OOT community than experimenters seem to believe. Instead, OOT workers place more emphasis upon other mechanisms such as composition, components, frameworks, architectural styles and design patterns. This leads us to conclude that the empirical researchers need to ensure that their work keeps pace with technological\u00a0\u2026", "num_citations": "62\n", "authors": ["152"]}
{"title": "Issues on the effective use of CBR technology for software project prediction\n", "abstract": " This paper explores some of the practical issues associated with the use of case-based reasoning (CBR) or estimation by analogy for software project effort prediction. Different research teams have reported varying experiences with this technology. We take the view that the problems hindering the effective use of CBR technology are twofold. First, the underlying characteristics of the datasets play a major role in determining which prediction technique is likely to be most effective. Second, when CBR is that technique, we find that configuring a CBR system can also have a significant impact upon predictive capabilities. In this paper we examine the performance of CBR when applied to various datasets using stepwise regression (SWR) as a benchmark. We also explore the impact of the choice of number of analogies and the size of the training dataset when making predictions.", "num_citations": "56\n", "authors": ["152"]}
{"title": "A short note on safest default missingness mechanism assumptions\n", "abstract": " A very common problem when building software engineering models is dealing with missing data. To address this there exist a range of imputation techniques. However, selecting the appropriate imputation technique can also be a difficult problem. One reason for this is that these techniques make assumptions about the underlying missingness mechanism, that is how the missing values are distributed within the data set. It is compounded by the fact that, for small data sets, it may be very difficult to determine what is the missingness mechanism. This means there is a danger of using an inappropriate imputation technique. Therefore, it is necessary to determine what is the safest default assumption about the missingness mechanism for imputation techniques when dealing with small data sets. We examine experimentally, two simple and commonly used techniques: Class Mean Imputation (CMI) and k\u00a0\u2026", "num_citations": "54\n", "authors": ["152"]}
{"title": "Software engineering metrics I: measures and validations\n", "abstract": " The thesis of this book is that software metrics is potentially one of the most important aspects of software engineering, but that software engineers have largely failed to achieve its potential. Three reasons are proposed for this: although software measurement is deceptively simple, obtaining useful measurements is less straightforward; the majority of metrics proposed in the literature never progress from speculation to thorough validation; and extremely complex products and processes cannot be adequately modeled by only a few metrics. This volume reviews the important software metrics developed over the past 20 years by reprinting, with commentary, 16 papers on the subject.(A second volume, also edited by Shepperd, to appear later this year deals with more specific topics such as tool support and techniques for collecting and analyzing metrics). The papers are presented in three sections: a historical review\u00a0\u2026", "num_citations": "53\n", "authors": ["152"]}
{"title": "Comparison of various methods for handling incomplete data in software engineering databases\n", "abstract": " Increasing the awareness of how missing data affects software predictive accuracy has led to increasing numbers of missing data techniques (MDTs). This paper investigates the robustness and accuracy of eight popular techniques for tolerating incomplete training and test data using tree-based models. MDTs were compared by artificially simulating different proportions, patterns, and mechanisms of missing data. A 4-way repeated measures design was employed to analyze the data. The simulation results suggest important differences. Listwise deletion is substantially inferior while multiple imputation (MI) represents a superior approach to handling missing data. Decision tree single imputation and surrogate variables splitting are more severely impacted by missing values distributed among all attributes. MI should be used if the data contain many missing values. If few values are missing, any of the MDTs might be\u00a0\u2026", "num_citations": "51\n", "authors": ["152"]}
{"title": "A systematic review of unsupervised learning techniques for software defect prediction\n", "abstract": " BackgroundUnsupervised machine learners have been increasingly applied to software defect prediction. It is an approach that may be valuable for software practitioners because it reduces the need for labeled training data.ObjectiveInvestigate the use and performance of unsupervised learning techniques in software defect prediction.MethodWe conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our inclusion criteria published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the confusion matrices and employed the Matthews Correlation Coefficient (MCC) as our main performance measure.ResultsOur meta-analysis shows that unsupervised models are comparable with supervised models for both within-project and cross-project prediction\u00a0\u2026", "num_citations": "45\n", "authors": ["152"]}
{"title": "Early life-cycle metrics and software quality models\n", "abstract": " Software quality is becoming a topic of increasing importance. While there is widespread acceptance of the desirability of quality, there are few proven techniques for obtaining it. Moreover, those techniques that are in existence rely heavily on exercising the finished product and the imposition of improved quality whenever defects are observed. However, software quality is primarily an early life-cycle issue \u2014 acceptance testing is too late to make a significant impact on many software quality characteristics. Metrics, particularly design metrics, provide an important means to assess quality at earlier stages of the software life-cycle. Some design metrics are presented, together with confirmatory empirical evidence of their efficacy. A goal-directed approach is described, as a mechanism for integrating early metrics into software development environments, to improve software quality.", "num_citations": "45\n", "authors": ["152"]}
{"title": "An evaluation of software product metrics\n", "abstract": " Publisher SummaryThis chapter describes an evaluation of software product metrics. The simplest product metric is lines of code (LOC). The basis for LOC is that program length can be used as a predictor of program characteristics such as reliability and ease of maintenance. Despite, or possibly even because of, the simplicity of this metric, it has been almost universally reviled. Certainly there are serious difficulties with defining what actually constitutes a line of code, consequently modifications such as the number of source statements, or machine code instructions generated have been advanced. The first family of metrics is those that purely deal with aspects of intra modular complexity, and contain two exceptions to the general model described in that they are both extensions to Halsteads software science. These allow the designer to estimate the various measures such as n1, and n2 prior to the completion of\u00a0\u2026", "num_citations": "43\n", "authors": ["152"]}
{"title": "Ensemble of missing data techniques to improve software prediction accuracy\n", "abstract": " Software engineers are commonly faced with the problem of incomplete data. Incomplete data can reduce system performance in terms of predictive accuracy. Unfortunately, rare research has been conducted to systematically explore the impact of missing values, especially from the missing data handling point of view. This has made various missing data techniques (MDTs) less significant. This paper describes a systematic comparison of seven MDTs using eight industrial datasets. Our findings from an empirical evaluation suggest listwise deletion as the least effective technique for handling incomplete data while multiple imputation achieves the highest accuracy rates. We further propose and show how a combination of MDTs by randomizing a decision tree building algorithm leads to a significant improvement in prediction performance for missing values up to 50%.", "num_citations": "42\n", "authors": ["152"]}
{"title": "Comparing use case writing guidelines\n", "abstract": " Use Cases are a widely used technique for requirements specification as part of the Unified Modelling Language (UML). However, Use Cases rely predominantly upon natural language. For this reason, the CREWS research group has proposed various guidelines to assist in writing Use Cases. Various research groups, including ourselves, have found that writing guidelines (CREWS) do help. However, our experience with students is that the CREWS guidelines are a little unwieldy, and can be difficult to apply. In this paper we propose some simplified Use Case guidelines, particularly in terms of admissible structures. We then describe a pilot experiment to explore whether the simplifications result in any loss of Use Case quality. Our initial results suggest that our simpler guidelines perform at least as well as the CREWS guidelines. Consequently, we suggest that, given these promising findings, further empirical studies, particularly industrial case studies, be conducted to confirm whether our simplified approach warrants industrial adoption.", "num_citations": "42\n", "authors": ["152"]}
{"title": "Missing data imputation techniques\n", "abstract": " Intelligent data analysis techniques are useful for better exploring real-world data sets. However, the real-world data sets always are accompanied by missing data that is one major factor affecting data quality. At the same time, good intelligent data exploration requires quality data. Fortunately, Missing Data Imputation Techniques (MDITs) can be used to improve data quality. However, no one method MDIT can be used in all conditions, each method has its own context. In this paper, we introduce the MDITs to the KDD and machine learning communities by presenting the basic idea and highlighting the advantages and limitations of each method.", "num_citations": "38\n", "authors": ["152"]}
{"title": "The use of function points to find cost analogies\n", "abstract": " Finding effective techniques for the early estimation of project effort remains an important \u2014 and frustratingly elusive \u2014 research objective for the software development community. We have conducted an empirical study of 21 real time projects for a major software developer. The study collected a range of counts and measures derived from specification documents, including a derivative of Function Points intended for highly constrained systems. Notwithstanding the fact that the projects were drawn from a comparatively stable environment, traditional approaches for building prediction systems, (for example, regression analysis) failed to yield a useful predictive model. By contrast, estimation based upon the automated search for analogous projects produced more accurate estimates. How much this is a characteristic of this particular dataset and how much these findings might be more generally replicated is uncertain. Nevertheless, these results should act as encouragement for follow up research on a much under utilised estimation technique.", "num_citations": "35\n", "authors": ["152"]}
{"title": "An experiment on software project size and effort estimation\n", "abstract": " Expert judgment is still the dominant estimation technique in practice today for software project size and effort. In this paper, we evaluate two techniques that are frequently suggested as effective support for human estimators: checklists and group discussions. A student experiment was conducted to investigate how checklists and group discussions help estimators to improve their estimates. The results suggest that both checklists and group discussions significantly contribute to improved estimation, but in distinct and complementary ways.", "num_citations": "33\n", "authors": ["152"]}
{"title": "Metrics, outlier analysis and the software design process\n", "abstract": " The tutorial paper considers the question of how the software designer is to make use of the many design metrics that have been proposed by researchers over the past few years. The activity of software design is an iterative process of decision making. Design methodologies provide qualitative criteria for this decision-making process. In contrast, design metrics claim to provide objective, quantitative guidance. Three application methods for metrics are identified: prediction, quality control, and outlier analysis. Strong evidence is presented to show that, at present, the field of design metrics is only sufficiently mature to allow outlier analysis to be recommended as a useful aid to the software design process. The paper demonstrates this technique by evaluating an example design using the information flow metric.", "num_citations": "33\n", "authors": ["152"]}
{"title": "An empirical and theoretical analysis of an information flow-based system design metric\n", "abstract": " This paper examines information flow metrics: a subset of a potentially valuable class of system architecture measures. Theoretical analysis of the underlying model reveals a number of anomalies which translate into poor performance, as revealed by a large empirical study. Attention to these theoretical deficiencies results in a marked improvement in performance. There are two themes to this paper. The first theme\u2014a minor one\u2014involves a critique and evaluation of one particular system design metric. The second theme\u2014a major one\u2014entails a critique of the metric derivation process adopted by the vast majority of the researchers in this field.", "num_citations": "31\n", "authors": ["152"]}
{"title": "Human judgement and software metrics: vision for the future\n", "abstract": " Background: There has been much research into building formal (metrics-based) prediction systems with the aim of improving resource estimation and planning of software projects. However the'objectivity'of such systems is illusory in the sense that many inputs need themselves to be estimated by the software engineer.Method: We review the uptake of past software project prediction research and identify relevant cognitive psychology research on expert behaviour. In particular we explore potential applications of recent metacognition research.Results: We find the human aspect is largely ignored, despite the availability of many important results from cognitive psychology.Conclusions: In order to increase the actual use of our metrics research eg effort prediction systems we need to have a more integrated view of how such research might be used and who might be using it. This leads to our belief that future\u00a0\u2026", "num_citations": "30\n", "authors": ["152"]}
{"title": "The role and value of replication in empirical software engineering results\n", "abstract": " ContextConcerns have been raised from many quarters regarding the reliability of empirical research findings and this includes software engineering. Replication has been proposed as an important means of increasing confidence.ObjectiveWe aim to better understand the value of replication studies, the level of confirmation between replication and original studies, what confirmation means in a statistical sense and what factors modify this relationship.MethodWe perform a systematic review to identify relevant replication experimental studies in the areas of (i) software project effort prediction and (ii) pair programming. Where sufficient details are provided we compute prediction intervals.ResultsOur review locates 28 unique articles that describe replications of 35 original studies that address 75 research questions. Of these 10 are external, 15 internal and 3 internal-same-article replications. The odds ratio of internal\u00a0\u2026", "num_citations": "28\n", "authors": ["152"]}
{"title": "How do I know whether to trust a research result?\n", "abstract": " A meta-analysis indicated that some areas of computer science research are subject to researcher bias. However, rather than mistrust all scientific research, researchers should examine research to determine its validity.", "num_citations": "28\n", "authors": ["152"]}
{"title": "An empirical study of evolution of inheritance in Java OSS\n", "abstract": " Previous studies of Object-Oriented (OO) software have reported avoidance of the inheritance mechanism and cast doubt on the wisdom of 'deep' inheritance levels. From an evolutionary perspective, the picture is unclear - we still know relatively little about how, over time, changes tend to be applied by developers. Our conjecture is that an inheritance hierarchy will tend to grow 'breadth-wise' rather than 'depth-wise'. This claim is made on the basis that developers will avoid extending depth in favour of breadth because of the inherent complexity of having to understand the functionality of superclasses. Thus the goal of our study is to investigate this empirically. We conduct an empirical study of seven Java Open-Source Systems (OSSs) over a series of releases to observe the nature and location of changes within the inheritance hierarchies. Results showed a strong tendency for classes to be added at levels one\u00a0\u2026", "num_citations": "28\n", "authors": ["152"]}
{"title": "Software productivity analysis of a large data set and issues of confidentiality and data quality\n", "abstract": " The paper reports on an ongoing investigation into software productivity and its influencing factors. Analysis of a data set containing project management of a large multinational company. The data set contains tables holding information about more than 25000 closed projects collected since 1990. Due to incomplete data only 1413 closed projects could be used for the investigation. Confidentiality was also considered as a major issue. The projects in the data set vary greatly in their productivity. Analysis of productivity and its influencing factors", "num_citations": "28\n", "authors": ["152"]}
{"title": "Enhancing practice and achievement in introductory programming with a robot olympics\n", "abstract": " Computer programming is notoriously difficult to learn. To this end, regular practice in the form of application and reflection is an important enabler of student learning. However, educators often find that first-year B.Sc. students do not readily engage in such activities. Providing each student with a programmable robot, however, could be used to facilitate application and reflection since, potentially, robots facilitate engaging learning experiences while providing immediate and intuitive feedback. This paper explores whether an introductory course centered upon programming personal robots in preparation for an end-of-course event day-a Robot Olympics-can help students to, first, engage in programming practice more frequently and, second, improve the quality of their code. A survey was conducted to examine the students' programming practice behavior, and students' final coursework submissions were also\u00a0\u2026", "num_citations": "26\n", "authors": ["152"]}
{"title": "\u201cBad smells\u201d in software analytics papers\n", "abstract": " ContextThere has been a rapid growth in the use of data analytics to underpin evidence-based software engineering. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies.ObjectiveOur goal is to provide guidance for producers and consumers of software analytics studies (computational experiments and correlation studies).MethodWe propose using \u201cbad smells\u201d, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in software analytics studies.ResultsWe list 12 \u201cbad smells\u201d in software analytics papers (and show their impact by examples).ConclusionsWe believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so\u00a0\u2026", "num_citations": "25\n", "authors": ["152"]}
{"title": "Filtering, robust filtering, polishing: Techniques for addressing quality in software data\n", "abstract": " Data quality is an important aspect of empirical analysis. This paper compares three noise handling methods to assess the benefit of identifying and either filtering or editing problematic instances. We compare a 'do nothing' strategy with (i) filtering, (ii) robust filtering and (Hi) filtering followed by polishing. A problem is that it is not possible to determine whether an instance contains noise unless it has implausible values. Since we cannot determine the true overall noise level we use implausible val.ues as a proxy measure. In addition to the ability to identify implausible values, we use another proxy measure, the ability to fit a classification tree to the data. The interpretation is low misclassification rates imply low noise levels. We found that all three of our data quality techniques improve upon the 'do nothing' strategy, also that the filtering and polishing was the most effective technique for dealing with noise since we\u00a0\u2026", "num_citations": "25\n", "authors": ["152"]}
{"title": "The problem of labels in e-assessment of diagrams\n", "abstract": " In this article we explore a problematic aspect of automated assessment of diagrams. Diagrams have partial and sometimes inconsistent semantics. Typically much of the meaning of a diagram resides in the labels; however, the choice of labeling is largely unrestricted. This means a correct solution may utilize differing yet semantically equivalent labels to the specimen solution. With human marking this problem can be easily overcome. Unfortunately with e-assessment this is challenging. We empirically explore the scale of the problem of synonyms by analyzing 160 student solutions to a UML task. From this we find that cumulative growth of synonyms only shows a limited tendency to reduce at the margin despite using a range of text processing algorithms such as stemming and auto-correction of spelling errors. This finding has significant implications for the ease in which we may develop future e-assessment\u00a0\u2026", "num_citations": "24\n", "authors": ["152"]}
{"title": "A replication of the use of regression towards the mean (R2M) as an adjustment to effort estimation models\n", "abstract": " The paper performs an independent replication of the Jorgensen et al. study that advocates exploiting a phenomenon known as regression to the mean for software project productivity when predicting software project effort. We used two further industrial data sets in which we compare accuracy levels with and without this adjustment. Our results were broadly consistent with those from the Jorgensen study. Using the R2M resulted in a small increase in predictive accuracy. For one data set it was necessary to first partition it into more homogeneous subsets. Also when there was very weak correlation between predicted and actual productivity using the sample mean was the least bad strategy. We believe that independent validation of results is an important activity. Specifically our results add further support for the R2M approach in that there is a small, but positive, effect upon prediction accuracy. By combining results\u00a0\u2026", "num_citations": "24\n", "authors": ["152"]}
{"title": "On configuring a case-based reasoning software project prediction system\n", "abstract": " This paper explores some of the practical issues associated with the use of case-based reasoning (CBR) or estimation by analogy for software project effort prediction. We note that different research teams have reported widely differing results with this technology. Whilst we accept that underlying characteristics of the datasets being used play a major role we also argue that configuring a CBR system can also have an impact. We examine the impact of the choice of number of analogies when making predictions. Our analysis is based on project effort data derived from two sources. The first data has been collected by a Canadian software house. The other data was obtained by means of simulation. The results show that using a larger number of analogies generated better predictions. On the other hand, the nearest neighbour is observed to be the better predictor for smaller training sets. We also found that the underlying distribution of the dataset does have a significant impact on the choice of the number of analogies. It was difficult to identify patterns for datasets with complex distribution properties. We believe that a considerable amount of work is required to understand the relationship between the properties of the dataset and configuring a CBR system.", "num_citations": "24\n", "authors": ["152"]}
{"title": "Realistic Assessment of Software Effort Estimation Models\n", "abstract": " Context: It is unclear that current approaches to evaluating or comparing competing software cost or effort models give a realistic picture of how they would perform in actual use. Specifically, we're concerned that the usual practice of using all data with some holdout strategy is at variance with the reality of a data set growing as projects complete.Objective: This study investigates the impact of using unrealistic, though possibly convenient to the researchers, ways to compare models on commercial data sets. Our questions are does this lead to different conclusions in terms of the comparisons and if so, are the results biased eg, more optimistic than those that might realistically be achieved in practice.Method: We compare a traditional approach based on leave one out cross-validation with growing the data set chronologically using the Finnish and Desharnais data sets.Results: Our realistic, time-based approach to\u00a0\u2026", "num_citations": "23\n", "authors": ["152"]}
{"title": "Empirically-based software engineering\n", "abstract": " This article gives a survey of activity in the field of empirically-based Software Engineering. It argues that this is an important area of research if practitioners are to make decisions on better evidence than mere subjective opinion. The article describes four areas where empirical data has improved our understanding of software technology. These are object-orientation, inspections, formal specification and project failure factors. The article concludes that empirical Software Engineering is likely to grow in importance but that there remain challenges, not least in assessing large scale processes and artifacts, in dealing with the human or creative aspects of processes and in overcoming the publication bias against \u2018negative\u2019results.", "num_citations": "22\n", "authors": ["152"]}
{"title": "Assessing Software Defection Prediction Performance: Why Using the Matthews Correlation Coefficient Matters\n", "abstract": " Context: There is considerable diversity in the range and design of computational experiments to assess classifiers for software defect prediction. This is particularly so, regarding the choice of classifier performance metrics. Unfortunately some widely used metrics are known to be biased, in particular F1.Objective: We want to understand the extent to which the widespread use of the F1 renders empirical results in software defect prediction unreliable.Method: We searched for defect prediction studies that report both F1 and the Matthews correlation coefficient (MCC). This enabled us to determine the proportion of results that are consistent between both metrics and the proportion that change.Results: Our systematic review identifies 8 studies comprising 4017 pairwise results. Of these results, the direction of the comparison changes in 23% of the cases when the unbiased MCC metric is employed.Conclusion: We find\u00a0\u2026", "num_citations": "21\n", "authors": ["152"]}
{"title": "Data quality: Cinderella at the software metrics ball?\n", "abstract": " In this keynote I explore what exactly do we mean by data quality, techniques to assess data quality and the very significant challenges that poor data quality can pose. I believe we neglect data quality at our peril since-whether we like it or not-our research results are founded upon data and our assumptions that data quality issues do not confound our results. A systematic review of the literature suggests that it is a minority practice to even explicitly discuss data quality. I therefore suggest that this topic should become a higher priority amongst empirical software engineering researchers.", "num_citations": "21\n", "authors": ["152"]}
{"title": "Products, processes and metrics\n", "abstract": " The paper reviews developments in the arena of software engineering product metrics, with special reference to system architecture metrics. Some of the weaknesses of current approaches are examined, in particular the very weak notion of process embodied by a product metric. It is argued that the consequence of this oversight is uncertainty in the application and interpretation of metrics. This in turn has led to a slow uptake of product metrics by the software industry. The paper then demonstrates, by means of a simple design metric example, that the application of concepts from the area of software process modelling to product metrics can help overcome many of these deficiencies; it also results in quantitative process models that have potential for the design and construction of software tools and environments.", "num_citations": "21\n", "authors": ["152"]}
{"title": "Data sets and data quality in software engineering: Eight years on\n", "abstract": " Context: We revisit our review of data quality within the context of empirical software engineering eight years on from our PROMISE 2008 article.Objective: To assess the extent and types of techniques used to manage quality within data sets. We consider this a particularly interesting question in the context of initiatives to promote sharing and secondary analysis of data sets.Method: We update the 2008 mapping study through four subsequently published reviews and a snowballing exercise.Results: The original study located only 23 articles explicitly considering data quality. This picture has changed substantially as our updated review now finds 283 articles, however, our estimate is that this still represents perhaps 1% of the total empirical software engineering literature.Conclusions: It appears the community is now taking the issue of data quality more seriously and there is more work exploring techniques to\u00a0\u2026", "num_citations": "20\n", "authors": ["152"]}
{"title": "Feature weighting techniques for CBR in software effort estimation studies: a review and empirical evaluation\n", "abstract": " Context: Software effort estimation is one of the most important activities in the software development process. Unfortunately, estimates are often substantially wrong. Numerous estimation methods have been proposed including Case-based Reasoning (CBR). In order to improve CBR estimation accuracy, many researchers have proposed feature weighting techniques (FWT).Objective: Our purpose is to systematically review the empirical evidence to determine whether FWT leads to improved predictions. In addition we evaluate these techniques from the perspectives of (i) approach (ii) strengths and weaknesses (iii) performance and (iv) experimental evaluation approach including the data sets used.Method: We conducted a systematic literature review of published, refereed primary studies on FWT (2000--2014).Results: We identified 19 relevant primary studies. These reported a range of different techniques. 17\u00a0\u2026", "num_citations": "20\n", "authors": ["152"]}
{"title": "An empirical study into the use of measurement to support OO design evaluation\n", "abstract": " This paper describes a case study into using measurement to aid design evaluation. The study was carried out as an assignment by final year B.Sc. students from an I.T. course. Approximately half of these were part time students working full time in industry and the others were full time students who had already undertaken a one year industrial placement. After some preparatory tuition, the subjects were given a small system (15 classes) written in Java and an analysis tool to enable them to apply a wide variety of metrics to this system. They were then asked to use measurement results and design heuristics to help them critique the design of the system. The main results from this work were that: measurement was found to be a useful aid to design evaluation; following the study, most subjects said that they would now be more inclined to use measurement than they there previously; support is needed for\u00a0\u2026", "num_citations": "17\n", "authors": ["152"]}
{"title": "Most cited journal articles in software engineering\n", "abstract": " Editorial: Most cited journal articles in software engineering: Information and Software Technology: Vol 49, No 1 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Information and Software Technology Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsInformation and Software TechnologyVol. , No. Editorial: Most cited journal articles in software engineering article Editorial: Most cited journal articles in software engineering Share on Authors: Claes Wohlin profile image Claes Wohlin Blekinge Institute of Technology, Department of Systems and Software Engineering, School of Engineering, Box 520, SE-372 25 Ronneby, Sweden Blekinge Institute -\u2026", "num_citations": "16\n", "authors": ["152"]}
{"title": "Class movement and re-location: An empirical study of Java inheritance evolution\n", "abstract": " Inheritance is a fundamental feature of the Object-Oriented (OO) paradigm. It is used to promote extensibility and reuse in OO systems. Understanding how systems evolve, and specifically, trends in the movement and re-location of classes in OO hierarchies can help us understand and predict future maintenance effort. In this paper, we explore how and where new classes were added as well as where existing classes were deleted or moved across inheritance hierarchies from multiple versions of four Java systems. We observed first, that in one of the studied systems the same set of classes was continuously moved across the inheritance hierarchy. Second, in the same system, the most frequent changes were restricted to just one sub-part of the overall system. Third, that a maximum of three levels may be a threshold when using inheritance in a system; beyond this level very little activity was observed, supporting\u00a0\u2026", "num_citations": "15\n", "authors": ["152"]}
{"title": "Algebraic validation of software metrics\n", "abstract": " A method is described for the formal evaluation of a software metric and its underlying model. This is based upon the specification of the model as an algebra and its desired behaviour as an associated axiom set. If these axioms can be proved to be invariant across the model, then the model may be considered to be valid with respect to its axioms. Where an axiom cannot be shown to be invariant this implies that either the model is anomalous or that the axiom was inappropriate. This approach is applied to a design metric based upon intermodule coupling. It is argued that this method of metric validation is a general one, and one which is capable of increasing confidence in the correctness of a metric particularly during the early stages of its development when empirical data may either be sparse or unavailable. It is intended as a practical means whereby metrics workers can eliminate pathological metrics\u00a0\u2026", "num_citations": "15\n", "authors": ["152"]}
{"title": "A literature review of expert problem solving using analogy\n", "abstract": " We consider software project cost estimation from a problem solving perspective. Taking a cognitive psychological approach, we argue that the algorithmic basis for CBR tools is not representative of human problem solving and this mismatch could account for inconsistent results. We describe the fundamentals of problem solving, focusing on experts solving ill-defined problems. This is supplemented by a systematic literature review of empirical studies of expert problem solving of nontrivial problems. We identified twelve studies. These studies suggest that analogical reasoning plays an important role in problem solving, but that CBR tools do not model this in a biologically plausible way. For example, the ability to induce structure and therefore find deeper analogies is widely seen as the hallmark of an expert. However, CBR tools fail to provide support for this type of reasoning for prediction. We conclude this mismatch between experts\u2019 cognitive processes and software tools contributes to the erratic performance of analogy-based prediction.", "num_citations": "14\n", "authors": ["152"]}
{"title": "Evaluating software project prediction systems\n", "abstract": " The problem of developing usable software project cost prediction systems is perennial and there are many competing approaches. Consequently, in recent years there have been exhortations to conduct empirically based evaluations in order that our understanding of project prediction might be based upon real world evidence. We now find ourselves in the interesting position of possessing this evidence in abundance. For example, a review of just three software engineering journals identified 50 separate studies and overall several hundred studies have been published. This naturally leads to the next step of needing to construct a body of knowledge, particularly when not all evidence is consistent. This process of forming a body of knowledge is generally referred to as metaanalysis. It is an essential activity if we are to have any hope of making sense of, and utilising, results from our empirical studies. However, it\u00a0\u2026", "num_citations": "13\n", "authors": ["152"]}
{"title": "Re-planning for a successful project schedule\n", "abstract": " Time-to-market or project duration has increasing significance for commercial software development. We report on a longitudinal study of a project at IBM Hursley Park. The focus of this study was schedule behaviour; however, we explored a range of related factors, including planned versus actual progress, resource allocation and functionality delivered. In the course of the 12-month study, evidence was collected from eight interviews, 49 project meetings, a number of project documents and a feedback workshop. The project leader considered the project to be a success, not only in terms of satisfying resource and schedule objectives, but also in the marketplace. Whilst many of the originally planned external commitments were met, it is clear that the project did not adhere to its original (detailed) plan and indeed there were no less than seven re-plans. These re-plans were mainly in response to mis-estimates in the\u00a0\u2026", "num_citations": "13\n", "authors": ["152"]}
{"title": "Using blind analysis for software engineering experiments\n", "abstract": " Context: In recent years there has been growing concern about conflicting experimental results in empirical software engineering. This has been paralleled by awareness of how bias can impact research results.Objective: To explore the practicalities of blind analysis of experimental results to reduce bias.Method: We apply blind analysis to a real software engineering experiment that compares three feature weighting approaches with a na\u00efve benchmark (sample mean) to the Finnish software effort data set. We use this experiment as an example to explore blind analysis as a method to reduce researcher bias.Results: Our experience shows that blinding can be a relatively straightforward procedure. We also highlight various statistical analysis decisions which ought not be guided by the hunt for statistical significance and show that results can be inverted merely through a seemingly inconsequential statistical nicety\u00a0\u2026", "num_citations": "12\n", "authors": ["152"]}
{"title": "Assessing the quality and cleaning of a software project dataset: An experience report\n", "abstract": " OBJECTIVE  \u2013 The aim is to report upon an assessment of the impact noise has on the predictive accuracy by comparing noise handling techniques. METHOD  \u2013 We describe the process of cleaning a large software management dataset comprising initially of more than 10,000 projects. The data quality is mainly assessed through feedback from the data provider and manual inspection of the data. Three methods of noise correction (polishing, noise elimination and robust algorithms) are compared with each other assessing their accuracy. The noise detection was undertaken by using a regression tree model. RESULTS  \u2013 Three noise correction methods are compared and different results in their accuracy where noted. CONCLUSIONS  \u2013 The results demonstrated that polishing improves classification accuracy compared to noise elimination and robust algorithms approaches.", "num_citations": "12\n", "authors": ["152"]}
{"title": "An empirical investigation into P2P file-sharing user behaviour\n", "abstract": " The use of P2P file-sharing networks is one of the most innovative yet under-researched forms of online behaviour. Few academic studies have examined P2P file-sharing networks from the user\u2019s point of view, consequently only little knowledge exists about user behaviour. This research, intended as a pilot study for a doctoral thesis, has revealed through semistructured interviews that (1) users in P2P networks exhibit heterogenous behaviour and that (2) a majority of them use different P2P applications for different tasks, and (3) that the perceived skill and knowledge level influences their online behaviour.", "num_citations": "11\n", "authors": ["152"]}
{"title": "Software measurement methods: an evaluation and perspective\n", "abstract": " Measurement is a much advocated, yet infrequently applied technique of software engineering. A major contributory factor to this state of affairs is that the majority of software metrics are developed, collected and applied in a haphazard fashion. The result is metrics that frequently are poorly formulated, inappropriate to the specific needs and environment of the using organisation and hard to analyse once collected. Over recent years a number of measurement methods-frameworks for developing and applying metrics-have been proposed and used to rectify this state of affairs. This paper describes and evaluates these various methods, identifies strengths and weaknesses leading to an agenda of further work. The requirements of a new measurement method are proposed and emphasises increased application of measurement within context of software development processes.< >", "num_citations": "11\n", "authors": ["152"]}
{"title": "Design metrics and software maintainability: an experimental investigation\n", "abstract": " An empirical study was conducted into the relationship between various design metrics and software maintainability. This was based upon maintenance changes made to four different versions of a project management tool carried out by a total of 60 programmers. The overall conclusion from the investigation, was that accurate prediction of quality characteristics for single maintenance changes is extremely difficult. This is due to the many sources of variation\u2014principally change type and programmer ability. Nevertheless, we show that measures of information flow local to specific modifications are significantly related to error rates, with a 600% greater probability of a residual error as a consequence of a change in a module with a high level of information flow\u2010based coupling, than a module with a low level of coupling. Furthermore, we show that different types of change reveal marked variations in their\u00a0\u2026", "num_citations": "11\n", "authors": ["152"]}
{"title": "Multi-dimensional modelling and measurement of software designs\n", "abstract": " Design structure measures are example of a class of metrics that may be derived early on in a software project; they are useful indicators of design weaknesses-weaknesses which, if uncorrected, lead to problems of implementation, reliability and maintainability. Unfortunately, structure metrics are limited in their ability to model system architecture since they are insensitive to component size. Thus, architectures that trade structural complexity for very large components may not be detected. This paper has two concerns. First, we consider the problem of adequately measuring component size at design time. Various existing metrics are evaluated and found to be deficient. Consequently, a new, more flexible approach based upon the traceability from system requirements to design components, is proposed. Second, we address the issue of multi-dimensional modelling (in this case structure and size). We apply outlier\u00a0\u2026", "num_citations": "11\n", "authors": ["152"]}
{"title": "Replication studies considered harmful\n", "abstract": " Context: There is growing interest in establishing software engineering as an evidence-based discipline. To that end, replication is often used to gain confidence in empirical findings, as opposed to reproduction where the goal is showing the correctness, or validity of the published results. Objective: To consider what is required for a replication study to confirm the original experiment and apply this understanding in software engineering. Method: Simulation is used to demonstrate why the prediction interval for confirmation can be surprisingly wide. This analysis is applied to three recent replications. Results: It is shown that because the prediction intervals are wide, almost all replications are confirmatory, so in that sense there is no 'replication crisis', however, the contributions to knowledge are negligible. Conclusion: Replicating empirical software engineering experiments, particularly if they are under-powered or\u00a0\u2026", "num_citations": "10\n", "authors": ["152"]}
{"title": "Using Counts as Heuristics for the Analysis of Static Models\n", "abstract": " The upstream activities of software development are often viewed as both the most important, in terms of cost, and the yet the least understood, and most problematic, particularly in terms of satisfying customer requirements. Business process modelling is one solution that is being increasingly used in conjunction with traditional software development, often feeding in to requirements and analysis activities. In addition, research in Systems Engineering for Business Process Change, highlights the importance of modelling business processes in evolving and maintaining the legacy systems that support those processes. However, the major use of business process modelling, is to attempt to restructure the business process, in order to improve some given aspect, e.g., cost or time. This restructuring may be seen either as separate activity or as a pre-cursor to the development of systems to support the new or improved process. Hence, the analysis of these business models is vital to the improvement of the process, and as a consequence to the development of supporting software systems. Supporting this analysis is the focus of this paper. Business processes are typically described with static (diagrammatic) models. This paper proposes the use of measures (counts) to aid analysis and comparison of these static process descriptions. The proposition is illustrated by showing how measures can be applied to a commonly used process-modelling notation, Role Activity Diagrams (RADs). Heuristics for RADs are described and measures suggested which support those heuristics. An example process is used to show how a coupling measure can be used to\u00a0\u2026", "num_citations": "10\n", "authors": ["152"]}
{"title": "Algebraic models and metric validation\n", "abstract": " A major problem in the field of software metrics is that much of the work can be characterised as speculative: that is, it requires considerably less effort to propose a metric than it does to produce a convincing validation of its utility. The outcome is a plethora of what might be regarded as putative metrics and a corresponding scarcity of properly validated metrics. This paper outlines a method for the formal evaluation of a software metric and its underlying model. This is based upon the specification of the model as an algebra and its desired behaviour as an associated axiom set. If these axioms can be proved to be invariant across the model, then the model may be considered to be valid with respect to its axioms. Where an axiom cannot be shown to be invariant this implies that either the model is anomalous or that the axiom was inappropriate. This approach is demonstrated with respect to a design metric\u00a0\u2026", "num_citations": "10\n", "authors": ["152"]}
{"title": "Bridging effort-aware prediction and strong classification: A just-in-time software defect prediction study\n", "abstract": " Context: Most research into software defect prediction ignores the differing amount of effort entailed in searching for defects between software components. The result is sub-optimal solutions in terms of allocating testing resources. Recently effort-aware (EA) defect prediction has sought to redress this deficiency. However, there is a gap between previous classification research and EA prediction.Objective: We seek to transfer strong defect classification capability to efficient effort-aware software defect prediction.Method: We study the relationship between classification performance and the cost-effectiveness curve experimentally (using six open-source software data sets).Results: We observe extremely skewed distributions of change size which contributes to the lack of relationship between classification performance and the ability to find efficient test orderings for defect detection. Trimming allows all effort-aware\u00a0\u2026", "num_citations": "9\n", "authors": ["152"]}
{"title": "Measurement of structure and size of software designs\n", "abstract": " Design structure measures are an example of a class of metrics that may be derived early on in a software project; they are useful numeric indicators of design weaknesses \u2014 weaknesses which, if uncorrected, lead to problems of implementation, reliability, and maintainability. Unfortunately, these metrics suffer from certain limitations. In particular, they are limited in their ability to model system architecture due to the fact that they are insensitive to component size. Thus architectures that trade structural complexity for large components by electing to comprise a small number of extremely large modules will not be adequately modelled. The paper has two concerns. First, there is the problem of adequately measuring component size at design time. Various existing metrics are evaluated and found to be deficient. Consequently, a new, more flexible approach, based on the traceability from system requirements to design\u00a0\u2026", "num_citations": "9\n", "authors": ["152"]}
{"title": "System architecture metrics for controlling software maintainability\n", "abstract": " Describes a class of system architecture metrics, based upon the notion of information flows. These metrics are a development of the pioneering work of Sallie Henry and Denis Kefura (1981), and are applicable to the problem of controlling software maintainability. Results from an industrial validation of the relationship between some metrics and actual maintainability are presented. It is concluded that this approach has considerable potential, although there remains much scope for refinement.< >", "num_citations": "9\n", "authors": ["152"]}
{"title": "Authors' Reply to \" Comments on 'Researcher Bias: The Use of Machine Learning in Software Defect Prediction' \"\n", "abstract": " In 2014 we published a meta-analysis of software defect prediction studies [1] . This suggested that the most important factor in determining results was Research Group, i.e., who conducts the experiment is more important than the classifier algorithms being investigated. A recent re-analysis [2] sought to argue that the effect is less strong than originally claimed since there is a relationship between Research Group and Dataset. In this response we show (i) the re-analysis is based on a small (21 percent) subset of our original data, (ii) using the same re-analysis approach with a larger subset shows that Research Group is more important than type of Classifier and (iii) however the data are analysed there is compelling evidence that who conducts the research has an effect on the results. This means that the problem of researcher bias remains. Addressing it should be seen as a matter of priority amongst those of us\u00a0\u2026", "num_citations": "8\n", "authors": ["152"]}
{"title": "Cost prediction and software project management\n", "abstract": " This chapter reviews the background and extent of the software project cost prediction problem. Given the importance of the topic, there has been a great deal of research activity over the past 40 years, most of which has focused on developing formal cost prediction systems. The problem is that presently there is limited evidence to suggest formal methods outperform experts, therefore detailed consideration is given to the available empirical evidence concerning expert performance. This shows that software professionals tend to be biased (optimistic) and over-confident, and there are a number of deep cognitive biases which help us understand why this is so. Finally, the chapter describes how this might best be tackled through a range of simple, practical and evidence-based methods.", "num_citations": "8\n", "authors": ["152"]}
{"title": "Group project work from the outset: An in-depth teaching experience report\n", "abstract": " We redesigned our undergraduate computing programmes to address problems of motivation and outdated content. The primary vehicle for the new curriculum was the group project which formed a central spine for the entire degree right from the first year. In terms of results, thus far this programme has been successfully run once. Failures, drop outs and students required to retake modules have been halved (from an average of 21.6% from the previous 4 years to 9.5%) and students obtaining the top two grades have increased from 25.2% to 38.9%. Whilst we cannot be certain that all improvement is due to the group projects, informally the change has been well received, however, we are looking for areas to improve including the possibility of more structured support for student metacognitive awareness.", "num_citations": "8\n", "authors": ["152"]}
{"title": "An improved method for label matching in e-assessment of diagrams\n", "abstract": " A challenging problem for e-assessment is automatic marking of diagrams. There are a number of difficulties not least that much of the meaning of a diagram resides in the labels and hence label matching is an important process in the e-assessment of diagrams. Previous research has shown that the labels used by the students in the diagrams can be diverse and imprecise which makes this problematic.In this paper we propose and evaluate a new method for label matching to support e-assessment of diagrams and address problems of synonyms, spelling errors and differing levels of decomposition.We have implemented the syntactic part of our method and evaluated it using 160 undergraduate assessments based upon a UML design task. We have found that our method performs better than the other syntax matching algorithms. This framework has significant implications for the ease in which we may develop\u00a0\u2026", "num_citations": "8\n", "authors": ["152"]}
{"title": "An evaluation of e-learning standards\n", "abstract": " The aim of this investigation is to perform an independent study of the various emerging elearning standards. This paper presents a summary of these standards in order to make them more accessible and understandable, and provide preliminary evidence as to their utility and adoption by the various UK higher and further education institutions. Recently there have been efforts to define standards for the elearning contents and elearning components like the IEEELOM, UKLOM, IMS, SCORM and OKI. Since it was not possible to cover all the standards in detail within the time available, so our independent study focuses on eight standards Although the results of the preliminary study suggest that the eight standards considered in the study may help interoperability, accessibility and reusability of the elearning content and elearning components, but it is yet to be seen how many of these are actually followed at UK higher education institutions.", "num_citations": "8\n", "authors": ["152"]}
{"title": "An investigation of rule induction based prediction systems\n", "abstract": " Traditionally, researchers have used either off-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to predict software effort estimates. More recently, attention has turned to a variety of machine learning methods such as artificial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This position paper outlines some preliminary research into the use of rule induction methods to build software cost models. We briefly describe the use of rule induction methods and then apply the technique to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We show that RI methods tend to be unstable and generally predict with quite variable accuracy. Pruning the feature set, however, has a significant impact upon accuracy. We also compare our results with a prediction system based upon a standard regression procedure. We suggest that further work is carried out to examine the effects of the relationships among, and between, the features of the attributes on the generated rules in an attempt to improve on current prediction techniques and enhance our understanding of machine learning methods.", "num_citations": "8\n", "authors": ["152"]}
{"title": "Making software cost data available for meta-analysis\n", "abstract": " In this paper we consider the increasing need for meta-analysis within empirical software engineering. However, we also note that a necessary precondition to such forms of analysis is to have both the results in an appropriate format and sufficient contextural information to avoid misleading inferences. We consider the implications in the field of software project effort estimation and show that for a sample of 12 seemingly similar published studies, the results are difficult to compare let alone combine. This is due to different reporting conventions. We argue that a protocol is required and make some suggestions as to what it should contain.", "num_citations": "7\n", "authors": ["152"]}
{"title": "An empirical study of software project managers using a case-based reasoner\n", "abstract": " BACKGROUND -- whilst substantial effort has been invested in developing and evaluating knowledge-based techniques for project prediction, little is known about the interaction between them and expert users. OBJECTIVE - the aim is to explore the interaction of cognitive processes and personality of software project managers undertaking tool-supported estimation tasks such as effort and cost prediction. METHOD - we conducted personality profiling and observational studies using think-aloud protocols with five senior project managers using a case-based reasoning (CBR) tool to predict effort for real projects. RESULTS - we found pronounced differences between the participants in terms of individual differences, cognitive behaviour and estimation outcomes, although there was a general tendency for over-optimism and over-confidence. CONCLUSIONS - in order to improve task effectiveness in the workplace\u00a0\u2026", "num_citations": "6\n", "authors": ["152"]}
{"title": "Dealing with Missing Software Project Data\n", "abstract": " Whilst there is a general consensus that quantitative approaches are an important adjunct to successful software project management there has been relatively little research into many of the obstacles to data collection and analysis in the real world. One feature that characterises many of the data sets we deal with is missing or highly questionable values. Naturally this problem is not unique to software engineering, so in this paper we explore the application of various data imputation techniques that have been used to good effect elsewhere. In order to assess the potential value of imputation we use two industrial data sets containing a number of missing values. The relative performance of effort prediction models derived by stepwise regression methods on the raw data and data sets with values imputed by various techniques is compared. In both data sets we find that k-Nearest Neighbour (kNN) and sample mean imputation (SMI) significantly improve effort prediction accuracy with the kNN method giving the best results.", "num_citations": "6\n", "authors": ["152"]}
{"title": "Predicting with Sparse Data\n", "abstract": " It is well known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. In this paper we describe our sparse data method based upon a pairwise comparison technique and Saaty's Analytic Hierarchy Process. Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show for two industrial projects how our approach based upon expert judgement adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then descri...", "num_citations": "6\n", "authors": ["152"]}
{"title": "Analysing process models quantitatively\n", "abstract": " Over the years, there has been much interest in modelling processes.  Processes include those associated with the development of software and those business processes that make use of software systems.  Recent research in Systems Engineering for Business Process Change  highlights the importance of modelling business processes in order to evolve and maintain the legacy systems that support those processes.  Business processes are typically described with static (diagrammatic) models.  This paper illustrates how quantitative techniques can facilitate analysis of such models.  This is illustrated with reference to the process modelling notation Role Activity Diagrams (RADs).  An example process, taken from an investigation of the bidding process of a large telecommunications systems supplier, is used to show how a quantitative approach can be used to highlight features in RADs that are useful to the process modeller.  We show how simple measures reveal high levels of role coupling and discrepancies between different perspectives.  Since the models are non-trivial \u2014 there are 101 roles and almost 300 activities \u2014 we argue that quantitative analysis can be a useful adjunct for the modeller.", "num_citations": "6\n", "authors": ["152"]}
{"title": "System architecture metrics: an evaluation\n", "abstract": " The research described in this dissertation is a study of the application of measurement, or metrics for software engineering. This is not in itself a new idea; the concept of measuring software was first mooted close on twenty years ago. However, examination of what is a considerable body of metrics work, reveals that incorporating measurement into software engineering is rather less straightforward than one might pre-suppose and despite the advancing years, there is still a lack of maturity.", "num_citations": "6\n", "authors": ["152"]}
{"title": "Systematic reviews in evidence-based software technology and software engineering\n", "abstract": " It is a pleasure to inform the authors and readers of Information and Software Technology that the journal now is welcoming a new type of contribution\u2014in addition to regular research papers\u2014in the form of systematic reviews. These are an integral part of an evidence-based approach to software technology and software engineering. The objective of systematic reviews is to collate available evidence regarding a specific technique or method in software development. This makes systematic reviews different from traditional literature reviews, where the objective mostly is to summarise the literature without any real synthesis of the findings. Systematic reviews on the other hand are intended to collect results/evidence from different sources and systematically analyse the findings in an objective and repeatable fashion so that the current best knowledge is presented.Systematic reviews are intended to be used as a\u00a0\u2026", "num_citations": "5\n", "authors": ["152"]}
{"title": "The use of cluster techniques and system design metrics in software maintenance\n", "abstract": " Software metrics are an attempt to quantify features of a software product which can be used for activities such as prediction and standards setting. The paper describes a novel use of such software metrics in which they are used to monitor the degree of structural degradation that occurs during the software maintenance process. The technique that is described relies on a branch of statistics known as cluster analysis and also employs algorithms borrowed from the field of classification.< >", "num_citations": "5\n", "authors": ["152"]}
{"title": "Changing the logic of replication: A case from infant studies\n", "abstract": " Among infant researchers there is growing concern regarding the widespread practice of undertaking studies that have small sample sizes and employ tests with low statistical power (to detect a wide range of possible effects). For many researchers, issues of confidence may be partially resolved by relying on replications. Here, we bring further evidence that the classical logic of confirmation, according to which the result of a replication study confirms the original finding when it reaches statistical significance, could be usefully abandoned. With real examples taken from the infant literature and Monte Carlo simulations, we show that a very wide range of possible replication results would in a formal statistical sense constitute confirmation as they can be explained simply due to sampling error. Thus, often no useful conclusion can be derived from a single or small number of replication studies. We suggest that, in order\u00a0\u2026", "num_citations": "4\n", "authors": ["152"]}
{"title": "Combining evidence and meta-analysis in software engineering\n", "abstract": " Recently there has been a welcome move to realign software engineering as an evidence-based practice. Many research groups are actively conducting empirical research e.g. to compare different fault prediction models or the value of various architectural patterns. However, this brings some challenges. First, for a particular question, how can we locate all the relevant evidence (primary studies) and make sense of them in an unbiased way. Second, what if some of these primary studies are inconsistent? In which case how do we determine the \u2018true\u2019 answer? To address these challenges, software engineers are looking to other disciplines where the systematic review is normal practice (i.e. systematic, objective, transparent means of locating, evaluating and synthesising evidence to reach some evidence-based answer to a particular question). This chapter examines the history of empirical software\u00a0\u2026", "num_citations": "4\n", "authors": ["152"]}
{"title": "Applying rule induction in software prediction\n", "abstract": " Recently, the use of machine learning (ML) algorithms has proven to be of great practical value in solving a variety of software engineering problems including software prediction, for example, cost and defect processes. An important advantage of machine learning over statistical analysis as a modelling technique lies in the fact that the interpretation of production rules is more straightforward and intelligible to human beings than, say, principal components and patterns with numbers that represent their meaning. The main focus of this chapter is upon rule induction (RI): providing some background and key issues on RI and further examining how RI has been utilised to handle uncertainties in data. Application of RI in prediction and other software engineering tasks is considered. The chapter concludes by identifying future research work when applying rule induction in software prediction. Such future research work\u00a0\u2026", "num_citations": "4\n", "authors": ["152"]}
{"title": "Looking at comparisons of regression and analogy-based software project cost prediction\n", "abstract": " OBJECTIVE? This paper builds on our previous research in which we found inconsistency within and between results in empirical studies of software engineering cost estimation which had compared regression and analogy techniques. To this end, this paper aims to determine why and how these inconsistencies occur. In addition, we attempt to provide a solution which might reduce inconsistencies in future. METHOD? We retrieved a sample of 11 journal papers and 9 conference papers which compared both regression and analogy-based prediction methods. Analysis of these papers uncovered inconsistencies within and between results. From this starting point we concluded that researchers should look beyond asking? What is the best prediction system?? Thus we now consider the reasons for these seemingly conflicting results. RESULTS? We found little consistency between studies both methodologically and in terms of outcome. CONCLUSIONS? We propose that researchers adopt more rigorous and uniform protocols when empirically investigating project cost prediction methods and interpreting the results.", "num_citations": "4\n", "authors": ["152"]}
{"title": "Meta-data to guide retrieval in CBR for software cost prediction\n", "abstract": " In recent years, case-based prediction has become a widely advocated technique for software cost estimation. Typically, such approaches are in essence k nearest neighbour methods supported by a case base of a feature vector per software project. Given that software project data is relatively rare\u2013data sets may contain as few as 20 cases\u2013it is common to find a relatively undiscriminating approach to the projects contained within the case bases. We hypothesise that meta-data generated by monitoring case performance can contribute to identifying misleading cases and improve predictions. This paper reports results of a pilot study in which we enriched our case base with meta-data to record performance behaviour of individual data sets. An external fuzzy model was used to classify individual cases as fit or unfit for future use. Misleading cases ie with poor predictive ability were seeded into the data set to assess the potential of the approach. Our results show that the model successfully identified the seed cases and refrained from using them during future retrievals.", "num_citations": "4\n", "authors": ["152"]}
{"title": "A Short Note on Using Multiple Imputation Techniques for Very Small Data Sets\n", "abstract": " This short note describes a simple experiment to investigate the value of using multiple imputation (MI) methods [2, 3]. We are particularly interested in whether a simple bootstrap based on a k-nearest neighbour (kNN) method can help address the problem of missing values in two very small, but typical, software project data sets. This is an important question because, unfortunately, many real-world project data sets contain few projects (cases) and so ignoring techniques lead to wasteage of precious data. Moreover, ignoring techniques can also lead to bias if the missingness mechanism is not random [1].", "num_citations": "4\n", "authors": ["152"]}
{"title": "A pragmatic approach to process modelling\n", "abstract": " Many current process modelling approaches are notationally complex, and therefore inappropriate within relatively small software development environments. What is needed is a more pragmatic approach. This paper describes some process modelling work, based upon data flow techniques, conducted at a small software development organization. Our findings suggest that significant benefits can accrue from even a low cost approach. We also discuss some practical lessons learnt for the would-be process modeller.", "num_citations": "4\n", "authors": ["152"]}
{"title": "Software metrics in software engineering and artificial intelligence\n", "abstract": " This paper examines the utility of much of the software metrics  research that has been carried out in software engineering to problems  in artificial intelligence. The paper first reviews the work that  has been carried out and then makes a number of suggestions about how  it could be transferred to the artificial intelligence arena \u2014  applying it in particular to expert system development.", "num_citations": "4\n", "authors": ["152"]}
{"title": "Evolving Software with Multiple Outputs and Multiple Populations.\n", "abstract": " In this research we are concerned with the automatic evolution of programs for control applications, the particular example we use being software for a simple fridge device with two inputs and three outputs. By careful choice of the target programming language-in a similar vein to a RISC processor-we are able to represent programs as variable length strings and use evolutionary computing techniques to search for fitter individuals. We used a fitness function that summed the fitness of each output channel, by various methods, in an attempt to encourage a total solution using a single population of candidate solutions. In general we were able to successfully evolve suitable solutions, however, the search sometimes suffered from premature convergence once the functionality for two out of the three output channels had evolved. More complex fitness assessment schemes, using mechanisms such as dynamically modifying the fitness associated with an output channel without additional benefit.. These difficulties in attempting to do too much with a single population pointed to a \u2018divide and conquer\u2019approach whereby one (or more) populations are dedicated to solving for one output channel alone-whilst being exposed to all inputs. This is seen to be an acceptable approach due to the growth in multi-tasking operating systems and multiprocessor platforms.", "num_citations": "3\n", "authors": ["152"]}
{"title": "The analytic hierarchy process and almost dataless prediction\n", "abstract": " A major challenge for managers of software projects is to be able to make accurate predictions. For example, how long will a project take and how much effort will it require? Moreover, these predictions are required at an early stage during the course of a project. To answer this type of question has been a major challenge of workers in field of software metrics for the past 25 years. Unfortunately, most approaches (eg regression based models and machine learning methods) require data for calibration or training purposes. This paper considers the problem of building useful effort prediction systems in the absence of structured historical data. In order to overcome this problem we propose the use of Analytic Hierarchy", "num_citations": "3\n", "authors": ["152"]}
{"title": "A metrics based tool for software design\n", "abstract": " Decision making is an essential part of the design process. Unfortunately, software engineers are frequently forced to make important, and for safety critical systems, vital decisions in the absence of feedback. A software tool is presented which addresses this need for input at the design stage by building a model of the system architecture over which a number of metrics may be defined. At a system level these design metrics enable the characterisation and comparison of differing architectures for the same specification. At an intra-system level the metrics can help pinpoint weaknesses and thus facilitate the generation of new architectures. Empirical work suggests that such measures can be very effective at identifying potentially problematic designs and design components. In addition the tool allows the user to tailor the metrics to his or her specific needs and environment.< >", "num_citations": "3\n", "authors": ["152"]}
{"title": "An Empirical Analysis of Software Productivity\n", "abstract": " The aim of our research is to discover what factors impact software project productivity (measured as function points per hour) using real world data. Within this overall goal we also compare productivity between different business sectors and project types. We analysed a data set of more than 661 projects that have been collected by STTF from a number of Finnish companies since 1978. These projects are quite diverse type (new and maintenance projects), in terms of size (6 to over 5000 function points), effort (55 to over 60000 person hours), application domain and implementation technology. There are three main findings. First productivity varies enormously between projects. Second, project type has limited influence on productivity. Third, application domain or business area has a major impact upon productivity. Because this data set is not a random sample generalisation is somewhat problematic, we hope that it contributes to an overall body of knowledge about software productivity and thereby facilitates the construction of a bigger picture.", "num_citations": "2\n", "authors": ["152"]}
{"title": "Assessing Case Base Quality\n", "abstract": " BACKGROUND-In Case\u2013Based Prediction systems, where the solution is a continuous value, there is heavy reliance upon problem\u2013solution regularity. Often, it is hard to measure, judge or even compare accuracy of such solutions. OBJECTIVE-Our aim is to deduce a reflection of the case base\u2019s regularity before deployment which may be crucial to determine the degree of confidence on the delivered solutions and potentially increase prediction accuracy. METHOD-We propose the use of Mantel\u2019s Randomisation test and case base regularity visualisation methods to judge overall case base regularity. Thereafter, we focus upon techniques that concentrate on individual constituent cases to single out ones that contribute towards overall poor performance. We then present a case discrimination system the successfully overlooks poor cases to enhance solution quality. RESULTS-Our results shed light on the quality of the case base and partially explain poor solution quality. We also identified problematic cases that substantially contributed to overall irregularity and provided the stepping stones to enhance prediction quality in our problem domain. Lastly, we exploited such insight into the case base to increase the accuracy of proposed solution.", "num_citations": "2\n", "authors": ["152"]}
{"title": "Dynamic models of maintenance behaviour\n", "abstract": " This position paper considers the problem of understanding the dynamic behaviour of software systems and in particular from a maintenance perspective. One approach is to consider the dynamics, in particular feedback loops, of such systems. We briefly report on the limited work in the area to date [1, 2] and then describe a case study derived from an information systems for a multi-national organisation. We examine change documents (CD) data over a period of four years. This enables us to treat the data as a time series and search for trends. Other researchers have investigated patterns in the evolution of systems over time and in particular evidence of feedback processes. We noted similar evidence and generated a model to simulate system behaviour over time. The model incorporates two levels of feedback. We compare actual with simulated behaviour and note a good correspondence. From this research, we argue that the analysis of the dynamic behaviour of systems may lead to a better understanding of the behaviour and management of large software systems over time.", "num_citations": "2\n", "authors": ["152"]}
{"title": "The analytic hierarchy process and data-less prediction\n", "abstract": " Building useful effort prediction systems for software engineering projects is difficult in the absence of historical data. To overcome this problem we propose the use of Saaty\u2019s Analytic Hierarchy Process (AHP) to elicit subjective expert opinion using pairwise comparisons. We briefly describe the theory of AHP and then outline how it might be harnessed to overcome a lack of historical data. Two case studies, using data from BT and another telecommunications company, illustrate the method\u2019s potential. If all the comparisons are correctly made, extremely accurate results are possible (of the order of mean magnitude of relative errors (MMRE)= 5%). The approach is also very tolerant to incorrect comparisons: even when 40% of all comparisons are randomised, the accuracy is still much better than a linear regression model or expert judgement. Next we describe a simple pilot study which indicates that project managers are capable of making the pairwise judgements our method requires. We then consider some problems that need to be overcome before this approach can easily be deployed. These include lack of homogeneity in the elements being compared and excessive numbers of comparisons. We believe this novel effort prediction approach has given encouraging results and warrants further investigation.", "num_citations": "2\n", "authors": ["152"]}
{"title": "The impact of using biased performance metrics on software defect prediction research\n", "abstract": " Context:Software engineering researchers have undertaken many experiments investigating the potential of software defect prediction algorithms. Unfortunately some widely used performance metrics are known to be problematic, most notably F1, but nevertheless F1 is widely used.Objective:To investigate the potential impact of using F1 on the validity of this large body of research.Method:We undertook a systematic review to locate relevant experiments and then extract all pairwise comparisons of defect prediction performance using F1 and the unbiased Matthews correlation coefficient (MCC).Results:We found a total of 38 primary studies. These contain 12,471 pairs of results. Of these comparisons, 21.95% changed direction when the MCC metric is used instead of the biased F1 metric. Unfortunately, we also found evidence suggesting that F1 remains widely used in software defect prediction research\u00a0\u2026", "num_citations": "1\n", "authors": ["152"]}
{"title": "Changing the logic of replication\n", "abstract": " Infant research is making considerable progresses. However, among infant researchers there is growing concern regarding the widespread habit of undertaking studies that have small sample sizes and are under-powered. For many researchers, issues of confidence may be partially resolved by relying on replications. Here, we argue that the classical logic of confirmation, according to which the result of a replication study confirms the original finding when reaches statistical significance, could be fruitfully abandoned. With real examples taken from the literature on infant socio-moral preferences and Monte Carlo simulations, we show that a very wide range of possible replication results would in a formal statistical sense constitute confirmation as they can be explained simply due to sampling error. Of course, this does not advance our understanding. No useful conclusion can be derived from a single or small number of replication studies. We therefore suggest that, in order to accumulate and generate new knowledge, the dichotomous view of replication as confirmatory/disconfirmatory can be replaced by an approach that places more emphasis on estimation of effect sizes via metaanalysis.", "num_citations": "1\n", "authors": ["152"]}
{"title": "Replicated results are more trustworthy\n", "abstract": " The need to replicate results is a central tenet throughout science, and empirical software engineering is no exception. The reasons are threefold.", "num_citations": "1\n", "authors": ["152"]}
{"title": "The scientific basis for prediction research\n", "abstract": " In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. We also need comparable studies. I will show through a meta-analysis of many primary studies that we are not presently in that situation and so the scientific basis for our collective research remains in doubt. By way of remedy I will argue that we need to address these issues of reporting protocols and expertise plus ensure blind analysis is routine.", "num_citations": "1\n", "authors": ["152"]}
{"title": "Personality and analogy-based project estimation\n", "abstract": " The aim of this research is to investigate the relationship between personality and expert prediction behaviour when estimating software project effort using analogical reasoning. For some years we have been developing tools and techniques for estimation by analogy (EBA). However the variability of results from using these tools and techniques can be difficult to interpret. We have conducted a pilot study to integrate knowledge from cognitive psychology and computer science to investigate how to improve estimation when using analogy-based tools. We interviewed and assessed the personality of two experienced project managers to gain an understanding of their background and the problem solving strategies they currently employ. Following these interviews, the project managers were given a typical project effort estimation task. The project managers were asked to complete the task using our analogical reasoning tool and articulate their processes by means of a \u2018think aloud\u2019 protocol. We found significant differences in prediction approach that may be in part be explained by personality differences. One aspect, i.e. the strong need to acquire personal understanding may present obstacles to the successful use of some prediction tools.", "num_citations": "1\n", "authors": ["152"]}
{"title": "The evolution of concurrent control software using genetic programming\n", "abstract": " Despite considerable progress in GP over the past 10 years, there are many outstanding challenges that need to be addressed before it will be widely deployed for developing useful software. In this paper we suggest a method for the automatic creation of concurrent control software using Linear Genetic Programming (LGP) and a \u2018divide and conquer\u2019 approach. The method involves decomposing the whole problem into a multi-task solution with multiple inputs and multiple outputs \u2013 similar to the process used to implement embedded control solutions. We describe the necessary architecture of typical embedded control systems and their relevance to this work, the software evolution scheme used and lastly demonstrate the technique for an embedded software problem, namely a washing machine controller.", "num_citations": "1\n", "authors": ["152"]}
{"title": "An empirical investigation into waiting in software development projects\n", "abstract": " Evidence, in the form of project status meeting minutes, was collected from two real-world software development projects in order to investigate aspects of waiting. These investigations addressed the prevalence of waiting at the end of the project, the relationship between the number of phases and the amount of waiting per week, the \u2018waiting relationships\u2019 between \u2018source\u2019and \u2018dependent\u2019functional areas, and the types of waiting that occur in a project. Part of the empirical study stands as a complementary study to a study conducted by Bradac, Perry and Votta [4]. The current study confirms Bradac et al.\u2019s conjecture that waiting is more prevalent at the end of a project, but does not confirm the types of waiting proposed by Bradac et al. The current study also found evidence for a correlation between the number of phases and the amount of waiting per week, and evidence that the Development and Test functional areas are the most likely to be waiting on other areas. The study is evaluated in terms of differences with Bradac et al.\u2019s study, differences between the two projects that were investigated, and possible explanations for the behaviour that was observed. Avenues for further research are also considered.", "num_citations": "1\n", "authors": ["152"]}
{"title": "A framework for investigating software project schedule behaviour1\n", "abstract": " This paper describes an ongoing empirical investigation into software project schedule behaviour at a leading, international software development organisation. Due to the complexity of the phenomena being observed and the diversity and volume of empirical evidence being collected, a framework is proposed. The framework both facilitates the organisation of single and multiple sources of evidence and acts as a foundation for the subsequent analysis of that evidence. The core of the framework is concerned with the interaction between three main constructs, duration, capability and workload. In addition to considering other evidence, the paper reports on the application of this framework to over 2000 statements from project meeting minutes. Rather than presenting definitive findings, the paper demonstrates how the framework can be used to organise and analyse \u201creal world\u201d complex evidence of a variety of types, and evaluates the use of the framework for such organisation and analysis. The main conclusion from the evaluation was that incompatibilities between the requirements of the framework and the nature of the evidence lead to difficulties in using the framework effectively. This has challenged our initial position, which was that the framework would prove to be a practical method for investigating software project schedule behaviour.", "num_citations": "1\n", "authors": ["152"]}
{"title": "ISO-9000-3 AND SOFTWARE MEASUREMENT\n", "abstract": " This review paper discusses the application of software metrication to companies who are registered to ISO 9000, or hoping to be assessed against ISO 9000 within a short time. Particular reference is made within the paper to ISO 9000-3. The paper examines a number of conventional product metrics, together with metrics associated with more conventional engineering approaches to development, for example, those numbers generated from earned value systems of project monitoring. After Briefly reviewing the metrics, the paper describes which parts of Iso 9000-3 they are able to support, with specific uses of the metrics being discussed, for example, its use as a qualify threshold mechanism, as a check on conventional costing methods, as a technique for monitoring project progress, as a technique for checking on the functional growth of the system, and as a method for ensuring that qualify does not degrade\u00a0\u2026", "num_citations": "1\n", "authors": ["152"]}