{"title": "The SeaHorn verification framework\n", "abstract": " In this paper, we present SeaHorn, a software verification framework. The key distinguishing feature of SeaHorn is its modular design that separates the concerns of the syntax of the programming language, its operational semantics, and the verification semantics. SeaHorn encompasses several novelties: it (a)\u00a0encodes verification conditions using an efficient yet precise inter-procedural technique, (b)\u00a0provides flexibility in the verification semantics to allow different levels of precision, (c)\u00a0leverages the state-of-the-art in software model checking and abstract interpretation for verification, and (d)\u00a0uses Horn-clauses as an intermediate language to represent verification conditions which simplifies interfacing with multiple verification tools based on Horn-clauses. SeaHorn provides users with a powerful verification tool and researchers with an extensible and customizable framework for experimenting with new\u00a0\u2026", "num_citations": "280\n", "authors": ["1641"]}
{"title": "SMT-based model checking for recursive programs\n", "abstract": " We present an SMT-based symbolic model checking algorithm for safety verification of recursive programs. The algorithm is modular and analyzes procedures individually. Unlike other SMT-based approaches, it maintains both over- and under-approximations of procedure summaries. Under-approximations are used to analyze procedure calls without inlining. Over-approximations are used to block infeasible counterexamples and detect convergence to a proof. We show that for programs and properties over a decidable theory, the algorithm is guaranteed to find a counterexample, if one exists. However, efficiency depends on an oracle for quantifier elimination (QE). For Boolean programs, the algorithm is a polynomial decision procedure, matching the worst-case bounds of the best BDD-based algorithms. For Linear Arithmetic (integers and rationals), we give an efficient instantiation of the algorithm by\u00a0\u2026", "num_citations": "207\n", "authors": ["1641"]}
{"title": "SeaHorn: A framework for verifying C programs (competition contribution)\n", "abstract": " seahorn is a framework and tool for verification of safety properties in C programs. The distinguishing feature of seahorn is its modular design that separates how program semantics is represented from the verification engine. This paper describes its verification approach as well as the instructions on how to install and use it.", "num_citations": "57\n", "authors": ["1641"]}
{"title": "Boxes: A symbolic abstract domain of boxes\n", "abstract": " Numeric abstract domains are widely used in program analyses. The simplest numeric domains over-approximate disjunction by an imprecise join, typically yielding path-insensitive analyses. This problem is addressed by domain refinements, such as finite powersets, which provide exact disjunction. However, developing correct and efficient disjunctive refinement is challenging. First, there must be an efficient way to represent and manipulate abstract values. The simple approach of using \u201csets of base abstract values\u201d is often not scalable. Second, while a widening must strike the right balance between precision and the rate of convergence, it is notoriously hard to get correct. In this paper, we present an implementation of the Boxes abstract domain \u2013 a refinement of the well-known Box (or Intervals) domain with finite disjunctions. An element of Boxes is a finite union of boxes, i.e., expressible as a\u00a0\u2026", "num_citations": "57\n", "authors": ["1641"]}
{"title": "Property directed polyhedral abstraction\n", "abstract": " This paper combines the benefits of Polyhedral Abstract Interpretation (poly-AI) with the flexibility of Property Directed Reachability (PDR) algorithms for computing safe inductive convex polyhedral invariants. We develop two algorithms that integrate Poly-AI with PDR and show their benefits on a prototype in Z3 using a preliminary evaluation. The algorithms mimic traditional forward Kleene and a chaotic backward iterations, respectively. Our main contribution is showing how to replace expensive convex hull and quantifier elimination computations, a major bottleneck in poly-AI, with demand-driven property-directed algorithms based on interpolation and model-based projection. Our approach integrates seamlessly within the framework of PDR adapted to Linear Real Arithmetic, and allows to dynamically decide between computing convex and non-convex invariants as directed by the property.", "num_citations": "41\n", "authors": ["1641"]}
{"title": "Interpolating property directed reachability\n", "abstract": " Current SAT-based Model Checking is based on two major approaches: Interpolation-based (Imc) (global, with unrollings) and Property Directed Reachability/IC3 (Pdr) (local, without unrollings). Imc generates candidate invariants using interpolation over an unrolling of a system, without putting any restrictions on the SAT-solver\u2019s search. Pdr generates candidate invariants by a local search over a single instantiation of the transition relation, effectively guiding the SAT solver\u2019s search. The two techniques are considered to be orthogonal and have different strength and limitations. In this paper, we present a new technique, called Avy, that effectively combines the key insights of the two approaches. Like Imc, it uses unrollings and interpolants to construct an initial candidate invariant, and, like Pdr, it uses local inductive generalization to keep the invariants in compact clausal form. On the one hand, Avy is an\u00a0\u2026", "num_citations": "40\n", "authors": ["1641"]}
{"title": "Compositional verification of procedural programs using horn clauses over integers and arrays\n", "abstract": " We present a compositional SMT-based algorithm for safety of procedural C programs that takes the heap into consideration as well. Existing SMT-based approaches are either largely restricted to handling linear arithmetic operations and properties, or are non-compositional. We use Constrained Horn Clauses (CHCs) to represent the verification conditions where the memory operations are modeled using the extensional theory of arrays (ARR). First, we describe an exponential time quantifier elimination (QE) algorithm for ARR which can introduce new quantifiers of the index and value sorts. Second, we adapt the QE algorithm to efficiently obtain under-approximations using models, resulting in a polynomial time Model Based Projection (MBP) algorithm. Third, we integrate the MBP algorithm into the framework of compositional reasoning of procedural programs using may and must summaries recently proposed\u00a0\u2026", "num_citations": "35\n", "authors": ["1641"]}
{"title": "Efficient predicate abstraction of program summaries\n", "abstract": " Predicate abstraction is an effective technique for scaling Software Model Checking to real programs. Traditionally, predicate abstraction abstracts each basic block of a program  to construct a small finite abstract model \u2013 a Boolean program BP, whose state-transition relation is over some chosen (finite) set of predicates. This is called Small-Block Encoding (SBE). A recent advancement is Large-Block Encoding (LBE) where abstraction is applied to a \u201csummarized\u201d program so that the abstract transitions of BP correspond to loop-free fragments of . In this paper, we expand on the original notion of LBE to promote flexibility. We explore and describe efficient ways to perform CEGAR bottleneck operations: generating and solving predicate abstraction queries (PAQs). We make the following contributions. First, we define a general notion of program summarization based on loop cutsets. Second, we give a\u00a0\u2026", "num_citations": "33\n", "authors": ["1641"]}
{"title": "SMT-based verification of parameterized systems\n", "abstract": " It is well known that verification of safety properties of sequential programs is reducible to satisfiability modulo theory of a first-order logic formula, called a verification condition (VC). The reduction is used both in deductive and automated verification, the difference is only in whether the user or the solver provides candidates for inductive invariants. In this paper, we extend the reduction to parameterized systems consisting of arbitrary many copies of a user-specified process, and whose transition relation is definable in first-order logic modulo theory of linear arithmetic and arrays. We show that deciding whether a parameterized system has a universally quantified inductive invariant is reducible to satisfiability of (non-linear) Constraint Horn Clauses (CHC). As a consequence of our reduction, we obtain a new automated procedure for verifying parameterized systems using existing PDR and CHC engines. While the\u00a0\u2026", "num_citations": "32\n", "authors": ["1641"]}
{"title": "Synthesizing ranking functions from bits and pieces\n", "abstract": " In this work, we present a novel approach based on recent advances in software model checking to synthesize ranking functions and prove termination (and non-termination) of imperative programs.                 Our approach incrementally refines a termination argument from an under-approximation of the terminating program state. Specifically, we learn bits of information from terminating executions, and from these we extrapolate ranking functions over-approximating the number of loop iterations needed for termination. We combine these pieces into piecewise-defined, lexicographic, or multiphase ranking functions.                 The proposed technique has been implemented in SeaHorn \u2013 an LLVM based verification framework \u2013 targeting C code. Preliminary experimental evaluation demonstrated its effectiveness in synthesizing ranking functions and proving termination of C programs.", "num_citations": "28\n", "authors": ["1641"]}
{"title": "Quantifiers on demand\n", "abstract": " Automated program verification is a difficult problem. It is undecidable even for transition systems over Linear Integer Arithmetic (LIA). Extending the transition system with theory of Arrays, further complicates the problem by requiring inference and reasoning with universally quantified formulas. In this paper, we present a new algorithm, Quic3, that extends IC3 to infer universally quantified invariants over the combined theory of LIA and Arrays. Unlike other approaches that use either IC3 or an SMT solver as a black box, Quic3 carefully manages quantified generalization (to construct quantified invariants) and quantifier instantiation (to detect convergence in the presence of quantifiers). While Quic3 is not guaranteed to converge, it is guaranteed to make progress by exploring longer and longer executions. We have implemented Quic3 within the Constrained Horn Clause solver engine of Z3 and\u00a0\u2026", "num_citations": "25\n", "authors": ["1641"]}
{"title": "A Context-Sensitive Memory Model for Verification of C/C++ Programs\n", "abstract": " Verification of low-level C/C++ requires a precise memory model that supports type unions, pointer arithmetic, and casts. We present a new memory model that splits memory into a finite set of disjoint regions based on a pointer analysis. The main contribution is a field-, array- and context-sensitive pointer analysis tailored to verification. We have implemented our memory model for the LLVM bitcode and used it on a C++ case study and on SV-COMP benchmarks. Our results suggests that our model can reduce verification time by producing a finer-grained partitioning in presence of function calls.", "num_citations": "25\n", "authors": ["1641"]}
{"title": "Reliability validation and improvement framework\n", "abstract": " Software-reliant systems such as rotorcraft and other aircraft have experienced exponential growth in software size and complexity. The current software engineering practice of build then test has made them unaffordable to build and qualify. This report discusses the challenges of qualifying such systems, presenting the findings of several government and industry studies. It identifies several root cause areas and proposes a framework for reliability validation and improvement that integrates several recommended technology solutions validation of formalized requirements an architecture-centric, model-based engineering approach that uncovers system-level problems early through analysis use of static analysis for validating system behavior and other system properties and managed confidence in qualification through system assurance. This framework also provides the basis for a set of metrics for cost-effective reliability improvement that overcome the challenges of existing software complexity, reliability, and cost metrics.Descriptors:", "num_citations": "25\n", "authors": ["1641"]}
{"title": "BDD-based symbolic model checking\n", "abstract": " Symbolic model checking based on Binary Decision Diagrams (BDDs) is one of the most celebrated breakthroughs in the area of formal verification. It was originally proposed in the context of hardware model checking, and advanced the state of the art in model-checking capability by several orders of magnitude in terms of the sizes of state spaces that could be explored successfully. More recently, it has been extended to the domain of software verification as well, and several BDD-based model checkers for Boolean programs and push-down systems have been developed. In this chapter, we summarize some of the key concepts and techniques that have emerged in this story of successful practical verification.", "num_citations": "23\n", "authors": ["1641"]}
{"title": "Synthesizing safe bit-precise invariants\n", "abstract": " Bit-precise software verification is an important and difficult problem. While there has been an amazing progress in SAT solving, Satisfiability Modulo Theory of Bit Vectors, and bit-precise Bounded Model Checking, proving bit-precise safety, i.e.\u00a0synthesizing a safe inductive invariant, remains a challenge. Although the problem is decidable and is reducible to propositional safety by bit-blasting, the approach does not scale in practice. The alternative approach of lifting propositional algorithms to bit-vectors is difficult. In this paper, we propose a novel technique that uses unsound approximations (i.e., neither over- nor under-) for synthesizing sound bit-precise invariants. We prototyped the technique using Z3/PDR engine and applied it to bit-precise verification of benchmarks from SVCOMP\u201913. Even with our preliminary implementation we were able to demonstrate significant (orders of magnitude) performance\u00a0\u2026", "num_citations": "23\n", "authors": ["1641"]}
{"title": "Four pillars for improving the quality of safety-critical software-reliant systems\n", "abstract": " Studies of safety-critical software-reliant systems developed using the current practices of build-then-test show that requirements and architecture design defects make up approximately 70 of all defects, many system level related to operational quality attributes, and 80 of these defects are discovered late in the development life cycle Redman 2010. Exponential growth in software size and complexity has pushed the cost for the current generation of aircraft to the limit of affordability.Descriptors:", "num_citations": "22\n", "authors": ["1641"]}
{"title": "Combining predicate and numeric abstraction for software model checking\n", "abstract": " Predicate (PA) and numeric (NA) abstractions are the two principal techniques for software analysis. In this paper, we develop an approach to couple the two techniques tightly into a unified framework via a single abstract domain called NumPredDom. In particular, we develop and evaluate four data structures that implement NumPredDom but differ in their expressivity and internal representation and algorithms. All our data structures combine BDDs (for efficient propositional reasoning) with data structures for representing numerical constraints. Our technique is distinguished by its support for complex transfer functions that allow two-way interaction between predicate and numeric information during state transformation. We have implemented a general framework for reachability analysis of C programs on top of our four data structures. Our experiments on non-trivial examples show that our proposed\u00a0\u2026", "num_citations": "22\n", "authors": ["1641"]}
{"title": "Property directed self composition\n", "abstract": " We address the problem of verifying k-safety properties: properties that refer to k interacting executions of a program. A prominent way to verify k-safety properties is by self composition. In this approach, the problem of checking k-safety over the original program is reduced to checking an \u201cordinary\u201d safety property over a program that executes k copies of the original program in some order. The way in which the copies are composed determines how complicated it is to verify the composed program. We view this composition as provided by a semantic self composition function that maps each state of the composed program to the copies that make a move.  Since the \u201cquality\u201d of a self composition function is measured by the ability to verify the safety of the composed program, we formulate the problem of inferring a self composition function together with the inductive invariant needed to verify safety of the\u00a0\u2026", "num_citations": "21\n", "authors": ["1641"]}
{"title": "Supervised learning for provenance-similarity of binaries\n", "abstract": " Understanding, measuring, and leveraging the similarity of binaries (executable code) is a foundational challenge in software engineering. We present a notion of similarity based on provenance--two binaries are similar if they are compiled from the same (or very similar) source code with the same (or similar) compilers. Empirical evidence suggests that provenance-similarity accounts for a significant portion of variation in existing binaries, particularly in malware. We propose and evaluate the applicability of classification to detect provenance-similarity. We evaluate a variety of classifiers, and different types of attributes and similarity labeling schemes, on two benchmarks derived from open-source software and malware respectively. We present encouraging results indicating that classification is a viable approach for automated provenance-similarity detection, and as an aid for malware analysts in particular.", "num_citations": "21\n", "authors": ["1641"]}
{"title": "Pushing to the top\n", "abstract": " IC3 is undoubtedly one of the most successful and important recent techniques for unbounded model checking. Understanding and improving IC3 has been a subject of a lot of recent research. In this regard, the most fundamental questions are how to choose Counterexamples to Induction (CTIs) and how to generalize them into (blocking) lemmas. Answers to both questions influence performance of the algorithm by directly affecting the quality of the lemmas learned. In this paper, we present a new IC3-based algorithm, called QUIP1, that is designed to more aggressively propagate (or push) learned lemmas to obtain a safe inductive invariant faster. QUIP modifies the recursive blocking procedure of IC3 to prioritize pushing already discovered lemmas over learning of new ones. However, a naive implementation of this strategy floods the algorithm with too many useless lemmas. In QUIP, we solve this by extending\u00a0\u2026", "num_citations": "18\n", "authors": ["1641"]}
{"title": ": A Predictive Run-Time Verification Framework Using Statistical Learning\n", "abstract": " Run-time Verification (RV) is an essential component of developing cyber-physical systems, where often the actual model of the system is infeasible to obtain or is not available. In the absence of a model, i.e., black-box systems, RV techniques evaluate a property on the execution path of the system and reach a verdict that the current state of the system satisfies or violates a given property. In this paper, we introduce , a predictive runtime verification framework, in which if a property is not currently satisfied, the monitor generates the probability based on the finite extensions of the execution path, that satisfy the specification property. We use Hidden Markov Model (HMM) to extend the partially observable paths of the system. The HMM is trained on a set of iid samples generated by the system. We then use reachability analysis to construct a lookup table that provides the probability that the extended path\u00a0\u2026", "num_citations": "16\n", "authors": ["1641"]}
{"title": "Druping for interpolates\n", "abstract": " We present a method for interpolation based on DRUP proofs. Interpolants are widely used in model checking, synthesis and other applications. Most interpolation algorithms rely on a resolution proof produced by a SAT-solver for unsatisfaible formulas. The proof is traversed and translated into an interpolant by replacing resolution steps with AND and OR gates. This process is efficient (once there is a proof) and generates interpolants that are linear in the size of the proof. In this paper, we address three known weakness of this approach: (i) performance degradation experienced by the SAT-solver and the extra memory requirements needed when logging a resolution proof; (ii) the proof generated by the solver is not necessarily the \"best\" proof for interpolantion, and (iii) combining proof logging with pre-processing is complicated. We show that these issues can be remedied by using DRUP proofs. First, we show\u00a0\u2026", "num_citations": "16\n", "authors": ["1641"]}
{"title": "Automated assume-guarantee reasoning for omega-regular systems and specifications\n", "abstract": " We develop a learning-based automated assume-guarantee (AG) reasoning framework for verifying \u03c9-regular properties of concurrent systems. We study the applicability of non-circular (AG-NC) and circular (AG-C) AG proof rules in the context of systems with infinite behaviors. In particular, we show that AG-NC is incomplete when assumptions are restricted to strictly infinite behaviors, while AG-C remains complete. We present a general formalization, called LAG, of the learning based automated AG paradigm. We show how existing approaches for automated AG reasoning are special instances of LAG. We develop two learning algorithms for a class of systems, called \u221e-regular systems, that combine finite and infinite behaviors. We show that for \u221e-regular systems, both AG-NC and AG-C are sound and complete. Finally, we show how to instantiate LAG to do automated AG reasoning for \u221e-regular, and \u03c9\u00a0\u2026", "num_citations": "14\n", "authors": ["1641"]}
{"title": "Validity-guided synthesis of reactive systems from assume-guarantee contracts\n", "abstract": " Automated synthesis of reactive systems from specifications has been a topic of research for decades. Recently, a variety of approaches have been proposed to extend synthesis of reactive systems from propositional specifications towards specifications over rich theories. We propose a novel, completely automated approach to program synthesis which reduces the problem to deciding the validity of a set of -formulas. In spirit of IC3/PDR, our problem space is recursively refined by blocking out regions of unsafe states, aiming to discover a fixpoint that describes safe reactions. If such a fixpoint is found, we construct a witness that is directly translated into an implementation. We implemented the algorithm on top of the JKind model checker, and exercised it against contracts written using the Lustre specification language. Experimental results show how the new algorithm outperforms JKind\u2019s already\u00a0\u2026", "num_citations": "13\n", "authors": ["1641"]}
{"title": "K-induction without unrolling\n", "abstract": " We present a flexible algorithmic framework KIC3 that combines IC3 and k-induction. The key underlying observation is that k-induction can be easily simulated by existing IC3 implementations by following a slightly different counterexample-queue management strategy.", "num_citations": "13\n", "authors": ["1641"]}
{"title": "Fast interpolating BMC\n", "abstract": " Bounded Model Checking (BMC) is well known for its simplicity and ability to find counterexamples. It is based on the idea of symbolically representing counterexamples in a transition system and then using a SAT solver to check for their existence or their absence. State-of-the-art BMC algorithms combine a direct translation to SAT with circuit-aware simplifications and work incrementally, sharing information between different bounds. While BMC is incomplete (it can only show existence of counterexamples), it is a major building block of several complete interpolation-based model checking algorithms. However, traditional interpolation is incompatible with optimized BMC. Hence, these algorithms rely on simple BMC engines that significantly hinder their performance. In this paper, we present a Fast Interpolating BMC (Fib) that combines state-of-the-art BMC techniques with\u00a0\u2026", "num_citations": "13\n", "authors": ["1641"]}
{"title": "Synthesizing modular invariants for synchronous code\n", "abstract": " In this paper, we explore different techniques to synthesize modular invariants for synchronous code encoded as Horn clauses. Modular invariants are a set of formulas that characterizes the validity of predicates. They are very useful for different aspects of analysis, synthesis, testing and program transformation. We describe two techniques to generate modular invariants for code written in the synchronous dataflow language Lustre. The first technique directly encodes the synchronous code in a modular fashion. While in the second technique, we synthesize modular invariants starting from a monolithic invariant. Both techniques, take advantage of analysis techniques based on property-directed reachability. We also describe a technique to minimize the synthesized invariants.", "num_citations": "13\n", "authors": ["1641"]}
{"title": "Using architecturally significant requirements for guiding system evolution\n", "abstract": " Rapidly changing technology is one of the key triggers of system evolution. Some examples are: physically relocating a data center, replacement of infrastructure such as migrating from an in-house broker to CORBA, moving to a new architectural approach such as migrating from client-server to a service-oriented architecture. At a high level, the goals of such an evolution are easy to describe. While the end goals of the evolution are typically captured and known, the key architecturally significant requirements that guide the actual evolution tasks are often unexplored. At best, they are tucked under maintainability and/or modifiability concerns. In this paper, we argue that eliciting and using architecturally significant requirements of an evolution has a potential to significantly improve the quality of the evolution effort. We focus on elicitation and representation techniques of architecturally significant evolution\u00a0\u2026", "num_citations": "13\n", "authors": ["1641"]}
{"title": "Multi-valued symbolic model-checking: Fairness, counter-examples, running time\n", "abstract": " Multi-valued model-checking is an effective technique for reasoning about systems with incomplete or inconsistent information. In particular, it is well suited for reasoning about abstract, partial, and feature-based system descriptions. The technique is based on extending the classical model-checking algorithm over two-valued logic to arbitrary finite logics whose truth values form a distributive De Morgan lattice.In this thesis we address several issues surrounding the usability of multi-valued modelchecking. Firstly, we provide an improved analysis of the worst-case complexity of the symbolic multi-valued model-checking algorithm, and show that it is independent of the height of the lattice. Secondly, we extend the notion of fairness to a multi-valued models, thus enabling application of multi-valued model-checking to asynchronous concurrent systems. Thirdly, we introduce multi-valued witnesses and counter-examples that aid in interpreting the results of the model-checker. Finally, we describe the design and implementation of a multi-valued modelchecker \u03c7Chek.", "num_citations": "13\n", "authors": ["1641"]}
{"title": "Small inductive safe invariants\n", "abstract": " Computing minimal (or even just small) certificates is a central problem in automated reasoning and, in particular, in automated formal verification. For example, Minimal Unsatisfiable Subsets (MUSes) have a wide range of applications in verification ranging from abstraction and generalization to vacuity detection and more. In this paper, we study the problem of computing minimal certificates for safety properties. In this setting, a certificate is a set of clauses In\u03c5 such that each clause contains initial states, and their conjunction is safe (no bad states) and inductive. A certificate is minimal, if no subset of In\u03c5 is safe and inductive. We propose a two-tiered approach for computing a Minimal Safe Inductive Subset (MSIS) of Inv. The first tier is two efficient approximation algorithms that under-and over-approximate MSIS, respectively. The second tier is an optimized reduction from MSIS to a sequence of computations of\u00a0\u2026", "num_citations": "11\n", "authors": ["1641"]}
{"title": "Predictive run-time verification of discrete-time reachability properties in black-box systems using trace-level abstraction and statistical learning\n", "abstract": " Run-time Verification (RV) has become a crucial aspect of monitoring black-box systems. Recently, we introduced , a predictive RV framework, in which the monitor is able to predict the future extensions of an execution, using a model inferred from the random sample executions of the system. The monitor maintains a table of the states of the prediction model, with the probability of the extensions from each state that satisfy a safety property.                 The size of the prediction model directly influences the monitor\u2019s memory usage and computational performance, due to the filtering techniques used for run-time state estimation, that depends on the size of the model. Hence, achieving a small prediction model is key in predictive RV.                 In this paper, we use symmetry reduction to apply abstraction, that, in the absence of a model in black-box systems, is performed on the observation space. The\u00a0\u2026", "num_citations": "10\n", "authors": ["1641"]}
{"title": "Global guidance for local generalization in model checking\n", "abstract": " SMT-based model checkers, especially IC3-style ones, are currently the most effective techniques for verification of infinite state systems. They infer global inductive invariants via local reasoning about a single step of the transition relation of a system, while employing SMT-based procedures, such as interpolation, to mitigate the limitations of local reasoning and allow for better generalization. Unfortunately, these mitigations intertwine model checking with heuristics of the underlying SMT-solver, negatively affecting stability of model checking. In this paper, we propose to tackle the limitations of locality in a systematic manner. We introduce explicit global guidance into the local reasoning performed by IC3-style algorithms. To this end, we extend the SMT-IC3 paradigm with three novel rules, designed to mitigate fundamental sources of failure that stem from locality. We instantiate these rules for the theory of Linear\u00a0\u2026", "num_citations": "9\n", "authors": ["1641"]}
{"title": "Verification of recurrent neural networks for cognitive tasks via reachability analysis\n", "abstract": " Recurrent Neural Networks (RNNs) are one of the most successful neural network architectures that deal with temporal sequences, eg, speech and text recognition. Recently, RNNs have been shown to be useful in cognitive neuroscience as a model of decision-making. RNNs can be trained to solve the same behavioral tasks performed by humans and other animals in decision-making experiments, allowing for a direct comparison between networks and experimental subjects. Analysis of RNNs is expected to be a simpler problem than the analysis of neural activity. However, in practice, reasoning about an RNN\u2019s behaviour is a challenging problem. In this work, we take an approach based on formal verification for the analysis of RNNs. We make two main contributions. First, we consider the cognitive domain and formally define a set of useful properties to analyse for a popular experimental task. Second, we\u00a0\u2026", "num_citations": "8\n", "authors": ["1641"]}
{"title": "Interpolating strong induction\n", "abstract": " The principle of strong induction, also known as k-induction is one of the first techniques for unbounded SAT-based Model Checking (SMC). While elegant and simple to apply, properties as such are rarely k-inductive and when they can be strengthened, there is no effective strategy to guess the depth of induction. It has been mostly displaced by techniques that compute inductive strengthenings based on interpolation and property directed reachability (Pdr). In this paper, we present kAvy, an SMC algorithm that effectively uses k-induction to guide interpolation and Pdr-style inductive generalization. Unlike pure k-induction, kAvy uses Pdr-style generalization to compute and strengthen an inductive trace. Unlike pure Pdr, kAvy uses relative k-induction to construct an inductive invariant. The depth of induction is adjusted dynamically by minimizing a proof of unsatisfiability. We have implemented kAvy within\u00a0\u2026", "num_citations": "8\n", "authors": ["1641"]}
{"title": "Executable counterexamples in software model checking\n", "abstract": " Counterexamples\u2014execution traces of the system that illustrate how an error state can be reached from the initial state\u2014are essential for understanding verification failures. They are one of the most salient features of Model Checkers, which distinguish them from Abstract Interpretation and other Static Analysis techniques by providing a user with information on how to debug their system and/or the specification. While in Hardware and Protocol verification, the counterexamples can be replayed in the system, in Software Model Checking (SMC) counterexamples take the form of a textual or semi-structured report. This is problematic since it complicates the debugging process by preventing developers from using existing processes and tools such as debuggers, fault localization, and fault minimization.               In this paper, we argue that for SMC the most useful form of a counterexample is an executable mock\u00a0\u2026", "num_citations": "8\n", "authors": ["1641"]}
{"title": "Automated analysis of Stateflow models\n", "abstract": " Stateflow is a widely used modeling framework for embedded and cyberphysical systems where control software interacts with physical processes. In this work, we present a framework and a fully automated safety verification technique for Stateflow models. Our approach is two-folded: (i) we faithfully compile Stateflow models into hierarchical state machines, and (ii) we use automated logic-based verification engine to decide the validity of safety properties. The starting point of our approach is a denotational semantics of Stateflow. We propose a compilation process using continuation-passing style (CPS) denotational semantics. Our compilation technique preserves the structural and modal behavior of the system. The overall approach is implemented as an open source toolbox that can be integrated into the existing Mathworks Simulink/Stateflow modeling framework. We present preliminary experimental evaluations that illustrate the effectiveness of our approach in code generation and safety verification of industrial scale Stateflow models.", "num_citations": "8\n", "authors": ["1641"]}
{"title": "Finding errors in python programs using dynamic symbolic execution\n", "abstract": " For statically typed languages, dynamic symbolic execution (also called concolic testing) is a mature approach to automated test generation. However, extending it to dynamic languages presents several challenges. Complex semantics, fragmented and incomplete type information, and calls to foreign functions lacking precise models make symbolic execution difficult. We propose a symbolic execution approach that mixes concrete and symbolic values and incrementally solves path constraints in search for alternate executions by lazily instantiating axiomatizations for called functions as needed. We present the symbolic execution model underlying this approach and illustrate the workings of our prototype concolic testing tool on an actual Python software package.", "num_citations": "8\n", "authors": ["1641"]}
{"title": "Instantiations, Zippers and EPR Interpolation.\n", "abstract": " This paper describes interpolation procedures for EPR. In principle, interpolation for EPR is simple: It is a special case of first-order interpolation. In practice, we would like procedures that take advantage of properties of EPR: EPR admits finite models and those models are sometimes possible to describe very compactly. Inspired by procedures for propositional logic that use models and cores, but not proofs, we develop a procedure for EPR that uses just models and cores.", "num_citations": "8\n", "authors": ["1641"]}
{"title": "Unification-based pointer analysis without oversharing\n", "abstract": " Pointer analysis is indispensable for effectively verifying heap-manipulating programs. Even though it has been studied extensively, there are no publicly available pointer analyses that are moderately precise while scalable to large real-world programs. In this paper, we show that existing context-sensitive unification-based pointer analyses suffer from the problem of oversharing - propagating too many abstract objects across the analysis of different procedures, which prevents them from scaling to large programs. We present a new pointer analysis for LLVM, called TEADSA, without such an oversharing. We show how to further improve precision and speed of TEADSA with extra contextual information, such as flow-sensitivity at call- and return-sites, and type information about memory accesses. We evaluate TEADSA on the verification problem of detecting unsafe memory accesses and compare it against two state\u00a0\u2026", "num_citations": "7\n", "authors": ["1641"]}
{"title": "IC3-flipping the E in ICE\n", "abstract": " Induction is a key element of state-of-the-art verification techniques. Automatically synthesizing and verifying inductive invariants is at the heart of Model Checking of safety properties. In this paper, we study the relationship between two popular approaches to synthesizing inductive invariants: SAT-based Model Checking (SAT-MC) and Machine Learning-based Invariant Synthesis (MLIS). Our goal is to identify and formulate the theoretical similarities and differences between the two frameworks. We focus on two flagship algorithms: IC3 (an instance of SAT-MC) and ICE (an instance of MLIS). We show that the two frameworks are very similar yet distinct. For a meaningful comparison, we introduce RICE, an extension of ICE with relative induction and show how IC3 can be implemented as an instance of RICE. We believe this work contributes to the understanding of inductive invariant synthesis and will serve\u00a0\u2026", "num_citations": "7\n", "authors": ["1641"]}
{"title": "Algorithmic analysis of piecewise fifo systems\n", "abstract": " Systems consisting of several components that communicate via unbounded perfect FIFO channels (i.e. FIFO systems) arise naturally in modeling distributed systems. Despite well-known difficulties in analyzing such systems, they are of significant interest as they can describe a wide range of communication protocols. Previous work has shown that piecewise languages play an important role in the study of FIFO systems. In this paper, we present two algorithms for computing the set of reachable states of a FIFO system composed of piecewise components. The problem of computing the set of reachable states of such a system is closely related to calculating the set of all possible channel contents, i.e. the limit language. We present new algorithms for calculating the limit language of a system with a single communication channel and a class of multi-channel system in which messages are not passed around in cycles\u00a0\u2026", "num_citations": "7\n", "authors": ["1641"]}
{"title": "FrankenBit: bit-precise verification with many bits\n", "abstract": " Bit-precise software verification is an important and difficult problem. While there has been an amazing progress in SAT solving, Satisfiability Modulo Theory of Bit Vectors, and bit-precise Bounded Model Checking, proving bit-precise safety, i.e. synthesizing a safe inductive invariant, remains a challenge. In this paper, we present FrankenBit \u2014 a tool that combines bit-precise invariant synthesis with BMC counterexample search. As the name suggests, FrankenBit combines a large variety of existing verification tools and techniques, including LLBMC, UFO, Z3, Boolector, MiniSAT and STP.", "num_citations": "6\n", "authors": ["1641"]}
{"title": "Synthesis from assume-guarantee contracts using skolemized proofs of realizability\n", "abstract": " The realizability problem in requirements engineering is to determine the existence of an implementation that meets the given formal requirements. A step forward after realizability is proven, is to construct such an implementation automatically, and thus solve the problem of program synthesis. In this paper, we propose a novel approach to pro- gram synthesis guided by k-inductive proofs of realizability of assume- guarantee contracts constructed from safety properties. The proof of re- alizability is performed over a set of forall-exists formulas, and synthesis is per- formed by extracting Skolem functions witnessing the existential quan- tification. These Skolem functions can then be combined into an imple- mentation. Our approach is implemented in the JSyn tool which con- structs Skolem functions from a contract written in a variant of the Lus- tre programming language and then compiles the Skolem functions into a C language implementation. For a variety of benchmark models that already contained hand-written implementations, we are able to identify the usability and effectiveness of the synthesized counterparts, assuming a component-based verification framework.", "num_citations": "5\n", "authors": ["1641"]}
{"title": "Reliability Improvement and Validation Framework\n", "abstract": " Rotorcraft and other military and commercial aircraft rely increasingly on complex and highly integrated hardware and software systems for safe and successful mission operation as they undergo migration from federated systems to Integrated Modular Avionics (IMA) architectures. The current software engineering practice of \u201cbuild then test\u201d is proving unaffordable; software costs for the latest generation commercial aircraft, for example, are expected to exceed $10 B [Feiler 2009a] despite the use of process standards and best practices and the incorporation of a safety culture. In particular, embedded software responsible for system safety and reliability is experiencing exponential growth in complexity and size [Leveson 2004a, Dvorak 2009], making it a challenge to qualify and certify the systems [Boydston 2009].The US Army Aviation and Missile Research Development and Engineering Center (AMRDEC) Aviation Engineering Directorate (AED) has funded the Carnegie Mellon\u00ae Software Engineering Institute (SEI) to develop a reliability validation and improvement framework. The purpose of this framework is to provide a foundation for addressing the challenges of qualifying increasingly software-reliant, safety-critical systems. It aims to overcome the limitations of current reliability engineering approaches, leverage the best emerging engineering technologies and practices to complement the process focus of current practice, find acceptance in industry, and lead to a new set of reliability improvement metrics. In this report, we\u2022 summarize the findings of the background research for the framework in terms of key challenges in qualifying safety\u00a0\u2026", "num_citations": "5\n", "authors": ["1641"]}
{"title": "The Science, Art, and Magic of Constrained Horn Clauses\n", "abstract": " Constrained Horn Clauses (CHC) is a fragment of First Order Logic modulo constraints that captures many program verification problems as constraint solving. Safety verification of sequential programs, modular verification of concurrent programs, parametric verification, and modular verification of synchronous transition systems are all naturally captured as a satisfiability problem for CHC modulo theories of arithmetic and arrays. In general, the satisfiability of CHC modulo theory of arithmetic is undecidable. Thus, solving them is a mix of science, art, and a dash of magic. In this tutorial, we explore several aspects of this problem. First, we illustrate how different problems are translated to CHC. Second, we present a framework, called Spacer, that reduces symbolically solving Horn clauses to multiple simpler Satisfiability Modulo Theories, SMT, queries. Third, we describe advances in SMT that are necessary to make\u00a0\u2026", "num_citations": "4\n", "authors": ["1641"]}
{"title": "Verification of parameterized systems with combinations of abstract domains\n", "abstract": " We present a framework for verifying safety properties of parameterized systems. Our framework is based on a combination of Abstract Interpretation and a backward-reachability algorithm. A parameterized system is a family of systems in which n processes execute the same program concurrently. The problem of parameterized verification is to decide whether for all values of n the system with n processes is correct. Despite well-known difficulties in analyzing such systems, they are of significant interest as they can describe a wide range of protocols from mutual-exclusion to transactional memory. We assume that neither the number of processes nor their statespaces are bounded a priori. Hence, each process may be infinte-state. Our key contribution is an abstract domain in which each element (a) represents the lower bound on the number of processes at a control location and (b) employs a numeric\u00a0\u2026", "num_citations": "4\n", "authors": ["1641"]}
{"title": "Compositional verification of smart contracts through communication abstraction (extended)\n", "abstract": " Solidity smart contracts are programs that manage up to 2^160 users on a blockchain. Verifying a smart contract relative to all users is intractable due to state explosion. Existing solutions either restrict the number of users to under-approximate behaviour, or rely on manual proofs. In this paper, we present local bundles that reduce contracts with arbitrarily many users to sequential programs with a few representative users. Each representative user abstracts concrete users that are locally symmetric to each other relative to the contract and the property. Our abstraction is semi-automated. The representatives depend on communication patterns, and are computed via static analysis. A summary for the behaviour of each representative is provided manually, but a default summary is often sufficient. Once obtained, a local bundle is amenable to sequential static analysis. We show that local bundles are relatively complete for parameterized safety verification, under moderate assumptions. We implement local bundle abstraction in SmartACE, and show order-of-magnitude speedups compared to a state-of-the-art verifier.", "num_citations": "3\n", "authors": ["1641"]}
{"title": "Building Program Verifiers from Compilers and Theorem Provers\n", "abstract": " NO WARRANTY. THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN \u201cAS-IS\u201d BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.", "num_citations": "3\n", "authors": ["1641"]}
{"title": "Efficient verification of periodic programs using sequential consistency and snapshots\n", "abstract": " We verify safety properties of periodic programs, consisting of periodically activated threads scheduled preemptively based on their priorities. We develop an approach based on generating, and solving, a provably correct verification condition (VC). The VC is generated by adapting Lamport's sequential consistency to the semantics of periodic programs. Our approach is able to handle periodic programs that synchronize via two commonly used types of locks - priority ceiling protocol (PCP) locks, and CPU locks. To improve the scalability of our approach, we develop a strategy called snapshotting, which leads to VCs containing fewer redundant sub-formulas, and are therefore more easily solved by current SMT engines. We develop two types of snapshotting - SS-ALL snapshots all shared variables aggressively, while SS-MOD snapshots only modified variables. We have implemented our approach in a tool\u00a0\u2026", "num_citations": "3\n", "authors": ["1641"]}
{"title": "Assurance cases for proofs as evidence\n", "abstract": " Proof-carrying code (PCC) provides a'gold standard'for establishing formal and objective confidence in program behavior. However, in order to extend the benefits of PCC-and other formal certification techniques-to realistic systems, we must establish the correspondence of a mathematical proof of a program's semantics and its actual behavior. In this paper, we argue that assurance cases are an effective means of establishing such a correspondence. To this end, we present an assurance case pattern for arguing that a proof is free from various proof hazards. We also instantiate this pattern for a proof-based mechanism to provide evidence about a generic medical device software.", "num_citations": "3\n", "authors": ["1641"]}
{"title": "Tlq: A query solver for states\n", "abstract": " In this paper, we present TLQ, a tool that finds the state solutions to any CTL query. It extends existing approaches specialized in finding state solutions to special kinds of queries, being at the same time more efficient than general query checkers that can find all solution to any query. Its generality allows its application to new problems, such as finding stable states, as required in some applications from genetics. We describe the implementation of TLQ on top of the model-checker NuSMV, and show its effectiveness in finding the stable states of a gene network.We describe our tool TLQ that applies to the analysis of state-transition models, and can answer questions of the type:\u201cWhat are the states that satisfy a property \u03d5?\u201d. We address those cases where \u03d5 cannot be formulated in a temporal logic appropriate for model checking. Instead, \u03d5 can be formulated as a temporal query suitable for query checking. An example of such a property \u03d5 is \u201creachability\u201d. We cannot formulate a CTL property whose model-checking results in those states reachable from a given state, but we can formulate the CTL query EF? that literally encodes the question: which states are reachable (from the initial state)? While most model-checkers routinely compute reachable states, they do so by employing specialized procedures. Our tool has been motivated by the need to answer similar questions in a more systematic way than is currently done.", "num_citations": "3\n", "authors": ["1641"]}
{"title": "Global Guidance for Local Generalization in Model Checking\n", "abstract": " SMT-based model checkers, especially IC3-style ones, are currently the most effective techniques for verification of infinite state systems. They infer global inductive invariants via local reasoning about a single step of the transition relation of a system, while employing SMT-based procedures, such as interpolation, to mitigate the limitations of local reasoning and allow for better generalization. Unfortunately, these mitigations intertwine model checking with heuristics of the underlying SMT-solver, negatively affecting stability of model checking. In this paper, we propose to tackle the limitations of locality in a systematic manner. We introduce explicit global guidance into the local reasoning performed by IC3-style algorithms. To this end, we extend the SMT-IC3 paradigm with three novel rules, designed to mitigate fundamental sources of failure that stem from locality. We instantiate these rules for the theory of Linear Integer Arithmetic and implement them on top of SPACER solver in Z3. Our empirical results show that GSPACER, SPACER extended with global guidance, is significantly more effective than both SPACER and sole global reasoning, and, furthermore, is insensitive to interpolation.", "num_citations": "2\n", "authors": ["1641"]}
{"title": "Reachability problems in piecewise fifo systems\n", "abstract": " Systems consisting of several finite components that communicate via unbounded perfect FIFO channels (i.e., FIFO systems) arise naturally in modeling distributed systems. Despite well-known difficulties in analyzing such systems, they are of significant interest as they can describe a wide range of communication protocols. In this article, we study the problem of computing the set of reachable states of a FIFO system composed of piecewise components. This problem is closely related to calculating the set of all possible channel contents, that is, the limit language, for each control location. We present an algorithm for calculating the limit language of a system with a single communication channel. For multichannel systems, we show that the limit language is piecewise if the initial language is piecewise. Our construction is not effective in general; however, we provide algorithms for calculating the limit language of a\u00a0\u2026", "num_citations": "2\n", "authors": ["1641"]}
{"title": "Verifying verified code\n", "abstract": " A recent case study from AWS by Chong et al. proposes an effective methodology for Bounded Model Checking in industry. In this paper, we report on a followup case study that explores the methodology from the perspective of three research questions: (a) can proof artifacts be used across verification tools; (b) are there bugs in verified code; and (c) can specifications be improved. To study these questions, we port the verification tasks for aws-c-common library to SeaHorn and KLEE. We show the benefits of using compiler semantics and cross-checking specifications with different verification techniques, and call for standardizing proof library extensions to increase specification reuse. The verification tasks discussed are publicly available online.", "num_citations": "1\n", "authors": ["1641"]}
{"title": "Logical characterization of coherent uninterpreted programs\n", "abstract": " An uninterpreted program (UP) is a program whose semantics is defined over the theory of uninterpreted functions. This is a common abstraction used in equivalence checking, compiler optimization, and program verification. While simple, the model is sufficiently powerful to encode counter automata, and, hence, undecidable. Recently, a class of UP programs, called coherent, has been proposed and shown to be decidable. We provide an alternative, logical characterization, of this result. Specifically, we show that every coherent program is bisimilar to a finite state system. Moreover, an inductive invariant of a coherent program is representable by a formula whose terms are of depth at most 1. We also show that the original proof, via automata, only applies to programs over unary uninterpreted functions. While this work is purely theoretical, it suggests a novel abstraction that is complete for coherent programs but can be soundly used on arbitrary uninterpreted (and partially interpreted) programs.", "num_citations": "1\n", "authors": ["1641"]}
{"title": "TeaDsa: Type-aware DSA-style pointer analysis for low level code\n", "abstract": " [2] Y. Sui, J. Xue: SVF: interprocedural static value-flow analysis in LLVM. CC 2016 [3] C. Lattner, A. Lenharth, VS Adve: Making context-sensitive points-to analysis with heap cloning practical for the real world. PLDI 2007 [4] CL Conway et al.: Pointer Analysis, Conditional Soundness, and Proving the Absence of Errors. SAS 2008", "num_citations": "1\n", "authors": ["1641"]}
{"title": "Verified Software: Theories, Tools, and Experiments: 7th International Conference, VSTTE 2015, San Francisco, CA, USA, July 18-19, 2015. Revised Selected Papers\n", "abstract": " This volume constitutes the thoroughly refereed post-conference proceedings of the 7th International Conference on Verified Software: Theories, Tools and Experiments, VSTTE 2015, held in July 2015 in San Francisco, CA, USA. The 12 revised full papers presented were carefully revised and selected from 25 submissions. The goal of this conference is to advance the state of the art in the science and technology of software verification, through the interaction of theory development, tool evolution, and experimental validation and large-scale verification efforts that involve collaboration, theory unification, tool integration, and formalized domain knowledge.", "num_citations": "1\n", "authors": ["1641"]}
{"title": "4 Semantic Comparison of Malware Functions\n", "abstract": " 4.1 PurposeMalware infections continue to pose an ever increasing threat to both DoD and non-DoD software. One of the most effective means of defending against malware is to collect, catalog, and analyze existing malware samples to develop better techniques for preventing, detecting, diagnosing, and eliminating future infections. An important tool in the fight against malware is the ability to compare malware samples and determine the degree to which they are similar. For our purposes, two malware samples are similar if they have been compiled from the same source code with different versions or optimizations of a compiler, or slightly different source code. We call this provenance similarity [Chaki 2011]. Detecting provenance similarity between malware has at least two potential applications:1. Partition: The number of malware samples collected grows rapidly over time. For example, CERT collects millions of syntactically different samples every three months or so. Clearly, all these samples do not correspond to new malware\u2014there simply aren\u2019t enough malware authors to go around. One purpose of a similarity detection tool would be to partition a large collection X of syntactically distinct malware samples into a much smaller number of subsets, where each subset consists of \u201csemantically\u201d(or behaviorally) similar malware. This could enable more efficient cataloging and lookup. 2. Query: One of the first steps performed by an analyst is to determine if the malware sample under study S is similar to something in a large collection X of previously studied samples. The result of this step provides important clues about the malware\u2019s origin\u00a0\u2026", "num_citations": "1\n", "authors": ["1641"]}
{"title": "Formal Verification of Real-Time Embedded Software for Multicore Platforms\n", "abstract": " Real-time embedded software (RTES) plays an increasingly critical role in all aspects of our lives. Ensuring that RTES behave in a predictable, safe and secure manner is an open challenge. The emergence of multicore hardware has introduced an additional level of complexity to this arena. In this paper, we take the position that formal verification is a very promising approach to find concurrency-related problems in multicore RTES. We argue that multicore RTES present unique domain-specific restrictions (and new challenge problems) that can be leveraged (and targeted) by formal verification to yield solutions that are precise, scalable, automated, and applicable to source code. We also believe that this effort will increase synergy between formal verification and real-time scheduling.", "num_citations": "1\n", "authors": ["1641"]}
{"title": "Model-Checking with Many Values\n", "abstract": " Automated verification is an important step towards reliable software systems of the future. In this thesis, we concentrate on model-checking---an automated technique for deciding whether a system satisfies a temporal formula. We present techniques to address several major challenges facing adoption of model-checking as a practical verification technology. The validity of model-checking is threatened by vacuous satisfaction---a case where the system passes the analysis for a wrong, and often trivial, reason. We build upon vacuity detection---a well-known sanity check. We argue that it is important to distinguish different levels of vacuity to provide a user with a better understanding of potential threats to the analysis. We present a more general definition of vacuity---mutual vacuity, that captures truth or falsity of a property together with its vacuity with respect to different subformulas and their occurrences. To address the challenge of specifying correctness properties, we extend query-checking---a technique for discovering system's properties based on user-supplied queries. We develop a language of queries based on Computation Tree Logic, and present an algorithm for it. The technique is implemented in TLQSOLVER. We report on analysis of a cruise control system, and demonstrate novel applications of query-checking. To address the scalability challenge, we develop an abstraction technique that allows for verification and refutation of arbitrary temporal properties. Our main contributions are the systematic derivation of abstraction following Abstract Interpretation and a new characterization of the most precise abstract model for a given\u00a0\u2026", "num_citations": "1\n", "authors": ["1641"]}