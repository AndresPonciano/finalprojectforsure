{"title": "Predicting aging-related bugs using software complexity metrics\n", "abstract": " Long-running software systems tend to show degraded performance and an increased failure occurrence rate. This problem, known as Software Aging, which is typically related to the runtime accumulation of error conditions, is caused by the activation of the so-called Aging-Related Bugs (ARBs). This paper aims to predict the location of Aging-Related Bugs in complex software systems, so as to aid their identification during testing. First, we carried out a bug data analysis on three large software projects in order to collect data about ARBs. Then, a set of software complexity metrics were selected and extracted from the three projects. Finally, by using such metrics as predictor variables and machine learning algorithms, we built fault prediction models that can be used to predict which source code files are more prone to Aging-Related Bugs.", "num_citations": "74\n", "authors": ["432"]}
{"title": "Is software aging related to software metrics?\n", "abstract": " This work presents an empirical analysis aiming at investigating what kind of relationship exists between software aging and several static features of the software. While past studies on software aging focused on predicting the aging effects by monitoring and analytically modeling resource consumption at runtime, this study intends to explore if the static features of the software, as derived by its source code, presents potential relationships with software aging. We adopt a set of common software metrics concerning program structure, such as size and cyclomatic complexity, along with some features specifically developed for this study; metrics were then computed from ten complex software applications affected by aging. A statistical analysis to infer their relationship with software aging was carried out. Results encourage further investigations in this direction, since they show that software aging effects are related to\u00a0\u2026", "num_citations": "46\n", "authors": ["432"]}
{"title": "Software aging analysis of the android mobile os\n", "abstract": " Mobile devices are significantly complex, feature-rich, and heavily customized, thus they are prone to software reliability and performance issues. This paper considers the problem of software aging in Android mobile OS, which causes the device to gradually degrade in responsiveness, and to eventually fail. We present a methodology to identify factors (such as workloads and device configurations) and resource utilization metrics that are correlated with software aging. Moreover, we performed an empirical analysis of recent Android devices, finding that software aging actually affects them. The analysis pointed out processes and components of the Android OS affected by software aging, and metrics useful as indicators of software aging to schedule software rejuvenation actions.", "num_citations": "40\n", "authors": ["432"]}
{"title": "Investigation of failure causes in workload-driven reliability testing\n", "abstract": " Virtual execution environments and middleware are required to be extremely reliable because applications running on top of them are developed assuming their correctness, and platform-level failures can result in serious and unexpected application-level problems. Since software platforms and middleware are often executed for long time without any interruption, large part of the testing process is devoted to investigate their behavior when long and stressful executions occur (these test cases are called workloads). When a problem is identified, software engineers examine log files to find its root cause. Unfortunately, since of the workloads length, log files can contain a huge amount of information and manual analysis is often prohibitive. Thus, de-facto, the identification of the root cause is mostly left to the intuition of the software engineer.", "num_citations": "28\n", "authors": ["432"]}
{"title": "A case study on state-based robustness testing of an operating system for the avionic domain\n", "abstract": " This paper investigates the impact of state on robustness testing, by enhancing the traditional approach with the inclusion of the OS state in test cases definition. We evaluate the relevance of OS state and the effects of the proposed strategy through an experimental campaign on the file system of a Linux-based OS, to be adopted by Finmeccanica for safety-critical systems in the avionic domain. Results show that the OS state plays an important role in testing those corner cases not covered by traditional robustness testing.", "num_citations": "20\n", "authors": ["432"]}
{"title": "On the testing resource allocation problem: Research trends and perspectives\n", "abstract": " In testing a software application, a primary concern is how to effectively plan the assignment of resources available for testing to the software components so as to achieve a target goal under given constraints. In the literature, this is known as testing resources allocation problem (TRAP). Researchers spent a lot of effort to propose models for supporting test engineers in this task, and a variety of solutions exist to assess the best trade-off between testing time, cost and quality of delivered products. This article presents a systematic mapping study aimed at systematically exploring the TRAP research area in order to provide an overview on the type of research performed and on results currently available. A sample of 68 selected studies has been classified and analyzed according to defined dimensions. Results give an overview of the state of the art, provide guidance to improve practicability and allow outlining a set of\u00a0\u2026", "num_citations": "10\n", "authors": ["432"]}
{"title": "A comprehensive study on software aging across android versions and vendors\n", "abstract": " This paper analyzes the phenomenon of software aging \u2013 namely, the gradual performance degradation and resource exhaustion in the long run \u2013 in the Android OS. The study intends to highlight if, and to what extent, devices from different vendors, under various usage conditions and configurations, are affected by software aging and which parts of the system are the main contributors. The results demonstrate that software aging systematically determines a gradual loss of responsiveness perceived by the user, and an unjustified depletion of physical memory. The analysis reveals differences in the aging trends due to the workload factors and to the type of running applications, as well as differences due to vendors\u2019 customization. Moreover, we analyze several system-level metrics to trace back the software aging effects to their main causes. We show that bloated Java containers are a significant contributor to\u00a0\u2026", "num_citations": "6\n", "authors": ["432"]}
{"title": "Performance Degradation Analysis of a Supercomputer\n", "abstract": " We analyze performance degradation phenomena due to software aging on a real supercomputer deployed at the Federico II University of Naples, by considering a dataset of ten months of operational usage. We adopted a statistical approach for identifying when and where the supercomputer experienced a performance degradation trend. The analysis pinpointed performance degradation trends that were actually caused by the gradual error accumulation within basic software of the supercomputer.", "num_citations": "5\n", "authors": ["432"]}
{"title": "Prediction of the testing effort for the safety certification of open-source software: a case study on a real-time operating system\n", "abstract": " The reuse of Open Source Software (OSS) for safety-critical systems is seen with interest by industries, such as automotive, medical, and aerospace, as it enables shorter time-to-market and lower development costs. However, safety certification demands to supply evidence about OSS quality, and a gap analysis is needed to assess if the cost to produce certification evidence is worthwhile. This paper presents an empirical study on an open-source RTOS (RTEMS). The study investigates the relationship between software complexity and the effort to achieve a high test coverage, which is one of the most impacting activity for certification. The objective is to figure out if, and to what extent, it is possible to predict such effort preventively, by looking at software complexity metrics. This would enable a preliminary screening and benchmarking of OSS items, supporting strategic decision making. The study shows that\u00a0\u2026", "num_citations": "4\n", "authors": ["432"]}
{"title": "Preventing recurrence of industrial control system accident using assurance case\n", "abstract": " Lessons learned from accident experiences in safety-critical infrastructures are valuable not only for the organizations operating the infrastructures but also for third-party organizations developing or operating similar safety-critical infrastructure systems. While such accident knowledge is often reported after rigorous investigations of the accidents, learning from the knowledge and applying them to improve other systems is not a trivial issue, since the report is not structured for such a purpose. In this paper, we present a method to elucidate the accident knowledge by assurance case consisting of structured arguments and evidence. We introduce a new assurance case pattern and create a post-failure safety case that argues over the avoidance of a similar accident. The effectiveness of the proposed method is evaluated through a case study concerning the PG&E accident in SCADA system.", "num_citations": "4\n", "authors": ["432"]}
{"title": "Test effort and test coverage: correlation analysis in a safety critical operating system\n", "abstract": " Il contesto del presente lavoro di tesi \u00e8 la certificazione di sistemi software usati in domini applicativi safety critical, ad esempio in ambito militare, medico, aerospaziale, etc.. In tali applicazioni, infatti, il sistema deve essere conforme ad un safety standard. Negli ultimi anni, numerosi standard sono stati rilasciati, come ad esempio lo IEC61508 per dispositivi elettrici/elettronici programmabili, lo standard CENELEC EN 50126 per applicazioni ferroviarie, il DO-178B per sistemi software dell\u201f aviazione civile e militare, e cos\u00ec via. Un safety standard specifica i requisiti, gli artefatti, e le linee guida da applicare al processo di sviluppo software (SDLC, Software Development Life Cycle) al fine di richiedere la certificazione. In particolare, focalizziamo l\u201f attenzione sul requisito di test coverage (in seguito riferito, per semplicit\u00e0, con il termine coverage). La coverage mira ad individuare le porzioni di codice del sistema software che sono state testate dalla suite di test, secondo particolari criteri, ad esempio dal punto di vista delle istruzioni testate (statement coverage), dei costrutti decisionali testati (decision coverage). Nei domini safety critical sono richiesti alti livelli di coverage la fine di minimizzare la probabilit\u00e0 che si verifichino fallimenti del sistema di tipo catastrofico (catastrophic failure), ossia con danni a persone e/o all\u201f ambiente, ma ci\u00f2 non \u00e8 semplice da garantire, soprattutto in sistemi complessi come i sistemi operativi. Nel presente lavoro di tesi si \u00e8 investigato in che maniera caratteristiche del sistema software sono legate alla coverage, al fine di predire lo sforzo necessario (effort) per ottenere un livello di coverage prestabilito. Tale studio aiuta a", "num_citations": "3\n", "authors": ["432"]}
{"title": "Software Aging in Image Classification Systems on Cloud and Edge\n", "abstract": " Image classification systems using machine learning are rapidly adopted in many software application systems. Machine learning models built for image classification tasks are usually deployed on either cloud computing or edge computers close to data sources depending on the performance and resource requirements. However, software reliability aspects during the operation of these systems have not been properly explored. In this paper, we experimentally investigate the software aging phenomena in image classification systems that are continuously running on cloud or edge computing environments. By performing statistical analysis on the measurement data, we detected a suspicious phenomenon of software aging induced by image classification workloads in the memory usages for cloud and edge computing systems. Contrary to the expectation, our experimental results show that the edge system is less\u00a0\u2026", "num_citations": "1\n", "authors": ["432"]}
{"title": "Measurements for Software Aging\n", "abstract": " Considerable attention has been devoted to the analysis of software aging based on measurements from real systems. This approach aims to infer the presence of aging trends (aging detection) and to forecast the future evolution of such trends to determine the optimal rejuvenation time through an analysis of collected data. This chapter will cover the main methods adopted for the analysis and detection of software aging phenomena based on measurements. It will discuss the methods for trend detection, estimation, forecasting, as well as data manipulation. These methods can be classified as threshold-based approaches, statistical approaches for time series analysis and machine learning approaches for aging state classification and failure prediction. Mathematical details of the discussed techniques will be described in Appendix A-2.", "num_citations": "1\n", "authors": ["432"]}
{"title": "Reliability-Oriented Verification of Mission-Critical Software Systems.\n", "abstract": " A mission-critical system is a system whose failure or disruption may lead to catastrophic loss in terms of cost, damage to the environment, or even human life. Mission-critical systems are adopted in a growing number of areas, ranging from banking to e-commerce, from avionics to railway, from automotive to health care scenarios. In such contexts, high level of reliability represents a crucial requirement to satisfy. However, activities needed to achieve the desired reliability can require huge development cost. In particular, it has been showed that the greatest part of the total development and maintenance cost is due to the verification process [1], especially in large systems. This is mainly due to the inadequateness of current verification strategies (i) to deal with complexity and size of today\u2019s software systems, and (ii) to sufficiently cope with issues related to high-reliability demands (eg, few strategies are specifically conceived to improve reliability).In several critical application domains the reliability level to be achieved (along with other quality goals) is imposed by domain-specific standards, such as CENELEC [2], in the field of railway applications, IEC 61508 [3], for electrical/electronic systems, or the DO-178B [4] for ATC systems. In order to achieve the required reliability, these standards also suggest potential techniques that can be adopted along the development cycle, but neglecting cost and effectiveness issues. The guidelines they provide are quite general, as also noticed by [5], since their purpose is not to define what techniques a company must use or what is their impact on company\u2019s cost. Hence, there is a gap between what they suggest\u00a0\u2026", "num_citations": "1\n", "authors": ["432"]}