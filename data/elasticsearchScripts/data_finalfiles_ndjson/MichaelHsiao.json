{"title": "Hardware Trojan attacks: Threat analysis and countermeasures\n", "abstract": " Security of a computer system has been traditionally related to the security of the software or the information being processed. The underlying hardware used for information processing has been considered trusted. The emergence of hardware Trojan attacks violates this root of trust. These attacks, in the form of malicious modifications of electronic hardware at different stages of its life cycle, pose major security concerns in the electronics industry. An adversary can mount such an attack with an objective to cause operational failure or to leak secret information from inside a chip-e.g., the key in a cryptographic chip, during field operation. Global economic trend that encourages increased reliance on untrusted entities in the hardware design and fabrication process is rapidly enhancing the vulnerability to such attacks. In this paper, we analyze the threat of hardware Trojan attacks; present attack models, types, and\u00a0\u2026", "num_citations": "599\n", "authors": ["1035"]}
{"title": "A region based approach for the identification of hardware Trojans\n", "abstract": " Outsourcing of SoC fabrication units has created the potential threat of design tampering using hardware Trojans. Methods based on side-channel analysis exist to differentiate such maligned ICs from the genuine ones but process variation in the foundries limit the effectiveness of such approaches. In this work, we propose a circuit partition based approach to detect and locate the embedded Trojan. Results show that our approach is effective in separating out candidate Trojans in the circuit. In addition, we provide a power profile based method for refining the candidate regions that may contain a Trojan. In many cases, such an isolation method leads to noticeable manifestation of the anomalous behavior of the circuit due to the presence of the Trojan thereby enhancing chances of their detection.", "num_citations": "299\n", "authors": ["1035"]}
{"title": "A novel sustained vector technique for the detection of hardware Trojans\n", "abstract": " Intentional tampering in the internal circuit structure by implanting Trojans can result in disastrous operational consequences. While a faulty manufacturing leads to a nonfunctional device, effect of an external implant can be far more detrimental. Therefore, effective detection and diagnosis of such maligned ICs in the post silicon testing phase is imperative, if the parts are intended to be used in mission critical applications. We propose a novel sustained vector methodology that proves to be very effective in detecting the presence of a Trojan in an IC. Each vector is repeated multiple times at the input of both the genuine and the Trojan circuits that ensures the reduction of extraneous toggles within the genuine circuit. Regions showing wide variations in the power behavior are analyzed to isolate the infected gate(s). Experimental results on ISCAS benchmark circuits show that this approach can magnify the behavioral\u00a0\u2026", "num_citations": "237\n", "authors": ["1035"]}
{"title": "Denial-of-Service Attacks on Battery-powered Mobile Computers\n", "abstract": " Sleep deprivation attacks are a form of denial of service attack whereby an attacker renders a pervasive computing device inoperable by draining the battery more quickly than it would be drained under normal usage. We describe three main methods for an attacker to drain the battery: (1) service request power attacks, where repeated requests are made to the victim for services, typically over a network - even if the service is not provided the victim must expend energy deciding whether or not to honor the request; (2) benign power attacks, where the victim is made to execute a valid but energy-hungry task repeatedly, and (3) malignant power attacks, where the attacker modifies or creates an executable to make the system consume more energy than it would otherwise. Our initial results demonstrate the increased power consumption due to these attacks, which we believe are the first real examples of these attacks\u00a0\u2026", "num_citations": "233\n", "authors": ["1035"]}
{"title": "Sequential circuit test generation using dynamic state traversal\n", "abstract": " A new method for state justification is proposed for sequential circuit test generation. The linear list of states dynamically obtained during the derivation of test vectors is used to guide the search during state justification. State-transfer sequences may already be known that drive the circuit from the current state to the target state. Otherwise, genetic engineering of existing state-transfer sequences is required. In both cases, genetic-algorithm-based techniques are used to generate valid state justification sequences for the circuit in the presence of the target fault. This approach achieves extremely high fault coverages and thus outperforms previous deterministic and simulation-based techniques.", "num_citations": "217\n", "authors": ["1035"]}
{"title": "Cognitive radio and networking research at Virginia Tech\n", "abstract": " More than a dozen Wireless @ Virginia Tech faculty are working to address the broad research agenda of cognitive radio and cognitive networks. Our core research team spans the protocol stack from radio and reconfigurable hardware to communications theory to the networking layer. Our work includes new analysis methods and the development of new software architectures and applications, in addition to work on the core concepts and architectures underlying cognitive radios and cognitive networks. This paper describes these contributions and points towards critical future work that remains to fulfill the promise of cognitive radio. We briefly describe the history of work on cognitive radios and networks at Virginia Tech and then discuss our contributions to the core cognitive processing underlying these systems, focusing on our cognitive engine. We also describe developments that support the cognitive engine\u00a0\u2026", "num_citations": "161\n", "authors": ["1035"]}
{"title": "Protection against hardware trojan attacks: Towards a comprehensive solution\n", "abstract": " With the increasing disintegration of the design and manufacturing chain of our microelectronic products, we should not only worry about including unintentional, unwanted hardware features (\u201cbugs\u201d), but also about including intentional malicious hardware features: \u201cTrojan Horses,\u201dwhich act as spies or terrorists. This article provides an overview of hardware Trojans and countermeasures.", "num_citations": "146\n", "authors": ["1035"]}
{"title": "Trusted RTL: Trojan detection methodology in pre-silicon designs\n", "abstract": " In this paper, we propose a four-step approach to filter and locate malicious insertion(s) implanted in a third party Intellectual Property (3PIP). In our approach, we first remove those easy-to-detect signals whose activation and propagation are easy using functional vectors. The remaining signals are subjected to a N-detect full-scan ATPG tool to identify those which are functionally hard-to-excite and/or propagate. But unlike recognizing hard-to-detect signal(s), behavioral change brought about by these insertion(s) needs to be taken into account to narrow down their implantation locations. So in our third step, detection condition of suspect signals are cross checked against the spec by a suspect-signal-guided equivalence checking set-up. Finally, a region isolation approach is applied on the filtered signals to determine clusters of untestable gates in the circuit. Experimental results on ISCAS'89 benchmarks show that\u00a0\u2026", "num_citations": "132\n", "authors": ["1035"]}
{"title": "Towards an intrusion detection system for battery exhaustion attacks on mobile computing devices\n", "abstract": " Mobile computers are subject to a unique form of denial of service attack known as a battery exhaustion attack, in which an attacker attempts to rapidly drain the battery of the device. In this paper we present our first steps in the design of an intrusion detection system for these attacks, a system that takes into account the performance, energy, and memory constraints of mobile computing devices. This intrusion detection system uses several parameters, such as CPU load and disk accesses, to estimate the power consumption using a linear regression model, allowing us to find the energy used on a per process basis, and thus identifying processes that are potentially battery exhaustion attacks.", "num_citations": "120\n", "authors": ["1035"]}
{"title": "Compiler-directed dynamic voltage/frequency scheduling for energy reduction in microprocessors\n", "abstract": " Dynamic voltage and frequency scaling of the CPU has been identified as one of the most effective ways to reduce energy consumption of a program. This paper discusses a compilation strategy that identifies scaling opportunities without significant overall performance penalty. Simulation results show CPU energy savings of 3.97%-23.75% for the SPECfp95 benchmark suite with a performance penalty of at most 2.53%.", "num_citations": "113\n", "authors": ["1035"]}
{"title": "Compiler-directed dynamic frequency and voltage scheduling\n", "abstract": " Dynamic voltage and frequency scaling has been identified as one of the most effective ways to reduce power dissipation. This paper discusses a compilation strategy that identifies opportunities for dynamic voltage and frequency scaling of the CPU without significant increase in overall program execution time. The paper introduces a simple, yet effective performance model to determine an effcient CPU slow-down factor for memory bound loop computations. Simulation results of a superscalar target architecture and a program kernel compiled at different optimizations levels show the potential benefit of the proposed compiler optimization. The energy savings are reported for a hypothetical target machine with power dissipation characteristics similar to Transmeta's Crusoe TM5400 processor.", "num_citations": "112\n", "authors": ["1035"]}
{"title": "Guided test generation for isolation and detection of embedded Trojans in ICs\n", "abstract": " Testing the genuineness of a manufactured chip is an important step in an IC product life cycle. This becomes more prominent with the outsourcing of the manufacturing process, since the manufacturer may tamper the internal circuit behavior using Trojan circuits in the original design. Traditional testing methods cannot detect these stealthy Trojans because the triggering scenario, which activates it, is unknown. Recently, approaches based on side-channel analysis have shown promising results in detecting Trojans. In this paper, we propose a novel test generation technique that aims at magnifying the disparity between side-channel signal waveforms of tampered and genuine circuits to indicate the possibility of internal tampering. Experimental results indicate that our approach could magnify the likelihood of Trojans 4 to 20 times more than existing side-channel analysis based approaches.", "num_citations": "109\n", "authors": ["1035"]}
{"title": "VITAMIN: Voltage inversion technique to ascertain malicious insertions in ICs\n", "abstract": " We propose an  inverted voltage  scheme for exciting and pronouncing the behavior of any undesirable logic that may be inserted in the IC manufactured abroad. The  inverted voltage  scheme is coupled with a  sustained vector  simulation technique to further enhance the behavioral difference between the genuine and targeted test IC. Experimental results on a variety of ISCAS'89 benchmarks show that we are able to significantly magnify the difference between the genuine and tampered designs. For most of the smaller benchmarks, our inverted voltage method is able to detect the effect of the tamper directly at the primary outputs of the IC, especially when existing techniques fail to make any observable distinction. For the larger circuits, our technique is able to magnify the power consumption of the tampered circuit by several times.", "num_citations": "95\n", "authors": ["1035"]}
{"title": "Fast algorithms for static compaction of sequential circuit test vectors\n", "abstract": " Two fast algorithms for static test sequence compaction are proposed for sequential circuits. The algorithms are based on the observation that test sequences traverse through a small set of states, and some states are frequently re-visited throughout the application of a test set. Subsequences that start and end on the same states may be removed if necessary and sufficient conditions are met for them. The techniques require only two fault simulation passes and are applied to test sequences generated by various test generators, resulting in significant compactions very quickly for circuits that have many revisited states.", "num_citations": "74\n", "authors": ["1035"]}
{"title": "ALAPTF: A new transition fault model and the ATPG algorithm\n", "abstract": " The work presents a new transition fault model called as late as possible transition fault (ALAPTF) model. The model aims at detecting smaller delays, which be missed by both the traditional transition fault model and the path delay model. The model makes sure that each transition is launched as late as possible at the fault site, accumulating the small delay defects along its way. Because some transition faults may require multiple paths to be launched, the simple path-delay model miss such faults. Results on ISCAS'85 and ISCAS'89 benchmark circuits shows that for all the cases, the new model is capable of detecting smaller gate delays and produces better results in case of process variations. For all circuits, on an average, 30% of the time the transition reaches later than traditional models. The algorithm proposed also detects robust and non-robust paths along with the transition faults and the execution time is\u00a0\u2026", "num_citations": "73\n", "authors": ["1035"]}
{"title": "Efficient sequential ATPG for functional RTL circuits\n", "abstract": " We present an efficient register-transfer level automatic test pattern generation (ATPG) algorithm. First, our ATPG generates a series of sequential justification and propagation paths for each RTL primitive via a deterministic branch-and-bound search process, called a test environment. Then the precomputed test vectors for the RTL primitives are plugged into the generated test environments to form gate-level test vectors. We augment a 9-valued algebra to efficiently represent the justification and propagation objectives at the RT Level. Our ATPG automatically extracts any finite state machine (FSM) from the circuit, constructs the state transition graph (STG), and uses high-level information to guide the search process. We propose new static methods to identify embedded counter structures, and we use implication-based techniques and static learning to find the FSM traversal sequences sufficient to control the counters. Finally, a simulation-based RTL extension is added to augment the deterministic test set in a few cases when there is additional room for the improvement in fault coverage. Experimental results show that our new deterministic RTL techniques achieve several orders of magnitude reduction of test generation time without compromising fault coverage when compared to gatelevel ATPG tools. Our ATPG also outperforms a recently reported simulation-based high-level ATPG tool in terms of both fault coverage and CPU time.", "num_citations": "72\n", "authors": ["1035"]}
{"title": "Testing, verification, and diagnosis in the presence of unknowns\n", "abstract": " Improvement of the accuracy of error and fault diagnosis as well as ATPG for IP-based designs are important problems in industry. In this paper we address these problems when portions of the design may be unspecified. Two approaches to solve these problems have been presented: (1) solving Boolean satisfiability under unknown constraints, and (2) a network modification-based solution. Experimental results on constrained equivalence checking, enhancement of error diagnosis resolution for combinational circuits, and ATPG for IP-based designs have been presented on the ISCAS 85 benchmark and industrial circuits.", "num_citations": "69\n", "authors": ["1035"]}
{"title": "Automatic test generation using genetically-engineered distinguishing sequences\n", "abstract": " A fault-oriented sequential circuit test generator is described in which various types of distinguishing sequences are derived, both statically and dynamically, to aid the test generation process. A two-phase algorithm is used during test generation. The first phase activates the target fault, and the second phase propagates the fault effects (FE's) from the flip-flops with assistance from the distinguishing sequences. This strategy improves the propagation of FE's to the primary outputs, and the overall fault coverage is greatly increased. In our new test generator, DIGATE, genetic algorithms are used to derive both activating and distinguishing sequences during test generation. Our results show very high fault coverages for the ISCAS89 sequential benchmark circuits and several synthesized circuits.", "num_citations": "68\n", "authors": ["1035"]}
{"title": "Using non-trivial logic implications for trace buffer-based silicon debug\n", "abstract": " An effective silicon debug technique uses a trace buffer to monitor and capture a portion of the circuit response during its functional, post-silicon operation. Due to the limited space of the available trace buffer, selection of the critical trace signals plays an important role in both minimizing the number of signals traced and maximizing the observability/restorability of other untraced signals during post-silicon validation. This paper presents a new method for trace buffer signal selection for the purpose of post-silicon debug. The selection is performed by favoring those signals with the most number of implications that are not implied by other signals. Then, based on the values of the traced signals during silicon debug, we introduce an algorithm which uses a SAT-based multi-node implication engine to restore the values of untraced signals across multiple time-frames. Experimental results for sequential benchmark\u00a0\u2026", "num_citations": "63\n", "authors": ["1035"]}
{"title": "E ects of delay model in peak power estimation of VLSI sequential circuits,\"\n", "abstract": " Previous work has shown that maximum switching density at a given node is extremely sensitive to a slight change in the delay at that node. However, when estimating the peak power for the entire circuit, the powers estimated must not be as sensitive to a slight variation or inaccuracy in the assumed gate delays because computing the exact gate delays for every gate in the circuit during simulation is expensive. Thus, we would like to use the simplest delay model possible to reduce the execution time for estimating power, while making sure that it provides an accurate estimate, ie, that the peak powers estimated will not vary due to a variation in the gate delays. Results for four delay models are reported for the ISCAS85 combinational benchmark circuits, ISCAS89 sequential benchmark circuits, and several synthesized circuits.", "num_citations": "62\n", "authors": ["1035"]}
{"title": "K2: An estimator for peak sustainable power of VLSI circuits\n", "abstract": " New measures of peak power in the context of sequential circuits are proposed. This paper presents an automatic procedure to obtain very good lower bounds on these measures as well as the actual input vectors that attain such bounds. The initial state of the circuit is an important factor in determining the amount of switching activity in sequential circuits and is taken into account. A peak power estimator tool K2 was developed using genetic techniques. Experiments show that vector sequences generated by K. 2 give much more accurate estimates for peak power dissipation than the estimates made from randomly generated sequences.", "num_citations": "61\n", "authors": ["1035"]}
{"title": "A novel SAT all-solutions solver for efficient preimage computation\n", "abstract": " In this paper, we present a novel all-solutions preimage SAT solver, SOLALL, with the following features: (1) a new success-driven learning algorithm employing smaller cut sets; (2) a marked CNF database non-trivially combining success/conflict-driven learning; (3) quantified-jump-back dynamically quantifying primary input variables from the preimage; (4) improved free BDD built on the fly, saving memory and avoiding inclusion of PI variables; finally, (5) a practical method of storing all solutions into a canonical OBDD format. Experimental results demonstrated the efficiency of the proposed approach for very large sequential circuits.", "num_citations": "60\n", "authors": ["1035"]}
{"title": "Peak power estimation using genetic spot optimization for large VLSI circuits\n", "abstract": " Estimating peak power involves optimization of the circuit's switching function. We propose genetic spot expansion and optimization in this paper to estimate tight peak power bounds for large sequential circuits. The optimization spot shifts and expands dynamically based on the maximum power potential (MPP) of the nodes under optimization. Four genetic spot optimization heuristics are studied for sequential circuits. Experimental results showed an average of 70.7% tighter peak power bounds for large sequential benchmark circuits was achieved in short execution times.", "num_citations": "60\n", "authors": ["1035"]}
{"title": "Odette: A non-scan design-for-test methodology for trojan detection in ics\n", "abstract": " In this paper, we propose a two-step non-scan design-for-test methodology that can ease detection of an embedded Trojan and simultaneously partially obfuscates a design against Trojan implantations. In the first step, we use Q signals of flip-flops in a circuit to increase the number of reachable states. In the second step, we partition these flip-flops into different groups enhancing the state-space variation. Creation of these new reachable states helps to trigger and propagate the Trojan effect more easily. Experimental results on ISCAS'89 benchmarks show that this method can effectively uncover Trojans which are otherwise very difficult to detect in the normal functional mode. In addition, partitioning the flip-flops of the circuit into different groups and selecting the output (Q or Q) based on input controlled ENABLE signals conceal its actual functionality beyond simple recognition thereby making it difficult for the\u00a0\u2026", "num_citations": "59\n", "authors": ["1035"]}
{"title": "Efficient spectral techniques for sequential ATPG\n", "abstract": " We present a new test generation procedure for sequential circuits using spectral techniques. Iterative processes of filtering via compaction and spectral analysis of the filtered test set are performed for each primary input, extracting inherent spectral information embedded within the test sequence. This information, when viewed in the frequency domain, reveals the characteristics of the input spectrum. The filtered and analyzed set of vectors is then used to predict and generate future vectors. We also developed a fault-dropping technique to speed up the process. We show that very high fault coverages and small vector sets are consistently obtained in short execution times for sequential benchmark circuits.", "num_citations": "59\n", "authors": ["1035"]}
{"title": "Fast, flexible, cycle-accurate energy estimation\n", "abstract": " Designing energy efficient hardware and software systems demands different tools at various levels in the design hierarchy. There is however a dearth of tools to enable investigation and implementation of energy efficient software and hardware architectures. Presented is a fast, flexible, cycleaccurate architectural simulator, Myrmigki, that models a commercial microcontroller and microprocessor family, and enables cycle-accurate power dissipation analyses through a combination of instruction level power analysis and circuit activity estimation.Myrmigki is intended to be used to study the effect of microarchitectural features on the energy efficiency of hardware and software systems. It provides facilities for dynamic voltage scaling, clock speed setting and per-cycle architecture reconfiguration, and is easily extended to add new microarchitectural features and model new instruction set architectures. The simulator\u00a0\u2026", "num_citations": "54\n", "authors": ["1035"]}
{"title": "Alternating strategies for sequential circuit ATPG\n", "abstract": " A new sequential circuit test generator, ALT-TEST, is described which alternates repeatedly between two phases of test generation. The first phase uses a simulation-based genetic algorithm, while the second phase uses a deterministic algorithm. The fast execution of the first phase combines with the more powerful test sequence generation and redundancy-identification capabilities of the second phase to produce test sets having high fault coverages in low execution times. The effectiveness of the approach is demonstrated on the ISCAS89 sequential benchmark circuits and several synthesized circuits.", "num_citations": "53\n", "authors": ["1035"]}
{"title": "A study of implication based pseudo functional testing\n", "abstract": " This paper presents a study of the implication based functional constraint extraction techniques to generate pseudo functional scan tests. Novel algorithms to extract pair-wise and multi-node constraints as Boolean expressions on arbitrary gates in the design are presented. Its impact on reducing the overkill in testing was analyzed, and report the trade-offs in coverage and scan-loads for a number of fault models. In the case of path-delay fault model, it was shown that the longest paths contribute most to the over-testing problem, raising the question about scan testing of the longest paths. Finally, the evaluation of the functional constraints on large industrial circuits show that the proposed constraint generation algorithm generate a powerful set of constraints most of which are not captured in the constraints extracted by designers for design-verification purposes", "num_citations": "51\n", "authors": ["1035"]}
{"title": "Peak power estimation of VLSI circuits: new peak power measures\n", "abstract": " New measures of peak power are proposed in the context of sequential circuits, and an efficient automatic procedure is presented to obtain very good lower bounds on these measures, as well as providing the actual input vectors that attain such bounds. Automatic generation of a functional vector loop for near-worst case power consumption is also attained. Experiments show that vector sequences generated give much more accurate estimates of peak power dissipation and are generated in significantly shorter execution times than estimates made from randomly generated sequences for four delay models.", "num_citations": "51\n", "authors": ["1035"]}
{"title": "Dynamic state traversal for sequential circuit test generation\n", "abstract": " A new method for state justification is proposed for sequential circuit test generation. The linear list of states dynamically obtained during the derivation of test vectors is used to guide the search during state justification. State-transfer sequences that drive the circuit from the current state to the target state may already be known. Otherwise, genetic engineering of existing state-transfer sequences is required. In both cases, genetic-algorithm-based techniques are used to generate valid state justification sequences for the circuit in the presence of the target fault. This approach achieves extremely high fault coverages, and thus outperforms previous deterministic and simulation-based techniques.", "num_citations": "51\n", "authors": ["1035"]}
{"title": "Constrained ATPG for broadside transition testing\n", "abstract": " In this paper we propose a new concept of testing only functionally testable transition faults in broadside transition testing via a novel constrained ATPG. For each functionally untestable transition fault f, a set of illegal (unreachable) states that enable detection of f is first computed. This set of undesirable illegal states is efficiently represented as a Boolean formula. Our constrained ATPG then incorporates this constraint formula to generate broadside vectors that avoid those undesirable states. In doing so, our method efficiently generates a rest set for functionally testable transition faults and minimizes detection of functionally untestable transition faults. Since we want to avoid launching and propagating transitions in the circuit that are not possible in the functional mode, a direct benefit of our method is the reduction of yield loss due to overtesting of these functionally untestable transitions.", "num_citations": "49\n", "authors": ["1035"]}
{"title": "State relaxation based subsequence removal for fast static compaction in sequential circuits\n", "abstract": " We extend the subsequence removal technique to provide significantly higher static compaction for sequential circuits. We show that state relaxation techniques can be used to identify more or larger cycles in a test set. State relaxation creates more opportunities for subsequence removal and hence, results in better compaction. Relaxation of a state is possible since not all memory elements in a finite state machine have to be specified for a state transition. The proposed technique has several advantages: (1) test sets that could not be compacted by existing subsequence removal techniques can now be compacted, (2) the size of cycles in a test set can be significantly increased by state relaxation and removal of the larger sized cycles leads to better compaction, (3) only two fault simulation passes are required as compared to trial and re-trial methods that require multiple fault simulation passes, and (4) significantly\u00a0\u2026", "num_citations": "49\n", "authors": ["1035"]}
{"title": "Power mode based macro-models for power estimation of electronic circuits\n", "abstract": " A method of creating models for power estimation of a circuit comprising generating an input space for the circuit. The input space is separated into multiple power modes corresponding to regions that display similar power behavior. Separate power models are generated for each of said multiple power modes. A power mode identification function is created that selects an appropriate power model from the separate power models based on the present and past values of the circuit inputs.", "num_citations": "48\n", "authors": ["1035"]}
{"title": "Parallel genetic algorithms for simulation-based sequential circuit test generation\n", "abstract": " The problem of test generation belongs to the class of NP-complete problems and it is becoming more and more difficult as the complexity of VLSI circuits increases, and as long as execution times pose an additional problem. Parallel implementations can potentially provide significant speedups while retaining good quality results. In this paper, we present three parallel genetic algorithms for simulation-based sequential circuit test generation. Simulation-based test generators are more capable of handling the constraints of complex design features than deterministic test generators. The three parallel genetic algorithm implementations are portable and scalable over a wide range of distributed and shared memory MIMD machines. Significant speedups were obtained, and fault coverages were similar to and occasionally better than those obtained using a sequential genetic algorithm, due to the parallel search\u00a0\u2026", "num_citations": "47\n", "authors": ["1035"]}
{"title": "Efficient preimage computation using a novel success-driven atpg\n", "abstract": " Preimage computation is a key step in formal verification. Pure OBDD-based symbolic method is vulnerable to the space-explosion problem. On the other hand, conventional ATPG/SAT-based method can handle large designs but can suffer from time explosion. Unlike methods that combine ATPG/SAT and OBDD, we present a novel success-driven learning algorithm which significantly accelerates an ATPG engine for enumerating all solutions (preimages). The algorithm effectively prunes redundant search space due to overlapped solutions and constructs a free BDD on the fly so that it becomes the representation of the preimage set at the end. Experimental results have demonstrated the effectiveness of the approach, in which we are able to compute preimages for large sequential circuits, where OBDD-based methods fail.", "num_citations": "46\n", "authors": ["1035"]}
{"title": "Accurate power macro-modeling techniques for complex RTL circuits\n", "abstract": " This paper presents novel techniques for the cycle-accurate power macro-modeling of complex RTL components. The proposed techniques are based on the observation that RTL components often exhibit significantly different \"power behavior\" for different parts of the input space, making it difficult for a single conventional macro-model to accurately estimate the power dissipation over the entire input space. We address this problem by identifying and separating the input space into regions that display \"similar\" power behavior. We refer to these regions as the power modes of the component. We then construct separate macro-models for each region, and construct a function that, given the input trace to the component, selects an appropriate power mode (and hence macro-model) for use in each cycle. The proposed ideas are complementary to, and improve upon, previously proposed techniques for power macro\u00a0\u2026", "num_citations": "46\n", "authors": ["1035"]}
{"title": "Latent fingerprint segmentation using ridge template correlation\n", "abstract": " Even with the high accuracy of automated fingerprint identification in matching plain to rolled prints, latent to rolled print matching continues to require human input. Latent prints are those that are lifted from a surface, typically at a crime scene, whereas plain prints are obtained under supervision with quality control. In comparison to plain or rolled prints, latent prints are usually of poor quality and have a small fingerprint surface area, making it difficult to extract a large number of features reliably. Manually processing latent prints is time consuming, so efforts are being made to speed up the process through partial automation. One of the first steps is image segmentation, which is the separation of the foreground (fingerprint region) from the background. Traditional automated methods for segmentation are designed for backgrounds with random noise and perform poorly on structured/textured backgrounds, resulting in\u00a0\u2026", "num_citations": "45\n", "authors": ["1035"]}
{"title": "Application of genetically engineered finite-state-machine sequences to sequential circuit ATPG\n", "abstract": " New methods for fault-effect propagation and state justification that use finite-state-machine sequences are proposed for sequential circuit test generation. Distinguishing sequences are used to propagate the fault effects from the flip-flops to the primary outputs by distinguishing the faulty machine state from the fault-free machine state. Set, clear, and pseudoregister justification sequences are used for state justification via a combination of partial state justification solutions. Reengineering of existing finite-state machine sequences may be needed for specific target faults. Moreover, conflicts imposed by the use of multiple sequences may need to be resolved. Genetic-algorithm-based techniques are used to perform these tasks. Very high fault coverages have been obtained as a result of this technique.", "num_citations": "44\n", "authors": ["1035"]}
{"title": "Design validation of RTL circuits using evolutionary swarm intelligence\n", "abstract": " In this paper, we present BEACON, a Branch-oriented Evolutionary Ant Colony OptimizatioN method which is a bio-inspired meta-heuristic for design validation and functional test generation. BEACON combines an evolutionary search technique with Ant Colony Optimization (ACO) for improved search capability. BEACON first cross-compiles the Verilog circuit source to a C++ base for fast simulation. Then, it profiles the code, keeping track of each branch and the number of times it has been visited in a database. Branch coverage provides a very useful metric for exploring the design, especially visiting the most critical states, including corner states, in the design. At 100% branch coverage, we can conclude that every control state described in the RTL has been visited. Thus, during execution, BEACON trims highly visited branches from the search and focuses the search on rarely occurring branches and paths. This\u00a0\u2026", "num_citations": "37\n", "authors": ["1035"]}
{"title": "A novel approach to design of user re-authentication systems\n", "abstract": " In the Internet age, security is a major concern as identity thefts often cause detrimental effects. Masquerading is an important factor for identity theft and current authentication systems using traditional methods woefully lack mechanisms to detect and prevent it. This paper presents an application independent, continual, non-intrusive, fast and easily deployable user re-authentication system based on behavioral biometrics. These behavioral attributes are extracted from the keyboard and mouse operations of the user. They are used to identify and non-intrusively authenticate the user periodically. To extract suitable user attributes, we propose a novel heuristic that uses the percentage of mouse-to-keyboard interaction ratio and interaction quotient (IQ). In the re-authentication process, every time, the current behavior of the user is compared with the stored ldquoexpectedrdquo behavior. All deviations are noted and\u00a0\u2026", "num_citations": "36\n", "authors": ["1035"]}
{"title": "Novel spectral methods for built-in self-test in a system-on-a-chip environment\n", "abstract": " This new method of built-in self-test (BIST) for sequential cores on a system-on-a-chip (SOC) generates test patterns using a real-time program that runs on an embedded processor. Alternatively, the same program can be run on an external low-cost tester. This program generates patterns using circuit-specific spectral information in the form of one or more Hadamard coefficients. The coefficients are extracted from high fault-coverage compacted pattern sets. When an embedded processor is available on SOC, the overhead is negligible. Also, sequential cores are tested in the functional mode, avoiding activation of nonfunctional timing paths. We present experimental results to show that for hard to test circuits, with any given test time, spectral patterns provide significantly higher fault coverage than weighted-random patterns.", "num_citations": "34\n", "authors": ["1035"]}
{"title": "A hardware architecture for dynamic performance and energy adaptation\n", "abstract": " Energy consumption of any component in a system may sometimes constitute just a small percentage of that of the overall system, making it necessary to address the issue of energy efficiency across the entire range of system components, from memory, to the CPU, to peripherals. Presented is a hardware architecture for detecting regions of application execution at runtime, for which there is opportunity to run a device at a slightly lower performance level, by reducing the operating frequency and voltage, to save energy. The proposed architecture, the Power Adaptation Unit (PAU) may be used to control the operating voltage of various system components, ranging from the CPU core, to memory and peripherals.               An evaluation of the tradeoffs in performance versus energy savings and hardware cost of the PAU is conducted, along with results on its efficacy for a set of benchmarks. It is shown that on\u00a0\u2026", "num_citations": "33\n", "authors": ["1035"]}
{"title": "Partial scan selection based on dynamic reachability and observability information\n", "abstract": " A partial scan selection strategy is proposed in which flip-flops are selected via newly proposed dynamic reachability and observability measures such that the remaining hard-to-detect faults are easily detected. This is done by taking advantage of the information available when a target fault is aborted by the test generator. A partial scan selection tool, IDROPS, has been developed which selects the best and smallest set of flip-flops to scan that will result in a high fault coverage. Results indicate that high fault coverage in hard-to-test circuits can be achieved using fewer scan flip-flops than in previous methods.", "num_citations": "33\n", "authors": ["1035"]}
{"title": "Enhancing SAT-based bounded model checking using sequential logic implications\n", "abstract": " We present a novel technique of improving the SAT-based Bounded Model Checking, by inducing powerful sequential signal correlations (crossing time-frame boundaries) into the original CNF formula of the unrolled circuit. A quick preprocessing on the circuit-under-verification, builds a large set of direct and indirect sequential implications. The non-trivial implications (spanning multiple time-frames) are converted into two-literal clauses. These clauses are quickly replicated throughout the unrolled sequential circuit, and appended to the existing CNF database. The added clauses prune the overall search space of SAT-solver engine and provide correlation among the different variables, which enhances the Boolean Constraint Propagation (BCP). Experimental Results for checking difficult instances of random safety properties on ISCAS'89 benchmark circuits show that more than 148x speedup can be achieved\u00a0\u2026", "num_citations": "32\n", "authors": ["1035"]}
{"title": "Bilateral testing of nano-scale fault-tolerant circuits\n", "abstract": " As the technology enters the nano dimension, the inherent unreliability of nanoelectronics is making fault-tolerant architectures increasingly necessary in building nano systems. Because fault-tolerant hardwares help to mask the effects caused by increased levels of defects, testing the functionality of the chip together with the embedded fault-tolerance becomes a tremendous challenge. In this paper, a new bilateral testing framework for nano circuits is proposed, where multiple stuck-at faults across different modules in a triple module redundancy (TMR) architecture are considered. In addition, a new test generator is presented for the bilateral testing that takes into account the enormous number of bilateral stuck-at faults possible with new types of guidance in the search, and it can generate a set of vectors that can test the TMR-based nano circuit as a single entity. Experimental results reported for ISCAS\u201985\u00a0\u2026", "num_citations": "31\n", "authors": ["1035"]}
{"title": "Reversible logic synthesis through ant colony optimization\n", "abstract": " We propose a novel synthesis technique for reversible logic based on ant colony optimization (ACO). In our ACO-based approach, reversible logic synthesis is formulated as a best-path search problem, where artificial ants, starting from their nest (reversible function output), attempt to find the best path to the food source (reversible function input). The experimental results have demonstrated superior performance in terms of both synthesis quality and computation time. They also show that the proposed method is scalable in handling large reversible functions.", "num_citations": "30\n", "authors": ["1035"]}
{"title": "Mining global constraints for improving bounded sequential equivalence checking\n", "abstract": " In this paper, we propose a novel technique on mining relationships in a sequential circuit to discover global constraints. In contrast to the traditional learning methods, our mining algorithm can find important relationships among several nodes efficiently. The nodes involved may often span several time-frames, thus improving the deductibility of the problem instance. Experimental results demonstrate that the application of these global constraints to SAT-based bounded sequential equivalence checking can achieve one to two orders of magnitude speedup. In addition, because it is orthogonal to the underlying SAT solver, it can help to enhance the efficacy of typical SAT based verification flows.", "num_citations": "30\n", "authors": ["1035"]}
{"title": "Multiplexed trace signal selection using non-trivial implication-based correlation\n", "abstract": " Silicon debug with a trace buffer provides real-time visibility to the design under debug. It traces a small subset of internal signals during its normal operation. The effectiveness of silicon debug, then, depends critically on the selection of trace signals. This paper proposes a new multiplexer-based trace signal interconnection scheme and a new heuristic for trace signal selection based on implication-based correlation. As a result, we can effectively trace twice as many signals with the same trace buffer width. We also propose a SAT-based heuristic to prune the selected trace signal list further to take into account those multi-node implications. Finally, we propose a state restoration algorithm for the multiplexer-based trace signal interconnection scheme. Experiments for sequential benchmark circuits showed that the proposed approach selects the trace signals effectively, giving superior restoration percentage\u00a0\u2026", "num_citations": "29\n", "authors": ["1035"]}
{"title": "Maximizing impossibilities for untestable fault identification\n", "abstract": " This paper presents a new fault-independent method for maximizing local conflicting value assignments for the purpose of untestable faults identification. The technique first computes a large number of logic implications across multiple time-frames and stores them in an implication graph. Then, by maximizing conflicting scenarios in the circuit, the algorithm identifies a large number of untestable faults that require such impossibilities. The proposed approach identifies impossible combinations locally around each Boolean gate in the circuit, and its complexity is thus linear in the number of nodes, resulting in short execution times. Experimental results for both combinational and sequential benchmark circuits showed that many more untestable faults can be identified with this approach efficiently.", "num_citations": "28\n", "authors": ["1035"]}
{"title": "Tackling the path explosion problem in symbolic execution-driven test generation for programs\n", "abstract": " Symbolic techniques have been shown to be very effective in path-based test generation, however, they fail to scale to large programs due to the exponential number of paths to be explored. In this paper, we focus on tackling this path explosion problem and propose search strategies to achieve quick branch coverage under symbolic execution, while exploring only a fraction of paths in the program. We present a reach ability-guided strategy that makes use of the reach ability graph of the program to explore unvisited portions of the program and a conflict-driven backtracking strategy that utilizes conflict analysis to perform nonchronological backtracking. We present experimental evidence that these strategies can significantly reduce the search space and improve the speed of test generation for programs.", "num_citations": "27\n", "authors": ["1035"]}
{"title": "A new hybrid solution to boost SAT solver performance\n", "abstract": " Due to the widespread demands for efficient SAT solvers in electronic design automation applications, methods to boost the performance of the SAT solver are highly desired. The paper proposed a hybrid solution to boost SAT solver performance in this paper, via an integration of local and DPLL-based search approaches. A local search is used to identify a subset of clauses to be passed to a DPLL SAT solver through an incremental interface. In addition, the solution obtained by the DPLL solver on the subset of clauses is fed back to the local search solver to jump over any locally optimal points. The proposed solution is highly portable to the existing SAT solvers. For satisfiable instances, up to an order of magnitude speedup can be obtained via the proposed hybrid solver", "num_citations": "27\n", "authors": ["1035"]}
{"title": "Integration of learning techniques into incremental satisfiability for efficient path-delay fault test generation\n", "abstract": " In recent years, several electronic design automation (EDA) problems in testing and verification have been formulated as Boolean satisfiability (SAT) instances due to the development of efficient general-purpose SAT solvers. Problem-specific learning techniques and heuristics can be integrated into the SAT solver to further speed-up the search for a satisfying assignment. In this paper, we target the problem of generating a complete test-suite for the path delay fault (PDF) model. We provide an incremental satisfiability framework that learns from (1) static logic implications, (2) segment-specific clauses, and (3) unsatisfiability cores of each untestable partial PDF. These learning techniques improvise the test generation for path delay faults that have common testable and/or untestable segments. The experimental results show that a significant portion of PDFs can be excluded dynamically in the proposed incremental\u00a0\u2026", "num_citations": "27\n", "authors": ["1035"]}
{"title": "Functional test generation at the rtl using swarm intelligence and bounded model checking\n", "abstract": " Although stochastic search techniques have shown promise in test generation and design validation, they often fail when there is a specific, random-resistant sequence of vectors required to exercise a target. In order to combat this, deterministic techniques are added, resulting in a hybrid solution to maintain high speed of execution while improving metric performance. This paper presents a formal hybridization that combines a Register Transfer Level (RTL) stochastic swarm intelligence based test vector generation with the Verilator Verilog-to-C++ source-to-source compiler. Verilator generates a fast cycle accurate C++ simulation unit for Verilog descriptions and provides instrumentation for branch and toggle coverage metrics. This RTL model can also be used to generate a bounded model checking (BMC) instance. During the stochastic search, the bounded model checker is launched to expand the unexplored\u00a0\u2026", "num_citations": "26\n", "authors": ["1035"]}
{"title": "A SMT-based diagnostic test generation method for combinational circuits\n", "abstract": " A diagnostic test pattern generator using a Satisfiability Modulo Theory (SMT) solver is proposed. Rather than targeting a single fault pair at a time, the proposed SMT approach can distinguish multiple fault pairs in a single instance. Several heuristics are proposed to constrain the SMT formula to further reduce the search space, including fault selection, excitation constraint, reduced primary output vector, and cone-of-influence reduction. Experimental results for the ISCAS85 and full-scan versions of ISCAS89 benchmark circuits show that fewer diagnostic vectors are generated compared with conventional diagnostic test generation methods. Up to 73% reduction in the number of vectors generated can be achieved in large circuits.", "num_citations": "26\n", "authors": ["1035"]}
{"title": "Enhancing sat-based equivalence checking with static logic implications\n", "abstract": " We propose a novel technique to improve SAT-based Combinational Equivalence Checking (CEC) by statically adding meaningful clauses to the CNF formula of the miter circuit. A fast preprocessing quickly builds up the implication graph for the miter circuit under verification, resulting in a large set of direct, indirect and extended backward implications. The non-trivial implications are converted into two-literal clauses and added to the miter CNF database. These added clauses constrain the search space, and provide correlation among the different variables, which enhances the Boolean Constraint Propagation (BCP). Experimental results on ISCAS'85 CEC instances show that with the added clauses, an average speedup of more than 950x was achieved.", "num_citations": "25\n", "authors": ["1035"]}
{"title": "On efficient error diagnosis of digital circuits\n", "abstract": " The rising trend in large scale integration and design complexity has greatly increased the need for efficient design error diagnosis. We present techniques for fast and efficient error diagnosis of digital circuits by eliminating to a large extent the set of false candidates identified by the diagnosis. The elimination of false candidate regions is conducted via distinguishing X's, flipping of values at the output of candidate regions, and combination of these techniques. The algorithms help to improve both the speed and resolution of error diagnosis. Experimental results on combinational benchmark circuits showed that up to 92% improvement in diagnostic resolution and 74% speedup over the original region-based diagnosis can be achieved with our approaches.", "num_citations": "25\n", "authors": ["1035"]}
{"title": "A new architectural-level fault simulation using propagation prediction of grouped fault-effects\n", "abstract": " A new technique is proposed to handle fault simulation at the architectural level. The technique bypasses the need for complete gate level structure and efficiently uses the architectural information. Symbolic data representing groups of stuck at faults, known as fault effects, are propagated across the circuit with intelligent propagation prediction. Fault effects may combine and form new groups in the process. Automated behavioral simulation using only three data types is used to propagate fault effects at the architectural level by propagation prediction; no additional high level constraints or precomputation of faulty behavior are needed for simulation. Although not a fully deterministic algorithm, the results of ALFSIM, Architectural Level Fault Simulation, show high accuracy when compared with the gate level fault simulation.", "num_citations": "25\n", "authors": ["1035"]}
{"title": "Efficient design validation based on cultural algorithms\n", "abstract": " We introduce a new semi-formal design validation framework to justify hard-to-reach corner-case states. We propose a cultural learning technique to identify the swarming of domain knowledge during the search. In addition, our guidance strategy abstracts sets of partitioned state variables, from which pre-images are computed to capture the expanded portions of the state spaces related to a target state. Experimental results show that our approach is very effective to reach hard-to-reach states than existing methods.", "num_citations": "24\n", "authors": ["1035"]}
{"title": "A novel transition fault ATPG that reduces yield loss\n", "abstract": " In this article, we have presented a novel constrained broadside transition ATPG algorithm to avoid overtesting functionally (sequentially) untestable transition faults. In some circuits, significantly more functionally untestable transition faults were identified. At the same time, more faults could be detected without incidental detection of functionally untestable transition faults. With a test set that reduces launching of transitions that are functionally impossible, we believe our method offers a practical solution to avoid overtesting these functionally impossible transitions, thus reducing yield loss. However, the runtime for our constrained ATPG is much longer than the conventional ATPG as a result of the constraint learning and construction process. Our future work concentrates on more-efficient constraint learning algorithms to reduce the over all ATPG runtime.", "num_citations": "24\n", "authors": ["1035"]}
{"title": "Partial scan beyond cycle cutting\n", "abstract": " This paper addresses the problem of flip flop selection for partial scan in sequential circuits. In particular it addresses some of the shortcomings of the popular flip flop selection methods, based on cutting cycles present in the graph of the circuit structure. Previous approaches assume that cutting all cycles makes the circuit totally testable, which is not always true. In the proposed approach, first subsets of flip flops are formed based on cycles in the S-graph and flip flops with self-loops. Flip flops are selected from these subsets based on a testability measure which uses an approximate valid state analysis. Once a flip flop is selected from a subset, testability measures may indicate the need for more flip flops, thus possibly selecting more flip flops than required for minimum cycle cutting. The goal is to select the fewest number of flip flops required to obtain high fault coverage for all partial scan circuits. Experimental\u00a0\u2026", "num_citations": "24\n", "authors": ["1035"]}
{"title": "Mining-guided state justification with partitioned navigation tracks\n", "abstract": " This work introduces a new guidance strategy to justify hard-to-reach target states in sequential circuits, which also applies to reaching corner-case states in design validation. We propose data-mining methods to extract several partition sets of state variables directly from the gate-level netlist of the design, such that they enable the search to reach the target state. The partition sets are used to compute partitioned navigation tracks (PNTs). PNTs capture the behavior of expanded portions of the state space as they relate to a target state of interest, thus providing a more accurate distance metric than traditional abstract guideposts. Moreover, the computation and storage costs of the PNTs are small, making our approach scalable to large circuits. With the proposed PNTs, we also need not refine the abstract models. Experiments showed that we are able to reach many more hard-to-reach states compared to state-of-the\u00a0\u2026", "num_citations": "23\n", "authors": ["1035"]}
{"title": "Information-theoretic and statistical methods of failure log selection for improved diagnosis\n", "abstract": " Diagnosis of each failed part requires the failed data captured on the test equipment. However, due to memory limitations on the tester, one often cannot store all the failed data for every chip tested. Consequently, truncated failure logs are used instead of complete logs for each part. Such truncation of the failure logs can result in very long turn-around times for diagnosis because important failure points may be removed from the log. Subsequently, the accuracy and resolution of final diagnosis may suffer even after multiple iterations of diagnosis. In addition, the existing test response compaction techniques though good for testing, either adversely affect diagnosis or are highly sensitive to deviation from the chosen fault model. In this context, the industry needs dynamic selection of better failure logs that enhances diagnosis. In this paper, we propose a number of metrics based on information theory that may help in\u00a0\u2026", "num_citations": "22\n", "authors": ["1035"]}
{"title": "Error diagnosis of sequential circuits using region-based model\n", "abstract": " Algorithms to locate multiple design errors using region-based model are studied for both combinational and sequential circuits. The model takes locality aspect of errors and is based on a 3-value, non-enumerative analysis technique. Studies show the effectiveness of the region based model for gate connection and gate substitution errors. For sequential circuits, we try to locate the time frame at which the error was first excited, by re-simulating as few vectors as possible preceding the erroneous vector in a fully initialized circuit to carry out the diagnosis. Experimental results on benchmark circuits are used to demonstrate rapid and accurate locating of multiple errors.", "num_citations": "22\n", "authors": ["1035"]}
{"title": "FSimGP^ 2: An efficient fault simulator with GPGPU\n", "abstract": " General Purpose computing on Graphical Processing Units (GPGPU) is a paradigm shift in computing that promises a dramatic increase in performance. But GPGPU also brings an unprecedented level of complexity in algorithmic design and software development. In this paper, we present an efficient parallel fault simulator, FSimGP 2 , that exploits the high degree of parallelism supported by a state-of-the-art graphic processing unit (GPU) with the NVIDIA Compute Unified Device Architecture (CUDA). A novel three-dimensional parallel fault simulation technique is proposed to achieve extremely high computation efficiency on the GPU. The experimental results demonstrate a speedup of up to 42\u00d7 compared to another GPU-based fault simulator and up to 53\u00d7 over a state-of-the-art algorithm on conventional processor architectures.", "num_citations": "21\n", "authors": ["1035"]}
{"title": "Pocket protection\n", "abstract": " In December 2006 the online Web site Xanga. com was fined $1 million for failing to protect children's privacy as required under the Children's Online Privacy Protection Act (COPPA). 1 The Federal Trade Commission (FTC) estimated that 1.7 million accounts were created by underaged children without their parent's knowledge or consent. 2 Although the site asked for a person's age before completing registration, warning those under thirteen that they could not participate, nevertheless the system allowed those who subsequently entered birthdates indicating that they were under thirteen to simply continue the process of registration and to access and post information on the site. 3 Xanga also collected information from the children, including name, address, cell phone number, and instant messenger identification, which they posted in the child's online profile.", "num_citations": "20\n", "authors": ["1035"]}
{"title": "Efficient transition fault ATPG algorithms based on stuck-at test vectors\n", "abstract": " This paper proposes novel algorithms for computing test patterns for transition faults in combinational circuits and fully scanned sequential circuits. The algorithms are based on the principle that s@ vectors can be effectively used to construct good quality transition test sets. Several algorithms are discussed. Experimental results obtained using the new algorithms show that there is a 20% reduction in test set size, test data volume and test application time compared to a state-of-the-art native transition test ATPG tool, without any reduction in fault coverage. Other benefits of our approach, viz. productivity improvement, constraint handling and design data compression are highlighted.", "num_citations": "20\n", "authors": ["1035"]}
{"title": "A novel statistical and circuit-based technique for counterfeit detection in existing ICs\n", "abstract": " Previously used ICs, which are resold as new, result in undue lost revenue, cause lower performance, reduced life span, and even catastrophic failure of platforms and systems. Non-invasive and inexpensive techniques are needed to establish the authenticity of such ICs that do not have special in-built structures for counterfeit detection. Although delay of circuit increases with its age, it cannot directly reveal the age of the chip, as it is also greatly influenced by process variation. In this work, we show that the relationship between two or more paths within the chip is a great indicator of its age. Using the proposed statistical and circuit-level technique, we observe over 97% correct detection of an aged IC from a new IC.", "num_citations": "19\n", "authors": ["1035"]}
{"title": "An ant colony optimization technique for abstraction-guided state justification\n", "abstract": " In this paper, a novel heuristic for abstraction-guided state justification is proposed based on ant colony optimization (ACO). A probabilistic state transition model is developed to help formulate the state justification problem as a searching scheme of artificial ants. The amount of pheromone left by the ants is directly proportional to the quality of the search so that it can serve as an effective guidance for the search. In addition, the intelligence based on the collective behavior is capable of avoiding critical dead-end states as well as fast convergence to the target state. Experimental results demonstrated that our approach is superior in reaching hard-to-reach states in sequential circuit compared to other methods.", "num_citations": "19\n", "authors": ["1035"]}
{"title": "Practical use of sequential ATPG for model checking: going the extra mile does pay off\n", "abstract": " We present a study of the practical use of a simulation-based automatic test pattern generation (ATPG) for model checking in large sequential circuits. Preliminary findings show that ATPGs which gradually build and learn from the state-space has the potential to achieve the verification objective without needing the complete state-space information. The success of verifying a useful set of properties relies on the performance and capacity of ATPG. We compared an excitation-only ATPG with one that performs both excitation and propagation. Even though the excitation-only strategy suffices to justify the objective, the excitation-and-propagation ATPG achieved higher signal-justification coverages than the excitation-only counterpart. This is because excitation-only ATPG falls short in obtaining pertinent state information helpful for traversing the state space, resulting in ATPG aborting the objective. Our experiments\u00a0\u2026", "num_citations": "19\n", "authors": ["1035"]}
{"title": "Combination of structural and state analysis for partial scan\n", "abstract": " Test generation complexity varies exponentially as the depth of cycles in the S-graph of the circuit. We map the hard-to-reach states obtained from a sequential test generator onto the cycles in the S-graph of the circuit. We then proceed to rank the cycles in terms of the testability gain that would result if the cycle were broken. The primary objective is not to cut all the cycles but to cut those cycles which are preventing the test generator from reaching these hard-to-reach states. To this end, we introduce new measures that combine conventional testability measures such as controllability and observability with the information from hard-to-reach states. We show that this approach overcomes some of the limitations of conventional cycle-cutting. This selective cutting of cycles is shown to yield better results in terms of fault coverage than conventional cycle-cutting.", "num_citations": "19\n", "authors": ["1035"]}
{"title": "POCKET: A tool for protecting children's privacy online\n", "abstract": " Children's privacy in the online environment has become critical. Use of the Internet is increasing for commercial purposes, in requests for information, and in the number of children who use the Internet for casual web surfing, chatting, games, schoolwork, e-mail, interactive learning, and other applications. Often, websites hosting these activities ask for personal information such as name, e-mail, street address, and phone number. In the United States, the children's online privacy protection act (COPPA) of 1998 was enacted in reaction to widespread collection of information from children and subsequent abuses identified by the Federal Trade Commission (FTC). COPPA is aimed at protecting a child's privacy by requiring parental consent before collecting information from children under the age of 13. To date, however, the business practices used and the technical approaches employed to comply with COPPA fail\u00a0\u2026", "num_citations": "18\n", "authors": ["1035"]}
{"title": "An overlapping scan architecture for reducing both test time and test power by pipelining fault detection\n", "abstract": " We present a novel scan architecture for simultaneously reducing test application time and test power (both average and peak power). Unlike previous works where the scan chain is partitioned only based on the excitation properties of the flip-flops (FFs), our work considers both the excitation and propagation properties of the scan FFs. In the proposed scan architecture, the scan chain is partitioned to maximize the overlapping between the excitation and propagation on different fault sets. The scan architecture also allows the entire set of detectable faults in the circuit under test (CUT) to be detected with only a portion of the scan elements active at a time, and thereby completely eliminates the need for the \"serial full-scan\" mode which is inefficient for both the test time and test power. Experimental results show that by introducing minimal hardware overhead, and without sacrificing fault coverage, an average peak\u00a0\u2026", "num_citations": "18\n", "authors": ["1035"]}
{"title": "New techniques for untestable fault identification in sequential circuits\n", "abstract": " This paper presents two low-cost fault-independent techniques that can be used to identify significantly more untestable faults than could be identified by earlier fault-independent techniques. A new theorem and an efficient implementation of the theorem for the purpose of identifying sequentially untestable faults are presented first. Unlike the single-fault theorem where the stuck-at fault is injected exclusively in the last time frame of the k-frame unrolled circuit, this theorem enables a fault injection in any time frame within the unrolled sequential circuit. To efficiently apply the authors' concept to untestable fault identification, sequential implications are used to extend the unobservability propagation of gates to multiple time frames during single-line conflict analysis. Then, a new technique called \"maximizing local impossibilities\" is proposed. This technique efficiently identifies multiple-node conflicting assignments by\u00a0\u2026", "num_citations": "18\n", "authors": ["1035"]}
{"title": "Automatic design validation framework for hdl descriptions via rtl atpg\n", "abstract": " We present a framework for high-level design validation using an efficient register-transfer level (RTL) automatic test pattern generator (ATPG). The RTL ATPG generates the test environments for validation targets, which include variable assignments, conditional statements, and arithmetic expressions in the HDL description. A test environment is a set of conditions that allow for full controllability and observability of the validation target. Each test environment is then translated to validation vectors by filling in the unspecified values in the environment. Since the observability of error effect is naturally handled by our ATPG, our approach is superior to methods that only focus on the excitation of HDL descriptions. The experimental results on ITC99 benchmark circuits and an industrial circuit demonstrate that very high design error coverage can be obtained in a small CPU times.", "num_citations": "17\n", "authors": ["1035"]}
{"title": "Effective safety property checking using simulation-based sequential ATPG\n", "abstract": " In this paper, we present a successful application of a simulation-based sequential Automatic Test Pattern Generation (ATPG) for safety property verification, with the target on verifying safety property of large, industrial-strength, hardware designs for which current formal methods fail. Several techniques are developed to increase the effectiveness and efficiency during state exploration and justification of the test generator for verification, including (1) incorporation of a small combinational ATPG engine, (2) reset signal masking, (3) threshold-value simulation, and (4) weighted Hamming distance. Experimental results on both ISCAS89 benchmark circuits and real industry circuits have shown that this simulation-based verifier achieves better or comparable results to current state-of-the-art formal verification tools BINGO and CHAFF.", "num_citations": "17\n", "authors": ["1035"]}
{"title": "Novel ATPG algorithms for transition faults\n", "abstract": " This paper proposes novel algorithms for computing test patterns for transition faults in combinational circuits and fully scanned sequential circuits. The algorithms are based on the principle that s@ vectors can be effectively used to construct good quality transition test sets. Several algorithms are discussed. Experimental results obtained using the new algorithms show that there is a 20% reduction in test set size, test data volume and test application time compared to a state-of-the-art native transition test ATPG tool, without any reduction in fault coverage. Other benefits of our approach, viz. productivity improvement, constraint handling and design data compression are highlighted.", "num_citations": "17\n", "authors": ["1035"]}
{"title": "Multi-node static logic implications for redundancy identification\n", "abstract": " This paper presents a method for redundancy identification (RID) using multi-node logic implications. The algorithm discovers a large number of direct and indirect implications by extending single node implications to multiple nodes. The large number of implications found by multi-node implication method introduces a new redundancy identification technique. Our approach uses an effective node-pair selection method which is O(n) in the number of nodes to reduce execution time, and it can be used as an efficient preprocessing phase for test generation. Application of these multi-node static logic implications uncovered more redundancies in ISCAS85 combinational circuits than previous single-node methods without excessive computational effort.", "num_citations": "17\n", "authors": ["1035"]}
{"title": "Mining complex boolean expressions for sequential equivalence checking\n", "abstract": " We propose a novel technique to mine powerful and generalized boolean relations among flip-flops in a sequential circuit for sequential equivalence checking. In contrast to traditional learning methods, our mining algorithm can detect inductive invariants as well as illegal state cubes. These invariants can be arbitrary boolean expressions and can thus prune a large don't care space during equivalence checking. Experimental results demonstrate that these general invariants can be very effective for sequential equivalence checking of circuits with no or very few equivalent signals between them, with low computational costs.", "num_citations": "16\n", "authors": ["1035"]}
{"title": "Efficient ATPG for design validation based on partitioned state exploration histories\n", "abstract": " This paper introduces a new concept of state partitioning and state/transition exploration histories to generate test stimulus for the purpose of design validation. With our new state partitioning, during vector generation, state and transition exploration histories for each state group are maintained by dynamically constructing partial state transition graphs (STGs) for all state groups. By limiting a maximum size any state group can be, maintaining the complete state and transition exploration histories for each state group is feasible even for very large sequential circuits. While such histories are being collected, test vectors are generated using extracted spectral information from existing tests and genetic algorithm (GA) is used to explore new scenarios that are not in the histories. Experiments showed that much higher design error coverages together with smaller test sets are achieved with very short execution times.", "num_citations": "16\n", "authors": ["1035"]}
{"title": "Characteristic faults and spectral information for logic BIST\n", "abstract": " We present a new method of built-in-self-test (BIST) for sequential circuits and system-on-a-chip (SOC) using characteristic faults and circuitspecific spectral information in the form of one or more Hadamard coefficients. The Hadamard coefficients are extracted from the test sequences for a small set of characteristic faults of the circuit. By extracting a few characteristic faults from the circuit, we show that detection of these characteristic faults is sufficient in detecting a vast majority of the remaining faults in the circuit. The small number of characteristic faults allows us to reduce the coefficients necessary for BIST. State relaxation is performed on the compacted test sequences to reduce the spectral noise further. Since we are targeting only a very small number of characteristic faults, the execution times for computing the spectra are greatly reduced. Our experimental results show that our new method can achieve high\u00a0\u2026", "num_citations": "16\n", "authors": ["1035"]}
{"title": "Efficient sequential test generation based on logic simulation\n", "abstract": " In this article, we present an efficient logic-simulation-based test generator that executes significantly more quickly than its fault-simulation-based counterparts. This test generator's fault coverage compares favorably with that of the latest techniques for large sequential circuits. It uses a genetic algorithm to achieve both high fault coverage and short test generation times.", "num_citations": "16\n", "authors": ["1035"]}
{"title": "Strategies for scalable symbolic execution-driven test generation for programs\n", "abstract": " With the advent of advanced program analysis and constraint solving techniques, several test generation tools use variants of symbolic execution. Symbolic techniques have been shown to be very effective in path-based test generation; however, they fail to scale to large programs due to the exponential number of paths to be explored. In this paper, we focus on tackling this path explosion problem and propose search strategies to achieve quick branch coverage under symbolic execution, while exploring only a fraction of paths in the program. We present a reachability-guided strategy that makes use of the reachability graph of the program to explore unvisited portions of the program and a conflict-driven backtracking strategy that utilizes conflict analysis to perform nonchronological backtracking. We present experimental evidence that these strategies can significantly reduce the search space and improve\u00a0\u2026", "num_citations": "15\n", "authors": ["1035"]}
{"title": "A framework for automatic design validation of RTL circuits using ATPG and observability-enhanced tag coverage\n", "abstract": " This paper presents a framework for high-level design validation using an efficient register-transfer level (RTL) automatic test pattern generator (ATPG). The RTL ATPG algorithm first generates a test environment for each validation objective, which includes variable assignments, conditional statements, and arithmetic expressions in the hardware description language (HDL) description. The test environment for a given validation objective is a set of symbolic conditions that allow for full controllability and observability of that objective. After the RTL ATPG terminates, a back-end translator intelligently translates the test environments into validation vectors by filling in the necessary values. Since the observability of error effect is naturally handled by the RTL ATPG algorithm, this approach is superior to most existing validation methods, which only focus on the excitation of HDL constructs. A set of heuristics is proposed to\u00a0\u2026", "num_citations": "15\n", "authors": ["1035"]}
{"title": "Testing embedded sequential cores in parallel using spectrum-based BIST\n", "abstract": " We present a new BIST (built-in-self-test) architecture for system-on-a-chip (SOC), which can test a cluster of embedded sequential cores simultaneously. The compressed spectrum for a cluster of cores under test is computed by performing spectral analysis individually on all cores. Because there is no need to combine the cores to extract the spectrum for the entire cluster, the computation complexity is greatly reduced. For each individual core, we propose an interleaved state relaxation on the compacted test sequence for its characteristic fault set, leading to a partially specified, interleaved sequence which can be merged in a much easier way. A delay network and a switching network are added selectively to allow for more aggressive merging of spectra. Experimental results show that the same level of fault coverage can be achieved for each individual core with negligible hardware overhead, while the test\u00a0\u2026", "num_citations": "15\n", "authors": ["1035"]}
{"title": "A novel, low-cost algorithm for sequentially untestable fault identification\n", "abstract": " This paper presents a new and low-cost approach for identifying sequentially untestable faults. Unlike the single fault theorem, where the stuck-at fault is injected only in the right-most time frame of the k-frame unrolled circuit, our approach can handle fault injection in any time frame within the unrolled sequential circuit. To efficiently apply our concept to untestable fault identification, powerful sequential implications are used to efficiently extend the unobservability propagation of gates in multiple time frames. Application of the proposed theorem to ISCAS '89 sequential benchmark circuits showed that more untestable faults could be identified using our approach, at practically no overhead in both memory and execution time.", "num_citations": "15\n", "authors": ["1035"]}
{"title": "Partitioning and reordering methods for static test sequence compaction of sequential circuits\n", "abstract": " Methods of compacting sequential circuit test vector set by partitioning of faults into hard and easy faults, re-ordering vectors in a test set by moving sequences that detect hard faults to the beginning of the test set, and a combination of partitioning and re-ordering.", "num_citations": "15\n", "authors": ["1035"]}
{"title": "Automated Program Synthesis from Object-Oriented Natural Language for Computer Games.\n", "abstract": " A prototype object-oriented natural-language programming system for computer/video games is described, in which sentences written in object-oriented English is automatically converted to a functional, executable game code in Javascript. In addition, new attributive words are automatically learned while converting the text to code. Any syntactic or semantic errors are reported during the compilation to help users to debug and fine-tune the game. With less than 20 plain English sentences, up to 1800 lines of game code can be generated.", "num_citations": "14\n", "authors": ["1035"]}
{"title": "Search state compatibility based incremental learning framework and output deviation based X-filling for diagnostic test generation\n", "abstract": " Silicon Diagnosis is the process of locating potential defect sites (candidates) in a defective chip. These candidates are then used as an aid during physical failure analysis. It is desired that the cardinality of the candidate set returned by silicon diagnosis be as small as possible. To this end, effective test patterns that can distinguish as many fault-pairs in the candidate set are critical. Generation of such diagnostic patterns is referred to as Automatic Diagnostic Test Generation (ADTG). In this paper, we propose an aggressive and efficient learning framework for such a diagnostic test generation engine. It allows us to identify and prune non-trivial redundant search states thereby allowing to easily solve hard to distinguish or hard to prove equivalent fault-pairs. Further, we propose an incremental flow for ADTG, where the information learned during detection-oriented test generation is passed to and\u00a0\u2026", "num_citations": "14\n", "authors": ["1035"]}
{"title": "Efficient techniques for transition testing\n", "abstract": " Scan-based transition tests are added to improve the detection of speed failures in sequential circuits. Empirical data suggests that both data volume and application time will increase dramatically for such transition testing. Techniques to address the above problem for a class of transition tests, called enhanced transition tests, are proposed in this article.The first technique, which combines the proposed transition test chains with the ATE repeat capability, reduces test data volume by 46.5% when compared with transition tests computed by a commercial transition test ATPG tool. However, the test application time may sometimes increase. To address the test time issue, a new DFT technique, Exchange Scan, is proposed. Exchange scan reduces both data volume and application time by 46.5%. These techniques rely on the use of hold-scan cells and highlight the effectiveness of hold-scan design to address test time\u00a0\u2026", "num_citations": "14\n", "authors": ["1035"]}
{"title": "Using Global Structural Relationships of Signals to Accelerate SAT-based Combinational Equivalence Checking.\n", "abstract": " We propose a novel technique to improve SAT-based Combinational Equivalence Checking (CEC). The idea is to perform a low-cost preprocessing that will statically induce global signal relationships into the original CNF formula of the miter circuit under verification, and hence reduce the complexity of the SAT instance. This efficient and effective preprocessing quickly builds up the implication graph for the miter circuit under verification, yielding a large set of direct, indirect and extended backward implications. These two-node implications spanning the entire circuit are converted into binary clauses, and they are added to the miter CNF formula. The added clauses constrain the search space of the SAT solver and provide correlation among the different variables, which enhances the Boolean Constraint Propagation (BCP). Experimental results on large and difficult ISCAS\u201985, ISCAS\u201989 (full scan) and ITC\u201999 (full\u00a0\u2026", "num_citations": "14\n", "authors": ["1035"]}
{"title": "Untestable fault identification using recurrence relations and impossible value assignments\n", "abstract": " This paper presents two novel and low cost techniques that can be used for the purpose of untestable fault identification. First, we present a new theorem and a practical method using static implications to identify unexcitable nets using recurrence relations in sequential circuits. Since each unexcitable net generally infers to more than one untestable fault, this theorem helps us to quickly identify significantly more sequentially untestable faults. In addition to discovering unexcitable nets using recurrence relations, we propose a second approach that aims at quickly identifying non-trivial multiple-node conflicts, which can then be used to identify additional untestable faults in both combinational and sequential circuits. Unlike previous techniques that concentrate on identifying local conflicts in the circuit, our approach efficiently extends the conflicting value analysis across multiple levels in the circuit to identify more\u00a0\u2026", "num_citations": "14\n", "authors": ["1035"]}
{"title": "Dual-purpose mixed-level test generation using swarm intelligence\n", "abstract": " Automatic test pattern generation for non-scan sequential circuits is an extremely challenging task. If successful, it can offer many benefits to the EDA community, ranging from manufacturing and functional test to post-silicon validation. High-level test generators often miss the low-level details, thus missing the detection of some gate-level faults. On the other hand, gate-level test generators miss the high-level path traversal knowledge to more effectively traverse the state space. In this work, we present a fine-grain mixed-level test generator that utilizes co-simulation of register-transfer and gate levels to generate high quality vectors. The algorithm, based on an ant colony optimization, targets branch coverage at the RTL and simultaneously attempts to associate rare fault excitations with a sequence of branch activations. By weighting these sequences within the fitness function across the two levels, the algorithm is\u00a0\u2026", "num_citations": "13\n", "authors": ["1035"]}
{"title": "A Bayesian approach to fingerprint minutia localization and quality assessment using adaptable templates\n", "abstract": " Fingerprints continue to serve as a reliable trait for human identification. Feature-based matching techniques, such as those used by Automated Fingerprint Identification Systems (AFIS), have demonstrated remarkable success in minutiae-based matching from good quality prints with relatively large extent. As the image quality degrades and acquired fingerprint area decreases, however, the number of reliable minutiae that can be automatically detected decreases, causing match performance to suffer. This paper presents a novel approach to improving the precision of features that can be extracted from fingerprint images. This is accomplished through improved minutia localization and quality assessment routines that are inspired in part by human visual perception. Initial results have shown an improvement in minutia accuracy for 88.2% of fingerprint minutia sets after applying the proposed localization method. An\u00a0\u2026", "num_citations": "13\n", "authors": ["1035"]}
{"title": "SAT-based equivalence checking of threshold logic designs for nanotechnologies\n", "abstract": " Novel nano-scale devices have shown promising potential to overcome physical barriers faced by complementary metal-oxide semiconductor (CMOS) technology in future circuit design. However, many nanotechnologies are intrinsically suitable for implementing threshold logic rather than Boolean logic which has dominated CMOS technology in the past. To fully take advantage of such emerging nanotechnologies, efficient design automation tools for threshold logic therefore become essential. In this work, we propose novel techniques of formulating a given threshold logic in conjunctive normal form (CNF) that facilitates efficient SAT-based equivalence checking. Three different strategies of CNF generation from threshold logic representations are implemented. Experimental results based on MCNC benchmarks are presented as a complete comparison. Our hybrid algorithm, which takes into account input\u00a0\u2026", "num_citations": "13\n", "authors": ["1035"]}
{"title": "Efficient fault collapsing via generalized dominance relations\n", "abstract": " Fault collapsing of a fault-set helps in obtaining smaller test-sets as well as in reducing fault-simulation times. In this paper, we propose two new theorems by making use of the generalized dominance relations exhibited by a pair of faults. In order to learn several unique requirements for the faults in a low-cost manner, we employ a fault-independent analysis and propose heuristics to reduce the number of fault-pair comparisons required. Experimental results on ISCAS85, 89 & 93 benchmarks show that significantly more faults can be collapsed as compared to the existing methods, with smaller run-times in many cases. For most circuits, collapsing to less than 30% of the total number of faults is achieved", "num_citations": "13\n", "authors": ["1035"]}
{"title": "Fast circuit topology based method to configure the scan chains in Illinois Scan architecture\n", "abstract": " High test data volume and long test application time are two major concerns for testing scan based circuits. The Illinois Scan (ILS) architecture has been shown to be effective in addressing both these issues. The ILS achieves a high degree of data compression thereby reducing both test data volume and test application time. However, the fault coverage achieved in the Broadcast Mode of the ILS architecture depends on the actual configuration of individual scan chains, i.e., the number of chains and the mapping of the individual flip-flops of the circuit to the respective scan chain positions. Current methods for constructing scan chains in the ILS are either ad-hoc or rely on test pattern information from an apriori ATPG run. In this paper, we present a novel low cost technique to construct ILS scan configuration for a given design. It efficiently utilizes the circuit topology and tries to optimize the flip-flop assignment to a\u00a0\u2026", "num_citations": "12\n", "authors": ["1035"]}
{"title": "Mining sequential constraints for pseudo-functional testing\n", "abstract": " Using DFT methods such as scan can improve testability and increase fault coverage. However, scan tests may scan in illegal or unreachable states during test application, which may result in incidental detection of functional untestable delay faults during the scan test. This paper presents novel mining techniques for fast top-down functional constraint extraction. The extracted functional constraints capture illegal states through internal signal relations. Imposing these relations as functional constraints to a commercial ATPG tool allows for the generation of effective pseudo-functional tests. We analyze its impact on minimizing the over-testing problem of the scan- based circuits. The experimental results on transition faults and path delay faults reveal that the proposed method produces a small fraction, yet extremely powerful functional constraints effective for constraining the state space.", "num_citations": "12\n", "authors": ["1035"]}
{"title": "Using scan-dump values to improve functional-diagnosis methodology\n", "abstract": " In this paper, we identify two main bottlenecks in the functional diagnosis flow and propose new ways to overcome these. Our approach completely eliminates the \"primary input (PI) pattern generation and simulation\" step and instead employs scan-dump values extracted from the tester. We utilize backward and forward logic implications of the scan-dump values to reconstruct more logic values for the circuit signals. Furthermore, we employ the reset state for the non-scan latches of the design to increase the number of specified signals in the overall circuit. Experimental results on stuck-at faults on industrial designs show that, in most cases, these reconstructed values are sufficient to correctly diagnose a fault, thereby avoiding hours of conventional functional diagnosis runtimes.", "num_citations": "12\n", "authors": ["1035"]}
{"title": "Energy-efficient logic BIST based on state correlation analysis\n", "abstract": " We present a new low-power BIST (built-in-self-test) for sequential circuits. State correlation analysis is first performed on the flip-flop values in the relaxed, compacted sequence for the undetected faults to extract spatial correlations among the flip-flops. The extracted spatial correlation matrix not only provides additional metrics through which the scan order may be altered, but also allows us to omit some flip-flops in the scan chain. By leaving flip-flops that need less control out of the scan chain, we can reduce transitions on those flip-flops, thereby reducing the overall power and energy. The omission of flip-flops are done in a way that the fault coverage is unaffected. Furthermore, reordering of the flip-flops in the scan chain allows the generated patterns to be more compatible with the state sequence necessary for exciting the random-pattern-resistant faults. Our experiments show that the same or higher fault\u00a0\u2026", "num_citations": "12\n", "authors": ["1035"]}
{"title": "Trace buffer-based silicon debug with lossless compression\n", "abstract": " The capacity of the available on-chip trace buffer is limited. To increase its capacity, we propose real-time compression of the trace data via novel source transformation functions, namely real-time difference vector computation, efficient interconnect network and real time alternate vector reversal that reduces the entropy of the trace data. The proposed compression technique is implemented on hardware and operates real-time to capture debug data. Experimental results for sequential benchmark circuits show that the proposed method gives better compression percentage compared to prior works. The area overhead of our trace compressor is up to 20X less compared to dictionary-based codes and yields up to 4X improvement in the compression ratio.", "num_citations": "11\n", "authors": ["1035"]}
{"title": "ATPG-based Preimage Computation: Efficient search space pruning with ZBDD\n", "abstract": " Computing image/preimage is a fundamental step in formal verification of hardware systems. Conventional OBDD-based methods for formal verification suffer from spatial explosion, since OBDDs can grow exponentially in large designs. On the other hand, SAT/ATPG based methods are less demanding on memory. But the run-time can be huge for these methods, since they must explore an exponential search space. In order to reduce this temporal explosion of SAT/ATPG based methods, efficient learning techniques are needed. In this paper, we present a new ZBDD based method to compactly store and efficiently search previously explored search-states for 'ATPG-based preimage computation'. We learn front these search-states and avoid searching their subsets or supersets. Both,solution and conflict subspaces are pruned based on simple set operations using ZBDDs. We integrate our techniques into an\u00a0\u2026", "num_citations": "11\n", "authors": ["1035"]}
{"title": "Techniques to reduce data volume and application time for transition test\n", "abstract": " Scan based transition tests are added to improve the detection of IC speed failures using scan tests. Empirical data suggests that both data volume and application time for transition test will increase dramatically. Techniques to address this problem, for a class of transition tests called \"enhanced transition tests\", are proposed. The first technique, which combines the ATE repeat capability and the notion of transition test chains, reduces test data volume by 46.5%, when compared with transition tests computed by a commercial transition test ATPG tool. The test application time could increase or decrease. To address the test time issue, exchange scan, a new DFT technique, is proposed. Exchange scan reduces both data volume and application time by 46.5%. These techniques rely on the use of hold scan cells and highlight the effectiveness of hold-scan design to address test time and test data volume issues.", "num_citations": "11\n", "authors": ["1035"]}
{"title": "Partitioning and Reordering Techniques for Static Test Sequence Compaction of Sequential Circuits\n", "abstract": " We propose a new static test set compaction method based on a careful examination of attributes of fault coverage curves. Our method is based on two key ideas: (1) fault-list and test-set partitioning, and (2) vector re-ordering. Typically, the first few vectors of a test set detect a large number of faults. The remaining vectors usually constitute a large fraction of the test set, but these vectors ore included to detect relatively few hard faults. We show that significant compaction can be achieved by partitioning faults into hard and easy faults. This significantly reduces the computational cost for static test set compaction without affecting quality of compaction. The second technique re-orders vectors in a test yet by moving sequences that detect hard faults to the beginning of the test set. Fault simulation of the newly concatenated re-ordered test set results in the omission of several vectors so that the compact test set is smaller\u00a0\u2026", "num_citations": "11\n", "authors": ["1035"]}
{"title": "RTL functional test generation using factored concolic execution\n", "abstract": " In this paper, we present CORT, a factored concolic execution based methodology for high-level functional test generation. Our test generation effort is visualized as the systematic unraveling of the control-flow response of the design over multiple explorations. We begin by transforming the Register Transfer Level (RTL) source for the design into a high-performance C++ compiled functional simulator which is instrumented for branch coverage. An exploration begins by simulating the design with concrete stimuli. Then, we perform an interleaved cycle-by-cycle symbolic evaluation over the concrete execution trace extracted from the Control Flow Graph (CFG) of the design. The purpose of this task is to dynamically discover means to divert the control flow of the system, by mutating primary-input stimulated control statements in this trace. We record the control-flow response as a Test Decision Tree (TDT), a novel\u00a0\u2026", "num_citations": "10\n", "authors": ["1035"]}
{"title": "Evaluation of online resources in assisting phishing detection\n", "abstract": " Phishing is an attempt to fraudulently acquire userspsila sensitive information, such as passwords or financial information, by masquerading as a trustworthy entity in online transactions. Recently, a number of researchers have proposed using external online resources like the Google Page Rank system to assist phishing detection. The advantage of such an approach is that the detection capability will gradually evolve and improve as the online resources become more sophisticated and manipulation-resistant. In this paper, we evaluate the effectiveness of three popular online resources in detecting phishing sites-viz, Google PageRank system, Yahoo! Inlink data, and Yahoo! directory service. Our results indicate that these online resources can be used to increase the accuracy of phishing site detection when used in conjunction with existing phishing countermeasures. The proposed approach involves examining\u00a0\u2026", "num_citations": "10\n", "authors": ["1035"]}
{"title": "Extended forward implications and dual recurrence relations to identify sequentially untestable faults\n", "abstract": " In this paper, we make two major contributions: First, to enhance Boolean learning, we propose a new class of logic implications called extended forward implications. Using a novel concept called implication-frontier, extended forward implications efficiently capture those nontrivial relationships which previous techniques failed to identify. Secondly, we introduce the concept of dual recurrence relations in sequential circuits, and propose a new theorem which uses this concept to quickly identify sequentially untestable faults. Our tool based on the proposed extended forward implications and the new theorem was applied to identify untestable faults in benchmark circuits. Significantly more untestable faults than reported by earlier techniques, low memory overhead and low computational complexity are the noteworthy features of our tool.", "num_citations": "10\n", "authors": ["1035"]}
{"title": "On identifying functionally untestable transition faults\n", "abstract": " This paper presents a new approach on identifying functionally untestable transition faults in nonscan sequential circuits. We formulate a new dominance relationship for transition faults and use it to identify more sequentially untestable transition faults. The proposed method consists of two phases: first, a large number of functionally untestable transition faults is identified by a fault-independent sequential logic implications implicitly crossing multiple time-frames, and the identified untestable faults are classified into three conflict categories. Next, additional functionally untestable transition faults are identified by dominance relationships from the previous identified untestable transition faults. The experimental results for ISCAS89 sequential benchmark circuits showed that our approach can quickly identify many more functionally untestable transition faults than previously reported.", "num_citations": "10\n", "authors": ["1035"]}
{"title": "State variable extraction to reduce problem complexity for ATPG and design validation\n", "abstract": " We present a new algorithm to extract characteristic flip-flops, which form a characteristic state set, using state correlation information. The extracted characteristic state set allows us to focus on a significantly smaller set of flip-flops while ignoring other flip-flops, thereby simplifying the target problem and reducing state explosion in very large sequential circuits. Next, partitioning is applied only on the characteristic state variables, and partial state transition graphs (STGs) are built. During test generation, test vectors are generated using a two-fold criteria: (1) whether the vector expand the overall STGs, and (2) whether this vector break the relationship among flip-flops within the correlated sets. Experiments showed that our extraction algorithm can reduce the original complete state set by up to 97%. In addition, with the reduced state variables, we achieve not only equal or better coverages for both stuck-at faults and\u00a0\u2026", "num_citations": "10\n", "authors": ["1035"]}
{"title": "Genetic Spot Optimization for Peak Power Estimation in Large VLSI Circuits\n", "abstract": " Estimating peak power involves optimization of the circuit's switching function. The switching of a given gate is not only dependent on the output capacitance of the node, but also heavily dependent on the gate delays in the circuit, since multiple switching events can result from uneven circuit delay paths in the circuit. Genetic spot expansion and optimization are proposed in this paper to estimate tight peak power bounds for large sequential circuits. The optimization spot shifts and expands dynamically based on the maximum power potential (MPP) of the nodes under optimization. Four genetic spot optimization heuristics are studied for sequential circuits. Experimental results showed an average of 70.7% tighter peak power bounds for large sequential benchmark circuits was achieved in short execution times.", "num_citations": "10\n", "authors": ["1035"]}
{"title": "Correlation analysis of compacted test vectors and the use of correlated vectors for test generation\n", "abstract": " In this paper we present novel correlation analyses methods to analyze the compacted test sets. In spatial correlation, we identified strong correlation chains, while in the temporal domain, correlating a segment of the test set with the entire sequence discovered periodicities in the compacted test set sequence. These suggest the potential of a new way for generating test sequences. We report preliminary experiments in which only the spatial correlation was used to generate random vectors, which when compacted, provided tests for several ISCAS\u201989 benchmarks.", "num_citations": "10\n", "authors": ["1035"]}
{"title": "GPU-based timing-aware test generation for small delay defects\n", "abstract": " A GPU-based timing-aware ATPG is proposed to generate a compact high-quality test set. The test generation algorithm backtraces and propagates along multiple long paths so that many test patterns are generated at the same time. Generated test patterns are then fault simulated and selected. Compared with an 8-core CPU-based timing-aware commercial ATPG, the proposed GPU-based technique achieved 36% test length reductions on large benchmark circuits while the SDQL quality remains almost the same.", "num_citations": "9\n", "authors": ["1035"]}
{"title": "Parents and the internet: Privacy awareness, practices and control\n", "abstract": " As children increasingly use the Internet, there have been mounting concerns about their privacy online. As a result, the US Congress enacted the Children\u2019s Online Privacy Protection Act (COPPA) to prohibit websites from collecting information from children under 13 years of age without verifiable parental consent. Unfortunately, few technologies are available for parents to provide this consent. Further, few parents are aware of the laws and technologies available. This research explored parental awareness of laws and technologies associated with protecting children\u2019s privacy online, and usage of technologies and techniques for parental control, using focus group research. The results of the study are used to propose an emergent framework of factors that will impact use of privacy protection tools and techniques by parents.", "num_citations": "9\n", "authors": ["1035"]}
{"title": "Efficient power droop aware delay fault testing\n", "abstract": " In today's deep sub-micron designs, large amounts of switching activity may cause a substantial voltage drop on the power rails, also called power droop. Faults that may result from a power droop include delay faults caused by the increased propagation delays from the reduced supply voltage. In order to assess the performance of a manufactured chip, its worst-case droop condition should be tested by applying a specific input pattern which can cause maximum switching activity. On the contrary, during delay fault diagnosis, it would be beneficial for the diagnostic patterns to induce less switching activity in order to filter the embedded noise. In this paper, we propose a new SAT formulation that incorporates depth-limited search to compute the test patterns for these power droop faults. Experimental results demonstrate the efficiency of the proposed approach for generating test patterns for both transition and path\u00a0\u2026", "num_citations": "9\n", "authors": ["1035"]}
{"title": "Bounded model checking of embedded software in wireless cognitive radio systems\n", "abstract": " We present a new verification approach that applies aggressive program slicing and a proof-based abstraction-refinement strategy to enhance the scalability of bounded model checking of embedded software. While many software model-checking tools use program slicing as a separate or optional step, our program slicing is integrated in the model construction and reduction process. And it is combined with the compilation optimization techniques so to compute a more accurate slice. We also explore a proof-based abstraction-refinement strategy using the under/overapproximation on our proposed software model, and propose a heuristic method of deciding new encoding size to refine the under-approximation. Experiments on C programs from wireless cognitive radio systems show this approach can greatly reduce the model size and shorten the solving time by the SAT-solver.", "num_citations": "9\n", "authors": ["1035"]}
{"title": "Explicit safety property strengthening in SAT-based induction\n", "abstract": " Strengthening a property allows it to be falsified/verified at an earlier induction depth. In this paper, we propose new preprocessing techniques for explicitly identifying co-invariants for a given safety property which are then added to that property for faster verification. First, we employ a path-oriented decision making engine to quickly identify several states which have paths to states violating the property. Next, we generate a set of candidate co-invariants and propose an induction-based technique to learn true co-invariants among those candidates. All the learned co-invariants are minimized using resolution and added to the original property to strengthen it. Experiments show that the induction depth needed to prove many safety properties can be significantly reduced via our strengthening, thereby achieving more than an order of magnitude runtime improvements", "num_citations": "9\n", "authors": ["1035"]}
{"title": "Fast illegal state identification for improving SAT-based induction\n", "abstract": " In this paper, we propose a novel framework to quickly extract illegal states of a sequential circuit and then use them as constraints during the SAT-based induction runs. First, we employ a low-cost combinational ATPG to identify unreachable partial-states among groups of related flip-flops. Second, we propose the concept of necessary-assignment looping to identify additional unachievable partial-states. Third, we extend the above unachievability theory to capture new non-trivial sequential logic dependencies among the circuit signals. Finally, we use a unified framework that utilizes all the above information and aims at maximizing the learning. All the learned illegal states are converted into constraint clauses and are replicated at all the unrolled transition relations to prune the search-space. Experimental results show that, due to the added constraints, many safety properties can be proved at earlier depths and\u00a0\u2026", "num_citations": "9\n", "authors": ["1035"]}
{"title": "Error diagnosis of sequential circuits using region-based model\n", "abstract": " Algorithms to locate multiple design errors using region-based model are studied for both combinational and sequential circuits. The model takes locality aspect of errors and is based on a 3-value, non-enumerative analysis technique. Studies show the effectiveness of the region based model for single and multiple stuck faults and gate connection errors. For sequential circuits, we try to locate the time frame at which the error was first excited, by re-simulating as few vectors as possible preceding the erroneous vector in a fully initialized circuit to carry out the diagnosis. Experimental results on benchmark circuits are used to demonstrate rapid and accurate locating of multiple errors.", "num_citations": "9\n", "authors": ["1035"]}
{"title": "High quality ATPG for delay defects\n", "abstract": " The paper presents a novel technique for generating effective vectors for delay defects. The test set achieves high path delay fault coverage to capture smalldistributed delay defects and high transition fault coverage to capture gross delay defects. Furthermore, non-robust paths for ATPG are filtered (selected) carefully so that there is a minimum overlap with the already tested robust paths. A relationship between path delay fault model and transition fault model has been observed which helps us reduce the number of non-robust paths considered for test generation. To generate tests for robust and non-robust paths, a deterministic ATPG engine is developed. Clustering of paths has been done in order to improve the test set quality. Implications were used to identify the untestable paths. Finally an incremental propagation based ATPG is used for transition faults. Results for ISCAS\u201985 and full-scan ISCAS\u201989 benchmark circuits show that the filtered nonrobust path set can be reduced to 40% smaller than the conventional path set without losing delay defect coverage. Clustering reduces vector size in average by about 40%.", "num_citations": "9\n", "authors": ["1035"]}
{"title": "Spectrum-based BIST in complex SOCs\n", "abstract": " Presents a spectral built-in-self-test (BIST) for a system-on-a-chip (SOC) environment. Test vectors are generated using the spectral properties of the embedded cores. Because some embedded cores may not have direct connections to the embedded TPG, it would be necessary to test them via other cores. As a result, testing such (cascaded) cores requires considerations on the spectral characteristics of the predecessor and successor cores. Matching spectral characteristics between the outputs of the predecessor core and dominant inputs of the successor core allows the successor core to be more testable. Experimental results for the spectral BIST showed that significantly more faults can be detected using spectral patterns than by conventional weighted random BIST technique.", "num_citations": "9\n", "authors": ["1035"]}
{"title": "A natural language programming application for Lego Mindstorms EV3\n", "abstract": " In this paper, a controlled natural language (CNL) based program synthesis system for the Lego Mindstorms EV3 (EV3) is introduced. The system is developed with the intention of helping middle and high school Lego robotics enthusiasts and non-programmers to learn the necessary skills for programming and engineering the robot with less effort. The system generates the resulting code in Microsoft Small Basic that controls the EV3 Intelligent Brick with supports for all EV3 sensors and motors. Preliminary results show that our approach is capable of generating functional, executable code based on the users' controlled natural language specifications. Detailed error messages are also given when confronted with unimplementable sentences.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Abstraction-based relation mining for functional test generation\n", "abstract": " Functional test generation and design validation frequently use stochastic methods for vector generation. However, for circuits with narrow paths or random-resistant corner cases, purely random techniques can fail to produce adequate results. Deterministic techniques can aid this process; however, they add significant computational complexity. This paper presents a Register Transfer Level (RTL) abstraction technique to derive relationships between inputs and path activations. The abstractions are built off of various program slices. Using such a variety of abstracted RTL models, we attempt to find patterns in the reduced state and input with their resulting branch activations. These relationships are then applied to guide stimuli generation in the concrete model. Experimental results show that this method allows for fast convergence on hard-to-reach states and achieves a performance increase of up to 9\u00d7 together\u00a0\u2026", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Establishing the quantitative basis for sufficiency thresholds and metrics for friction ridge pattern detail and the foundation for a standard\n", "abstract": " The purpose of this two-year project has been to address the need for a sound, quantitative basis for assessing the quality of fingerprint images. Latent prints, in particular, can be problematic because they are often partial, smudged, and otherwise distorted. Prints of sufficiently high quality routinely allow for identification (ie, originates from one known source) or exclusion (ie, could not have originated from a reference source). However, image quality problems related to identifiable Level 1, 2, or 3 details can be a major source of uncertainty and potential error, or may contribute to a (sometimes incorrect) determination of no conclusion. An ability to assess fingerprint image quality therefore represents a crucial step in reaching correct determinations.The high-level goal of this cross-disciplinary collaboration has been to derive a scientific foundation for measurement of fingerprint image quality, particularly for latent prints. The objectives of this effort have been the following: to make a significant contribution to increasing accuracy, reliability, repeatability, verification, defensibility, and uniform assessment of fingerprint pattern analysis and practice; to provide a demonstrable and defensible basis for engagement of the relevant practitioner and stakeholder communities to incorporate and accept standards into friction ridge pattern analysis, reporting, and use; to provide for substantial improvements to training, proficiency testing, quality assurance, and control (quality management) that are more consistent across the forensic science community; to incorporate metrics that can be documented into the ACE-V or other accepted friction ridge examination\u00a0\u2026", "num_citations": "8\n", "authors": ["1035"]}
{"title": "A novel SMT-based technique for LFSR reseeding\n", "abstract": " In order for logic built-in-self-test (LBIST) to achieve coverages comparable with deterministic tests, multiple (and frequently many) seeds are often needed. Unlike previous methods that attempt to chain/compact the number of seeds, we present a novel Satisfiability Modulo Theory (SMT) based technique that can reduce the number of seeds significantly while simultaneously achieving high coverage for LBIST. In this technique we integrate the process of deterministic test generation and seed generation in one SMT process to eliminate the problems of chaining the separately generated deterministic patterns. Experimental results show the promise of the approach.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Minutiae+ friction ridges= triplet-based features for determining sufficiency in fingerprints\n", "abstract": " In order to provide statistical and qualitative backing to latent fingerprint evidence, an algorithm is proposed to discover statistically rare features or patterns in fingerprint images. These features would help establish an objective minimum- quality baseline for latent prints as well as aid in the latent examination process in reaching a matching decision. The proposed algorithm uses minutia triplet-based features in a hierarchical fashion, where minutia points are used along with ridge information toestablish relations between minutiae. Preliminary results show that a set of distinctive features can be found that have sufficient discriminatory power to aid in quality assessment. An example set of 10 statistically rare features is presented, resulting from analysis of a set of 93 images.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Rethinking fingerprint evidence through integration of very large digital libraries\n", "abstract": " Fingerprints play a key role in biometrics and forensic science because of their uniqueness. Essential is contextual integration of fingerprint evidence from different sources, which involves composing, reusing, and aggregating a large amount of information. Thus, this paper (1) describes different types of fingerprint information from a digital library perspective;(2) investigates compound object concepts as used in connection with fingerprints; and (3) presents a preliminary integration of very large fingerprint digital libraries.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "A new scan architecture for both low power testing and test volume compression under SOC test environment\n", "abstract": " A new scan architecture for both low power testing and test volume compression is proposed. For low power test requirements, only a subset of scan cells is loaded with test stimulus and captured with test responses by freezing the remaining scan cells according to the distribution of unspecified bits in the test cubes. In order to optimize the proposed process, a novel graph-based heuristic is proposed to partition the scan chains into several segments. For test volume reduction, a new LFSR reseeding based test compression scheme is proposed by reducing the maximum number of specified bits in the test cube set, s                         max, virtually. The performance of a conventional LFSR reseeding scheme highly depends on s                         max. In this paper, by using different clock phases between an LFSR and scan chains, and grouping the scan cells by a graph-based grouping heuristic, s\u00a0\u2026", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Decision selection and associated learning for computing all solutions in automatic test pattern generation (ATPG) and satisfiability\n", "abstract": " An all solutions automatic test pattern generation (ATPG) engine method uses a decision selection heuristic that makes use of the \u201cconnectivity of gates\u201d in the circuit in order to obtain a compact solution-set. The \u201csymmetry in search-states\u201d is analyzed using a \u201cSuccess-Driven Learning\u201d technique which is extended to prune conflict sub-spaces. A metric is used to determine the use of learnt information a priori, which information is stored and used efficiently during \u201csuccess driven learning\u201d.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Delay testing\n", "abstract": " Publisher SummaryThis chapter describes the common delay test approaches, with a focus on structural delay testing using two-pattern tests, where transitions are launched into the circuit and then the results are captured within the rated clock cycle time. The use of design-for-test features such as scan chains provides the controllability and observability to make structural testing practical. This is combined with a slow scan in of the initialization vector, then application of the test vector via launch-on-shift or launch-on-capture, and then capture of the results. With knowledge of the expected path delays, faster-than-at-speed testing can be used to increase detection of small delay defects. Traditional delay fault models, the transition fault, gate-delay fault, and path-delay fault, as well as newer models are also presented, including inline delay fault, propagation delay fault, segment delay fault, and defect-based delay\u00a0\u2026", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Mining global constraints with domain knowledge for improving bounded sequential equivalence checking\n", "abstract": " We present a novel technique on mining relationships in a sequential circuit to discover global constraints. We utilize domain knowledge to prune the search space for our mining process, via which the mined relations may often span several time frames, thus improving the deductibility of the problem instance. Experimental results demonstrate that the application of these global constraints to satisfiability (SAT)-based bounded sequential equivalence checking can achieve a one to two orders-of-magnitude speedup.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Q-PREZ: QBF evaluation using partition, resolution and elimination with ZBDDs\n", "abstract": " In recent years, there has been an increasing interest in quantified Boolean formula (QBF) evaluation, since several VLSI CAD problems can be formulated efficiently as QBF instances. Since the original resolution-based methods can suffer from space explosion, existing QBF solvers perform decision tree search using the Davis-Putnam Logemann and Loveland (DPLL) procedure. In this paper, we propose a new QBF solver, Q-PREZ, that overcomes the space explosion problem faced in resolution by using efficient data structures and algorithms, which in turn can outperform DPLL-based QBF solvers. We partition the CNF and store the clauses compactly in zero-suppressed binary decision diagrams (ZBDDs). Then, we introduce new and powerful operators to perform existential and universal quantification on the partitioned ZBDD clauses as resolution and elimination procedures. Our preliminary experimental\u00a0\u2026", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Efficient sequential ATPG based on partitioned finite-state-machine traversal\n", "abstract": " We present a new automatic test pattern generation algorithm for sequential circuits by traversing the partitioned state spaces. The new features include:(1) nondisjoint state groups are obtained such that two different state groups may have common flip-flops,(2) partial state transition graphs (STGs) are constructed at run time for each state group,(3) spectral information for state variables are extracted and the spectral infomation of the state variables helps to identify the behavior of the flip-flops in the, frequency domain. This information will help us to intelligently partition the state space. We focus only on the STGs for the flip-flops that are grouped together instead of building the STG for the entire circuit, and the ATPG tries to traverse all states and transitions within'each partial STG. By exercising states visited and arcs traversed, the vectors generated often lead to the detection of, hard faults. Since we limit a maximum size any state group can be, construction of partitioned STGs is feasible even for very large sequential circuits. Only logic simulation is needed in our ATPG; as a result, the execution time is greatly reduced while achieving high fault coverages compared with other test generators. For some large sequential circuits, highest fault coverages have been achieved.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Behavioral-level dft via formal operator testability measures\n", "abstract": " The focus of this research is on the testability analysis of the operators in the behavioral description prior to synthesis. The controllabilities of the inputs to an operator and the observabilities of the outputs of the operation are computed from the value ranges of the variables that serve as the inputs and outputs. Candidate hard-to-test operations are selected for testability enhancement based on the computed testability measures for all the involved operations in the behavioral description. While value ranges have been applied to compute variable testability measures, this paper addresses computation of the operator testability. Experimental results show that circuits synthesized using our technique exhibit higher testability than variable selection while keeping the area-performance overhead to a minimum.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Embedded core testing using genetic algorithms\n", "abstract": " Testing of embedded cores is very difficult in SOC (system-on-a-chip), since the core user may not know the gate level implementation of the core, and the controllability and observability of the core are limited by other cores and the user defined logic surrounding the core. One simple but expensive method to solve this problem is to add a wrapper around each core in the SOC, and shift in/out every bit at the core input, output, and possibly its internal state. An approach to remove part of these wrappers using controllability and observability evaluation via random inputs is proposed at the high level (i.e. no gate-level information needed). To achieve better results than the random input vectors, a genetic algorithm is used in this paper to justify the test patterns provided by the core designer. Several high level benchmarks are experimented and results show that with the test patterns generated by the genetic algorithm\u00a0\u2026", "num_citations": "8\n", "authors": ["1035"]}
{"title": "An integrated approach to behavioral-level design-for-testability using value-range and variable testability techniques\n", "abstract": " This research applies formal dataflow analysis and techniques to high-level DFT. Our proposed approach improves testability of the behavioral-level circuit description (such as in VHDL) based on propagation of the value ranges of variables through the circuit's Control-Data Flow Graph (CDFG). The resulting testable circuit is accomplished via controllability and observability computations from these value ranges and insertion of appropriate testability enhancements, while keeping the design area-performance overhead to a minimum.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Partial scan using multi-hop state reachability analysis\n", "abstract": " Sequential test generators fail to yield tests for some stuck-at-faults because they are unable to reach certain states necessary for exciting/propagating these target faults. Adding scan to the circuit increases reachability of these hard-to-reach and/or previously unreachable states. In this paper, we postulated that fewer scan flip-flops are needed to make these states reachable. The states necessary for detecting the hard-to-detect faults, when reached, will facilitate reaching other hard-to-reach states in one or more hops by the sequential test generator, resulting in significantly higher fault coverage. We collect information on the hard-to-reach, aborted, and easy states in our analysis. Results from our approach have indicated that higher fault coverage can be achieved with significantly fewer scan flip-flops for some circuits.", "num_citations": "8\n", "authors": ["1035"]}
{"title": "Signal domain based reachability analysis in rtl circuits\n", "abstract": " Register-transfer level (RTL) verification is a challenging problem for today's complex circuits. A sub-problem of verification is reachability of basic blocks or branches in the code. This paper proposes a novel analysis based on the domain of signal values in the RTL code to reason about the reachability of all branches without explicit circuit unrolling. This analysis takes into account all assignments, activating and preceding conditions in the code, to derive an assignment table that lists all the possible sequences of branches required to reach a target branch. In the process, it proves unreachable branches as well as provides guidance for reachable branches. This analysis resolves branches more efficiently compared to other methods for the ITC99 [1] benchmarks, especially for larger benchmarks containing previously unresolved branches.", "num_citations": "7\n", "authors": ["1035"]}
{"title": "Tacue: A timing-aware cuts enumeration algorithm for parallel synthesis\n", "abstract": " Achieving timing-closure has become one of the hardest tasks in logic synthesis due to the required stringent timing constraints in very large circuit designs. In this paper, we propose a novel synthesis paradigm to achieve timing-closure called Timing-Aware CUt Enumeration (TACUE). In TACUE, optimization is conducted through three aspects: (1) a new divide-and-conquer strategy is proposed that generates multiple sub-cuts on the critical parts of the circuit; (2) two cut enumeration strategies are proposed; (3) an efficient parallel synthesis framework is offered to reduce computation time. Experiments on large and difficult industrial benchmarks show the promise of the proposed method.", "num_citations": "7\n", "authors": ["1035"]}
{"title": "Utilizing gpgpus for design validation with a modified ant colony optimization\n", "abstract": " In this paper, we propose a novel parallel state justification tool, GACO, utilizing Ant Colony Optimization (ACO) on Graphical Processing Units (GPU). With the high degree of parallelism supported by the GPU, GACO is capable of launching a large number of artificial ants to search for the target state. A novel parallel simulation technique, utilizing partitioned navigation tracks as guides during the search, is proposed to achieve extremely high computation efficiency for state justification. We present the results on a GPU platform from NVIDIA (a GeForce GTX 285 graphics card) that demonstrate a speedup of up to 228\u00d7 compared to deterministic methods and a speedup of up to 40\u00d7 over previous state-of-the-art heuristic based serial tools.", "num_citations": "7\n", "authors": ["1035"]}
{"title": "Experiment and analysis services in a fingerprint digital library for collaborative research\n", "abstract": " Fingerprint management systems support millions of images and complicated but imperfect image identification algorithms. The forensic community requires a set of digital library services to support large image collections, execute identification algorithms, and analyze experiments that test identification algorithms in development. We present a model and prototype system capable of testing and analyzing fingerprinting algorithms in terms of identification performance based on matches of a known image to partial images, distortions of the images, and sub-regions of the images. These services are provided based on our framework for composing a set of services and a fingerprint image collection. The prototype will be useful in collaborations connecting several algorithm development efforts, and in composing an experimentation workflow. We also describe extensions of these services into other domains.", "num_citations": "7\n", "authors": ["1035"]}
{"title": "Design-for-test methodology for non-scan at-speed testing\n", "abstract": " While scan-based testing achieves a high fault coverage, it requires long test application times and substantial tester memory, in addition to the overhead in chip area and high test power. Functional testing, on the other hand, suffers from low coverage but can be applied at-speed. In this paper, we propose a novel three-step design-for-test (DFT) methodology which enhances the performance of functional testing to a great extent. In the first step we expand the state space of the circuit beyond functionally reachable space without scan or reset. These new states create conditions to activate/propagate fault effects that are otherwise hard-to-detect. Since structural correlation between D flip-flops (DFFs) of a circuit restricts its state space variation, the second step consists of partitioning the DFFs into different groups that helps to break such correlations. In the third step, we make internal hard-to-observe points in the\u00a0\u2026", "num_citations": "7\n", "authors": ["1035"]}
{"title": "Diagnostic test generation for silicon diagnosis with an incremental learning framework based on search state compatibility\n", "abstract": " Silicon Diagnosis is the process of locating potential defect sites (candidates) in a defective chip. These candidates are then used as an aid during physical failure analysis. It is desired that the cardinality of the candidate set returned by silicon diagnosis be as small as possible. To this end, effective test patterns that can distinguish many faults in the candidate set is critical. Generation of such diagnostic patterns is referred to as Automatic Diagnostic Test Generation (ADTG). In this paper, we propose an aggressive and efficient learning framework for such a diagnostic test generation engine. It allows us to identify and prune non-trivial redundant search states thereby allowing to easily solve hard to distinguish or hard to prove equivalent fault pairs. Further, we propose an incremental flow for ADTG, where the information learned during detection-oriented test generation is passed to and incrementally used by ADTG\u00a0\u2026", "num_citations": "7\n", "authors": ["1035"]}
{"title": "A fast approximation algorithm for MIN-ONE SAT\n", "abstract": " In this paper, we propose a novel approximation algorithm (RelaxSAT) for MIN-ONE SAT. RelaxSAT generates a set of constraints from the objective function to guide the search. The constraints are gradually relaxed to eliminate the conflicts with the original Boolean SAT formula until a solution is found. The experiments demonstrate that RelaxSAT is able to handle very large instances which cannot be solved by existing MIN-ONE algorithms; furthermore, very tight bounds on the solution were obtained with one to two orders of magnitude speedup.", "num_citations": "7\n", "authors": ["1035"]}
{"title": "Reducing power consumption by utilizing retransmission in short range wireless network\n", "abstract": " Low-power issues become critical when the networked computing devices depend mainly on non-AC power sources such as batteries or self-generated power. In the application of using short-range wireless networks to convey low-speed but important information, low power consumption becomes a top design consideration in wireless design. Traditionally, signal strength level is fixed during the entire time of communication to maintain a certain acceptable error rate. A new approach is proposed in this paper to explore the potential and practicality of decreasing the signal strength, thus reducing the total energy consumption. Although the error rate of one frame at the DLL layer increases, it is compensated by retransmission. And it is shown in this paper both theoretically and experimentally that the power consumed could be lowered, without incurring a higher frame loss rate. An adaptive power-adjusting algorithm\u00a0\u2026", "num_citations": "7\n", "authors": ["1035"]}
{"title": "Compaction-based test generation using state and fault information\n", "abstract": " Presents a new test generation procedure for sequential circuits using newly-traversed state information and newly-detected fault information obtained between successive iterations of vector compaction. Two types of technique are considered. One is based on which new states a sequential circuit is driven into, and the other is based on the new faults that are detected in the circuit between consecutive iterations of vector compaction. These data modify an otherwise random selection of vectors to bias vector sequences that cause the circuit to reach new states and cause previously undetected faults to be detected. The biased vectors, when used to extend the compacted test set, provide an intelligent selection of vectors. The extended test set is then compacted. Repeated applications of state and fault analysis, vector generation and compaction produce significantly high fault coverage using relatively small\u00a0\u2026", "num_citations": "7\n", "authors": ["1035"]}
{"title": "A control path aware metric for grading functional test vectors\n", "abstract": " Functional, at-speed test vectors play a critical role in targeting circuit defects not easily detected by traditional scan vectors. Fault simulating these vectors is generally computationally expensive since these tests are often applied at the system level without individual testing of modules within the hierarchy. As a result, fault grading techniques have become necessary to judge the quality of these test vectors. In this work, we propose a control path aware, rule-based statement coverage metric at the Register Transfer Level (RTL) to capture faulty behavior of statements along distinct operation paths within the circuit description. Experiments show that our metric has a strong correlation with gate level defect models across a variety of benchmarks, including the microprocessor or 1200 with a power management unit. Additionally, the metric shows high levels of scalability, providing up to two orders of magnitude\u00a0\u2026", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Fast stimuli generation for design validation of rtl circuits using binary particle swarm optimization\n", "abstract": " Generating high quality test sequences for complex digital circuits is known to be extremely challenging. In this paper, we introduce a test generation algorithm using Binary Particle Swarm Optimization (BPSO) to generate high-quality test sequences that achieve high branch coverage in short execution times for synthesizable RTL designs. Initially, a global search is conducted using Binary Particle Swarm Optimization which is later supported by a controlled graphical search method to reach target corner cases. The controlled search uses the control-flow graph to provide hints at critical points in the state space to reach hard corner cases. The fast convergence of BPSO allows the proposed method to deliver high coverage while generating short final test sequences. Substantial speedups over the state of the art methods have also been achieved.", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Branch guided functional test generation at the RTL\n", "abstract": " In this paper, we propose a functional test generation method for Register Transfer Level circuits. A popular metric for measuring the effectiveness of an RTL test suite is branch coverage. The challenge in exercising a hard-to-reach branch is in the understanding of the semantics of the design. Without a good guidance, hard branches might require unnecessarily long test sequences or missed altogether. In our method, we extract such semantics from the circuit using a lightweight static analysis of the code in order to guide the search. Experimental results show that a high level of coverage can be obtained for several benchmark circuits, while reducing the test vector lengths by up to two orders of magnitude in some circuits.", "num_citations": "6\n", "authors": ["1035"]}
{"title": "On the uniqueness of fingerprints via mining of statistically rare features\n", "abstract": " A new method for automatically identifying rare features in fingerprints based on a combination of level 1 features and minutia-based triangular descriptors is described. A feature is considered rare if it is statistically uncommon; for example, such a rare feature should be unique among N>1000 randomly sampled prints. A fingerprint feature that is rare has higher discriminatory power when it is identified in a print (latent or otherwise), and multiple rare features in a single print can increase discriminatory power dramatically. In the case of latent matching, such information can be significant for reaching a decision. The new approach was tested experimentally using the NIST SD-27 database and an FBI database of 11,036 unique fingerprints. The results indicated that every randomly selected fingerprint from the composite database has a small set of highly distinctive statistically rare features, some of with occurrence of\u00a0\u2026", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Novel sat-based invariant-directed low-power synthesis\n", "abstract": " Dynamic power consumption is a critical concern in the design of both high performance and low-power circuits. Clock-gating is one of the most efficient and prominent approaches to reduce dynamic power. In this paper, (1) we propose the first scalable SAT-based approaches for Observability Dont Care (ODC) based clock gating; (2) we intelligently choose those inductive invariants candidates such that their validation will benefit clock-gating-based low-power design. Our approach shows an average 23.2% reduction in dynamic power with an average 9.5% increase in area.", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Ensuring trust of third-party hardware design with constrained sequential equivalence checking\n", "abstract": " Globalization of semiconductor design and manufacturing has led to a concern of trust in the final product. The effect of any modifications made by an adversary can be catastrophic in critical applications. Because of the stealthy nature of such insertions, it is extremely difficult to detect them using traditional testing and verification methods. In this paper, we propose a novel technique for detection of malicious alteration(s) in a third party soft intellectual property (IP) using a clever combination of sequential equivalence checking (SEC) and test generation. The use of powerful inductive invariants can prune a large illegal state space, and test generation helps to provide a sensitization path for nodes of interest. Results for a set of hard-to-verify designs show that our method can either ensure that the suspect design is free from the functional effect of any malicious change(s) or return a small group of most likely malicious\u00a0\u2026", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Kiss the scan goodbye: A non-scan architecture for high coverage, low test data volume and low test application time\n", "abstract": " Scan-based DFT is the de-facto industrial practice for testing integrated circuits (ICs). Variations in the scan architecture to improve test metrics have been the primary focus in recent years. In this paper, we propose a new nonscan DFT in which a subset of the circuit flip-flops are made directly loadable from the primary inputs and another subset of flip-flops are made observable at the output via a state compactor. In this architecture, multiple flip-flops may share the same primary input in the loading mode. A load-enable pin is added to distinguish the direct-loading mode from the functional mode. With a modest area overhead, this architecture offers several attractive features, including (1) at-speed testing, which eliminates the need for scan-shifting and would thus capture delay-related defects, (2) low test data volume and test application time, as we no longer need to store all the scan and response data, (3) high\u00a0\u2026", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Children Online Privacy: Issues with Parental Awareness and Control\n", "abstract": " As children increasingly use the Internet, there have been mounting concerns about their privacy online. As a result, the US Congress enacted the Children\u2019s Online Privacy Protection Act (COPPA) in 1998 to prohibit web sites from collecting information from children under 13 years of age without veri\ufb01able parental consent. Unfortunately, few technologies and tools are available for parents to provide this consent. Further, our research demonstrates that very few parents are even aware of the laws and technologies available to protect their children\u2019s privacy online. In addition to exploring parental awareness of laws and technologies associated with protecting children\u2019s privacy online, this research explored factors that would in\ufb02uence the usage of technologies and techniques for parental control, using a focus group research methodology. The results of the content analysis of the data are used to propose an emergent framework of factors that impact use of privacy protection tools and techniques by parents. Three categories of factors are identi\ufb01ed: awareness factors (laws and risks), behavioral adoption", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Boosting sat solver performance via a new hybrid approach\n", "abstract": " Due to the widespread demands for efficient SAT solvers in Electronic Design Automation applications, methods to boost the performance of the SAT solver are highly desired. We propose a Hybrid Solution to boost SAT solver performance in this paper, via an integration of local and DPLL-based search approaches. A local search is used to identify a subset of clauses from the original formula to be passed to a DPLL SAT solver incrementally until all the clauses have been passed. In addition, the solution obtained by the DPLL solver on the subset of clauses is fed back to the local search solver to jump over any locally optimal points. The proposed solution is highly portable to the existing SAT solvers. For satisfiable instances, up to an order of magnitude speedup was obtained via the proposed hybrid solver. For unsatisfiable instances, the speedup was smaller due to the overhead.", "num_citations": "6\n", "authors": ["1035"]}
{"title": "On Providing Automatic Parental Consent over Information Collection from Children.\n", "abstract": " Children\u2019s privacy has become critical with the increasing use of the Internet for commercial purposes and corresponding increase in requests for information. 65% of children between the ages of 10 and 13 use the Internet for casual web surfing, chatting, games, schoolwork, e-mail, interactive learning, and other applications. Often, websites hosting these activities ask for personal information such as name, e-mail, street address, and phone number. The Children\u2019s Online Privacy Protection Act (COPPA) of 1998 was enacted in reaction to the widespread collection of information from children and subsequent abuses identified by the Federal Trade Commission (FTC). COPPA is aimed at protecting a child\u2019s privacy by requiring parental consent before collecting information from children under 13.In this paper, we describe an automated tool for protecting child privacy called Parental Online Consent for Kids Electronic Transaction (or POCKET). The POCKET framework is a novel, technically feasible and legally sound solution to automatically enforce COPPA. Parents answer a simple questionnaire regarding their privacy requirements and the POCKET user agent automatically converts it into a privacy preferences file. These preferences are enforced when a child uses the Internet. Only websites that adhere to the preferences can receive the child\u2019s information, while websites whose policies do not match are blocked. A merchant-specific privacy information package and a signed digital agreement are uploaded to the qualified merchant from the client (child\u2019s machine). POCKET framework incorporates a secure", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Increasing the deducibility in CNF instances for efficient SAT-based bounded model checking\n", "abstract": " In this paper, we propose low-cost static deduction techniques by combining binary resolution and static logic implications to efficiently extract invariant relations from a gate-level netlist. We show that processing our techniques across the circuit nodes helps us to learn highly nontrivial relations. All the relations learned in a user-defined finite window are then quickly replicated over the entire bound for BMC. These powerful relations, when added as new constraint clauses to the original formula, help to significantly increase the deductive power for the SAT engine, thereby pruning a larger portion of the search space. Experimental results on ISCAS89 and ITC99 benchmarks show that more than an order of magnitude performance improvement can be obtained using the proposed learning techniques.", "num_citations": "6\n", "authors": ["1035"]}
{"title": "VERISEC: Verifying equivalence of sequential circuits using SAT\n", "abstract": " In this paper we propose a framework to verify equivalence of sequential circuits using Boolean satisfiability (SAT). We tackle a problem that is harder than the traditional sequential hardware equivalence; specifically, we address the uninvestigated problem of verifying delay replaceability as stated in V. Singhal et al. (2001) of two sequential designs. This notion of sequential equivalence does not make any assumptions either about the design-environment or about the design's steady state behavior. Thus, verifying delay replaceability is considered as hard as verifying safe replaceability according to V. Singhal et al. (2001) of sequential circuits (conjectured as EXPSPACE complete). Our SAT-based framework has the following salient features: (a) a methodology to inductively prove equivalence (delay replaceability) of sequential circuits with no assumptions about any initial state; (b) a scheme to include sequential\u00a0\u2026", "num_citations": "6\n", "authors": ["1035"]}
{"title": "A formal framework for modeling and analysis of system-level dynamic power management\n", "abstract": " Recent advances in dynamic power management (DPM) techniques have resulted in designs that support a rich set of power management options, both at the hardware and software levels. This has resulted in an explosion of the design space when analyzing the system-level tradeoffs of candidate DPM strategy designs. This paper proposes a design space exploration methodology based on a high-level, multi-layered modeling framework that facilitates rapid estimation of system-wide energy by providing the designer with a global view of the system. The framework is based on the extended finite state machine formalism and abstracts the component power modes, the operating environment and the DPM architecture into interacting, concurrent layers within a single, unified model. The modeling framework is coupled with a symbolic simulation engine to allow for rapid traversal of the large design space. We first\u00a0\u2026", "num_citations": "6\n", "authors": ["1035"]}
{"title": "On the evaluation of arbitrary defect coverage of test sets\n", "abstract": " Efficient methods to evaluate the quality of a test set in terms of its coverage of arbitrary defects in a circuit are presented. Our techniques rapidly estimate arbitrary defect coverage because they are independent of specific, physical, fault models. We overcome the potentially explosive computational requirements associated with considering all possible defects by implicitly evaluating multiple faults (of all types) simultaneously and by exploiting the local nature of defects. Our experiments show that a strong correlation exists between stuck-at fault coverage and defects whose behavior is independent of the input vectors. Our techniques are capable of identifying regions in the circuit where defects may escape the test set. We also demonstrate how the chances of detection of an arbitrary defect by a test set vary when a single stuck-at-fault within the vicinity of that defect is detected multiple times by the test set.", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Sequential circuit test generation using genetic techniques\n", "abstract": " The majority of the time spent by automatic test generators for sequential circuits is used to find test sequences for hard-to-detect faults. These faults are either hard to excite, hard to propagate, or both. Although simulation-based test generators avoid the complexity of backtracking by processing in the forward direction only, they often fall short when targeting the hard faults. The problem faced by the genetic algorithm (GA)-based approaches is mainly the lack of knowledge of the sequences necessary to activate the hard faults or propagate the fault effects.", "num_citations": "6\n", "authors": ["1035"]}
{"title": "Controlled natural language framework for generating assertions from hardware specifications\n", "abstract": " In this paper, we present a controlled natural language (CNL) framework for automatic processing and generation of assertions from hardware design specification. Current CNL systems have limitations in mapping differently worded sentences with the same meaning to the same logic structures. We aim to mitigate this limitation by developing a dependency grammar based CNL where the constructed parse tree does not follow strict surface-structure dependencies and instead extract additional relationship based on semantic information that is embedded in the grammar. In addition, current translation schemes for creating executable assertions from hardware design specifications do not provide feedback on wrongly or ambiguously written input sentences. Our natural language understanding algorithm is guided by the dependencies in the parse tree and has the capability to offer useful feedback for sentences\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Fast multi-level test generation at the rtl\n", "abstract": " Functional, at-speed vectors continue to provide added value to the testing community as circuit complexity rises. Complex defects may escape traditional scan vectors and thus often require at-speed patterns. However, generation of functional/sequential vectors is an extremely challenging problem. Previous methods rely on formal models of the RTL or calls to gate level ATPG, both of which are computationally expensive, limiting the efficacy of gains made in RTL stimuli generation. In this work, we present an efficient engine for the generation of high quality functional tests at the RTL which are effective for both validation and at-speed defect detection. The proposed method utilizes a rule based, behavioral coverage metric to accurately assess the activation of circuit modules by the generated stimuli. Based on this metric, we are able to effectively generate functional test vectors at the RTL, without additional gate\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "A diagnosis-friendly LBIST architecture with property checking\n", "abstract": " LBIST is a popular technique for on-chip at-speed testing of digital circuits. In LBIST, output compression techniques are used to reduce hardware overhead of storing test responses but such compression makes diagnosis extremely challenging as the failing vector and the output to which the fault effect propagated are unknown. We propose a new property checking based LBIST architecture which monitors certain properties in the output responses. If any property is violated, the failing vector and property number are stored for diagnosis. The proposed architecture improves diagnosability considerably with minimal hardware overhead. Experimental results show that the diagnostic resolution achieved by our architecture is comparable to the diagnostic resolution achieved in a non-BIST setup for many circuits.", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Anti-counterfeit integrated circuits using fuse and tamper-resistant time-stamp circuitry\n", "abstract": " Counterfeit Integrated Circuits (ICs) have become an important security issue in recent years, in which counterfeit ICs that perform incorrectly or sub-par to the expected can lead to catastrophic consequences in safety and/or mission-critical applications, in addition to the tremendous economic toll they incur to the semiconductor industry. In this paper, we propose two novel methods to validate the authenticity of ICs. First, a fuse with a charge pump is proposed to serve as a \u201cseal\u201d for the IC, in which any functional use will break the seal, and the broken seal is extremely hard to replace. Second, a novel time-stamp is proposed that can provide the date at which the IC was manufactured. The time-stamp circuitry is constructed using a Linear-Feedback Shift-Register (LFSR) such that any small change to the circuit would result in an entirely different date either in a distant past or future, beyond the lifetime of a typical IC\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Selecting critical implications with set-covering formulation for sat-based bounded model checking\n", "abstract": " The effectiveness of SAT-based Bounded Model Checking (BMC) critically relies on the deductive power of the BMC instance. Although implication relationships have been used to help SAT solver to make more deductions, frequently an excessive number of implications has been used. Too many such implications can result in a large number of clauses that could potentially degrade the underlying SAT solver performance. In this paper, we first propose a framework for a parallel deduction engine to reduce implication learning time. Secondly, we propose a novel set-cover technique for optimal selection of constraint clauses. This technique depends on maximizing the number of literals that can be deduced by the SAT solver during the BCP (Boolean Constraint Propagation) operation. Our parallel deduction engine can achieve a 5.7\u00d7 speedup on a 36-core machine. In addition, by selecting only those critical\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Continuous authentication in computers\n", "abstract": " In the Internet age, identity theft is a major security issue because contemporary authentication systems lack adequate mechanisms to detect and prevent masquerading. This chapter discusses the current authentication systems and identifies their limitations in combating masquerading attacks. Analysis of existing authentication systems reveals the factors to be considered and the steps necessary in building a good continuous authentication system. As an example, we present a continual, non-intrusive, fast and easily deployable user re-authentication system based on behavioral biometrics. It employs a novel heuristic based on keyboard and mouse attributes to decipher the behavioral pattern of each individual user on the system. In the re-authentication process, the current behavior of user is compared with stored \u201cexpected\u201d behavior. If user behavior deviates from expected behavior beyond an allowed\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "An efficient 2-phase strategy to achieve high branch coverage\n", "abstract": " We present a new 2-phase symbolic execution driven strategy that achieves high branch coverage in software quickly. Phase 1 follows a greedy approach that quickly covers as many branches as possible by exploring each branch through its corresponding shortest path prefix. Phase 2 covers the remaining branches that are left uncovered if the shortest path to the branch was infeasible. In Phase 1, a basic conflict-driven learning is used to skip all the paths that may have any of the earlier encountered conflicting conditions, while in Phase 2, a more intelligent conflict-driven learning is used to skip regions that do not have a feasible path to any unexplored branch. This results in considerable reduction in unnecessary SMT solver calls. Experimental results show that significant speedup can be achieved, effectively reducing the time to detect a bug and providing higher branch coverage for a fixed time-out period\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Method and apparatus for delay fault coverage enhancement\n", "abstract": " A hybrid clocking scheme for simultaneously detecting a b-cycle path-delay fault in a b-cycle (false) path and a c-cycle path-delay fault in a c-cycle (false) path using at least n+ 1 at-speed clock pulses during a capture operation in a clock domain in a scan design or a scan-based BIST design, where 1<= b<= c<= n. The scan design or BIST design includes multiple scan chains, each scan chain comprising multiple scan cells coupled in series. The design includes one or more clock domains each running at its intended operating frequency or at-speed. The hybrid clocking scheme comprises at least one at-speed shift clock pulse or one at-speed capture clock pulse immediately followed by at least two at-speed capture clock pulses during the capture operation to simultaneously detect the b-cycle path-delay fault and the c-cycle path-delay fault within the clock domain.", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Sufficiency-based Filtering of Invariants for Sequential Equivalence Checking\n", "abstract": " Inductive Invariants play critical roles in restricting the search space during Sequential Equivalence Checking (SEC), especially for those instances with few internal equivalent points. For large circuits, there can be too many potential invariants relating signals between the two circuits, thereby requiring much time to prove. However, we observe that a large portion of the potential invariants may not even contribute to equivalence checking. Moreover, equivalence checking can be significantly helped if there exists a method to check if a subset of potential invariants would be sufficient (e.g., whether two-nodes are enough or multi-nodes are also needed) prior to the verification step. In this paper, we address these problems and propose a sufficiency-based approach to identify useful invariants out of the initial potential invariants for SEC. Experimental results show that our approach can either demonstrate insufficiency\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "SAT-based state justification with adaptive mining of invariants\n", "abstract": " We present a new approach to intelligently mine three types of invariants from a sequential circuit to significantly improve SAT-based state justification. We adaptively generate mining databases targeting on the hard-to-reach corner-case states, from which global invariants, target state related invariants, and observability-don't-care extended invariants are mined. Each mined invariant involves two or more signals that span across multiple time-frames, which capture the knowledge of the state spaces related to a target state. These invariants are then checked for their validity, and they can significantly increase the deductive power of the instance by pruning a larger portion of the search space. Experimental results show that more than an order of magnitude performance improvement can be obtained when justifying hard-to-justify states.", "num_citations": "5\n", "authors": ["1035"]}
{"title": "A new testability guided abstraction to solving bit-vector formula\n", "abstract": " We present a new abstraction approach based on the concept of the under-and over-approximation to efficiently solve bit-vector formulae generated from software verification instances, which include intensive control structures. Our proposed approach applies two common testability metrics: controllability metric (CM) and observability metric (OM) for guiding the abstraction refinement procedure. We implement the under-approximation by enforcing constant constraints on a small set of single-bit variables that control the branch selection of some ITE nodes. Subsequently, each constructed under-approximate model includes only a subset of paths in the formula. We use CM and OM to build such models so that a counterexample can be obtained with little effort. If the under-approximate model is unsatisfiable, an over-approximate abstraction is obtained by refining along the paths included in the model. This is\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Quality-driven proactive computation elimination for power-aware multimedia processing\n", "abstract": " We present a novel, quality-driven, architectural-level approach that trades-off the output quality to enable power-aware processing of multimedia streams. The error tolerance of multimedia data is exploited to selectively eliminate computation while maintaining a specified output quality. We construct relaxed, synthesized power macro-models for power-hungry units to predict the cycle-accurate power consumption of the input stream on the fly. The macro-models, together with an effective quality model, are integrated into a programmable architecture that allows both power savings and quality to be dynamically tuned with the available battery-life. In a case study, power monitors are integrated with functional units of the IDCT module of a MPEG-2 decoder. Experiments indicate that, for a moderate power monitor energy overhead of 5%, power savings of 72% in the functional units can be achieved resulting in an\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Success-driven learning in ATPG for preimage computation\n", "abstract": " Unbounded model checking fundamentally requires either image or preimage calculations. We introduce a hybrid method for making preimage calculations using ATPG and binary decision diagrams (BDDs). Experimental results show that the proposed method achieves a speedup of two to three orders of magnitude over pure ATPG methods.", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Decision selection and learning for an'all-solutions ATPG engine'\n", "abstract": " 'All-solutions ATPG' based methods have found applications in model checking sequential circuits, and they can also improve the defect coverage of a test-suite, by generating distinct multiple-detect patterns. Conventional decision selection heuristics and learning techniques for an ATPG engine were originally developed to 'quickly' find any available (single) solution. Such decision selection heuristics may not be the best for an 'all-solutions ATPG' engine, where all the solutions need to be found. In this paper, we explore new techniques to guide an 'all-solutions ATPG engine'. We first present a new decision selection heuristic that makes use of the 'connectivity of gates' in the circuit in order to obtain a compact solution-set. Next, we analyze the 'symmetry in search-states' that was exploited in 'success-driven learning' and extend it to prune conflict sub-spaces as well. Finally, we propose a new metric that\u00a0\u2026", "num_citations": "5\n", "authors": ["1035"]}
{"title": "Hardware-in-the-loop model-less diagnostic test generation\n", "abstract": " Iterative scan diagnosis is often needed for both the first silicon and the hard-to-diagnose chips. The chips in question are extracted from wafers and re-tested on a debug platform to arrive at a reasonable number of probable defect candidates that can be physically analyzed. This requires a large setup time and multiple iterations of deterministic diagnostic test pattern generation and application. In every iteration, offline software tools are used to diagnose observed failures and generate the needed new patterns to prune the list of defect candidates. In this paper, we propose an online approach for generating additional diagnostic patterns for the hard-to-diagnose chips without moving them to the debug platform. We generate these patterns directly on the tester through a fault model independent hardware-in-the-loop evolutionary algorithm. This algorithm is guided by a lightweight fitness metric that is solely based\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "Set-cover-based critical implications selection to improvesat-based bounded model checking\n", "abstract": " The effectiveness of Satisfiability (SAT)-based Bounded Model Checking (BMC) critically relies on the deductive power of the BMC instance. Although implication relationships have been used to help SAT solver to make more deductions, too many such implications would result in a large number of clauses and potentially degrade the underlying SAT solver performance. In this paper, we have formulated clause selection as a set-cover problem. Secondly, we have proposed a novel greedy strategy for optimal selection of static logic clauses. This technique maximizes the number of literals that can be deduced by the SAT solver during the Boolean Constraint Propagation (BCP) operation. Our strategy improves BMC by 1.74 x against the case where all extended implications were added to the BMC instance. Compared with the original BMC without any implications, up to 55.32 x speedup can be achieved.", "num_citations": "4\n", "authors": ["1035"]}
{"title": "LFSR seed computation and reduction using SMT-based fault-chaining\n", "abstract": " We propose a new method to derive a small number of LFSR seeds for Logic BIST to cover all detectable faults as a first-order satisfiability problem involving extended theories. We use an SMT (Satisfiability Modulo Theories) formulation to efficiently combine the tasks of test-generation and seed-computation. We make use of this formulation in an iterative seed-reduction flow which enables the \u201cchaining\u201d of hard-to-test faults using very few seeds. Experimental results demonstrate that up to 79% reduction in the number of seeds can be achieved.", "num_citations": "4\n", "authors": ["1035"]}
{"title": "A novel concurrent cache-friendly binary decision diagram construction for multi-core platforms\n", "abstract": " Currently, BDD packages such as CUDD depend on chained hash tables. Although they are efficient in terms of memory usage, they exhibit poor cache performance due to dynamic allocation and indirections of data. Moreover, they are less appealing for concurrent environments as they need thread-safe garbage collectors. Furthermore, to take advantage of the benefits from multi-core platforms, it is best to re-engineer the underlying algorithms, such as whether traditional depth-first search (DFS) construction, breadth-first search (BFS) construction, or a hybrid BFS with DFS would be best. In this paper, we introduce a novel BDD package friendly to multicore platforms that builds on a number of heuristics. Firstly, we re-structure the Unique Table (UT) using a concurrency-friendly Hopscotch hashing to improve caching performance. Secondly, we re-engineer the BFS Queues with hopscotch hashing. Thirdly, we\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "Sequential equivalence checking of hard instances with targeted inductive invariants and efficient filtering strategies\n", "abstract": " We propose two approaches to significantly boost the power of sequential equivalence checking: (1) In contrast with invariants involving only two or three signals, we introduce a novel multisignal invariant generation technique that is scalable to large circuits; (2) We utilize static and dynamic filters to reduce the number of potential inductive invariants that need to be proved to further reduce the computational cost. Experimental results show that the proposed method can handle hard SEC instances with little or no internal equivalences that conventional methods fail; in addition, one to three orders of magnitude speedup have been achieved for many instances.", "num_citations": "4\n", "authors": ["1035"]}
{"title": "Reducing descriptor measurement error through Bayesian estimation of fingerprint minutia location and direction\n", "abstract": " Fingerprint-based recognition relies on the matching of features derived from the ridges and valleys of the friction ridge surface. When a large quantity of good-quality features are available, identification can be made with a high level of confidence. When portions of the fingerprint image are of lower quality, accuracy will suffer in the feature extraction and subsequent matching steps. This study is concerned with the correction of descriptor errors in minutiae (ridge endings and bifurcations), and with evaluation of the effect of feature accuracy on matching performance. The approach applies Bayesian filtering to refine minutia location and direction descriptors, using Sequential Monte Carlo approximation of a joint probability distribution near each minutia. The distribution approximates the location and orientation of the minutia, given measurements on local greyscale information, from which an expectation can be\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "DFT+ DFD: An integrated method for design for testability and diagnosability\n", "abstract": " While conventional test point insertions commonly used in design for testability can improve fault coverage, the test points selected may not necessarily be the best candidates to aid silicon diagnosis. In this paper, test point insertions are conducted with the aim to detect more faults and also synergistically distinguish currently indistinguishable fault-pairs. We achieve this by identifying those points in the circuit, which are not only hard-to-test but also lie on distinguishable frontiers, as Testability-Diagnosability (TD) points. To this end, we propose a novel low-cost metric to identify such TD points. Further, we propose a new DFT + DFD architecture, which adds just one pin (to identify test/functional mode) and small additional combinational logic to the circuit under test. Our experiments indicate that the proposed architecture can distinguish 4\u00d7 more previously indistinguishable fault-pairs than existing DFT architectures\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "A new simulation-based property checking algorithm based on partitioned alternative search space traversal\n", "abstract": " We present a new logic-simulation-based algorithm on verifying safety properties of large sequential hardware designs. This algorithm explores the search space defined by partitioned internal circuit nodes. Two powerful features are proposed to increase the effectiveness during search space exploration and counterexample generation for verifying safety properties. These include 1) new search space constituted by internal nodes instead of state variables and 2) static learning on multiple nodes to further enlarge the target. These two features are integrated with the following techniques during our simulation: incorporation of a BCP (Boolean constraint propagation) engine for multiple nodes implication and multiple-time-frame GA (genetic algorithm) search. Because only logic simulation is needed in our algorithm, the computational effort is low. Experimental results on large benchmark circuits have shown that\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "State set management for sat-based unbounded model checking\n", "abstract": " In recent years, Boolean satisfiability (SAT) has been shown to hold potential for unbounded model checking (UMC). The success of SAT-based UMC largely relies on (i) the SAT solver efficiency, (it) solution cube enlargement, and (Hi) state-set management. In this paper, we propose a simple, yet efficient, clause conversion technique to account for the state set obtained by SAT-based UMC. Our state set is stored in a zero-suppressed binary decision diagram (ZBDD), and the shared structures in the ZBDD are exploited to aggressively avoid repeated manipulation of common subsets in the state-set. The resulting number of clauses, generated for the state set, now depends on the number of nodes in the ZBDD, rather than the number of solutions found. We integrated the proposed techniques in an unbounded model checking framework that uses a pure SAT solver. The experimental results show that we can attain\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "CNF Formula Simplification Using Implication Reasoning\n", "abstract": " We propose a novel preprocessing technique that helps to significantly simplify a CNF instance, such that the resulting formula is easier for any SAT-solver to solve. The core of this simplification centers on a suite of lemmas and theorems derived from nontrivial Boolean reasoning. These theorems help us to deduce powerful unary and binary clauses which aid in the identification of necessary assignments, equivalent signals, complementary signals and other implication relationships among the CNF variables. The nontrivial clauses, when added to the original CNF database, subsequently simplify the CNF formula. We illustrate through experimental results that the CNF formula simplification obtained using our tool outperforms the simplification obtained using the recent preprocessors namely Hypre [F. Bacchus et al., (2003)] and NIVER [S. Subbarayan et al. (2004)]. Also, considerable savings in computation time\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "Identifying untestable transition faults in latch based designs with multiple clocks\n", "abstract": " This work presents a novel technique to identify functionally untestable transition faults in latch based designs with multiple clock domains, bringing to light unaddressed issues related to untestable fault identification in such design environments. We also introduce and provide a solution to a new variant of un-testability analysis wherein \"architectural constraints'' are absorbed during the analysis. We give our tool the capability of handling transition faults resulting from defects of varying sizes, and evaluate our tool for various industrial circuits. The proposed algorithm is compared with a state-of-the-art sequential ATPG tool, and our method has shown much better performance both in the context of scan ATPG and functional test development. Results indicate that the proposed technique identifies considerably more untestable transition faults than those that can be deduced from the knowledge of untestable stuck-at\u00a0\u2026", "num_citations": "4\n", "authors": ["1035"]}
{"title": "State relaxation based subsequence removal method for fast static compaction in sequential circuits\n", "abstract": " A method for fast static compaction in sequential circuits with finite output states by removing subsequences of test vectors from a vector test set. The method has the following steps:(1) relaxing the output states of the sequential circuits;(2) identifying a candidate subsequence of test vectors from the vector test set for removal;(3) temporarily removing the candidate subsequence of test vectors from the vector test set;(4) performing fault simulation on remaining test vectors from the vector test set;(5) examining fault simulation results against a set of removal criteria;(6) permanently removing the temporarily removed candidate subsequence if said set of removal criteria are met;(7) replacing the temporarily removed candidate subsequence if said set of removal criteria are not met; and (8) repeating steps (1) through (7) until all candidate subsequences of test vectors have been identified.", "num_citations": "4\n", "authors": ["1035"]}
{"title": "Formal value-range and variable testability techniques for high-level design-for-testability\n", "abstract": " This research applies formal dataflow analysis and techniques to high-level DFT. Our proposed approach improves testability of the behavioral-level circuit description (such as in VHDL) based on propagation of the value ranges of variables through the circuit's Control-Data Flow Graph (CDFG). The resulting testable circuit is accomplished via controllability and observability computations from these value ranges and insertion of appropriate testability enhancements, while keeping the design area-performance overhead to a minimum.", "num_citations": "4\n", "authors": ["1035"]}
{"title": "Hardware IP Trust\n", "abstract": " The chapter focuses on the trust issues in hardware intellectual property (IP) blocks \u2013 specially the scope of Trojan insertion in modern IPs, types of IP-level Trojan, and challenges in detecting them. The chapter starts with a discussion on the political and socioeconomic factors that made such a possibility, a reality. It also contains a brief history of the early developments in Trojan research done by DARPA in 2000s. A detailed taxonomy of Trojans and their operational behavior is presented to illustrate the wide range of possibilities associated with the deployment and action of a Trojan. A general discussion on detection techniques follows. Toward the latter half of the chapter two detection techniques specifically aimed for detecting Trojans at the IP level are discussed in detail. One of them is a \u201cprevention technique,\u201d while the other is a \u201cdetection technique.\u201d", "num_citations": "3\n", "authors": ["1035"]}
{"title": "A Test Pattern Quality Metric for Diagnosis of Multiple Stuck-at and Transition faults\n", "abstract": " The test patterns computed for detecting the manufacturing defects in the electronic circuits are generally insufficient for diagnosis. The test set compaction and failure log truncation lead to loss of critical failure observations that diagnosis might depend on. In this context, it is beneficial to know the diagnostic usefulness of failures so that we can log the more useful failures instead of logging the initial failures. In this paper, we evaluate three metrics to gauge such diagnostic usefulness in real-time by observing the circuit responses on the tester. We implement a pattern selection framework for failure logging and compare the results with those achieved by logging the initial failures. Using one of our proposed metrics, we were able to improve the diagnosis quality for a significant number of faulty instances of ISCAS'89 and IWLS'05 benchmarks having 1-7 inserted stuck-at and transition faults.", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Dynamic partitioning strategy to enhance symbolic execution\n", "abstract": " Software testing is a fundamental part of the software development process. In the context of embedded-software applications, testing can find defects which cause unprecedented risks. The path explosion problem often necessitates one to consider an extremely large number of paths in order to reach a specific target. Symbolic execution can reduce this cost by using symbolic values and heuristic exploration strategies. Although various exploration strategies have been proposed in the past, the number of SMT solver calls for reaching a target is still large, resulting in long execution times for programs containing many paths. In this paper, we present a dynamic partitioning strategy in order to mitigate this problem, consequently reducing unnecessary SMT solver calls as well. Using this strategy on SSA-applied code, the code sections are analyzed in a nonconsecutive order guided by data dependency metrics within\u00a0\u2026", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Test generation for circuits with embedded memories using SMT\n", "abstract": " One of the important challenges in testing modern SOCs is the presence of small embedded memories. These memories are too small to employ memory BIST. Also, making these embedded memories scan-able or employing MBIST would increase the area overhead and/or test application time.", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Temporal analysis of fingerprint impressions\n", "abstract": " Fingerprint analysis has been performed on static images since the establishment of friction-ridge patterns as a reliable human trait for identification. The image of a fingerprint can vary considerably, however, as a result of such factors as skin elasticity and pressure on the imaging surface. With today's live-scan imaging devices, changes in appearance are notable as a finger presses against the scanning during acquisition. During the brief imaging period, fingerprint ridges stretch and compress, while some portions of the fingertip display varying levels of visibility and quality. Imaging systems select a single static image frame, thereby ignoring information that is absent from a particular image but present in others. This paper presents a novel approach that employs temporal analysis to a sequence of images captured during enrollment. Minutiae are extracted and tracked throughout the sequence to build a\u00a0\u2026", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Robust feature extraction in fingerprint images using ridge model tracking\n", "abstract": " This paper addresses the problem of feature extraction from low-quality regions in fingerprint images. Features such as minutiae are difficult to detect automatically when high levels of noise are present, as in wet, dry, or latent prints. The approach presented here is a novel application of Bayesian filtering to the problem of ridge tracing. A friction ridge is followed by recursively estimating posterior distributions representing the direction of each new step along the ridge. This approach benefits from previous state information when in an area that exhibits low ridge clarity, causing ridge-flow estimation to be unreliable. The new technique has been tested experimentally using a database of 880 grayscale fingerprint images with varying quality. The ability of the proposed method to detect features more reliably is confirmed by a reduction in Equal Error Rate of 2.1% and 2.5% over two traditional methods. In addition, the\u00a0\u2026", "num_citations": "3\n", "authors": ["1035"]}
{"title": "RAG: an efficient reliability analysis of logic circuits on graphics processing units\n", "abstract": " In this paper, we present RAG, an efficient Reliability Analysis tool based on Graphics processing units (GPU). RAG is a fault injection based parallel stochastic simulator implemented on a state-of-the-art GPU. A two-stage simulation framework is proposed to exploit the high computation efficiency of GPUs. Experimental results demonstrate the accuracy and performance of RAG. An average speedup of 412\u00d7 and 198\u00d7 is achieved compared to two state-of-the-art CPU-based approaches for reliability analysis.", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Implicit Search-Space Aware Cofactor Expansion: A Novel Preimage Computation Technique\n", "abstract": " In this paper, we introduce a novel preimage computation technique that directly computes the circuit cofactors without an explicit search for any satisfiable solution. We use an implicit search on the primary inputs of a sequential circuit to compute all the circuit cofactors for the target preimage. In order to alleviate the computational cost, aggressive learning techniques are introduced that reason on the search-states by analyzing the relations among circuit cofactors. Such analysis generates search-state induced clauses that directly help to prune the cofactor space during preimage computation and to perform non-chronological backtracking. Experimental results show that a significant improvement can be achieved in both performance and capacity as compared to the existing techniques.", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Using symbolic simulation and weakening abstraction for formal verification of embedded software\n", "abstract": " ION FOR FORMAL VERIFICATION OF EMBEDDED SOFTWARE Nannan He and Michael S. Hsiao Bradley Department of Electrical and Computer Engineering Virginia Tech, Blacksburg, VA, 24060, USA ABSTRACT Model checking is an important formal verification technique. Bounded Model Checking may also be applied to software programs, and it relies on a Boolean satisfiability problem (SAT) solver to explore the state space of the finite state program in C. It considers each variable as a bit-vector and transforms the unwound program traces into a propositional formula for satisfiability checking. However, this flattened Boolean formula loses the structural information in the program which can be utilized to direct the state exploration. In this paper, we propose a structural model of the program that explicitly represents data and control dependencies. We also reduce the safety property checking of embedded\u00a0\u2026", "num_citations": "3\n", "authors": ["1035"]}
{"title": "A methodology for a verifiable software platform to secure software defined and cognitive radios\n", "abstract": " Software defined radios (SDR) introduce many new challenges, one of which is the proper development, maintenance, and distribution of the core software. As with any software venture, SDR requires industry, government, and the independent development community to work together to produce an environment that fosters software development and innovation. SDR differs from other areas of software development by the long history of radio regulatory requirements that must be satisfied. In this paper, we propose a methodology to bring to the SDR world the same level of development and innovation that has made other software ventures a success. The verification platform we propose allows software developments to guarantee regulatory compliance even when faced with the challenges of open source software and cognitive radio regulation. The system itself first verifies that the software meets regulations and sends back to the developer the object code along with a security key that grants access to the radio for download. The system\u2019s security policy relies on standard industry encryption and authentication schemes. Therefore, it requires no new developments but rather the application of existing methodology for this application.", "num_citations": "3\n", "authors": ["1035"]}
{"title": "A fast, accurate, and non-statistical method for fault coverage estimation\n", "abstract": " We present a fast, dynamic fault coverage estimation technique for sequential circuits that achieves high degrees of accuracy by significantly reducing the number of injected faults and faulty-event evaluations. Specficdy, we dynamically reduce injection of two types of faults:(1) hyperactive faults that never get detected, and (2) faults whose effects never propagate to a flip-flop or primary output. The cost of fault simulation is greatly reduced as injection of most of these two types of faults is prevented. Experiments show that our technique gives very accurate estimates with frequently greater speedups than the sarnpfing techniques for most circuits. Most significantly, the proposed technique can be combined with the sampling approach to obtain speedups equivalent of smd sample sizes and retain estimation accuracy of large fault samples.", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Optimal blade placement for large turbofan balancing\n", "abstract": " Considers the problem of mounting n blades with different moments on a moving wheel, such that the resultant imbalance is minimized. In the absence of analytical solution, the authors present three stochastic optimization methods, iterative random search, genetic algorithm and simulated annealing, for the general turbofan balancing problem. Specific characteristics of the first two methods regarding this problem are discussed. Numerical results using these methods gave satisfactory results. It is observed that (1) random search is good enough for the off-line design purpose; (2) genetic algorithms have the ability to \"memorize\" some good substructures for the subsequent candidate solution selection, however proper parameter selection is necessary for great performance. (3) the neighbor set and parameters selection for the simulated annealing can be disadvantageous in this specific application.< >", "num_citations": "3\n", "authors": ["1035"]}
{"title": "Ease: Enabling hardware assertion synthesis from english\n", "abstract": " In this paper, we present EASE (Enabling hardware Assertion Synthesis from English) which translates hardware design specifications written in English to a formal assertion language. Existing natural language processing (NLP) tools for hardware verification utilize the vocabulary and grammar of a few specification documents only. Hence, they lack the ability to provide linguistic variations in parsing and writing natural language assertions. The grammar used in EASE does not follow a strict English syntax for writing design specifications. Our grammar incorporates dependency rules for syntactic categories which are coupled with semantic category dependencies that allow users to specify the same design specification using different word sequences in a sentence. Our NLP engine consists of interleaving operations of semantic and syntactic analyses to understand the input sentences and map differently\u00a0\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Property-checking based LBIST for improved diagnosability\n", "abstract": " We propose a new property-checking-based LBIST architecture which uses hardware monitors to check certain properties in the output responses. If any property is violated, the failing property number is stored for diagnosis. The proposed architecture improves diagnosability considerably with minimal hardware overhead. Experimental results show that the diagnostic resolution achieved by our architecture is comparable to that achieved in a non-BIST setup for many circuits.", "num_citations": "2\n", "authors": ["1035"]}
{"title": "High-performance diagnostic fault simulation on gpus\n", "abstract": " In this paper, we present an efficient diagnostic fault simulator based on a state-of-the-art graphics processing unit (GPU). Diagnostic fault simulation plays an important role to identify and locate the causes of circuit failures. However, today's complex VLSI circuits pose ever higher computational demand for such simulators. Our GPU based diagnostic fault simulator (GDSim) is based on a novel two-stage simulation framework which exploits high computation efficiency on the GPU. The fault pair based simulation is proposed to overcome the limited capacity of GPU memory as well as achieve a substantial fine-grained parallelism. Multi-fault-signature and dynamic load balancing techniques are introduced for the best usage of computing resources on-board. Experimental results demonstrate a speedup of up to 121\u00d7 (with average speedup of 38.43\u00d7) compared to a state-of-the-art CPU-based diagnostic fault simulator.", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Fault simulation and test generation\n", "abstract": " Publisher SummaryThis chapter discusses the major very large-scale integration (VLSI) testing topics: fault simulation and test generation. One of the most important tasks in VLSI testing is to minimize the number of defective chips shipped to customers. The quality of test patterns is critical in determining the thoroughness of testing. This requires the assessment of the quality of test patterns either developed manually or generated automatically so that a desired product quality can be achieved. For fault simulation, both event-driven simulation and compiled-code simulation techniques can be found in commercially available electronic design automation (EDA) applications. The fault simulators can be stand-alone tools or used as an integrated feature in the automatic test pattern generation (ATPG) programs. Test generation remains to be an important research area as circuit sizes and complexities continue to\u00a0\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "On dynamic switching of navigation for semi-formal design validation\n", "abstract": " Abstraction-guided simulation is a promising semi-formal framework for design validation. Unlike previously proposed approaches that utilized potentially costly abstraction-refinement for altering the abstraction when encountering hard corner cases, in this paper, a novel and low-cost method is proposed. The search begins with an initial abstraction and dynamically switches guidance to a new, different abstract model when it becomes apparent that the current model does not provide a sufficiently detailed map of the state space to advance the search towards the target. We automatically and efficiently identify those state variables that influence the circuitpsilas transition towards the target state using binary resolution, and these variables are used to create new abstractions on-the-fly during the search. The new abstractions provide a fine-grained abstract view of local segments of the concrete state space, which the\u00a0\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Integrating validation and verification in the digital design curriculum\n", "abstract": " This paper presents a module-based approach on integrating verification terminology, concepts and examples into the current computer engineering design curriculum. The primary goal is to equip students with the necessary verification and validation skills which are either ignored or are dealt with in an ad-hoc manner in current design courses. By making these courses verification-centric and emphasizing design-for-verifiability, we can produce highly qualified designers to handle the ever-larger and complex designs of the future.", "num_citations": "2\n", "authors": ["1035"]}
{"title": "State variable extraction and partitioning to reduce problem complexity for ATPG and design validation\n", "abstract": " This paper presents a new algorithm to extract characteristic flip-flops, which form a characteristic state set, using state-correlation information. The extracted characteristic state set allows us to focus on a significantly smaller set of flip-flops while ignoring other flip-flops, thereby simplifying the target problem and reducing state explosion in very large sequential circuits. Next, partitioning is applied only on the characteristic state variables, and partial state transition graphs (STGs) are built. During test generation, no specific fault or design error is targeted; instead, test vectors are generated using a twofold criteria: 1) whether the vector will expand the overall STGs and 2) whether this vector will break the relationship among flip-flops within the correlated sets. While generating vectors, state and transition exploration histories for each state group are maintained by dynamically constructing partial STGs for all state\u00a0\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Simulation-based internal variable range coverage metric and test generation model\n", "abstract": " In this paper, we present a new subdomain-based testing strategy which is associated with a new coverage metric called Internal Variable Range coverage. Unlike the traditional input domain partition technique, we apply partitioning on the value ranges of critical internal variables in a program. A simulation-based approach is proposed for range analysis and definition according to the variable data value distribution as well as the program structure information related to the internal variables. In addition, we exploit a sub-range transition finite state machine model to facilitate an internal variable range coverage oriented automatic test data generation process. Our experimental results on a class of extended triangle programs showed that our model can generate test set that achieves up to 44% higher internal variable range coverage than random test set as well as with better quality in terms of mutant killing\u00a0\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Untestable Multi-Cycle Path Delay Faults in Industrial Designs\n", "abstract": " The need for high-performance pipelined architectures has resulted in the adoption of latch based designs with multiple, interacting clocks. For such designs, time sharing across latches results in signals which propagate across multiple clock cycles along paths with multiple latches. These paths need to be tested for delay failures to ensure reliability of performance. However, many of these multi-cycle paths can be untestable and significant computational effort is wasted in targeting such paths during test generation and fault grading. To save this computational effort, a-priori identification of untestable multicycle paths is desired. We address this issue in our paper through a novel and unique framework: unlike traditional techniques, which focus only on single-cycle path delay faults (for flip-flop based designs with single clock), our framework efficiently identifies untestable multi-cycle path delay faults (Mpdfs) in latch\u00a0\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Efficient implication-based untestable bridge fault identifier\n", "abstract": " This paper presents a novel, low cost technique based on implications to identify untestable bridging faults in sequential circuits. Sequential symbolic simulation is first performed, as a preprocessing step, to identify nets which are uncontrollable to a specific logic value. Then, an implication-based analysis is carried out for each fault to determine if a particular fault is testable or not. We also use information about the untestable stuck-at faults to filter out some bridges early in the analysis process. The application of our technique to ISCAS '89 sequential benchmark circuits and a few industrial circuits showed that a large number of untestable bridges could be identified at a low cost, both in terms of memory and execution time.", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Test set and fault partitioning techniques for static test sequence compaction for sequential circuits\n", "abstract": " We propose a new static test set compaction method based on a careful examination of attributes of fault coverage curves. Our method is based on two key ideas: (1) fault-list and test-set partitioning, and (2) vector re-ordering. Typically, the first few vectors of a test set detect a large number of faults. The remaining vectors usually constitute a large fraction of the test set, but these vectors are included to detect relatively few hard faults. We show that significant compaction can still be achieved by partitioning faults into hard and easy faults, and compaction is performed only for the hard faults. This significantly reduces the computational cost for static test set compaction without affecting quality of compaction. The second key idea re-orders vectors in a test set by moving sequences that detect hard faults to the beginning of the test set. Fault simulation of the newly concatenated re-ordered test set results in the\u00a0\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Automatic test generation\n", "abstract": " Automatic test generation | Genetic algorithms for VLSI design, layout & test automation ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksGenetic algorithms for VLSI design, layout & test automationAutomatic test generation chapter Automatic test generation Share on Authors: Elizabeth M Rudnick profile image EM Rudnick View Profile , Michael Hsiao profile image MS Hsiao View Profile , Janak H. Patel profile image JH Patel View Profile Authors Info & Affiliations Publication: Genetic algorithms for VLSI design, layout & test automationJanuary 1999 Pages 158\u2013226 0citation 0 Downloads Metrics Total Citations0 Total Downloads\u2026", "num_citations": "2\n", "authors": ["1035"]}
{"title": "Breaking down high-level robot path-finding abstractions in natural language programming\n", "abstract": " Natural language programming (NLPr) allows people to program in natural language (NL) for specific domains. It poses great potential since it gives non-experts the ability to develop projects without exhaustive training. However, complex descriptions can sometimes have multiple interpretations, making program synthesis difficult. Thus, if the high-level abstractions can be broken down into a sequence of precise low-level steps, existing natural language processing (NLP) and NLPr techniques could be adaptable to handle the tasks. In this paper, we present an algorithm for converting high-level task descriptions into low-level specifications by parsing the sentences into sentence frames and using generated low-level NL instructions to generate executable programs for pathfinding tasks in a LEGO Mindstorms EV3 robot. Our analysis shows that breaking down the high-level pathfinding abstractions into a\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Multi-Label Classification on Natural Language Sentences for Video Game Design\n", "abstract": " This paper investigates the effectiveness of multi-label classification (MLC) in classifying controlled natural language sentences that used to write video games. The performance of such multi-label classifying tasks can help us both understand the complexities of program synthesis as well as providing suggestions to the users whenever a sentence is not understood. Real-user data is used in the experiments. The proposed classification paradigm shows that classification and prediction in code generation are feasible, and can offer suggestions to the users when the sentence is unclear. The impact of label correlation is also discussed.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Optimization of Mutant Space for RTL Test Generation\n", "abstract": " Mutations in the RTL design can help test generation to uncover bugs. However, since the mutant space is infinite, identifying the right mutants is critical. In this paper, we present two filtering approaches to identify the key mutants without loss in the branch coverage. The final set of test vectors generated is able to achieve both high branch coverage and mutation coverage. Experimental results show that we were able to reduce the overall test generation time up to 85% for all ITC99 benchmarks. In addition, we also found that the test suite length also decreased up to 78% for most of the ITC99 circuits.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "A framework for fast test generation at the RTL\n", "abstract": " We present a framework for high quality functional test generation at the RTL. The method utilizes static learning to derive cross cycle relationships which helps the search algorithm make inferences about the potential execution path in the next cycle. Additionally, dynamic taint analysis is adapted to model behavioral propagation. Finally, an ant colony optimization swarm intelligence algorithm is combined with an operator level behavioral coverage metric. We show that the method can be used to generate high quality functional test patterns at the RTL. By leveraging RTL code for test generation, we can generate test vectors with comparable levels of coverage up to 100\u00d7 times faster than gate level ATPG and more than 10\u00d7 faster than mixed level generation techniques. Additionally, without gate level information, we are able to approach the highest levels of coverage seen by previous techniques and in the case\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Fault Collapsing Using a Novel Extensibility Relation\n", "abstract": " Fault Collapsing of a target fault-list can help in obtaining a compact test set, decreasing test-generation/fault simulation time, and indirectly reducing test data volume and test application time during Manufacturing Test. These factors have a direct impact on test economics, thus obtaining a compact fault list is essential. In this paper, we propose a novel extensibility relation that aids in identifying non-trivial dominance relationships among fault-pairs. We show that our technique supersedes existing dominance-based collapsing techniques and thus may identify more dominance relations among faults. To this end, we learn several necessary assignments for faults in a low-cost fault independent manner, in which memory requirements are also low. Further, from a theoretical point of interest, we theorize a lower bound on the size of a collapsed fault-list. Experimental results on ISCAS85 and full-scan versions of\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "A Novel Learning Framework for State Space Exploration Based on Search State Extensibility Relation\n", "abstract": " Model Checking is an effective method for design verification, useful for proving temporal properties of the underlying system. In model checking, computing the pre-image (or image) space of a given temporal property plays a critical role. In this paper, we propose a novel learning framework for efficient state space exploration based on search state extensibility relation. This allows for the identification and pruning of several non-trivial redundant search spaces, thereby reducing the computational cost. We also propose a probability-based heuristic to guide our learning method. Experimental evidence is given to show the practicality of the proposed method.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "An efficient path-oriented bitvector encoding width computation algorithm for bit-precise verification\n", "abstract": " Bit-precise verification with variables modeled as bitvectors has recently drawn much interest. However, a huge search space usually results after bit-blasting. To accelerate the verification of bit-vector formulae, we propose an efficient algorithm to discover non-uniform encoding widths. W e  of variables in the verification model, which may be smaller than their original modeling widths but sufficient to find a counterexample. Different from existing approaches, our algorithm is path-oriented, in that it takes advantage of the controllability and observability values in the structure of the model to guide the computation of the paths, their encoding widths and the effective adjustment of these widths in subsequent steps. For path selection, a subset of single-bit path-controlling variables is set to constant values. This can restrict the search from those paths deemed less favorable or have been checked in previous steps, thus\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Introduction to special section on high-level design, validation, and test\n", "abstract": " Continuing advances in transistor integration have resulted in hardware designs of ever-increasing complexity. Such systems are providing unprecedented challenges in design, validation, and manufacturing test. This special section contains three articles on verification and one on manufacturing test generation. Advances detailed by the verification articles include improved SAT-based model checking, automatic synthesis of hardware assertion checkers, and deadlock detection in on-chip networks. The test generation article introduces a novel technique for stuck-at fault testing of multipliers. In the first article,\u201cBoosting Interpolation with Dynamic Localized Abstraction and Redundancy Removal\u201d by S. Quer, G. Cabodi, M. Murciano, and S. Nocco, the authors propose three techniques for improving SAT-based unbounded model checking. Together, the techniques help contain the size of Craig interpolants and\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Characteristic States and Cooperative Game Based Search for Efficient Sequential ATPG and Design Validation\n", "abstract": " This paper introduces a new cooperative traversal on the partitioned finite-state-machine (FSM) space to generate test stimuli for both sequential test generation and design validation. The new features for this logic-simulation-based automatic test pattern generation (ATPG) include: (1) cooperative search that exploits orthogonality is performed on two global state partition sets; (2) feedback information is dynamically extracted from characteristic states (each characteristic state examines a subset of corner cases of the design) to prune away redundant search spaces; (3) different from dynamic partitioning, interactions between two global partition sets are emphasized in the proposed approach. The proposed cooperative search is similar to a cooperative two-player game, where each partition set presents the state space for a player. Experimental results show that test vectors generated by such a cooperative search\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Test Generation\n", "abstract": " Publisher SummaryTest generation is the task of producing an effective set of vectors that can achieve high fault coverage for a specified fault model. Much progress is made in automatic test pattern generation (ATPG). Without powerful ATPGs, chips are increasingly depending on (DFT) techniques to alleviate the high cost of generating vectors. This chapter deals with the fundamental issues behind the design of an ATPG, as well as the underlying learning mechanisms that can improve the overall performance of ATPG. It provides an overview of the problem of test generation followed by random a discussion of test generation. Deterministic algorithms for test generation for stuck-at faults are also addressed in the chapter including techniques that enhance the deterministic engines such as static and dynamic learning. Moreover, test generation for other fault models such as delay faults is explained, including ATPG\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Interleaved invariant checking with dynamic abstraction\n", "abstract": " The notion of dynamic abstraction was recently introduced as a means of abstracting a model during the process of model checking. In this paper we show, theoretically and practically, how dynamic abstraction can be used with different algorithms for invariant checking, namely forward, backward and interleaved state-space traversal. Further, we formalize the correctness guarantees that can be made under different invariant checking algorithms operating on a dynamically abstracted model. We report experimental results on industrial strength benchmarks to further demonstrate the power and versatility of this abstraction mechanism in conjuction with interleaved state-space traversal.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "An effective and efficient ATPG-based combinational equivalence checker\n", "abstract": " Combinational equivalence checking (CEC) is an important component in the overall design and verification of today\u2019s digital systems. There has been success in the area of Boolean Satisfiability (SAT) solvers as well as their application to CEC. However, circuit-structure-based search found in ATPG is intuitively better suited for incremental CEC. Early ATPG-based approaches to CEC suffered from temporal explosion due to the false negative phenomenon associated with incremental verification. We propose a robust ATPG-based CEC independent of BDDs that can effectively reduce the number of encountered false negatives as well as efficiently decide between real and false negatives. The efficacy of our approach is demonstrated with experimental results for various CEC problems, where several orders of magnitude speedup could be obtained.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Untestable fault identification through enhanced necessary value assignments\n", "abstract": " In this paper, we propose novel low-cost methods that combine static logic implications and binary resolution to significantly increase the number of non-trivial signal relations learned from the circuit. The proposed method first applies resolution techniques to learn new static single-node implications and then uses them to learn powerful multi-node implications. All the newly learned relations help in extracting more necessary assignments for a given fault, potentially increasing the chance for a conflict to occur among the necessary assignments. Experimental results on ISCAS89 and ITC99 benchmarks show that our method can identify significantly more untestable faults compared to existing non branch-and-bound based techniques.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Forward image computation with backtracing ATPG and incremental state-set construction\n", "abstract": " Image computation is a fundamental step in formal verification of sequential systems, including sequential equivalence checking and symbolic model checking. Since conventional Reduced Ordered Binary Decision Diagram (ROBDD) based methods can potentially suffer from memory explosion, there has been a growing interest in using Automatic Test Pattern Generation (ATPG)/Boolean Satisfiability (SAT) based techniques in recent years. While ATPG has been successful for computing pre-image, image computation presents a very different set of problems. In this paper, we present a novel backtracing-based ATPG technique for forward image computation. We carefully alter the ATPG engine to compute the image cubes and store them incrementally in a Zero-Suppressed Binary Decision Diagram (ZBDD). In order to improve the efficiency of image computation, we propose three heuristics:(i) gate-observability\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "State and fault information for compaction-based test generation\n", "abstract": " We present a new test generation procedure for sequential circuits using newly traversed state and newly detected fault information obtained between successive iterations of vector compaction. Two types of techniques are considered. One is based on the new states a sequential circuit is driven into, and the other is based on the new faults that are detected between consecutive iterations of vector compaction. These data modify an otherwise random selection of vectors, to bias vector sequences that cause the circuit to reach new states, and cause previously undetected faults to be detected. The biased vectors, when used to extend the compacted test set, provide a more intelligent selection of vectors. The extended test set is then compacted. Repeated applications of state and fault analysis, vector generation and compaction produce significantly high fault coverage using relatively small computing\u00a0\u2026", "num_citations": "1\n", "authors": ["1035"]}
{"title": "On quality of test sets: relating fault coverage to defect coverage\n", "abstract": " The true quality of a test set can be evaluated by estimating its coverage of arbitrary defects. In evaluating the quality of test sets, test vectors were generated from different test generators. We estimate the corresponding defect coverages for these test sets of varying lengths, generated by different test generators. The coverage estimation is performed using a region based model, in which it is independent of specific, physical, fault models. Experimental results show that similar fault coverages of varying test set sizes yield similar defect coverage. Further, similar activity and observability levels were observed in spite of the fact that test sets were generated with different methods and had different sizes.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "Formal operator testability methods for behavioral-level DFT using value ranges\n", "abstract": " The focus of this research is on the testability analysis of the operators in the behavioral description prior to synthesis. The controllabilities of the inputs to an operator and the observabilities of the outputs of the operation are computed from the value ranges of the variables that serve as the inputs and outputs. The proposed technique uses a formal data flow analysis instead of profiling or simulation, to accurately pin-point the hard-to-test operations in the design. Variable selection for testability enhancement of hard-to-test operations is accomplished based on the computed testability measures for all the involved operations in the behavioral description. The insertion of appropriate testability enhancements is then performed for the hard-to-test operators to achieve significantly higher test coverages, while keeping the design area-performance overhead to a minimum.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "On non-statistical techniques for fast fault coverage estimation\n", "abstract": " We present fast, dynamic fault coverage estimation techniques for sequential circuits that achieve high degrees of accuracy and significant reductions in the number of injected faults and faulty-event evaluations. In the proposed techniques, we dynamically reduce injection of hyperactive faults as well as faults whose effects never propagate to a flip-flop or primary output. Suppression and over-specification of potential fault-effects are also investigated to reduce faulty-event evaluations. Experiments show that our methods give very accurate estimates with frequently greater speedups than the sampling techniques for most circuits. Most significantly, the proposed techniques can be combined with the sampling approach to obtain speedups comparable to small sample sizes and retain estimation accuracy of large fault samples.", "num_citations": "1\n", "authors": ["1035"]}
{"title": "A Survey of General and Architecture-Specific Compiler Optimization Techniques\n", "abstract": " Experience with commercial and research high-performance architectures has indicated that the compiler plays an increasingly important role in real application performance. In particular, the di culty in programming some of the so-called\\hardware rst\" machines underscores the need for integrating architecture design and compilation strategy. In addition, architectures featuring novel hardware optimizations require compilers that can take advantage of them in order to be commercially viable. We survey a variety of compiler optimization techniques of current interest: general techniques, vectorizing compiler techniques, ne-grained parallelism techniques. For architecture-speci c techniques, we analyze what features of the architecture require special attention from the compiler in order to achieve best performance, and summarize implementation complexity and observed performance for a variety of past approaches. We pay particular attention to hardware/software trade-o s and limits on the compiler's ability to enhance performance on a particular hardware architecture. We also discuss some hardware optimizations which currently do not rely on smart compilers, and compare their performance to compiler schemes which try to achieve the same e ects.", "num_citations": "1\n", "authors": ["1035"]}