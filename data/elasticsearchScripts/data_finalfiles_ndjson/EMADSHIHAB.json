{"title": "What are mobile developers asking about? a large scale study using stack overflow\n", "abstract": " The popularity of mobile devices has been steadily growing in recent years. These devices heavily depend on software from the underlying operating systems to the applications they run. Prior research showed that mobile software is different than traditional, large software systems. However, to date most of our research has been conducted on traditional software systems. Very little work has focused on the issues that mobile developers face. Therefore, in this paper, we use data from the popular online Q&A site, Stack Overflow, and analyze 13,232,821 posts to examine what mobile developers ask about. We employ Latent Dirichlet allocation-based topic models to help us summarize the mobile-related questions. Our findings show that developers are asking about app distribution, mobile APIs, data management, sensors and context, mobile tools, and user interface development. We also determine what\u00a0\u2026", "num_citations": "216\n", "authors": ["182"]}
{"title": "An exploratory study on self-admitted technical debt\n", "abstract": " Throughout a software development life cycle, developers knowingly commit code that is either incomplete, requires rework, produces errors, or is a temporary workaround. Such incomplete or temporary workarounds are commonly referred to as 'technical debt'. Our experience indicates that self-admitted technical debt is common in software projects and may negatively impact software maintenance, however, to date very little is known about them. Therefore, in this paper, we use source-code comments in four large open source software projects-Eclipse, Chromium OS, Apache HTTP Server, and ArgoUML to identify self-admitted technical debt. Using the identified technical debt, we study 1) the amount of self-admitted technical debt found in these projects, 2) why this self-admitted technical debt was introduced into the software projects and 3) how likely is the self-admitted technical debt to be removed after their\u00a0\u2026", "num_citations": "188\n", "authors": ["182"]}
{"title": "Characterizing and predicting blocking bugs in open source projects\n", "abstract": " As software becomes increasingly important, its quality becomes an increasingly important issue. Therefore, prior work focused on software quality and proposed many prediction models to identify the location of software bugs, to estimate their fixing-time, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These blocking bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems.", "num_citations": "120\n", "authors": ["182"]}
{"title": "Commit guru: analytics and risk prediction of software commits\n", "abstract": " Software quality is one of the most important research sub-areas of software engineering. Hence, a plethora of research has focused on the prediction of software quality. Much of the software analytics and prediction work has proposed metrics, models and novel approaches that can predict quality with high levels of accuracy. However, adoption of such techniques remain low; one of the reasons for this low adoption of the current analytics and prediction technique is the lack of actionable and publicly available tools. We present Commit Guru, a language agnostic analytics and prediction tool that identifies and predicts risky software commits. Commit Guru is publicly available and is able to mine any GIT SCM repository. Analytics are generated at both, the project and commit levels. In addition, Commit Guru automatically identifies risky (ie, bug-inducing) commits and builds a prediction model that assess the\u00a0\u2026", "num_citations": "88\n", "authors": ["182"]}
{"title": "Detecting and quantifying different types of self-admitted technical debt\n", "abstract": " Technical Debt is a term that has been used to express non-optimal solutions during the development of software projects. These non optimal solutions are often shortcuts that allow the project to move faster in the short term, at the cost of increased maintenance in the future. To help alleviate the impact of technical debt, a number of studies focused on the detection of technical debt. More recently, our work shown that one possible source to detect technical debt is using source code comments, also referred to as self-admitted technical debt. However, what types of technical debt can be detected using source code comments remains as an open question. Therefore, in this paper we examine code comments to determine the different types of technical debt. First, we propose four simple filtering heuristics to eliminate comments that are not likely to contain technical debt. Second, we read through more than 33K\u00a0\u2026", "num_citations": "78\n", "authors": ["182"]}
{"title": "A distributed asynchronous directional-to-directional MAC protocol for wireless ad hoc networks\n", "abstract": " The use of directional antennas in ad hoc networks has received growing attention because of its high spatial reuse and high antenna gains. The medium access control (MAC) protocol design with directional antennas is nontrivial due to the deafness and collision problems. The existing protocols assumed that the nodes can operate in both directional and omnidirectional modes. However, using both modes leads to the asymmetry-in-gain problem. In this paper, we propose a directional-to-directional (DtD) MAC protocol, where both senders and receivers operate in a directional-only mode. We also derive the saturation throughput of ad hoc networks using DtD MAC. The analytical and simulation results show that DtD MAC can significantly improve the throughput and maintain good fairness among competing flows. We further evaluate the DtD MAC with millimeter-wave (mmWave) communication technologies\u00a0\u2026", "num_citations": "78\n", "authors": ["182"]}
{"title": "Wireless mesh networks for in-home IPTV distribution\n", "abstract": " IPTV is considered to be the next killer application. A key, challenging issue is in-home IPTV distribution with affordable deployment cost and sufficient flexibility, scalability, and reliability. In this article, we first survey and compare the emerging wired and wireless communication technologies for broadband home networks, including transmission over power lines, phone lines, coaxial cables or Ethernet cables, and IEEE 802.1 In, ultra wideband and millimeter wave wireless technologies. Considering these promising candidates for future broadband home networks, we propose three wireless mesh network architectures. These enable consumers to enjoy anywhere, anytime IPTV services without rewiring their homes. We compare the cost, reliability, and scalability of the three architectures. We further study their admission regions for IPTV, that is, the number of IPTV connections that can be supported simultaneously\u00a0\u2026", "num_citations": "65\n", "authors": ["182"]}
{"title": "An exploration of challenges limiting pragmatic software defect prediction\n", "abstract": " Software systems continue to play an increasingly important role in our daily lives, making the quality of software systems an extremely important issue. Therefore, a significant amount of recent research focused on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention is Software Defect Prediction (SDP), where predictions are made to determine where future defects might appear. Our survey showed that in the past decade, more than 100 papers were published on SDP. Nevertheless, the adoption of SDP in practice to date is limited.", "num_citations": "44\n", "authors": ["182"]}
{"title": "Cccd: Concolic code clone detection\n", "abstract": " Code clones are multiple code fragments that produce similar results when provided the same input. Prior research has shown that clones can be harmful since they elevate maintenance costs, increase the number of bugs caused by inconsistent changes to cloned code and may decrease programmer compre-hensibility due to the increased size of the code base. To assist in the detection of code clones, we propose a new tool known as Concolic Code Clone Discovery (CCCD). CCCD is the first known clone detection tool that uses concolic analysis as its primary component and is one of only three known techniques which are able to reliably detect the most complicated kind of clones, type-4 clones.", "num_citations": "35\n", "authors": ["182"]}
{"title": "The evolution of mobile apps: An exploratory study\n", "abstract": " As mobile apps continue to grow in popularity, it is important to study their evolution. Lehman's laws of software evolution have been proposed and used to study the evolution of traditional, large software systems (also known as desktop apps). However, do Lehman's laws of software evolution hold for mobile apps?, especially since developing mobile apps presents different challenges compared to the development of desktop apps.", "num_citations": "34\n", "authors": ["182"]}
{"title": "A quantitative comparison of overlapping and non-overlapping sliding windows for human activity recognition using inertial sensors\n", "abstract": " The sliding window technique is widely used to segment inertial sensor signals, ie, accelerometers and gyroscopes, for activity recognition. In this technique, the sensor signals are partitioned into fix sized time windows which can be of two types:(1) non-overlapping windows, in which time windows do not intersect, and (2) overlapping windows, in which they do. There is a generalized idea about the positive impact of using overlapping sliding windows on the performance of recognition systems in Human Activity Recognition. In this paper, we analyze the impact of overlapping sliding windows on the performance of Human Activity Recognition systems with different evaluation techniques, namely, subject-dependent cross validation and subject-independent cross validation. Our results show that the performance improvements regarding overlapping windowing reported in the literature seem to be associated with the underlying limitations of subject-dependent cross validation. Furthermore, we do not observe any performance gain from the use of such technique in conjunction with subject-independent cross validation. We conclude that when using subject-independent cross validation, non-overlapping sliding windows reach the same performance as sliding windows. This result has significant implications on the resource usage for training the human activity recognition systems. View Full-Text", "num_citations": "33\n", "authors": ["182"]}
{"title": "Studying and detecting log-related issues\n", "abstract": " Logs capture valuable information throughout the execution of software systems. The rich knowledge conveyed in logs is highly leveraged by researchers and practitioners in performing various tasks, both in software development and its operation. Log-related issues, such as missing or having outdated information, may have a large impact on the users who depend on these logs. In this paper, we first perform an empirical study on log-related issues in two large-scale, open source software systems. We find that the files with log-related issues have undergone statistically significantly more frequent prior changes, and bug fixes. We also find that developers fixing these log-related issues are often not the ones who introduced the logging statement nor the owner of the method containing the logging statement. Maintaining logs is more challenging without clear experts. Finally, we find that most of the defective\u00a0\u2026", "num_citations": "33\n", "authors": ["182"]}
{"title": "Performance analysis of IPTV traffic in home networks\n", "abstract": " A heterogeneous wired and wireless network architecture is proposed for in-home IPTV distribution. To identify the bottleneck in the home network and estimate the network capacity, we develop an analytical framework to quantify the maximum number of IPTV connections that can be supported with guaranteed QoS in the wired and multi-hop wireless networks, respectively. We extend the fluid flow model analysis to capture both the burstiness of IPTV sources and the time-varying characteristics of multi-hop wireless channels. Extensive NS-2 simulations with H.264 HDTV sources over wired and multi- hop wireless paths are given, which validate the analysis. The analytical and simulation results provide important guidelines for the planning of future home networks and IPTV systems.", "num_citations": "33\n", "authors": ["182"]}
{"title": "Identifying unmaintained projects in github\n", "abstract": " Background: Open source software has an increasing importance in modern software development. However, there is also a growing concern on the sustainability of such projects, which are usually managed by a small number of developers, frequently working as volunteers. Aims: In this paper, we propose an approach to identify GitHub projects that are not actively maintained. Our goal is to alert users about the risks of using these projects and possibly motivate other developers to assume the maintenance of the projects. Method: We train machine learning models to identify unmaintained or sparsely maintained projects, based on a set of features about project activity (commits, forks, issues, etc). We empirically validate the model with the best performance with the principal developers of 129 GitHub projects. Results: The proposed machine learning approach has a precision of 80%, based on the feedback of\u00a0\u2026", "num_citations": "27\n", "authors": ["182"]}
{"title": "Practical software quality prediction\n", "abstract": " Software systems continue to play an increasingly important role in our daily lives, making the quality of software systems an extremely important issue. Therefore, a significant amount of recent research focused on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention is Software Defect Prediction (SDP), where predictions are made to determine where future defects might appear. Our survey showed that in the past decade, more than 100 papers were published on SDP. Nevertheless, the practical adoption of SDP to date is limited. In this paper, we highlight the findings of our thesis, which identifies the challenges that hinder the adoption of SDP in practice. These challenges include the fact that the majority of SDP research rarely considers the impact of defects when performing their predictions, seldom provides guidance on how to use the\u00a0\u2026", "num_citations": "24\n", "authors": ["182"]}
{"title": "Test re-prioritization in continuous testing environments\n", "abstract": " New changes are constantly and concurrently being made to large software systems. In modern continuous integration and deployment environments, each change requires a set of tests to be run. This volume of tests leads to multiple test requests being made simultaneously, which warrant prioritization of such requests. Previous work on test prioritization schedules queued tests at set time intervals. However, after a test has been scheduled it will never be reprioritized even if new higher risk tests arrive. Furthermore, as each test finishes, new information is available which could be used to reprioritize tests. In this work, we use the conditional failure probability among tests to reprioritize tests after each test run. This means that tests can be reprioritized hundreds of times as they wait to be run. Our approach is scalable because we do not depend on static analysis or coverage measures and simply prioritize tests\u00a0\u2026", "num_citations": "22\n", "authors": ["182"]}
{"title": "Empirical study on the discrepancy between performance testing results from virtual and physical environments\n", "abstract": " Large software systems often undergo performance tests to ensure their capability to handle expected loads. These performance tests often consume large amounts of computing resources and time since heavy loads need to be generated. Making it worse, the ever evolving field requires frequent updates to the performance testing environment. In practice, virtual machines (VMs) are widely exploited to provide flexible and less costly environments for performance tests. However, the use of VMs may introduce confounding overhead (e.g., a higher than expected memory utilization with unstable I/O traffic) to the testing environment and lead to unrealistic performance testing results. Yet, little research has studied the impact on test results of using VMs in performance testing activities. To evaluate the discrepancy between the performance testing results from virtual and physical environments, we perform a\u00a0\u2026", "num_citations": "16\n", "authors": ["182"]}
{"title": "Subject cross validation in human activity recognition\n", "abstract": " K-fold Cross Validation is commonly used to evaluate classifiers and tune their hyperparameters. However, it assumes that data points are Independent and Identically Distributed (i.i.d.) so that samples used in the training and test sets can be selected randomly and uniformly. In Human Activity Recognition datasets, we note that the samples produced by the same subjects are likely to be correlated due to diverse factors. Hence, k-fold cross validation may overestimate the performance of activity recognizers, in particular when overlapping sliding windows are used. In this paper, we investigate the effect of Subject Cross Validation on the performance of Human Activity Recognition, both with non-overlapping and with overlapping sliding windows. Results show that k-fold cross validation artificially increases the performance of recognizers by about 10%, and even by 16% when overlapping windows are used. In addition, we do not observe any performance gain from the use of overlapping windows. We conclude that Human Activity Recognition systems should be evaluated by Subject Cross Validation, and that overlapping windows are not worth their extra computational cost.", "num_citations": "15\n", "authors": ["182"]}
{"title": "IPTV distribution technologies in broadband home networks\n", "abstract": " Understanding the pros and cons of different distribution technologies is the first step in designing efficient future broadband home networks for IPTV service delivery. In this paper, we survey the emerging wired and wireless communication technologies, including powerline, phone line, UltraWide Band (UWB), millimeter wave (MMW), etc., which are promising candidates for distributing broadband signals in home networks. Their characteristics such as data rate, QoS support, deployment cost, etc., are compared. The comparison can provide important guidelines for future home network architecture and protocol design.", "num_citations": "15\n", "authors": ["182"]}
{"title": "MSRBot: Using bots to answer questions from software repositories\n", "abstract": " Software repositories contain a plethora of useful information that can be used to enhance software projects. Prior work has leveraged repository data to improve many aspects of the software development process, such as, help extract requirement decisions, identify potentially defective code and improve maintenance and evolution. However, in many cases, project stakeholders are not able to fully benefit from their software repositories due to the fact that they need special expertise to mine their repositories. Also, extracting and linking data from different types of repositories (e.g., source code control and bug repositories) requires dedicated effort and time, even if the stakeholder has the expertise to perform such a task. Therefore, in this paper, we use bots to automate and ease the process of extracting useful information from software repositories. Particularly, we lay out an approach of how bots, layered on top of\u00a0\u2026", "num_citations": "14\n", "authors": ["182"]}
{"title": "Learning to generate corrective patches using neural machine translation\n", "abstract": " Bug fixing is generally a manually-intensive task. However, recent work has proposed the idea of automated program repair, which aims to repair (at least a subset of) bugs in different ways such as code mutation, etc. Following in the same line of work as automated bug repair, in this paper we aim to leverage past fixes to propose fixes of current/future bugs. Specifically, we propose Ratchet, a corrective patch generation system using neural machine translation. By learning corresponding pre-correction and post-correction code in past fixes with a neural sequence-to-sequence model, Ratchet is able to generate a fix code for a given bug-prone code query. We perform an empirical study with five open source projects, namely Ambari, Camel, Hadoop, Jetty and Wicket, to evaluate the effectiveness of Ratchet. Our findings show that Ratchet can generate syntactically valid statements 98.7% of the time, and achieve an F1-measure between 0.29 - 0.83 with respect to the actual fixes adopted in the code base. In addition, we perform a qualitative validation using 20 participants to see whether the generated statements can be helpful in correcting bugs. Our survey showed that Ratchet's output was considered to be helpful in fixing the bugs on many occasions, even if fix was not 100% correct.", "num_citations": "14\n", "authors": ["182"]}
{"title": "Admission region of triple-play services in wireless home networks\n", "abstract": " A heterogeneous wired and wireless network architecture is considered for home networks to support Internet Protocol TV (IPTV), voice, and data, the so-called triple-play services. To satisfy the quality of service (QoS) requirements for different traffic classes, class-based queueing (CBQ) is deployed at home gateways and routers. To estimate the network capacity and decide on an appropriate resource management scheme, we develop an analytical framework to quantify the maximum number of IPTV connections that can be supported with guaranteed QoS over wired and multi-hop wireless networks. We extend the fluid-flow model to capture both the burstiness of IPTV sources and the time-varying characteristics of multi-hop wireless paths. Heterogeneous traffic and CBQ are considered in the model. Simulation results over wired and multi-hop wireless paths are given which validate the analysis. The results\u00a0\u2026", "num_citations": "11\n", "authors": ["182"]}
{"title": "A distributed directional-to-directional MAC protocol for asynchronous ad hoc networks\n", "abstract": " The use of directional antennae in ad hoc networks has received growing attention in recent years. However, most existing directional MAC protocols assume interchangeable directional and omnidirectional modes of operation. Such operation reduces the spatial gain and introduces the asymmetry-in-gain problem. In this paper, we propose a directional-to-directional (DtD) MAC protocol for ad-hoc networks that operates in the directional mode exclusively. The protocol is fully distributed, does not require any synchronization, eliminates the asymmetry- in-gain problem, and alleviates the deafness problem. To study the performance of the proposed DtD MAC, we develop an analytical model that estimates the saturation throughput as a function of the number of antenna sectors, packet size and number of contending nodes. The analytical results are validated by extensive simulations with the QualNet simulator. We\u00a0\u2026", "num_citations": "11\n", "authors": ["182"]}
{"title": "The modular and feature toggle architectures of Google Chrome\n", "abstract": " Software features often span multiple directories and conceptual modules making the extraction of feature architectures difficult. In this work, we extract a feature toggle architectural view and show how features span the conceptual, concrete, and reference architectures. Feature toggles are simple conditional flags that allow developers to turn a feature on or off in a running system. They are commonly used by large web companies, including Google, Netflix and Facebook to selectively enable and disable features. Recently, libraries to help inject and manage toggles have been created for all the major programming languages. We extract the feature toggles from the Google Chrome web browser to illustrate their use in understanding the architecture of a system. Since there is no high-level conceptual and concrete architectures for Chrome, we had to manually derive these representations from available\u00a0\u2026", "num_citations": "10\n", "authors": ["182"]}
{"title": "Pragmatic prioritization of software quality assurance efforts\n", "abstract": " A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.", "num_citations": "8\n", "authors": ["182"]}
{"title": "Is self-admitted technical debt a good indicator of architectural divergences?\n", "abstract": " Large software systems tend to be highly complex and often contain unaddressed issues that evolve from bad design practices or architectural implementations that drift from definition. These design flaws can originate from quick fixes, hacks or shortcuts to a solution, hence they can be seen as Technical Debt. Recently, new work has focused on studying source code comments that indicate Technical Debt, i.e., Self-Admitted Technical Debt (SATD). However, it is not known if addressing information left by developers in the form source code comments can give insight about the design flaws in a system and have the potential to provide fixes for bad architectural implementations. This paper investigates the possibility of using SATD comments to resolve architectural divergences. We leverage a data set of previously classified SATD comments to trace them to the architectural divergences of a large open source\u00a0\u2026", "num_citations": "7\n", "authors": ["182"]}
{"title": "Towards Detecting Biceps Muscle Fatigue in Gym Activity Using Wearables\n", "abstract": " Fatigue is a naturally occurring phenomenon during human activities, but it poses a bigger risk for injuries during physically demanding activities, such as gym activities and athletics. Several studies show that bicep muscle fatigue can lead to various injuries that may require up to 22 weeks of treatment. In this work, we adopt a wearable approach to detect biceps muscle fatigue during a bicep concentration curl exercise as an example of a gym activity. Our dataset consists of 3000 bicep curls from twenty middle-aged volunteers at ages between 27 to 30 and Body Mass Index (BMI) ranging between 18 to 28. All volunteers have been gym-goers for at least 1 year with no records of chronic diseases, muscle, or bone surgeries. We encountered two main challenges while collecting our dataset. The first challenge was the dumbbell\u2019s suitability, where we found that a dumbbell weight (4.5 kg) provides the best tradeoff between longer recording sessions and the occurrence of fatigue on exercises. The second challenge is the subjectivity of RPE, where we average the reported RPE with the measured heart rate converted to RPE. We observed from our data that fatigue reduces the biceps\u2019 angular velocity; therefore, it increases the completion time for later sets. We extracted a total of 33 features from our dataset, which have been reduced to 16 features. These features are the most overall representative and correlated with bicep curl movement, yet they are fatigue-specific features. We utilized these features in five machine learning models, which are Generalized Linear Models (GLM), Logistic Regression (LR), Random Forests (RF), Decision Trees (DT\u00a0\u2026", "num_citations": "3\n", "authors": ["182"]}
{"title": "Simplifying the Search of npm Packages\n", "abstract": " ContextCode reuse, generally done through software packages, allows developers to reduce time-to-market and improve code quality. The npm ecosystem is a Node.js package management system which contains more than 700\u00a0K Node.js packages and to help developers find high-quality packages that meet their needs, npms developed a search engine to rank Node.js packages in terms of quality, popularity, and maintenance. However, the current ranking mechanism for npms tends to be arbitrary and contains many different equations, which increases complexity and computation.ObjectiveThe goal of this paper is to empirically improve the efficiency of npms by simplifying the used components without impacting the current npms package ranks.MethodWe use feature selection methods with the aim of simplifying npms\u2019 equations. We remove the features that do not have a significant effect on the package\u2019s\u00a0\u2026", "num_citations": "2\n", "authors": ["182"]}
{"title": "A directional-to-directional mac protocol for ad-hoc networks\n", "abstract": " The use of directional antennae in ad-hoc networks has received growing attention in recent years. However, most directional MAC protocols assume interchangeable directional and omni-directional modes of operation. Such operation reduces the spatial gain and introduces the asymmetry in gain problem. In this paper, we propose a directional-to-directional (DtD) MAC protocol for ad-hoc networks that operates in directional mode exclusively. The protocol is fully distributed, does not require any synchronization, eliminates the asymmetry in gain problem and alleviates the deafness problem. To study the performance of DtD MAC, we develop an analytical model that quantifies the saturation throughput in terms of the number of antennae sectors, the packet size and number of contending nodes per channel. The analytical results are validated by simulations. We show that the DtD MAC protocol can provide significant throughput improvement in ad-hoc networks, compared to the omni-directional antennae, if the number of antennae sectors is chosen appropriately.", "num_citations": "2\n", "authors": ["182"]}
{"title": "The Impact of Data Reduction on Wearable-Based Human Activity Recognition\n", "abstract": " One crucial step toward improving any pattern recognition model is refining the data (feature extraction) and simplifying it (feature selection) for the classifier. In this paper, we investigate the impact of feature reduction on the performance of HAR. We collected step data from two subjects and answer research questions related to the impact of feature reduction in terms of performance, generalizability and varying classifiers. Our findings indicate feature reduction can reduce the number of features by close to 90%, while only having an impact of 1-2% in model performance. Moreover, we find that feature reduction can impact the generalizability of HAR models. Lastly, we find that feature reduction does not have a major impact on most classifiers examined. Our results are useful for designers of HAR systems to help them optimize their models while ensuring high performance.", "num_citations": "1\n", "authors": ["182"]}
{"title": "Admission Region of Home Networks for IPTV Traffic\n", "abstract": " A heterogeneous wired and wireless mesh network architecture is proposed for in-home IPTV distribution. To identify the bottleneck in the home network and estimate the network capacity, we develop an analytical framework to quantify the maximum number of IPTV connections that can be supported with guaranteed QoS in the wired and multi-hop wireless networks, respectively. We extend the fluid flow model analysis to capture both the burstiness of IPTV sources and the timevarying characteristics of multi-hop wireless channels. Extensive NS-2 simulations with H. 264 HDTV sources over wired and multi-hop wireless paths are given, which validate the analysis. The analytical and simulation results can provide important guidelines for the planning of future home networks and the IPTV system.", "num_citations": "1\n", "authors": ["182"]}