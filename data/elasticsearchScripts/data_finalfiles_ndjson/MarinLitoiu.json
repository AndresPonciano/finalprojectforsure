{"title": "Resource provisioning for cloud computing\n", "abstract": " In resource provisioning for cloud computing, an important issue is how resources may be allocated to an application mix such that the service level agreements (SLAs) of all applications are met. A performance model with two interactive job classes is used to determine the smallest number of servers required to meet the SLAs of both classes. For each class, the SLA is specified by the relationship: Prob [response time\u2264 x]\u2265 y. Two server allocation strategies are considered: shared allocation (SA) and dedicated allocation (DA). For the case of FCFS scheduling, analytic results for response time distribution are used to develop a heuristic algorithm that determines an allocation strategy (SA or DA) that requires the smallest number of servers. The effectiveness of this algorithm is evaluated over a range of operating conditions. The performance of SA with non-FCFS scheduling is also investigated. Among the\u00a0\u2026", "num_citations": "213\n", "authors": ["1500"]}
{"title": "Introducing STRATOS: A cloud broker service\n", "abstract": " This paper introduces a cloud broker service (STRATOS) which facilitates the deployment and runtime management of cloud application topologies using cloud elements/services sourced on the fly from multiple providers, based on requirements specified in higher level objectives. Its implementation and use is evaluated in a set of experiments.", "num_citations": "185\n", "authors": ["1500"]}
{"title": "Exploring alternative approaches to implement an elasticity policy\n", "abstract": " An elasticity policy governs how and when resources (e.g., application server instances at the PaaS layer) are added to and/or removed from a cloud environment. The elasticity policy can be implemented as a conventional control loop or as a set of heuristic rules. In the control-theoretic approach, complex constructs such as tracking filters, estimators, regulators, and controllers are utilized. In the heuristic, rule-based approach, various alerts(e.g., events) are defined on instance metrics (e.g., CPU utilization), which are then aggregated at a global scale in order to make provisioning decisions for a given application tier. This work provides an overview of our experiences designing and working with both approaches to construct an auto scaler for simple applications. We enumerate different criteria such as design complexity, ease of comprehension, and maintenance upon which we form an informal comparison\u00a0\u2026", "num_citations": "149\n", "authors": ["1500"]}
{"title": "SHriMP views: an interactive environment for information visualization and navigation\n", "abstract": " The SHriMP (Simple Hierarchical Multi-Perspective) visualization technique was designed to enhance how people browse and explore complex information spaces. SHriMP uses a nested graph view to present information that is hierarchically structured. It introduces the concept of nested interchangeable views to allow a user to explore multiple perspectives of information at different levels of abstraction. SHriMP combines a hypertext following metaphor with animated panning and zooming motions over the nested graph to provide continuous orientation and contextual cues for the user. In this demo, we show how these ideas are proving useful in the areas of software visualization, knowledge management and flow diagram visualization.", "num_citations": "109\n", "authors": ["1500"]}
{"title": "Software engineering for self-adaptive systems\n", "abstract": " Deploying and managing autonomic applications in cloud is a time consuming operation, that require many components to work together. The management will need to extract metrics from the deployed system, analyze them and the make a decision for changes that need to be implemented.Usually, a researcher's work is focused in only one component (investigating different strategies for adaptation, evaluating the impact of various metrics, developing new methods to analyze data to uncover problems, etc.), while the rest must just work, without the researcher having to spend too much time on them.", "num_citations": "88\n", "authors": ["1500"]}
{"title": "A design for adaptive web service evolution\n", "abstract": " In this paper, we define the problem of simultaneously deploying multiple versions of a web service in the face of independently developed unsupervised clients. We then propose a solution in the form of a design technique called Chain of Adapters and argue that this approach strikes a good balance between the various requirements. The Chain of Adapters technique is particularly suitable for self-managed systems since it makes many version-related reconfiguration tasks safe, and thus subject to automation.", "num_citations": "85\n", "authors": ["1500"]}
{"title": "Optimal autoscaling in a IaaS cloud\n", "abstract": " An application provider leases resources (ie, virtual machine instances) of variable configurations from a IaaS provider over some lease duration (typically one hour). The application provider (ie, consumer) would like to minimize their cost while meeting all service level obligations (SLOs). The mechanism of adding and removing resources at runtime is referred to as autoscaling. The process of autoscaling is automated through the use of a management component referred to as an autoscaler. This paper introduces a novel autoscaling approach in which both cloud and application dynamics are modeled in the context of a stochastic, model predictive control problem. The approach exploits trade-off between satisfying performance related objectives for the consumer's application while minimizing their cost. Simulation results are presented demonstrating the efficacy of this new approach.", "num_citations": "83\n", "authors": ["1500"]}
{"title": "Real-time task scheduling with fuzzy deadlines and processing times\n", "abstract": " A set of n independent and periodical tasks are considered. The processing times and the deadlines are described by fuzzy numbers. We try to find the optimal assignment of priorities not to miss deadlines. We manage the problem in two ways: first, we solve the problem by introducing the new cost functions; second, we solve the problem by using the ordering relations defined on fuzzy numbers. Examples with trapezoidal and triangular fuzzy numbers are provided.", "num_citations": "81\n", "authors": ["1500"]}
{"title": "Distributed, application-level monitoring for heterogeneous clouds using stream processing\n", "abstract": " As utility computing is widely deployed, organizations and researchers are turning to the next generation of cloud systems: federating public clouds, integrating private and public clouds, and merging resources at all levels (IaaS, PaaS, SaaS). Adaptive systems can help address the challenge of managing this heterogeneous collection of resources. While services and libraries exist for basic management tasks that enable implementing decisions made by the manager, monitoring is an open challenge. We define a set of requirements for aggregating monitoring data from a heterogeneous collections of resources, sufficient to support adaptive systems. We present and implement an architecture using stream processing to provide near-realtime, cross-boundary, distributed, scalable, fault-tolerant monitoring. A case study illustrates the value of collecting and aggregating metrics from disparate sources. A set of\u00a0\u2026", "num_citations": "75\n", "authors": ["1500"]}
{"title": "A design technique for evolving web services\n", "abstract": " In this paper, we define the problem of simultaneously deploying multiple versions of a web service in the face of independently developed unsupervised clients. We then propose a solution in the form of a design technique called Chain of Adapters and argue that this approach strikes a good balance between the various requirements. We recount our experiences in automating the application of the technique and provide an initial analysis of the performance degradations it may occasion. The Chain of Adapters technique is particularly suitable for self-managed systems since it makes many version-related reconfiguration tasks safe, and thus subject to automation.", "num_citations": "73\n", "authors": ["1500"]}
{"title": "Autonomic load-testing framework\n", "abstract": " In this paper, we present a method for performance testing of transactional systems. The methods models the system under test, finds the software and hardware bottlenecks and generate the workloads that saturate them. The framework is autonomic, the model and workloads are determined during the performance test execution by measuring the system performance, fitting a performance model and by analytically computing the number and mix of users that will saturate the bottlenecks.", "num_citations": "71\n", "authors": ["1500"]}
{"title": "A performance analysis method for autonomic computing systems\n", "abstract": " In an autonomic computing system, an autonomic manager makes tuning, load balancing, or provisioning decisions based on a predictive model of the system. This article investigates performance analysis techniques used by the autonomic manager. It looks at the complexity of the workloads and presents algorithms for computing the bounds of performance metrics for distributed systems under asymptotic and nonasymptotic conditions, that is, with saturated and nonsaturated resources. The techniques used are hybrid in nature, making use of performance evaluation and linear and nonlinear programming models. The workloads are characterized by the workload intensity, which represents the total number of users in the system, and by the workload mixes, which depict the number of users in each class of service. The results presented in this article can be applied to distributed transactional systems. Such\u00a0\u2026", "num_citations": "69\n", "authors": ["1500"]}
{"title": "Feedback-based optimization of a private cloud\n", "abstract": " The optimization problem addressed by this paper involves the allocation of resources in a private cloud such that cost to the provider is minimized (through the maximization of resource sharing) while attempting to meet all client application requirements (as specified in the SLAs). At the heart of any optimization based resource allocation algorithm, there are two models: one that relates the application level quality of service to the given set of resources and one that maps a given service level and resource consumption to profit metrics. In this paper we investigate the optimization loop in which each application\u2019s performance model is dynamically updated at runtime to adapt to the changes in the system. These changes could be perturbations in the environment that had not been included in the model. Through experimentation we show that using these tracking models in the optimization loop will result in a more\u00a0\u2026", "num_citations": "62\n", "authors": ["1500"]}
{"title": "Partitioning applications for hybrid and federated clouds.\n", "abstract": " On-demand access to computing resources asa-service has the potential to allow enterprises to temporarily scale out of their private data center into the infrastructure of a public cloud provider during times of peak demand. However, concerns about privacy and security may limit the adoption of this technique. We describe an approach to partitioning a software application (particularly a client-facing web application) into components that can be run in the public cloud and components that should remain in the private data center. Static code analysis is used to automatically establish a partitioning based on low-effort input from the developer. Public and private versions of the application are created and deployed; at runtime, user navigation proceeds seamlessly with requests routed to the public or private data center as appropriate. We present implementations for both Java and PHP web applications, tested on sample applications.", "num_citations": "59\n", "authors": ["1500"]}
{"title": "Towards mitigation of low and slow application ddos attacks\n", "abstract": " Distributed Denial of Service attacks are a growing threat to organizations and, as defense mechanisms are becoming more advanced, hackers are aiming at the application layer. For example, application layer Low and Slow Distributed Denial of Service attacks are becoming a serious issue because, due to low resource consumption, they are hard to detect. In this position paper, we propose a reference architecture that mitigates the Low and Slow Distributed Denial of Service attacks by utilizing Software Defined Infrastructure capabilities. We also propose two concrete architectures based on the reference architecture: a Performance Model-Based and Off-The-Shelf Components based architecture, respectively. We introduce the Shark Tank concept, a cluster under detailed monitoring that has full application capabilities and where suspicious requests are redirected for further filtering.", "num_citations": "56\n", "authors": ["1500"]}
{"title": "Efficiency analysis of provisioning microservices\n", "abstract": " Microservice architecture has started a new trend for application development/deployment in cloud due to its flexibility, scalability, manageability and performance. Various microservice platforms have emerged to facilitate the whole software engineering cycle for cloud applications from design, development, test, deployment to maintenance. In this paper, we propose a performance analytical model and validate it by experiments to study the provisioning performance of microservice platforms. We design and develop a microservice platform on Amazon EC2 cloud using Docker technology family to identify important elements contributing to the performance of microservice platforms. We leverage the results and insights from experiments to build a tractable analytical performance model that can be used to perform what-if analysis and capacity planning in a systematic manner for large scale microservices with\u00a0\u2026", "num_citations": "52\n", "authors": ["1500"]}
{"title": "Designing process replication and activation: A quantitative approach\n", "abstract": " Distributed application systems are composed of classes of objects with instances that interact to accomplish common goals. Such systems can have many classes of users with many types of requests. Furthermore, the relative load of these classes can shift throughout the day, causing changes to system behavior and bottlenecks. When designing and deploying such systems, it is necessary to determine a process replication and threading policy for the server processes that contain the objects, as well as process activation policies. To avoid bottlenecks, the policy must support all possible workload conditions. Licensing, implementation or resource constraints can limit the number of permitted replicas or threads of a server process. Process activation policies determine whether a server is persistent or should be created and terminated with each call. This paper describes quantitative techniques for choosing\u00a0\u2026", "num_citations": "49\n", "authors": ["1500"]}
{"title": "System and method for visualizing process flows\n", "abstract": " A method and system for visualizing process flows for business and manufacturing processes are described that includes a first view of the process as a tree showing the process elements and their interconnections, and a second view of the process in the form of a zoomable interface that displays selected nodes at magnifications that are continuously variable at the user's option. A selected node in the tree view opens the same node in the zoomable node view. When a second node is selected in the tree view, the zoomable node view zooms from the first selected node to the second selected node through each node intermediate between the first and second selected nodes. This enables a process analyst to view the content of selected process nodes in the context of nodes with which the selected nodes are associated and thus to have a thorough understanding of the process.", "num_citations": "46\n", "authors": ["1500"]}
{"title": "Migrating to Web services-latency and scalability\n", "abstract": " A common way of deploying Web services is to create proxies that expose the legacy application as a Web service. But when it comes to performance, Web services are facing the same barrage of distrust as any new middleware. Are the critics of Web services right? In this paper we will look at several performance pitfalls that Web services are facing today and at the performance penalties that have to be paid when exposing a legacy application as a Web service. We show results about the latency and scalability of Apache's implementation of SOAP, compare it with the performance of established middleware such as RMI, and look at end-to-end performance of Web services built on top of existing EJB applications.", "num_citations": "46\n", "authors": ["1500"]}
{"title": "Model-based performance testing: NIER track\n", "abstract": " In this paper, we present a method for performance testing of transactional systems. The methods models the system under test, finds the software and hardware bottlenecks and generate the workloads that saturate them. The framework is adaptive, the model and workloads are determined during the performance test execution by measuring the system performance, fitting a performance model and by analytically computing the number and mix of users that will saturate the bottlenecks. We model the software system using a two layers queuing model and use analytical techniques to find the workload mixes that change the bottlenecks in the system. Those workload mixes become stress vectors and initial starting points for the stress test cases. The rest of test cases are generated based on a feedback loop that drives the software system towards the worst case behaviour.", "num_citations": "44\n", "authors": ["1500"]}
{"title": "Pattern-based deployment service for next generation clouds\n", "abstract": " This paper presents a flexible deployment service for cloud computing. The service facilitates the specification and the execution of cloud deployment plans for applications. An application is described through a pattern, an abstract view that captures the logical view of the application and its mapping into cloud resources. The services instantiate the pattern in the cloud and allows for runtime updates of the deployment. The service is accessible through a RESTful interface. We identify the requirements for the service, describe its interfaces and show several case studies that capture the main features of the service.", "num_citations": "41\n", "authors": ["1500"]}
{"title": "Migrating to Web services: a performance engineering approach\n", "abstract": " In this paper we look at several performance pitfalls that Web services are facing today and at the performance penalties that have to be paid when exposing a legacy application as a Web service. We investigate two performance metrics of Web services, latency and scalability, and compare them with those of legacy middleware. The goal of the paper is to show how the performance penalties can be mitigated by following the principles and methods of performance engineering. Performance models can help the migration decisions, especially when new architectures or new deployment topologies are sought. The paper shows the mechanisms of building and solving a performance model involving Web services. An example is presented throughout the paper. Copyright \u00a9 2004 John Wiley & Sons, Ltd.", "num_citations": "40\n", "authors": ["1500"]}
{"title": "Designing adaptive applications deployed on cloud environments\n", "abstract": " Designing an adaptive system to meet its quality constraints in the face of environmental uncertainties can be a challenging task. In a cloud environment, a designer has to consider and evaluate different control points, that is, those variables that affect the quality of the software system. This article presents a methodology for designing adaptive systems in cloud environments. The proposed methodology consists of several phases that take high-level stakeholders\u2019 adaptation goals and transform them into lower-level MAPE-K loop control points. The MAPE-K loops are then activated at runtime using search-based algorithms. Our methodology includes the elicitation, ranking, and evaluation of control points, all meant to enable a runtime search-based adaptation. We conducted several experiments to evaluate the different phases of our methodology and to validate the runtime adaptation efficiency.", "num_citations": "39\n", "authors": ["1500"]}
{"title": "Replica placement in cloud through simple stochastic model predictive control\n", "abstract": " This paper presents a model and an algorithm for optimal service placement (OSP) of a set of N-tier software systems, subject to dynamic changes in the workload, Service Level Agreements (SLA), and administrator preferences. The objective function models the resources' cost, the service level agreements and the trashing cost. The optimization algorithm is predictive: its allocation or reallocation decisions are based not only on the current metrics but also on predicted evolution of the system. The solution of the optimization, in each step, is a set some service replicas to be added or removed from the available hosts. These deployment changes are optimal with regards to overall objectives defined over time. In addition, the optimization considers the restrictions imposed on the number of possible service migrations at each time interval. We present experimental results that show the effectiveness of our approach.", "num_citations": "39\n", "authors": ["1500"]}
{"title": "A web service for cloud metadata\n", "abstract": " Descriptive information about available cloud services (i.e., metadata) is required in order to make good decisions about which cloud service provider(s) to utilize when deploying an application topology to the cloud. Presently, there are no uniform mechanisms for describing these services. Further, there is no unifying process that aggregates this metadata from the set of cloud providers and makes it available to a user in a programmatic fashion from a single location. This paper presents a methodology for and an implementation of a service-oriented application that provides relevant metadata information describing offered cloud services via a uniform RESTful web service. The data provided by this service is automatically acquired and mapped to a standard ontology. Community members can submit performance benchmarks using a metrics agent that submits metrics via a web service. Several example\u00a0\u2026", "num_citations": "39\n", "authors": ["1500"]}
{"title": "Model-based adaptive dos attack mitigation\n", "abstract": " Denial of Service (DoS) attacks overwhelm online services, preventing legitimate users from accessing a service, often with impact on revenue or consumer trust. Approaches exist to filter network-level attacks, but application level attacks are harder to detect at the firewall. Filtering at this level can be computationally expensive and difficult to scale, while still producing false positives that block legitimate users. This paper presents a model-based adaptive architecture and algorithm for detecting DoS attacks at the web application level and mitigating them. Using a performance model to predict the impact of arriving requests, a decision engine adaptively generates rules for filtering traffic and sending suspicious traffic for further review, which may ultimately result in dropping the request or presenting the end user with a CAPTCHA to verify they are a legitimate user. Experiments performed on a scalable implementation\u00a0\u2026", "num_citations": "39\n", "authors": ["1500"]}
{"title": "A real-time adaptive control of autonomic computing environments\n", "abstract": " Autonomic computing has received a great deal of attention from the research community in recent years. Many techniques have been proposed to monitor, analyze, and change the system under observation, but less attention has been paid to adapting the autonomic computing loop itself. Based on previous results on the design and implementation of a reference real-time architecture for autonomic computing, a self-adapting loop based on system-specific adaptation knowledge, which includes the types and properties of autonomic computing components, behavioural constraints, and strategies for adaptation is proposed in this paper. The proposed system is an integral part of the real-time system that controls the behaviour of the computing environment, evaluating its global behaviour using criteria that take into account the mathematical description of the time variation of the number of users in the system\u00a0\u2026", "num_citations": "38\n", "authors": ["1500"]}
{"title": "An architecture for overlaying private clouds on public providers\n", "abstract": " Organizations shifting to a public cloud infrastructure face potential hurdles regarding control and security, and must acquire a new set of best practices regarding developing and deploying to a cloud infrastructure. We propose a reference architecture for a virtual private cloud built on cross-provider ondemand compute instances, with a set of components, services, and algorithms to produce a managed platform that reduces the level of trust required for infrastructure-as-a-service (IaaS) providers, increases control and isolation, improves security and data protection, and allows architects, developers, and operations staff to deploy applications to the cloud using their existing body of knowledge and best practices. Two concrete architectures based on this reference are presented, and a prototype implementation is described and tested.", "num_citations": "36\n", "authors": ["1500"]}
{"title": "Hogna: A platform for self-adaptive applications in cloud environments\n", "abstract": " We propose Hogna, a platform for deploying self-managing web applications on cloud. The platform enables the deployment of the applications based on the automation of a set of operations (starting instances, installing necessary software and configuring the instances, etc.), and then the continuous monitoring of the health of the applications. The gathered monitoring data is analyzed using a performance model and an action plan is created and executed. Any components involved (for monitoring, analyzing, planning and deployment changes) can be customized to fit the needs of the application and/or researcher.", "num_citations": "35\n", "authors": ["1500"]}
{"title": "Sipresk: A big data analytic platform for smart transportation\n", "abstract": " In this paper, we propose a platform for performing analytics on urban transportation data to gain insights into traffic patterns. The platform consists of data, analytics and management layers and it can be leveraged by overlay traffic-related applications or directly by researchers, traffic engineers and planners. The platform is cluster-based and leverages the cloud to achieve reliability, scalability and adaptivity to the changing operating conditions. It can be leveraged for both on-line and retrospective analysis. We validated several use cases such as finding average speed and congested segments in the major highways in Greater Toronto Area (GTA).", "num_citations": "34\n", "authors": ["1500"]}
{"title": "Fuzzy scheduling with application to real-time systems\n", "abstract": " Task scheduling is a main activity in the design of real-time systems (RTS). It assures both functionality and safety of such systems. RTS can be modeled as a set of periodic tasks that must be completed before specific deadlines. In this paper, we investigate the fuzzy scheduling models on RTS and the main methodologies that solve these models. Thus, we present general periodic task scheduling models with fuzzy deadlines and fuzzy processing times; scheduling algorithms based on optimal assignment of the priorities; and a more general framework for designing RTS, Rate Monotonic Scheduling Theory, that includes the scheduling algorithms. A case study will illustrate the use of the theory.", "num_citations": "31\n", "authors": ["1500"]}
{"title": "Designing autonomic management systems for cloud computing\n", "abstract": " Autonomic Computing Systems are systems which are capable of adapting themselves to changes in their working environment in order to maintain required service level agreements, protect the execution of the system from external attacks or prevent and recover from failures. Within the field of autonomic computing, autonomic systems are developed as control loops which monitor and analyze the execution of the system and then plan and execute changes if needed in order to adapt the system to its environment. This paper will present an approach for designing and building autonomic systems for cloud computing, based on an architecture previously developed which was rooted in real-time software patters. Furthermore, the paper presents an application of the autonomic management architecture to a cluster of application servers running on top of a cloud. It is thus demonstrated how the development\u00a0\u2026", "num_citations": "28\n", "authors": ["1500"]}
{"title": "Real time task scheduling allowing fuzzy due dates\n", "abstract": " The paper deals with periodical task scheduling. The tasks are described by fuzzy due dates and fuzzy execution times. The goal of scheduling is to find an optimal assignment of priorities such that the satisfaction associated with due dates and execution times be minimized. The paper shows how the rules associated with task priorities improve the optimal assignment search.", "num_citations": "28\n", "authors": ["1500"]}
{"title": "Towards a real-time reference architecture for autonomic systems\n", "abstract": " Autonomic computing aims to embed automation in IT management software such that it can adapt to changes in the configuration, provisioning, protection, and resource utilization variations of the IT infrastructure at run time. It is, therefore, almost natural to consider this control software framework as being designed with control principles in mind. One of the research trends considers autonomic computing as a control system that resolves constraints related to the optimal usage of resources based on external requests made by users or processes in a reactive way. In this paper, a real-time reference architecture is introduced in which components implementing functions of realtime system elements or blocks such as transducers, controllers, and actuators are designed. The architecture of the autonomic computing software also contains components that implement functionalities specific to real-time systems. The\u00a0\u2026", "num_citations": "27\n", "authors": ["1500"]}
{"title": "Managing a SaaS application in the cloud using PaaS policy sets and a strategy-tree\n", "abstract": " This paper introduces a framework and a methodology to manage a SaaS application on top of a PaaS infrastructure. This framework utilizes PaaS policy sets to implement the SaaS providers elasticity policy for its application server tier. A strategy-tree is utilized at the SaaS layer to actively guide policy set selection at runtime in order to maintain alignment with the SaaS providers business objectives. Results from an experiment conducted on a real cloud are presented in support of this approach.", "num_citations": "25\n", "authors": ["1500"]}
{"title": "A robust autonomic computing architecture for server virtualization\n", "abstract": " The increasing heterogeneity and complexity of the Information Technology infrastructure encompassing hardware and software applications, services and networks, led to complex, unmanageable and insecure systems. Correspondingly, new approaches in regards to the IT infrastructure automation and operation have been investigated. Autonomic computing has been born and successfully launched as a research area in response to the \"must have\" solutions for the problems above. It approaches subjects such as self-configuration, self- protection, self-healing, and self-optimization. Its ambitious goals in making a computer environment behaving in a self controlled and intelligently guided manner, lead to various approaches such as intelligent systems, control, learning machines and others. In this paper, a special adaptive technique will be used for the implementation of self-provisioning and self- optimization\u00a0\u2026", "num_citations": "24\n", "authors": ["1500"]}
{"title": "Visualizing flow diagrams in websphere studio using shrimp views\n", "abstract": " This paper describes the integration of an information visualization tool, called SHriMP Views, with IBM WebSphere Studio Application Developer Integration Edition, which was developed with Eclipse technology. Although SHriMP was originally developed for visualizing programs, it is content-independent. We have re-targeted SHriMP for visualizing flow diagrams. Flow diagrams, as supported by WebSphere Studio Application Developer Integration Edition, can be hierarchically composed, thus leveraging the key features of SHriMP that allow a user to easily navigate hierarchically composed information spaces. We discuss the differences between programs and flow diagrams, in terms of their semantics and their visual representation. We also report on the main technical challenges we faced, due to the different widget sets used by SHriMP (Swing/AWT) and Eclipse (SWT).", "num_citations": "24\n", "authors": ["1500"]}
{"title": "Automating discovery of software tuning parameters\n", "abstract": " Software Tuning Panels for Autonomic Control (STAC) is a project to assist in the integration of existing software into autonomic frameworks. It works by identifying tuning parameters and rearchitecting to expose them as a separate control panel module. The project poses three distinct research challenges: automating the identification of tuning parameters, rearchitecting to centralize and expose them, and combining these two capabilities to facilitate the integration of existing software into autonomic frameworks. Our previous work focused on the second problem, automating the rearchitecture to expose and isolate tuning parameters. In this paper we concentrate on the first problem, automating the identification of tuning parameters. We begin with an empirical study of documented tuning parameters in a number of open source applications. From our observations of these known tuning parameters, we create a\u00a0\u2026", "num_citations": "23\n", "authors": ["1500"]}
{"title": "Customizing lotus notes to build software engineering tools.\n", "abstract": " Many software engineering research tools are stand-alone applications that have trouble interoperating with other development tools and do not fit well into the software developers\u2019 established work processes. Our main hypothesis is that in order for new tools to be adopted effectively, they must be compatible with both existing users and existing tools. Typically, software engineering teams in an organization share a set of common applications for their development activities that are a permanent part of each developer\u2019s everyday workflow. Among these applications are shrink-wrapped office tools such as Lotus Notes, which are used for, among other tasks, email, scheduling, and project reports and presentations. These office tools, are highly integrated and offer a mature, well-tested working environment, which can be customized to an extent that allows to provide support for advanced software engineering tasks. This paper describes RENotes, a reverse engineering tool built by customizing Lotus Notes. RENotes targets software developers who use Notes as part of their work environment. We describe Notes\u2019 features and how they can be leveraged to layer new reverse engineering functionality on top.", "num_citations": "23\n", "authors": ["1500"]}
{"title": "Scalable adaptive web services\n", "abstract": " Software as a service creates the possibility of composing software applications from web services spread across different application domains. To guarantee certain quality of services of the composite service, one can think of two paths ahead: quality of service negotiation and guarantee prior to service deployment and bindings; or a more speculative and adaptive behavior at runtime. In this position paper we propose a hybrid approach, combining development and runtime information to make the web services adapt to workload variations. The approach combines control theory with performance modeling and is built around a model of the web service. A control loop theory approach is taken to model discovery. The control loop allows for keeping the web service's performance even when the model is not completely known and failure of components of the control loop are likely to happen. The approach is related\u00a0\u2026", "num_citations": "22\n", "authors": ["1500"]}
{"title": "CAAMP: Completely automated DDoS attack mitigation platform in hybrid clouds\n", "abstract": " Distributed Denial of Service (DDoS) attacks are one of the main concerns for online service providers because of their impact on cost/revenue and reputation. This paper presents Completely Automated DDoS Attack Mitigation Platform (CAAMP), a novel platform to mitigate DDoS attacks on public cloud applications using capabilities of software defined infrastructure and network function virtualization techniques. When suspicious traffic is identified, CAAMP deploys a copy of the application's topology on-the-fly (a shark tank) on an isolated environment in a private cloud. It then creates a virtual network that will host the shark tank. Software defined networking (SDN) controller programs the virtual switches dynamically to redirect the suspicious traffic to the shark tank until final decision is made. If traffic is proved to be non-malicious, SDN controller installs flow rules on the switches to redirect the traffic back to the\u00a0\u2026", "num_citations": "20\n", "authors": ["1500"]}
{"title": "A decentralized autonomic architecture for performance control in the cloud\n", "abstract": " In this paper, we introduce a decentralized autonomic architecture for multi-tier applications deployed in cloud environments. The architecture maintains the application's service level objective at a predefined level and, implicitly, reduces the cost. The architecture uses a series of autonomic controllers, in which each controller independently regulates a tier of the application. The architecture utilizes feedback loops and implements Proportional, Integrative and Derivative control laws at each autonomic controller. A prototype is described and an initial set of experiments is conducted on a public commercial cloud. The experiments demonstrate the effectiveness of this approach at maintaining a service level objective through the decomposition of an application's aggregate performance into its set of discretely managed component tiers.", "num_citations": "20\n", "authors": ["1500"]}
{"title": "Business process performance prediction on a tracked simulation model\n", "abstract": " Business processes need to achieve key performance indicators with minimum resources in changing operating conditions. Changes include hardware and software failures, load variation and variations in user interaction with the system. By incorporating simulation in the prediction model it is possible to predict with more confidence system performance degradations. We present our dynamic predictive model which uses forecasting techniques on historical process performance estimates for business process optimization. The parameters of the simulation model are estimates tuned at run-time by tracking the system with a particle filter.", "num_citations": "20\n", "authors": ["1500"]}
{"title": "Designing search based adaptive systems: a quantitative approach\n", "abstract": " Designing an adaptive system to meet its quality constraints in the face of environmental uncertainties can be a challenging task. In cloud environment, a designer has to also consider and evaluate different control points, ie, those variables that affect the quality of the software system. This paper presents a method for eliciting, evaluating and ranking control points for web applications deployed in cloud environments. The proposed method consists of several phases that take high-level stakeholders' adaptation goals and transform them into lower level MAPE-K loop control points. The MAPE-K loops are then activated at runtime using search-based algorithms. We conducted several experiments to evaluate the different phases of our methodology.", "num_citations": "18\n", "authors": ["1500"]}
{"title": "A performance evaluation framework for web applications\n", "abstract": " Performance engineering for Web applications must take into account both the development and runtime information about the target system and its environment. At development time, the architects have to choose from many architecture styles and consider all performance requirements across a multitude of workloads. At runtime, an Autonomic Manager has to compensate for the changing operating and environment conditions not accounted for at the design time and make decisions about changes in the architecture so the performance requirements are met. This paper proposes a formal framework called Software Performance for Autonomic Computing for making decisions with regard to a possible set of candidate architectures: usage scenarios are criteria according to which architectures are evaluated; actual performance metrics, such as response time or throughput, are obtained by solving performance\u00a0\u2026", "num_citations": "17\n", "authors": ["1500"]}
{"title": "Observability and controllability of autonomic computing systems for composed web services\n", "abstract": " Autonomic Computing is a research area whose aim is to embed \u201cintelligent algorithms\u201d in the IT infrastructure management software such that it can adapt to changes in regards to the configuration, provisioning, external attacks, and resource utilization variations at run time. It is therefore, almost natural to consider this IT infrastructure control software framework as being designed upon methods and technologies used for the design of control systems. In this paper the control system design methodology is extended to the analysis of the intrinsic properties of the autonomic system itself. Thus the controllability and observability properties of the computing process itself are defined and examined in more details. These properties are also investigated for the case of cloud services where the serial and parallel composition of these services is considered. These cloud based services are connected through cooperation\u00a0\u2026", "num_citations": "17\n", "authors": ["1500"]}
{"title": "Autonomic computing control of composed web services\n", "abstract": " Software as a service (SaaS) can be delivered by composing software applications using web services hosted in one or more administrative domains. In this context, web service composition has a considerable impact on the delivered service affecting its quality (QoS). There is therefore a need to keep the service QoS parameters under control when the service is delivered by a combination of different web services. This paper investigates the composition of web services, and its implications on the overall QoS guarantees as represented by the service response times, when autonomic computing control mechanisms are in place. A control based approach of the autonomic computing concept is used to investigate the dynamic composition of web services when the service is provided by a set of web services linked via cooperation protocols that define a global process choreography. Web services are modeled as\u00a0\u2026", "num_citations": "17\n", "authors": ["1500"]}
{"title": "Integrating SHriMP with the IBM websphere studio workbench\n", "abstract": " This paper provides an experience report for researchers who are interested in integrating their tools with the new IBM WebSphere Studio Workbench. The Workbench (open source at www. eclipse. org) provides an open framework for building integrated development environments. We report on our experience integrating an information visualization tool (called SHriMP Views) with the IBM Workbench. Although SHriMP was originally developed for visualizing programs, it is content independent. We have re-targeted SHriMP for visualizing flow diagrams. Flow diagrams can be hierarchically composed, thus leveraging the key features of SHriMP that allow a user to easily navigate hierarchically composed information spaces. We discuss the differences between programs and flow diagrams both in terms of their semantics and in their visual representation. Terminals, which are a first-class entity that mediate between nodes and arcs in flow diagrams, presented the main challenges here. We also report on the main technical challenges we faced, due to the different widgets sets used by SHriMP (Swing/awt) and the Workbench (swt).", "num_citations": "16\n", "authors": ["1500"]}
{"title": "Object allocation for distributed applications with complex workloads\n", "abstract": " The architects of today\u2019s distributed applications have a wide range of Internet technologies, platforms, and design patterns to choose from. In addition to the usual selection criteria of security, portability, maintainability, and cost, performance often determines the selection of one system architecture over another. This paper presents a quantitative technique that can help an architect understand the expected behaviour of an application deployed within a target environment. The technique automatically finds an object allocation that optimizes a performance metric specified by the architect. The technique supports multiple classes of requests and mean response time requirements for multiple workload conditions. Capacity constraints are also considered. These include device utilization limits and the maximum number of customers. A deployed application is described using a Layered Queuing Model (LQM\u00a0\u2026", "num_citations": "16\n", "authors": ["1500"]}
{"title": "Towards a multi-cluster analytical engine for transportation data\n", "abstract": " In the new digital age, the pace and volume of growing transportation related data is exceeding our ability to manage and analyze it. In this position paper, we present a data engine, Godzilla, to ingest real-time traffic data and support analytic and data mining over traffic data. Godzilla is a multi-cluster approach to handle large volumes of growing data, changing workloads and varying number of users. The data originates at multiple sources, and consists of multiple types. Meanwhile, the workloads belong to three camps, namely batch processing, interactive queries and graph analysis. Godzilla support multiple language abstractions from scripting to SQL-like language.", "num_citations": "15\n", "authors": ["1500"]}
{"title": "Mitigating dos attacks using performance model-driven adaptive algorithms\n", "abstract": " Denial of Service (DoS) attacks overwhelm online services, preventing legitimate users from accessing a service, often with impact on revenue or consumer trust. Approaches exist to filter network-level attacks, but application-level attacks are harder to detect at the firewall. Filtering at this level can be computationally expensive and difficult to scale, while still producing false positives that block legitimate users. This article presents a model-based adaptive architecture and algorithm for detecting DoS attacks at the web application level and mitigating them. Using a performance model to predict the impact of arriving requests, a decision engine adaptively generates rules for filtering traffic and sending suspicious traffic for further review, where the end user is given the opportunity to demonstrate they are a legitimate user. If no legitimate user responds to the challenge, the request is dropped. Experiments performed on\u00a0\u2026", "num_citations": "15\n", "authors": ["1500"]}
{"title": "Navigating the clouds with a MAP\n", "abstract": " This paper presents a management platform referred to as the Morphable Applications Platform (MAP) that supports the deployment and management of applications on a multi-cloud. Specifically, MAPs support two dimensions of adaptation not addressed previously: application-informed request routing and transposition across clouds. Specifically, a cluster is automatically moved from one datacenter to a second datacenter without service interruption and while preserving its state. A MAP represents the creation of a personal cloud fabric across a set of cloud providers. We have implemented a MAP on the public cloud and used it to deploy and manage an application.", "num_citations": "14\n", "authors": ["1500"]}
{"title": "Business process adaptation on a tracked simulation model\n", "abstract": " Business processes need to adapt to changes in the operating conditions and to meet the service-level agreements (SLAs) with a minimum of resources. Changes in operating conditions include hardware and software failures, load variation and variations in user interaction with the system. An integral component to adaptation is the awareness over the behavior of self and environment (or having an estimation of the current situation). Aiming at estimation, this paper investigates the automatic building of a dynamic predictive model of the business process that is used for business process optimization. The model is a simulation model whose parameters are tuned at run time by tracking the system with a particle filter.", "num_citations": "14\n", "authors": ["1500"]}
{"title": "Building automation system-BIM integration using a linked data structure\n", "abstract": " Buildings Automation Systems (BAS) are ubiquitous in contemporary buildings, both monitoring building conditions and managing the building system control points. At present, these controls are prescriptive and pre-determined by the design team, rather than responsive to actual building performance. These are further limited by prescribed logic, possess only rudimentary visualizations, and lack broader system integration capabilities. Advances in machine learning, edge analytics, data management systems, and Facility Management-enabled Building Information Models (FM-BIMs) permit a novel approach: cloud-hosted building management. This paper presents an integration technique for mapping the data from a building Internet of Things (IoT) sensor network to an FM-BIM. The sensor data naming convention and time-series analysis strategies integrated into the data structure are discussed and presented\u00a0\u2026", "num_citations": "13\n", "authors": ["1500"]}
{"title": "K-Feed-a data-oriented approach to application performance management in cloud\n", "abstract": " This paper presents K-Feed (Knowledge Feed), a platform for real-time application performance analysis and provisioning in the cloud. K-Feed can perform at scale monitoring, analysis, and provisioning of cloud applications. We explain the components, the implementation and the validation of the K-feed platform. To illustrate its feasibility, we use it to monitor and build a performance model of a clustered web application. To model the application, we use off the shelf components.", "num_citations": "13\n", "authors": ["1500"]}
{"title": "Optimizing serverless computing: introducing an adaptive function placement algorithm\n", "abstract": " The main concept behind serverless computing is to build and run applications without the need for server management. It refers to a fine-grained deployment model where applications, comprising of one or more functions, are uploaded to a platform and then executed, scaled, and billed in response to the exact demand needed at the moment. While elite cloud vendors such as Amazon, Google, Microsoft, and IBM are now providing serverless computing, their approach for the placement of functions, ie associated container or sandbox, on servers is oblivious to the workload which may lead to poor performance and/or higher operational cost for software owners. In this paper, using statistical machine learning, we design and evaluate an adaptive function placement algorithm which can be used by serverless computing platforms to optimize the performance of running functions while minimizing the operational cost\u00a0\u2026", "num_citations": "12\n", "authors": ["1500"]}
{"title": "Systems and methods of precision sharing of big data\n", "abstract": " An ecosystem that allows fine-grained multi-party control over access to information stored in one or more data sources of a data provider. A requesting party can submit a query job to the data provider. Resellers in the chain can introduce their own query modifiers to the query job, adding additional data access, data transformation and segmentation functions to the query job. The data provider can append its own query modifier to the query job and execute the query job with all of the query modifiers. Access control for each link in the chain is checked before the query modifier for the link is executed. After execution of all query modifiers and the query job, the results can be provided to the requesting party.", "num_citations": "12\n", "authors": ["1500"]}
{"title": "Toward an ecosystem for precision sharing of segmented big data\n", "abstract": " As the amount of data created and stored by organizations continues to increase, attention is turning to extracting knowledge from that raw data, including making some data available outside of the organization to enable crowd analytics. The adoption of the MapReduce paradigm has made processing Big Data more accessible, but is still limited to data that is currently available, often only within an organization. Fine-grained control over what information is shared outside an organization is difficult to achieve with Big Data, particularly in the MapReduce model. We introduce a novel approach to sharing that enables fine-grained control over what data is shared. Users submit analytics tasks that run on infrastructure near the actual data, reducing network bottlenecks. Organizations allow access to a logical version of their data created at runtime by filtering and transforming the actual data without creating storage\u00a0\u2026", "num_citations": "12\n", "authors": ["1500"]}
{"title": "Optimizing resources in cloud, a SOA governance view\n", "abstract": " This position paper proposes a governance model for efficient power and computing resources usage in cloud computing environments. We show how optimization of resource allocation can achieve substantial cost reduction. Then we show the governance that needs to be in place and the steps that ensure an efficient optimization.", "num_citations": "12\n", "authors": ["1500"]}
{"title": "Adaptation as a service.\n", "abstract": " Current and emerging complex systems of many types including but not limited to big data systems, web-based systems, data centers and cloud infrastructure, social networks and the Internet of Things (IoT) have increasingly distributed and dynamic architecture that provide unprecedented flexibility in creating and supporting applications. However, such highly distributed architecture also increases the complexity of end-to-end management of such systems. Due to the sheer complexity, uncertainty and at the same time programmability of cloud environments, microservices and finally big data analytics, it is now required, and possible, to enable autonomic management in distributed systems in a dependable manner. In this paper, we argue that building autonomic management systems is a challenging task and requires its own set of expertise and knowledge. Therefore, in the light of current challenges, available enablers and recent successful stories, we propose the idea of moving from self-adaptation to ADaptation-as-a-Service (ADaaS).", "num_citations": "11\n", "authors": ["1500"]}
{"title": "Identifying implicitly declared self-tuning behavior through dynamic analysis\n", "abstract": " Autonomic computing programming models explicitly address self management properties by introducing the notion of ldquoAutonomic Element. However, most of currently developed systems do not employ autonomic self-managing programming paradigms. Thus, a current challenge is to find mechanisms to identify the self-tuning behavior and self-tuning parameters which have implicitly been declared using non-autonomic elements, and to expose them for monitoring or to an analysis framework. Static analysis, although it shows a good potential, it results in many false positives. In this paper, we provide a mechanism to identify the tuning parameters more accurately through dynamic analysis.", "num_citations": "11\n", "authors": ["1500"]}
{"title": "Dynamic task scheduling in distributed real time systems using fuzzy rules\n", "abstract": " This paper addresses scheduling on distributed real time systems. A distributed scheduling algorithm is presented. It consists of a guarantee routine, bidder algorithm and a decision maker. The guarantee routine checks whether a newly arrived task is schedulable or not. In order to make it suitable for distributed allocation and scheduling, an optimal guarantee routine is analyzed and extended. A node that cannot guarantee a task, asks for bidding and a decision algorithm evaluates the offers. Bidders send incomplete information about their states and, to handle this, the decision algorithm makes use of fuzzy rules to dispatch the unguaranteed task.", "num_citations": "11\n", "authors": ["1500"]}
{"title": "A data platform for the highway traffic data\n", "abstract": " Both short and long term information of the transportation network is needed by commuters and planners. In order to obtain this information, there is a pressing need to consolidate, mine and analyze data collected from multiple sources. To enable these activities under a single umbrella, we propose a data platform in this position paper that transforms data into information. Finally, we discuss the research challenges facing our platform.", "num_citations": "10\n", "authors": ["1500"]}
{"title": "A runtime cloud efficiency software quality metric\n", "abstract": " This paper introduces the Cloud Efficiency (CE) metric, a novel runtime metric which assesses how effectively an application uses software-defined infrastructure. The CE metric is computed as the ratio of two functions: i) a benefit function which captures the current set of benefits derived from the application, and ii) a cost function which describes the current charges incurred by the application's resources. We motivate the need for the CE metric, describe in further detail how to compute it, and present experimental results demonstrating its calculation.", "num_citations": "10\n", "authors": ["1500"]}
{"title": "Model-driven elasticity and dos attack mitigation in cloud environments\n", "abstract": " Workloads for web applications can change rapidly. When the change is an increase in customers, a common adaptive approach to maintain SLAs is elasticity, the on-demand allocation of computing resources. However, application-level denial-of-service (DoS) attacks can also cause changes in workload, and require an entirely different response. These two issues are often addressed separately (in both research and application). This paper presents a model-driven adaptive management mechanism which can correctly scale a web application, mitigate a DoS attack, or both, based on an assessment of the business value of workload. This approach is enabled by modifying a layered queuing network model previously used to model data centers to also accurately predict short-term cloud behavior, despite cloud variability over time. We evaluate our approach on Amazon EC2 and demonstrate the ability to horizontally scale a sample web application in response to an increase in legitimate traffic while mitigating multiple DoS attacks, achieving the established performance goal.", "num_citations": "10\n", "authors": ["1500"]}
{"title": "Method and tool for business process adaptation using goal modeling and analysis\n", "abstract": " A business process (BP) adaptation system (400) includes a High-Variability enriched Goal Model (HVGM)(402) that captures and refines goals of a business process (BP) while modeling alternative options where the model captures non-functional or quality attributes used in an evaluation of a performance of the BP and an estimation of how various BP alternatives affect the quality attributes. The system further includes a High-Variability workflow-level/directly Executable Model (HVEM)(404), where the system is based on goal modeling and analysis for eliciting intentions behind a BP to achieve a desired goal and the HVGM explicitly models non-functional or quality concerns. The system can further include a semi-automatic generator (604) of BP metrics based on the quality attributes specified in the HVGM and a runtime infrastructure (610) where each deployed BP instance reflects a configuration selected for\u00a0\u2026", "num_citations": "10\n", "authors": ["1500"]}
{"title": "Model-driven engineering for autonomic provisioned systems\n", "abstract": " Autonomic systems received lately a great deal of interest from the research and industrial communities due to their ability to configure, optimize, heal and protect themselves with little to no human intervention. Such systems must be able to analyze themselves and their environment in order to determine how best they can achieve the high-level goals and policies given to them by system managers. Among many approaches to this subject, real-time control loops have imposed themselves due to the direct mapping of their components onto the components of the generic autonomic computing architecture. In this paper we explore the model-driven approach to the development of real-time architecture for autonomic computing for a self-optimization scenario. This leads to the synthesis of a platform independent model (PIM) for generic autonomic computing systems and a platform specific model (PMS) for a specific\u00a0\u2026", "num_citations": "10\n", "authors": ["1500"]}
{"title": "Performance Modeling of Microservice Platforms\n", "abstract": " Microservice architecture has transformed the way developers are building and deploying applications in the nowadays cloud computing centers. This new approach provides increased scalability, flexibility, manageability, and performance while reducing the complexity of the whole software development life cycle. The increase in cloud resource utilization also benefits microservice providers. Various microservice platforms have emerged to facilitate the DevOps of containerized services by enabling continuous integration and delivery. Microservice platforms deploy application containers on virtual or physical machines provided by public/private cloud infrastructures in a seamless manner. In this paper, we study and evaluate the provisioning performance of microservice platforms by incorporating the details of all layers (i.e., both micro and macro layers) in the modelling process. To this end, we first build a\u00a0\u2026", "num_citations": "9\n", "authors": ["1500"]}
{"title": "A smart testing framework for iot applications\n", "abstract": " The number of IoT devices has been growing exponentially as new products are developed and legacy systems become Internet enabled. As a consequence, the large amount of traffic generated by IoT devices require new approaches to network architecture design. The primary challenge with IoT devices is that the traffic can be highly variable due to the device type and the time of use. In order to maintain Quality of Service standards in a dynamic IoT network, these traffic patterns need to be modeled and understood so that we can adapt the architecture dynamically to maintain a desired level of Quality of Service. To this effect, we propose a Smart Testing Framework that can detect bottlenecks and predict the demand for computing resources in a dynamic IoT network. Results obtained from using our framework indicate that we can predict the demand for computing resources in a dynamic IoT network with a high\u00a0\u2026", "num_citations": "9\n", "authors": ["1500"]}
{"title": "Towards a cloud optimization architecture using strategy-trees\n", "abstract": " Cloud computing represents an emerging approach to on-demand computing. This paper proposes an extension to a three-layered cloud computing architecture through the use of strategy-trees. Managers at each layer of the cloud architecture, representing a provider\u2019s perspective, will utilize strategy-trees to implement feedback loops to achieve sets of objectives over defined horizons of time.", "num_citations": "9\n", "authors": ["1500"]}
{"title": "Evaluating Adaptation Methods for Cloud Applications: An Empirical Study\n", "abstract": " Web software systems generally reside in highly volatile environments, their incoming traffic may be subject to sharp fluctuations from reasons that cannot always be captured or predicted. Cloud computing provides a solution to this problem by offering flexible resources, like containers, which can be quickly and easily scaled according to the current workload needs. Automating this process is a key aspect for the management of modern web software systems, and there is a plethora of methods to implement autonomic management systems. In this work, we review three of these methods, a threshold-based approach, a control-based approach and a model-based approach. We design and run a number of experiments for all three systems with different workloads to evaluate their ability to manage the software system and how well they do so. Our experiments were conducted on the Amazon EC2 cloud with Docker\u00a0\u2026", "num_citations": "8\n", "authors": ["1500"]}
{"title": "Promotion of features in reusable software component types\n", "abstract": " In the process of hierarchical composition of software component types, the reusability of software component types is improved through the \u201cpromotion of features\u201d. That is, a feature of an instance of a predetermined software component type may be promoted to a software component type containing instances of the predetermined software component type. The promoted feature may then be customized when the containing software component type is instantiated.", "num_citations": "8\n", "authors": ["1500"]}
{"title": "Distributed clouds for collaborative applications\n", "abstract": " With the advent of social networking and the appearance of Web 2.0, collaborative applications which allow users to share data online, often in real-time, have gained increasing prominence. Whether for sharing images, sharing videos, or even sharing live gaming sessions, such applications must deal with session sizes from tens to tens of thousands of people. However, existing products have not been able to provide a scalable cloud-based system that synchronizes disparate web content among many users. Such a goal is desired in order to provide the benefits of cloud deployments to collaborative applications. Many such applications cannot predict the number of connections which they may need to handle. As such, applications must either provision a higher number of servers in anticipation of more traffic, or be faced with a degradation of the user experience when a large number of clients connect to the\u00a0\u2026", "num_citations": "7\n", "authors": ["1500"]}
{"title": "Method for solving application failures using social collaboration\n", "abstract": " A computer-implemented method, system and computer usable program code for solving an application failure using social collaboration are provided. A search request to search a central repository of knowledge is received. The search request comprises a user identity and an application failure problem to be solved. The central repository of knowledge comprises data regarding attempts to solve an application failure problem compiled from registered users of the central repository of knowledge. A determination is made as to whether the application failure problem to be solved exists in the central repository. If the application failure problem to be solved exists within the central repository, search results for previous attempts at solving the application failure problem are collected. The search results are ranked based on a frequency of access and feedback from users and are grouped according to social groupings\u00a0\u2026", "num_citations": "7\n", "authors": ["1500"]}
{"title": "4th international workshop on adoption-centric software engineering\n", "abstract": " The ACSE series of events aims to advance the adoption of software engineering tools and techniques by bringing together researchers and practitioners who investigate novel approaches to fostering the transition between limited-use research prototypes and broadly applicable practical solutions. One proven technique to aid adoption is to leverage existing commercial platforms and infrastructure. The key objective of ACSE 2004 is to explore innovative approaches to the adoption of proofof- concept systems by embedding them in extensions of Commercial Off-The-Shelf (COTS) products and/or using middleware technologies to integrate the prototypes into existing toolsets.", "num_citations": "7\n", "authors": ["1500"]}
{"title": "IP Spoofing in and out of the public cloud: from policy to practice\n", "abstract": " In recent years, a trend that has been gaining particular popularity among cybercriminals is the use of public Cloud to orchestrate and launch distributed denial of service (DDoS) attacks. One of the suspected catalysts for this trend appears to be the increased tightening of regulations and controls against IP spoofing by world-wide Internet service providers (ISPs). Three main contributions of this paper are (1) For the first time in the research literature, we provide a comprehensive look at a number of possible attacks that involve the transmission of spoofed packets from or towards the virtual private servers hosted by a public Cloud provider.(2) We summarize the key findings of our research on the regulation of IP spoofing in the acceptable-use and term-of-service policies of 35 real-world Cloud providers. The findings reveal that in over 50% of cases, these policies make no explicit mention or prohibition of IP spoofing, thus failing to serve as a potential deterrent.(3) Finally, we describe the results of our experimental study on the actual practical feasibility of IP spoofing involving a select number of real-world Cloud providers. These results show that most of the tested public Cloud providers do a very good job of preventing (potential) hackers from using their virtual private servers to launch spoofed-IP campaigns on third-party targets. However, the same very own virtual private servers of these Cloud providers appear themselves vulnerable to a number of attacks that involve the use of spoofed IP packets and/or could be deployed as packet-reflectors in attacks on third party targets. We hope the paper serves as a call for awareness and action and\u00a0\u2026", "num_citations": "6\n", "authors": ["1500"]}
{"title": "Adaptive service management for cloud applications using overlay networks\n", "abstract": " This paper presents an adaptive service management mechanism that maintains service level agreement through use of overlay networks that are deployed over the cloud provider network. The application autonomic manager strives to maintain the SLA without provisioning new resources for as long as possible. Through continuous monitoring and analysis, autonomic manager uses software defined networking (SDN) to dynamically apply policies to the flows of requests that travel through the application components. We implement and evaluate the proposed method on a hybrid cloud environment. Through extensive experiments, we show that the management mechanism can successfully maintain the SLA of services while it avoids provisioning extra resources which is the common approach in cloud.", "num_citations": "6\n", "authors": ["1500"]}
{"title": "On efficiency and scalability of software-defined infrastructure for adaptive applications\n", "abstract": " This paper proposes and evaluates a novel analytical performance model to study the efficiency and scalability of software-defined infrastructure (SDI) to host adaptive applications. The SDI allows applications to communicate their adaptation requirements at run-time. Adaptation scenarios require computing and networking resources to be provided to applications in a timely manner to facilitate seamless service delivery. Our analytical model yields the response time of realizing adaptations on the SDI and reveals the scalability limitations. We conduct extensive testbed experiments on a cloud environment to verify the accuracy and fidelity of the model. Cloud service providers can leverage the proposed model to perform capacity planning and bottleneck analysis when they accommodate adaptive applications.", "num_citations": "6\n", "authors": ["1500"]}
{"title": "Smart applications on virtual infrastructure\n", "abstract": " Smart Applications on Virtual Infrastructure (SAVI) research is focused on the design of future application platforms built on flexible, versatile and evolvable infrastructure that can readily deploy, maintain, and retire the large-scale, possibly short-lived, distributed applications that are the future of software systems.", "num_citations": "6\n", "authors": ["1500"]}
{"title": "A performance engineering method for web applications\n", "abstract": " Performance engineering for informational and transactional distributed systems must take into account both the development and runtime information about the target system and its environment. At development time, the architects have to choose from many architecture styles and consider all performance requirements across a multitude of workload. At runtime, an Autonomic Manager has to compensate for changing operating and environment conditions not accounted for at the design time and make decisions about changes in architecture so the performance requirements are met. This paper proposes a formal framework, SPAC, for making decisions with regard to a possible set of candidate architectures: usage scenarios are criteria according to which architectures are evaluated; actual performance metrics, such as response time or throughput, are obtained by solving performance models and then matched\u00a0\u2026", "num_citations": "6\n", "authors": ["1500"]}
{"title": "Web service distributed management framework for autonomic server virtualization\n", "abstract": " Virtualization for the x86 platform has imposed itself recently as a new technology that can improve the usage of machines in data centers and decrease the cost and energy of running a high number of servers. Similar to virtualization, autonomic computing and more specifically self-optimization, aims to improve server farm usage through provisioning and deprovisioning of instances as needed by the system. Autonomic systems are able to determine the optimal number of server machines - real or virtual - to use at a given time, and add or remove servers from a cluster in order to achieve optimal usage. While provisioning and deprovisioning of servers is very important, the way the autonomic system is built is also very important, as a robust and open framework is needed. One such management framework is the Web Service Distributed Management (WSDM) system, which is an open standard of the\u00a0\u2026", "num_citations": "6\n", "authors": ["1500"]}
{"title": "A performance engineering tool and method for distributing applications.\n", "abstract": " Recent advances in distributed object and Internet technologies have made it attractive for organizations to distribute application functions. Typical projects include: the re-hosting of legacy applications that move application functionality to or from mainframe/server environments, the creation of new target independent interfaces for legacy systems, and the development of new applications altogether. Design concerns for such systems include security, reliability, and performance. The performance of these systems often defy intuition and must be taken into account during their design. In this paper we present a performance engineering tool for developing predictive models for such systems. The tool automates model construction by developing the structure of the model and measuring parameters that are difficult to estimate or capture manually. Designers can then focus on the performance impact of system configuration alternatives. We show how these results have been integrated into a prototype of IBM\u2019s Distributed Application Development Toolkit (DADT). A case study is presented that considers the hosting of sample application across three architectural models: Client/Server using DCE, Web Server/Server using HTML/HTTP/CGI, and a JAVA/CORBAORB/Server model.", "num_citations": "6\n", "authors": ["1500"]}
{"title": "Self-organizing autonomic computing systems\n", "abstract": " Recently a great deal of research has been under-taken in the area of automating the enterprise IT Infrastructure. For enterprises with a large number of computers the IT Infrastructure operation represents a considerable amount of the enterprise budget. Autonomic Computing Systems are systems which were created for minimizing this budget component. They were meant to correct and optimize the IT infrastructure's own self-functioning by executing corrective operations without any need for human interventions. In most cases, where autonomic computing systems have been developed, this was achieved by the addition of external global controllers monitoring the sub-systems of the enterprise IT Infrastructure, determining where changes should be made and applying appropriate commands to implement these changes. Self-Organizing systems on the other hand are systems which reach a global desired state\u00a0\u2026", "num_citations": "5\n", "authors": ["1500"]}
{"title": "Measurements and identification of autonomic computing processes\n", "abstract": " There is a growing need for the automation of the IT infrastructure of enterprises. Autonomic computing provided a theoretical support for the foundation of mechanisms for self-optimization of computational resources at all the levels of the IT infrastructure of the enterprise. As the Autonomic Computing paradigm requires collecting information in regards to specific parameters based on which a decision module will act, the architecture of an autonomic computing system is very much similar to a real-time control system. Thus the validation of the model used for the mathematical characterization of the autonomic computing processes is crucial. In this paper, starting from the model of autonomic computing processes an identification technique adapted to autonomic computing processe, is introduced. The identification is based on injecting pseudo random arrival rates into the autonomic system as disturbances. The\u00a0\u2026", "num_citations": "5\n", "authors": ["1500"]}
{"title": "3rd international workshop on adoption-centric software engineering\n", "abstract": " THIS CARNEGIE MELLON UNIVERSITY AND SOFTWARE ENGINEERING INSTITUTE MATERIAL IS FURNISHED ON AN\" AS-IS\" BASIS. CARNEGIE MELLON UNIVERSITY MAKES NO WARRANTIES OF ANY KIND, EITHER EXPRESSED OR IMPLIED, AS TO ANY MATTER INCLUDING, BUT NOT LIMITED TO, WARRANTY OF FITNESS FOR PURPOSE OR MERCHANTABILITY, EXCLUSIVITY, OR RESULTS OBTAINED FROM USE OF THE MATERIAL. CARNEGIE MELLON UNIVERSITY DOES NOT MAKE ANY WARRANTY OF ANY KIND WITH RESPECT TO FREEDOM FROM PATENT, TRADEMARK, OR COPYRIGHT INFRINGEMENT.", "num_citations": "5\n", "authors": ["1500"]}
{"title": "Performance stress vectors and capacity planning for e-commerce applications\n", "abstract": " Computing systems are essential resources for both the business and public sectors. With the increasing interdependence of integrated electronic commerce and business applications within the global computing environment, performance and reliability are of great concern. Poor performance can mean lost cooperation, opportunity, and revenue. This paper describes performance challenges that these applications face over the short and long term. We present an analytic technique that can predict the performance of an e-commerce application over a given deployment period. This technique can be used to deduce performance stress testing vectors over this period and for design and capacity planning exercises. A Web-based shopping server case study is used as an example.", "num_citations": "5\n", "authors": ["1500"]}
{"title": "Designing process replication and threading policies: A quantitative approach\n", "abstract": " When designing and deploying distributed systems it is necessary to determine process activation policy. A process\u2019s activation policy determines whether it is persistent or should be created and terminated with each call. For persistent processes the replication or threading levels must be decided. Inappropriate replication/threading levels can lead to unnecessary queuing delays for callers or an unnecessarily high consumption of memory resources. The purpose of this paper is to present quantitative techniques that determine appropriate process replication/threading levels. The results also provide information that can be used to guide the choice of process activation policy. Chosen replication levels are sensitive to the total number of customers using the system and the portion of customers belonging to each class. The algorithms presented consider all workload conditions, are iterative in nature, and are\u00a0\u2026", "num_citations": "5\n", "authors": ["1500"]}
{"title": "Understanding brain dynamics for color perception using wearable EEG headband\n", "abstract": " The perception of color is an important cognitive feature of the human brain. The variety of colors that impinge upon the human eye can trigger changes in brain activity which can be captured using electroencephalography (EEG). In this work, we have designed a multiclass classification model to detect the primary colors from the features of raw EEG signals. In contrast to previous research, our method employs spectral power features, statistical features as well as correlation features from the signal band power obtained from continuous Morlet wavelet transform instead of raw EEG, for the classification task. We have applied dimensionality reduction techniques such as Forward Feature Selection and Stacked Autoencoders to reduce the dimension of data eventually increasing the model's efficiency. Our proposed methodology using Forward Selection and Random Forest Classifier gave the best overall accuracy of 80.6\\% for intra-subject classification. Our approach shows promise in developing techniques for cognitive tasks using color cues such as controlling Internet of Thing (IoT) devices by looking at primary colors for individuals with restricted motor abilities.", "num_citations": "4\n", "authors": ["1500"]}
{"title": "Runtime Performance Management for Cloud Applications with Adaptive Controllers\n", "abstract": " Adaptability is an expected property of modern software systems in order to cope with changes in the environment by self-adjusting their structure and behaviour. Robustness is a crucial component of adaptability and it refers to the ability of the systems to deal with uncertainty, ie perturbations or unmodelled system dynamics that can affect the quality of the adaptation. Cost is another important property to ensure that resources are used prudently and frugally, whenever possible. Engineering robust and cost-effective adaptive systems can be accomplished using a control theory approach. In this paper, we show how to implement a model identification adaptive controller (MIAC) using a combination of performance and control models and how such a system satisfies the goals for robustness and cost-effectiveness. The controller we employ is multi-input, meaning that it can issue a variety of commands to adapt the\u00a0\u2026", "num_citations": "4\n", "authors": ["1500"]}
{"title": "From devops to bizops: Economic sustainability for scalable cloud applications\n", "abstract": " Virtualization of resources in cloud computing has enabled developers to commission and recommission resources at will and on demand. This virtualization is a coin with two sides. On one hand, the flexibility in managing virtual resources has enabled developers to efficiently manage their costs; they can easily remove unnecessary resources or add resources temporarily when the demand increases. On the other hand, the volatility of such environment and the velocity with which changes can occur may have a greater impact on the economic position of a stakeholder and the business balance of the overall ecosystem. In this work, we recognise the business ecosystem of cloud computing as an economy of scale and explore the effect of this fact on decisions concerning scaling the infrastructure of web applications to account for fluctuations in demand. The goal is to reveal and formalize opportunities for\u00a0\u2026", "num_citations": "4\n", "authors": ["1500"]}
{"title": "Realtime big data analytics for event detection in highways\n", "abstract": " The unprecedented growing and accessibility of traffic data has materialized various opportunities to improve the quality of transportation related services. In this paper, we extend Sipresk, a big data analytic platform, to ingest real-time traffic data and provide online analytics for various stakeholders. We leverage extended Sipresk to detect and classify events in Ontario highways. More specifically, we designed and implemented a set of algorithms that detect four traffic patterns each of which detects a specific type of event. The platform reports the event specifications no later than 20 seconds that provides the opportunity to react promptly to incidents in highways. Also, we configure Sipresk to process traffic sensor data from January 2015 to June 2016 for detection and classification of events for every single day. In less than 2 minutes, Sipresk is able to process 24 hours of data and generate a list of events occurred\u00a0\u2026", "num_citations": "4\n", "authors": ["1500"]}
{"title": "Supporting application development with structured queries in the cloud\n", "abstract": " To facilitate software development for multiple, federated cloud systems, abstraction layers have been introduced to mask the differences in the offerings, APIs, and terminology of various cloud providers. Such layers rely on a common ontology, which a) is difficult to create, and b) requires developers to understand both the common ontology and how various providers deviate from it. In this paper we propose and describe a structured query language for the cloud, Cloud SQL, along with a system and methodology for acquiring and organizing information from cloud providers and other entities in the cloud ecosystem such that it can be queried. It allows developers to run queries on data organized based on their semantic understanding of the cloud. Like the original SQL, we believe the use of a declarative query language will reduce development costs and make the multi-cloud accessible to a broader set of\u00a0\u2026", "num_citations": "4\n", "authors": ["1500"]}
{"title": "A model-based application autonomic manager with fine granular bandwidth control\n", "abstract": " In this paper, we propose and implement a machine learning based application autonomic management system that controls the bandwidth rates allocated to each scenario of a web application to postpone scaling out for as long as possible. Through experiments on Amazon AWS cloud, we demonstrate that the autonomic manager is able to quickly meet Service level Agreement (SLA) and reduce the SLA violations by 56% compared to a previous heuristic-based approach.", "num_citations": "3\n", "authors": ["1500"]}
{"title": "An economic model for scaling cloud applications\n", "abstract": " Web technologies along with the virtualization of computation and storage resources have allowed extensive flexibility in the development, deployment and delivery of software solutions. On one hand, this flexibility has allowed engineers to commission and re purpose resources on demand and at will. On the other hand, the flexibility has given rise to new business models and interesting implications in the economics of software ecosystems. In this work, we propose a model to capture the technical and economic transactions within a web software ecosystem deployed on a dynamic cloud environment. Next, we demonstrate the use of such model to define rules for a more economically efficient adaptation strategy for web applications as their incoming traffic fluctuates.", "num_citations": "3\n", "authors": ["1500"]}
{"title": "Performance Management and Monitoring\n", "abstract": " The main contribution of chapter is the creation, definition, implementation, and evaluation of a novel approach to application management on multi\u2010clouds that confers autonomic properties on applications at runtime and that embraces devops\u2010style management and facilitates experimentation with diverse autonomic management approaches (e.g., model\u2010based, rules/threshold driven, classic control, etc.) while abstracting away many of the low\u2010level cloud programming details and nuisances. An important use case for X\u2010Cloud Application Management Platform (XCAMP) will be the management framework for the Smart applications on virtual infrastructure (SAVI) testbed to streamline the life\u2010cycle management of applications on a novel cloud architecture and to simplify the process of deploying runtime management, facilitating research on this two\u2010tier cloud system by noncloud experts and students. The chapter\u00a0\u2026", "num_citations": "3\n", "authors": ["1500"]}
{"title": "Self-optimizing autonomic control of geographically distributed collaboration applications\n", "abstract": " In the past few years, cloud computing has become an integral technology both for the day to day running of corporations, as well as in everyday life as more services are offered which use a backend cloud. At the same time online collaboration tools are becoming more important as both businesses and individuals need to share information and collaborate with other entities. Previous work has presented an architecture for a collaboration online application which allows users in different locations to share videos, images and documents while at the same time video chatting. The application's servers are deployed in a cloud environment which can scale up and down based on demand. Furthermore, the design allows the application to be deployed on multiple clouds which are deployed in different geographic locations. Previous work however did not introduce how the application's up and down scaling is to be\u00a0\u2026", "num_citations": "3\n", "authors": ["1500"]}
{"title": "Enabling an enhanced data-as-a-service ecosystem\n", "abstract": " The sharing of large and interesting Big Data in cloud environments can be achieved using data-as-a-service, where a provider offers data to interested users. In enhanced data-as-a-service, the data provider also supplies compute infrastructure, allowing users to run analytics tasks local to the data and reducing the (expensive and slow) transmission of data over networks. This paper describes a services-based ecosystem that allows providers to precisely share portions of their data with users, using a model where users submit MapReduce jobs that run on the provider's Hadoop infrastructure. Providers are given mechanisms to filter, segment, and/or transform data before it reaches the user's task. The ecosystem also allows for intermediaries who offer value-added filtrations, segmentations, or transformations of the data (for example, pre-filtering a dataset to only include high-income users). We describe the\u00a0\u2026", "num_citations": "3\n", "authors": ["1500"]}
{"title": "Online simulation model optimization\n", "abstract": " An online simulation model optimization receives data representative of a business process captured in real time to form instance metrics, aggregates the instance metrics to form aggregated instance metrics, and uses a particle filter for filtering the aggregated instance metrics to form calibrated data. The process iteratively computes an output value using the calibrated data, by a simulation model. Responsive to a determination that the output value is not within a predetermined tolerance of an error threshold, the process adjusts a weight previously assigned to an aggregated instance metric by the particle filter to form recalibrated data, whereby the recalibrated data is submitted to the simulation model for computation. Responsive to a determination that the output value is within the predetermined tolerance, the process sends a result to a correction selection process of a business process optimizer, the result\u00a0\u2026", "num_citations": "3\n", "authors": ["1500"]}
{"title": "Method for dynamically managing a performance model for a data center\n", "abstract": " An autonomic computing system for dynamically managing a performance model for a data center. Measured performance data for a data processing system and performance model performance data from a performance model of the data processing system are obtained from which is generated estimated performance data. Upon comparison, if a difference between the measured performance data and the estimated performance data falls within defined limits, the performance module in the performance model structure is identified as an accurate model. If the difference does not fall within defined limits, the estimated performance data, the model performance data, and the measured performance data is analyzed to estimate new performance parameters for the performance model structure. Responsive to estimating new performance parameters for the performance model structure, the performance model\u00a0\u2026", "num_citations": "3\n", "authors": ["1500"]}
{"title": "Systems and Virtualization Management. Standards and New Technologies, chapter Web Service Distributed Management Framework for Autonomic Server Virtualization\n", "abstract": " The second edition of the Atlas of Echocardiography is a complete reference for cardiology professionals who rely on echocardiographic imaging techniques. The Atlas represents the collective effort of over 30 leading authorities in the field of cardiology working together to compile the most up-to-date and pertinent information in the field of echocardiography. All areas of echocardiography are covered, from diagnostic basics to cutting edge techniques, such as three-dimensional imaging, contrast imaging, and tissue Doppler imaging. Each figure is accompanied by an informative legend that fully explains the image and its importance. This striking, four color atlas serves as both a resource and a reference and is an invaluable tool to all readers, including cardiologists, non-cardiologist physicians, and sonographers.Contents: Preface.-Basic Principles of Echocardiography.-M-Mode Echocardiography.-Cardiac\u00a0\u2026", "num_citations": "3\n", "authors": ["1500"]}
{"title": "Fault detection in sensors using single and multi-channel weighted convolutional neural networks\n", "abstract": " The success of sensor based application depends on the availability of clean data and its fail safe operation. However, due to their inherent dynamical behavior, detecting faults in sensors can be challenging. To this end, we propose a signal processing and nonlinear dynamics based fault detection approach for learning the normal/abnormal states of sensors using machine learning. The characteristic traits in detecting faults are the textured images that are generated from the time series of sensor measurements. Our approach uses these textured images that capture the dynamical properties of the sensor systems along with spectrograms with weighted Convolutional Neural Networks (wCNNs). We first test our approach on a radar sensor system since identifying faults in such a system is difficult due to their inherent complex dynamics. An evaluation of the multi-channel wCNN model for radar fault detection shows\u00a0\u2026", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Rule-based security management system for data-intensive applications\n", "abstract": " Applications in today's software development landscape evolve at a rapid rate, constantly providing their users with new updates and features. This can result in growing complexity to understand the entire application even within the scope of a small enterprise. The security team may not understand such a large application completely and the developers may not understand or properly incorporate important security measures, thus creating a less secure system. As a result, the application can be subjected to security vulnerabilities that can result in serious data and reputation loss. In this work, we propose a platform for security control that uses a Business Rule Engine to provide a more simplified way of defining security rules at an operational level, allowing collaboration between developers and security analysts. The proposed platform is external to the system and enables the development and security teams to\u00a0\u2026", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Adaptive Load Management of Web Applications on Software Defined Infrastructure\n", "abstract": " Auto-scalability is a common approach for management of cloud applications where resources are provisioned and de-provisioned on demand. Because of its automatic nature, auto-scaling can be exploited for various reasons that can hugely reduce the overall profit. Therefore, it becomes vital to consider the trade-off between the added revenue as a result of auto-scaling and its corresponding cost. To this end, we propose a novel autonomic solution to the management of cloud Web applications whose goal is to optimize the profit. At the core of the solution, there is an optimization module that considers revenue model as well as cost model and uses a number of run-time performance models to derive the best course of action if there is a need for adaptation. Using software defined features that include compute and network programmability, our proposed solution helps applications to optimize their resource\u00a0\u2026", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Look Ahead Distributed Planning For Application Management In Cloud\n", "abstract": " In this paper, we propose and implement a distributed autonomic manager to maintain service level agreements (SLA) for each application' scenario. The proposed autonomic manager seeks to support SLAs by configuring bandwidth ratios for each application scenario using overlay network before provisioning more computing resources. The most important aspect of the proposed autonomic manager is scalability which allows us to deal with geographically distributed cloud-based applications and large volume of computation. This can be useful in look ahead optimization and when using complex models, such as machine learning. Through experiments on Amazon AWS cloud, we demonstrate the elasticity of the autonomic manager.", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Engineering Self-Adaptive Applications on Cloud with Software Defined Networks\n", "abstract": " Self-adaptive applications are engineered to adapt to operation conditions to continuously meet application requirements at run-time. Before the emergence of Software Defined Networking (SDN) and new network virtualization technologies, the common run-time adaptation actions were limited to changes in computing and storage resources (i.e., scaling in/out). However, SDN creates new avenues for adaptation strategies that complement or solves issues associated with computing adaptations. Hence, in this thesis, we propose to use SDN and network virtualization techniques to build self-adaptive applications. The research goal is to build applications that are self-protecting, self-managing and self-optimizing using both computing and networking adaptations. We design, implement and verify such self-adaptive applications on real cloud environment.", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Engineering cyber physical systems\n", "abstract": " CPS are smart systems that encompass computational and physical components, seamlessly integrated and closely interacting to sense the context of the real world [1]. These systems involve a high degree of complexity at numerous spatial and temporal scales and controlling software and physical components with highly networked communications. Thus, CPS comprise tightly integrated networking, computing, controlling, sensing and actuation capabilities. The societal impact of CPS is enormous. Virtually every engineered system is affected by advances in these interconnected capabilities. Future CPS applications are expected to be more transformative than the IT revolution of the past three decades [2]. Today CPS R&D affords spectacular and transformative opportunities due to the convergence of analytical and cognitive capabilities, real-time and networked control, pervasive sensing and actuating, as well\u00a0\u2026", "num_citations": "2\n", "authors": ["1500"]}
{"title": "A runtime sharing mechanism for Big Data platforms\n", "abstract": " In order to extract value from Big Data, a data source provider has to share data among many consumers. As such, data sharing becomes an important feature of Big Data platforms. However, privacy concerns are the key obstacles that prevent organizations from implementing data sharing solutions. Moreover, currently, the data owner is responsible for preparing the data before releasing it to a 3rd party. The preparation of data for release is a complex task and can become an obstacle. In this paper, we propose an ecosystem which enables data sharing responsibilities among producers and consumers and mitigates some of the obstacles.", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Leaky bucket model for autonomic control of distributed, collaborative systems\n", "abstract": " In recent years cloud computing has taken off and has become an underlying technology in many online applications and websites, whether using a private or public cloud. At the same time with the increase in globalization, online collaborative tools have become very important in order to allow different businesses to communicate with each other, or even different parts of the same company to communicate more easily and share data. Previous work introduced an architecture for a geographically distributed, cloud based collaboration application. While the architecture allowed the scaling up and down of the number of cloud instances being used to host media servers, no control scheme for such cloud scaling was provided. In this paper a model and control scheme for cloud scaling for a collaboration application is provided. The model is based on the leaky bucket model commonly used in network control, while\u00a0\u2026", "num_citations": "2\n", "authors": ["1500"]}
{"title": "The 4th International Workshop on Cloud Computing\n", "abstract": " Cloud computing is emerging as a new computational model in which software is hosted, run and administered in large web data centers, and provided as a service over the web. In general, it is well accepted that the cloud will provide three types of services, organized in layers. The first layer of the cloud will provide Infrastructure as a Service (IaaS), in which raw computing services such as CPU, storage and network bandwidth are requested, reserved and used over the web using a pay per usage model. These services can be consumed by an end user in need of resources or by an upper layer of the cloud, Platform as a Service (PaaS). PaaS, the second layer, offers a higher level of services, including web servers, data servers, etc,. In general, it offers a platform for deploying classes of applications, such as web and transactional applications. The third layer of the cloud is Software as a Service (SaaS), that\u00a0\u2026", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Using components to build software engineering tools\n", "abstract": " Traditionally, software engineering research tools have been developed from scratch without much reuse. Software components offer now a different way of tool-building, which replaces hand-crafted code with pre-packaged functionality. Tools based on components have unique characteristics for developers as well as tool users. This paper describes three common approaches to tool-building with components and evaluates the approaches from a toolbuilder\u2019s perspective.", "num_citations": "2\n", "authors": ["1500"]}
{"title": "3/sup rd/international workshop on adoption-centric software engineering ACSE 2003\n", "abstract": " The key objective of this workshop is to explore innovative approaches to the adoption of software engineering tools and practices-in particular by embedding them in extensions of Commercial Off-The-Shelf (COTS) software products and/or middleware technologies. The workshop aims to advance the understanding and evaluation of adoption of software engineering tools and practices by bringing together researchers and practitioners who investigate novel solutions to software engineering adoption issues.", "num_citations": "2\n", "authors": ["1500"]}
{"title": "Building Automation System Data Integration with BIM: Data Structure and Supporting Case Study\n", "abstract": " Buildings Automation Systems (BAS) are ubiquitous in contemporary buildings, both collecting room condition, equipment operational data and sending control points developed by integrated sequences, but are limited by prescribed logic, possess only rudimentary visualizations, and lacN broader system integration capabilities. Advances in machine learning, edge analytics, data management systems, and Facilitates Management Building Information Model software (FM-BIM) permit a novel approach to cloud-hosted building management. This paper presents an integration technique for mapping the data from a building Internet of Things (IoT) sensor networN to an FM-BIM. The sensor data ontology and time series analysis strategies integrated into the data structure are discussed and presented, including the use of a 3D nested list to permit time-series data to be mapped to the FM-BIM and readily visualized. The developed approach is presented through a case study of an office living lab consisting of a local sensor networN mimicNing a BAS, which streams to a cloud server via a virtual private networN connection. The resultant data structure and Ney visualizations are presented to demonstrate the value of this approach, which permits the end-user to select the desired timeframe for visualization and readily step through the spatio-temporal building performance data.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Systems and methods of controlled sharing of big data\n", "abstract": " Methods and systems for controlled data sharing are provided. According to one example, a data provider defines one or more data policies and allows access to data to one or more data consumers. Each data consumer submits analytics tasks (jobs) that include two phases: data transformation and data mining. The data provider verifies that data is trans-formed (eg, anonymized) according to the data policies. Upon verification, the data consumer is provided with access to the results of the data mining phase. An ecosystem of data providers and data consumers can be loosely coupled through the use of web services that permit discovery and sharing in a flexible, secure environment.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Implementation of self-managing applications on cloud using overlay networks\n", "abstract": " In this paper, we present an architecture and implementation for self-managing cloud application using overlay networks and software defined networking (SDN). Through real world experiments on Amazon EC2 and Smart Applications on Virtual Infrastructure (SAVI) cloud, we demonstrate how our management mechanism autonomously maintains SLAs of application scenarios without provisioning extra resources.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Engineering cybersecurity into cyber physical systems\n", "abstract": " Advances in the interconnected capabilities of cyber physical systems (CPS) affect virtually every engineered system. Today, software approaches dominate all aspects of connecting the physical and cyber worlds in part due to the convergence of computing, control and communications software technologies. Unfortunately, software technologies are more vulnerable to cybersecurity problems than traditional hardware solutions. This workshop aims to develop a research agenda for engineering cybersecurity into cyber physical systems (CPS) through designtime requirements engineering, continuous assurance at runtime, and cognitive security analytics.CPS are distributed, software-intensive smart systems that control tightly integrated computational and physical components. A combination of design-time and runtime techniques are needed to support reasoning about cybersecurity. We advocate a holistic requirements engineering approach, continuous validation with feedback loops supported by models at runtime and networked control, and cognitive security analytics using Watson.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "To default or not to default: Exposing limitations to hbase cluster deployers\n", "abstract": " With the advent of sensor networks and portable devices, data has been produced rapidly and in great amount. As a result storing and processing Big Data, in combination with the advances in cloud and virtual infrastructures, pose interesting challenges. In our previous work, we studied these challenges with various experiments around different HBase cluster configurations and their impact on the performance of the cluster. A by-product of our experiments was that, in spite of advances in tooling support to set up and configure a Big Data cluster, the various tools are not always aligned to produce the optimal or near-optimal performance for data clusters. More specifically, we show that the default configuration values of state-of-the-art cluster deployers, including Cloudera, IBM BigInsights, Apache Hortonworks and the manual HBase deployment, do not take in to account the underlying infrastructure resulting in subpar performance.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "The 7th CASCON workshop on cloud computing\n", "abstract": " Hybrid clouds are private and public sub-clouds working together to mitigate privacy and security concerns while addressing the need for large computation and storage capacity. Academic research into hybrid clouds has focused on the middleware and abstraction layers for creating, managing, and using hybrid clouds. For example, researchers used the MapReduce paradigm to split a data-intensive workload into mapping tasks sorted by the sensitivity of the data, with the most sensitive data being processed locally and the least sensitive processed in a public cloud. Commercial support for hybrid clouds is growing in response to the business case for cloud federation. IBM offers both PureApplication System (to manage a private cloud) and PureApplication Service (a public cloud offering) and software to bridge the two at the Software-as-a-Service (SaaS) level. More recently, IBM Blue Mix Platform-as-a-Service\u00a0\u2026", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Evaluating cluster configurations for big data processing: An exploratory study\n", "abstract": " As data continues to grow rapidly, NoSQL clusters have been increasingly adopted to address the storage and processing demands of these large amounts of data. In parallel, cloud computing is also increasingly being adopted due to its flexibility, cost efficiency and scalability. However, evaluating and modelling NoSQL clusters present many challenges. In this work, we explore these challenges by performing a series of experiments with various configurations. The intuition is that this process is laborious and expensive and the goal of our experiments is to confirm this intuition and to identify the factors that impact the performance of a Big Data cluster. Our experiments mostly focus on three factors: data compression, data schema and cluster topology. We performed a number of experiments based on these factors and measured and compared the response times of the resulting configurations. Eventually, the\u00a0\u2026", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Toward a solution for the cloud account delegation problem.\n", "abstract": " Cloud account delegation refers to the situation in which a cloud account owner (delegator) allows one or more second parties (ie, delegates) to acquire and use cloud infrastructure resources from the owner\u2019s account at the expense of the owner. This situation can exist in research groups, cloud testbeds, university courses, software development teams, and in general in cases where pooled resources are managed as software-defined infrastructure. The delegator assumes the risk of inefficient, wasted, or abused resources, and must rely on delegates, who are often not experienced cloud users, using the virtual infrastructure effectively and responsibly. This paper introduces the cloud account delegation problem, including three primary categories of risk, introduces a solution outline, and identifies research challenges.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Hierarchical Self-Optimization of SaaS Applications in Clouds\n", "abstract": " This chapter introduces a framework and a methodology to manage a SaaS application on top of a PaaS infrastructure. This framework utilizes PaaS policy sets to implement the SaaS provider\u2019s elasticity policy for its application server tier. Adaptation is based on strategy-trees, which allow for systematic capture, representation and reasoning about adaptation variability, based on hierarchically organizing different levels of temporal granularity. Thus, a strategy-tree is utilized at the SaaS layer to actively guide policy set selection at runtime in order to maintain alignment with the SaaS provider\u2019s business objectives. This way, the SaaS provider\u2019s attitudes and preferences reflecting their general business needs are incorporated into the adaptation mechanism in an organized and accessible manner. Results from an experiment conducted on a real cloud are presented in support of this approach.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Geographically Distributed Cloud-Based Collaborative Application\n", "abstract": " The amount of multimedia content on the Internet has been growing at a remarkable rate, and users are increasingly looking to share online media with colleagues and friends on social networks. Several commercial and academic solutions have attempted to make it easier to share this large variety of online content with others, but they are generally limited to only sending Web links. At the same time, existing products have not been able to provide a scalable system that synchronizes disparate Web content sources among many users in real-time. Such a goal is especially desired in order to provide the benefits of cloud deployments to collaborative applications. Many Web-based applications cannot predict the number of connections that they may need to handle. As such, applications must either provision a higher number of servers in anticipation of more traffic, or be faced with a degradation of the user\u00a0\u2026", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Evolvability in Service Oriented Systems.\n", "abstract": " The paper investigates the evolution and maintenance of service oriented systems deployed in SOA and cloud infrastructures. It analyzes the challenges entailed by the frequent modifications of business environments, discussing their causes, grasping the evolution points in service architectures, studying classifications of human actors involved across the whole life cycle, as well as pointing out possible risks and difficulties encountered in the process of change. Based on the lessons learned in our study, four pillars for improving service evolvability are identified: orientation towards the users, increasing the level of abstraction, supporting automation and enabling adaptivity through feedback loops.", "num_citations": "1\n", "authors": ["1500"]}
{"title": "Designing high performance software systems-a quantitative approach.\n", "abstract": " 4.3 Analysis of Response Times and Object Allocation at Very High Population Levels 4.3. 1 Response Time in Single-Class Systems 4.3. 2 Response Time in Multi-Class Systems with a Common Natural Bottleneck 4.3. 3 Response Time in Multi-Class Systems with Multiple Natural Bottlenecks", "num_citations": "1\n", "authors": ["1500"]}