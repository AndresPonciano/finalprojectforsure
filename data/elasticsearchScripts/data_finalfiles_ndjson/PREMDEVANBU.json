{"title": "Mining email social networks\n", "abstract": " Communication & Co-ordination activities are central to large software projects, but are difficult to observe and study in traditional (closed-source, commercial) settings because of the prevalence of informal, direct communication modes. OSS projects, on the other hand, use the internet as the communication medium, and typically conduct discussions in an open, public manner. As a result, the email archives of OSS projects provide a useful trace of the communication and co-ordination activities of the participants. However, there are various challenges that must be addressed before this data can be effectively mined. Once this is done, we can construct social networks of email correspondents, and begin to address some interesting questions. These include questions relating to participation in the email; the social status of different types of OSS participants; the relationship of email activity and commit activity (in the\u00a0\u2026", "num_citations": "736\n", "authors": ["147"]}
{"title": "Software engineering for security: a roadmap\n", "abstract": " Is there such a thing anymore as a software system that doesn't need to be secure? Almost every softwarecontrolled system faces threats from potential adversaries, from Internet-aware client applications running on PCs, to complex telecommunications and power systems accessible over the Internet, to commodity software with copy protection mechanisms. Software engineers must be cognizant of these threats and engineer systems with credible defenses, while still delivering value to customers. In this paper, we present our perspectives on the research issues that arise in the interactions between software engineering and security.", "num_citations": "658\n", "authors": ["147"]}
{"title": "A knowledge-based software information system\n", "abstract": " The difficulty of maintaining very large software systems is becoming more widely acknowledged. One of the primary problems is the need to access information about a complex and evolving system. We are exploring the contribution to be made by applying explicit knowledge representation and reasoning to the management of information about large systems. LaSSIE is a prototype tool (based on the ARGON system) that uses a frame-based description language and classification inferences to facilitate a programmer's discovery of the structure of a complex system. It also supports the retrieval of software for possible reuse in a new development task. Among LaSSIE's features are an integrated natural language frontend (TELI) that allows users to express requests in an informal and compact fashion. Although not without some limitations, LaSSIE represents significant progress over existing software retrieval methods and strictly bottom-up cross-referencing facilities.", "num_citations": "633\n", "authors": ["147"]}
{"title": "An investigation into coupling measures for C++\n", "abstract": " This paper proposes a comprehensive suite of measures to quantify the level of class coupling during the design of object-oriented systems. This suite takes into account the different 00 design mechanisms provided by the C++ language (eg, friendship between classes, specialization, and aggregation) but it can be tailored to other 00 languages. The different measures in our suite thus reflect different hypotheses about the different mechanisms of coupling in 00 systems. Based on actual project defect data, the hypotheses underlying our coupling measures are empirically validated by analyzing their relationship with the probability of fault detection across classes. The results demonstrate that some of these coupling measures may be useful early quality indicators of the design'of 00 systems. These measures are conceptually different from the 00 design measures defined by Chidamber and Kemerer; in addition\u00a0\u2026", "num_citations": "514\n", "authors": ["147"]}
{"title": "A survey of machine learning for big code and naturalness\n", "abstract": " Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.", "num_citations": "461\n", "authors": ["147"]}
{"title": "Fair and balanced?: bias in bug-fix datasets\n", "abstract": " Software engineering researchers have long been interested in where and why bugs occur in code, and in predicting where they might turn up next. Historical bug-occurence data has been key to this research. Bug tracking systems, and code version histories, record when, how and by whom bugs were fixed; from these sources, datasets that relate file changes to bug fixes can be extracted. These historical datasets can be used to test hypotheses concerning processes of bug introduction, and also to build statistical bug prediction models. Unfortunately, processes and humans are imperfect, and only a fraction of bug fixes are actually labelled in source code version histories, and thus become available for study in the extracted datasets. The question naturally arises, are the bug fixes recorded in these historical datasets a fair representation of the full population of bug fixes? In this paper, we investigate historical\u00a0\u2026", "num_citations": "435\n", "authors": ["147"]}
{"title": "A large scale study of programming languages and code quality in github\n", "abstract": " What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static vs dynamic typing, strong vs weak typing on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing\u00a0\u2026", "num_citations": "398\n", "authors": ["147"]}
{"title": "Don\u2019t Touch My Code! Examining the Effects of Ownership on Software Quality\n", "abstract": " Ownership is a key aspect of large-scale software development. We examine the relationship between different ownership measures and software failures in two large software projects: Windows Vista and Windows 7. We find that in all cases, measures of ownership such as the number of low-expertise developers, and the proportion of ownership for the top owner have a relationship with both pre-release faults and post-release failures. We also empirically identify reasons that low-expertise developers make changes to components and show that the removal of low-expertise contributions dramatically decreases the performance of contribution based defect prediction. Finally we provide recommendations for source code change policies and utilization of resources such as code inspections based on our results.", "num_citations": "389\n", "authors": ["147"]}
{"title": "Latent social structure in open source projects\n", "abstract": " Commercial software project managers design project organizational structure carefully, mindful of available skills, division of labour, geographical boundaries, etc. These organizational\" cathedrals\" are to be contrasted with the\" bazaar-like\" nature of Open Source Software (OSS) Projects, which have no pre-designed organizational structure. Any structure that exists is dynamic, self-organizing, latent, and usually not explicitly stated. Still, in large, complex, successful, OSS projects, we do expect that subcommunities will form spontaneously within the developer teams. Studying these subcommunities, and their behavior can shed light on how successful OSS projects self-organize. This phenomenon could well hold important lessons for how commercial software teams might be organized. Building on known well-established techniques for detecting community structure in complex networks, we extract and study\u00a0\u2026", "num_citations": "369\n", "authors": ["147"]}
{"title": "Authentic third-party data publication\n", "abstract": " Integrity critical databases, such as financial data used in high-value decisions, are frequently published over the Internet. Publishers of such data must satisfy the integrity, authenticity, and non-repudiation requirements of clients. Providing this protection over public networks is costly.             This is partly because building and running secure systems is hard. In practice, large systems can not be verified to be secure and are frequently penetrated. The consequences of a system intrusion at the data publisher can be severe. This is further complicated by data and server replication to satisfy availability and scalability requirements.             We aim to reduce the trust required of the publisher of large, infrequently updated databases. To do this, we separate the roles of owner and publisher. With a few trusted digital signatures from the owner, an untrusted publisher can use techniques based on Merkle hash trees to\u00a0\u2026", "num_citations": "317\n", "authors": ["147"]}
{"title": "Does distributed development affect software quality?: an empirical case study of Windows Vista\n", "abstract": " It is widely believed that distributed software development is riskier and more challenging than collocated development. Prior literature on distributed development in software engineering and other fields discuss various challenges, including cultural barriers, expertise transfer difficulties, and communication and coordination overhead. We evaluate this conventional belief by examining the overall development of Windows Vista and comparing the post-release failures of components that were developed in a distributed fashion with those that were developed by collocated teams. We found a negligible difference in failures. This difference becomes even less significant when controlling for the number of developers working on a binary. We also examine component characteristics such as code churn, complexity, dependency information, and test code coverage and find very little difference between distributed and\u00a0\u2026", "num_citations": "312\n", "authors": ["147"]}
{"title": "How, and why, process metrics are better\n", "abstract": " Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from\u00a0\u2026", "num_citations": "303\n", "authors": ["147"]}
{"title": "A general model for authenticated data structures\n", "abstract": " Query answers from on-line databases can easily be corrupted by   hackers or malicious database publishers. Thus it is   important to provide mechanisms which allow clients to trust the   results from on-line queries.  Authentic publication allows untrusted publishers to answer securely queries   from clients on behalf of trusted off-line data owners.  Publishers   validate answers using hard-to-forge verification   objects VOs), which clients can check efficiently.  This approach   provides greater scalability, by making it easy to add more publishers, and better   security, since on-line publishers do not need to be trusted.  To make authentic publication attractive, it is important for the  VOs to be small, efficient to compute, and efficient to verify. This has lead researchers to  develop independently   several different schemes for efficient VO computation based on specific data structures. Our goal is to develop a\u00a0\u2026", "num_citations": "279\n", "authors": ["147"]}
{"title": "Static checking of dynamically generated queries in database applications\n", "abstract": " Many data-intensive applications dynamically construct queries in response to client requests and execute them. Java servlets, e.g., can create string representations of SQL queries and then send the queries, using JDBC, to a database server for execution. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. Thus, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this paper, we present a sound, static, program analysis technique to verify the correctness of dynamically generated query strings. We describe our analysis technique and provide soundness results for our static\u00a0\u2026", "num_citations": "259\n", "authors": ["147"]}
{"title": "Recalling the\" imprecision\" of cross-project defect prediction\n", "abstract": " There has been a great deal of interest in defect prediction: using prediction models trained on historical data to help focus quality-control resources in ongoing development. Since most new projects don't have historical data, there is interest in cross-project prediction: using data from one project to predict defects in another. Sadly, results in this area have largely been disheartening. Most experiments in cross-project defect prediction report poor performance, using the standard measures of precision, recall and F-score. We argue that these IR-based measures, while broadly applicable, are not as well suited for the quality-control settings in which defect prediction models are used. Specifically, these measures are taken at specific threshold settings (typically thresholds of the predicted probability of defectiveness returned by a logistic regression model). However, in practice, software quality control processes\u00a0\u2026", "num_citations": "255\n", "authors": ["147"]}
{"title": "On the localness of software\n", "abstract": " The n-gram language model, which has its roots in statistical natural language processing, has been shown to successfully capture the repetitive and predictable regularities (\u201cnaturalness\") of source code, and help with tasks such as code suggestion, porting, and designing assistive coding devices. However, we show in this paper that this natural-language-based model fails to exploit a special property of source code: localness. We find that human-written programs are localized: they have useful local regularities that can be captured and exploited. We introduce a novel cache language model that consists of both an n-gram and an added \u201ccache\" component to exploit localness. We show empirically that the additional cache component greatly improves the n-gram approach by capturing the localness of software, as measured by both cross-entropy and suggestion accuracy. Our model\u2019s suggestion accuracy is\u00a0\u2026", "num_citations": "234\n", "authors": ["147"]}
{"title": "Clones: What is that smell?\n", "abstract": " Clones are generally considered bad programming practice in software engineering folklore. They are identified as a bad smell\u00a0(Fowler et\u00a0al. 1999) and a major contributor to project maintenance difficulties. Clones inherently cause code bloat, thus increasing project size and maintenance costs. In this work, we try to validate the conventional wisdom empirically to see whether cloning makes code more defect prone. This paper analyses the relationship between cloning and defect proneness. For the four medium to large open source projects that we studied, we find that, first, the great majority of bugs are not significantly associated with clones. Second, we find that clones may be less defect prone than non-cloned code. Third, we find little evidence that clones with more copies are actually more error prone. Fourth, we find little evidence to support the claim that clone groups that span more than one file or\u00a0\u2026", "num_citations": "232\n", "authors": ["147"]}
{"title": "Ownership, Experience and Defects: a fine-grained study of Authorship\n", "abstract": " Recent research indicates that\" people\" factors such as ownership, experience, organizational structure, and geographic distribution have a big impact on software quality. Understanding these factors, and properly deploying people resources can help managers improve quality outcomes. This paper considers the impact of code ownership and developer experience on software quality. In a large project, a file might be entirely owned by a single developer, or worked on by many. Some previous research indicates that more developers working on a file might lead to more defects. Prior research considered this phenomenon at the level of modules or files, and thus does not tease apart and study the effect of contributions of different developers to each module or file. We exploit a modern version control system to examine this issue at a fine-grained level. Using version history, we examine contributions to code\u00a0\u2026", "num_citations": "226\n", "authors": ["147"]}
{"title": "Are deep neural networks the best choice for modeling source code?\n", "abstract": " Current statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add & remove text, and mix & swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N-gram, as well as RNN, and LSTM deep\u00a0\u2026", "num_citations": "218\n", "authors": ["147"]}
{"title": "Authentic data publication over the internet\n", "abstract": " Integrity critical databases, such as financial information used in high-value decisions, are frequently published over the Internet. Publishers of such data must satisfy the integrity, authenticity, and non-repudiation requirements of clients. Providing this protection over public data networks is an expensive proposition. This is, in part, due to the difficulty of building and running secure systems. In practice, large systems can not be verified to be secure and are frequently penetrated. The negative consequences of a system intrusion at the publisher can be severe. The problem is further complicated by data and server replication to satisfy availability and scalability requirements.", "num_citations": "213\n", "authors": ["147"]}
{"title": "GENOA: a customizable language-and front-end independent code analyzer\n", "abstract": " Programmers working on large software systems spend a great deal of time examining code and trying to understand it. Code Analysis tools (eg, cross referencing tools such as CIA and CSCOPE) can be very helpful in this process. In this paper we describe GENOA, an application generator that can produce a whole range of useful code analysis tools. GENOA is designed to be language- and froni-end independent; itcan be interfaced to any front-end for any language that produces an attributed parse tree, simply by writing an interface specification. While GENOA programs can perform arbitrary analyses on the parse tree, the GENOA language has special, compact iteration operators that are tuned for expressing simple, polynomial time analysis programs; in fact, there is a useful sublanguage of GENOA that can express precisely all (and only) polynomial time (PTIME) analysis programs on parse-trees. Thus, we\u00a0\u2026", "num_citations": "212\n", "authors": ["147"]}
{"title": "JDBC checker: A static analysis tool for SQL/JDBC applications\n", "abstract": " In data-intensive applications, it is quite common for the implementation code to dynamically construct database query strings and execute them. For example, a typical Java servlet Web service constructs SQL query strings and dispatches them over a JDBC connector to an SQL-compliant database. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. For example, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this paper, we describe JDBC Checker, a sound static analysis tool to verify the correctness of dynamically generated query strings. We have successfully applied\u00a0\u2026", "num_citations": "196\n", "authors": ["147"]}
{"title": "Open borders? immigration in open source projects\n", "abstract": " Open source software is built by teams of volunteers. Each project has a core team of developers, who have the authority to commit changes to the repository; this team is the elite, committed foundation of the project, selected through a meritocratic process from a larger number of people who participate on the mailing list. Most projects carefully regulate admission of outsiders to full developer privileges; some projects even have formal descriptions of this process. Understanding the factors that influence the \"who, how and when\" of this process is critical, both for the sustainability of FLOSS projects, and for outside stakeholders who want to gain entry and succeed. In this paper we mount a quantitative case study of the process by which people join FLOSS projects, using data mined from the Apache Web server, Postgres, and Python. We develop a theory of open source project joining, and evaluate this theory based\u00a0\u2026", "num_citations": "188\n", "authors": ["147"]}
{"title": "On the\" naturalness\" of buggy code\n", "abstract": " Real software, the kind working programmers produce by the kLOC to solve real-world problems, tends to be \u201cnatural\u201d, like speech or natural language; it tends to be highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising, to a good statistical language model is \u201cunnatural\u201d in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca. 7,139), from 10 different Java projects, and focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic (i.e. unnatural), becoming less so as bugs are fixed. Ordering files for\u00a0\u2026", "num_citations": "186\n", "authors": ["147"]}
{"title": "Flexible authentication of XML documents\n", "abstract": " XML is increasingly becoming the format of choice for information exchange on the Internet. As this trend grows, one can expect that documents (or collections thereof) may get quite large, and clients may wish to query for specific segments of these documents. In critical areas such as healthcare, law and finance, integrity is essential. In such applications, clients must be assured that they are getting complete and correct answers to their queries. Existing methods for signing XML documents cannot be used to establish that an answer to a query is complete. A simple approach has a server processing queries and certifying answers by digitally signing them with an on-line private key; however, the server, and its on-line private key, would be vulnerable to external hacking and insider attacks. We propose a new approach to signing XML documents which allows untrusted servers to answer certain types of path queries\u00a0\u2026", "num_citations": "176\n", "authors": ["147"]}
{"title": "Analytical and empirical evaluation of software reuse metrics\n", "abstract": " How much can be saved by using existing software components when developing new software systems? With the increasing adoption of reuse methods and technologies, this question becomes critical. However, directly tracking the actual cost savings due to reuse is difficult. A worthy goal would be to develop a method of measuring the savings indirectly by analyzing the code for reuse of components. The focus of the paper is to evaluate how well several published software reuse metrics measure the \"time, money and quality\" benefits of software reuse. We conduct this evaluation both analytically and empirically. On the analytic front, we introduce some properties that should arguably hold of any measure of \"time, money and quality\" benefit due to reuse. We assess several existing software reuse metrics using these properties. Empirically, we constructed a toolset (using GEN+S) to gather data on all published\u00a0\u2026", "num_citations": "136\n", "authors": ["147"]}
{"title": "GlueQoS: Middleware to sweeten quality-of-service policy interactions\n", "abstract": " A holy grail of component-based software engineering is write-once, reuse everywhere. However, in modern distributed, component-based systems supporting emerging application areas such as service-oriented e-business (where Web services are viewed as components) and peer-to-peer computing, this is difficult. Non-functional requirements (related to quality-of-service (QoS) issues such as security, reliability, and performance) vary with deployment context, and sometimes even at run-time, complicating the task of re-using components. In this paper, we present a middleware-based approach to managing dynamically changing QoS requirements of components. Policies are used to advertise non-functional capabilities and vary at run-time with operating conditions. We also provide middleware enhancements to match, interpret, and mediate QoS requirements of clients and servers at deployment time and/or\u00a0\u2026", "num_citations": "127\n", "authors": ["147"]}
{"title": "Comparing static bug finders and statistical prediction\n", "abstract": " The all-important goal of delivering better software at lower cost has led to a vital, enduring quest for ways to find and remove defects efficiently and accurately. To this end, two parallel lines of research have emerged over the last years. Static analysis seeks to find defects using algorithms that process well-defined semantic abstractions of code. Statistical defect prediction uses historical data to estimate parameters of statistical formulae modeling the phenomena thought to govern defect occurrence and predict where defects are likely to occur. These two approaches have emerged from distinct intellectual traditions and have largely evolved independently, in \u201csplendid isolation\u201d. In this paper, we evaluate these two (largely) disparate approaches on a similar footing. We use historical defect data to apprise the two approaches, compare them, and seek synergies. We find that under some accounting principles, they\u00a0\u2026", "num_citations": "124\n", "authors": ["147"]}
{"title": "Ecological Inference in Empirical Software Engineering\n", "abstract": " Software systems are decomposed hierarchically, for example, into modules, packages and files. This hierarchical decomposition has a profound influence on evolvability, maintainability and work assignment. Hierarchical decomposition is thus clearly of central concern for empirical software engineering researchers; but it also poses a quandary. At what level do we study phenomena, such as quality, distribution, collaboration and productivity? At the level of files? packages? or modules? How does the level of study affect the truth, meaning, and relevance of the findings? In other fields it has been found that choosing the wrong level might lead to misleading or fallacious results. Choosing a proper level, for study, is thus vitally important for empirical software engineering research; but this issue hasn't thus far been explicitly investigated. We describe the related idea of ecological inference and ecological fallacy from\u00a0\u2026", "num_citations": "106\n", "authors": ["147"]}
{"title": "Dual ecological measures of focus in software development\n", "abstract": " Work practices vary among software developers. Some are highly focused on a few artifacts; others make wideranging contributions. Similarly, some artifacts are mostly authored, or \u201cowned\u201d, by one or few developers; others have very wide ownership. Focus and ownership are related but different phenomena, both with strong effect on software quality. Prior studies have mostly targeted ownership; the measures of ownership used have generally been based on either simple counts, information-theoretic views of ownership, or social-network views of contribution patterns. We argue for a more general conceptual view that unifies developer focus and artifact ownership. We analogize the developer-artifact contribution network to a predator-prey food web, and draw upon ideas from ecology to produce a novel, and conceptually unified view of measuring focus and ownership. These measures relate to both cross\u00a0\u2026", "num_citations": "102\n", "authors": ["147"]}
{"title": "Detecting patch submission and acceptance in oss projects\n", "abstract": " The success of open source software (OSS) is completely dependent on the work of volunteers who contribute their time and talents. The submission of patches is the major way that participants outside of the core group of developers make contributions. We argue that the process of patch submission and acceptance into the codebase is an important piece of the open source puzzle and that the use of patch-related data can be helpful in understanding how OSS projects work. We present our methods in identifying the submission and acceptance of patches and give results and evaluation in applying these methods to the Apache webserver, Python interpreter, Postgres SQL database, and (with limitations) MySQL database projects. In addition, we present valuable ways in which this data has been and can be used.", "num_citations": "97\n", "authors": ["147"]}
{"title": "GENOA\u2014a customizable, front-end-retargetable source code analysis framework\n", "abstract": " Code analysis tools provide support for such software engineering tasks as program understanding, software metrics, testing, and reengineering. In this article we describe GENOA, the framework underlying application generators such as Aria and GEN++ which have been used to generate a wide range of practical code analysis tools. This experience illustrates front-end retargetability of GENOA; we describe the features of the GENOA framework that allow it to be used with different front ends.  While permitting arbitrary parse tree computations, the GENOA specification language has special, compact iteration operators that are tuned for expressing simple, polynomial-time analysis programs; in fact, there is a useful sublanguage of the GENOA language that  can express precisely all (and only) polynomial-time (PTIME) analysis programs on parse trees. Thus, we argue that the GENOA language is a simple and\u00a0\u2026", "num_citations": "94\n", "authors": ["147"]}
{"title": "Static checking of dynamically generated queries in database applications\n", "abstract": " Many data-intensive applications dynamically construct queries in response to client requests and execute them. Java servlets, for example, can create strings that represent SQL queries and then send the queries, using JDBC, to a database server for execution. The servlet programmer enjoys static checking via Java's strong type system. However, the Java type system does little to check for possible errors in the dynamically generated SQL query strings. Thus, a type error in a generated selection query (e.g., comparing a string attribute with an integer) can result in an SQL runtime exception. Currently, such defects must be rooted out through careful testing, or (worse) might be found by customers at runtime. In this article, we present a sound, static program analysis technique to verify that dynamically generated query strings do not contain type errors. We describe our analysis technique and provide soundness\u00a0\u2026", "num_citations": "91\n", "authors": ["147"]}
{"title": "The Willow architecture: comprehensive survivability for large-scale distributed applications\n", "abstract": " The Willow architecture is a comprehensive approach to survivability in critical distributed applications. Survivability is achieved in a deployed system using a unique combination of a fault avoidance by disabling vulnerable network elements intentionally when a threat is detected or predicted, b fault elimination by replacing system software elements when faults are discovered, and c fault tolerance by reconfiguring the system if non-maskable damage occurs. The key to the architecture is a powerful reconfiguration mechanism that is combined with a general control structure in which network state is sensed, analyzed, and required changes effected. The architecture can be used to deploy software functionality enhancements as well as survivability. Novel aspects include node configuration control mechanisms a workflow system for resolving conflicting configurations communications based on wide-area event notification tolerance for wide-area, hierarchic and sequential faults and secure, scalable and delegatable trust models.Descriptors:", "num_citations": "91\n", "authors": ["147"]}
{"title": "DADO: enhancing middleware to support crosscutting features in distributed, heterogeneous systems\n", "abstract": " Some \"non-\" or \"extra-functional\" features, such as reliability, security, and tracing, defy modularization mechanisms in programming languages. This makes such features hard to design, implement, and maintain. Implementing such features within a single platform, using a single language, is hard enough With distributed, heterogeneous (DH) systems, these features induce complex implementations which cross-cut different languages, OSs, and hardware platforms, while still needing to share data and events. Worse still, the precise requirements for such features are often locality-dependent and discovered late (e.g., security policies). The DADO/sup 1/ approach helps program cross-cutting features by improving DH middleware. A DADO service comprises pairs of adaplets which are explicitly modeled in IDL. Adaplets may be implemented in any language compatible with the target application, and attached to\u00a0\u2026", "num_citations": "84\n", "authors": ["147"]}
{"title": "Recommending random walks\n", "abstract": " We improve on previous recommender systems by taking advantage of the layered structure of software. We use a random-walk approach, mimicking the more focused behavior of a developer, who browses the caller-callee links in the callgraph of a large program, seeking routines that are likely to be related to a function of interest. Inspired by Kleinberg's work [10], we approximate the steady-state of an infinite random walk on a subset of a callgraph in order to rank the functions by their steady-state probabilities. Surprisingly, this purely structural approach works quite well. Our approach, like that of Robillard's\" Suade\" algorithm [15], and earlier data mining approaches [13] relies solely on the always available current state of the code, rather than other sources such as comments, documentation or revision information. Using the Apache API documentation as an oracle, we perform a quantitative evaluation of our\u00a0\u2026", "num_citations": "81\n", "authors": ["147"]}
{"title": "System and method for providing assurance to a host that a piece of software possesses a particular property\n", "abstract": " A system and method for providing assurance to a host executing a piece of software that the software possesses a particular property. A certifier determines if a piece of software possesses a particular property, and if it does, it cryptographically signs the software, producing a signature. The software and a certificate that includes the signature is then distributed to a host. The host checks the signature. If the signature is valid, then the host is provided with assurance that the software possesses the particular property. If the signature is not valid, then the host is provided with no such assurance.", "num_citations": "80\n", "authors": ["147"]}
{"title": "Plan-based terminological reasoning\n", "abstract": " Plan-based terminological reasoning | Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsKR'91Plan-based terminological reasoning Article Plan-based terminological reasoning Share on Authors: Prem T Devanbu profile image Premkumar T. Devanbu View Profile , Diane Judith Litman profile image Diane J. Litman View Profile Authors Info & Affiliations Publication: KR'91: Proceedings of the Second International Conference on Principles of Knowledge Representation and ReasoningApril 1991 Pages 128\u2013138 0\u2026", "num_citations": "74\n", "authors": ["147"]}
{"title": "Will they like this? Evaluating code contributions with language models\n", "abstract": " Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected change sets do contain code significantly less similar to the project than accepted ones, furthermore, the less similar change sets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.", "num_citations": "72\n", "authors": ["147"]}
{"title": "Taxonomic plan reasoning\n", "abstract": " Abstract clasp (CLAssification of scenarios and Plans) is a knowledge representation system that extends the notion of subsumption from terminological languages to plans. The clasp representation language provides description-forming operators that specify temporal and conditional relationships between actions represented in classic (a current subsumption-based knowledge representation language). clasp supports subsumption inferences between plan concepts and other plan concepts, as well as between plan concepts and plan instances. These inferences support the automatic creation of a plan taxonomy. Subsumption in clasp builds on term subsumption in classic and illustrates how term subsumption can be exploited to serve special needs. In particular, the clasp algorithms for plan subsumption integrate work in automata theory with work in term subsumption.", "num_citations": "65\n", "authors": ["147"]}
{"title": "System and method for using container documents as multi-user domain clients\n", "abstract": " A multi-user domain is implemented in a compound document framework to collaboratively modify a compound document in accordance with a concurrency model. A server hosts a multi-user domain in which a plurality of clients participate. The multi-user domain includes a compound document having a shared container part and at least one shared content part. Each participating client can use its own part editors to modify the document. Modification information is sent to the server, which updates the shared document to other clients in accordance with a concurrency model.", "num_citations": "64\n", "authors": ["147"]}
{"title": "Adding more \u201cDL\u201d to IDL: towards more knowledgeable component inter-operability\n", "abstract": " In an open component market place, interface description languages (IDLs), such as CORBA\u2019S, pro-vide for the consumer only a weak guarantee (concerning type signatures) that a software service will work in a particular context as anticipated. Stronger guarantees, regarding the intended semantics of the service, would help, especially if formalized in a language that allowed effective, automatic and static checking of compatibility between the server and the client\u2019s service descriptionsWe propose an approach based on a family of formalisms called description logics (DLs), providing three examples of the use of DLs to augment IDL:(1) for the CORBA Cos Relationship service;(2) for capturing information models described using STEP Express, the IS0 standard language used in the manufacturing domain (and a basis of the OMG PDM effort); and (3) constraints involving methods.", "num_citations": "63\n", "authors": ["147"]}
{"title": "Assert use in github projects\n", "abstract": " Asserts have long been a strongly recommended (if non-functional) adjunct to programs. They certainly don't add any user-evident feature value; and it can take quite some skill and effort to devise and add useful asserts. However, they are believed to add considerable value to the developer. Certainly, they can help with automated verification; but even in the absence of that, claimed advantages include improved understandability, maintainability, easier fault localization and diagnosis, all eventually leading to better software quality. We focus on this latter claim, and use a large dataset of asserts in C and C++ programs to explore the connection between asserts and defect occurrence. Our data suggests a connection: functions with asserts do have significantly fewer defects. This indicates that asserts do play an important role in software quality; we therefore explored further the factors that play a role in assertion\u00a0\u2026", "num_citations": "60\n", "authors": ["147"]}
{"title": "Techniques for trusted software engineering\n", "abstract": " How do we decide if it is safe to run a given piece of software on our machine? Software used to arrive in shrink-wrapped packages from known vendors. But increasingly, software of unknown provenance arrives over the internet as applets or agents. Running such software risks serious harm to the hosting machine. Risks include serious damage to the system and loss of private information. Decisions about hosting such software are preferably made with good knowledge of the software product itself, and of the software process used to build it. We use the term Trusted Software Engineering to describe tools and techniques for constructing safe software artifacts in a manner designed to inspire trust in potential hosts. Existing approaches have considered issues such as schedule, cost and efficiency; we argue that the traditionally software engineering issues of configuration management and intellectual property\u00a0\u2026", "num_citations": "60\n", "authors": ["147"]}
{"title": "Determining dynamic properties of programs\n", "abstract": " Techniques for obtaining information about dynamic properties of a first program. The techniques involve making a static analysis of the first program to determine what instrumentation is needed to produce the desired information and then producing a second program which causes a programmable instrumentation apparatus to produce an instrumented version of the first program. The programmable instrumentation apparatus may be a programmable binary editor or aa debugger. Execution of the instrumented program provides the information about the dynamic properties. The program for the instrumentation apparatus is produced by a source code analyzer which statically analyzes the source code. The source code analyzer is in turn produced by a code analyzer generator. Output from the instrumentation may be provided to a graphical display system. In one use of the techniques, the code analyzer generator\u00a0\u2026", "num_citations": "56\n", "authors": ["147"]}
{"title": "The use of description logics in KBSE systems\n", "abstract": " The increasing size and complexity of many software systems demand a greater emphasis on capturing and maintaining knowledge at many different levels within the software development process. This knowledge includes descriptions of the hardware and software components and their behavior, external and internal design specifications, and support for system testing. The Knowledge-based software engineering (KBSE) research paradigm is concerned with systems that use formally represented knowledge, with associated inference precedures, to support the various subactivities of software development. As they growing scale, KBSE systems must balance expressivity and inferential power with the real demands of knowledge base construction, maintenance, performance, and comprehensibility. Description logics (DLs) possess several features\u2014a terminological orientation, a formal semantics, and efficient\u00a0\u2026", "num_citations": "56\n", "authors": ["147"]}
{"title": "Cacheca: A cache language model based code suggestion tool\n", "abstract": " Nearly every Integrated Development Environment includes a form of code completion. The suggested completions (\"suggestions\") are typically based on information available at compile time, such as type signatures and variables in scope. A statistical approach, based on estimated models of code patterns in large code corpora, has been demonstrated to be effective at predicting tokens given a context. In this demo, we present CACHECA, an Eclipse plug in that combines the native suggestions with a statistical suggestion regime. We demonstrate that a combination of the two approaches more than doubles Eclipse's suggestion accuracy. A video demonstration is available at https://www.youtube.com/watch?v=3INk0N3JNtc.", "num_citations": "55\n", "authors": ["147"]}
{"title": "Expertise identification and visualization from CVS\n", "abstract": " As software evolves over time, the identification of expertise becomes an important problem. Component ownership and team awareness of such ownership are signals of solid project. Ownership and ownership awareness are also issues in open-source software (OSS) projects. Indeed, the membership in OSS projects is dynamic with team members arriving and leaving. In large open source projects, specialists who know the system very well are considered experts. How can one identify the experts in a project by mining a particular repository like the source code? Have they gotten help from other people?", "num_citations": "53\n", "authors": ["147"]}
{"title": "Stack and queue integrity on hostile platforms\n", "abstract": " When computationally intensive tasks have to be carried out on trusted, but limited platforms such as smart cards, it becomes necessary to compensate for the limited resources (memory, CPU speed) by off-loading implementations of data structures onto an available (but insecure, untrusted) fast coprocessor. However, data structures such as stacks, queues, RAMs, and hash tables can be corrupted (and made to behave incorrectly) by a potentially hostile implementation platform or by an adversary knowing or choosing data structure operations. The paper examines approaches that can detect violations of data structure invariants, while placing limited demands on the resources of the secure computing platform.", "num_citations": "52\n", "authors": ["147"]}
{"title": "To what extent could we detect field defects? an empirical study of false negatives in static bug finding tools\n", "abstract": " Software defects can cause much loss. Static bug-finding tools are believed to help detect and remove defects. These tools are designed to find programming errors; but, do they in fact help prevent actual defects that occur in the field and reported by users? If these tools had been used, would they have detected these field defects, and generated warnings that would direct programmers to fix them? To answer these questions, we perform an empirical study that investigates the effectiveness of state-of-the-art static bug finding tools on hundreds of reported and fixed defects extracted from three open source programs: Lucene, Rhino, and AspectJ. Our study addresses the question: To what extent could field defects be found and detected by state-of-the-art static bug-finding tools? Different from past studies that are concerned with the numbers of false positives produced by such tools, we address an orthogonal issue\u00a0\u2026", "num_citations": "48\n", "authors": ["147"]}
{"title": "Mining stack exchange: Expertise is evident from initial contributions\n", "abstract": " Stack Exchange is a very popular Question and Answer internet community. Users can post questions on a wide variety of topics, other users provide answers, usually within minutes. Participants are not compensated for their services and anyone can freely gain value from the efforts of the users, Stack Exchange is therefore a gift economy. Users, however, do gain reputation points when other users \" upvote\" their questions and/or answers. Stack Exchange thus functions as a learning community with a strong reputation-seeking element that creates a valuable public good, viz. the question and answer archive. The incentive structure of the community suggests that over time, the quality of the product (viz., delivered answers) steadily improves, and furthermore, that any individual who durably participates in this community for an extended period also would enjoy an increase in the quality of their output (viz., the\u00a0\u2026", "num_citations": "46\n", "authors": ["147"]}
{"title": "Structure and dynamics of research collaboration in computer science\n", "abstract": " Complex systems exhibit emergent patterns of behavior at different levels of organization. Powerful network analysis methods, developed in physics and social sciences, have been successfully used to tease out patterns that relate to community structure and network dynamics. In this paper, we mine the complex network of collaboration relationships in computer science, and adapt these network analysis methods to study collaboration and interdisciplinary research at the individual, within-area and network-wide levels. We start with a collaboration graph extracted from the DBLP bibliographic database and use extrinsic data to define research areas within computer science. Using topological measures on the collaboration graph, we find significant differences in the behavior of individuals among areas based on their collaboration patterns. We use community structure analysis, betweenness centralization, and\u00a0\u2026", "num_citations": "43\n", "authors": ["147"]}
{"title": "Visualizing social interaction in open source software projects\n", "abstract": " Open source software projects such as Apache and Mozilla present an opportunity for information visualization. Since these projects typically require collaboration between developers located far apart, the amount of electronic communication between them is large. Our goal is to apply information visualization techniques to assist software engineering scientists and project managers with analyzing the data. We present a visualization technique that provides an intuitive, time-series, interactive summary view of the social groups that form, evolve and vanish during the entire lifetime of the project. This visualization helps software engineering researchers understand the organization, structure, and evolution of the communication and collaboration activities of a large, complex software project.", "num_citations": "43\n", "authors": ["147"]}
{"title": "Validity of network analyses in open source projects\n", "abstract": " Social network methods are frequently used to analyze networks derived from Open Source Project communication and collaboration data. Such studies typically discover patterns in the information flow between contributors or contributions in these projects. Social network metrics have also been used to predict defect occurrence. However, such studies often ignore or side-step the issue of whether (and in what way) the metrics and networks of study are influenced by inadequate or missing data. In previous studies email archives of OSS projects have provided a useful trace of the communication and co-ordination activities of the participants. These traces have been used to construct social networks that are then subject to various types of analysis. However, during the construction of these networks, some assumptions are made, that may not always hold; this leads to incomplete, and sometimes incorrect networks\u00a0\u2026", "num_citations": "41\n", "authors": ["147"]}
{"title": "The Willow survivability architecture\n", "abstract": " The Willow architecture provides a comprehensive architectural approach to the provision of survivability [8] in critical information networks. It is based on the notion that survivability of a network requires reconfiguration at both the system and the application levels. The Willow notion of reconfiguration is very general, and the architecture provides reconfiguration mechanisms for both automatic and manual network control.", "num_citations": "41\n", "authors": ["147"]}
{"title": "Talk and work: a preliminary report\n", "abstract": " Developers in Open Source Software (OSS) projects communicate using mailing lists. By convention, the mailing lists used only for task-related discussions, so they are primarily concerned with the software under development, and software process issues (releases, etc.). We focus on the discussions concerning the software, and study the frequency with which software entities (functions, methods, classes, etc) are mentioned in the mail. We find a strong, striking, cumulative relationship between this mention count in the email, and the number of times these entities are included in changes to the software. When we study the same phenomena over a series of time-intervals, the relationship is much less strong. This suggests some interesting avenues for future research.", "num_citations": "36\n", "authors": ["147"]}
{"title": "Measuring the effect of social communications on individual working rhythms: A case study of open source software\n", "abstract": " This paper proposes novel quantitative methods to measure the effects of social communications on individual working rhythms by analyzing the communication and code committing records in tens of Open Source Software (OSS) projects. Our methods are based on complex network and time-series analysis. We define the notion of a working rhythm as the average time spent on a commit task and we study the correlation between working rhythm and communication frequency. We build communication networks for code developers, and find that the developers with higher social status, represented by the nodes with larger number of outgoing or incoming links, always have faster working rhythms and thus contribute more per unit time to the projects. We also study the dependency between work (committing) and talk (communication) activities, in particular the effect of their interleaving. We introduce multi-activity\u00a0\u2026", "num_citations": "33\n", "authors": ["147"]}
{"title": "Translating description logics to information server queries\n", "abstract": " Description Logic (DL) Systems can be useful as front-ends for databases, particularly in applications that involve browsing and exploring, such as data mining. Such use of DL sy~ terns raises some pragmatic and theoretical issues, In this paper, we describe a general architecture for \u201cloose coupling\u201d DL systems with databases; in the context of this architecture, we have built a system called qindJ that can generate, from a specification, a translator from a descrip tion logic to a databaee query language. QINDL was used to generate translators from CLASSIC, a typical DL, to 3 different database query languages. We also present one view of safety of CLASSIC when it is used to query databsses, and present a semantical basis for this view.1 introductionThere is currently a great deal of interest in \u201cdata mining\u201d applications, where a large source of data is combed for useful knowledge[15]. Following[4], we distinguish\u00a0\u2026", "num_citations": "33\n", "authors": ["147"]}
{"title": "An empirical study on the influence of pattern roles on change-proneness\n", "abstract": " Identifying change-prone sections of code can help managers plan and allocate maintenance effort. Design patterns have been used to study change-proneness and are widely believed to support certain kinds of\u00a0changes, while inhibiting others. Recently, several studies have analyzed recorded changes to classes playing design pattern roles and find that the patterns \u201cfolklore\u201d offers a reasonable explanation for the reality: certain pattern roles do seem to be less change-prone than others. We push this analysis on two fronts: first, we deploy W. Pree\u2019s metapatterns, which group patterns purely by structure (rather than intent), and argue that metapatterns are a simpler model to explain recent findings by\u00a0Di Penta et al. (2008). Second, we study the effect of the size of the classes playing the design pattern and metapattern roles. We find that size explains more of the variance in change-proneness than either\u00a0\u2026", "num_citations": "32\n", "authors": ["147"]}
{"title": "Generating wrappers for command line programs: the Cal-Aggie Wrap-O-Matic project\n", "abstract": " Software developers writing new software have strong incentives to make their products compliant to standards such as CORBA, COM, and Java Beans. Standards compliance facilitates interoperability, component based software assembly, and software reuse, thus leading to improved quality and productivity. Legacy software, on the other hand, is usually monolithic and hard to maintain and adapt. Many organizations, saddled with entrenched legacy software, are confronted with the need to integrate legacy assets into more modern, distributed, componentized systems that provide critical business services. Thus, wrapping legacy systems for interoperability has been an area of considerable interest. Wrappers are usually constructed by hand which can be costly and error-prone. We specifically target command-line oriented legacy systems and describe a tool framework that automates away some of the drudgery\u00a0\u2026", "num_citations": "32\n", "authors": ["147"]}
{"title": "Modeling and verifying a broad array of network properties\n", "abstract": " Motivated by widely observed examples in nature, society and software, where groups of related nodes arrive together and attach to existing networks, we consider network growth via sequential attachment of linked node groups or graphlets. We analyze the simplest case, attachment of the three node\\bigvee-graphlet, where, with probability \u03b1, we attach a peripheral node of the graphlet, and with probability (1-\u03b1), we attach the central node. Our analytical results and simulations show that tuning \u03b1 produces a wide range in degree distribution and degree assortativity, achieving assortativity values that capture a diverse set of many real-world systems. We introduce a fifteen-dimensional attribute vector derived from seven well-known network properties, which enables comprehensive comparison between any two networks. Principal Component Analysis of this attribute vector space shows a significantly larger\u00a0\u2026", "num_citations": "30\n", "authors": ["147"]}
{"title": "Focus-shifting patterns of OSS developers and their congruence with call graphs\n", "abstract": " Developers in complex, self-organized open-source projects often work on many different files, and over time switch focus between them. Shifting focus can have impact on the software quality and productivity, and is thus an important topic of investigation. In this paper, we study focus shifting patterns (FSPs) of developers by comparing trace data from a dozen open source software (OSS) projects of their longitudinal commit activities and file dependencies from the projects call graphs. Using information theoretic measures of network structure, we find that fairly complex focus-shifting patterns emerge, and FSPs in the same project are more similar to each other. We show that developers tend to shift focus along with, rather than away from, software dependency links described by the call graphs. This tendency becomes weaker as either the interval between successive commits, or the organizational distance\u00a0\u2026", "num_citations": "29\n", "authors": ["147"]}
{"title": "A comparative study of inductive logic programming methods for software fault prediction\n", "abstract": " We evaluate inductive logic programming (ILP) methods for predicting fault density in C++ classes. In this problem, each training example is a C++ class de nition, represented as a calling tree, and labeled as\\positive\" i faults (ie, errors) were discovered in its implementation. We compare two ILP systems, FOIL and FLIPPER, and explore the reasons for their di ering performance, using both natural and arti cial data. We then propose two extensions to FLIPPER: a user-directed bias towards easy-to-evaluate clauses, and an extension that allows FLIPPER to learn\\counting clauses\". Counting clauses augment logic programs with a variation of the\\number restrictions\" used in description logics, and signi cantly improve performance on this problem when prior knowledge is used.", "num_citations": "29\n", "authors": ["147"]}
{"title": "Apparatus and methods for source code discovery\n", "abstract": " Apparatus for discovering information about the source code of a computer program. The apparatus includes a translator for translating a parse tree produced for the program's language in a specific programming environment into an independent parse tree and an analyzer for analyzing the independent parse tree to discover the information. The translator is generated by a translator generator from a specification for the language and the programming environment and the analyzer is generated by an analyzer generator from a specification of the information to be discovered. The specification of the information to be discovered may specify only methods of analysis which can be completed in polynomial time. Methods of using the apparatus are also disclosed.", "num_citations": "27\n", "authors": ["147"]}
{"title": "Cryptographic verification of test coverage claims\n", "abstract": " The market for software components is growing, driven on the \u2018&demand side\u201d by the need for rapid deployment of highly functional products, and on the \u201csupply side\u201d by distributed object standards. As components and component vendors proliferate, there is naturally a growing concern about quality, and the effectiveness of testing processes. White box testing, particularly the use of coverage criteria, is a widely used method for measuring the \u2018thoroughness\u201d of testing efforts. High levels of test coverage are used as indicators of good quality control procedures. Software vendors who can demonstrate high levels of test coverage have a credible claim to high quality. However, verifying such claims involves knowledge of the source code, test cases, build procedures etc. In applications where reliability and quality are critical, it would be desirable to verify test coverage claims without forcing vendors to give up\u00a0\u2026", "num_citations": "26\n", "authors": ["147"]}
{"title": "Improving scientific software component quality through assertions\n", "abstract": " We are proposing research on self-adaptive interface assertion enforcement for the purposes of improving scientific software component quality. Demonstrating software correctness through assertions is a well-known technique for quality improvement. However, the performance penalty is often considered too high for deployment. In order to determine if partial enforcement based on adaptive sampling is a viable solution in performance critical environments, we are pursuing research on mechanisms combining static and dynamic analyses to efficiently maximize assertion checking within performance constraints. This paper gives an overview of our initial experiments, current work, and plans.", "num_citations": "22\n", "authors": ["147"]}
{"title": "Bend, don\u2019t break: Using reconfiguration to achieve survivability\n", "abstract": " Our national interests are becoming increasingly dependent on the continuous, proper functioning of large-scale, heterogeneous, and decentralized computing enterprises. Examples of such systems abound, ranging from military command and control to vital national security assets such as the financial and banking system. They are formed from large numbers of components originating from multiple sources, some trusted and some not, assembled into complex and dynamically evolving structures. Protecting these interests is critical, yet their sheer scale and diversity has gone far beyond our organizational and technical abilities to protect them. Manual procedures\u2014however well designed and tested\u2014cannot keep pace with the dynamicity of the environment and cannot react to security breaches in a timely and coordinated fashion, especially in the context of a networked enterprise.We are designing a secure, automated framework for proactive and reactive reconfiguration of large-scale, heterogeneous, distributed systems so that critical networked computing enterprises can tolerate intrusions and continue to provide an acceptable level of service. Proactive reconfiguration adds, removes, and replaces components and interconnections to cause a system to assume postures that achieve enterprise-wide intrusion tolerance goals, such as increased resilience to specific kinds of attacks or increased preparedness for recovery from specific kinds of failures. Proactive reconfiguration can also cause a relaxation of tolerance procedures once a threat has passed, in order to reduce costs, increase system performance, or even restore previously\u00a0\u2026", "num_citations": "22\n", "authors": ["147"]}
{"title": "An aspect-oriented approach to bypassing middleware layers\n", "abstract": " The layered architecture of middleware platforms (such as CORBA, SOAP, J2EE) is a mixed blessing. On the one hand, layers provide services such as demarshaling, session management, request despatching, quality-of-service (QoS) etc. In a typical middleware platform, every request passes through each layer, whether or not the services provided by that layer are needed for that specific request. This rigid layer processing can lower overall system throughput, and reduce availability and/or increase vulnerability to denial-of-service attacks. For use cases where the response is a simple function of the request input parameters, bypassing middleware layers may be permissible and highly advantageous. Unfortunately, if an application developer desires to selectively bypass the middleware, and process some requests in the lower layer, she has to write platform-specific, intricate low-level code. To evade this trap\u00a0\u2026", "num_citations": "21\n", "authors": ["147"]}
{"title": "MoHCA-Java: a tool for C++ to Java conversion support\n", "abstract": " As Java increases in popularity and maturity, many people find it desirable to convert legacy C++ or C programs to Java. Our hypothesis is that a tool which performs rigorous analysis on a C++ program, providing detailed output on the changes necessary, will make conversion a much more efficient and reliable process. MoHCA-Java is such a tool. It performs detailed analysis on a C++ abstract syntax tree; the parameters of the analysis can be specified and extended very quickly and easily using a rule-based language. We have found that MoHCA-Java is very useful for identifying and implementing source code changes, and that its extensibility is a very important factor, specially to adapt the tool to assist in the conversion of C++ code that makes extensive use of libraries to Java code that uses similar libraries.", "num_citations": "21\n", "authors": ["147"]}
{"title": "Security for automated, distributed configuration management\n", "abstract": " Installation, configuration, and administration of desktop software is a non-trivial process. Even a simple application can have numerous dependencies on hardware, device drivers, operating system versions, dynamically linked libraries, and even on other applications. These dependencies can cause surprising failures during the normal process of installations, updates and re-configurations. Diagnosing and resolving such failures involves detailed knowledge of the hardware and software installed in the machine, configuration manifests of particular applications, version incompatibilities, etc. This is usually too hard for end-users, and even for technical support personnel, specially in small businesses. It may be necessary to involve software vendors and outside consultants or laboratories. Employees working on sensitive, proprietary projects may even have to resort to calling the help line of an application vendor and discussing details of their desktop configuration. In order establish valid licensing, the user may be forced to disclose additional details such as the user\u2019s identity, machine identification, software serial number, etc. This type of disclosure may reveal proprietary information or (worse) security vulnerabilities, and increase the risk of attack by hackers or cyber-criminals. An adequate solution to the distributed configuration management problem needs to address the security concerns of users, administrators, software vendors and outside consultants: keeping details of installations private, authenticating licensed users and software vendors, protecting the integrity of software, secure delegation across administrative boundaries, and\u00a0\u2026", "num_citations": "21\n", "authors": ["147"]}
{"title": "When would this bug get reported?\n", "abstract": " Not all bugs in software would be experienced and reported by end users right away: Some bugs manifest themselves quickly and may be reported by users a few days after they get into the code base; others manifest many months or even years later, and may only be experienced and reported by a small number of users. We refer to the period of time between the time when a bug is introduced into code and the time when it is reported by a user as bug reporting latency. Knowledge of bug reporting latencies has an implication on prioritization of bug fixing activities-bugs with low reporting latencies may be fixed earlier than those with high latencies to shift debugging resources towards bugs highly concerning users. To investigate bug reporting latencies, we analyze bugs from three Java software systems: AspectJ, Rhino, and Lucene. We extract bug reporting data from their version control repositories and bug\u00a0\u2026", "num_citations": "20\n", "authors": ["147"]}
{"title": "Toward a software information system\n", "abstract": " A software information system collects information about large software systems in knowledge and data bases. It includes applications built on these bases to provide easy access to software information or to automate elements of the software development process. Work in this area is proceeding at AT&T Bell Laboratories in three related directions. The C Information Abstraction system automatically extracts information from C language programs and stores it in a relational database. It includes a growing collection of applications of this information. The MView system provides graphical views of interrelated software and lets a user quickly browse software systems. The LaSSIE system represents large amounts of software information in a formal language and provides intelligent assistance in retrieving software components. Progress in these three directions is reviewed in this paper.", "num_citations": "20\n", "authors": ["147"]}
{"title": "To what extent could we detect field defects? An extended empirical study of false negatives in static bug-finding tools\n", "abstract": " Software defects can cause much loss. Static bug-finding tools are designed to detect and remove software defects and believed to be effective. However, do such tools in fact help prevent actual defects that occur in the field and reported by users? If these tools had been used, would they have detected these field defects, and generated warnings that would direct programmers to fix them? To answer these questions, we perform an empirical study that investigates the effectiveness of five state-of-the-art static bug-finding tools (FindBugs, JLint, PMD, CheckStyle, and JCSC) on hundreds of reported and fixed defects extracted from three open source programs (Lucene, Rhino, and AspectJ). Our study addresses the question: To what extent could field defects be detected by state-of-the-art static bug-finding tools? Different from past studies that are concerned with the numbers of false positives produced by\u00a0\u2026", "num_citations": "18\n", "authors": ["147"]}
{"title": "Database techniques for the analysis and exploration of software repositories\n", "abstract": " In a typical software engineering project, there is a large and diverse body of documents that a development team produces, including requirement documents, specifications, designs, code, and bug reports. Documents typically have different formats and are managed in several repositories. The heterogeneity among document formats and the diversity of repositories make it often not feasible to query and explore the repositories in an integrated and transparent fashion during the different phases of the software development process. Here, we present a framework for the analysis and exploration of software repositories. Our approach applies database techniques to integrate, and manage different documents produced by a team. Tools that exploit the database functionality then allow for the processing of complex queries against a document collection to extract trends and analyze correlations, which provide\u00a0\u2026", "num_citations": "16\n", "authors": ["147"]}
{"title": "Databases that tell the Truth: Authentic Data Publication.\n", "abstract": " The publication of high-value and mission critical data on the Internet plays an important role in the government, industry, and health-care sectors. However, owners of such data are often not able or willing to serve millions of query requests per day and furthermore satisfy clients\u2019 data requirements regarding the integrity, availability, and authenticity of the data they manage in their databases. In this article, we give an overview of our work on authentic publication schemes in which a data owner employs a (possibly untrusted) data publisher to answer queries from clients on behalf of the owner. In addition to query answers, publishers provide clients with verification objects a client uses to verify whether the answer is the same as the owner would have provided. We consider two popular types of database systems, those managing relational data and those managing XML data in the form of XML repositories.", "num_citations": "15\n", "authors": ["147"]}
{"title": "Research directions for automated software verification: Using trusted hardware\n", "abstract": " Service providers hosting software on servers at the request of content providers need assurance that the hosted software has no undesirable properties. This problem applies to browsers which host applets, networked software which can host software agents, etc. The hosted software's properties are currently verified by testing and/or verification processes by the hosting computer. This increases cost, causes delay, and leads to difficulties in version control. By furnishing content providers with a physically secure computing device with an embedded certified private key, such properties can be verified and/or enforced by the secure computing device at the content provider's site; the secure device can verify such properties, statically whenever possible, and by inserting checks into the executable binary when necessary. The resulting binary is attested by a trusted signature, and can be hosted with confidence. The\u00a0\u2026", "num_citations": "15\n", "authors": ["147"]}
{"title": "Converging work-talk patterns in online task-oriented communities\n", "abstract": " Much of what we do is accomplished by working collaboratively with others, and a large portion of our lives are spent working and talking; the patterns embodied in the alternation of working and talking can provide much useful insight into task-oriented social behaviors. The available electronic traces of the different kinds of human activities in online communities are an empirical goldmine that can enable the holistic study and understanding of these social systems. Open Source Software (OSS) projects are prototypical examples of collaborative, task-oriented communities, depending on volunteers for high-quality work. Here, we use sequence analysis methods to identify the work-talk patterns of software developers in online communities of Open Source Software projects. We find that software developers prefer to persist in same kinds of activities, i.e., a string of work activities followed by a string of talk activities and so forth, rather than switch them frequently; this tendency strengthens with time, suggesting that developers become more efficient, and can work longer with fewer interruptions. This process is accompanied by the formation of community culture: developers\u2019 patterns in the same communities get closer with time while different communities get relatively more different. The emergence of community culture is apparently driven by both \u201ctalk\u201d and \u201cwork\u201d. Finally, we also find that workers with good balance between \u201cwork\u201d and \u201ctalk\u201d tend to produce just as much work as those that focus strongly on \u201cwork\u201d; however, the former appear to be more likely to continue to be active contributors in the communities.", "num_citations": "14\n", "authors": ["147"]}
{"title": "New initiative: The naturalness of software\n", "abstract": " This paper describes a new research consortium, studying the Naturalness of Software. This initiative is supported by a pair of grants by the US National Science Foundation, totaling $2,600,000: the first, exploratory (\"EAGER\") grant of $600,000 helped kickstart an inter-disciplinary effort, and demonstrate feasibility; a follow-on full grant of $2,000,000 was recently awarded. The initiative is led by the author, who is at UC Davis, and includes investigators from Iowa State University and Carnegie-Mellon University (Language Technologies Institute).", "num_citations": "14\n", "authors": ["147"]}
{"title": "Adaptable assertion checking for scientific software components\n", "abstract": " We present a proposal for lowering the overhead of interface contract checking for science and engineering applications. Run-time enforcement of assertions is a well-known technique for improving the quality of software; however, the performance penalty is often too high for their retention during deployment, especially for long-running applications that depend upon iterative operations. With an efficient adaptive approach the benefits of run-time checking can continue to accrue with minimal overhead. Examples from scientific software interfaces being developed in the high performance computing research community will be used to measure the efficiency and effectiveness of this approach.", "num_citations": "14\n", "authors": ["147"]}
{"title": "Extracting formal domain models from exsisting code for generative reuse\n", "abstract": " The existence of a domain model that formally describes the meaning of a piece of software can be helpful for re-use. We illustrate an approach, where by reengineering a formal domain model out of an existing system, one can generalize the existing system to an application generator: we map a domain model for program representations into the front end of a compiler, using the Genii system, and use this in Genoa, an application generator for language tools. This approach shows promise in other domains, as well.", "num_citations": "12\n", "authors": ["147"]}
{"title": "On\" A framework for source code search using program patterns\"\n", "abstract": " The need to query and understand source code is an important practical problem for software engineers in large development projects. A paper by Paul and Prakash (1994) proposes a workable solution to this problem. However, there are several previously reported systems that can also address this problem. The relationship of their work to the body of existing work is the subject of the paper.", "num_citations": "12\n", "authors": ["147"]}
{"title": "Studying the difference between natural and programming language corpora\n", "abstract": " Code corpora, as observed in large software systems, are now known to be far more repetitive and predictable than natural language corpora. But why? Does the difference simply arise from the syntactic limitations of programming languages? Or does it arise from the differences in authoring decisions made by the writers of these natural and programming language texts? We conjecture that the differences are not entirely due to syntax, but also from the fact that reading and writing code is un-natural for humans, and requires substantial mental effort; so, people prefer to write code in ways that are familiar to both reader and writer. To support this argument, we present results from two sets of studies: 1) a first set aimed at attenuating the effects of syntax, and 2) a second, aimed at measuring repetitiveness of text written in other settings (e.g. second language, technical/specialized jargon), which are also\u00a0\u2026", "num_citations": "11\n", "authors": ["147"]}
{"title": "Stable, flexible, peephole pretty-printing\n", "abstract": " Programmers working on large software systems are faced with an extremely complex, information-rich environment. To help navigate through this, modern development environments allow flexible, multi-window browsing and exploration of the source code. Our focus in this paper is on pretty-printing algorithms that can display source code in useful, appealing ways in a variety of styles. Our algorithm is flexible, stable, and peephole-efficient. It is flexible in that it is capable of screen-optimized layouts that support source code visualization techniques such as fisheye views. The algorithm is peephole-efficient, in that it performs work proportional to the size of the visible window and not the size of the entire file. Finally, the algorithm is stable, in that the rendered view is identical to that which would be produced by formatting the entire file. This work has 2 benefits. First, it enables rendering of source codes in multiple\u00a0\u2026", "num_citations": "11\n", "authors": ["147"]}
{"title": "Trust Mediation for Distributed Information Systems\n", "abstract": " Distributed information systems are increasing in prevalence and complexity as we see an increase in the number of both information consumers and information providers. Applications often need to integrate information from several different information providers. Current approaches for securing this process of integration do not scale well to handle complex trust relationships between consumer applications and providers. Trust mediation is a technique we introduce to address this problem by incorporating a model for representing trust into a framework for retrieving information in a distributed system. Our model for representing trust uses a type system by which data from a source is labeled with a trust type based on qualities of the data itself or the information source(s) providing the data. With this model we develop algorithms to perform static analysis of data queries to infer how the result of the data\u00a0\u2026", "num_citations": "10\n", "authors": ["147"]}
{"title": "The ultimate reuse nightmare: Honey, i got the wrong dll\n", "abstract": " Software reuse is now a reality. If you think I\u2019m kidding, take a look at the installation of any major desktop software product, and ask yourself this question: Which files belong only to this application, and are not reused? OK, quick, can you tell? Is this really a word processing application, or is it a COM component that is also used by a spread sheet application and my presentation application? Does this Java class belong to the internet browser, or my accounting package? Who knows? More importantly, who cares?But that\u2019s exactly the point. Reuse should mean never having to say you care! Component builder Ahmed should be able to write a text formatting component TP, version 1.0, packages it up as a shareable library (DLL, or Java. class file) and ship it off to application developers Fuyuko and Est\u00e9fan. Fuyuko may use it to build an internet browser Nitscope, and Est\u00e9fan may use it in a game, Deadmeat. So, now, what do I mean about not caring? I mean this: Ahmed shouldn\u2019t care who is using TP; Fuyuko shouldn\u2019t care who else is using TP, and likewise Est\u00e9fan. Most importantly, the users of Nitscope and Deadmeat should never have to worry that Nitscope and Deadmeat both use the same component T P. Let us posit a poor user, Vic,(short for \u201cvictim\u201d) and track his plight. Let us assume that Vic has copies of both Nitscope and Deadmeat. Nitscope was purchased for Vic, and supported by his employer\u2019s tech support staff, and Deadmeat, he bought it on his own, though he shouldn\u2019t be running it on his company-bought PC.", "num_citations": "10\n", "authors": ["147"]}
{"title": "Gen++\u2014An analyzer generator for C++ Programs\n", "abstract": " 2 Background {The GENOA/GENII Querying Utility gen++ runs in two phases. In the translation phase, the source is parsed and typechecked to produce an intermediate representation, here called a\\decorated parse tree\", since it can be viewed as a parse tree enhanced with semantic information.(In practice, it may not match the input syntactically since some transformations are done during type checking.) In the analysis phase, the decorated parse tree is traversed and user-speci ed operations are performed. gen++ is based on GENOA/GENII, a portable, language independent analyzer generator system that can be used to create arbitrary analysis tools. Tools are implemented by writing speci cations in GENOA1, a special-purpose query language. The GENOA language has traversal, iteration and other operators designed to process abstract decorated parse trees, independent of the particular source language or front end implementation. GENOA", "num_citations": "10\n", "authors": ["147"]}
{"title": "On the naturalness of proofs\n", "abstract": " Proofs play a key role in reasoning about programs and verification of properties of systems. Mechanized proof assistants help users in developing and checking the consistency of proofs using the proof language developed by the systems; but even then writing proofs is tedious and could benefit from automated insight. In this paper, we analyze proofs in two different proof assistant systems (Coq and HOL Light) to investigate if there is evidence of\" naturalness\" in these proofs: viz., recurring linguistic patterns that are amenable to language models, in the way that programming languages are known to be. Such models could be used to find errors, rewrite proofs, help suggest dependencies, and perhaps even synthesize (steps of) proofs. We apply state-of-the-art language models to large corpora of proofs to show that this is indeed the case: proofs are remarkably predictable, much like other programming languages\u00a0\u2026", "num_citations": "9\n", "authors": ["147"]}
{"title": "MIC check: A correlation tactic for ESE data\n", "abstract": " Empirical software engineering researchers are concerned with understanding the relationships between outcomes of interest, e.g. defects, and process and product measures. The use of correlations to uncover strong relationships is a natural precursor to multivariate modeling. Unfortunately, correlation coefficients can be difficult and/or misleading to interpret. For example, a strong correlation occurs between variables that stand in a polynomial relationship; this may lead one mistakenly, and eventually misleadingly, to model a polynomially related variable in a linear regression. Likewise, a non-monotonic functional, or even non-functional relationship might be entirely missed by a correlation coefficient. Outliers can influence standard correlation measures, tied values can unduly influence even robust non-parametric rank correlation, measures, and smaller sample sizes can cause instability in correlation\u00a0\u2026", "num_citations": "9\n", "authors": ["147"]}
{"title": "THEX: Mining metapatterns from java\n", "abstract": " Design patterns are codified solutions to common object-oriented design (OOD) problems in software development. One of the proclaimed benefits of the use of design patterns is that they decouple functionality and enable different parts of a system to change frequently without undue disruption throughout the system. These OOD patterns have received a wealth of attention in the research community since their introduction; however, identifying them in source code is a difficult problem. In contrast, metapatterns have similar effects on software design by enabling portions of the system to be extended or modified easily, but are purely structural in nature, and thus easier to detect. Our long-term goal is to evaluate the effects of different OOD patterns on coordination in software teams as well as outcomes such as developer productivity and software quality. we present THEX, a metapattern detector that scales to large\u00a0\u2026", "num_citations": "9\n", "authors": ["147"]}
{"title": "Instant multi-tier web applications without tears\n", "abstract": " We describe how development productivity for multi-tier web-based database'forms' oriented applications can be significantly improved using'InstantApps', an interpretive framework that uses efficient runtime model interpretation and features an integrated'wysiwig''point-and click'design editor for developing forms, database schema, control flow, and functional logic. As compared to related academic as well as commercial work, our approach has the distinct advantage of retaining an industry standard architecture that yields high performance and enables model driven functionality to be augmented with hand-written extensions using a well known architectural style and leveraging standard skill sets. In particular, the interface's' look and feel'can be completely custom built even as the application functionality is developed using the instant'WYSIWYG'editor. Efficient implementation of interpretation and reflection\u00a0\u2026", "num_citations": "9\n", "authors": ["147"]}
{"title": "The Next Revolution: Free, Full, Open Person-2-Person (P2P) E-commerce\n", "abstract": " 1 BackgroundOur goal here is to present a long-term perspective on the evolution of a new, fine-grained type of e-commerce on the internet. We begin with a motivating scenario.Abuelita wants to buy the latest Neal Stephenson cypherpunk thriller. Almost without thinking, she visits the website of riverofbooks. com, a \u201ccategory killer\u201d e-tailer. She quickly finds what she wants, and with a few easy clicks, has purchased the book for $28.99, plus $4.99 shipping and handling. She can\u2019t wait for the book to arrive in 3 business days. Later that day, she rides her bicycle over to the local mall to buy herself a smoothie, and peeks in the window of local used-book seller, where a Lute concert is underway. Imagine her chagrin when she notices that the very same book is on sale in the window for $14.99, plus $2.61 in local taxes! She could have had the book, and supported her local book store (and the lute concert!). Bah humbug, she says to herself. What good is the internet, if it can\u2019t tell me that the same book is on sale in my local used bookstore for less?", "num_citations": "9\n", "authors": ["147"]}
{"title": "Automatically exploring hypotheses about fault prediction: A comparative study of inductive logic programming methods\n", "abstract": " We evaluate a class of learning algorithms known as inductive logic programming (ILP) methods on the task of predicting fault density in C++ classes. Using these methods, a large space of possible hypotheses is searched in an automated fashion; further, the hypotheses are based directly on an abstract logical representation of the software, eliminating the need to manually propose numerical metrics that predict fault density. We compare two ILP systems, FOIL and FLIPPER, and conclude that FLIPPER generally outperforms FOIL  on this problem. We analyze the reasons  for the differing performance of these two systems, and based on the analysis, propose two extensions to FLIPPER: a user-directed bias towards easy-to-evaluate clauses, and an extension that allows FLIPPER to learn \"counting clauses\". Counting clauses augment logic programs with a variation of the \"number restrictions\" used in description\u00a0\u2026", "num_citations": "9\n", "authors": ["147"]}
{"title": "Code to Comment \u201cTranslation\u201d: Data, Metrics, Baselining & Evaluation\n", "abstract": " The relationship of comments to code, and in particular, the task of generating useful comments given the code, has long been of interest. The earliest approaches have been based on strong syntactic theories of comment-structures, and relied on textual templates. More recently, researchers have applied deep-learning methods to this task-specifically, trainable generative translation models which are known to work very well for Natural Language translation (e.g., from German to English). We carefully examine the underlying assumption here: that the task of generating comments sufficiently resembles the task of translating between natural languages, and so similar models and evaluation metrics could be used. We analyze several recent code-comment datasets for this task: CODENN, DEEPCOM, FUNCOM, and Docstring. We compare them with WMT19, a standard dataset frequently used to train state-of-the-art\u00a0\u2026", "num_citations": "8\n", "authors": ["147"]}
{"title": "On the naturalness of software\n", "abstract": " Programming languages, like their\" natural\" counterparts, are rich, powerful and expressive. But while skilled writers like Zadie Smith, Aravind Adiga, and Salman Rushdie delight us with their elegant, creative deployment of the power and beauty of English, most of what us regular mortals say and write everyday is Very Repetitive and Highly Predictable.", "num_citations": "8\n", "authors": ["147"]}
{"title": "InstantApps: A WYSIWYG model driven interpreter for web applications\n", "abstract": " We describe InstantApps, a WYSIWIG, model driven interpreter for developing and running web based database oriented applications. Applications are dynamically rendered through efficient runtime interpretation of meta-data which is manipulated through an intuitive visual designer. We also capture complex (multi-table) forms, workflow as well as business logic (using a variant of Google's MapReduce abstraction), distinguishing our approach from similar platforms in the literature as well as others recently made available on the Internet.", "num_citations": "8\n", "authors": ["147"]}
{"title": "Test coverage in python programs\n", "abstract": " We study code coverage in several popular Python projects: flask, matplotlib, pandas, scikit-learn, and scrapy. Coverage data on these projects is gathered and hosted on the Codecov website, from where this data can be mined. Using this data, and a syntactic parse of the code, we examine the effect of control flow structure, statement type (e.g., if, for) and code age on test coverage. We find that coverage depends on control flow structure, with more deeply nested statements being significantly less likely to be covered. This is a clear effect, which holds up in every project, even when controlling for the age of the line (as determined by git blame). We find that the age of a line per se has a small (but statistically significant) positive effect on coverage. Finally, we find that the kind of statement (try, if, except, raise, etc) has varying effects on coverage, with exception-handling statements being covered much less often\u00a0\u2026", "num_citations": "7\n", "authors": ["147"]}
{"title": "A framework for flexible evolution in distributed heterogeneous systems\n", "abstract": " Distributed, heterogeneous systems are becoming very common, as globalized organizations integrate applications running on different platforms, possibly written in different languages. Certain requirements for such features as security, QoS, and flexible administration are specially critical to distributed heterogeneous systems. Unfortunately, such requirements are often formulated late, since they depend upon a particular installation, and/or change rapidly with business and political climate. Distributed, heterogeneous systems are particularly difficult to evolve, since the elements are written in different languages, and the operational environment is heterogeneous and distributed. We would like to address this problem with solutions that are animated by practical software engineering goals: type safety of scattered changes, and their interactions; explicit design models, with traceability to code; and inter-operability\u00a0\u2026", "num_citations": "7\n", "authors": ["147"]}
{"title": "Re-targetability in software tools\n", "abstract": " Software tool construction is a risky business, with uncertain rewards. Many tools never get used. This is a truism: software tools, however brilliantly conceived, well-designed, and meticulously constructed, have little impact unless they are actually adopted by real programmers. While there are no sure-fire ways of ensuring that a tool will be used, experience indicates that retargetability is an important enabler for wide adoption. In this paper, we elaborate on the need for retargetability in software tools, describe some mechanisms that have proven useful in our experience, and outline our future research in the broader area of inter-operability and retargetability.", "num_citations": "7\n", "authors": ["147"]}
{"title": "Learning lenient parsing & typing via indirect supervision\n", "abstract": " Both professional coders and teachers frequently deal with imperfect (fragmentary, incomplete, ill-formed) code. Such fragments are common in StackOverflow; students also frequently produce ill-formed code, for which instructors, TAs (or students themselves) must find repairs. In either case, the developer experience could be greatly improved if such code could somehow be parsed & typed; this makes such code more amenable to use within IDEs and allows early detection and repair of potential errors. We introduce a lenient parser, which can parse & type fragments, even ones with simple errors. Training a machine learner to leniently parse and type imperfect code requires a large training set including many pairs of imperfect code and its repair (and/or type information); such training sets are limited by human effort and curation. In this paper, we present a novel, indirectly supervised, approach to train a lenient\u00a0\u2026", "num_citations": "6\n", "authors": ["147"]}
{"title": "A theory of dual channel constraints\n", "abstract": " The surprising predictability of source code has triggered a boom in tools using language models for code. Code is much more predictable than natural language, but the reasons are not well understood. We propose a dual channel view of code; code combines a formal channel for specifying execution and a natural language channel in the form of identifiers and comments that assists human comprehension. Computers ignore the natural language channel, but developers read both and, when writing code for longterm use and maintenance, consider each channel\u2019s audience: computer and human. As developers hold both channels in mind when coding, we posit that the two channels interact and constrain each other; we call these dual channel constraints. Their impact has been neglected. We describe how they can lead to humans writing code in a way more predictable than natural language, highlight\u00a0\u2026", "num_citations": "6\n", "authors": ["147"]}
{"title": "Status, identity, and language: A study of issue discussions in GitHub\n", "abstract": " Successful open source software (OSS) projects comprise freely observable, task-oriented social networks with hundreds or thousands of participants and large amounts of (textual and technical) discussion. The sheer volume of interactions and participants makes it challenging for participants to find relevant tasks, discussions and people. Tagging (e.g., @AmySmith) is a socio-technical practice that enables more focused discussion. By tagging important and relevant people, discussions can be advanced more effectively. However, for all but a few insiders, it can be difficult to identify important and/or relevant people. In this paper we study tagging in OSS projects from a socio-linguistics perspective. First we argue that textual content per se reveals a great deal about the status and identity of who is speaking and who is being addressed. Next, we suggest that this phenomenon can be usefully modeled using modern deep-learning methods. Finally, we illustrate the value of these approaches with tools that could assist people to find the important and relevant people for a discussion.", "num_citations": "6\n", "authors": ["147"]}
{"title": "Extraction of contributor information from software repositories\n", "abstract": " Open-source projects derive their vitality and dynamism through the contributions of many volunteers. While only a relatively small number of people are developers, who have commit privileges, many others actually contribute to the source code. In this paper, we examine the relationship of contributors to developers, and how much help the contributors actually provide. To discover this information we mine a CVS data source to find bugs, submissions, and contributors per transaction. The output is the inner network of a particular developer or the contributors to the project through a developer. We enhance the output using a couple of information visualization metaphors that allow a better exploration of the contributors to a project. We present a prototype implementation that describes our research using the Apache HTTP Web server project as a case study.", "num_citations": "6\n", "authors": ["147"]}
{"title": "OntoCat: Automatically categorizing knowledge in API Documentation\n", "abstract": " Most application development happens in the context of complex APIs; reference documentation for APIs has grown tremendously in variety, complexity, and volume, and can be difficult to navigate. There is a growing need to develop well-organized ways to access the knowledge latent in the documentation; several research efforts deal with the organization (ontology) of API-related knowledge. Extensive knowledge-engineering work, supported by a rigorous qualitative analysis, by Maalej & Robillard [3] has identified a useful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain independent technique to extract the knowledge types from the given API reference documentation. Our system, OntoCat, introduces total nine different features and their semantic and statistical combinations to classify the different knowledge types. We tested OntoCat on python API reference documentation. Our experimental results show the effectiveness of the system and opens the scope of probably related research areas (i.e., user behavior, documentation quality, etc.).", "num_citations": "5\n", "authors": ["147"]}
{"title": "Aspect-oriented development of crosscutting features in distributed, heterogeneous systems\n", "abstract": " Some \u201cnon-\u201d or \u201cextra-functional\u201d features, such as reliability, security, and tracing, defy modularization mechanisms in programming languages. This makes such features hard to design, implement, and maintain. Implementing such features within a single platform, using a single language, is hard enough. With distributed, heterogeneous systems, these features induce complex implementations which crosscut different languages, OSs, and hardware platforms, while still needing to share data and events. Worse still, the precise requirements for such features are often locality-dependent and discovered late (e.g., security policies). This paper describes an interface description language, Dado, to help program crosscutting features in CORBA based middleware software through an aspect-oriented programming. A Dado specification comprises pairs of adaptlets which are explicitly modeled in an extended\u00a0\u2026", "num_citations": "5\n", "authors": ["147"]}
{"title": "Evolution in distributed heterogeneous systems\n", "abstract": " Distributed, heterogeneous systems are becoming very common, as globalized organizations integrate applications running on different platforms, possibly written in different languages. Component-interoperability standards such as CORBA are critical enablers of this trend. Certain non-functional requirements for such features as security, quality of service, flexible administration are specially critical to distributed heterogeneous systems. Unfortunately, such requirements are often formulated late, since they depend upon a particular installation, and/or change rapidly with business and political climate. Distributed, heterogeneous systems are particularly difficult to evolve, since the elements are written in different languages, and the operational environment is heterogenous and distributed. Adding \u201cnon-functional\u201d features late in the game is specially hard; the required modifications are scattered through the implementations of the different components. Their design and implementation is also obscured by code delocalization, as well as by complexities arising from co-ordination and synchronization considerations. We would like to address this problem with solutions that are animated by practical software engineering goals: type safety of scattered changes, and their interactions; explicit design models, with tracability to code; inter-operability with legacy components and binary COTS components; and opportunistic optimization, leveraging any available optimizations existing in the compilers that are in use.", "num_citations": "5\n", "authors": ["147"]}
{"title": "Accommodating evolution in AspectJ\n", "abstract": " The intent of Aspect-Oriented Programming is to encapsulate, reuse, and maintain concerns that crosscut modularization constructs provided in today\u2019s programming languages. As AOP solutions find their way into mainstream software development, complex systems, and libraries designed with aspects will eventually be prevalent. This new code-base will become the legacy code of the future and will be subject to the same evolutionary demands as today\u2019s code. Designers of tools and languages supporting AOP must be mindful of evolutionary pressures. Aspect-oriented programming systems based on program transformations [BP00] use powerful pattern-matching techniques to identify and apply crosscutting concerns in software. AspectJ1 [AJ], specifically, uses join points as program locations where advice may be weaved. An aspect refers to both sets of join points, called pointcuts, and the associated advice. The advice in an aspect is inserted at all join points associated with that advice. In many cases, it may be quite reasonable to define pointcuts applicable to a specific current state of a program, such as certain particular naming conventions, etc. Therein lurks the evolutionary problem. As the code base evolves, the pointcuts defined for the original code may capture more join points than originally intended (or even imagined)[GK01]. If the AOP system does not support the evolution of aspects, then it may be cumbersome to adjust the pointcuts or the applicable advice. This issue is addressed in AspectJ through such language features as abstract aspects, introduction, and aspect domination. This paper evaluates these features and\u00a0\u2026", "num_citations": "5\n", "authors": ["147"]}
{"title": "Whom are you going to call? determinants of@-mentions in github discussions\n", "abstract": " Open Source Software (OSS) project success relies on crowd contributions. When an issue arises in pull-request based systems, @-mentions are used to call on people to task; previous studies have shown that @-mentions in discussions are associated with faster issue resolution. In most projects there may be many developers who could technically handle a variety of tasks. But OSS supports dynamic teams distributed across a wide variety of social and geographic backgrounds, as well as levels of involvement. It is, then, important to know whom to call on, i.e., who can be relied or trusted with important task-related duties, and why. In this paper, we sought to understand which observable socio-technical attributes of developers can be used to build good models of them being future @-mentioned in GitHub issues and pull request discussions. We built overall and project-specific predictive models of future\u00a0\u2026", "num_citations": "4\n", "authors": ["147"]}
{"title": "Do people prefer\" natural\" code?\n", "abstract": " Natural code is known to be very repetitive (much more so than natural language corpora); furthermore, this repetitiveness persists, even after accounting for the simpler syntax of code. However, programming languages are very expressive, allowing a great many different ways (all clear and unambiguous) to express even very simple computations. So why is natural code repetitive? We hypothesize that the reasons for this lie in fact that code is bimodal: it is executed by machines, but also read by humans. This bimodality, we argue, leads developers to write code in certain preferred ways that would be familiar to code readers. To test this theory, we 1) model familiarity using a language model estimated over a large training corpus and 2) run an experiment applying several meaning preserving transformations to Java and Python expressions in a distinct test corpus to see if forms more familiar to readers (as predicted by the language models) are in fact the ones actually written. We find that these transformations generally produce program structures that are less common in practice, supporting the theory that the high repetitiveness in code is a matter of deliberate preference. Finally, 3) we use a human subject study to show alignment between language model score and human preference for the first time in code, providing support for using this measure to improve code.", "num_citations": "4\n", "authors": ["147"]}
{"title": "Operating system compatibility analysis of Eclipse and Netbeans based on bug data\n", "abstract": " Eclipse and Netbeans are two top of the line Integrated Development Environments (IDEs) for Java development. Both of them provide support for a wide variety of development tasks and have a large user base. This paper provides an analysis and comparison for the compatibility and stability of Eclipse and Netbeans on the three most commonly used operating systems, Windows, Linux and Mac OS. Both IDEs are programmed in Java and use a Bugzilla issue tracker to track reported bugs and feature requests. We looked into the Bugzilla repository databases of these two IDEs, which contains the bug records and histories of these two IDEs. We used some basic data mining techniques to analyze some historical statistics of the bug data. Based on the analysis, we try to answer certain stability-comparison oriented questions in the paper, so that users can have a better idea which of these two IDEs is designed\u00a0\u2026", "num_citations": "4\n", "authors": ["147"]}
{"title": "Measuring the benefits of software reuse\n", "abstract": " How much can be saved by using existing software components when developing software? With the increasing adoption of reuse methods and technologies, this question becomes critical. However, accounting for the actual cost savings due to reuse may be di cult. It would be desirable to measure the savings indirectly by analyzing the code for reuse of components. The central focus of our work is the development of some properties that (we believe) should hold of any reasonable measure of reuse bene t. We explore the relationship between several existing approaches to reuse measurement and these properties. We have developed an\\expanded source\" measure as an indirect measure for reuse bene t, and evaluate it theoretically, using our properties; we have also built tools to gather our measures and the previously proposed ones, and done some preliminary empirical work using some public domain\u00a0\u2026", "num_citations": "4\n", "authors": ["147"]}
{"title": "How to write a GEN++ specification\n", "abstract": " In this tutorial-style manual document, we describe the use of gen++, an application generator for creating code analyzers for C++ programs. gen++ is based on a language independent speci cation language and generation system called genoa 1]. This document is structured as follows: we begin with an illustrative example to explain the general structure of a language tool, and the basic notion of the abstract semantic graph representation of an input program. We then explain the basic fundamentals of genoa, the underlying portable system that forms the basis of gen++, and the important language independent notion of the gtree.With that, we focus on the C++ implementation of genoa, gen++. First, we consider a simple example application for gen++ that can nd where variables are modi ed, and accessed in the body of a C++ program. We show the short gen++ speci cation that implements the tool, and present the mechanics of building a running executable from this speci cation. We then continue by brie y describing the various constructs in gen++, and then present a nal tutorial examples. If you're reading this, you are probably a friendly user; we are grateful for your interest in this software, and solicit your input. Feel free to contact mozart! laurae or research! genoa for help etc.", "num_citations": "4\n", "authors": ["147"]}
{"title": "Patching as Translation: the Data and the Metaphor\n", "abstract": " Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that software patching is like language translation. We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better\u00a0\u2026", "num_citations": "3\n", "authors": ["147"]}
{"title": "Building statistical language models of code\n", "abstract": " We present the Source Code Statistical Language Model data analysis pattern. Statistical language models have been an enabling tool for a wide array of important language technologies. Speech recognition, machine translation, and document summarization (to name a few) all rely on statistical language models to assign probability estimates to natural language utterances or sentences. In this data analysis pattern, we describe the process of building n-gram language models over software source files. We hope that by introducing the empirical software engineering community to best practices that have been established over the years in research for natural languages, statistical language models can become a tool that SE researchers are able to use to explore new research directions.", "num_citations": "3\n", "authors": ["147"]}
{"title": "Reverse Engineering the Bazaar: Collaboration and Communication in Open Source Development\n", "abstract": " Open source project data has been a bonanza for software engineering research, much like Affymetrix's chips and gene sequencers have been for bio-informaticians. We are swimming in an ocean of data, merrily analyzing, clustering, correlating and model-fitting. However, while researchers have been busily mining the wealth of information in open source code repositories to study phenomena such as co-change patterns, evolution, defect occurrences and so on, with a few exceptions, the human side has largely remained unexplored. How do people collaborate? Who do they talk with? What do they talk about? How much do they talk? How do new people join? How do existing project members associate? These are questions are of vital importance to the organization of any software project; successful open source projects have a lot to teach us about such phenomena. Fortunately, there is a wealth of\u00a0\u2026", "num_citations": "3\n", "authors": ["147"]}
{"title": "An empirical comparison of adaptive assertion enforcement perform ance\n", "abstract": " Run-time enforcement of assertions is a well-known technique for improving the quality of software; however, the performance penalty is typically considered so high it is disabled during deployment. We propose the use of adaptive techniques to reduce that burden so that enforcement can be retained. This paper presents results from an empirical comparison at the component interface level using five enforcement strategies. All three adaptive policies incur a median overhead of 4.4% or less over never checking versus 31.5% for always checking. Three simplified, iterative algorithms accessing large data structures at up to nine levels of granularity over five data sets were used. The component is an implementation of an interface standard being developed in the high performance computing research community. The compute-intensive nature of applications in that domain necessitate an assessment of the performance impact of any new technique or tool.", "num_citations": "3\n", "authors": ["147"]}
{"title": "Static type-inference for trust in distributed information systems\n", "abstract": " Decision-makers in critical fields such as medicine and finance make use of a wide range of information available over the Internet. Mediation, a data integration technique for distributed, heterogeneous data sources, manages the complexity and diversity of the information schemas on behalf of clients. We raise here the issue of trust: is the information so obtained trustworthy? Each client can have different perspectives on the desired trustworthiness the information he or she needs. We consider here the scaling problem that arises from a very large number of users accessing information from many different sources. A mediator cannot be expected to manage the potentially quadratic scaling of trust relationships clients can have with information sources. Furthermore, the possibility of using untrustworthy data increases the risk that the resulting data will be unacceptable: a mediator might evaluate a complex\u00a0\u2026", "num_citations": "3\n", "authors": ["147"]}
{"title": "A lazy approach to separating architectural concerns\n", "abstract": " To achieve the goal of composable software systems, we define new notions of components and higher order connectors using a higher-order polymorphic functional language. We describe higher order connectors that take other connector functions as arguments and compose them, returning a new connector. The benefits of language features such a curried functions, lazy evaluation, and pure computation lead to an elegant separation of component interaction and functionality. We demonstrate our ideas by showing how they could apply to a client-server architecture and an event service component.", "num_citations": "3\n", "authors": ["147"]}
{"title": "System and method for using a second resource to store a data element from a first resource in a first-in first-out queue\n", "abstract": " A system and method are provided for storing a data element from a first resource in a queue at a second resource. A combination of a data element X N+ 1 and a signature S Q= S [N] are signed at a first resource to obtain a signature S [X N+ 1, S [N]], where N is an integer. The data element data element X N+ 1 and the signature S [X N+ 1, S [N]] are sent from the first resource to the second resource to be stored in the queue at the second resource. The signature S [X N+ 1, S [N]] is stored at the first resource as the new value for S Q.", "num_citations": "3\n", "authors": ["147"]}
{"title": "Knowledge Base Management Systems using Description Logics and their Role in Software Information Systems\n", "abstract": " Knowledge Base Management Systems using Description Logics and Their Role in Software Information Systems | Proceedings of the IFIP 12th World Computer Congress on Personal Computers and Intelligent Systems - Information Processing '92 - Volume 3 - Volume 3 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the IFIP 12th World Computer Congress on Personal Computers and Intelligent Systems - Information Processing '92 - Volume 3 - Volume 3Knowledge Base Management Systems using Description Logics and Their Role in Software Information Systems Article Knowledge Base Systems and \u2026", "num_citations": "3\n", "authors": ["147"]}
{"title": "Taxonomic plan reasoning\n", "abstract": " TerllJilloll)~ ic;\":,,~'st ern:\"[~ I]< trl'III the I\": L-O'\\E fa. llJily [: 3] of knowlf'dge representat iOIl Icll1,~ II; I.!;('S,< 111.. 1 PI'l)\\'idl'relJl'l~ sClitilt iOllal support III many areas of Artificial Intelligel1n'.(\"'lit ral III termillOl\" gical approaches are tll (, interpretation of frames< IS df'SCTiptiulI~. tbe use uf clC\\:-;~ ifi ('at. iol1 ilnd term sllbsllmption inferencesl to orga.-nize frclllw I axullomics,; 11lt1 differf'l1tiarion bet\\\\'f'en terminological and assertional< lspects of kl1o\\\\'kdc; e,.:\\lIJajur limitation of current. terminological systems, howeWT, is, In in: d) ilit.\\\u00b7 III reprf',.; en1 ami n'asoll wit h pIa liS. Plans, compositions of act ions t hill; whil.\u00b7\\\" t, giw'll goals, playa celltral role in many areas that use terminological klJo\\\\'led, l!:('reprt':\"(, lllarioll sys (. ems (natllred language processing [28], expert s~'sterns [: 3~ J. II~ er interracc:',.:[111,: viJ. 111< 111 synthesis [: 36], software information s~'s tems [iJI.\\\\\" ltil ('lIlt'.! JIlllmlfflll< 111< 1/\" IrOYllitioll of plans has been lhe focus of much!\" 1'C; Crlr ('11 ill illllolll: 11 i,'!\"/'; I:-'Ullill;';, I IH:'kll (J', dl~ dge re\\J]\":'::'I:'llta. tiolJ task of lIulI)(/9; n9 col, 1 ('('ril) JJ:, lif p!; tllS h; I.\" Ltr~ d.\\\u00b7 11l'(~ JI uuaddre:; s (: d. III this pcl\\)('r we present a knowledge reprcst'lltatioll eilid J'('; Ic.(oIlill!.!:-\\',, 11-'111 Ih< 11 extends r-! w notion of subsurnption from lermincJlogi,-; tl 1: lll.!.! U: lgl''''te) pl:. l1s.", "num_citations": "3\n", "authors": ["147"]}
{"title": "Do Programmers Prefer Predictable Expressions in Code?\n", "abstract": " Source code is a form of human communication, albeit one where the information shared between the programmers reading and writing the code is constrained by the requirement that the code executes correctly. Programming languages are more syntactically constrained than natural languages, but they are also very expressive, allowing a great many different ways to express even very simple computations. Still, code written by developers is highly predictable, and many programming tools have taken advantage of this phenomenon, relying on language model surprisal as a guiding mechanism. While surprisal has been validated as a measure of cognitive load in natural language, its relation to human cognitive processes in code is still poorly understood. In this paper, we explore the relationship between surprisal and programmer preference at a small granularity\u2014do programmers prefer more predictable\u00a0\u2026", "num_citations": "2\n", "authors": ["147"]}
{"title": "Does Surprisal Predict Code Comprehension Difficulty?\n", "abstract": " Recognition of the similarities between programming and natural languages has led to a boom in the adoption of language modeling techniques in tools that assist developers. However, language model surprisal, which guides the training and evaluation in many of these methods, has not been validated as a measure of cognitive difficulty for programming language comprehension as it has for natural language. We perform a controlled experiment to evaluate human comprehension on fragments of source code that are meaning-equivalent but with different surprisal. We find that more surprising versions of code take humans longer to finish answering correctly. We also provide practical guidelines to design future studies for code comprehension and surprisal.", "num_citations": "2\n", "authors": ["147"]}
{"title": "Replication of assert use in GitHub projects\n", "abstract": " Replication is an important aspect of empirical science. In medicine, for example, there is a great deal of effort spent on replication. The evidence-based medicine movement gives highest weight to research that has been repeated several times, with consistent results. When multiple studies provide consistent results, they are collected into online compendia such as the Cochrane Collaboration 1 or the National Guidline Clearinghouse 2. Replications are most warranted for surprising results [6], small samples [2], or small effect sizes [8],[1]. While replications are useful for validating results, they are unfortunately uncommon in software engineering, due to complexities associated with the software development environment [7].In our paper Assert Use in GitHub Projects [4], we addressed several questions relating who added asserts to projects, where they were added, and their relationship with code defects. In that paper, one of results (that Asserts are negatively associated with bugs) had a small effect size (< 1%), and was significant. This per se [1] provides a strong motivation for replication. In addition, while investigating further properties of asserts, we discovered a deficiency in the tool used to extract names of functions from changes in git logs. Since the observed effect sizes of the relationship between assertions and defects in our original study were small, we are using an exact, dependent replication [9] to determine what, if any, models change when using a more accurate method of function name extraction.", "num_citations": "2\n", "authors": ["147"]}
{"title": "Optimizing layered middleware\n", "abstract": " Middleware is often built using a layered architectural style. Layered design provides good separation of the different concerns of middleware, such as communication, marshaling, request dispatching, thread management, etc. Layered architecture helps in the development and evolution of the middleware. It also provides tactical side-benefits: layers provide convenient protection boundaries for enforcing security policies. However, the benefits of this layered structure come at a cost. Layered designs can hinder performance-related optimizations, and actually make it more difficult to adapt systems to conveniently address late-bound requirements such as dependability, access control, virus protection, and so on. We present some examples of this issue, and outline a new approach, under investigation at UC Davis, which includes ideas in middleware, architectures, and programming models.", "num_citations": "2\n", "authors": ["147"]}
{"title": "Willow system demonstration\n", "abstract": " Dealing with damage that arises during operation of networked information systems is essential if such systems are to provide the dependability required by modem critical applications. Extensive damage can arise from environmental factors, malicious actions and so on, and in most cases it is impractical to mask the effects of such damage using typical redundancy techniques. Reconfiguration is required of both the application and the underlying computing and communications fabric. Such reconfiguration is difficult to achieve because it requires communication with a significant number of nodes both to determine the problem and to effect a repair In this demonstration we present an approach to the implementation of such reconfiguration. The approach to reactive control includes formal description of the error states, synthesis of the implementation, a novel new communications mechanism for communication\u00a0\u2026", "num_citations": "2\n", "authors": ["147"]}
{"title": "Interactive Poster: Addressing Scale and Context in Source Code Visualization\n", "abstract": " Program comprehension is key to software maintenance, taking 50% or more of programmer time. Understanding programs is complicated by scale and contextuality. We introduce a visualization system for source code, based on a \u201csource code listings on the walls\u201d metaphor. The entire source code of large systems is displayed on the walls of a navigable virtual space. Within this visual metaphor, we introduce enhanced versions of traditional techniques such as \u201csemantic-zooming\u201d and \u201cfisheye\u201d views, which are specifically tuned for dealing with scale and context issues applicable to source code. This poster describes and illustrates these techniques.", "num_citations": "2\n", "authors": ["147"]}
{"title": "Program understanding-does it offer hope for aging software?\n", "abstract": " Two questions are examined: what does it mean to understand a program? What solutions to the aging software crisis does program understanding offer? One view is that the aging software problem is a form of support for program maintenance. Another view is that the problem of applying program understanding techniques to aging software is one of both extending the life of existing relics, and mining them for valuable components. Yet another view is that domain specific cliches are necessary in program understanding and that program understanding support for software is how programmers can come to understand a program through cooperative interaction with a knowledge base. The work focuses on the flexibility provided by natural language interaction.<>", "num_citations": "2\n", "authors": ["147"]}
{"title": "Naturally!: How Breakthroughs in Natural Language Processing Can Dramatically Help Developers\n", "abstract": " Taking advantage of the naturalness hypothesis for code, recent development, and research has focused on applying machine learning (ML) techniques originally developed for natural language processing (NLP) to drive a new wave of tools and applications aimed specifically for software engineering (SE) tasks. This drive to apply ML and deep learning (DL) has been animated by the large-scale availability of software development data (e.g., source code, code comments, code review comments, commit data, and so on) available from open source platforms such as GitHub and Bitbucket.", "num_citations": "1\n", "authors": ["147"]}
{"title": "Rebuttal to Berger et al., TOPLAS 2019\n", "abstract": " Berger et al., published in TOPLAS 2019, is a critique of our 2014 FSE conference abstract and its archival version, the 2017 CACM paper: A Large-Scale Study of Programming Languages and Code Quality in Github. In their paper Berger et al. make academic claims about the veracity of our work. Here, we respond to their technical and scientific critiques aimed at our work, attempting to stick with scientific discourse. We find that Berger et al. largely replicated our results, and agree with us in their conclusion: that the effects (in a statistical sense) found in the data are small, and should be taken with caution, and that it is possible that an absence of effect is the correct interpretation. Thus, our CACM paper's conclusions still hold, even more so now that they have been reproduced, and our paper is eminently citable.", "num_citations": "1\n", "authors": ["147"]}
{"title": "Two party aspect agreement using a COTS solver\n", "abstract": " A number of researchers have proposed an aspect-oriented approach for integrating concerns with component based applications. With this approach, components only implement a functional interface; aspects such as security are left unresolved until deployment time. In this paper we present the latest version of our declarative language, GlueQoS, used to specify aspect deployment policies. Our work is focused on automating the process of configuring cooperating remote aspects using a client-server handshake. During the handshake the two parties agree on aspect configuration by using mixed integer programming. A security example is presented as well as initial performance observations.", "num_citations": "1\n", "authors": ["147"]}
{"title": "Design and implementation of distributed crosscutting features with DADO\n", "abstract": " Some\" non-\" or\" extra-functional\" features, such as reliability, security, and tracing, defy modularization mechanisms in programming languages. With distributed, heterogeneous (DH) systems, these features induce complex implementations which crosscut different languages, OSs, and hardware platforms, while still needing to share data and events. The DADO approach helps program crosscuting features by improving DH middleware. A DADO service comprises pairs of adaplets which are explicitly modeled in IDL. DADO supports flexible and type-checked interactions (using generated stubs and skeletons) between adaplets and between objects and adaptlets. Adaptlets can be attached at run-time to an application object.", "num_citations": "1\n", "authors": ["147"]}
{"title": "Memory issues in hardware-supported software safety\n", "abstract": " Rising chip densities have led to dramatic improvements in the costperformance ratio of processors. At the same time, software costs are burgeoning. Large software systems are expensive to develop and are riddled with errors. Certain types of defects (e.g., those related to memory access, concurrency, and security) are particularly difficult to locate and can have devastating consequences. We believe it is time to explore using some of the increasing silicon real estate to provide extra functionality to support software development. We propose dedicating a portion of these new transistors to provide hardware structures to enhance software development, make debugging more efficient, increase reliability, and provide run-time security.             Unfortunately, software safety tasks, such as checking pointer references, involve extensive bookkeeping, which results in unacceptably high overhead. We describe work\u00a0\u2026", "num_citations": "1\n", "authors": ["147"]}
{"title": "DADO: A novel programming model for distributed, heterogenous, late-bound QoS implementations\n", "abstract": " Quality of service implementations, such as security and reliability, are notoriously difficult for software engineers. They have fragmented, cross-cutting implementations, with elements required in application components on both client and server sides. Heterogeniety and distribution make things even more difficult. To cap it all, precise requirements, specially for security, are often deployment-specific. DADO is a new programming model, with roots in aspect-oriented programming, that aims to improve matters. The goal of DADO is to provide a CORBA-like programming model to the developers of distributed, heterogenous QoS features. DADO comprises a modeling language, a deployment language, code generators, and a run-time environment. DADO allows QoS features to be modeled at the IDL level. Communication between QoS elements is explicitly modeled, as is the interaction between QoS\u00a0\u2026", "num_citations": "1\n", "authors": ["147"]}
{"title": "Extending commodity microprocessors for software safety: An experiment\n", "abstract": " Rising chip densities have led to dramatic improvements in the cost-performance ratio of processors. At the same time, software costs are burgeoning. Large software systems are expensive to develop and are riddled with errors. Certain types of defects (eg, those related to memory access, concurrency, and security) are particularly difficult to locate and can have devastating consequences. We believe it is time to explore using some of the increasing silicon real-estate to provide extra functionality to support software development. Our goal is to investigate approaches to exploit the increasing bounty of transistors in order to provide hardware structures that enhance software development, make debugging easier, increase reliability and provide run-time security. Unlike previous designs, we are specifically interested in \u201clow-impact\u201d, evolutionary designs that conservatively extend current, commodity microprocessors. In this paper, we focus on run-time monitoring for pointer safety. We present new hardware designs aimed at boosting the performance of the best current (purely software) approaches, and evaluate the resulting performance gains. Our evaluations provide evidence of considerable acceleration and suggest some new opportunities for even greater gains. We also place our approach in the broad context of software safety, including both pure software approaches such as strong typing, sandboxing, proof-carrying code and software fault isolation, as well as traditional hardware-assisted approaches such as tagged architectures.", "num_citations": "1\n", "authors": ["147"]}
{"title": "System and method for using a second resource to store a data element from a first resource in a first-in last-out stack\n", "abstract": " A system and method are provided for using a second resource to store a data element from a first resource in a stack. A data element X N+ 1 and a signature S [N] are signed at a first resource to obtain a signature S [X N+ 1, S [N]], where N is an integer. The data element X N+ 1 and the signature S [N] are sent from the first resource to the second resource to be stored in a stack. The signature S [X N+ 1, S [N]] is stored at the first resource.", "num_citations": "1\n", "authors": ["147"]}
{"title": "Authentic Re-Publication by Untrusted Servers: A Novel Approach to Database Survivability\n", "abstract": " High-value information, such as geophysical (or cartographic) data, pharmacological information, and business data, which are used in high-value decisions, are frequently made available for on-line querying. Customers dependent on this information for their work need high reliability and accuracy. However, operating such an on-line querying service, securely, on open networks is very difficult. Most large systems have vulnerabilities that can be exploited. An adversary who can break in and publish false data might cause huge losses for users. Replicating the data service for reliability and scalability only makes matters worse. We introduce a new approach, by which an untrusted publisher can answer queries from customers on behalf of the owner, or creator of the data. With just a few trusted digital signatures from the owner, the untrusted publisher can use techniques based on Merkle hash trees [4], to provide authenticity and non-repudiation of the answer to a database query. We do not require a key to be held in an on-line system, thus reducing the impact of system penetrations. The use of untrusted publishers provides a novel approach to greater information survivability.", "num_citations": "1\n", "authors": ["147"]}
{"title": "5th international conference on software reuse (ICSR'5) conference summary\n", "abstract": " 5th international conference on software reuse (ICSR'5) conference summary | ACM SIGSOFT Software Engineering Notes ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search ACM SIGSOFT Software Engineering Notes Newsletter Home Latest Issue Archive Authors Affiliations Award Winners More HomeSIGsSIGSOFTACM SIGSOFT Software Engineering NotesVol. , No. 5 th international conference on software reuse (ICSR'5) conference summary article 5 th international conference on software reuse (ICSR'5) conference summary Share on Authors: Jeffrey S Poulin profile image Jeffrey S. Poulin View Profile , P. Devanbu profile image Prem Devanbu View Profile Authors Info & Affiliations Publication: ACM :/\u2026", "num_citations": "1\n", "authors": ["147"]}
{"title": "Research Issues with Application Generators\n", "abstract": " Application generators, which generate domain-specic systems from specications formulated in a special-purpose high-level formal language, can achieve very high levels of software reuse and impressive gains in productivity. Several tools available in the market have greatly simplied the construction of application generators. There still are, however, signicant diculties in the practical use of application generators. I\u2019m particularly interested in the pursuit of methodological and technical solutions to the following problems: designing domain-tailored specication languages, re-engineering domain specic frameworks out of existing systems, and assistance for debugging specications.", "num_citations": "1\n", "authors": ["147"]}