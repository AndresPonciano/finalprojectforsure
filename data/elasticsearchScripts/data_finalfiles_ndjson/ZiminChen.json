{"title": "A literature study of embeddings on source code\n", "abstract": " Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.", "num_citations": "53\n", "authors": ["2116"]}
{"title": "Learning to fix build errors with graph2diff neural networks\n", "abstract": " Professional software developers spend a significant amount of time fixing builds; for example, one large-scale study found that developers build their code 7\u201310 times per day [12], with a significant number of builds being unsuccessful. Build errors include simple errors such as syntax errors, but for professional developers these are a small minority of errors; instead, the majority are linking errors such as unresolved symbols, type errors, and incorrect build dependencies [12]. A recent paper by Google reports roughly 10 developer-months of effort are spent every month fixing small build errors [10]. Therefore, automatically repairing build errors is a research problem that has potential to ease a frequent pain point in the developer workflow. Happily, there are good reasons to think that automatic build repair is feasible: fixes are often short, and we can test a proposed fix before suggesting it. Build repair is thus a\u00a0\u2026", "num_citations": "29\n", "authors": ["2116"]}
{"title": "The codrep machine learning on source code competition\n", "abstract": " CodRep is a machine learning competition on source code data. It is carefully designed so that anybody can enter the competition, whether professional researchers, students or independent scholars, without specific knowledge in machine learning or program analysis. In particular, it aims at being a common playground on which the machine learning and the software engineering research communities can interact. The competition has started on April 14th 2018 and has ended on October 14th 2018. The CodRep data is hosted at https://github.com/KTH/CodRep-competition/.", "num_citations": "13\n", "authors": ["2116"]}
{"title": "The remarkable role of similarity in redundancy-based program repair\n", "abstract": " Recently, there have been original attempts to use the concept of \"code similarity\" in program repair, suggesting that similarity analysis has an important role in the repair process. However, there is no dedicated work to characterize and quantify the role of similarity in redundancy-based program repair, where the patch is composed from source code taken from somewhere else. This is where our paper makes a major contribution: we perform a deep and systematic analysis of the role of code similarity during the exploration of the repair search space. We define and set up a large-scale experiment based on four code similarity metrics that capture different similarities: character, token, semantic and structure similarity. Overall, we have computed 56 million similarity score over 15 million source code components. We show that with similarity analysis, at least 90% of search space can be ignored to find the correct patch. Code similarity is capable of ranking the correct repair ingredient first in 4 - 33 % of the considered cases.", "num_citations": "12\n", "authors": ["2116"]}
{"title": "Using Sequence-to-Sequence Learning for Repairing C Vulnerabilities\n", "abstract": " Software vulnerabilities affect all businesses and research is being done to avoid, detect or repair them. In this article, we contribute a new technique for automatic vulnerability fixing. We present a system that uses the rich software development history that can be found on GitHub to train an AI system that generates patches. We apply sequence-to-sequence learning on a big dataset of code changes and we evaluate the trained system on real world vulnerabilities from the CVE database. The result shows the feasibility of using sequence-to-sequence learning for fixing software vulnerabilities.", "num_citations": "2\n", "authors": ["2116"]}
{"title": "Multimodal Representation for Neural Code Search\n", "abstract": " Semantic code search is about finding semantically relevant code snippets for a given natural language query. In the state-of-the-art approaches, the semantic similarity between code and query is quantified as the distance of their representation in the shared vector space. In this paper, to improve the vector space, we introduce tree-serialization methods on a simplified form of AST and build the multimodal representation for the code data. We conduct extensive experiments using a single corpus that is large-scale and multi-language: CodeSearchNet. Our results show that both our tree-serialized representations and multimodal learning model improve the performance of code search. Last, we define intuitive quantification metrics oriented to the completeness of semantic and syntactic information of the code data, to help understand the experimental findings.", "num_citations": "1\n", "authors": ["2116"]}