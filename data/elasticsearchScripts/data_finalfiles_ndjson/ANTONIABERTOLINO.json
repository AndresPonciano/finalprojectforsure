{"title": "Software testing research: Achievements, challenges, dreams\n", "abstract": " Software engineering comprehends several disciplines devoted to prevent and remedy malfunctions and to warrant adequate behaviour. Testing, the subject of this paper, is a widespread validation approach in industry, but it is still largely ad hoc, expensive, and unpredictably effective. Indeed, software testing is a broad term encompassing a variety of activities along the development cycle and beyond, aimed at different goals. Hence, software testing research faces a collection of challenges. A consistent roadmap of the most relevant challenges to be addressed is here proposed. In it, the starting point is constituted by some important past achievements, while the destination consists of four identified goals to which research ultimately tends, but which remain as unreachable as dreams. The routes from the achievements to the dreams are paved by the outstanding research challenges, which are discussed in the\u00a0\u2026", "num_citations": "1171\n", "authors": ["322"]}
{"title": "WS-TAXI: A WSDL-based testing tool for web services\n", "abstract": " Web services (WSs) are the W3C-endorsed realization of the Service-Oriented Architecture (SOA). Since they are supposed to be implementation-neutral, WSs are typically tested black-box at their interface. Such an interface is generally specified in an XML-based notation called the WS Description Language (WSDL). Conceptually, these WSDL documents are eligible for fully automated WS test generation using syntax-based testing approaches. Towards such goal, we introduce the WS-TAXI framework, in which we combine the coverage of WS operations with data-driven test generation. In this paper we present an early-stage implementation of WS-TAXI, obtained by the integration of two existing softwares: soapUI, a popular tool for WS testing, and TAXI, an application we have previously developed for the automated derivation of XML instances from a XML schema. WS-TAXI delivers a complete suite of test\u00a0\u2026", "num_citations": "172\n", "authors": ["322"]}
{"title": "Using spanning sets for coverage testing\n", "abstract": " A test coverage criterion defines a set E/sub r/ of entities of the program flowgraph and requires that every entity in this set is covered under some test Case. Coverage criteria are also used to measure the adequacy of the executed test cases. In this paper, we introduce the notion of spanning sets of entities for coverage testing. A spanning set is a minimum subset of E/sub r/, such that a test suite covering the entities in this subset is guaranteed to cover every entity in E/sub r/. When the coverage of an entity always guarantees the coverage of another entity, the former is said to subsume the latter. Based on the subsumption relation between entities, we provide a generic algorithm to find spanning sets for control flow and data flow-based test coverage criteria. We suggest several useful applications of spanning sets: They help reduce and estimate the number of test cases needed to satisfy coverage criteria. We also\u00a0\u2026", "num_citations": "166\n", "authors": ["322"]}
{"title": "Automatic generation of path covers based on the control flow analysis of computer programs\n", "abstract": " Branch testing a program involves generating a set of paths that will cover every arc in the program flowgraph, called a path cover, and finding a set of program inputs that will execute every path in the path cover. This paper presents a generalized algorithm that finds a path cover for a given program flowgraph. The analysis is conducted on a reduced flowgraph, called a ddgraph, and uses graph theoretic principles differently than previous approaches. In particular, the relations of dominance and implication which form two trees of the arcs of the ddgraph are exploited. These relations make it possible to identify a subset of ddgraph arcs, called unconstrained arcs, having the property that a set of paths exercising all the unconstrained arcs also cover all the arcs in the ddgraph. In fact, the algorithm has been designed to cover all the unconstrained arcs of a given ddgraph: the paths are derived one at a time, each path\u00a0\u2026", "num_citations": "166\n", "authors": ["322"]}
{"title": "On the use of testability measures for dependability assessment\n", "abstract": " Program \"testability\" is informally, the probability that a program will fail under test if it contains at least one fault. When a dependability assessment has to be derived from the observation of a series of failure free test executions (a common need for software subject to \"ultra high reliability\" requirements), measures of testability can-in theory-be used to draw inferences on program correctness. We rigorously investigate the concept of testability and its use in dependability assessment, criticizing, and improving on, previously published results. We give a general descriptive model of program execution and testing, on which the different measures of interest can be defined. We propose a more precise definition of program testability than that given by other authors, and discuss how to increase testing effectiveness without impairing program reliability in operation. We then study the mathematics of using testability to\u00a0\u2026", "num_citations": "158\n", "authors": ["322"]}
{"title": "The cow_suite approach to planning and deriving test suites in UML projects\n", "abstract": " Cow_Suite provides an integrated and practical approach to the strategic generation and planning of UML-based test suites, since the early stages of system analysis and modeling. It consists of two original components working in combination: the Cowtest strategy, which organizes the testing process and helps the manager to select among the many potential test cases, and the UIT method, which performs the automated generation of test cases from the UML diagrams. The approach can be used in incremental way, starting from the preliminary (even incomplete) UML diagrams, and is applied to integration subsystems, as interactively selected by the tester. The emphasis is on usability, in that we use exactly the same UML diagrams developed for analysis and design, without requiring any additional formalism or ad-hoc effort specifically for testing purposes. Cow_Suite has been implemented in a\u00a0\u2026", "num_citations": "140\n", "authors": ["322"]}
{"title": "The audition framework for testing web services interoperability\n", "abstract": " Service oriented architectures and Web services are emerging technologies, which have overall inherited problems and advantages from the component-based approach, but exacerbated the aspects of loose coupling, distribution and dynamism of \"components\", here elements furnishing published services on external client requests. In this paper, we highlight the urgent need for methodologies supporting Web services reliable interaction, and in particular deal with testing concerns. We then propose a framework that extends UDDI registry role from the current one of a \"passive\" service directory, to also sort of an accredited testing organism, which validates service behaviour before actually registering it. This testing stage (called audition) mainly focuses on interoperability issues, so to facilitate the coordination among services registered at the same UDDI. The audition needs to rely on a Web service specification\u00a0\u2026", "num_citations": "133\n", "authors": ["322"]}
{"title": "Model-based generation of testbeds for web services\n", "abstract": " A Web Service is commonly not an independent software entity, but plays a role in some business process. Hence, it depends on the services provided by external Web Services, to provide its own service. While developing and testing a Web Service, such external services are not always available, or their usage comes along with unwanted side effects like, e.g., utilization fees or database modifications. We present a model-based approach to generate stubs for Web Services which respect both an extra-functional contract expressed via a Service Level Agreement (SLA), and a functional contract modeled via a state machine. These stubs allow a developer to set up a testbed over the target platform, in which the extra-functional and functional behavior of a Web Service under development can be tested before its publication.", "num_citations": "102\n", "authors": ["322"]}
{"title": "Software testing research and practice\n", "abstract": " The paper attempts to provide a comprehensive view of the field of software testing. The objective is to put all the relevant issues into a unified context, although admittedly the overview is biased towards my own research and expertise. In view of the vastness of the field, for each topic problems and approaches are only briefly tackled, with appropriate references provided to dive into them. I do not mean to give here a complete survey of software testing. Rather I intend to show how an unwieldy mix of theoretical and technical problems challenge software testers, and that a large gap exists between the state of the art and of the practice.", "num_citations": "83\n", "authors": ["322"]}
{"title": "A practical approach to UML-based derivation of integration tests\n", "abstract": " We present an on-going project for developing a tool supported test methodology based on UML descriptions.", "num_citations": "83\n", "authors": ["322"]}
{"title": "Testing software components for integration: a survey of issues and techniques\n", "abstract": " Component\u2010based development has emerged as a system engineering approach that promises rapid software development with fewer resources. Yet, improved reuse and reduced cost benefits from software components can only be achieved in practice if the components provide reliable services, thereby rendering component analysis and testing a key activity. This paper discusses various issues that can arise in component testing by the component user at the stage of its integration within the target system. The crucial problem is the lack of information for analysis and testing of externally developed components. Several testing techniques for component integration have recently been proposed. These techniques are surveyed here and classified according to a proposed set of relevant attributes. The paper thus provides a comprehensive overview which can be useful as introductory reading for newcomers in\u00a0\u2026", "num_citations": "78\n", "authors": ["322"]}
{"title": "Automatic test data generation for XML schema-based partition testing\n", "abstract": " We present the XML-based partition testing (XPT) approach for the automatic generation of XML instances from a XML schema. The approach is inspired by the well-known category partition method for black-box testing. The generated instances can be used for inter-operability testing of applications that expect in input conforming XML instances, as well as for other interesting purposes, such as database population, XML Schema benchmarking, Web services testing, and so on. The implementation of XPT in a prototype tool called TAXI is described. To limit the number of generated instances, TAXI also incorporates practical strategies for handling element weights and type values.", "num_citations": "78\n", "authors": ["322"]}
{"title": "Whitening SOA testing\n", "abstract": " Service Oriented Architectures (SOAs) are becoming increasingly popular and powerful. Fueling that growth is the availability of independent web services that can be cost-effectively composed with other services to provide richer functionality. The reasons that make these systems easier to build, however, also make them more challenging to test. Independent web services usually provide just an interface, enough to invoke them and develop some general (black-box) tests, but insufficient for a tester to develop an adequate understanding of the integration quality between the application and independent web services. To address this lack we propose a\" whitening\" approach to make web services more transparent through the addition of an intermediate coverage service. The approach, named Service Oriented Coverage Testing (SOCT), provides a tester with feedback about how a whitened service, called a\u00a0\u2026", "num_citations": "72\n", "authors": ["322"]}
{"title": "A framework for component deployment testing\n", "abstract": " Component-based development is the emerging paradigm in software production, though several challenges still slow down its full taking up. In particular, the \"component trust problem\" refers to how adequate guarantees and documentation about a component's behaviour can be transferred from the component developer to its potential users. The capability to test a component when deployed within the target application environment can help establish the compliance of a candidate component to the customer's expectations and certainly contributes to \"increase trust\". To this purpose, we propose the CDT framework for Component Deployment Testing. CDT provides the customer with both a technique to early specify a deployment test suite and an environment for running and reusing the specified tests on any component implementation. The framework can also be used to deliver the component developer's test\u00a0\u2026", "num_citations": "66\n", "authors": ["322"]}
{"title": "A brief essay on software testing\n", "abstract": " Testing is an important and critical part of the software development process, on which the quality and reliability of the delivered product strictly depend. Testing is not limited to the detection of \u201cbugs\u201d in the software, but also increases confidence in its proper functioning and assists with the evaluation of functional and nonfunctional properties. Testing related activities encompass the entire development process and may consume a large part of the effort required for producing software. In this chapter we provide a comprehensive overview of software testing, from its definition to its organization, from test levels to test techniques, from test execution to the analysis of test cases effectiveness. Emphasis is more on breadth than depth: due to the vastness of the topic, in the attempt to be all-embracing, for each covered subject we can only provide a brief description and references useful for further reading.", "num_citations": "60\n", "authors": ["322"]}
{"title": "Software testing\n", "abstract": " Testing is an important, mandatory part of software development; it is a technique for evaluating product quality and also for indirectly improving it, by identifying defects and problems.As more extensively discussed in the Software Quality chapter of the Guide to the SWEBOK, the right attitude towards quality is one of prevention: it is obviously much better to avoid problems, rather than repairing them. Testing must be seen as a means primarily for checking whether the prevention has been effective, but also for identifying anomalies in those cases in which, for some reason, it has been not. It is perhaps obvious, but worth recognizing, that even after successfully completing an extensive testing campaign, the software could still contain faults; nor is defect free code a synonymous for quality product. The remedy to system failures that are experienced after delivery is provided by (corrective) maintenance actions. Maintenance topics are covered into the Software Maintenance chapter of the Guide to the SWEBOK.", "num_citations": "59\n", "authors": ["322"]}
{"title": "Software testing and industry needs\n", "abstract": " We've invited five renowned experts in software testing to give a brief answer to an admittedly broad question: does the practice of software testing effectively meet industry needs?", "num_citations": "57\n", "authors": ["322"]}
{"title": "Soa test governance: Enabling service integration testing across organization and technology borders\n", "abstract": " The service-oriented architecture (SOA) is the emerging paradigm in information technology. More than a true \"architecture\", SOA provides a general reference model for the development, deployment and management of distributed dynamic systems. Companies are progressively adopting service-oriented technology, because of its many (real or idealized) foreseen benefits, among which notably loose coupling and dynamic interoperability. Such benefits, however, can only be achieved through discipline and standardization: in this respect, SOA governance qualifies a framework of policies, procedures, design rules and documentation standards to be enforced for ensuring that different services and components can successfully cooperate towards a shared business goal. What about testing of such composite SOA applications? Little attention has been devoted so far by researchers to SOA testing, but awareness\u00a0\u2026", "num_citations": "55\n", "authors": ["322"]}
{"title": "Bringing white-box testing to service oriented architectures through a service oriented approach\n", "abstract": " The attractive feature of Service Oriented Architecture (SOA) is that pieces of software conceived and developed by independent organizations can be dynamically composed to provide richer functionality. The same reasons that enable flexible compositions, however, also prevent the application of some traditional testing approaches, making SOA validation challenging and costly. Web services usually expose just an interface, enough to invoke them and develop some general (black-box) tests, but insufficient for a tester to develop an adequate understanding of the integration quality between the application and the independent web services. To address this lack we propose an approach that makes web services more transparent to testers through the addition of an intermediary service that provides coverage information. The approach, named Service Oriented Coverage Testing (SOCT), provides testers with\u00a0\u2026", "num_citations": "51\n", "authors": ["322"]}
{"title": "Architecture-based software testing\n", "abstract": " Jn recent years the study of So&are Architecture emerged as an autonomous discipline in the software engineering field which requires its own concepts, formalisms, methods and tools [9, 3, 4, 7, 81. The softwze architecture of a large complex system supplies information about how the software is structured in parts and how those parts interact. Besides the static characteristics of a system, Software Architectures require the ability to describe dynamics at a rel-evant level of abstraction as well as the ability of expressing extra-functional requirements. As the complexity of software systems increases, the importance of Software Architectures becomes evident for the analyis, design and construction of the overall system structure. Testing is one of the most expensive activity in the development of complex software systems and represents a mandatory activity to improve the dependability of complex software systems\u00a0\u2026", "num_citations": "51\n", "authors": ["322"]}
{"title": "Systematic XACML request generation for testing purposes\n", "abstract": " A widely adopted security mechanism is the specification of access control policies by means of the XACML language. In this paper, we propose a framework, called X-CREATE, for the systematic generation of test inputs (XACML requests). Differently from existing tools, XCREATE exploits the XACML Context Schema. In particular, the tool applies a XML-based methodology (XPT) to systematically produce a set of intermediate instances, covering the XACML Context Schema. Moreover, for request generation, X-CREATE applies a procedure for parsing the policy under test and assigning values to the generated intermediate instances. The aim of the proposed framework is twofold: testing of policy evaluation engines and testing of access control policies. The experimental results show that the fault detection effectiveness of X-CREATE is similar or higher than that of existing approaches.", "num_citations": "50\n", "authors": ["322"]}
{"title": "Towards automated WSDL-based testing of web services\n", "abstract": " With the emergence of service-oriented computing, proper approaches are needed to validate a Web Service (WS) behaviour. In the last years several tools automating WS testing have been released. However, generally the selection of which and how many test cases should be run, and the instantiation of the input data into each test case, is still left to the human tester.             In this paper we introduce a proposal to automate WSDL-based testing, which combines the coverage of WS operations with data-driven test case generation. We sketch the general architecture of a test environment that basically integrates two existing tools: soapUI, which is a popular tool for WS testing, and TAXI, which is a tool we have previously developed for the automated derivation of XML instances from a XML Schema.             The test suite generation can be driven by basic coverage criteria and by the application of some\u00a0\u2026", "num_citations": "49\n", "authors": ["322"]}
{"title": "Xacmut: Xacml 2.0 mutants generator\n", "abstract": " Testing of security policies is a critical activity and mutation analysis is an effective approach for measuring the adequacy of a test suite. In this paper, we propose a set of mutation operators addressing specific faults of the XACML 2.0 access control policy and a tool, called XACMUT (XACml MUTation) for creating mutants. The tool generates the set of mutants, provides facilities to run a given test suite on the mutants set and computes the test suite effectiveness in terms of mutation score. The tool includes and enhances the mutation operators of existing security policy mutation approaches.", "num_citations": "47\n", "authors": ["322"]}
{"title": "Towards a model-driven infrastructure for runtime monitoring\n", "abstract": " In modern pervasive dynamic and eternal systems, software must be able to self-organize its structure and self-adapt its behavior to enhance its resilience and provide the desired quality of service. In this high-dynamic and unpredictable scenario, flexible and reconfigurable monitoring infrastructures become key instruments to verify at runtime functional and non-functional properties. In this paper, we propose a property-driven approach to runtime monitoring that is based on a comprehensive Property Meta-Model (PMM) and on a generic configurable monitoring infrastructure. PMM supports the definition of quantitative and qualitative properties in a machine-processable way making it possible to configure the monitors dynamically. Examples of implementation and applications of the proposed model-driven monitoring infrastructure are excerpted from the ongoing connect European Project.", "num_citations": "47\n", "authors": ["322"]}
{"title": "Data flow-based validation of web services compositions: Perspectives and examples\n", "abstract": " Composition of Web Services (WSs) is anticipated as the future standard way to dynamically build distributed applications, and hence their verification and validation is attracting great attention. The standardization of BPEL as a composition language and of WSDL as a WS interface definition language has led researchers to investigate verification and validation techniques mainly focusing on the sequence of events in the composition, while minor attention has been paid to the validation of the data flow exchange. In this chapter we study the potential of using data flow modelling for testing composite WSs. After an exhaustive exploration of the issues on testing based on data-related models, we schematically settle future research issues on the perspectives opened by data flow-based validation and present examples for some of them, illustrated on the case study of a composite WS that we have developed\u00a0\u2026", "num_citations": "47\n", "authors": ["322"]}
{"title": "The plastic framework and tools for testing service-oriented applications\n", "abstract": " The emergence of the Service Oriented Architecture (SOA) is changing the way in which software applications are developed. A service-oriented application consists of the dynamic composition of autonomous services independently developed by different organizations and deployed on heterogenous networks. Therefore, validation of SOA poses several new challenges, without offering any discount for the more traditional testing problems. In this chapter we overview the PLASTIC validation framework in which different techniques can be combined for the verification of both functional and extra-functional properties, spanning over both off-line and on-line testing stages. The former stage concerns development time testing, at which services are exercised in a simulated environment. The latter foresees the monitoring of a service live usage, to dynamically reveal possible deviations from the expected\u00a0\u2026", "num_citations": "47\n", "authors": ["322"]}
{"title": "TAXI--a tool for XML-based testing\n", "abstract": " We present the tool TAXI which implements the XML-based partition testing approach for the automated generation of XML instances conforming to a given XML schema. In addition it provides a set of weighted test strategies to guide the systematic derivation of instances. TAXI can be used for black-box testing of applications accepting in input XML instances and for benchmarking of database management systems.", "num_citations": "46\n", "authors": ["322"]}
{"title": "Feasible test path selection by principal slicing\n", "abstract": " We propose to improve current path-wise methods for automatic test data generation by using a new method named principal slicing. This method statically derives program slices with a near minimum number of influencing predicates, using both control and data flow information. Paths derived on principal slices to reach a certain program point are therefore very likely to be feasible. We discuss how our method improves on earlier proposed approaches, both static and dynamic. We also provide an algorithm for deriving principal slices. Then we illustrate the application of principal slicing to testing, considering a specific test criterion as an example, namely branch coverage. The example provided is an optimised method for automated branch testing: not only do we use principal slicing to obtain feasible test paths, but also we use the concept of spanning sets of branches to guide the selection of each next\u00a0\u2026", "num_citations": "46\n", "authors": ["322"]}
{"title": "Automatic XACML requests generation for policy testing\n", "abstract": " Access control policies are usually specified by the XACML language. However, policy definition could be an error prone process, because of the many constraints and rules that have to be specified. In order to increase the confidence on defined XACML policies, an accurate testing activity could be a valid solution. The typical policy testing is performed by deriving specific test cases, i.e. XACML requests, that are executed by means of a PDP implementation, so to evidence possible security lacks or problems. Thus the fault detection effectiveness of derived test suite is a fundamental property. To evaluate the performance of the applied test strategy and consequently of the test suite, a commonly adopted methodology is using mutation testing. In this paper, we propose two different methodologies for deriving XACML requests, that are defined independently from the policy under test. The proposals exploit the values\u00a0\u2026", "num_citations": "45\n", "authors": ["322"]}
{"title": "Automated testing of eXtensible Access Control Markup Language\u2010based access control systems\n", "abstract": " The trustworthiness of sensitive data needs to be guaranteed and testing is a common activity among privacy protection solutions, even if quite expensive. Accesses to data and resources are ruled by the policy decision point (PDP), which relies on the eXtensible Access Control Markup Language (XACML) standard language for specifying access rights. In this study, the authors propose a testing strategy for automatically deriving test requests from a XACML policy and describe their pilot experience in test automation using this strategy. Considering a real two\u2010level PDP implemented for health data security, the authors compare the effectiveness of the test plan automatically derived with the one derived by a standard manual testing process.", "num_citations": "41\n", "authors": ["322"]}
{"title": "The (im) maturity level of software testing\n", "abstract": " A large gap exists between the state-of-the-art in software testing literature, and the state of software testing practice. Empirical research should (and could) play a first class role for bridging this gap. Empirical studies in software testing have focused mainly on the evaluation of techniques for test case selection. But effective selection of test cases by itself is not sufficient to warrant successful testing: we need also empirical studies to start collecting proven patterns that test practitioners can use to predictably solve software testing problems.", "num_citations": "39\n", "authors": ["322"]}
{"title": "Scope-aided test prioritization, selection and minimization for software reuse\n", "abstract": " Software reuse can improve productivity, but does not exempt developers from the need to test the reused code into the new context. For this purpose, we propose here specific approaches to white-box test prioritization, selection and minimization that take into account the reuse context when reordering or selecting test cases, by leveraging possible constraints delimiting the new input domain scope. Our scope-aided testing approach aims at detecting those faults that under such constraints would be more likely triggered in the new reuse context, and is proposed as a boost to existing approaches. Our empirical evaluation shows that in test suite prioritization we can improve the average rate of faults detected when considering faults that are in scope, while remaining competitive considering all faults; in test case selection and minimization we can considerably reduce the test suite size, with small to no extra impact\u00a0\u2026", "num_citations": "38\n", "authors": ["322"]}
{"title": "How many paths are needed for branch testing?\n", "abstract": " A bound on the number of test cases needed to achieve branch coverage is important to evaluate the effort needed to test a given program. However, the bounds proposed so far in the literature are not effective to measure testing effort. In this article, we introduce a new, meaningful lower bound on the number of test cases needed to achieve branch coverage. We first identify the set of unconstrained arcs in a ddgraph. This is the minimum set of arcs such that a set of paths that exercises these arcs covers all the arcs in the program ddgraph. In general, a path may cover more than one unconstrained arc: the strategy we use to combine more unconstrained arcs into one path determines the cardinality of the set of test paths i.e., the bound we are looking for. It is now commonly accepted that the real problem in branch testing is to derive an executable set of test paths. Therefore, we will consider those control flow paths\u00a0\u2026", "num_citations": "38\n", "authors": ["322"]}
{"title": "A qos test-bed generator for web services\n", "abstract": " In the last years both industry and academia have shown a great interest in ensuring consistent cooperation for business-critical services, with contractually agreed levels of Quality of Service. Service Level Agreement specifications as well as techniques for their evaluation are nowadays irremissible assets. This paper presents Puppet (Pick UP Performance Evaluation Test-bed), an approach and a tool for the automatic generation of test-beds to empirically evaluate the QoS features of a Web Service under development. Specifically, the generation exploits the information about the coordinating scenario (be it choreography or orchestration), the service description (WSDL) and the specification of the agreements (WS-Agreement).", "num_citations": "36\n", "authors": ["322"]}
{"title": "Systematic generation of XML instances to test complex software applications\n", "abstract": " We introduce the XPT approach for the automated systematic generation of XML instances which conform to a given XML Schema, and its implementation into the proof-of-concept tool TAXI. XPT can be used to automatize the black-box testing of any general application that expects in input the XML instances. We generate a comprehensive set of instances by sampling all the possible combinations of elements within the schema, applying and adapting the well known Category-Partition strategy for functional testing. Originally, XPT has been conceived for application to the e-Learning domain, within which we briefly discuss some examples.", "num_citations": "34\n", "authors": ["322"]}
{"title": "Assessing the risk due to software faults: Estimates of failure rate versus evidence of perfection\n", "abstract": " In the debate over the assessment of software reliability (or safety), as applied to critical software, two extreme positions can be discerned: the \u2018statistical\u2019 position, which requires that the claims of reliability be supported by statistical inference from realistic testing or operation, and the \u2018perfectionist\u2019 position, which requires convincing indications that the software is free from defects. These two positions naturally lead to requiring different kinds of supporting evidence, and actually to stating the dependability requirements in different ways, not allowing any direct comparison. There is often confusion about the relationship between statements about software failure rates and about software correctness, and about which evidence can support either kind of statement. This note clarifies the meaning of the two kinds of statement and how they relate to the probability of failure\u2010free operation, and discusses their practical\u00a0\u2026", "num_citations": "34\n", "authors": ["322"]}
{"title": "Glimpse: a generic and flexible monitoring infrastructure\n", "abstract": " To respond to the growing needs of evolution and adaptation coming from the modern open connected world, applications must continuously monitor their own execution and the surrounding context. The events to be observed, belonging to guaranteed functional and non-functional properties, can themselves vary in scope and along time. Therefore the monitor must be easily configurable and able to serve differing event consumers. To address these requirements, we developed the glimpse monitoring infrastructure conceived having flexibility and generality as main concerns. The paper introduces the architecture of glimpse and shows how it can support runtime performance analysis through a simple example.", "num_citations": "32\n", "authors": ["322"]}
{"title": "Enhancing service federation trustworthiness through online testing\n", "abstract": " Security, data protection, trust management, authentication, and authorization are crucial assets in the Internet of Services. Online testing enhances trustworthiness among federated services that are often independently developed, deployed, and maintained.", "num_citations": "30\n", "authors": ["322"]}
{"title": "CoWTeSt: A cost weighed test strategy\n", "abstract": " In this paper we present CoWTeSt (COst Weighted TEst STrategy), an original strategy for selecting and prioritarising test cases. Cowtest supports managers to schedule and make cost estimates of the testing stages since the early phases of development. The derivation of test cases is based on the software analysis and design documentation, and uses the UML-based methodology UIT, Use Interaction Test. We report about the application of the proposed strategy to a real case study with some preliminary results.", "num_citations": "30\n", "authors": ["322"]}
{"title": "Reducing and estimating the cost of test coverage criteria\n", "abstract": " Test coverage criteria define a set of entities of a program flowgraph and require that every entity is covered by some test. We first identify E/sub c/, the set of entities to be covered according to a criterion c, for a family of widely used test coverage criteria. We then present a method to derive a minimum set of entities, called a spanning set, such that a set of test paths covering the entities in this set covers every entity in E/sub c/. We provide a generalised algorithm, which is parametrized by the coverage criterion. We suggest several useful applications of spanning sets of entities to testing. In particular they help to reduce and to estimate the number of tests needed to satisfy test coverage criteria.", "num_citations": "30\n", "authors": ["322"]}
{"title": "Yet another meta-model to specify non-functional properties\n", "abstract": " In service-oriented systems non-functional properties become very important to support run-time service discovery and composition. Software engineers should take care of them for guaranteeing the service quality in all the software life-cycle phases, from requirements specification to design, to system deployment and execution monitoring. This wide scope and the criticality of non-functional properties demand that they are expressed in a language which is intuitive and easy to use for the service quality specification, and at the same time is machine-processable to be automatically handled at run-time. In this paper we present a Property Meta-Model that aims to reach these two main objectives and show as a proof of concept its use for the modeling of two different properties.", "num_citations": "29\n", "authors": ["322"]}
{"title": "Is my model right? Let me ask the expert\n", "abstract": " Defining a domain model is a costly and error-prone process. It requires that the knowledge possessed by domain experts be suitably captured by modeling experts. Eliciting what is in the domain expert\u2019s mind and expressing it using a modeling language involve substantial human effort. In the process, conceptual errors may be introduced that are hard to detect without a suitable validation methodology. This paper proposes an approach to support such validation, by reducing the knowledge gap that separates modeling experts and domain experts. While our methodology still requires the domain expert\u2019s judgement, it partially automates the validation process by generating a set of yes/no questions from the model. Answers differing from expected ones point to elements in the model which require further consideration and can be used to guide the dialogue between domain experts and modeling experts. Our\u00a0\u2026", "num_citations": "29\n", "authors": ["322"]}
{"title": "The X-CREATE Framework-A Comparison of XACML Policy Testing Strategies.\n", "abstract": " The specification of access control policies with the XACML language could be an error prone process, so a testing is usually the solution for increasing the confidence on the policy itself. In this paper, we compare two methodologies for deriving test cases for policy testing, ie XACML requests, that are implemented in the X-CREATE tool. We consider a simple combinatorial strategy and a XML-based approach (XPT) which exploit policy values and the XACML Context Schema. A stopping criterion for the test cases generation is also provided and used for the comparison of the strategies in terms of fault detection effectiveness.", "num_citations": "28\n", "authors": ["322"]}
{"title": "Scaling up SLA monitoring in pervasive environments\n", "abstract": " Service level agreements (SLA) are contracts defining the obligations and rights between the provider of a web service and a client with respect to the service's quality. An essential component of SLA management is the run-time checking of relevant quality parameters. SLA checking must be precise and continuous to timely detect any possible SLA violations. Available techniques and mechanisms to check SLAs, however, are unlikely to scale to more complex families of SLAs tailored to fit diverse and very large groups of clients whose devices have limited storage and bandwidth capabilities. This challenge results not only in the commitment of valuable resources to SLA checking (reducing the service provider capacity), but it can also trigger additional violations and cause disagreements between client and server due to perceived violations. In this paper we focus on the elaboration of this challenge and on\u00a0\u2026", "num_citations": "28\n", "authors": ["322"]}
{"title": "Unconstrained edges and their application to branch analysis and testing of programs\n", "abstract": " Structural testing is based on the control flow of programs. In particular, branch testing requires that every branch in a program be exercised at least once; accordingly, a test path set must be selected from the potentially infinite executions paths. In this article, program structure is analyzed using a reduced graph representation, called ddgraph, which has been developed ad hoc. Two relationships (dominance and implication) between the edges of a ddgraph are introduced; these relationships make it possible to identify an edge subset, called unconstrained, with the property that, when the unconstrained edges are exercised, the traversal of all the other edges is guaranteed. Applying this property to program testing, we can state that a test path set that covers all unconstrained program branches will satisfy the branch coverage criterion.", "num_citations": "27\n", "authors": ["322"]}
{"title": "An overview of automated software testing\n", "abstract": " The paper provides a brief, yet comprehensive, introduction to the practice of dynamic testing of computer software and to the currently available automated testing tools.We first illustrate the basic concepts of testing, outlining its properties and its limitations, and then propose a control flow diagram for the testing process. Following this diagram step by step, we describe all the activities involved and the relative tools.It is understood that testing must be prepared through an accurate and well-documented specification phase.", "num_citations": "26\n", "authors": ["322"]}
{"title": "Integration of \u201cComponents\u201d to test software components\n", "abstract": " We present an ongoing research project aimed at developing a framework for component-based testing, in which we re-use and suitably combine some existing tools: the system architecture and the components are specified by the UML, and specifically the recently proposed UML Components methodology; the test cases are derived by applying the Cow_Suite, an environment for UML-based testing, previously conceived for the integration testing of OO systems; and the tests are codified and executed within the CDT, a framework under development, allowing for the decoupling between the abstract specification of tests, which is made against an architectural model, and their concrete execution, which needs to take into account the component implementations.", "num_citations": "25\n", "authors": ["322"]}
{"title": "Testing of PolPA-based usage control systems\n", "abstract": " The implementation of an authorization system is a critical and error-prone activity that requires a careful verification and testing process. As a matter of fact, errors in the authorization system code could grant accesses that should instead be denied, thus jeopardizing the security of the protected system. In this paper, we address the testing of the implementation of the Policy Decision Point (PDP) within the PolPA authorization system that enables history-based and usage-based control of accesses. Accordingly, we propose two testing strategies specifically conceived for validating the history-based access control and the usage control functionalities of the PolPA PDP. The former is based on a fault model able to highlight the problems and vulnerabilities that could occur during the PDP implementation. The latter combines the standard technique for conditions coverage with a methodology for simulating the\u00a0\u2026", "num_citations": "23\n", "authors": ["322"]}
{"title": "A uml profile and a methodology for real-time systems design\n", "abstract": " Modern real-time systems are increasingly complex and pervasive. Model driven engineering (MDE) is the emerging approach for the design of complex systems, strongly based on the usage of abstract models as key artifacts, from which an implementation is derived through a series of well-defined (automated) transformations. The widely adopted input notation in MDE is the Unified Modeling Language (UML). To express models in a particular domain, and notably for the modeling of real-time embedded systems, UML profiles have been proposed, which enrich the set of UML native elements with a consistent set of extensions. In this trend, this paper develops an approach to the design of realtime systems, based on a UML profile which is obtained from the OMG standard SPT-Profile, with a few necessary modifications", "num_citations": "22\n", "authors": ["322"]}
{"title": "Never-stop learning: Continuous validation of learned models for evolving systems through monitoring\n", "abstract": " Archive ouverte HAL - Never-stop Learning: Continuous Validation of Learned Models for Evolving Systems through Monitoring Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support hal Accueil D\u00e9p\u00f4t Consultation Les derniers d\u00e9p\u00f4ts Par type de publication Par discipline Par ann\u00e9e de publication Par structure de recherche Les portails de l'archive Recherche Documentation hal-00661027, version 1 Article dans une revue Never-stop Learning: Continuous Validation of Learned Models for Evolving Systems through Monitoring A. Bertolino 1 A. Calabr\u00f2 1 M. Merten 2 B. Steffen 2 D\u00e9tails 1 ISTI - Istituto di Scienza e Tecnologie dell'Informazione \u201cA. Faedo\" 2 TU - Technische Universit\u00e4t Dortmund [Dortmund] [\u2026", "num_citations": "21\n", "authors": ["322"]}
{"title": "Using testability measures for dependability assessment\n", "abstract": " Program \"testability\" is the probability that a fault in a program, if present, will cause the program to fail. Measures of testability can be used to draw inferences on program correctness from the observation of a series of failure-free test executions, a common need for software with \"ultra-high reliability\" requirements. For a program that has passed a certain number of tests without failing, a high value of testability implies a high probability that the program is correct. We give a general descriptive model of program execution and testing, and propose a more precise definition of program testability than that given by other authors. We then study the use of testability in: i) providing, through testing, confidence in the absence of faults and ii) bounding the probability of failures, from the results of operational testing. We derive the probability of absence of faults through a Bayesian inference procedure, criticise previously\u00a0\u2026", "num_citations": "21\n", "authors": ["322"]}
{"title": "Introducing service-oriented coverage testing\n", "abstract": " Testing of service-oriented systems is challenged by loose coupling and high dynamism, which are the founding characteristics of this emerging paradigm. Traditional test approaches needs to be revised for such systems, and in particular white-box techniques cannot be applied because services only grant black-box access. We introduce here a novel methodology, called SOCT (service-oriented coverage testing), that adapts the notion of coverage testing to the service-oriented domain, by exploiting the very features of service technology. In SOCT, both the probes inserted into the instrumented service code, and the retrieval of coverage-related information are themselves implemented as pure service invocations. A TCov service is published that makes available these coverage-related services through a WSDL interface. This simple idea elegantly enables the usage of practical test adequacy criteria also to\u00a0\u2026", "num_citations": "20\n", "authors": ["322"]}
{"title": "Let the puppets move! automated testbed generation for service-oriented mobile applications\n", "abstract": " There is a growing interest for techniques and tools facilitating the testing of mobile systems. The movement of nodes is one of the relevant factors of context change in ubiquitous systems and a key challenge in the validation of context-aware applications. An approach is proposed to generate a testbed for service-oriented systems that takes into account a mobility model of the nodes of the network in which the accessed services are deployed. This testbed allows a tester to assess off-line the QoS properties of a service under test, by considering possible variations in the response of the interacting services due to node mobility.", "num_citations": "20\n", "authors": ["322"]}
{"title": "Understanding and estimating quality of experience in WebRTC applications\n", "abstract": " WebRTC comprises a set of technologies and standards that provide real-time communication with web browsers, simplifying the embedding of voice and video communication in web applications and mobile devices. The perceived quality of WebRTC communication can be measured using quality of experience (QoE) indicators. QoE is defined as the degree of delight or annoyance of the user with an application or service. This paper is focused on the QoE assessment of WebRTC-based applications and its contribution is threefold. First, an analysis of how WebRTC topologies affect the quality perceived by users is provided. Second, a group of Key Performance Indicators for estimating the QoE of WebRTC users is proposed. Finally, a systematic survey of the literature on QoE assessment in the WebRTC arena is presented.", "num_citations": "19\n", "authors": ["322"]}
{"title": "A systematic review on cloud testing\n", "abstract": " A systematic literature review is presented that surveyed the topic of cloud testing over the period 2012--2017. Cloud testing can refer either to testing cloud-based systems (testing of the cloud) or to leveraging the cloud for testing purposes (testing in the cloud): both approaches (and their combination into testing of the cloud in the cloud) have drawn research interest. An extensive paper search was conducted by both automated query of popular digital libraries and snowballing, which resulted in the final selection of 147 primary studies. Along the survey, a framework has been incrementally derived that classifies cloud testing research among six main areas and their topics. The article includes a detailed analysis of the selected primary studies to identify trends and gaps, as well as an extensive report of the state-of-the-art as it emerges by answering the identified Research Questions. We find that cloud testing is an\u00a0\u2026", "num_citations": "19\n", "authors": ["322"]}
{"title": "Does code coverage provide a good stopping rule for operational profile based testing?\n", "abstract": " We introduce a new coverage measure, called the operational coverage, which is customized to the usage profile (count spectrum) of the entities to be covered. Operational coverage is proposed as an adequacy criterion for operational profile based testing, i.e., to assess the thoroughness of a black box test suite derived from the operational profile. To validate the approach we study the correlation between operational coverage of branches, statements, and functions, and the probability that the next test input will not fail. On the three subjects considered, we observed a moderate correlation in all cases (except a low correlation for function coverage for one subject), and consistently better results than traditional coverage measure.", "num_citations": "18\n", "authors": ["322"]}
{"title": "Monitoring service choreographies from multiple sources\n", "abstract": " Modern software applications are more and more conceived as distributed service compositions deployed over Grid and Cloud technologies. Choreographies provide abstract specifications of such compositions, by modeling message-based multi-party interactions without assuming any central coordination. To enable the management and dynamic adaptation of choreographies, it is essential to keep track of events and exchanged messages and to monitor the status of the underlying platform, and combine these different levels of information into complex events meaningful at the application level. Towards this goal, we propose a Multi-source Monitoring Framework that we are developing within the EU Project CHOReOS, which can correlate the messages passed at business-service level with observations relative to the infrastructure resources. We present the monitor architecture and illustrate it on a use\u00a0\u2026", "num_citations": "18\n", "authors": ["322"]}
{"title": "Acceptance criteria for critical software based on testability estimates and test results\n", "abstract": " Testability is defined as the probability that a program will fail a test, conditional on the program containing some fault. In this paper, we show that statements about the testability of a program can be more simply described in terms of assumptions on the probability distribution of the failure intensity of the program. We can thus state general acceptance conditions in clear mathematical terms using Bayesian inference. We develop two scenarios, one for software for which the reliability requirements are that the software must be completely fault-free, and another for requirements stated as an upper bound on the acceptable failure probability.", "num_citations": "18\n", "authors": ["322"]}
{"title": "Testing of PolPA authorization systems\n", "abstract": " The implementation of an authorization system is a difficult and error-prone activity that requires a careful verification and testing process. In this paper, we focus on testing the implementation of the PolPA authorization system and in particular its Policy Decision Point (PDP), used to define whether an access should be allowed or not. Thus exploiting the PolPA policy specification, we present a fault model and a test strategy able to highlight the problems, vulnerabilities and faults that could occur during the PDP implementation, and a testing framework for the automatic generation of a test suite that covers the fault model. Preliminary results of the test framework application to a realistic case study are presented.", "num_citations": "17\n", "authors": ["322"]}
{"title": "Unconstrained duals and their use in achieving all-uses coverage\n", "abstract": " Testing takes a considerable amount of the time and resources spent on producing software. It would therefore be useful to have ways 1) to reduce the cost of testing and 2) to estimate this cost. In particular, the number of tests to be executed is an important and useful attribute of the entity\" testing effort\". All-uses coverage is a data flow testing strategy widely researched in recent years. In this paper we present spanning sets of duas for the all-uses coverage criterion. A spanning set of duas is a minimum set of duas (definition-use associations) such that a set of test paths covering them covers every dua in the program. We give a method to find a spanning set of duas using the relation of subsumption between duas. Intuitively, there exists a natural ordering between the duas in a program: some duas are covered more easily than others, since coverage of the former is automatically guaranteed whenever the latter are\u00a0\u2026", "num_citations": "17\n", "authors": ["322"]}
{"title": "When the testing gets tough, the tough get ElasTest\n", "abstract": " We present ElasTest, an open-source generic and extensible platform supporting end-to-end testing of large complex cloud systems, including web, mobile, network and WebRTC applications. ElasTest is developed following a fully transparent and open agile process around which a community of developers, contributors and users is collected. We demonstrate ElasTest in action by testing the FullTeachingest application: the video is available from http://elastest.io/videos/icse2018-demo.", "num_citations": "16\n", "authors": ["322"]}
{"title": "A toolchain for designing and testing access control policies\n", "abstract": " Security is an important aspect of modern information management systems. The crucial role of security in this systems demands the use of tools and applications that are thoroughly validated and verified. However, the testing phase is an effort consuming activity that requires reliable supporting tools for speeding up this costly stage. Access control systems, based on the integration of new and existing tools are available in the Service Development Environment (SDE). We introduce an Access Control Testing toolchain (ACT) for designing and testing access control policies that includes the following features: (i) the graphical specification of an access control model and its translation into an XACML policy; (ii) the derivation of test cases and their execution against the XACML policy; (iii) the assessment of compliance between the XACML policy execution and the access control model. In addition, we illustrate\u00a0\u2026", "num_citations": "16\n", "authors": ["322"]}
{"title": "Automatic generation of test-beds for pre-deployment qos evaluation of web services\n", "abstract": " This paper presents Puppet (Pick UP Performance Evaluation Test-bed), an approach for the automatic generation of test-beds to empirically evaluate different QoS features of a Web Service under development. Specifically, the generation exploits the information about the coordinating scenario, the service description and the specification of the agreements that the roles will abide. The approach is supported by a proof-of-concept tool to validate the feasibility of the idea.", "num_citations": "16\n", "authors": ["322"]}
{"title": "WCT: a wrapper for component testing\n", "abstract": " Within component based (CB) software development, testing becomes a crucial activity for interoperability validation, but enabling test execution over an externally acquired component is difficult and labor-intensive. To address such problem, we propose the WCT component, a generic test wrapper that dynamically adapts the component interfaces, making the test execution independent from any specific component implementation. The wrapper does not require that the component implements any specific interface for testing purposes, it permits to easily reconfigure the subsystem under test after the introduction/ removal/substitution of a component, and helps optimize test reuse, by keeping trace of the test cases that directly affect the wrapped component.", "num_citations": "16\n", "authors": ["322"]}
{"title": "An automated test strategy based on uml diagrams\n", "abstract": " This is a work-in-progress report about the Cow_Suite tool currently under development for automating CoWTeSt (COst Weighted TEst STrategy), an original strategy for selecting and prioritarising test cases. The tool supports managers to schedule and make cost estimates of the integration test stages since the early phases of development. The derivation of test cases is based on the software analysis and design documentation, and uses the UML-based original test methodology UIT, Use Interaction Test. We describe the tool architecture and show the provided features through an example of application to a real case study with some results.CowSuite tool and CoWTeSt have been planned and developed by PISATEL. PISATEL is a Software Laboratory established in Istituto Elaborazione Informatica (IEI) Pisa in cooperation with Ericsson Lab Italy (ERI). Starting in January 2001, teams from IEI and ERI are\u00a0\u2026", "num_citations": "16\n", "authors": ["322"]}
{"title": "A tour of secure software engineering solutions for connected vehicles\n", "abstract": " The growing number of vehicles daily moving on roads increases the need of protecting the safety and security of passengers, pedestrians, and vehicles themselves. This need is intensified when considering the pervasive introduction of Information and Communication Technologies (ICT) systems into modern vehicles, because this makes such vehicles potentially vulnerable from the point of view of security. The convergence of safety and security requirements is one of the main outstanding research challenges in software-intensive systems. This work reviews existing methodologies and solutions addressing security issues in the automotive domain with a focus on the integration between safety and security aspects. In particular, we identify the main security issues with vehicular communication technologies and existing gaps between state-of-the-art methodologies and their implementation in the real\u00a0\u2026", "num_citations": "15\n", "authors": ["322"]}
{"title": "An extensible framework for online testing of choreographed services\n", "abstract": " Service choreographies present numerous engineering challenges, particularly with respect to testing activities, that traditional design-time approaches cannot properly address. A proposed online testing solution offers a powerful, extensible framework to effectively assess service compositions, leading to a more trustworthy and reliable service ecosystem.", "num_citations": "15\n", "authors": ["322"]}
{"title": "More testable service compositions by test metadata\n", "abstract": " In previous work we proposed testable services as a solution to provide third-party testers with structural coverage information after a test session, yet without revealing their internal details. However, service testers, e.g., integrators that use testable services into their compositions, do not have enough information to improve their test set when they get a low coverage measure because they do not know which test requirements have not been covered. This paper proposes an approach in which testable services are provided along with test metadata that will help their testers to get a higher coverage. We show the approach on a case study of a real system that uses orchestrations and testable services.", "num_citations": "15\n", "authors": ["322"]}
{"title": "(role) cast: A framework for on-line service testing\n", "abstract": " Engineering of service-oriented systems is still an immature discipline. Traditional software engineering approaches do not adequately fit development needs arising in this widely adopted paradigm. In particular, because of dynamic service composition, several engineering activities typically performed off-line (i.e., predeployment) have to be carried on also on-line (i.e., during real usage). In this paper, we present a framework called (role)CAST which supports an instantiation of the concept of on-line testing of services, for the purpose of validating their compliance to role-based service access policies.", "num_citations": "15\n", "authors": ["322"]}
{"title": "Preventing untestedness in data\u2010flow based testing\n", "abstract": " A large number of path\u2010oriented testing criteria have been proposed in the last twenty years. Surprisingly, almost all of them suffer from a serious weakness, which is called the untestedness syndrome: even though a criterion is satisfied, some statements of the program under test may remain \u2018untested\u2019, i.e., the observed test output does not depend on them. A new data\u2010flow based testing criterion is introduced which does not suffer from untestedness, called the All Program Function (APF) criterion. Intuitively, it requires that each possible computation to every output statement in a program be covered by some test; but for lots of programs APF would require an infinite number of tests. A second, applicable criterion is thus introduced, derived from APF and called the Basic Program Function (BPF) criterion. BPF leaves no statement untested and yields finite test suites. Some examples show the application of BPF and\u00a0\u2026", "num_citations": "15\n", "authors": ["322"]}
{"title": "Trends and research issues in SOA validation\n", "abstract": " Service Oriented Architecture (SOA) is changing the way in which software applications are designed, deployed and maintained. A service-oriented application consists of the runtime composition of autonomous services that are typically owned and controlled by different organizations. This decentralization impacts on the dependability of applications that consist of dynamic services agglomerates, and challenges their validation. Different techniques can be used or combined for the verification of dependability aspects, spanning over traditional off-line testing approaches, monitoring, and on-line testing. In this chapter we discuss issues and opportunities of SOA validation, we identify three different stages for validation along the service life-cycle model, and we overview some proposed research approaches and tools. The emphasis is on on-line testing, which to us is the most peculiar stage in the SOA validation\u00a0\u2026", "num_citations": "14\n", "authors": ["322"]}
{"title": "Social coverage for customized test adequacy and selection criteria\n", "abstract": " Test coverage information can be very useful for guiding testers in enhancing their test suites to exercise possible uncovered entities and in deciding when to stop testing. However, for complex applications that are reused in different contexts and for emerging paradigms (eg, component-based development, service-oriented architecture, and cloud computing), traditional coverage metrics may no longer provide meaningful information to help testers on these tasks. Various proposals are advocating to leverage information that come from the testing community in a collaborative testing approach. In this work we introduce a coverage metric, the Social Coverage, that customizes coverage information in a given context based on coverage data collected from similar users. To evaluate the potential of our proposed approach, we instantiated the social coverage metric in the context of a real world service oriented\u00a0\u2026", "num_citations": "13\n", "authors": ["322"]}
{"title": "Dependability and performance assessment of dynamic connected systems\n", "abstract": " In this chapter we present approaches for analysis and monitoring of dependability and performance of connected systems, and their combined usage. These approaches need to account for dynamicity and evolvability of connected systems. In particular, the chapter covers the quantitative assessment of dependability and performance properties through a stochastic model-based approach: first an overview of dependability-related measurements and stochastic model-based approaches provides the necessary background. Then, our proposal in connect of an automated and modular dependability analysis framework for dynamically connected systems is described. This framework can be used off-line for system design (specifically, in connect, for connector synthesis), and on-line, to continuously assess system behaviour and detect possible issues arising at run-time. For the latter purpose, a generic\u00a0\u2026", "num_citations": "13\n", "authors": ["322"]}
{"title": "Predicting software reliability from testing taking into account other knowledge about a program\n", "abstract": " Inference from statistical testing is the only sound method available for estimating software reliability. However, if one ignores evidence other than testing (eg, evidence from the track record of a developer, or from the quality of the development process), the results are going to be so conservative that they are often felt to be useless for decision-making. Bayesian inference is the main mathematical tool for taking into account such knowledge. Evidence from sources other than testing is modelled as prior probabilities (for values of the failure rate of the program) and is updated on the basis of test results to produce posterior probabilities. We explain these methods and demonstrate their use on simple examples. The measure of interest is the probability that a program satisfies a given reliability requirement, given that it has passed a certain number of tests. The procedures of Bayesian inference explicitly show the weights of prior assumptions vs. test results in determining this probability. We also demonstrate how one can model different assumptions about the faultrevealing efficacy of testing. We believe that these methods are a powerful aid for improving the quality of decision-making in matters related to software reliability.", "num_citations": "13\n", "authors": ["322"]}
{"title": "An assessment of operational coverage as both an adequacy and a selection criterion for operational profile based testing\n", "abstract": " While the relation between code coverage measures and fault detection is actively studied, only few works have investigated the correlation between measures of coverage and of reliability. In this work, we introduce a novel approach to measuring code coverage, called the operational coverage, that takes into account how much the program\u2019s entities are exercised so to reflect the profile of usage into the measure of coverage. Operational coverage is proposed as (i) an adequacy criterion, i.e., to assess the thoroughness of a black box test suite derived from the operational profile, and as (ii) a selection criterion, i.e., to select test cases for operational profile-based testing. Our empirical evaluation showed that operational coverage is better correlated than traditional coverage with the probability that the next test case derived according to the user\u2019s profile will not fail. This result suggests that our approach\u00a0\u2026", "num_citations": "12\n", "authors": ["322"]}
{"title": "Do we need new strategies for testing systems-of-systems?\n", "abstract": " This paper overviews the main Systems-of-Systems (SoS) characteristics that can impact on their verification, validation and testing (VV&T). Furthermore, it addresses technical, conceptual, social and organizational challenges, discusses which existing approaches of VV&T can be used for SoS, and points out future research in the field.", "num_citations": "12\n", "authors": ["322"]}
{"title": "An automated model-based test oracle for access control systems\n", "abstract": " In the context of XACML-based access control systems, an intensive testing activity is among the most adopted means to assure that sensible information or resources are correctly accessed. Unfortunately, it requires a huge effort for manual inspection of results: thus automated verdict derivation is a key aspect for improving the cost-effectiveness of testing. To this purpose, we introduce XACMET, a novel approach for automated model-based oracle definition. XACMET defines a typed graph, called the XAC-Graph, that models the XACML policy evaluation. The expected verdict of a specific request execution can thus be automatically derived by executing the corresponding path in such graph. Our validation of the XACMET prototype implementation confirms the effectiveness of the proposed approach.", "num_citations": "12\n", "authors": ["322"]}
{"title": "Adequate monitoring of service compositions\n", "abstract": " Monitoring is essential to validate the runtime behaviour of dynamic distributed systems. However, monitors can inform of relevant events as they occur, but by their very nature they will not report about all those events that are not happening. In service-oriented applications it would be desirable to have means to assess the thoroughness of the interactions among the services that are being monitored. In case some events or message sequences or interaction patterns have not been observed for a while, in fact, one could timely check whether this happens because something is going wrong. In this paper, we introduce the novel notion of monitoring adequacy, which is generic and can be defined on different entities. We then define two adequacy criteria for service compositions and implement a proof-of-concept adequate monitoring framework. We validate the approach on two case studies, the Travel Reservation\u00a0\u2026", "num_citations": "12\n", "authors": ["322"]}
{"title": "A generative approach for the adaptive monitoring of SLA in service choreographies\n", "abstract": " Monitoring is an essential means in the management of service-oriented applications. Here, event correlation results crucial when monitoring rules aim at checking the exposed levels of Quality of Service against the Service Level Agreements established among the choreography participants. However, when choreographies are enacted over distributed networks or clouds, the relevant monitoring rules might not be completely defined a-priori, as they may need to be adapted to the specific infrastructure and to the evolution of events. This paper presents an adaptive multi-source monitoring architecture synthesizing instances of rules at run-time and shows examples of use on a demonstration scenario from the European Project CHOReOS.", "num_citations": "12\n", "authors": ["322"]}
{"title": "Approaches to functional, structural and security SOA testing\n", "abstract": " In this chapter, we provide an overview of recently proposed approaches and tools for functional and structural testing of SOA services. Typically, these two classes of approaches have been considered separately. However, since they focus on different perspectives, they are generally non-conflicting and could be used in a complementary way. Accordingly, we make an attempt at such a combination, briefly showing the approach and some preliminary results of the experimentation. The combined approach provides encouraging results from the point of view of the achievements and the degree of automation obtained. A very important concern in designing and developing web services is security. In the chapter we also discuss the security testing challenges and the currently proposed solutions.", "num_citations": "12\n", "authors": ["322"]}
{"title": "On-the-fly dependable mediation between heterogeneous networked systems\n", "abstract": " The development of next generation Future Internet systems must be capable to address complexity, heterogeneity, interdependency and, especially, evolution of loosely connected networked systems. The European project Connect addresses the challenging and ambitious topic of ensuring eternally functioning distributed and heterogeneous systems through on-the-fly synthesis of the Connectors through which they communicate. In this paper we focus on the Connect enablers that dynamically derive such connectors ensuring the required non-functional requirements via a framework to analyse and assess dependability and performance properties. We illustrate the adaptive approach under development integrating synthesis of Connectors, stochastic model-based analysis performed at design time and run-time monitoring. The proposed framework is illustrated on a case study.", "num_citations": "12\n", "authors": ["322"]}
{"title": "Software testing forever: Old and new processes and techniques for validating today\u2019s applications\n", "abstract": " Software testing is a very complex activity deserving a first-class role in software development. Testing related activities encompass the entire development process and may consume a large part of the effort required for producing software. In this talk, I will first organize into a coherent framework the many topics and tasks forming the software testing discipline, pointing at relevant open issues [1]. Then, among the outlined challenges, I will focus on some hot ones posed by the testing of modern complex and highly dynamic systems [2]. What is assured is that software testers do not risk to remain without their job, and testing researchers are not at short of puzzles. Software testing is and will forever be a fundamental activity of software engineering: notwithstanding the revolutionary advances in the way it is built and employed (or perhaps exactly because of), the software will always need to be eventually tried\u00a0\u2026", "num_citations": "12\n", "authors": ["322"]}
{"title": "On-the-fly interoperability through automated mediator synthesis and monitoring\n", "abstract": " Interoperability is a key and challenging requirement in today\u2019s and future systems, which are often characterized by an extreme level of heterogeneity. To build an interoperability solution between the networked systems populating the environment, both their functional and non-functional requirements have to be met.               Because of the continuous evolution of such systems, mechanisms that are fixed a-priori are inadequate to achieve interoperability. In such challenging settings, on-the-fly approaches are best suited.               This paper presents, as an interoperability solution, an approach that integrates an automated technique for the synthesis of mediator protocols with a monitoring mechanism. The former aims to provide interoperability taking care of functional characteristics of the networked systems, whereas the latter makes it possible to assess the non-functional characteristics of the connected\u00a0\u2026", "num_citations": "11\n", "authors": ["322"]}
{"title": "PLASTIC: Providing lightweight & adaptable service technology for pervasive information & communication\n", "abstract": " The PLASTIC project adopts and revisits service-oriented computing for Beyond 3 rd  Generation (B3G) networks, in particular aiming at assisting the development of services targeted at mobile devices. Specifically, PLASTIC introduces the PLASTIC platform to enable robust distributed lightweight services in B3G networking environments through: (a) A development environment for the thorough development of SLA- and resource-aware services, which may be deployed on the various networked nodes, including handheld devices; (b) A service-oriented middleware leveraging multi-radio devices and multi-network environments for applications and services deployed on mobile devices, further enabling context-aware and secure discovery and access to such services; (c) A validation framework enabling off-line and on-line validation of networked services regarding functional and non-functional properties.", "num_citations": "10\n", "authors": ["322"]}
{"title": "Software Component Integration Testing: A Survey\n", "abstract": " Component-based development has emerged as a system engineering approach that promises rapid software development with fewer resources. Yet, improved reuse and reduced cost benefits from software components can only be achieved in practice if the components provide reliable services, which makes component analysis and testing a key activity. This paper discusses various issues that can arise at component integration phase. The crucial problem is the lack of information for analysis and testing of components externally developed, aggravated by differing perspectives of the key players in component based software development. Several component integration testing techniques have been recently proposed to provide a solution for those issues. These techniques are here surveyed and classified according to a proposed set of relevant attributes. The paper thus provides a useful and comprehensive\u00a0\u2026", "num_citations": "10\n", "authors": ["322"]}
{"title": "Re-thinking the development process of component-based software\n", "abstract": " This paper contribution to the ECBS workshop is a position statement that a wide gap exists between the technologies for Component-based Software Engineering and the scientific foundations on which this technology relies. What is mostly lacking is a revised model for the development process. We very quickly outline a skeleton for re-thinking the models that have shaped the software production in the last decades, and we start to make some speculations, in particular for what concerns the testing stages. As a working example, we take in consideration the Enterprise Java Beans framework. However, our research goal is to draw generally valid conclusions and insights.", "num_citations": "10\n", "authors": ["322"]}
{"title": "Validation and Verification Policies for Governance of Service Choreographies.\n", "abstract": " The Future Internet (FI) sustains the emerging vision of a software ecosystem in which pieces of software, developed, owned and run by different organizations, can be dynamically discovered and bound to each other so to readily start to interact. Nevertheless, without suitable mechanisms, paradigms and tools, this ecosystem is at risk of tending towards chaos. Indeed the take off of FI passes through the introduction of paradigms and tools permitting to establish some discipline. Choreography specifications and Governance are two different proposals which can contribute to such a vision, by permitting to define rules and functioning agreements both at the technical level and at the social (among organizations) level. In this paper we discuss such aspects and introduce a policy framework so to support a FI ecosystem in which V&V activities are controlled and perpetually run so to contribute to the quality and trustworthiness perceived by all the involved stakeholders.", "num_citations": "9\n", "authors": ["322"]}
{"title": "XML Every-Flavor Testing.\n", "abstract": " With XML and XML Schema widely acknowledged as the de facto standard for data exchange and interoperability between remote applications, the need for checking integrity and adequacy of XML documents, also by means of automated tools, increases. In this perspective, this paper addresses two objectives: we provide a classification and a short overview of the diverse existing approaches for the testing of XML-based documents; then, pushing further the potential of XML for testing purposes, we pursuit the application of traditional testing methods to programs using XML input data. We discuss the use of XML and XML schema as a basis for formalizing and automatizing the testing of applications using such kind of data, with particular reference to recent proposals for specification-based and perturbation-based testing approaches.", "num_citations": "9\n", "authors": ["322"]}
{"title": "Softure: Adaptable, reliable and performing software for the future\n", "abstract": " This paper discusses the approach that will be taken by the PLASTIC project (http://www. ist-plastic. org) in order to assist the development of adaptable, reliable and performing software services for Beyond 3rd Generation networks.", "num_citations": "9\n", "authors": ["322"]}
{"title": "A categorization scheme for software engineering conference papers and its application\n", "abstract": " BackgroundIn Software Engineering (SE), conference publications have high importance both in effective communication and in academic careers. Researchers actively discuss how a paper should be organized to be accepted in mainstream conferences.AimingThis work tackles the problem of generalizing and characterizing the type of papers accepted at SE conferences.MethodThe paper offers a new perspective in the analysis of SE literature: a categorization scheme for SE papers is obtained by merging, extending and revising related proposals from a few existing studies. The categorization scheme is used to classify the papers accepted at three top-tier SE conferences during five years (2012\u20132016).ResultsWhile a broader experience is certainly needed for validation and fine-tuning, preliminary outcomes can be observed relative to what problems and topics are addressed, what types of contributions are\u00a0\u2026", "num_citations": "8\n", "authors": ["322"]}
{"title": "Towards business process execution adequacy criteria\n", "abstract": " Monitoring of business process execution has been proposed for the evaluation of business process performance. An important aspect to assess the thoroughness of the business process execution is to monitor if some entities have not been observed for some time and timely check if something is going wrong. We propose in this paper business process execution adequacy criteria and provide a proof-of-concept monitoring framework for their assessment. Similar to testing adequacy, the purpose of our approach is to identify the main entities of the business process that are covered during its execution and raise a warning if some entities are not covered. We provide a first assessment of the proposed approach on a case study in the learning context.", "num_citations": "8\n", "authors": ["322"]}
{"title": "An automated approach to robustness testing of BPEL orchestrations\n", "abstract": " Service compositions are increasingly used for the fast development of loosely-coupled dynamic distributed applications. Such compositions are prone to potential failures arising from their complex interaction scheme and from various kinds of network problems. Functional testing could overlook such failures, hence we claim it is important to specifically consider test cases for validating how the composite service reacts when invalid or unexpected events occur. We have developed the TASSA framework for automated generation and execution of robustness test cases. The framework addresses BPEL orchestrations and relies on fault-injection mechanisms. In the paper we motivate BPEL robustness testing, introduce the framework and present its preliminary assessment on a case study.", "num_citations": "8\n", "authors": ["322"]}
{"title": "Automated testing of healthcare document transformations in the PICASSO interoperability platform\n", "abstract": " In every application domain, achieving interoperability among heterogenous information systems is a crucial challenge and alliances are formed to standardize data-exchange formats. In the healthcare sector, HL7-V3 provides the current international reference models for clinical and administrative documents. Codices, an Italian company, provides the PICASSO platform that uses HL7-V3 as the pivot format to fast achieve a highly integrated degree of interoperability among health-related applications. Given the XML structure of HL7-V3, PICASSO can exploit the XSLT technology to flexibly transform documents. However, Codices spends a large part of the PICASSO deployment workflow for manually validating the required XSL stylesheets. In this paper, we describe a pilot experience in test automation, based on the TAXI tool that applies systematic black-box techniques to generate a set of XML instances from a\u00a0\u2026", "num_citations": "8\n", "authors": ["322"]}
{"title": "ISSTA 2002 panel: is ISSTA research relevant to industrial users?\n", "abstract": " ISSTA is at its twelfth edition. Also this year, researchers from academy and industry have contributed with many interesting studies and experience reports in software analysis and testing. We---the ISSTA partakers-have (or at least believe to have) clear ideas about which are the problems to be solved, which are the real challenges, and probably each of us has already settled an agenda of the next steps to take for solving them looking ahead to the next ISSTA edition. Are we doing right? Do we know which are the real issues in the field? Is our research addressing relevant points, or just aesthetic questions? Do, and how much, industrial users---the ISSTA addressees-value our papers and our achievements? This panel will address such questions by grouping a set of managers from different industries around a table and asking their opinions. As the above questions are very general, in the intent to tackle the\u00a0\u2026", "num_citations": "8\n", "authors": ["322"]}
{"title": "Advances in test automation for software with special focus on artificial intelligence and machine learning\n", "abstract": " Software testing is an integral part of the software engineering (SE) discipline. Effective testing with reduced costs can be achieved through automating the testing process. In the past decades, a great amount of research effort has been spent on automatic test case generation, automatic test selection, automatic test oracles, etc. and there has been a rapid growth of practices in using automated software testing tools. Work on this topic has long been published as an important part of the software engineering discipline, and in recent years testing is consistently among the top-most popular topics in submissions to SE conferences. In the past few years, a large number of software test tools have been developed and made available on the market. The practice of software test automation (TA) has also moved forward significantly, from record-and-replay techniques to support automated testing of graphical user\u00a0\u2026", "num_citations": "7\n", "authors": ["322"]}
{"title": "Know your neighbor: fast static prediction of test flakiness\n", "abstract": " Flaky tests plague regression testing in Continuous Integration environments by slowing down change releases, wasting development e ort, and also eroding testers trust in the test process. We present FLAST, the rst static approach to akiness detection using test code similarity. Our extensive evaluation on 24 projects taken from repositories used in three previous studies showed that FLAST can identify aky tests with up to 0.98 Median and 0.92 Mean precision. For six of those projects it could already yield\u223c 0.98 average precision values with a training set containing less than 100 tests. Besides, where known aky tests are classi ed according to their causes, the same approach can also predict a aky test category with alike precision values. The cost of the approach is negligible: the average train time over a dataset of\u223c 1,700 test methods is less than one second, while the average prediction time for a new test is\u00a0\u2026", "num_citations": "7\n", "authors": ["322"]}
{"title": "Online robustness testing of distributed embedded systems: An industrial approach\n", "abstract": " Having robust systems that behave properly even in presence of faults is becoming increasingly important. This is the case of the system we investigate in this paper, which is an embedded distributed system consisting of components that communicate with each other via messages exchange in the RBS (Radio Based Station) at Ericsson AB in Gothenburg, Sweden. Specifically, this paper describes a novel fault injection approach for testing the robustness of distributed embedded systems with very limited computation power. The new approach is inspired by Netflix's ChaosMonkey, a fault injection approach that has been developed for testing distributed systems hosted in the cloud. However, ChaosMonkey cannot be used in the context of RBS since the latter consists of small-embedded components with specific requirements of performance, programming language, and communication paradigm. This paper\u00a0\u2026", "num_citations": "7\n", "authors": ["322"]}
{"title": "Testing access control policies against intended access rights\n", "abstract": " Access Control Policies are used to specify who can access which resource under which conditions, and ensuring their correctness is vital to prevent security breaches. As access control policies can be complex and error-prone, we propose an original framework that supports the validation of the implemented policies (specified in the standard XACML notation) against the intended rights, which can be informally expressed, eg in tabular form. The framework relies on well-known software testing technology, such as mutation and combinatorial techniques. The paper presents the implemented environment and an application example.", "num_citations": "7\n", "authors": ["322"]}
{"title": "Improving test coverage measurement for reused software\n", "abstract": " Test coverage adequacy measures provide a widely used stopping criterion. Engineering of modern software-intensive systems emphasizes reuse. In the case that a program uses reused code or third-party components in a context that is different from the original one, some of their entities (e.g. Branches) might never be exercised, thus producing a code coverage level far from full and not meaningful anymore as a stopping rule for the program at hand. We introduce a new coverage criterion, called \"Relevant Coverage\", that in each testing context in which a code is reused calculates coverage measures over the set of relevant entities for that context. We provide an approach for identifying relevant entities using dynamic symbolic execution. The introduced coverage adequacy criterion is assessed in an exploratory study against traditional coverage in terms of test suite size reduction factor, cost-effectiveness ratio\u00a0\u2026", "num_citations": "7\n", "authors": ["322"]}
{"title": "Adaptive SLA monitoring of service choreographies enacted on the cloud\n", "abstract": " The deployment and the execution of applications on dynamic Cloud infrastructures introduces new requirements of adaptability with respect to monitoring. Specifically, the governance of service choreographies enacted over Cloud-based solutions relies on the observation and analysis of events happening at different abstraction layers. Adaptability requirements are even more evident when monitoring deals with Service Level Agreements (SLA) established among the choreography participants. In fact, as the Cloud paradigm offers on-demand solutions as a service, often monitoring rules cannot be completely defined off-line. Thus also the monitoring infrastructure must keep track of the continuous evolution of the underlying environment, and adapt itself accordingly. This paper proposes an adaptive multi-source monitoring architecture that can synthesize on-the-fly SLA monitoring rules following the evolution of\u00a0\u2026", "num_citations": "7\n", "authors": ["322"]}
{"title": "Software testing for dependability assessment\n", "abstract": " Software testing can be aimed at two different goals: removing faults and evaluating dependability. Testing methods described in textbooks having the word \u201ctesting\u201d in their title or more commonly used in the industry are mostly intended to accomplish the first goal: revealing failures, so that the faults that caused them can be located and removed. However, the final goal of a software validation process should be to achieve an objective measure of the confidence that can be put on the software being developed. For this purpose, conventional reliability theory has been applied to software engineering and nowadays several reliability growth models can be used to accurately predict the future reliability of a program based on the failures observed during testing. Paradoxically, the most difficult situation is that of a software product that does not fail during testing, as is normally the case for safety-critical\u00a0\u2026", "num_citations": "7\n", "authors": ["322"]}
{"title": "Security assessment of systems of systems\n", "abstract": " Engineering Systems of Systems is one of the new chal-lenges of the last few years. This depends on the increasing number of systems that must interact one with another to achieve a goal. One peculiarity of Systems of Systems is that they are made of systems able to live on their own with well-established functionalities and requirements, and that are not necessarily aware of the joint mission or prepared to collaborate. In this emergent scenario, securi-ty is one crucial aspect that must be considered from the very beginning. In fact, the security of a System of Sys-tems is not automatically granted even if the security of each constituent system is guaranteed. The aim of this paper is to address the problem of assessing security properties in Systems of Systems. We discuss the specific security aspects of such emergent systems, and propose the TeSSoS approach, which includes modelling and testing security\u00a0\u2026", "num_citations": "6\n", "authors": ["322"]}
{"title": "A toolchain for designing and testing XACML Policies\n", "abstract": " In modern pervasive application domains, such as Service Oriented Architectures (SOAs) and Peer-to-Peer (P2P) systems, security aspects are critical. Justified confidence in the security mechanisms that are implemented for assuring proper data access is a key point. In the last years XACML has become the de facto standard for specifying policies for access control decisions in many application domains. Briefly, an XACML policy defines the constraints and conditions that a subject needs to comply with for accessing a resource and doing an action in a given environment. Due to the complexity of the language, XACML policy specification is a difficult and error prone process that requires specific knowledge and a high effort to be properly managed.", "num_citations": "6\n", "authors": ["322"]}
{"title": "Service lifecycle management\n", "abstract": " The ability to effectively manage the different steps of the service lifecycle is fundamental to the success of mobile service platforms. Indeed, while service lifecycle has been traditionally viewed as largely isolated steps, the need to take into account the changing mobile environment and the distribution of resources and service components is leading to stronger intertwining. This white paper presents the work on service lifecycle management carried out within projects of the Mobile Services Platform cluster. Specifically, the problems of service creation, monitoring, validation, deployment, discovery and composition in B3G networks are addressed. While these projects have investigated the same research domains, the variations in the targeted application domains or in the underlying infrastructure have lead to different approaches and solutions.This white paper was produced as a result of work in the Mobile Service Platforms (MSP) cluster of IST FP6 projects. The MSP cluster is part of the Sixth Framework Program of European research activities IST (Information Society Technologies). The cluster covers all work providing elements of platforms, which facilitate the development and deployment of mobile services. The cluster provides a forum to facilitate knowledge sharing on all aspects of B3G service platform technologies. The problem space includes: hiding the heterogeneous service execution platforms, managing the personal communication sphere (devices, groups, networks), creating autonomous systems, context/knowledge management, automatic service composition, end-user/3rd party service development environments, service\u00a0\u2026", "num_citations": "6\n", "authors": ["322"]}
{"title": "A user-oriented framework for component deployment testing\n", "abstract": " The capability to test an externally acquired software component deployed within the target application environment can help establish the compliance of a candidate component to the customer\u2019s expectations, and helps \u201cincrease trust.\u201d To this end, in this chapter we introduce the CDT framework that is conceived to ease the planning and the execution of test suites by the component user. We discuss the main challenges raised by the new component-based software production methodology and how the proposed framework can facilitate the evaluation of candidate components. Notably, CDT permits codifying a component deployment test suite in the early stages of development without reference to any (yet unknown) real implementation, and, later, during the integration stage, adapting and executing the test suite on a found candidate component.", "num_citations": "6\n", "authors": ["322"]}
{"title": "Testing relative to usage scope: revisiting software coverage criteria\n", "abstract": " Coverage criteria provide a useful and widely used means to guide software testing; however, indiscriminately pursuing full coverage may not always be convenient or meaningful, as not all entities are of interest in any usage context. We aim at introducing a more meaningful notion of coverage that takes into account how the software is going to be used. Entities that are not going to be exercised by the user should not contribute to the coverage ratio. We revisit the definition of coverage measures, introducing a notion of relative coverage. According to this notion, we provide a definition and a theoretical framework of relative coverage, within which we discuss implications on testing theory and practice. Through the evaluation of three different instances of relative coverage, we could observe that relative coverage measures provide a more effective strategy than traditional ones: we could reach higher coverage\u00a0\u2026", "num_citations": "5\n", "authors": ["322"]}
{"title": "Governing regression testing in systems of systems\n", "abstract": " Great advances in network technology and software engineering have triggered the development and spread of Systems of Systems (SoSs). The dynamic and evolvable nature of SoSs poses important challenges on the validation of such systems and in particular on their regression testing, aiming at assessing that run-time changes and evolutions do not introduce regression in SoS behavior. This paper outlines issues and challenges of regression testing of SoSs, identifying the main kinds of evolution that can impact on their regression testing activity. Furthermore, it presents a conceptual framework for governing the regression testing of SoSs. The proposed framework leverages the concept of an orchestration graph that describes the flow of test cases and sketches a solution for deriving a regression test plan according to test cases dependencies.", "num_citations": "5\n", "authors": ["322"]}
{"title": "Annotated buzzwords and key references for software testing in the cloud\n", "abstract": " In contemporary IT and Internet society, Cloud computing brings a paradigm shift not only for successfully evolving large distributed software applications but also for delivering a number of innovative services and applications. Cloud computing is an umbrella enveloping several models of service, like SaaS, IaaS, PaaS, TaaS. In particular, software testing in/on/over cloud is the focus of this paper. We see software testing in the Cloud and TaaS as a novel domain-specific approach to software testing. However, the field is still immature and there does not yet exist a clear understanding of its concepts and constituent technologies. In this paper we provide an introductory overview of software testing in the cloud, based also on a systematic search of literature, along with its buzzwords and tools.", "num_citations": "5\n", "authors": ["322"]}
{"title": "Learning Path Specification for Workplace Learning based on Business Process Management.\n", "abstract": " In modern society, workers are continuously challenged to acquire new skills and competencies while at work. Novel approaches and tools to support effective and efficient workplace learning in collaborative and engaging ways are needed. On the other hand, Business Process Management (BPM) is more and more employed to support and manage the complex processes carried out within organizations. We propose to use BPM also to drive workplace learning, with the advantage of aligning real tasks to training tasks. We introduce a specification of learning path that maps BPM tasks and activities into sequences of learning tasks that can be customized to learners competence. The learning path specification can be used to both drive learning sessions, and to inform a monitor that can assess learner\u2019s progress. We describe a platform that is under development, and provide a simple motivational example to illustrate the approach. The goal is to combine work and learning in natural and effective way.", "num_citations": "5\n", "authors": ["322"]}
{"title": "A tool-supported methodology for validation and refinement of early-stage domain models\n", "abstract": " Model-driven engineering (MDE) promotes automated model transformations along the entire development process. Guaranteeing the quality of early models is essential for a successful application of MDE techniques and related tool-supported model refinements. Do these models properly reflect the requirements elicited from the owners of the problem domain? Ultimately, this question needs to be asked to the domain experts. The problem is that a gap exists between the respective backgrounds of modeling experts and domain experts. MDE developers cannot show a model to the domain experts and simply ask them whether it is correct with respect to the requirements they had in mind. To facilitate their interaction and make such validation more systematic, we propose a methodology and a tool that derive a set of customizable questionnaires expressed in natural language from each model to be validated\u00a0\u2026", "num_citations": "5\n", "authors": ["322"]}
{"title": "An automated testing framework of model-driven tools for XACML policy specification\n", "abstract": " Access Control is among the most important security mechanisms to put in place in order to secure applications. XACML is the de facto standard for storing and deploying access control policies. However, due to the complexity of the XACML language, policy definition becomes a difficult and error prone process. In recent years, the combined use of models for the access control policy specification, and the model-to-code facilities, for the automatic transformation of the model into the XACML language, has been proposed as a possible solution. These model-driven methodologies and facilities need to be thoroughly validated and verified. In this paper we provide an integrated framework for testing the automatic translation of the specification of an access control model into an XACML policy. The framework includes different test strategies for the derivation of test cases and some facilities for making easier their\u00a0\u2026", "num_citations": "5\n", "authors": ["322"]}
{"title": "A requirements-led approach for specifying qos-aware service choreographies: An experience report\n", "abstract": " [Context and motivation] Choreographies are a form of service composition in which partner services interact in a global scenario without a single point of control. The absence of an explicitly specified orchestration requires changes to requirements practices to recognize the need to optimize software services choreography and monitoring for satisfaction with system requirements. [Question/problem] We developed a requirements-led approach that aims to provide tools and processes to transform requirements expressed on service-based systems to QoS-aware choreography specifications. [Principal ideas/results] The approach is used by domain experts to specify natural language requirements on a service-based system, and by choreography designers to adapt their models to satisfy requirements more effectively. Non-functional requirements are mapped to BPMN choreography diagrams as\u00a0\u2026", "num_citations": "5\n", "authors": ["322"]}
{"title": "Governance policies for verification and validation of service choreographies\n", "abstract": " The Future Internet (FI) sustains the emerging vision of a software ecosystem in which pieces of software, developed, owned and run by different organizations, can be dynamically discovered and bound to each other so to readily start to interact. Nevertheless, without suitable mechanisms, paradigms and tools, this ecosystem is at risk of tending towards chaos. Indeed the take off of FI passes through the introduction of paradigms and tools permitting to establish some discipline. Choreography specifications and Governance are two different proposals which can contribute to such a vision, by permitting to define rules and functioning agreements both at the technical level and at the social (among organizations) level. In this paper we discuss such aspects and introduce a policy framework so to support a FI ecosystem in which V&V activities are controlled and perpetually run so to contribute to the quality and\u00a0\u2026", "num_citations": "5\n", "authors": ["322"]}
{"title": "Approaches to testing service-oriented software systems\n", "abstract": " The attractiveness and popularity of Service-Oriented Software Systems (SOSSs) stem from the growing availability of independent services that can be cost-effectively composed with other services to dynamically provide richer functionality. Service-orientation however poses new and difficult challenges to testers, especially when it comes to testing the interactions between heterogeneous, loosely coupled and independently developed services. Service integration testing requires discipline, standardized processes, and agreed policies to be put in place, which we referred to as SOA (Service Oriented Architecture) Test Governance (STG). Discovered services usually provide just a syntactical interface, enabling some general black-box tests, but insufficient to develop an adequate understanding of the integration quality between the interacting services. Besides, testing for the functional and extra-functional\u00a0\u2026", "num_citations": "5\n", "authors": ["322"]}
{"title": "VCR: Virtual capture and replay for performance testing\n", "abstract": " This paper proposes a novel approach to performance testing, called virtual capture and replay (VCR), that couples capture and replay techniques with the checkpointing capabilities provided by the latest virtualization technologies. VCR enables software performance testers to automatically take a snapshot of a running system when certain critical conditions are verified, and to later replay the scenario that led to those conditions. Several in-depth analyses can be separately carried out in the laboratory just by rewinding the captured scenario and replaying it using different probes and analysis tools.", "num_citations": "5\n", "authors": ["322"]}
{"title": "Weighting influence of user behavior in software validation\n", "abstract": " Validation is an essential part of software development, and testing is a practical and widely used approach. The emerging methodology is model-based testing, in which test cases are derived from a model of software behaviour. In this paper we claim that also user behaviour should be taken into account for test planning purposes. We introduce a pragmatic approach called WSA, which derives the test cases from a state-based model, while also accounting for weights that consider relevance wrt user behaviour.", "num_citations": "5\n", "authors": ["322"]}
{"title": "Changing software in a changing world: How to test in presence of variability, adaptation and evolution?\n", "abstract": " Modern software-intensive and pervasive systems need to be able to manage different requirements of variability, adaptation and evolution. The latter are surely related properties, all bringing uncertainty, but covering different aspects and requiring different approaches. Testing of such systems introduces many challenges: variability would require the test of too many configurations and variants well beyond feasibility; adaptation should be based on context-aware testing over many predictable or even unpredictable scenarios; evolution would entail testing a system for which the reference model has become out-of-date. It is evident how current testing approaches are not adequate for such types of systems. We make a brief overview of testing challenges for changing software in a changing world, and hint at some promising approaches, arguing how these would need to be part of a holistic validation\u00a0\u2026", "num_citations": "4\n", "authors": ["322"]}
{"title": "CARS: Context Aware Reputation Systems to Evaluate Vehicles' Behaviour\n", "abstract": " The introduction of new generation ICT systems into vehicles makes them highly connected with the external World. As drawback, vehicle becomes potentially vulnerable to security attacks. Here, we consider a scenario in which Vehicular Networks and a Urban Network work together to realize a defence mechanism based on Reputation Systems. In this way, we are able to identify and isolate possible malicious vehicles acting that could send messages with the aim of reducing the availability of the network. We propose Context Aware Reputation Systems, CARS, able to identify insider attackers and isolate them taking into account contextual conditions derived from sensors spread along the entire urban network. Then, we experimentally evaluate CARS on a real data-set of mobility traces of taxis in Rome to compare the proposed systems with existing ones that do not consider contextual conditions. The\u00a0\u2026", "num_citations": "4\n", "authors": ["322"]}
{"title": "Monitoring of Learning Path for Business Process Models.\n", "abstract": " In modern society the employees of complex organizations are under pressure to constantly improve their knowledge and skills. Novel approaches and tools to support effective and efficient workplace learning in collaborative and engaging ways are needed. On the other hand, Business Process Management (BPM) is more and more employed to support and manage the complex processes carried out within organizations. BPM can be used as well to guide workplace learning, with the advantage of naturally aligning training tasks to real tasks. We introduce a specification of learning path that maps BPM tasks and activities into sequences of learning tasks. Our learning path specification can thus be used to both drive learning sessions carried out by simulation, and to inform a monitor that can assess learner\u2019s progress. The goal is to combine work and learning in natural and effective way and use available business monitoring techniques to monitor the learning progress of the learners. In the paper we describe our specification, the e-learning platform under development, and the approach to derive monitoring rules. The approach is illustrated through a simple motivational example.", "num_citations": "4\n", "authors": ["322"]}
{"title": "Towards automated robustness testing of BPEL orchestrators\n", "abstract": " Recently, a growing interest on robustness testing of services composition is emerging. In this paper, we propose a robustness testing approach of services composition by means of fault injection of the BPEL process. The proposed solution is implemented into the existing TASSA framework aimed at functional and non-functional testing of service applications. A case study is provided for assessing the effectiveness of the proposed approach.", "num_citations": "4\n", "authors": ["322"]}
{"title": "Automated refinement of dependability analysis through monitoring in dynamically connected systems\n", "abstract": " Model-based analysis is a well-established method to assess the dependability of a system before deployment. It is well known that, in highly dynamic contexts, the accuracy of the analysis results can be limited because unpredictable phenomena may affect the system during its operation. In such contexts, the analysis typically needs to be refined with data obtained from real system executions. In this paper we tackle the issue of refining model-based dependability analysis in automated systems through monitoring. Specifically, we report on our preliminary results on the development of a system that exploits the synergic use of an automated approach for model-based dependability analysis and a flexible monitoring architecture.", "num_citations": "4\n", "authors": ["322"]}
{"title": "Ipermob: Towards an information system to handle urban mobility data\n", "abstract": " This paper presents an innovative project in the domain of Intelligent Transportation Systems of the new generation. Following the directives coming from the European Commission and the Tuscan Regional Executive Board, IPERMOB encompasses a set of ICT proposals to control mobility and accessibility in diversified scenarios, notably those of urban areas. IPERMOB addresses the integration, optimization, and inter-operability of the chain formed by data collection systems; aggregation, management, and on-line control systems; off-line systems aiming at infrastructure planning; information systems targeted to citizen and municipality to control the vehicle mobility.", "num_citations": "4\n", "authors": ["322"]}
{"title": "Test suite reduction in good order: comparing heuristics from a new viewpoint\n", "abstract": " A new perspective in assessing test suite reduction techniques based on their rate of fault detection is introduced in this paper. This criterion, which is standard in assessing test-suite prioritization, has never been used for reduction. Our proposal stems from the consideration that under pressure testing could be stopped before all tests in the reduced test-suite are run, and in such cases the ordering in the reduced test-suite is also important. We compare four well-known reduction heuristics showing that by considering the rate of fault detection, the reduction technique to be chosen when time is an issue might be different from the one performing the best when testing can be completed.", "num_citations": "4\n", "authors": ["322"]}
{"title": "On-line validation of service oriented systems in the european project tas3\n", "abstract": " The European Project TAS 3  addresses the challenge of combining the openness, flexibility, dynamicity offered by service-oriented applications, together with privacy, security and reliability characteristics that are required when personal information is handled. In addition to appropriate authorization and authentication mechanisms, it is important to put in place appropriate validation procedures that can check the trustworthiness and dependability of services provided. We outline the Audition framework and discuss how such on-line validation approach can fit within the TAS 3  vision of a seamless connected world, where the final users remain in charge for releasing and administering their data.", "num_citations": "4\n", "authors": ["322"]}
{"title": "XModel-Based Testing of XSLT Applications.\n", "abstract": " Model-based testing is nowadays the emerging paradigm for software testing in many domains. In the Web arena XML Schema is becoming the technology of reference to describe data structure and applications input domains. The proposed tool (TAXI-Testing by Automatically generated XML Instances) exploits such a model to automatically derive correct XML instances applying the well-known Category-partition methodology. In this paper we introduce an improvement of TAXI to test XSLT Stylesheets. Indeed, with XSLT Stylesheets increasingly getting larger and more complex, their correctness becomes a crucial factor for software quality and hence we believe that they need careful validation. Two different case studies illustrate the approach to the validation of XML to XML and XML to XHTML transformations.", "num_citations": "4\n", "authors": ["322"]}
{"title": "Modeling and early performance estimation for Network Processor Applications\n", "abstract": " The design of modern embedded systems has to cope with quite challenging requirements in terms of flexibility, performance, and domain space exploration. To this purpose, we present a general methodology joining the principles of Platform Based Design and Model Driven Engineering. The former was especially conceived for embedded systems design, the latter focuses on models as the primary design artifacts. From their combination, we can to introduce a methodology for the design of Network Processor Applications. Starting from models described using the UML notation, we provide an early estimation of performance related parameters and compare in advance possible alternative implementations. In particular, the system behavior is specified by a collection of Sequence Diagrams describing the various usage scenarios, merged into an internal representation called Message Sequence Net. To\u00a0\u2026", "num_citations": "4\n", "authors": ["322"]}
{"title": "Cow Suite: A UML-based tool for test-suite planning and derivation\n", "abstract": " The tool execution starts by importing information on the UML design elements and organising it in a sort of hierarchy, whose root is represented by an Actor and leaves by Sequence or Communication Diagrams (see Figure 1). This hierarchy provides the user with a complete view of the status of the functionality specification and up-to-date documentation on Use Cases and their Realisation, Sequence and Communication Diagrams associated with each specification level, the reused nodes and those elements not linked with the other parts of the design.", "num_citations": "4\n", "authors": ["322"]}
{"title": "The ElasTest platform: supporting automation of end-to-end testing of large complex applications\n", "abstract": " Compounding both sides, developers know well that before they can release their software, testing is a must. Notwithstanding, it is often ignored or given low priority, why? Are the integrated systems tested enough at their ends to meet the user-demanded functionalities? Software code is written, transformed, and updated, then it is checked-in and verified before a new product is finally launched. But not always this translates into the best software solution or the best experience for users. Is it all about continuous integration process or are there more reasons? With the increasing need of distributed and more interconnected software systems, are developers ready to satisfy this demand?To answer these and even more questions (continue reading for a longer, and yet incomplete, list of difficult questions faced by testers and developers of distributed systems), a diverse group of researchers and industry practitioners, with a common vision, have been participating for the past year and a half in the ElasTest project. This white paper provides an overview of the project and its goals, while providing answers to industrial and research challenges.", "num_citations": "3\n", "authors": ["322"]}
{"title": "Software Testing and/or Software Monitoring: Differences and Commonalities\n", "abstract": " Validation is an essential part of the software life cycle. The actual functional and non-functional behaviour of an application needs to be checked against the expected or intended behaviour. Both testing and monitoring are widely used approaches for this purpose. More traditionally testing is considered as a technique for fault removal and forecasting during development. Monitoring is instead conceived for run-time observation of deployed software. Testing and monitoring approaches are usually contrasted as being, respectively, in-the-laboratory vs. in-the-field, and active vs. passive. In this talk I will overview concepts and techniques for software testing and monitoring, and will discuss how for modern pervasive and dynamic software systems the two approaches tend to converge in combined and synergic ways.", "num_citations": "3\n", "authors": ["322"]}
{"title": "Validation of access control systems\n", "abstract": " Access Control is among the most important security mechanisms to put in place in order to secure applications, and XACML is the de facto standard for defining access control policies. Due to the complexity of XACML language it is important to perform efficient testing to identify potential security flaws and bugs. However, in practice, exhaustive testing is impossible due to budget constraints. Test cases selection and prioritization are two well-known solutions to maximize the effectiveness of the test suite in terms of discovered faults, reducing as much as possible the required effort for tests execution and results analysis. In this chapter, after providing a survey on validation approaches for XACML based access control systems, we present a coverage based selection strategy and a similarity based test prioritization solution, both applied to XACML test cases. Then we compare the effectiveness of the two\u00a0\u2026", "num_citations": "3\n", "authors": ["322"]}
{"title": "An approach to adaptive dependability assessment in dynamic and evolving connected systems\n", "abstract": " Complexity, heterogeneity, interdependency and, especially, evolution of system/services specifications, related operating environments and user needs, are more and more highly relevant characteristics of modern and future software applications. Taking advantage of the experience gained in the context of the European project Connect, which addresses the challenging and ambitious topic of eternally functioning distributed and heterogeneous systems, this paper presents a framework to analyse and assess dependability and performance properties in dynamic and evolving contexts. The goal is to develop an adaptive approach by coupling stochastic model-based analysis, performed at design time to support the definition and implementation of software products complying with their stated dependability and performance requirements, with run-time monitoring to re-calibrate and enhance the dependability and\u00a0\u2026", "num_citations": "3\n", "authors": ["322"]}
{"title": "Towards ensuring eternal connectability\n", "abstract": " The aim of the CONNECT Project is to achieve universal interoperability between heterogeneous Networked Systems. For this, the non-functional properties required at each side of the connection going to be established must be fulfilled. By the inclusive term \"CONNECTability\" we comprehend properties belonging to all four non-functional concerns of interest for CONNECT, namely dependability, performance, security and trust. In this paper we focus on approaches for analysis and monitoring of dependability and performance of CONNECTed systems, and their combined usage.", "num_citations": "3\n", "authors": ["322"]}
{"title": "Partition testing from xml schema\n", "abstract": " Testing is a crucial part of the software development process, which strongly affects the quality and reliability of the delivered product. As it consumes a huge part of the effort required in software production, approaches that can effectively systematize and automate software testing are ceaselessly sought after both in industry and academy.It was already in the earliest days of the software testing discipline that the intuitive term of \u201cpartition testing\u201d has been coined (see, eg, Richardson and Clarke 1) to refer to a broad class of test criteria that systematically sample a program\u2019s input domain. The basic idea behind partition testing is that the input domain can be divided into subdomains, such that, for testing purposes, within each of them the program can be assumed to \u201cbehave the same\u201d, ie, for every point within a subdomain the program either succeeds or fails. This assumption is what allows for getting a finite set of tests out of the infinite execution domain, since, based on it, from observing its execution on one or few points within each subdomain, the program\u2019s behaviour over the whole domain can be (hypothetically) deduced.", "num_citations": "3\n", "authors": ["322"]}
{"title": "Guest editor's corner achieving quality in software\n", "abstract": " This special issue of the Journal of Systems and Sof~~~ e contains a selection of articles from the Second International Conference on Achieving Quality in Software (AQuIS \u2018931, held in Venice, Italy, on October 18-20, 1993. The focus is on methods and tools for achieving software quality. But which methods and tools among those used in software production actually achieve quality? We can count those methods and tools traditionally used in software quality assurance (SQA), ie, for ensuring that a (developing) software product and its process conform to prespecified standards. Thus, for example, product reviews or testing are typical activities in quality achievement. But they are not the only ones, despite what some people might believe. A wider and, in my opinion, more appropriate view is that software quality can be achieved by means of any method or tool for software development that proves effective in\u00a0\u2026", "num_citations": "3\n", "authors": ["322"]}
{"title": "An approach to efficient distributed transactions\n", "abstract": " Most distributed systems proposed on the basis of the concept of atomic action or transaction strongly limit parallelism, thus reducing their level of efficiency. In this paper, features of efficiency in a distributed transaction system are investigated.               Two mechanisms are proposed in order to enhance potential concurrency both among different transactions and within a single transaction during the commit phase:                                        - a synchronization mechanism has been designed which suggests an approach to concurrency control by allowing the release of acquired locks before transaction completion. The possibility of exploiting this mechanism to implement nested transactions is also discussed.                                                           - a distributed commit protocol is developed which enhances concurrency among the participants in an atomic action, thus achieving quick execution with high modularity\u00a0\u2026", "num_citations": "3\n", "authors": ["322"]}
{"title": "Digital persona portrayal: Identifying pluridentity vulnerabilities in digital life\n", "abstract": " The increasing use of the Internet for social purposes enriches the data available online about all of us and promotes the concept of the Digital Persona. Actually, most of us are represented online by more than one identity, what we define here as a Pluridentity. This trend brings increased risks: it is well known that the security of a Digital Persona can be exploited if its data and security are not effectively managed. In this paper, we focus specifically on a new type of digital attack that can be perpetrated by combining pieces of data belonging to one same Pluridentity in order to profile their target. Some victims can be so accurately depicted when looking at their Pluridentity that by using the gathered information attackers can execute very personalized social engineering attacks, or even bypass otherwise safe security mechanisms. We characterize these Pluridentity attacks as a security issue of a virtual System of\u00a0\u2026", "num_citations": "2\n", "authors": ["322"]}
{"title": "Addressing security properties in systems of systems: Challenges and ideas\n", "abstract": " Within growing pervasive information systems, Systems of Systems (SoS) emerge as a new research frontier. A SoS is formed by a set of constituent systems that live on their own with well-established functionalities and requirements, and, in certain circumstances, they must collaborate to achieve a common mission. In this scenario, security is one crucial property that needs to be considered since the early stages of SoS lifecycle. Unfortunately, SoS security cannot be guaranteed by addressing the security of each constituent system separately. The aim of this paper is to discuss the challenges faced in addressing the security of SoS and to propose some research ideas centered around the notion of a mission to be carried out by the SoS.", "num_citations": "2\n", "authors": ["322"]}
{"title": "Perceived needs and gains from an industrial study in cloud testing automation\n", "abstract": " Challenges, methods and tools for testing in the Cloud have been actively researched, however there is lack of evidence about the actual motivations, issues and gains for adoption of automated cloud testing technology in real world industrial contexts. In this paper we report our findings from an empirical study involving four quasi-experiments within different application domains, namely e-commerce, 5G networking, WebRTC and IoT. The study is part of the ElasTest validation strategy, aiming at assessing the impact of the ElasTest open source platform for end-to-end testing of large distributed systems.", "num_citations": "2\n", "authors": ["322"]}
{"title": "Guest editorial foreword for the special issue on automated software testing: trends and evidence\n", "abstract": " Society and industry are increasingly dependent on software. Software-supported environments can be critical, pervasive, persistent, mobile, distributed, real-time, context-aware, and adaptive. Thus, a growing need emerges for fast and rigorous approaches to develop and evolve these systems. This drives, among others, the research for techniques, criteria and supporting tools for software testing. Systematic and automated approaches have shown capable of reducing the overwhelming cost of engineering such systems. Industrial success cases have been openly reported and academic interest continues to grow as observed by the increasing number of researchers in the field. While there exist various trends on evolving the automation in software testing, the provision of sound empirical evidence is still needed on such area. It is our pleasure to open this special issue of the Journal of Software Engineering\u00a0\u2026", "num_citations": "2\n", "authors": ["322"]}
{"title": "What paper types are accepted at the international conference on software engineering?\n", "abstract": " With the aim of identifying good structures and examples for papers in the software engineering field, we conducted a study of the type of papers accepted along four decades in the Research Track of the International Conference on Software Engineering (ICSE). We used for this purpose a categorization scheme for Software Engineering papers that was obtained by merging, extending and revising a few existing paper scheme proposals. This paper summarizes some outcomes relative to what topics and problems are addressed, what types of contribution are presented and how they are validated. Insights from the study could help ICSE authors, reviewers and conference organizers in focusing and improving future efforts.", "num_citations": "2\n", "authors": ["322"]}
{"title": "Learn PAd: Collaborative and Model-based Learning in Public Administrations.\n", "abstract": " In modern society public administrations (PAs) are undergoing a transformation of their perceived role from controllers to proactive service providers. PAs are today under pressure to constantly improve the quality of delivered services, while coping with quickly changing context (changes in law and regulations, societal globalization, fast technology evolution) and decreasing budgets. As a result civil servants delivering such services to citizens are challenged to understand and put in action latest procedures and rules within tight time constraints. The European project Learn PAd copes with this transformation by proposing an e-learning platform that enables process-driven learning and fosters cooperation and knowledge-sharing. The platform supports both an informative learning approach, based on enriched business process (BP) models, and a procedural learning approach, based on simulation and monitoring, while relating them as well to learning objectives and key performance indicators.", "num_citations": "2\n", "authors": ["322"]}
{"title": "Deliverable D6. 4: Assessment report: Experimenting with CONNECT in Systems of Systems, and Mobile Environments\n", "abstract": " The core objective of WP6 is to evaluate the CONNECT technologies under realistic situations. To achieve this goal, WP6 concentrated a significant amount of its 4th year effort on the finalization of the implementation of the GMES scenario defined during the 3rd year. The GMES scenario allows the consortium to assess the validity of CONNECT claims and to investigate the exploitation of CONNECT technologies to deal with the integration of real systems. In particular, GMES requires the connection of highly heterogeneous and independently built systems provided by the industry partners. WP6 contributed also in providing mobile collaborative applications and case studies showing the exploitation of CONNECTORs on mobile devices.", "num_citations": "2\n", "authors": ["322"]}
{"title": "Complex Events Specification for Properties Validation\n", "abstract": " Run-time validation of non-functional properties becomes very important to evaluate and keep under control dynamic and evolving systems. Event-driven monitoring is a commonly adopted approach for observing and analyzing that these properties are satisfied. As the events to be observed become more and more complex, a powerful events specification language is needed. In this paper we present a complex events specification language that is included into the Property Meta-Model (PMM). It is intuitive and easy to use and at the same time machine-processable, thus allowing for the automated run-time configuration of a model-driven event-based monitoring system. The PMM complex events specification language combines features of two existing and well-known event specification languages that are GEM and Drools Fusion, and in addition presents new features not included in the considered languages\u00a0\u2026", "num_citations": "2\n", "authors": ["322"]}
{"title": "Metrics for QoS analysis in dynamic, evolving and heterogeneous CONNECTed systems\n", "abstract": " Dynamic, evolving systems pose new challenges from the point of view of Quality of Service (QoS) analysis, calling for techniques able to combine traditional offline methods with new ones applied at run-time. Tracking the evolution and updating the assessment consistently with such system evolution require not only advanced analysis methods, but also appropriate metrics well representative of QoS properties in the addressed context. The ongoing European project Connect addresses systems evolution, and aims at bridging technological gaps arising from heterogeneity of networked systems, by synthesising on-the-fly interoperability connectors. Moving from such ambitious goal, in this paper we present a metrics framework, whereby classical dependability/QoS metrics can be refined and combined to characterise Connect applications and to support their monitoring and analysis.", "num_citations": "2\n", "authors": ["322"]}
{"title": "Dependability in dynamic, evolving and heterogeneous systems: the CONNECT approach\n", "abstract": " The EU Future and Emerging Technologies (FET) Project Connect aims at dropping the heterogeneity barriers that prevent the eternality of networking systems through a revolutionary approach: to synthesise on-the-fly the Connectors via which networked systems communicate. The Connect approach, however, comes at risk from the standpoint of dependability, stressing the need for methods and tools that ensure resilience to faults, errors and malicious attacks of the dynamically Connected system. We are investigating a comprehensive approach, which combines dependability analysis, security enforcement and trust assessment, and is centred around a lightweight adaptive monitoring framework. In this project paper, we overview the research that we are undertaking towards this objective and propose a unifying workflow process that encompasses all the Connect dependability/security/trust concepts and models.", "num_citations": "2\n", "authors": ["322"]}
{"title": "Perspectives on data flow-based validation of web services compositions\n", "abstract": " \u2713 Control-based 1. T he user enters a date and a location 2. VT A sends am essage to FS w ith date and location 3. VTA gets a flight from FS or an em pty m essage 4. According to the answ er: 5. VT A sends am essage to HS w ith date and location 6. VTA gets a hotelfrom HS or an em pty m essage 7. According to the answ er: 8. VT A sends am essage to M ap w ith the hoteland airport location 9. VTA gets am ap from M ap", "num_citations": "2\n", "authors": ["322"]}
{"title": "KA Description of Software Testing V. 0.5\n", "abstract": " This is Version 0.5 of the Software Testing KA Description. I have done my best to provide-within the close deadline-a preliminary document according to the specifications. However, I am heavily relying on the feedback from the first review round to improve this draft document, as I am fully aware that it still needs more work than that I could realistically put on it. Above all, some of the topics certainly need more or better reference material: I have left some reference fields annotated with\"???\", for topics I would like to include, but for which I have not yet been able to find adequate reference material. On the other side, the number of references is already higher than the suggested size of 15. In this regard, note that by referencing books or survey articles that report on them, I have omitted to include some famous papers in the testing literature, in order to reduce the size of references list. For these matters, I warmly ask\u00a0\u2026", "num_citations": "2\n", "authors": ["322"]}
{"title": "A simple model to predict how many more failures will appear in testing\n", "abstract": " This paper deals with dynamic models to evaluate how many more failures will be observed in future tests, based on the failures observed so far. The assessment of reliability through testing is now one of the most mature fields in software engineering. There exist tens of reliability growth models, and several tools for applying them. The major assumptions of these models are that test cases are randomly drawn from the operational profile, and that as defects are found and removed, reliability will exhibit an increasing trend. Both assumptions are hardly satisfied in the first stages of the testing process or for the testing of small modules. Besides, there are not reasons why commonly used test methods at this time, such as specification-based testing or branch coverage, should exhibit a regular trend in reliability. These are the motivations for the work reported here. A dynamic model is introduced that can be applied to\u00a0\u2026", "num_citations": "2\n", "authors": ["322"]}
{"title": "Deriving path expressions recursively\n", "abstract": " Program representation plays an important role in software engineering, because it is used by the tools supporting software life cycle activities. To represent a program's control structure, the dominator tree and the implied tree, derived from the program's ddgraph, can be profitably used. In fact, thanks to their recursive structure, these trees are especially suitable for designing very simple and efficient algorithms for program path analysis, which is widely used in measurement and testing activities. In particular, this paper presents a recursive algorithm PE for computing path expressions from the dominator and the implied trees. The algorithm proposed is of interest to program comprehension for two reasons: representation of programs by path expressions is widely applied, e.g., to testing, data flow analysis and development of complexity metrics. An algorithm such as PE, which computes path expressions from\u00a0\u2026", "num_citations": "2\n", "authors": ["322"]}
{"title": "Software validation: A government-imposed challenge to the state-of-the-art in certification\n", "abstract": " Software certification can well be a problem when the products to be certified are: 1.i) implemented differently by different manufacturers;2.ii) only functionally equivalent for a subset of their outputs;3.iii) validated by a single, public institution.The paper presents a case of this type and describes the efforts made by the working group responsible for performing the validation task.", "num_citations": "2\n", "authors": ["322"]}
{"title": "What We Talk About When We Talk About Software Test Flakiness\n", "abstract": " Software test flakiness is drawing increasing interest among both academic researchers and practitioners. In this work we report our findings from a scoping review of white and grey literature, highlighting variations across flaky tests key concepts. Our study clearly indicates the need of a unifying definition as well as of a more comprehensive analysis for establishing a conceptual map that can better guide future research.", "num_citations": "1\n", "authors": ["322"]}
{"title": "Quality-of-Experience driven configuration of WebRTC services through automated testing\n", "abstract": " Quality of Experience (QoE) refers to the end users level of satisfaction with a real-time service, in particular in relation to its audio and video quality. Advances in WebRTC technology have favored the spread of multimedia services through use of any browser. Provision of adequate QoE in such services is of paramount importance. The assessment of QoE is costly and can be done only late in the service lifecycle. In this work we propose a simple approach for QoE-driven non-functional testing of WebRTC services that relies on the ElasTest open-source platform for end-to-end testing of large complex systems. We describe the ElasTest platform, the proposed approach and an experimental study. In this study, we compared qualitatively and quantitatively the effort required in the ElasTest supported scenario with respect to a \"traditional\" solution, showing great savings in terms of effort and time.", "num_citations": "1\n", "authors": ["322"]}
{"title": "EDUFYSoS: A factory of educational system of systems case studies\n", "abstract": " We propose a factory of educational System of Systems (SoS) case studies that can be used for evaluating SoS research results, in particular in SoS testing. The factory includes a first set of constituent systems that can collaborate within different SoS architectures to accomplish different missions. In the paper, we introduce three possible SoSs and outline their missions. For more detailed descriptions, diagrams and the source code, we refer to the online repository of EDUFYSoS. The factory is meant to provide an extensible playground, which we aim to grow to include more systems and other missions with the support of the community.", "num_citations": "1\n", "authors": ["322"]}
{"title": "Extending UML testing profile towards non-functional test modeling\n", "abstract": " The research community has broadly recognized the importance of the validation of non-functional properties including performance and dependability requirements. However, the results of a systematic survey we carried out evidenced the lack of a standard notation for designing non-functional test cases. For some time, the greatest attention of Model-Based Testing (MBT) research has focused on functional aspects. The only exception is represented by the UML Testing Profile (UML-TP) that is a lightweight extension of UML to support the design of testing artifacts, but it only provides limited support for non-functional testing. In this paper we provide a first attempt to extend UML-TP for improving the design of non-functional tests. The proposed extension deals with some important concepts of non-functional testing such as the workload and the global verdicts. As a proof of concept we show how the extended UML\u00a0\u2026", "num_citations": "1\n", "authors": ["322"]}
{"title": "Applying Structural Testing to Services Using Testing Interfaces and Metadata.\n", "abstract": " By their very nature, services are accessible only as black-boxes through their published interfaces. It is a well known issue that lack of implementation details may reduce service testability. In previous work, we proposed testable services as a solution to provide third-party services with structural coverage information after a test session, yet without revealing their internal details. However, integrators do not have enough information to improve their test set when they get a low coverage measure because they do not know which test requirements have not been covered. This paper proposes an approach in which testable services are provided along with test metadata that may help integrators to get a higher coverage. The approach is illustrated on a case study of a real system that uses orchestrations and testable services. A formal experiment designed to compare the proposed solution with a functional approach is also presented. The results show evidences that subjects using the testable service approach augmented with metadata can achieve better coverage than subjects using only a functional approach.", "num_citations": "1\n", "authors": ["322"]}
{"title": "Verification and analysis of autonomic systems for networked enterprises\n", "abstract": " Autonomic Computing is an innovative research area, that proposes self-management features for dynamic configuration, healing purpose, optimization and protection. Autonomic systems adapt themselves quickly to changes in the environment in which they operate, but, while this feature helps the automatic management of complex systems, it makes the job of validating and verifying them extremely difficult. In this chapter we point out the major challenges in validating such systems and we overview some proposals and methodologies for supporting their validation, analyzing in particular model checking techniques, testing methodologies, model-based dependability analysis and monitoring. Moreover we propose our model checking approach for verifying the autonomic workflow, developed for ArtDeco project and described in Chapter 7.", "num_citations": "1\n", "authors": ["322"]}
{"title": "Enhancing Trustworthiness within Service Federations by Continuous On-line Testing\n", "abstract": " Security, data protection, trust management, authentication and authorization are crucial assets in the Internet of Services. We propose and illustrate On-line Testing as an important means to enhance trustworthiness among federated services that are often independently developed, deployed, and maintained.", "num_citations": "1\n", "authors": ["322"]}
{"title": "Conceptual models for assessment & assurance of dependability, security and privacy in the eternal connected world\n", "abstract": " This is the first deliverable of WP5, which covers Conceptual Models for Assessment & Assurance of Dependability, Security and Privacy in the Eternal CONNECTed World. As described in the project DOW, in this document we cover the following topics: \u2022 Metrics definition \u2022 Identification of limitations of current V&V approaches and exploration of extensions/refinements/ new developments \u2022 Identification of security, privacy and trust models WP5 focus is on dependability concerning the peculiar aspects of the project, i.e., the threats deriving from on-the-fly synthesis of CONNECTors. We explore appropriate means for assessing/guaranteeing that the CONNECTed System yields acceptable levels for non-functional properties, such as reliability (e.g., the CONNECTor will ensure continued communication without interruption), security and privacy (e.g., the transactions do not disclose confidential data), trust (e.g., Networked Systems are put in communication only with parties they trust). After defining a conceptual framework for metrics definition, we present the approaches to dependability in CONNECT, which cover: i) Model-based V&V, ii) Security enforcement and iii) Trust management. The approaches are centered around monitoring, to allow for on-line analysis. Monitoring is performed alongside the functionalities of the CONNECTed System and is used to detect conditions that are deemed relevant by its clients (i.e., the other CONNECT Enablers). A unified lifecycle encompassing dependability analysis, security enforcement and trust management is outlined, spanning over discovery time, synthesis time and execution time.", "num_citations": "1\n", "authors": ["322"]}
{"title": "A meaningful bound for branch testing\n", "abstract": " Branch coverage is often used to evaluate testing thoroughness. For this reason, it is useful to set a lower bound on the number of test paths needed to achieve branch coverage when estimating how much effort will be needed to test a given program. McCabe\u2019s number [1] is often (mis) used as such a bound, whereas it actually provides the number of maximal linearly independent paths in the program flowgraph; this number is not related to branch coverage. On the other hand, the theoretical minimum number of paths needed to cover all the arcs in the program flowgraph[2] is not useful in practictx for instance, if a program consists of a loop with a very complex body, one path would be sufficient tQ cover all the arcs in its flowgraph. Evidently, no tester would rely on such a bound.The real problem in branch testing is to derive an executable set of test paths. It is intuitive that the lower the number of decisions in a\u00a0\u2026", "num_citations": "1\n", "authors": ["322"]}
{"title": "A systematic approach for integration testing of complex systems\n", "abstract": " For modular systems with hierarchical structure, approaches like topdown, bottom-up, and mixed strategies thereof, have traditionally been used for integration testing purposes. Such approaches are evidently no longer adequate for modern complex systems, which are made of many components (each, likely, a complex system itself) variously distributed and interconnecting. New strategies for arbitrary software architecture need to be identified.", "num_citations": "1\n", "authors": ["322"]}