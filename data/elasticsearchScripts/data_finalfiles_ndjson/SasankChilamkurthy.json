{"title": "Pytorch: An imperative style, high-performance deep learning library\n", "abstract": " Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.", "num_citations": "11239\n", "authors": ["833"]}
{"title": "Deep learning algorithms for detection of critical findings in head CT scans: a retrospective study\n", "abstract": " BackgroundNon-contrast head CT scan is the current standard for initial imaging of patients with head trauma or stroke symptoms. We aimed to develop and validate a set of deep learning algorithms for automated detection of the following key findings from these scans: intracranial haemorrhage and its types (ie, intraparenchymal, intraventricular, subdural, extradural, and subarachnoid); calvarial fractures; midline shift; and mass effect.MethodsWe retrospectively collected a dataset containing 313\u2008318 head CT scans together with their clinical reports from around 20 centres in India between Jan 1, 2011, and June 1, 2017. A randomly selected part of this dataset (Qure25k dataset) was used for validation and the rest was used to develop algorithms. An additional validation dataset (CQ500 dataset) was collected in two batches from centres that were different from those used for the development and Qure25k\u00a0\u2026", "num_citations": "385\n", "authors": ["833"]}
{"title": "2D-3D fully convolutional neural networks for cardiac MR segmentation\n", "abstract": " In this paper, we develop a 2D and 3D segmentation pipelines for fully automated cardiac MR image segmentation using Deep Convolutional Neural Networks (CNN). Our models are trained end-to-end from scratch using the ACD Challenge 2017 dataset comprising of 100 studies, each containing Cardiac MR images in End Diastole and End Systole phase. We show that both our segmentation models achieve near state-of-the-art performance scores in terms of distance metrics and have convincing accuracy in terms of clinical parameters. A comparative analysis is provided by introducing a novel dice loss function and its combination with cross entropy loss. By exploring different network structures and comprehensive experiments, we discuss several key insights to obtain optimal model performance, which also is central to the theme of this challenge.", "num_citations": "82\n", "authors": ["833"]}
{"title": "Development and validation of deep learning algorithms for detection of critical findings in head CT scans\n", "abstract": " Importance: Non-contrast head CT scan is the current standard for initial imaging of patients with head trauma or stroke symptoms. Objective: To develop and validate a set of deep learning algorithms for automated detection of following key findings from non-contrast head CT scans: intracranial hemorrhage (ICH) and its types, intraparenchymal (IPH), intraventricular (IVH), subdural (SDH), extradural (EDH) and subarachnoid (SAH) hemorrhages, calvarial fractures, midline shift and mass effect. Design and Settings: We retrospectively collected a dataset containing 313,318 head CT scans along with their clinical reports from various centers. A part of this dataset (Qure25k dataset) was used to validate and the rest to develop algorithms. Additionally, a dataset (CQ500 dataset) was collected from different centers in two batches B1 & B2 to clinically validate the algorithms. Main Outcomes and Measures: Original clinical radiology report and consensus of three independent radiologists were considered as gold standard for Qure25k and CQ500 datasets respectively. Area under receiver operating characteristics curve (AUC) for each finding was primarily used to evaluate the algorithms. Results: Qure25k dataset contained 21,095 scans (mean age 43.31; 42.87% female) while batches B1 and B2 of CQ500 dataset consisted of 214 (mean age 43.40; 43.92% female) and 277 (mean age 51.70; 30.31% female) scans respectively. On Qure25k dataset, the algorithms achieved AUCs of 0.9194, 0.8977, 0.9559, 0.9161, 0.9288 and 0.9044 for detecting ICH, IPH, IVH, SDH, EDH and SAH respectively. AUCs for the same on CQ500 dataset were 0.9419, 0\u00a0\u2026", "num_citations": "44\n", "authors": ["833"]}
{"title": "Parameter estimation of linear and quadratic chirps by employing the fractional fourier transform and a generalized time frequency transform\n", "abstract": " This paper is targeted towards a general readership in signal processing. It intends to provide a brief tutorial exposure to the Fractional Fourier Transform, followed by a report on experiments performed by the authors on a Generalized Time Frequency Transform (GTFT) proposed by them in an earlier paper. The paper also discusses the extension of the uncertainty principle to the GTFT. This paper discusses some analytical results of the GTFT. We identify the eigenfunctions and eigenvalues of the GTFT. The time shift property of the GTFT is discussed. The paper describes methods for estimation of parameters of individual chirp signals on receipt of a noisy mixture of chirps. A priori knowledge of the nature of chirp signals in the mixture \u2013 linear or quadratic is required, as the two proposed methods fall in the category of model-dependent methods for chirp parameter estimation.", "num_citations": "11\n", "authors": ["833"]}
{"title": "SkullBreak/SkullFix\u2013Dataset for automatic cranial implant design and a benchmark for volumetric shape learning tasks\n", "abstract": " The article introduces two complementary datasets intended for the development of data-driven solutions for cranial implant design, which remains to be a time-consuming and laborious task in current clinical routine of cranioplasty. The two datasets, referred to as the SkullBreak and SkullFix in this article, are both adapted from a public head CT collection CQ500 (http://headctstudy.qure.ai/dataset) with CC BY-NC-SA 4.0 license. The SkullBreak contains 114 and 20 complete skulls, each accompanied by five defective skulls and the corresponding cranial implants, for training and evaluation respectively. The SkullFix contains 100 triplets (complete skull, defective skull and the implant) for training and 110 triplets for evaluation. The SkullFix dataset was first used in the MICCAI 2020 AutoImplant Challenge (https://autoimplant.grand-challenge.org/) and the ground truth, i.e., the complete skulls and the implants in the\u00a0\u2026", "num_citations": "4\n", "authors": ["833"]}
{"title": "Morphology of the Brain: Changes in Ventricular and Cranial Vault Volumes in 15000 subjects with Aging and Hydrocephalus\n", "abstract": " Methods and ResultsTo train our deep learning model, lateral ventricles were manually annotated in 103 scans. We split these scans randomly with a ratio of 4: 1 for training and validation respectively. We trained a U-Net to segment lateral ventricles in each slice. Another U-Net model was trained to segment cranial vault using a similar process. Models were validated using DICE score metric versus the annotations.", "num_citations": "1\n", "authors": ["833"]}