{"title": "RT-level ITC'99 benchmarks and first ATPG results\n", "abstract": " New design flows require reducing work at the gate level and performing most activities before the synthesis step, including evaluation of testability of circuits. We propose a suite of RT-level benchmarks that help improve research in high-level ATPG tools. First results on the benchmarks obtained with our prototype tool show the feasibility of the approach.", "num_citations": "577\n", "authors": ["502"]}
{"title": "Soft-error detection using control flow assertions\n", "abstract": " Over the last few years, an increasing number of safety-critical tasks have been demanded of computer systems. In this paper, a software-based approach for developing safety-critical applications is analyzed. The technique is based on the introduction of additional executable assertions to check the correct execution of the program control flow. By applying the proposed technique, several benchmark applications have been hardened against transient errors. Fault injection campaigns have been performed to evaluate the fault detection capability of the proposed technique in comparison with state-of-the-art alternative assertion-based methods. Experimental results show that the proposed approach is far more effective than the other considered techniques in terms of fault detection capability, at the cost of a limited increase in memory requirements and in performance overhead.", "num_citations": "310\n", "authors": ["502"]}
{"title": "On the optimal design of triple modular redundancy logic for SRAM-based FPGAs\n", "abstract": " Triple modular redundancy (TMR) is a suitable fault tolerant technique for SRAM-based FPGA. However, one of the main challenges in achieving 100% robustness in designs protected by TMR running on programmable platforms is to prevent upsets in the routing from provoking undesirable connections between signals from distinct redundant logic parts, which can generate an error in the output. This paper investigates the optimal design of the TMR logic (e.g., by cleverly inserting voters) to ensure robustness. Four different versions of a TMR digital filter were analyzed by fault injection. Faults were randomly inserted straight into the bitstream of the FPGA. The experimental results presented in this paper demonstrate that the number and placement of voters in the TMR design can directly affect the fault tolerance, ranging from 4.03% to 0.98% the number of upsets in the routing able to cause an error in the TMR circuit.", "num_citations": "289\n", "authors": ["502"]}
{"title": "Microprocessor software-based self-testing\n", "abstract": " This article discusses the potential role of software-based self-testing in the microprocessor test and validation process, as well as its supplementary role in other classic functional- and structural-test methods. In addition, the article proposes a taxonomy for different SBST methodologies according to their test program development philosophy, and summarizes research approaches based on SBST techniques for optimizing other key aspects.", "num_citations": "279\n", "authors": ["502"]}
{"title": "Soft-error detection through software fault-tolerance techniques\n", "abstract": " The paper describes a systematic approach for automatically introducing data and code redundancy into an existing program written using a high-level language. The transformations aim at making the program able to detect most of the soft-errors affecting data and code, independently of the Error Detection Mechanisms (EDMs) possibly implemented by the hardware. Since the transformations can be automatically applied as a pre-compilation phase, the programmer is freed from the cost and responsibility of introducing suitable EDMs in its code. Preliminary experimental results are reported, showing the fault coverage obtained by the method, as well as some figures concerning the slow-down and code size increase it causes.", "num_citations": "220\n", "authors": ["502"]}
{"title": "Software-implemented hardware fault tolerance\n", "abstract": " Software-Implemented Hardware Fault Tolerance addresses the innovative topic of software-implemented hardware fault tolerance (SIHFT), ie, how to deal with faults affecting the hardware by only (or mainly) acting on the software. The first SIHFT techniques were proposed and adopted several decades ago, but they have been the object of new interest in the past few years, mainly due to the need for developing low-cost safety-critical computer-based applications in fields such as automotive, biomedics, and telecommunications. Therefore, several new approaches to detect, and when possible correct, transient and permanent faults in the hardware have been recently proposed. These approaches are innovative (with respect to those proposed in the past) since they are of higher applicability (often starting from the source-level code of an application) and generality, being capable of coping with many different fault types. The book presents the theory behind software-implemented hardware fault tolerance, as well as the practical aspects related to put it at work on real examples. By evaluating accurately the advantages and disadvantages of the already available approaches, the book provides a guide to developers willing to adopt software-implemented hardware fault tolerance in their applications. Moreover, the book identifies open issues for researchers willing to improve the already available techniques.", "num_citations": "186\n", "authors": ["502"]}
{"title": "Automatic test program generation: a case study\n", "abstract": " Design validation is a critical step in the development of present-day microprocessors, and some authors suggest that up to 60% of the design cost is attributable to this activity. Of the numerous activities performed in different stages of the design flow and at different levels of abstraction, we focus on simulation-based design validation performed at the behavioral register-transfer level. Designers typically write assertions inside hardware description language (HDL) models and run extensive simulations to increase confidence in device correctness. Simulation results can also be useful in comparing the HDL model against higher-level references or instruction set simulators. Microprocessor validation has become more difficult since the adoption of pipelined architectures, mainly because you can't evaluate the behavior of a pipelined microprocessor by considering one instruction at a time; a pipeline's behavior\u00a0\u2026", "num_citations": "181\n", "authors": ["502"]}
{"title": "GATTO: A genetic algorithm for automatic test pattern generation for large synchronous sequential circuits\n", "abstract": " This paper deals with automated test pattern generation for large synchronous sequential circuits and describes an approach based on genetic algorithms. A prototype system named GATTO is used to assess the effectiveness of the approach in terms of result quality and CPU time requirements. An account is also given of a distributed version of the same algorithm, named GATTO*. Being based on the PVM library, it runs on any network of workstations and is able to either reduce the required time, or improve the result quality with respect to the monoprocessor version. In the latter case, in terms of Fault Coverage, the results are the best ones reported in the literature for most of the largest standard benchmark circuits. The flexibility of GATTO enables users to easily tradeoff fault coverage and CPU time to suit their needs.", "num_citations": "170\n", "authors": ["502"]}
{"title": "Identification and classification of single-event upsets in the configuration memory of SRAM-based FPGAs\n", "abstract": " This paper presents the radiation testing of a commercial-off-the-shelf SRAM-based field-programmable gate arrays (FPGAs) with heavy ions. Test experiments have been conducted to identify and to classify the single-event upsets (SEUs) in the configuration memory that induce single-event functional interrupt for the user-implemented circuit. Moreover the paper presents a new approach for assessing the effects of SEUs based on the combination of radiation testing and simulation-based fault injection tool. First experimental results show the FPGA look-up table (LUT) resources (used to implement combinatorial logic) are the most sensitive to SEUs, whereas interconnect resources are the most critical for the device cross section because they use the largest number of configuration bits. The analysis of experimental data underlines that the most probable error affecting interconnections is the shorting of two nets\u00a0\u2026", "num_citations": "160\n", "authors": ["502"]}
{"title": "Low power BIST via non-linear hybrid cellular automata\n", "abstract": " In the last decade, researchers devoted much effort to reduce the average power consumption in VLSI systems during normal operation mode, while power consumption during test operation mode was usually neglected. However, during test application, circuits are subjected to an activity level higher than the normal one: the extra power consumption due to test application may thus cause severe hazards to circuit reliability. Moreover, it can dramatically shorten battery life when periodic testing of battery-powered systems is considered. In this paper we propose an algorithm to design a test pattern generator based on cellular automata for testing combinational circuits that effectively reduces power consumption while attaining high fault coverage. Experimental results show that our approach reduces the power consumed during test by 34% on average, without affecting fault coverage, test length and area overhead.", "num_citations": "160\n", "authors": ["502"]}
{"title": "Experimentally evaluating an automatic approach for generating safety-critical software with respect to transient errors\n", "abstract": " This paper deals with a software modification strategy allowing on-line detection of transient errors. Being based on a set of rules for introducing redundancy in the high-level code, the method can be completely automated, and is therefore particularly suited for low-cost safety-critical microprocessor-based applications. Experimental results are presented and discussed, demonstrating the effectiveness of the approach in terms of fault detection capabilities.", "num_citations": "138\n", "authors": ["502"]}
{"title": "A test pattern generation methodology for low power consumption\n", "abstract": " This paper proposes an ATPG technique that reduces power dissipation during the test of sequential circuits. The proposed approach exploits some redundancy introduced during the test pattern generation phase and selects a subset of sequences able to reduce the consumed power without reducing the fault coverage. The method is composed of three independent steps: redundant test pattern generation, power consumption measurement, optimal test sequence selection. The experimental results gathered on the ISCAS benchmark circuits show that our approach decreases the power consumption by 70% on average with respect to the original test pattern, generated ignoring the heat dissipation problem.", "num_citations": "138\n", "authors": ["502"]}
{"title": "A source-to-source compiler for generating dependable software\n", "abstract": " Over the last years, an increasing number of safety-critical tasks have been demanded for computer systems. In particular, safety-critical computer-based applications are hitting market areas where cost is a major issue, and thus solutions are required which conjugate fault tolerance with low costs. A source-to-source compiler supporting a software-implemented hardware fault tolerance approach is proposed, based on a set of source code transformation rules. The proposed approach hardens a program against transient memory errors by introducing software redundancy: every computation is performed twice and results are compared, and control flow invariants are checked explicitly. By exploiting the tool's capabilities, several benchmark applications have been hardened against transient errors. Fault injection campaigns have been performed to evaluate the fault detection capability of the hardened applications\u00a0\u2026", "num_citations": "135\n", "authors": ["502"]}
{"title": "A diagnostic test pattern generation algorithm\n", "abstract": " The authors present a novel ATPG (automatic test pattern generation) algorithm, based on PODEM, that makes diagnostic test pattern generation feasible for medium-sized combinational circuits described at the gate level with the single-stuck-at-fault assumption. The input to the ATPG is a couple of faults, and either the output is a test pattern that distinguishes them or they are tagged as indistinguishable. The need to consider the fault-free circuit and the two faulty circuits at the same time required the extension of the algebra to encompass two additional values, Delta and delta . A Delta appears on the nodes of the circuit whenever a difference between the two faulty circuits exists. The presence of a delta marks the locations where a difference might exist if the X values on one or both faulty circuits were suitably set. The algorithm excites and propagates Delta s onto the primary outputs and is thus called the Delta\u00a0\u2026", "num_citations": "134\n", "authors": ["502"]}
{"title": "Fully automatic test program generation for microprocessor cores\n", "abstract": " Microprocessor cores are a major challenge in the test arena: not only is their complexity always increasing, but also their specific characteristics intensify all difficulties. A microprocessor embedded inside a SoC is even harder to test since its input might be harder to control and its behavior may be harder to observe. Functional testing is an effective solution which consists in forcing the microprocessor to execute a suitable test program. This paper presents a new approach to automatic test program generation exploiting an evolutionary paradigm. It overcomes the main limitations of previous methodologies and provides significantly better results. Human intervention is limited to the enumeration of all assembly instructions. Also internal parameters of the optimizer are auto-adapted Experimental results show the effectiveness of the approach.", "num_citations": "131\n", "authors": ["502"]}
{"title": "Evaluating the effects of SEUs affecting the configuration memory of an SRAM-based FPGA\n", "abstract": " This paper analyses the effects of single event upsets in an SRAM-based FPGA, with special emphasis for the transient faults affecting the configuration memory. Two approaches are combined: from one side, by exploiting the available information and tools dealing with the device configuration memory, we were able to make hypothesis on the meaning of every bit in the configuration memory. From the other side, radiation testing was exploited to validate the hypothesis and to gather experimental evidence about the correctness of the obtained results. As a major result, we can provide detailed information about the effects of SEUs affecting the configuration memory of a commercial FPGA device. As a second contribution, we describe a method for obtaining the same result with similar devices. Finally, the obtained results are crucial to allow the possible usage of SRAM-based FPGAs in safety-critical environments\u00a0\u2026", "num_citations": "124\n", "authors": ["502"]}
{"title": "An FPGA-based approach for speeding-up fault injection campaigns on safety-critical circuits\n", "abstract": " In this paper we describe an FPGA-based approach to speed-up fault injection campaigns for the evaluation of the fault-tolerance of VLSI circuits. Suitable techniques are proposed, allowing emulating the effects of faults and observing faulty behavior. The proposed approach combines the efficiency of hardware-based techniques, and the flexibility of simulation-based techniques. Experimental results are provided showing that significant speed-up figures can be achieved with respect to state-of-the-art simulation-based fault injection techniques.", "num_citations": "122\n", "authors": ["502"]}
{"title": "An automatic test pattern generator for large sequential circuits based on genetic algorithms\n", "abstract": " This paper is concerned with the question of automated test pattern generation for large synchronous sequential circuits and describes an approach based on Genetic Algorithms suitable for even the largest benchmark circuits, together with a prototype system named GATTO. Its effectiveness (in terms of result quality and CPU time requirements) for circuits previously unmanageable is illustrated. The flexibility of the new approach enables users to easily trade off fault coverage and CPU time to suit their needs.", "num_citations": "121\n", "authors": ["502"]}
{"title": "Exploiting circuit emulation for fast hardness evaluation\n", "abstract": " Hardware designers need effective techniques for early evaluation of the hardening mechanisms adopted in safety-critical VLSI circuits. We propose field-programmable gate-array based circuit emulation for performing fault-injection campaigns. Experimental results show that the new technique is about four orders of magnitude faster than simulation-based fault injection.", "num_citations": "114\n", "authors": ["502"]}
{"title": "On the test of microprocessor IP cores\n", "abstract": " Testing is a crucial issue in SOC development and production process. A popular solution for SOCs that include microprocessor cores is based on making them execute a test program. Thus, implementing a very attractive BIST solution. This paper describes a method for the generation of effective programs for the self-test of a processor. The method can be partially automated and combines ideas from traditional functional approaches and from the ATPG field. We assess the feasibility and effectiveness of the method by applying it to a 8051 core.", "num_citations": "111\n", "authors": ["502"]}
{"title": "Testability analysis and ATPG on behavioral RT-level VHDL\n", "abstract": " This paper proposes an environment to address testability analysis and test pattern generation on VHDL descriptions at the RT-level. The proposed approach, based on a suitable fault model and an ATPG algorithm, is experimentally shown to provide a good estimate of the final gate-level fault coverage, and to give test patterns with excellent fault coverage properties. The approach, being based on an abstract representation, is particularly suited for large circuits, where gate-level ATPGs are often inefficient.", "num_citations": "105\n", "authors": ["502"]}
{"title": "New techniques for speeding-up fault-injection campaigns\n", "abstract": " Fault-tolerant circuits are currently required in several major application sectors, and a new generation of CAD tools is required to automate the insertion and validation of fault-tolerant mechanisms. This paper outlines the characteristics of a new fault-injection platform and its evaluation in a real industrial environment. The fault-injection platform is mainly used for assessing the correctness and effectiveness of the fault tolerance mechanisms implemented within ASIC and FPGA designs. The platform works on register transfer-level VHDL descriptions which are then synthesized, and is based on commercial tools for VHDL parsing and simulation. It also details techniques devised and implemented within the platform to speed-up fault-injection campaigns. Experimental results are provided, showing the effects of the different techniques, and demonstrating that they are able to reduce the total time required by fault\u00a0\u2026", "num_citations": "102\n", "authors": ["502"]}
{"title": "A new hybrid fault detection technique for systems-on-a-chip\n", "abstract": " Hardening SoCs against transient faults requires new techniques able to combine high fault detection capabilities with the usual requirements of SoC design flow, e.g., reduced design-time, low area overhead, and reduced (or null) accessibility to source core descriptions. This paper proposes a new hybrid approach which combines hardening software transformations with the introduction of an Infrastructure IP with reduced memory and performance overheads. The proposed approach targets faults affecting the memory elements storing both the code and the data, independently of their location (inside or outside the processor). Extensive experimental results, including comparisons with previous approaches, are reported, which allow practically evaluating the characteristics of the method in terms of fault detection capabilities and area, memory, and performance overheads.", "num_citations": "101\n", "authors": ["502"]}
{"title": "An accurate analysis of the effects of soft errors in the instruction and data caches of a pipelined microprocessor\n", "abstract": " Instruction and data caches are well known architectural solutions that allow significant improvement in the performance of high-end processors. Due to their sensitivity to soft errors, they are often disabled in safety critical applications, thus sacrificing performance for improved dependability. In this paper, we report an accurate analysis of the effects of soft errors in the instruction and data caches of a soft core implementing the SPARC architecture. Thanks to an efficient simulation-based fault injection environment we developed, we are able to present in this paper an extensive analysis of the effects of soft errors on a processor running several applications under different memory configurations. The procedure we followed allows the precise computation of the processor failure rate when the cache is enabled even without resorting to expensive radiation experiments.", "num_citations": "93\n", "authors": ["502"]}
{"title": "Fast sequential circuit test generation using high-level and gate-level techniques\n", "abstract": " A new approach for sequential circuit test generation is proposed that combines software based testing techniques at the high level with test enhancement techniques at the gate level. Several sequences are derived to ensure 100% coverage of all statements in a high-level VHDL description, or to maximize coverage of paths. The sequences are then enhanced at the gate level to maximize coverage of single stuck-at faults. High fault coverages have been achieved very quickly on several benchmark circuits using this approach.", "num_citations": "89\n", "authors": ["502"]}
{"title": "GALLO: A genetic algorithm for floorplan area optimization\n", "abstract": " The paper describes a Genetic Algorithm for the Floorplan Area Optimization problem. The algorithm is based on suitable techniques for solution encoding and evaluation function definition, effective cross-over and mutation operators, and heuristic operators which further improve the method's effectiveness. An adaptive approach automatically provides the optimal values for the activation probabilities of the operators. Experimental results show that the proposed method is competitive with the most effective ones as far as the CPU time requirements and the result accuracy is considered, but it also presents some advantages. It requires a limited amount of memory, it is not sensible to special structures which are critical for other methods, and has a complexity which grows linearly with the number of implementations. Finally, we demonstrate that the method is able to handle floorplans much larger (in terms of number\u00a0\u2026", "num_citations": "89\n", "authors": ["502"]}
{"title": "Using benchmarks for radiation testing of microprocessors and FPGAs\n", "abstract": " Performance benchmarks have been used over the years to compare different systems. These benchmarks can be useful for researchers trying to determine how changes to the technology, architecture, or compiler affect the system's performance. No such standard exists for systems deployed into high radiation environments, making it difficult to assess whether changes in the fabrication process, circuitry, architecture, or software affect reliability or radiation sensitivity. In this paper, we propose a benchmark suite for high-reliability systems that is designed for field-programmable gate arrays and microprocessors. We describe the development process and report neutron test data for the hardware and software benchmarks.", "num_citations": "78\n", "authors": ["502"]}
{"title": "The selfish gene algorithm: a new evolutionary optimization strategy\n", "abstract": " This paper proposes a new general approach for optimization algorithms in the Evolutionary Computation field. The approach is inspired by the Selfish Gene theory, an interpretation of the Darwinian theory given by the biologist Richard Dawkins, in which the basic element of evolution is the gene, rather than the individual. The paper defines the Selfish Gene Algorithm, which implements such a view of the evolution mechanism. We tested the approach by implementing a Selfish Gene Algorithm on a case study, and we found better results than those provided by a Genetic Algorithm on the same problem and with the same fitness function.", "num_citations": "74\n", "authors": ["502"]}
{"title": "On the evaluation of SEU sensitiveness in SRAM-based FPGAs\n", "abstract": " The growing adoption of SRAM-based field programmable gate arrays (FPGAs) in safety-critical applications demands for efficient methodologies for evaluating their reliability. Single event upsets (SEUs) affecting the configuration memory of SRAM-based FPGAs are a major concern, since they can permanently affect the function implemented by the device. We exploited a fault-injection environment developed at our institution to analyze the impact of such faults on SRAM-based FPGAs when fault tolerant design techniques are adopted. The experimental results allow quantitative evaluations of the effects of these faults, and show that the sensitivity of the TMR design technique mainly depends on the characteristics of the adopted TMR architecture in terms of placing and routing.", "num_citations": "73\n", "authors": ["502"]}
{"title": "New static compaction techniques of test sequences for sequential circuits\n", "abstract": " This paper describes an algorithm for compacting the Test Sequences generated by an ATPG tool without reducing the number of faults they detect. The algorithm is based on re-ordering the sequences so that some of them can be shortened and some others eliminated. The problem is NP-complete, and we adopt Genetic Algorithms to obtain optimal solutions with acceptable computational requirements. As it requires just one preliminary Fault Simulation experiment, the approach is much more efficient than others proposed before; experimental results gathered with Test Sets generated by different ATPG tools show that the method is able to reduce the size of the Test Set by a factor varying between 50% and 62%.", "num_citations": "73\n", "authors": ["502"]}
{"title": "New techniques for accelerating fault injection in VHDL descriptions\n", "abstract": " Simulation-based fault injection in VHDL descriptions is increasingly common due to the popularity of top-down design flows exploiting this language. However, the large CPU time required to perform VHDL simulations often represents a major drawback stemming from the adoption of this method. This paper presents some techniques for reducing the time to perform the fault injection experiments. Static and dynamic methods are proposed to analyze the list of faults to be injected, and for removing faults as soon as their behaviour is known. Common features available in most VHDL simulation environments are also exploited. Experimental results show that the proposed techniques are able to reduce the time required by a typical fault injection campaign by a factor ranging from 51% to 96%.", "num_citations": "72\n", "authors": ["502"]}
{"title": "Exploiting FPGA for accelerating fault injection experiments\n", "abstract": " The widespread adoption of VLSI devices for safety-critical applications asks for effective tools for the evaluation and validation of their reliability. Fault injection is commonly adopted for this task, and the effectiveness of the adopted techniques is therefore a key factor for the reliability of the final products. In this paper we present new techniques for exploiting FPGAs to speed-up fault injection in VLSI circuits. Thanks to the suitable circuitry added to the original circuit, transient faults affecting memory elements in the circuit can be considered. The proposed approach allows performing fault injection campaigns that are comparable to those performed with hardware-based techniques in terms of speed, but shows a much higher flexibility in terms of supported fault models.", "num_citations": "71\n", "authors": ["502"]}
{"title": "Diagnosis oriented test pattern generation\n", "abstract": " This paper addresses the generation of test patterns having diagnostic properties. The authors goal is to produce patterns able not only to detect, but also to distinguish faults in combinational circuits. A general formalization of the problem is first given; a new technique is then introduced to improve the diagnostic capabilities of a traditional automatic test pattern generation (ATPG); the experimental results showing its effectiveness are finally presented.<>", "num_citations": "71\n", "authors": ["502"]}
{"title": "A new BIST architecture for low power circuits\n", "abstract": " In the last decade, researchers have devoted increasing efforts to reduce the average power consumption in VLSI systems during normal operation mode. During test application the circuits are subject to an activity higher than in the normal mode: the extra power consumption due to test application can rise severe hazards to circuit reliability. Moreover, it can dramatically shorten the battery life when on-line testing is considered. In this paper we propose a low power BIST architecture inspired by the precomputation architecture. Experimental results show that our approach can achieve an average power reduction ranging from 31% to 95% without affecting the quality of the test. The new architecture can be easily integrated into an existing design flow and is barely invasive with respect to the original BIST circuit.", "num_citations": "70\n", "authors": ["502"]}
{"title": "Exploiting FPGA-based techniques for fault injection campaigns on VLSI circuits\n", "abstract": " Proposes an FPGA-based system to speed-up fault injection campaigns for the evaluation of the fault-tolerant capabilities of VLSI circuits. An environment is described, relying on FPGA-based emulation of the circuit. Suitable techniques are described, allowing one to emulate the effects of faults and to observe faulty behavior. The proposed approach allows the combination of the speed of hardware-based techniques, and the flexibility of simulation-based techniques. Experimental results are provided showing that significant speed-up figures with respect to state-of-the-art simulation-based techniques can be achieved.", "num_citations": "69\n", "authors": ["502"]}
{"title": "Advanced techniques for GA-based sequential ATPGs\n", "abstract": " Genetic Algorithms (GAs) have been recently investigated as an efficient approach to test generation for synchronous sequential circuits. In this paper we propose a set of techniques which significantly improves the performance of the GA-based ATPG algorithm proposed by Prinetto et al. (1994): in particular, the new techniques enhance the capability of the algorithm in terms of test length minimization and fault excitation. We report some experimental results gathered with a prototypical tool and show that a well-tuned GA-based ATPG is generally superior to both symbolic and topological ones in terms of achieved fault coverage and required CPU time.", "num_citations": "69\n", "authors": ["502"]}
{"title": "Automatic test bench generation for validation of RT-level descriptions: an industrial experience\n", "abstract": " In current microprocessors and systems, an increasingly high silicon portion is derived through automatic synthesis, with designers working exclusively at the RT-level, and design productivity is greatly enhanced. However, in the new design flow, validation still remains a challenge: while new technologies based on formal verification are only marginally accepted, standard techniques based on simulation are beginning to fall behind the increased circuit complexity. This paper proposes a new approach to simulation-based validation, in which a Genetic Algorithm helps the designer in generating useful input sequences to be included in the test bench. The technique has been applied to an industrial circuit, showing that the quality of the validation process is increased.", "num_citations": "66\n", "authors": ["502"]}
{"title": "Deformation properties of osmium, platinum, mercury isotopes from self-consistent calculations: Influence of the pairing treatment\n", "abstract": " The deformation properties of several isotopes of the elements Os, Pt and Hg have been computed by means of Hartree-Fock plus BCS calculations. The Hartree-Fock potential has been derived from the Skyrme interaction SIII. Two approximations have been used for the treatment of pairing correlations: the constant (versus deformation) gap method and the constant (versus deformation) pairing matrix element method. A good agreement with experimental data is obtained for ground state deformation properties except for the exact location of the prolate-oblate transition as a function of the neutron number. For one nucleus 184Hg, the pairing matrix elements have been calculated from the Gogny interaction D1, in order to study their single-particle state \u2014 and deformation \u2014 dependence. From these results, the validity of the two approximations used for pairing correlations is discussed.", "num_citations": "65\n", "authors": ["502"]}
{"title": "Industrial BIST of embedded RAMs\n", "abstract": " High-quality memory testing is increasingly important, especially when RAMs and ROMs are deeply embedded in bigger systems, as the techniques based on control and observation points fail. Adopting a built-in self-test scheme for deeply embedded memories seems advantageous and industrial experience at Italtel, a telecom company, confirms it. The scheme implements in hardware the test pattern generation algorithm proposed by R. Nair, SM Thatte, and JA Abraham\\cite {NTAb78}, extending it to word-based memories. Area overhead, performance degradation, additional pins, and test time are minimal, whereas we guarantee high fault coverage for the significant failure modes and full testability of the BIST hardware, as the experimental results confirm.", "num_citations": "63\n", "authors": ["502"]}
{"title": "Simulation-based analysis of SEU effects in SRAM-based FPGAs\n", "abstract": " SRAM-based field programmable gate arrays (FPGAs) are particularly sensitive to single event upsets (SEUs) that, by changing the FPGA's configuration memory, may affect dramatically the functions implemented by the device. In This work we describe a new approach for predicting SEU effects in circuits mapped on SRAM-based FPGAs that combines radiation testing with simulation. The former is used to characterize (in terms of device cross section) the technology on which the FPGA device is based, no matter which circuit it implements. The latter is used to predict the probability for a SEU to alter the expect behavior of a given circuit. By combining the two figures, we then compute the cross section of the circuit mapped on the pre-characterized device. Experimental results are presented that compare the approach we developed with a traditional one based on radiation testing only, to measure the cross section\u00a0\u2026", "num_citations": "62\n", "authors": ["502"]}
{"title": "A new software-based technique for low-cost fault-tolerant application\n", "abstract": " A new software approach providing fault detection and correction capabilities by using software techniques is described. The approach is suitable for developing commercial-off-the-shelf processor-based architectures for safety-critical applications. Data and code duplications are exploited to provide fault detection and correction capabilities. Preliminary results coming from fault injection experiments support the effectiveness of the method.", "num_citations": "61\n", "authors": ["502"]}
{"title": "GARDA: A diagnostic ATPG for large synchronous sequential circuits\n", "abstract": " The paper deals with automated generation of diagnostic test sequences for synchronous sequential circuits. An algorithm is proposed, named GARDA, which is suitable to produce good results with acceptable CPU time and memory requirements even for the largest benchmark circuits. The algorithm is based on Genetic Algorithms, and experimental results are provided which demonstrate the effectiveness of the approach.< >", "num_citations": "58\n", "authors": ["502"]}
{"title": "An RT-level fault model with high gate level correlation\n", "abstract": " With the advent of new RT-level design and test flows, new tools are needed to migrate at the RT-level the activities of fault simulation testability analysis, and test pattern generation. This paper focuses on fault simulation at the RT-level, and aims at exploiting the capabilities of VHDL simulators to compute faulty responses. The simulator was implemented as a phototypical tool, and experimental results show that simulation of a faulty circuit is no more costly than simulation of the original circuit. The reliability of the fault coverage figures computed at the RT-level is increased thanks to an analysis of inherent VHDL redundancies, and by foreseeing classical synthesis optimizations. A set of \"rules\" is used to compute a fault list that exhibits good correlation with stuck-at faults.", "num_citations": "56\n", "authors": ["502"]}
{"title": "High-level observability for effective high-level ATPG\n", "abstract": " This paper focuses on observability, one of the open issues in high-level test generation. Three different approximate metrics for taking observability into account during RT-level ATPG are presented. Metrics range from a really naive and optimistic one to more sophisticated analysis. Metrics are evaluated including them in the calculation of the fitness function used in a RT-level ATPG. Advantages and disadvantages are illustrated. Experimental results show how sharp observability metrics are crucial for making effective RT-level ATPG possible: test sequences generated at RT-level outperform commercial gate-level ATPGs on some ITC99 benchmark circuits.", "num_citations": "54\n", "authors": ["502"]}
{"title": "Exploiting Programmable BIST For The Diagnosis of Embedded Memory Cores.\n", "abstract": " This paper addresses the issue of testing and diagnosing a memory core embedded in a complex SOC. The proposed solution is based on a P1500-compliant wrapper that follows a programmable BIST approach and is able to support both testing and diagnosis. Experimental results are provided allowing to evaluate the benefits and limitations of the adopted solution and to compare it with previously proposed ones. The solution takes into account several constraints existing in an industrial environment, such as minimizing the cost of test development, easing the reuse of the available architectures for test and diagnosis of different memory types and minimizing the cost of the external ATE.", "num_citations": "53\n", "authors": ["502"]}
{"title": "Multiple errors produced by single upsets in FPGA configuration memory: a possible solution\n", "abstract": " The very high integration levels reached by SRAM-based field programmable gate arrays (FPGAs) lead to high occurrence rate of single event upsets (SEUs) in their configuration memory, which can produce multiple errors affecting routing resources. Based on detailed analysis of this phenomenon, we devised a reliability-oriented place and route algorithm able to significantly improve the reliability of SRAM-based FPGAs with limited costs in terms of performance degradation and resource occupation. To evaluate the effectiveness of the algorithm we performed extensive fault injection experiments.", "num_citations": "50\n", "authors": ["502"]}
{"title": "Simulation-based analysis of SEU effects on SRAM-based FPGAs\n", "abstract": " Commercial-Off-The-Shelf SRAM-based FPGA devices are becoming of interests for applications where high dependability and low cost are mandatory constraints. This paper proposes a new fault injection environment, which offers an alternative to radiation testing for evaluating the effects of charged particles on the configuration memory of SRAM-based FPGA devices. This paper describes the fault injection environment and reports preliminary results gathered on some benchmark circuits.", "num_citations": "50\n", "authors": ["502"]}
{"title": "An error-detection and self-repairing method for dynamically and partially reconfigurable systems\n", "abstract": " Reconfigurable systems are gaining an increasing interest in the domain of safety-critical applications, for example in the space and avionic domains. In fact, the capability of reconfiguring the system during run-time execution and the high computational power of modern Field Programmable Gate Arrays (FPGAs) make these devices suitable for intensive data processing tasks. Moreover, such systems must also guarantee the abilities of self-awareness, self-diagnosis and self-repair in order to cope with errors due to the harsh conditions typically existing in some environments. In this paperwe propose a self-repairing method for partially and dynamically reconfigurable systems applied at a fine-grain granularity level. Our method is able to detect correct and recover errors using the run-time capabilities offered by modern SRAM-based FPGAs. Fault injection campaigns have been executed on a dynamically\u00a0\u2026", "num_citations": "49\n", "authors": ["502"]}
{"title": "A new approach to software-implemented fault tolerance\n", "abstract": " A new approach for providing fault detection and correction capabilities by using software techniques only is described. The approach is suitable for developing safety-critical applications exploiting unhardened commercial-off-the-shelf processor-based architectures. Data and code duplications are exploited to detect and correct transient faults affecting the processor data segment, while control flow instruction duplication is used for detecting and correcting faults affecting the code segment. Results coming from extensive fault injection campaigns showed the effectiveness and the limitations of the method.", "num_citations": "48\n", "authors": ["502"]}
{"title": "Evaluating the fault tolerance capabilities of embedded systems via BDM\n", "abstract": " Fault injection is a viable solution for verifying the correct design and implementation of fault tolerance mechanisms at different levels (hardware and software). The paper discusses the use of the background diagnostic mode (BDM), available on several Motorola microprocessors and microcontrollers, for implementing a fault injection environment. BDM is well suited to implement some of the most critical operations required by a fault injection environment, such as activating the injection procedure, injecting the fault in memory or registers, and observing the faulty system behavior. The characteristics of a BDM-based fault injection environment in terms of intrusiveness, flexibility, time efficiency, and system requirements are analyzed. The authors exploit a prototypical environment they implemented to validate this analysis. As a result, the approach appears to be well suited for implementing low-cost fault injection\u00a0\u2026", "num_citations": "48\n", "authors": ["502"]}
{"title": "Fault behavior observation of a microprocessor system through a VHDL simulation-based fault injection experiment\n", "abstract": " Evaluating and possibly improving the fault tolerance and error detecting mechanisms is becoming a key issue when designing safety-critical electronic systems. The proposed approach is based on simulation-based fault injection and allows the analysis of the system behavior when faults occur. The paper describes how a microprocessor board employed in an automated light-metro control system has been modeled in VHDL and a Fault Injection Environment has been set up using a commercial simulator. Preliminary results about the effectiveness of the hardware fault-detection mechanisms are also reported. Such results will address the activity of experimental evaluation in subsequent phases of the validation process.", "num_citations": "48\n", "authors": ["502"]}
{"title": "High-level and hierarchical test sequence generation\n", "abstract": " Test generation at the gate-level produces high-quality tests but is computationally expensive in the case of large systems. Recently, several research efforts have investigated the possibility of devising test generation methods and tools to work on high-level descriptions. The goal of these methods is to provide the designers with testability information and test sequences in the early design stages. The cost for generating test sequences in the high abstraction levels is often lower than that for generating test sequences at the gate-level, with comparable or even higher fault coverage. This paper first analyses several high-level fault models in order to select the most suitable one for estimating the testability of circuits by reasoning on their behavioral descriptions and for guiding the test generation process at the behavioral level. We assess then the effectiveness of high-level test generation with a simple ATPG\u00a0\u2026", "num_citations": "47\n", "authors": ["502"]}
{"title": "An industrial environment for high-level fault-tolerant structures insertion and validation\n", "abstract": " When designing a VLSI circuits, most of the efforts are now performed at levels of abstractions higher than gate. Correspondingly to this clear trend, there is a growing request to tackle safety-critical issues directly at the RT-level. This paper presents a complete environment for considering safety issues at the RT level. The environment was implemented and tested by an industry for devising a sample safety-critical device. Designers were permitted to assess the effects of transient faults, automatically add fault-tolerant structures, and validate the results working on the same circuit descriptions and acting in a coherent framework. The evaluation showed the effectiveness of the proposed environment.", "num_citations": "45\n", "authors": ["502"]}
{"title": "FPGA-based fault injection for microprocessor systems\n", "abstract": " In this paper we propose an approach to speed-up fault injection campaigns for the evaluation of dependability properties of processor-based systems. The approach exploits FPGA devices for system emulation, and new techniques are described, allowing emulating the effects of faults and to observe faulty behavior. The proposed approach combines the speed of hardware-based techniques, and the flexibility of simulation-based techniques. Experimental results are provided showing that speed-up figures up to 3 orders of magnitude with respect to state-of-the-art simulation-based techniques can be achieved.", "num_citations": "45\n", "authors": ["502"]}
{"title": "Coping with SEUs/SETs in microprocessors by means of low-cost solutions: a comparison study\n", "abstract": " In this paper two low-cost solutions devoted to provide processor-based systems with error detection capabilities are compared. The effects of SEUs and SETs are studied through simulation-based fault injection. The error detection capabilities of a hardware-implemented solution, based on parity code, are compared with those of a software-implemented solution based on source-level code modification. Radiation testing experiments confirmed results obtained by simulation.", "num_citations": "44\n", "authors": ["502"]}
{"title": "A new evolutionary algorithm inspired by the selfish gene theory\n", "abstract": " This paper proposes a new evolutionary algorithm inspired by the selfish gene theory, an interpretation of the Darwinian natural selection theory given by the biologist Richard Dawkins. In his theory the basic element of evolution is the gene, rather than the individual. The paper defines the selfish gene algorithm, which implements such a view of the evolution mechanism. The strongest and seemingly counter-intuitive assumption of the algorithm is discussed and an experiment that demonstrates its validity is reported. The approach was rested by implementing the selfish gene algorithm on a case study, and it provided better results than those given by a genetic algorithm on the same problem and with the same fitness function.", "num_citations": "44\n", "authors": ["502"]}
{"title": "EXFI: a low-cost fault injection system for embedded microprocessor-based boards\n", "abstract": " Evaluating the faulty behavior of low-cost embedded microprocessor-based boards is an increasingly important issue, due to their adoption in many safety critical systems. The architecture of a complete Fault Injection environment is proposed, integrating a module for generating a collapsed list of faults, and another for performing their injection and gathering the results. To address this issue, the paper describes a software-implemented Fault Injection approach based on the Trace Exception Mode available in most microprocessors. The authors describe EXFI, a prototypical system implementing the approach, and provide data about some sample benchmark applications. The main advantages of EXFI are the low cost, the good portability, and the high efficiency", "num_citations": "42\n", "authors": ["502"]}
{"title": "System-in-package testing: problems and solutions\n", "abstract": " System-in-package integrates multiple dies in a common package. Therefore, testing SiP technology is different from system-on-chip, which integrates multiple vendor parts. This article provides test strategies for known good die and known good substrate in the SiP. Case studies prove feasibility using the IEEE 1500 test structure", "num_citations": "41\n", "authors": ["502"]}
{"title": "A software fault tolerance method for safety-critical systems: Effectiveness and drawbacks\n", "abstract": " An automatic software technique suitable for on-line detection of transient errors due to the effects of the environment (radiation, EMC,...) is presented. The proposed approach, particularly well suited for low-cost safety-critical microprocessor-based applications, has been validated through fault injection experiments and radiation testing campaigns. The experimental results demonstrate the effectiveness of the approach in terms of fault detection capabilities. Undetected faults have been analyzed to point out the limitations of the method.", "num_citations": "41\n", "authors": ["502"]}
{"title": "Hybrid genetic algorithms for the traveling salesman problem\n", "abstract": " A comparative analysis is performed on an experimental basis among four different cross-over operators. In order to exploit the benefits of the different operators, a new one (called Mixed Cross-over) is introduced, trading-off the CPU time requirements and the obtained results. A new operator is then proposed, whose goal is to include in the genetic mechanism some heuristic knowledge drawn from the already proposed local-optimization techniques. The performance of the new operator is discussed.", "num_citations": "41\n", "authors": ["502"]}
{"title": "Analysis of SEU effects in a pipelined processor\n", "abstract": " Modern processors embed features such as pipelined execution units and cache memories that can hardly be controlled by programmers through the processor instruction set. As a result, software-based fault injection approaches are no longer suitable for assessing the effects of SEUs in modern processors, since they are not able to evaluate the effects of SEUs affecting pipelines and caches. In this paper we report an analysis of a commercial processor core where the effects of SEUs located in the processor pipeline and cache memories are studied. Moreover the obtained results are compared with those software-based approaches provide. Experimental results show that software-based approaches may lead to errors during the failure rate estimation of up to 400%.", "num_citations": "40\n", "authors": ["502"]}
{"title": "A suite of IEEE 1687 benchmark networks\n", "abstract": " The saturation of the IJTAG concept and its approval as the IEEE 1687 standard in 2014 has generated a wave of research activities and created demand for a set of appropriate and challenging benchmarks. This paper presents such a set developed by an industrial and academic consortium and constructed in a way that facilitates objective comparison of experimental results across research groups as well as represents challenging network examples exhaustively utilizing features and constructs defined by the standard. The suite is arranged in four comprehensive categories, each having its particular purpose and composition principles, as described in the paper. We have also made an analysis of limitations of previous popular and ad-hoc benchmark sets as these limitations majorly motivated our current action. The new public-domain benchmarks are distributed together with source files and documentation\u00a0\u2026", "num_citations": "39\n", "authors": ["502"]}
{"title": "An experimental analysis of the effects of migration in parallel genetic algorithms\n", "abstract": " The paper presents some experimental results concerning parallel genetic algorithms. Genetic algorithms are a well-established technique for the solution of large optimization problems; a parallel version has been proposed for them, based on the concept of migration. Several parameters concerning migration deeply affect the performance of the approach, but it is often difficult to optimize their value in order to obtain the best result. The paper presents a system which produces good solutions to the traveling salesman problem using parallel genetic algorithms, and reports some results concerning the influence of the parameters on the performance of the system.< >", "num_citations": "39\n", "authors": ["502"]}
{"title": "Effective techniques for high-level ATPG\n", "abstract": " The ASIC design flow is rapidly moving towards higher description levels, and most design activities are now performed at the RT-level. However, test-related activities are lacking behind this trend, mainly since effective fault models and test pattern generation tools are still missing. This paper proposes techniques for implementing a high-level ATPG. The proposed algorithm mixes a code coverage-oriented approach with fault-oriented optimizations. Moreover, it exploits a fault model at the RT-level that enables efficient fault simulation and guarantees good correlation with gate-level fault coverage. Experimental results show that the achieved results are comparable or better than those obtained at the gate level or by similar RT-level approaches.", "num_citations": "37\n", "authors": ["502"]}
{"title": "Hardware and software transparency in the protection of programs against SEUs and SETs\n", "abstract": " Processor cores embedded in systems-on-a-chip (SoCs) are often deployed in critical computations, and when affected by faults they may produce dramatic effects. When hardware hardening is not cost-effective, software implemented hardware fault tolerance (SIHFT) can be a solution to increase SoCs\u2019 dependability, but it increases the time for running the hardened application, as well as the memory occupation. In this paper we propose a method that eliminates the memory overhead, by exploiting a new approach to instruction hardening and control flow checking. The proposed method hardens an application online during its execution, without the need for introducing any change in its source code, and is non-intrusive, since it does not require any modification in the main processor\u2019s architecture. The method has been tested with two widely used architectures: a microcontroller and a RISC processor\u00a0\u2026", "num_citations": "36\n", "authors": ["502"]}
{"title": "A new functional fault model for FPGA application-oriented testing\n", "abstract": " The objective of this paper is to propose a new fault model suitable for test pattern generation for an FPGA configured to implement a given application. The paper demonstrates that the faults affecting the bit cells of the look-up tables (LUTs) are not redundant, although they store constant values. We demonstrate that these faults cannot be neglected and that the fault model corresponding to modifying the content of each LUT memory cell must be considered in order to cover the full range of possible faults. In order to evaluate the fault coverage of the proposed fault model, a set of circuits mapped on a Xilinx Virtex 300 FPGA have been considered. Test sequences generated by a gate-level commercial ATPG and an academic RT-level one have been fault simulated on these benchmark circuits. The obtained figures show that a high percentage of faults affecting the LUT bit cells are undetected, thus suggesting that\u00a0\u2026", "num_citations": "36\n", "authors": ["502"]}
{"title": "Behavioral-level test vector generation for system-on-chip designs\n", "abstract": " Co-design tools represent an effective solution for reducing costs and shortening time-to-market, when system-on-chip design is considered. In a top-down design flow, designers would greatly benefit from the availability of tools able to automatically generate test sequences, which can be reused during the following design steps, from the system-level specification to the gate-level description. This would significantly increase the chance of identifying testability problems early in the design flow, thus reducing the costs and increasing the final product quality. The paper proposes an approach for integrating the ability to generate test sequences into an existing co-design tool. Preliminary experimental results are reported, assessing the feasibility of the proposed approach.", "num_citations": "36\n", "authors": ["502"]}
{"title": "Efficient machine-code test-program induction\n", "abstract": " Technology advances allow integrating an entire system on a single chip, including memories and peripherals. The testing of these devices is becoming a major issue for chip manufacturing industries. This paper presents a methodology, similar to genetic programming, for inducing test programs. However, it includes the ability to explicitly specify registers and resorts to directed acyclic graphs instead of trees. Moreover, it exploits a database containing the assembly-level semantics associated with each graph node. This approach is extremely efficient and versatile: candidate solutions are translated into source-code programs allowing millions of evaluations per second. The proposed approach is extremely versatile: the macro library allows the target processor and the environment to be changed easily. The approach was verified on three processors with different instruction sets, different formalisms and different\u00a0\u2026", "num_citations": "35\n", "authors": ["502"]}
{"title": "Software-based hardening strategies for neutron sensitive FFT algorithms on GPUs\n", "abstract": " In this paper we assess the neutron sensitivity of Graphics Processing Units (GPUs) when executing a Fast Fourier Transform (FFT) algorithm, and propose specific software-based hardening strategies to reduce its failure rate. Our research is motivated by experimental results with an unhardened FFT that demonstrate a majority of multiple errors in the output in the case of failures, which are caused by data dependencies. In addition, the use of the built-in error-correction code (ECC) showed a large overhead, and proved to be insufficient to provide high reliability. Experimental results with the hardened algorithm show a two orders of magnitude failure rate improvement over the original algorithm (one order of magnitude over ECC) and an overhead 64% smaller than ECC.", "num_citations": "34\n", "authors": ["502"]}
{"title": "An effective technique for the automatic generation of diagnosis-oriented programs for processor cores\n", "abstract": " A large part of microprocessor cores in use today are designed to be cheap and mass produced. The diagnostic process, which is fundamental to improve yield, has to be as cost effective as possible. This paper presents a novel approach to the construction of diagnosis-oriented software-based test sets for microprocessors. The methodology exploits existing manufacturing test sets designed for software-based self-test and improves them by using a new diagnosis-oriented approach. Experimental results are reported in this paper showing the feasibility, robustness, and effectiveness of the approach for diagnosing stuck-at faults on an Intel i8051 processor core.", "num_citations": "34\n", "authors": ["502"]}
{"title": "Evolutionary test program induction for microprocessor design verification\n", "abstract": " Design verification is a crucial step in the design of any electronic device. Particularly when microprocessor cores are considered, devising appropriate test cases may be a difficult task. This paper presents a methodology able to automatically induce a test program for maximizing a given verification metric. The methodology is based on an evolutionary paradigm and exploits a syntactical description of microprocessor assembly language and an RT-level functional model. Experimental results show the effectiveness of the approach.", "num_citations": "34\n", "authors": ["502"]}
{"title": "A new model for improving symbolic product machine traversal\n", "abstract": " This paper presents algorithms for traversing product machines which improve on [6] of a speedup ranging from 3 up to 6. New features include a model that generalizes the product machine, resulting in simpler and more efficient representations and computations, as well as optimizations in symbolic image computation. In the latter case, the speed-up ranges from 1.5 to 4.", "num_citations": "34\n", "authors": ["502"]}
{"title": "On-line functionally untestable fault identification in embedded processor cores\n", "abstract": " Functional testing of embedded processors is a challenging task and additional constraints are imposed when a functional test procedure has to be executed online. In the latter case, a significant amount of the processor faults cannot be detected since related to the debug/test circuitry or because of memory configuration constraints. In this paper we identify several sources of on-line functional untestability and propose a set of techniques to exactly measure their impact on the fault coverage. Experimental results related to an industrial case study are reported, showing that the fault coverage loss due to the considered untestability sources may reach more than 13%.", "num_citations": "33\n", "authors": ["502"]}
{"title": "Test program generation for communication peripherals in processor-based SoC devices\n", "abstract": " Testing communication peripherals in an environment of systems on a chip is particularly challenging. The authors explore two test program generation approaches-one fully automated and one deterministically guided-and propose a novel combination of the two schemes that can be applied in a generic manner on a wide set of communication cores.", "num_citations": "33\n", "authors": ["502"]}
{"title": "A low-cost SEE mitigation solution for soft-processors embedded in systems on pogrammable chips\n", "abstract": " The availability of multimillion Commercial-Off-The-Shelf (COTS) Field Programmable Gate Arrays (FPGAs) is making now possible the implementation on a single device of complex systems embedding processor cores as well as huge memories and ad-hoc hardware accelerators exploiting the programmable logic (Systems on Programmable Chip, or SoPCs). When deployed in safety- or mission-critical applications, as avionic- and space-oriented ones, Singe Event Effects (SEEs) affecting COTS FPGA, which may have catastrophic effects if neglected, have to be considered and SEE mitigation techniques have to be employed. In this paper we explore the adoption of known techniques (such as lockstep, checkpointing and rollback recovery) for SEE mitigation to processors cores embedded in SoPCs, and propose their customization, specifically addressing the characteristics of programmable devices. Since the\u00a0\u2026", "num_citations": "33\n", "authors": ["502"]}
{"title": "Using infrastructure IPs to support SW-based self-test of processor cores\n", "abstract": " SoCs normally include microprocessor/microcontroller cores. Testing them following the software-based self-test approach is attractive, mainly because this allows at speed testing, and does not require internally modifying the core. However, this raises some issues, such as how to upload and launch the test, how to monitor the results, how to embed the adopted solutions into a suitable wrapper to enhance core modularity and test reusability. The paper proposes a possible solution to the above issues exploiting Infrastructure IPs, and reports the results gathered on two case studies.", "num_citations": "33\n", "authors": ["502"]}
{"title": "Accurate and efficient analysis of single event transients in VLSI circuits\n", "abstract": " Single event transients (SETs) on combinational gates are becoming an issue in deep sub-micron technologies, thus efficient and accurate techniques for assessing their impact are strongly required. This paper presents a new technique that embeds time-related information in the topology of the analyzed circuit, allowing evaluating the effects of SETs via zero-delay simulation instead of timed simulation. The analysis of complex designs becomes thus possible at a very limited cost in terms of CPU time. The paper reports results showing how the proposed method can be effectively used to analyze complex designs.", "num_citations": "33\n", "authors": ["502"]}
{"title": "A fault injection environment for microprocessor-based boards\n", "abstract": " Evaluating the faulty behaviour of low-cost microprocessor-based boards is an increasingly important issue, due to their usage in many safety critical systems. To address this issue, the paper describes a software-implemented fault injection system based on the trace exception mode available in most microprocessors. The architecture of the complete fault injection environment is proposed, integrating modules for generating a fault list, for performing their injection and for gathering the results, respectively. Data gathered from some sample benchmark applications are presented The main advantages of the approach are low cost, good portability, and high efficiency.", "num_citations": "33\n", "authors": ["502"]}
{"title": "Improved software-based processor control-flow errors detection technique\n", "abstract": " This paper presents software implemented hardware fault detection (SIHFD) for developing safety critical applications. This fault detection technique provides low-cost solutions to enhance the reliability of computer-based systems without modifying the hardware. This technique is applicable to programs coded with high-level programming languages and it is based on the analysis of the control flow graph of the program. In this work we performed an in depth analysis to identify the reasons of escaping errors. We proposed some rules which, being applied on high-level descriptions of the program, allow overcoming detected problems and further increasing error coverage. Experiments showed the effectiveness of the proposed approach.", "num_citations": "32\n", "authors": ["502"]}
{"title": "Comparing topological, symbolic and GA-based ATPGs: an experimental approach\n", "abstract": " The goal of this paper is-to compare from an experimental point of view the performance of three ATPG tools for synchronous sequential circuits. The three tools are stare-of-the-art implementations of the topological, symbolic, and GA-based approaches, respectively. The environment set up for obtaining a fair comparison is described: the same hardware platform, circuit and fault list description, and detection mechanism are adopted. The obtained results allow the reader to more deeply understand the characteristics and relative advantages/disadvantages of these methods.", "num_citations": "32\n", "authors": ["502"]}
{"title": "On the functional test of branch prediction units\n", "abstract": " Branch prediction units (BPUs) are highly efficient modules that can significantly decrease the negative impact of branches in pipelined processors. Traditional test solutions, mainly based on Design for Testability techniques, are often inadequate to tackle specific test constraints, such as those found when incoming inspection or online test is considered. Following a functional approach based on running a suitable test program and checking the processor behavior may represent an alternative solution, provided that an effective test algorithm is available for the target unit. In this paper, a functional approach targeting the test of the BPU memory is proposed, which leads to the generation of suitable test programs whose effectiveness is independent of the specific implementation of the BPU. Two very common BPU architectures (branch history table and branch target buffer) are considered. The effectiveness of the\u00a0\u2026", "num_citations": "31\n", "authors": ["502"]}
{"title": "A low-cost solution for deploying processor cores in harsh environments\n", "abstract": " Nowadays, a number of processor cores are available, either as soft intellectual property (IP) cores or as hard macros that can be employed in developing new systems on a chip. Developers of applications targeting harsh environments like the atmospheric radiation environment or the space radiation environment may benefit from the computing power of processor cores, provided that suitable techniques are available for guaranteeing their correct operations in presence of the ionizing radiation that abounds in such environments. In this paper, we describe a design flow and hardware/software architecture to successfully deploy processor IP cores in harsh environments. Experimental data are provided that confirm the robustness of the presented architecture with respect to transient errors induced by radiation and suggest the possibility of employing such architectures in deep-space exploration missions.", "num_citations": "31\n", "authors": ["502"]}
{"title": "An effective technique for minimizing the cost of processor software-based diagnosis in SoCs\n", "abstract": " The ever increasing usage of microprocessor devices is sustained by a high volume production that in turn requires a high production yield, backed by a controlled process. Fault diagnosis is an integral part of the industrial effort towards these goals. This paper presents a novel cost-effective approach to the construction of diagnostic software-based test sets for microprocessors. The methodology exploits an existing post-production test set, designed for software-based self-test, and an already developed infrastructure IP to perform the diagnosis. An initial diagnostic test set is built, and then iteratively refined resorting to an evolutionary method. Experimental results are reported in the paper showing the feasibility and effectiveness of the approach for an Intel i8051 processor core", "num_citations": "31\n", "authors": ["502"]}
{"title": "Impact of data cache memory on the single event upset-induced error rate of microprocessors\n", "abstract": " Cache memories embedded in most of complex processors significantly contribute to the global single event upset-induced error rate. Three different approaches allowing the study of this contribution by fault injection are investigated in this paper.", "num_citations": "30\n", "authors": ["502"]}
{"title": "On the testability of IEEE 1687 networks\n", "abstract": " Due to the increasing usage of embedded instruments in many electronic devices, new solutions to effectively access these instruments appeared, including the new IEEE 1687 standard. The approach supported by IEEE 1687 allows a flexible access to embedded instruments through the Boundary Scan interface. The IEEE 1687 network includes a set of reconfigurable scan chains. This paper addresses the issue of testing the circuitry implementing them, checking whether any permanent hardware fault exists, affecting either the registers associated to the instruments made accessible by the network, or the configuration structures it embeds (e.g., the multiplexers and the associated flip-flops). The paper proposes an approach, in which the IEEE 1687 network undergoes a sequence of test sessions, each composed of a configuration phase and a test phase. By properly selecting the network configurations to be\u00a0\u2026", "num_citations": "29\n", "authors": ["502"]}
{"title": "On the use of embedded debug features for permanent and transient fault resilience in microprocessors\n", "abstract": " Microprocessor-based systems are employed in an increasing number of applications where dependability is a major constraint. For this reason detecting faults arising during normal operation while introducing the least possible penalties is a main concern. Different forms of redundancy have been employed to ensure error-free behavior, while error detection mechanisms can be employed where some detection latency is tolerated. However, the high complexity and the low observability of microprocessors\u2019 internal resources make the identification of adequate on-line error detection strategies a very challenging task, which can be tackled at circuit or system level. Concerning system-level strategies, a common limitation is in the mechanism used to monitor program execution and then detect errors as soon as possible, so as to reduce their impact on the application. In this work, an on-line error detection approach\u00a0\u2026", "num_citations": "29\n", "authors": ["502"]}
{"title": "Fault injection-based reliability evaluation of SoPCs\n", "abstract": " Systems-on-programmable-chip (SoPCs) include processors, memories and programmable logic that allow to catch multiple application requirements such as high performance, reconfigurability and low-costs. Due to these characteristics, they are also becoming very attractive for safety-critical applications. However, the issue of assessing the reliability they can provide and debugging the possible safety-related mechanisms they embed is still open. In this paper, we present a new fault-injection approach for evaluating the impact of transient faults in SoPCs. Fault-injection experiments are reported on a case study consisting of a Web server implemented on a Xilinx Virtex-II FPGA embedding a PowerPC 405 and running the whole TCP/IP stack", "num_citations": "29\n", "authors": ["502"]}
{"title": "Cellular automata for deterministic sequential test pattern generation\n", "abstract": " This paper addresses the issue of identifying a Cellular Automaton able to generate deterministic input patterns to detect stuck-at faults inside an FSM. A suitable hardware structure is first identified. An evolutionary algorithm is then proposed, which directly identifies a Cellular Automaton able to reach a very good Fault Coverage. The novelty of the method consists in combining the generation of test patterns with the synthesis of a Cellular Automaton able to reproduce them. Experimental results are provided, which show that the proposed hardware architecture and algorithmic approach outperform more traditional solutions, based on ATPG tools and FSM synthesis, from the point of view of both applicability and area occupation, while reaching the same Fault Coverage.", "num_citations": "29\n", "authors": ["502"]}
{"title": "Embedded memory diagnosis: An industrial workflow\n", "abstract": " Embedded memory modules are sensitive components that deeply influence production yield of integrated devices. For fast yield improvement, an efficient manufacturing test must supply advanced defect characterization that helps in discovering technology weaknesses and finding strategies for improvement. This paper presents an industrial workflow for embedded memory diagnosis. It is based on the integration of March-based diagnostic BIST hardware in an IEEE 1500-compliant environment, and on a novel diagnostic algorithm for determining the fault model associated to the retrieved syndromes. An experimental implementation showing the feasibility of the approach is presented", "num_citations": "28\n", "authors": ["502"]}
{"title": "RoRA: A reliability-oriented place and route algorithm for SRAM-based FPGAs\n", "abstract": " SRAM-based FPGA designs are extremely susceptible to single event upset (SEUs). Since the configuration memory defines which is the circuit an SRAM-based field programmable gate array (FPGA) implements, any change induced by SEUs in the configuration memory may modify drastically the implemented circuit. When such devices are used in safety-critical applications, fault tolerant techniques are needed able to mitigate the effects of SEUs in FPGA's configuration memory. In this paper we present a reliability-oriented place and route algorithm that is able to mitigate the effects of the considered upsets.", "num_citations": "28\n", "authors": ["502"]}
{"title": "A programmable BIST for DRAM testing and diagnosis\n", "abstract": " This paper proposes a programmable Built-In Self-Test (BIST) approach for DRAM test and diagnosis. The proposed architecture suits well for embedded core testing as well as for stacked and stand-alone DRAMs and it provides programmability features for executing both March and NPSF-oriented test algorithms. The proposed BIST structure is designed to be easily customized with memory topology parameters such as scrambling and mirroring, in order to automatically adapt the test circuitry to the specific memory design. Experimental results show that area overhead is negligible when considering medium-large memory cuts, while executing at-speed and Back-to-Back algorithms at more than 1GHz.", "num_citations": "27\n", "authors": ["502"]}
{"title": "A hybrid approach to the test of cache memory controllers embedded in SoCs\n", "abstract": " Software-based self-test (SBST) is increasingly used for testing processor cores embedded in SoCs, mainly because it allows at-speed, low-cost testing, while requiring limited (if any) hardware modifications to the original design. However, the method requires effective techniques for generating suitable test programs and for monitoring the results. In the case of processor core testing, a particularly complex module to test is the cache controller, due to its limited accessibility and observability. In this paper we propose a hybrid methodology that exploits an Infrastructure Intellectual Property (I-IP) to complement an SBST algorithm for testing the data and instruction cache controllers of embedded processors in SoCs. In particular, the I-IP may be programmed to monitor the system buses and generate the appropriate feedback about the correct result of the executed programs (in terms of obtained hit or miss operations\u00a0\u2026", "num_citations": "27\n", "authors": ["502"]}
{"title": "Evaluating different solutions to design fault tolerant systems with SRAM-based FPGAs\n", "abstract": " The latest SRAM-based FPGA devices are making the development of low-cost, high-performance, re-configurable systems feasible, paving the way for innovative architectures suitable for mission- or safety-critical applications, such as those dominating the space or avionic fields. Unfortunately, SRAM-based FPGAs are extremely sensitive to Single Event Upsets (SEUs) induced by radiation. SEUs may alter the logic value stored in the memory elements the FPGAs embed. A large part of the FPGA memory elements is dedicated to the configuration memory, whose content dictates how the resources inside the FPGA have to be used to implement any given user circuit, SEUs affecting configuration memory cells can be extremely critics. Facing the effects of SEUs through radiation-hardened FPGAs is not cost-effective. Therefore, various fault-tolerant design techniques have been devised for developing\u00a0\u2026", "num_citations": "27\n", "authors": ["502"]}
{"title": "Optimal vector selection for low power BIST\n", "abstract": " In the last decade, researchers have devoted increasing efforts to reduce the average power consumption in VLSI systems during normal operation mode, while power consumption during test operation mode was usually neglected. However, during test application the circuits are subject to an activity higher than the normal one: the extra power consumption due to test application may thus give rise to severe hazards to the circuit reliability. Moreover, it can dramatically shorten the battery life when periodic testing of battery-powered systems is considered. In this paper we propose a low power BIST architecture devised for full scan testing of sequential circuits. Experimental results show that our approach can achieve an average power reduction ranging from 37% to 89% without affecting the quality of the test. The new architecture can be easily integrated into an existing design flow and is barely invasive with\u00a0\u2026", "num_citations": "27\n", "authors": ["502"]}
{"title": "On the in-field functional testing of decode units in pipelined RISC processors\n", "abstract": " The paper is dealing with the in-field test of the decode unit of RISC processors through functional test programs following the SBST approach. The paper details a strategy based on instruction classification and manipulation, and signatures collection. The method does not require the knowledge of detailed implementation information (e.g., the netlist), but is based on the Instruction Set of the processor. The proposed method is evaluated on an industrial SoC device, which includes a PowerPC derived processor. Results demonstrate the efficiency and effectiveness of the strategy; the proposed solution reaches over 90% of stuck-at fault coverage while an instruction coverage based approach does not overcome 70%.", "num_citations": "26\n", "authors": ["502"]}
{"title": "On the evaluation of soft-errors detection techniques for GPGPUs\n", "abstract": " Recently, General Purpose Graphic Processing Units (GPGPUs) have begun to be preferred to CPUs for several computationally intensive applications, not necessarily related to computer graphics. However, due to their complexity GPGPUs also show a relatively high sensitivity to soft errors. Hence, there is some interest in devising and applying software techniques able to exploit their computational power by just acting on the executed code. In this paper we report some preliminary results obtained by applying two different software redundancy techniques aimed at soft-error detection; these techniques are completely algorithm independent, and have been applied on a sample application running on a Commercial-Off-The-Shelf GPGPU. The results have been gathered resorting to a neutron testing campaign. Some experimental results, explaining the capabilities of the methods, are presented and commented.", "num_citations": "26\n", "authors": ["502"]}
{"title": "On-line software-based self-test of the address calculation unit in RISC processors\n", "abstract": " Software-based Self-Test (SBST) can be used during the mission phase of microprocessor-based systems to periodically assess the hardware integrity. However, several constraints are imposed to this approach, due to the coexistence of test programs with the mission application. This paper proposes a method for the generation of SBST programs to test on-line the Address Calculation Unit of embedded RISC processors, which is one of the most heavily impacted by the online constraints. The proposed strategy achieves high stuck-at fault coverage on both a MIPS-like processor and an industrial 32-bit pipelined processor; these two case studies show the effectiveness of the technique and the low effort.", "num_citations": "26\n", "authors": ["502"]}
{"title": "A functional power evaluation flow for defining test power limits during at-speed delay testing\n", "abstract": " High power consumption during test may lead to yield loss and premature aging. In particular, excessive peak power during at-speed delay fault testing represents an important issue. In the literature, several techniques have been proposed to reduce peak power consumption during at-speed LOC or LOS delay testing. On the other hand, some experiments have proved that too much test power reduction might lead to test escape and reliability problems. So, in order to avoid any yield loss and test escape due to power issues during test, test power has to map the power consumed during functional mode. In literature, some techniques have been proposed to apply test vectors that mimic functional operation from the switching activity point of view. The process consists of shifting-in a test vector (at low speed) and then applying several successive at-speed clock cycles before capturing the test response. In this paper\u00a0\u2026", "num_citations": "26\n", "authors": ["502"]}
{"title": "A pattern ordering algorithm for reducing the size of fault dictionaries\n", "abstract": " Determining the relation between defects and faults (fault diagnosis) in digital circuits is a key concept for semiconductors production yield improvement. Nowadays, fault diagnosis requires heavy computations and large data structures. This paper proposes a novel technique for reducing fault dictionary size for combinational and scanned circuits by means of pattern-ordering. The proposed algorithm manipulates conventional tree-based fault dictionaries. In such structures, faults are diagnosed by traversing the tree from its root to a leaf; we aim at globally reducing the length of such paths by a modified patterns order, thus also reducing the dictionary size. This approach does not cause any loss of information, since it is demonstrated for combinational circuits that the ability of a pattern set in diagnosing faults remains unaltered when modifying the patterns order. The effectiveness of the proposed approach is\u00a0\u2026", "num_citations": "26\n", "authors": ["502"]}
{"title": "An integrated HW and SW Fault Injection environment for real-time systems\n", "abstract": " This paper describes a system suited to support the Fault Injection process for microprocessor-based embedded systems. The system exploits a low-cost hardware board to monitor the processor status, to activate the fault injection procedure, and to gather information about the fault-free system behavior required to implement a set of fault collapsing rules. The overall environment allows at-speed fault injection experiments with negligible intrusiveness in the target system, and can therefore be used to efficiently evaluate real-time systems dependability.", "num_citations": "26\n", "authors": ["502"]}
{"title": "Making the circular self-test path technique effective for real circuits\n", "abstract": " The paper assesses the effectiveness of the circular self-test path BIST technique from an experimental point of view and proposes an algorithm to overcome the low fault coverage that often arises when real circuits are examined. Several fault simulation experiments have been performed on the ISCAS89 benchmark set, as well as on a set of industrial circuits: in contrast to the theoretical analysis proposed in [PKKa92], a very high fault coverage is attained with a limited number of clock cycles, but this happens only when the circuit does not enter a loop. This danger cannot be avoided even if clever strategies for flip-flops ordering, aimed at reducing the functional adjacency, are adopted. Instead, we suggest that loops can be avoided and fault coverage increased by carefully choosing the initial state, and we present an approach based on binary decision diagrams and symbolic techniques to solve the problem.", "num_citations": "26\n", "authors": ["502"]}
{"title": "An on-line fault detection technique based on embedded debug features\n", "abstract": " An increasing number of applications require being able to detect possible faults arising during the normal activity of the electronic system: for this reason, on-line fault detection is a hot topic today. This paper proposes a new technique which is suitable for microprocessor-based systems (no matter whether they are implemented in a single device or with discrete COTS) that exploit hardware duplication and combines it with the On-Chip Debug features existing in many processors. The new technique increases the observability of faults (thus increasing detection probability and reducing latency) and is characterized by a very reduced intrusiveness in terms of changes required in the application code.", "num_citations": "25\n", "authors": ["502"]}
{"title": "Efficient techniques for automatic verification-oriented test set optimization\n", "abstract": " Most Systems-on-a-Chips include a custom microprocessor core, and time and resource constraints make the design of such devices a challenging task. This paper presents a simulation-based methodology for the automatic completion and refinement of verification test sets. The approach extends the\u00a0\u03bcGP, an evolutionary test program generator, with the possibility to enhance existing test sets. Already devised test programs are not merely included in the new set, but assimilated and used as a starting point for a new test-program cultivation task. Reusing existing material cuts down the time required to generate a verification test set during the microprocessor design. Experimental results are reported on a small pipelined microprocessor, and show the effectiveness of the approach. Additionally, the use of the proposed methodology enabled to experimentally analyze the relationship of the different code\u00a0\u2026", "num_citations": "25\n", "authors": ["502"]}
{"title": "On the transformation of manufacturing test sets into on-line test sets for microprocessors\n", "abstract": " In software-based self-test (SBST), a microprocessor executes a set of test programs devised for detecting the highest possible percentage of faults. The main advantages of this approach are its high defect fault coverage (being performed at-speed) and the reduced cost (since it does not require any change in the processor hardware). SBST can also be used for online test of a microprocessor-based system. However, some additional constraints exist in this case (e.g. in terms of test length and duration, as well as intrusiveness). This paper faces the issue of automatically transforming a test set devised for manufacturing test in a test set suitable for online test. Experimental results are reported on an Intel 8051 microcontroller.", "num_citations": "25\n", "authors": ["502"]}
{"title": "Exploiting an I-IP for in-field SoC test\n", "abstract": " Today's complex system-on-chip integrated circuits include a wide variety of functional IPs whose correct manufacturing must be guaranteed by IC producers. Infrastructure IPs are increasingly often inserted to achieve this purpose; such blocks, explicitly designed for test, are coupled with functional IPs both to obtain yield improvement during the manufacturing process and to perform volume production test. In this paper, a new test control schema based on the use of an infrastructure IP (I-IP) is proposed for the test on-site of SoCs. The proposed in-field test strategy is based on the ability of a single I-IP to periodically monitor the behavior of the system by reusing the test structures introduced for manufacturing test. The feasibility of this approach has been proved for SoCs including microprocessors and memories equipped with P1500 compliant solutions. Experimental results highlight the advantages in term of\u00a0\u2026", "num_citations": "25\n", "authors": ["502"]}
{"title": "Automatic test program generation for pipelined processors\n", "abstract": " The continuous advances in microelectronics design are creating a significant challenge to design validation in general, but tackling piplined microprocessors is remarkably more demanding. This paper presents a methodology to automatically induce a test program for a microprocessor maximizing a given verification metric. The approach exploits a new evolutionary algorithm, close to Genetic Programming, able to cultivate effective assembly-language programs. The proposed methodology was used to verify the DLX/pII, an open-source processor with a 5-stage pipeline. Code-coverage was adopted in the paper, since it can be considered the required starting point for any simulation-based functional verification processes. Experimental results clearly show the effectiveness of the approach.", "num_citations": "25\n", "authors": ["502"]}
{"title": "Exploring the impact of functional test programs re-used for power-aware testing\n", "abstract": " High power consumption during at-speed delay fault testing may lead to yield loss and premature aging. On the other hand, reducing too much test power might lead to test escape and reliability problems. Thus, to avoid these issues, test power has to map the power consumed during functional mode. Existing works target the generation of functional test programs able to maximize the power consumption in functional mode of microprocessor cores. The obtained power consumption will be used as threshold to tune the power consumed during testing. This paper investigates the impact of re-using such functional test programs for testing purposes. We propose to apply them by exploiting existing DfT architecture to maximize the delay fault coverage. Then, we combine them with the classical at-speed LOC and LOS delay fault testing schemes to further increase the fault coverage. Results show that it is possible to\u00a0\u2026", "num_citations": "24\n", "authors": ["502"]}
{"title": "A new SBST algorithm for testing the register file of VLIW processors\n", "abstract": " Feature size reduction drastically influences permanent faults occurrence in nanometer technology devices. Among the various test techniques, Software-Based Self-Test (SBST) approaches have been demonstrated to be an effective solution for detecting logic defects, although achieving complete fault coverage is a challenging issue due to the functional-based nature of this methodology. When VLIW processors are considered, standard processor-oriented SBST approaches result deficient since not able to cope with most of the failures affecting VLIW multiple parallel domains. In this paper we present a novel SBST algorithm specifically oriented to test the register files of VLIW processors. In particular, our algorithm addresses the cross-bar switch architecture of the VLIW register file by completely covering the intrinsic faults generated between the multiple computational domains. Fault simulation campaigns\u00a0\u2026", "num_citations": "24\n", "authors": ["502"]}
{"title": "Evaluating the effects of transient faults on vehicle dynamic performance in automotive systems\n", "abstract": " Current automotive systems are integrating more and more electronic components in the handling and performance areas, for supporting advanced comfort and safety features. The effects of component or network failures raise serious concerns about the overall vehicle stability and safety. This work proposes a methodology for analyzing at the system level (taking into account both mechanical and electronic components) the implications of transient faults in the electronic part on the overall vehicle response. A prototypical fault injection environment is also presented, and experimental results show how safety specifications for components can be derived from performance objectives set at the vehicle level.", "num_citations": "24\n", "authors": ["502"]}
{"title": "On-line analysis and perturbation of CAN networks\n", "abstract": " The controller area network (CAN) is a well-known standard, and it is widely used in many safety-critical applications, spanning from automotive electronics to aircraft and aerospace electronics. Due to its widespread adoption in critical applications, the capability of accurately evaluating the dependability properties of CAN-based networks is becoming a major concern. In this paper we present a new environment that can be fruitfully exploited to assess the effects of faults in CAN-based networks, which is particularly suited for being exploited when a prototype of the network under analysis is available. The core of our new environment is a special-purpose board that plugs into an existing CAN network, and that is able to monitor and, when asked, to modify the information traveling over the bus. Observation and modification of CAN frames are done concurrently with normal CAN-bus operations without introducing\u00a0\u2026", "num_citations": "24\n", "authors": ["502"]}
{"title": "New techniques for efficiently assessing reliability of SOCs\n", "abstract": " In this paper we propose an approach to speed-up Fault Injection campaigns for the evaluation of dependability properties of complex digital systems. The approach exploits FPGA devices for system emulation, and new techniques are described, allowing emulating the effects of faults and to observe faulty behavior. Thanks to its flexibility and efficiency, the approach is suitable to be applied to SOC devices. The paper points out the flexibility of the approach, able to inject different faults of different types in custom logic, memory blocks, and processor cores. The proposed approach combines the speed of hardware-based techniques, and the flexibility of simulation-based techniques. Experimental results are provided showing that speed-up figures of up to 3 orders of magnitude with respect to state-of-the-art simulation-based techniques can be achieved.", "num_citations": "24\n", "authors": ["502"]}
{"title": "Experiences in the use of evolutionary techniques for testing digital circuits\n", "abstract": " The generation of test patterns for sequential circuits is one of the most challenging problems arising in the field of Computer-Aided Design for VLSI circuits. In the past decade, Genetic Algorithms have been deeply investigated as a possible approach: several algorithms have been described, and significant improvements have been proposed with respect to their original versions. As a result, GA-based test pattern generators can now effectively compete with other methods, such as topological or symbolic ones. This paper discusses the advantages and disadvantages of GA-based approaches and describes GATTO, a state-of-the-art GA-based test pattern generator. Other algorithms belonging to the same category are outlined as well. The paper puts GATTO and other GA-based tools in perspective, and shows that Evolutionary computation techniques can successfully compete with more traditional approaches\u00a0\u2026", "num_citations": "24\n", "authors": ["502"]}
{"title": "Online test of control flow errors: A new debug interface-based approach\n", "abstract": " Detecting the effects of transient faults is a key point in many processor-based safety-critical applications. This paper proposes to adopt the debug interface module existing today in several processors/controllers available on the market. In this way, we can achieve a good detection capability and small latency with respect to control flow errors, while the cost for adopting the proposed technique is rather limited and does not involve any change either in the processor hardware or in the application software. The method works even if the processor uses caches and we experimentally evaluated its characteristics demonstrating the advantages and showing the limitations on two pipelined processors. Experimental results performed by fault injection using different software applications demonstrate that the method is able to archieve high fault coverage (more than 95 percent in nearly all the considered cases) with a\u00a0\u2026", "num_citations": "23\n", "authors": ["502"]}
{"title": "MIHST: A hardware technique for embedded microprocessor functional on-line self-test\n", "abstract": " Testing processor cores embedded in systems-on-chip (SoCs) is a major concern for industry nowadays. In this paper, we describe a novel solution which merges the SBST and BIST principles. The technique we propose forces the processor to execute a compact SBST-like test sequence by using a hardware module called MIcroprocessor Hardware Self-Test (MIHST) unit, which is intended to be connected to the system bus like a normal memory core, requesting no modification of the processor core internal structure. The benefit of using the MIHST approach is manifold: while guaranteeing the same or higher defect coverage of the traditional SBST approach, it reduces the time for test execution, better preserves the processor core Intellectual Property (IP), does not require the system memory to store the test program nor the test data, and can be easily adopted for non-concurrent on-line testing, since it minimizes\u00a0\u2026", "num_citations": "23\n", "authors": ["502"]}
{"title": "On the automation of the test flow of complex SoCs\n", "abstract": " Modern systems-on-chip (SoCs) allow integrating many different functional cores in the same piece of silicon. Their test requires taking fast decisions in the selection of structures and strategies at different stages of the design flow: early computation of area overhead, power consumption and test application time are indispensable in order to develop effective and efficient test for the overall chip, while taking into account physical constraints imposed by the available test equipment. Furthermore, once the test strategy has been selected and patterns generated for each module, additional nonnegligible effort is required to integrate the test program in an ATE-readable format. In this paper, we tackle these problems by means of a new software platform, leveraging descriptions of both the core-level test structure and the system-level requirements. Experimental results related to a realistic case of study underline the\u00a0\u2026", "num_citations": "23\n", "authors": ["502"]}
{"title": "Automatic test programs generation driven by internal performance counters\n", "abstract": " In the past performance counters have been available to top-end microprocessors as hardware luxuries for profiling critical applications. Today, on the contrary', several desktop microprocessors contain hardware support for monitoring performance events. This paper proposes a new approach to automatic test program generation that exploits such hardware to monitor specific micro-architectural events. In the approach, the generation tool repeatedly evaluates and improves candidate programs directly running on the target microprocessor: candidate programs are not \"simulated\", but rather \"executed\". The fast evaluation of candidate tests enables the use of an automatic methodology even on large designs. As a case study, an experiment targeting the Intel/spl reg/ Pentium/spl reg/ 4 microprocessor is reported.", "num_citations": "23\n", "authors": ["502"]}
{"title": "Hybrid soft error detection by means of infrastructure IP cores [SoC implementation]\n", "abstract": " High integration levels, coupled with the increased sensitivity to soft errors even at ground level, make the task of guaranteeing adequate dependability levels more difficult then ever. In this paper, we propose to adopt low-cost infrastructure-intellectual-property (I-IP) cores in conjunction with software-based techniques to perform soft error detection. Experimental results are reported that show the effectiveness of the proposed approach.", "num_citations": "23\n", "authors": ["502"]}
{"title": "Code generation for functional validation of pipelined microprocessors\n", "abstract": " Functional validation of pipelined microprocessors is a challenging task, as the behavior of a pipeline is determined by a sequence of instructions and by the interaction between their operands. This paper describes an approach to automatic test-program generation based on an evolutionary algorithm. The proposed methodology is able to tackle complex pipelined designs. Human intervention is limited to the formalized listing of the instruction set, and also internal parameters of the test program generator are auto-adapted. A prototype was built and exploited to generate test programs for the DLX/pII, a pipelined microprocessor. For the purpose of these experiments, test programs were devised trying to maximize the RT-level statement coverage. However, the method can be used to generate test programs on different target metrics. Results show the feasibility and effectiveness of the method.", "num_citations": "23\n", "authors": ["502"]}
{"title": "Analyzing SEU effects is SRAM-based FPGAsb\n", "abstract": " Commercial-off-the-shelf SRAM-based FPGA devices are becoming of interests for applications where high dependability and low cost are mandatory constraints. This paper proposes a new method for assessing the effects of SEUs in the device configuration memory. The method combines radiation testing for technology characterization and simulation-based fault injection for SEU propagation. Experimental results we gathered with the purpose of modeling the effects of SEUs in the FPGA configuration memory are reported and commented.", "num_citations": "23\n", "authors": ["502"]}
{"title": "Integrating BIST techniques for on-line SoC testing\n", "abstract": " Today's complex system-on-chip integrated circuits include a wide variety of functional IPs whose correct manufacturing must be guaranteed by IC producers. Infrastructure IPs are increasingly often inserted to achieve this purpose; such blocks, explicitly designed for test, are coupled with functional IPs both to obtain yield improvement during the manufacturing process and to perform volume production test. In some fields (e.g., the automotive one) there is a strong need for flexible and reusable test architectures able to guarantee effective and low-cost solutions for mission-mode fault detection capabilities within complex SoCs. In this paper, we propose to reuse structures inserted to support the manufacturing test to perform non-concurrent on-line test of SoCs. The feasibility of this approach and its costs have been evaluated on a real case of study including processor, memory and user defined logic cores.", "num_citations": "22\n", "authors": ["502"]}
{"title": "Early evaluation of bus interconnects dependability for system-on-chip designs\n", "abstract": " This paper presents a methodology for designing system-on-chip interconnection architectures providing a high level of protection from crosstalk and single-event upsets. An event driven simulator enriched with fault injection capabilities is exploited to evaluate the dependability level of the system being designed. The simulation environment supports several bus coding protocols and thus designers can easily evaluate different design alternatives. To enhance the dependability level of the interconnection architecture, we propose a distributed bus guardian scheme, where dedicated hardware modules monitor the integrity of the information transmitted over the bus and provide error correction mechanisms. Preliminary experimental results on a small benchmark system are reported showing the effectiveness of the proposed methodology.", "num_citations": "22\n", "authors": ["502"]}
{"title": "RT-level fault simulation techniques based on simulation command scripts\n", "abstract": " With the advent of new RT-level design and test flow, new tools are needed to migrate at the RT-level the activities of fault simulation, testability analysis, and test pattern generation. This paper focuses on fault simulation at the RT-level, and aims at exploiting the capabilities of commercial VHDL simulators to compute faulty responses without modifying the VHDL source code. The proposed approach was implemented as a prototypical tool, and experimental results show that simulation of a faulty circuit is no more costly than simulation of the original circuit. For defining RT-level faults, we adopted a refinement of the observabilityenhanced statement coverage metric. While this metric usually handles observability in an approximated way, we were able to efficiently and exactly determine the observability of single-bit stuck-at faults on all assignment statements.", "num_citations": "22\n", "authors": ["502"]}
{"title": "Scan insertion criteria for low design impact\n", "abstract": " The paper focuses an the constraints that the new silicon technologies impose on the implementation of partial and full scan approach. In particular the ordering of Flip-Flops inside each scan chain must be decided taking into account the capacitance constraints imposed by new technologies. The main goal of this paper is to prove that recent technologies impose a new design flow, exploiting layout information for scan chain reordering. Two algorithms are then described, which reduce both the average and the maximum distance between FFs in the chains, thus reducing the power dissipation of the circuit, too. Preliminary results, obtained through the implementation of the algorithms in the Italtel Design Environment and their application on a sample circuit, are reported.", "num_citations": "22\n", "authors": ["502"]}
{"title": "Are all social networks structurally similar?\n", "abstract": " The modern age has seen an exponential growth of social network data available on the web. Analysis of these networks reveal important structural information about these networks in particular and about our societies in general. More often than not, analysis of these networks is concerned in identifying similarities among social networks and how they are different from other networks such as protein interaction networks, computer networks and food web. In this paper, our objective is to perform a critical analysis of different social networks using structural metrics in an effort to highlight their similarities and differences. We use five different social network datasets which are contextually and semantically different from each other. We then analyze these networks using a number of different network statistics and metrics. Our results show that although these social networks have been constructed from different\u00a0\u2026", "num_citations": "21\n", "authors": ["502"]}
{"title": "A novel SBST generation technique for path-delay faults in microprocessors exploiting gate-and RT-level descriptions\n", "abstract": " This paper presents an innovative approach for the generation of functional programs to test path- delay faults within microprocessors. The proposed method takes advantage of both the gate- and RT-level description of the processor. The former is used to build binary decision diagrams (BDDs) for deriving fault excitation conditions; the latter is exploited for the automatic generation of test programs able to excite and propagate fault effects, based on an evolutionary algorithm and fast RTL simulation. Experimental results on a simple microcontroller show that the proposed methodology is able to generate suitable test sets in reduced times.", "num_citations": "21\n", "authors": ["502"]}
{"title": "Evolving cellular automata for self-testing hardware\n", "abstract": " Testing is a key issue in the design and production of digital circuits: the adoption of BIST (Built-In Self-Test) techniques is increasingly popular, but requires efficient algorithms for the generation of the logic which generates the test vectors applied to the Unit Under Test. This paper addresses the issue of identifying a Cellular Automaton able to generate input patterns to detect stuckat faults inside a Finite State Machine (FSM) circuit. Previous results already proposed a solution based on a Genetic Algorithm which directly identifies a Cellular Automaton able to reach good Fault Coverage of the stuck-at faults. However, such method requires 2-bit cells in the Cellular Automaton, thus resulting in a high area overhead. This paper presents a new solution, with an area occupation limited to 1 bit per cell; the improved results are possible due to the adoption of a new optimization algorithm, the Selfish Gene\u00a0\u2026", "num_citations": "21\n", "authors": ["502"]}
{"title": "A low-cost programmable board for speeding-up fault injection in microprocessor-based systems\n", "abstract": " This paper describes a low-cost, programmable board suited to support the fault injection process for microprocessor-based embedded systems. The board monitors the processor bus and is able to count the number of executed instructions, and can therefore be used to trigger the execution of the fault injection procedure. Moreover, the board can be of great help in gathering the information about the fault-free system behavior required to implement the fault collapsing rules described in a previous paper. When integrated in a fault injection environment, the board allows to perform at-speed fault injection experiments with minimum intrusiveness, and to implement effective fault collapsing procedures.", "num_citations": "21\n", "authors": ["502"]}
{"title": "A parallel genetic algorithm for automatic generation of test sequences for digital circuits\n", "abstract": " The paper deals with the problem of Automatic Generation of Test Sequences for digital circuits. Genetic Algorithms have been successfully proposed to solve this industrially critical problem; however, they have some drawbacks, e.g., they are often unable to detect some hard to test faults, and require a careful tuning of the algorithm parameters. In this paper, we describe a new parallel version of an existing GA-based ATPG, which exploits competing sub-populations to overcome these problems. The new approach has been implemented in the PVM environment and has been evaluated on a workstation network using some of the standard benchmark circuits. The results show that it is able to significantly improve the results quality (by testing some critical faults) at the expense of increased CPU time requirements.", "num_citations": "21\n", "authors": ["502"]}
{"title": "A hybrid approach for detection and correction of transient faults in SoCs\n", "abstract": " Critical applications based on Systems-on-Chip (SoCs) require suitable techniques that are able to ensure a sufficient level of reliability. Several techniques have been proposed to improve fault detection and correction capabilities of faults affecting SoCs. This paper proposes a hybrid approach able to detect and correct the effects of transient faults in SoC data memories and caches. The proposed solution combines some software modifications, which are easy to automate, with the introduction of a hardware module, which is independent of the specific application. The method is particularly suitable to fit in a typical SoC design flow and is shown to achieve a better trade-off between the achieved results and the required costs than corresponding purely hardware or software techniques. In fact, the proposed approach offers the same fault-detection and -correction capabilities as a purely software-based approach\u00a0\u2026", "num_citations": "20\n", "authors": ["502"]}
{"title": "Multi-level fault effects evaluation\n", "abstract": " Abstract                      The problem of analyzing the effects of transient faults in a digital system is very complex, and it may be addressed successfully only if it is performed at different steps of the design process. In this work we report and overview of fault injection, discussing which techniques are available that can be used starting from the early conception of the system and arriving to the transistor level.", "num_citations": "20\n", "authors": ["502"]}
{"title": "Initializability analysis of synchronous sequential circuits\n", "abstract": " This article addresses the problem of initializing synchronous sequential circuits, that is, of generating the shortest sequence able to drive the circuit to a known state, regardless of the initial state. Logic initialization is considered, being the only one compatible with current commercial tools. A hybrid Genetic Algorithm is proposed, which combines general ideas from evolutionary computation with specific techniques, well suited to the addressed problem. For the first time, experimental results provide data about the complete set of ISCAS'89 circuits, and show that, despite the inherent algorithm incompleteness, the method is capable of finding the optimum result for the considered circuits. A prototypical tool implementing the algorithm found better results than previous methods.", "num_citations": "20\n", "authors": ["502"]}
{"title": "Effectiveness and limitations of various software techniques for\" soft error\" detection: a comparative study\n", "abstract": " Deals with different software based strategies allowing the on-line detection of bit flip errors arising in microprocessor-based digital architectures as the consequence of the interaction with radiation. Fault injection experiments put in evidence the detection capabilities and the limitations of each of the studied techniques.", "num_citations": "20\n", "authors": ["502"]}
{"title": "Exploiting the selfish gene algorithm for evolving hardware cellular automata\n", "abstract": " Testing is a key issue in the design and production of digital circuits and the adoption of built-in self test techniques is increasingly popular. This paper shows an application in the field of electronic CAD of the Selfish Gene algorithm, an evolutionary algorithm based on a recent interpretation of the Darwinian theory. A three-phase optimization algorithm is exploited for determining the structure of a built-in self test architecture that is able to achieve good fault coverage results with a reduced area overhead. Experimental results show that the attained fault coverage is substantially higher than what can be obtained by previously proposed methods with comparable area requirements.", "num_citations": "20\n", "authors": ["502"]}
{"title": "A portable ATPG tool for parallel and distributed systems\n", "abstract": " The use of parallel architectures for the solution of CPU and memory critical problems in the electronic CAD area has been limited up to now by several factors, like the lack of efficient algorithms the reduced portability of the code, and the cost of the hardware. However, portable message-passing libraries are now available, and the same code runs on high-cost supercomputers, as well as on common workstation networks. The paper presents an effective ATPG system for large sequential circuits developed using the PVM library and based on a genetic algorithm. The tool, named GATTO has been run on a DEC Alpha AXP farm and on a CM-5. Experimental results are provided.", "num_citations": "20\n", "authors": ["502"]}
{"title": "Using symbolic techniques to find the maximum clique in very large sparse graphs\n", "abstract": " Several problems arising in CAD for VLSI, especially in logic and high level synthesis, are modeled as graph-theoretical problems. In particular minimization problems often require the knowledge of the cliques in a graph. This paper presents a new approach for finding the maximum clique in realistic graphs. The algorithm is built around a classical branch-and-bound, but exploits the efficiency of Binary Decision Diagrams and Symbolic Techniques to avoid explicit enumeration of the search space. The approach is proven to be more efficient than classical algorithms, which suffer from the enumeration problem, as well as purely symbolic implementations, which suffer from the explosion in the size of BDDs. As a result, we are able to compute the maximum clique without introducing approximations for graphs with billions of vertices and transitions.< >", "num_citations": "20\n", "authors": ["502"]}
{"title": "On the automatic generation of optimized software-based self-test programs for VLIW processors\n", "abstract": " Very long instruction word (VLIW) processors are increasingly employed in a large range of embedded signal processing applications, mainly due to their ability to provide high performances with reduced clock rate and power consumption. At the same time, there is an increasing request for efficient and optimal test techniques able to detect permanent faults in VLIW processors. Software-based self-test (SBST) methods are a consolidated and effective solution to detect faults in a processor both at the end of the production phase or during the operational life; however, when traditional SBST techniques are applied to VLIW processors, they may prove to be ineffective (especially in terms of size and duration), due to their inability to exploit the parallelism intrinsic in these architectures. In this paper, we present a new method for the automatic generation of efficient test programs specifically oriented to VLIW processors\u00a0\u2026", "num_citations": "19\n", "authors": ["502"]}
{"title": "Reliability analysis reloaded: How will we survive?\n", "abstract": " In safety related applications and in products with long lifetimes reliability is a must. Moreover, facing future technology nodes of integrated circuit device level reliability may decrease, i.e., counter-measures have to be taken to ensure product level reliability. But assessing the reliability of a large system is not a trivial task. This paper revisits the state-of-the-art in reliability evaluation starting from the physical device level, to the software system level, all the way up to the product level. Relevant standards and future trends are discussed.", "num_citations": "19\n", "authors": ["502"]}
{"title": "Automatic test bench generation for simulation-based validation\n", "abstract": " In current design practice synthesis tools play a key role, letting designers to concentrate on the specification of the system being designed by carrying out repetitive tasks such as architecture synthesis and technology mapping. However, in the new design flow, validation still remains a challenge: while new technologies based on formal verification are only marginally accepted for large designs, standard techniques based on simulation are beginning to fall behind the increased system complexity. This paper proposes an approach to simulation-based validation, in which an evolutionary algorithm computes useful input sequences to be included in the test bench. The feasibility of the proposed approach is assessed with a preliminary implementation of the proposed algorithm.", "num_citations": "19\n", "authors": ["502"]}
{"title": "Optimizing deceptive functions with the SG-Clans algorithm\n", "abstract": " Starting from a different view of natural evolution, namely that of English biologist R. Dawkins, called the selfish gene theory, a new evolutionary computation approach can be developed, the selfish gene (SG) algorithm. This paper presents a significant improvement to the SG algorithm that is able to find and exploit linkages among different genes thanks to the evolution of isolated groups called clans. The resulting SG-Clans algorithm is shown to be able to find the absolute maximum of Holland Royal Road functions, which were specifically designed to create insurmountable difficulties for a wide class of hill-climbing approaches. We support experimental evidence that SG-Clans shares the speed of a hill-climber with the ability of broadly exploring the search space.", "num_citations": "19\n", "authors": ["502"]}
{"title": "VEGA: a verification tool based on genetic algorithms\n", "abstract": " While modern state-of-the-art optimization techniques can handle designs with up to hundreds of flip-flops, equivalence verification is still a challenging task in many industrial design flows. This paper presents a new verification methodology that, while sacrificing exactness, is able to handle larger circuits and give designers the opportunity to trade off CPU time with confidence on the result. The proposed methodology is able to fruitfully support an exact verification tool, dramatically increasing the confidence on the validity of an optimization process. A prototypical tool has been developed and preliminary experimental results that support this claim are shown in the paper.", "num_citations": "19\n", "authors": ["502"]}
{"title": "Circular self-test path for FSMs\n", "abstract": " Circular self test path (CSTP) is an attractive method for automatically transforming sequential circuits generated by automatic synthesis tools into BIST structures. The authors extend this method-making it more suitable for FSMs derived from synthesized control parts-and are integrating it into an industrial design flow supporting testable synthesis. The CSTP approach provides good results in terms of test length and fault coverage in large circuits. It requires substitution of all or some of the flip-flops in the circuit with special cells and their connection to constitute a circular chain. CSTP also has application in industrial environments, and several commercial CAE environments, such as that used by AT&T, now support CSTP as an approach for automatic introduction of BIST in circuits.", "num_citations": "19\n", "authors": ["502"]}
{"title": "Improving topological ATPG with symbolic techniques\n", "abstract": " This paper presents a new approach to Automatic Test Pattern Generation for sequential circuits. Traditional topological algorithms nowadays are able to deal with very large circuits, but often fail when highly sequential subnetworks are found. On the other hand, symbolic techniques based on Binary Decision Diagrams proved themselves very efficient on small or medium circuits, no matter their sequential complexity. A state-of-the-art structural ATPG is extended by identifying some critical areas in the circuit and resorting to symbolic techniques when such areas need to be considered. Experimental results prove that the combined approach considerably enhances fault coverage while reducing CPU time when compared to a purely topological approach.", "num_citations": "19\n", "authors": ["502"]}
{"title": "On test program compaction\n", "abstract": " While compaction of binary test sequences for generic sequential circuits has been widely explored, the compaction of test programs for processor-based systems is still an open area of research. Test program compaction is practically important because there are several scenarios in which Software-based Self-Test (SBST) is adopted, and the size of the test program is often a critical parameter. This paper is among the first to propose algorithms able to automatically compact an existing test program. The proposed solution is based on instruction removal and restoration, which is shown to significantly reduce the computational cost compared with instruction removal alone. Experimental results are reported, showing the compaction capabilities and computational costs of the proposed algorithms.", "num_citations": "18\n", "authors": ["502"]}
{"title": "On the functional test of the register forwarding and pipeline interlocking unit in pipelined processors\n", "abstract": " When the result of a previous instruction is needed in the pipeline before it is available, a \u201cdata hazard\u201d occurs. Register Forwarding and Pipeline Interlock (RF&PI) are mechanisms suitable to avoid data corruption and to limit the performance penalty caused by data hazards in pipelined microprocessors. Data hazards handling is part of the microprocessor control logic; its test can hardly be achieved with a functional approach, unless a specific test algorithm is adopted. In this paper we analyze the causes for the low functional testability of the RF&PI logic and propose some techniques able to effectively perform its test. In particular, we describe a strategy to perform Software-Based Self-Test (SBST) on the RF&PI unit. The general structure of the unit is analyzed, a suitable test algorithm is proposed and the strategy to observe the test responses is explained. The method can be exploited for test both at the end of\u00a0\u2026", "num_citations": "18\n", "authors": ["502"]}
{"title": "Online hardening of programs against SEUs and SETs\n", "abstract": " Processor cores embedded in systems-on-a-chip (SoCs) are often deployed in critical computations, and when affected by faults they may produce dramatic effects. When hardware hardening is not cost-effective, software implemented hardware fault tolerance (SIHFT) can be a solution to increase SoCs' dependability. However, SIHFT increases the time for running the hardened application, and the memory occupation. In this paper we propose a method that eliminates the memory overhead, using a new approach to instruction hardening and control flow checking during the execution of the application, without the need for introducing any change in its source code. The proposed method is also non-intrusive, since it does not require any modification in the main processor's architecture. The method is suitable for hardening SoCs against transient faults and also for detecting permanent faults", "num_citations": "18\n", "authors": ["502"]}
{"title": "Early, accurate dependability analysis of CAN-based networked systems\n", "abstract": " Many safety-critical applications today rely on computer-based systems in which several computing nodes communicate through a network backbone. As the complexity of the systems under analysis grows, designers must devise fault-injection models that strike a balance between two conflicting requirements: On the one hand, models should be as close as possible to a system's physical implementation to reflect precisely the effects of real faults. On the other hand, abstract, easily manageable models minimize the time required for the fault-injection experiments, letting designers analyze sets of faults wide enough to provide statistically meaningful information. In addressing this issue, we have devised a fault-injection environment to study the effects of soft errors in CAN networks. Our cosimulation environment consists of two modules. The first, a traffic generator module implemented in software, emulates the\u00a0\u2026", "num_citations": "18\n", "authors": ["502"]}
{"title": "Efficient estimation of SEU effects in SRAM-based FPGAs\n", "abstract": " SRAM-based FPGAs are becoming very appealing for several applications where high dependability is a mandatory requirement. Unfortunately, the technology of SRAM-based FPGAs is very sensitive to single event upsets (SEUs) and particular concerns arise from SEUs affecting the FPGAs' configuration memory. In this paper we propose a new method for assessing the impact of faults in the configuration memory on the FPGA dependability. The method uses static analysis, thus reducing greatly the time for performing dependability evaluation.", "num_citations": "18\n", "authors": ["502"]}
{"title": "Test program generation from high-level microprocessor descriptions\n", "abstract": " This chapter describes and analyzes a methodology for gathering together test-programs for microprocessor cores during the complete design cycle starting from early design phases. The methodology is based on an almost automatic tool and could be applied to generate test-programs for stand-alone microprocessor cores as well as for these embedded in systems-on-chip. The main idea is to take advantage of all possible microprocessor descriptions delivered through the whole design cycle to generate test-programs able to achieve a high FC% at gate-level. Most of the efforts of the methodology presented are focused on test program generation from high-level microprocessor descriptions. A case study is presented tackling a pipelined microprocessor core.", "num_citations": "18\n", "authors": ["502"]}
{"title": "A P1500-compatible programmable BIST approach for the test of Embedded Flash Memories\n", "abstract": " In this paper we present a microprocessor-based approach suitable for embedded flash memory testing in a System-on-a-chip (SoC) environment. The main novelty of the approach is the high flexibility, which guarantees easy exploitation of the same architecture to different memory cores. The proposed approach is compatible with the P1500 standard. A case study has been developed and demonstrates the advantages of the proposed core test strategy in terms of area overhead and test application time.", "num_citations": "18\n", "authors": ["502"]}
{"title": "A P1500 compliant BIST-based approach to embedded RAM diagnosis\n", "abstract": " This paper deals with the diagnosis of faulty embedded RAMs and outlines the solution which is currently under evaluation within STMicroelectronics. The proposed solution exploits a BIST module implementing a March algorithm, defines a wrapper allowing its interface with a TAP controller, and describes a diagnostic procedure running in the external ATE software environment. The approach allows one to test multiple modules in the same chip through a single TAP interface and is compliant with the proposed P1500 standard for Embedded Core Test. Some preliminary experimental results gathered using a sample circuit are reported, showing the effectiveness of the proposed solution in terms of area and time requirements.", "num_citations": "18\n", "authors": ["502"]}
{"title": "Partial scan flip flop selection for simulation-based sequential ATPGs\n", "abstract": " The partial scan approach is now widely adopted and several commercial tools support this technique. However, there is no general agreement on how to select the scan flip flops, in general each technique is tailored to a particular ATPG algorithm and results effective when coupled with the right ATPG tool. In this paper, we propose an approach suitable for GA-based ATPGs, which is barred on exploiting some information coming from the ATPG itself we compare the results of our method with the ones of the approach based on cutting the topological loops and use a GA-based ATPG to demonstrate its effectiveness in terms of fault coverage and CPU time.", "num_citations": "18\n", "authors": ["502"]}
{"title": "A parallel tester architecture for accelerometer and gyroscope MEMS calibration and test\n", "abstract": " This paper describes a tester architecture for Accelerometer and Gyroscope Micro-ElectroMechanical System (MEMS) devices test and calibration, allowing increased parallelism rate and process accuracy. The proposed tester architecture tackles some critical issues related to MEMS testing, such as mitigating mechanical concerns that potentially impact on the equipment Mean Time Between Maintenance and guaranteeing a sufficient number of measurements in the time unit. The proposed strategy consists in an innovative and low cost tester resource partitioning that overcomes current limitations to multisite Accelerometer and Gyroscope MEMS testing. A tester prototype was implemented exploiting FPGAs; feasibility and effectiveness of the proposed methodology was demonstrated on commercial accelerometer and gyroscope MEMS devices.", "num_citations": "17\n", "authors": ["502"]}
{"title": "An automated methodology for cogeneration of test blocks for peripheral cores\n", "abstract": " Test of peripheral modules has not yet been deeply investigated by the research community. When embedded in a system on a chip, peripheral cores introduce new issues for post-production testing. A peripheral core embedded in a SoC requires a test set able to properly perform two different tasks: configure the device in different operation modes and properly exercise it. In this paper an automatic approach able to generate test sets for peripheral cores embedded in a SoC is described. The presented approach is based on an evolutionary algorithm that exploits high-level simulation and gathers coverage metrics information to produce the test sets. The method compares favorably with results obtained by hand.", "num_citations": "17\n", "authors": ["502"]}
{"title": "On-line Detection of Control-Flow Errors in SoCs by means of an Infrastructure IP core\n", "abstract": " In sub-micron technology circuits high integration levels coupled with the increased sensitivity to soft errors even at ground level make the task of guaranteeing systems' dependability more difficult than ever. In this paper we present a new approach to detect control-flow errors by exploiting a low-cost infrastructure intellectual property (I-IP) core that works in cooperation with software-based techniques. The proposed approach is particularly suited when the system to be hardened is implemented as a system-on-chip (SoC), since the I-IP can be added easily and it is independent on the application. Experimental results are reported showing the effectiveness of the proposed approach.", "num_citations": "17\n", "authors": ["502"]}
{"title": "A genetic algorithm-based system for generating test programs for microprocessor ip cores\n", "abstract": " The current digital systems design trend is quickly moving toward a design-and-reuse paradigm. In particular, intellectual property cores are becoming widely used. Since the cores are usually provided as encrypted gate-level netlist, they raise several testability problems. The authors propose an automatic approach targeting processor cores that, by resorting to genetic algorithms, computes a test program able to attain high fault coverage figures. Preliminary results are reported to assess the effectiveness of our approach with respect to a random approach.", "num_citations": "17\n", "authors": ["502"]}
{"title": "FlexFi: A flexible fault injection environment for microprocessor-based systems\n", "abstract": " Microprocessor-based systems are increasingly used to control safety-critical systems (e.g., air and railway traffic control, nuclear plant control, aircraft and car control). In this case, fault tolerance mechanisms are introduced at the hardware and software level. Debugging and verifying the correct design and implementation of these mechanisms ask for effective environments, and Fault Injection represents a viable solution for their implementation. In this paper we present a flexible environment suitable to compute the fault coverage provided by hardware and software mechanisms existing in most microprocessor-based systems. The environment, called FlexFI, is flexible, since it allows the adoption of different solutions for implementing the most critical modules, which differ in terms of cost, speed, and intrusiveness in the original system behavior.", "num_citations": "17\n", "authors": ["502"]}
{"title": "A genetic algorithm for automatic generation of test logic for digital circuits\n", "abstract": " Testing is a key issue in the design and production of digital circuits: the adoption of BIST (Built-in Self-Test) techniques is increasingly popular, but sometimes requires efficient algorithms for the automatic generation of the logic which generates the test vectors applied to the unit under test. This paper addresses the issue of identifying a cellular automaton able to generate input patterns to detect stuck-at faults inside a finite state machine (FSM). A suitable hardware structure is first identified. A genetic algorithm is then proposed, which directly identifies a cellular automaton able to reach a very good fault coverage of the stuck-at faults. The novelty of the method consists in combining the generation of test patterns with the synthesis of a cellular automaton able to reproduce them. Experimental results are provided, which show that in most of the standard benchmark circuits the cellular automaton selected by the\u00a0\u2026", "num_citations": "17\n", "authors": ["502"]}
{"title": "FlexGripPlus: An improved GPGPU model to support reliability analysis\n", "abstract": " General Purpose Graphics Processing Units (GPGPUs) have been extensively used in the last decade as accelerators in high demanding applications, such as multimedia processing and high-performance computing. Nowadays, these devices are becoming popular even in safety-critical applications, such as in autonomous and semi-autonomous vehicles. However, these devices can suffer from the effects of transient faults, such as those produced by radiation effects. Among those effects, Single Event Upsets (SEUs), which are the focus of this paper, can cause application misbehaviors, which may lead to catastrophic consequences. In this work, we first describe how we extended the capabilities of an open-source VHDL GPGPU model (FlexGrip) and developed a new version named FlexGripPlus to study and analyze the effects of SEUs in a GPGPU in a much more detailed manner. We also performed\u00a0\u2026", "num_citations": "16\n", "authors": ["502"]}
{"title": "A low-cost reliability vs. cost trade-off methodology to selectively harden logic circuits\n", "abstract": " Selecting the ideal trade-off between reliability and cost associated with a fault tolerant architecture generally involves an extensive design space exploration. Employing state-of-the-art reliability estimation methods makes this exploration un-scalable with the design complexity. In this paper we introduce a low-cost reliability analysis methodology that helps taking this key decision with less computational effort and orders of magnitude faster. Based on this methodology we also propose a selective hardening technique using a hybrid fault tolerant architecture that allows meeting the soft-error rate constraints within a given design cost-budget and vice versa. Our experimental validation shows that the methodology offers huge gain (1200 \u00d7) in terms of computational effort in comparison with fault injection-based reliability estimation method and produces results within acceptable error limits.", "num_citations": "16\n", "authors": ["502"]}
{"title": "New techniques to reduce the execution time of functional test programs\n", "abstract": " The compaction of test programs for processor-based systems is of utmost practical importance: Software-Based Self-Test (SBST) is nowadays increasingly adopted, especially for in-field test of safety-critical applications, and both the size and the execution time of the test are critical parameters. However, while compacting the size of binary test sequences has been thoroughly studied over the years, the reduction of the execution time of test programs is still a rather unexplored area of research. This paper describes a family of algorithms able to automatically enhance an existing test program, reducing the time required to run it and, as a side effect, its size. The proposed solutions are based on instruction removal and restoration, which is shown to be computationally more efficient than instruction removal alone. Experimental results demonstrate the compaction capabilities, and allow analyzing computational costs\u00a0\u2026", "num_citations": "16\n", "authors": ["502"]}
{"title": "Identification and rejuvenation of nbti-critical logic paths in nanoscale circuits\n", "abstract": " The Negative Bias Temperature Instability (NBTI) phenomenon is agreed to be one of the main reliability concerns in nanoscale circuits. It increases the threshold voltage of pMOS transistors, thus, slows down signal propagation along logic paths between flip-flops. NBTI may cause intermittent faults and, ultimately, the circuit\u2019s permanent functional failures. In this paper, we propose an innovative NBTI mitigation approach by rejuvenating the nanoscale logic along NBTI-critical paths. The method is based on hierarchical identification of NBTI-critical paths and the generation of rejuvenation stimuli using an Evolutionary Algorithm. A new, fast, yet accurate model for computation of NBTI-induced delays at gate-level is developed. This model is based on intensive SPICE simulations of individual gates. The generated rejuvenation stimuli are used to drive those pMOS transistors to the recovery phase, which are\u00a0\u2026", "num_citations": "16\n", "authors": ["502"]}
{"title": "A new hybrid nonintrusive error-detection technique using dual control-flow monitoring\n", "abstract": " Hybrid error-detection techniques combine software techniques with an external hardware module that monitors the execution of a microprocessor. The external hardware module typically observes the control flow at the input or at the output of the microprocessor and compares it with the expected one. This paper proposes a new hybrid technique that monitors the control flow at both points and compares them to detect possible errors. The proposed approach does not require any software modification to detect control-flow errors. Fault-injection campaigns have been performed on an LEON3 microprocessor. The results show full control-flow error detection with no performance degradation and small area overhead. A complete solution can be obtained by complementing the proposed approach with software fault-tolerance techniques for data errors.", "num_citations": "16\n", "authors": ["502"]}
{"title": "Fault list compaction through static timing analysis for efficient fault injection experiments\n", "abstract": " With the adoption of deep sub-micron technologies, faults modeled as single event transients (SETS) on combinational gates are becoming an issue, but efficient and accurate techniques for assessing their impact on VLSI designs are still missing. This paper presents a new approach for generating the list of faults to be addressed during fault injection experiments tackling SET effects. By resorting to static timing analysis, the approach is able to prune the set of possible faults and to identify a superset of the ones that may produce effects on the circuit outputs. Experimental results are reported on standard benchmarks assessing the effectiveness of the proposed approach.", "num_citations": "16\n", "authors": ["502"]}
{"title": "Exploiting the selfish gene algorithm for evolving cellular automata\n", "abstract": " This paper shows an application in the field of Electronic CAD of the Selfish Gene algorithm, an evolutionary algorithm based on a recent interpretation of the Darwinian theory. Testing is a key issue in the design and production of digital circuits and the adoption of Built-In Self-Test (BIST) techniques is increasingly popular. In this paper, the Selfish Gene algorithm is adopted for determining the logic for a BIST architecture based on Cellular Automata (CA). A Genetic Algorithm has already been proposed for identifying good BIST architectures based on CA. However, by adopting 2-bit cells, such a method introduced a significant area overhead. Thanks to the adoption of the new and more powerful search engine, we were able to identify simpler BIST structures with a lower area overhead, but still able to obtain the same fault coverage.", "num_citations": "16\n", "authors": ["502"]}
{"title": "A new approach for initialization sequences computation for synchronous sequential circuits\n", "abstract": " This paper presents a new approach to the automated generation of an initialization sequence for synchronous sequential circuits. Finding an initialization sequence is a hard task when a global reset signal is not available, and functional techniques often cannot handle large circuits. We propose a Genetic Algorithm providing a sequence that aims at initializing the highest number of flip flops with the lowest number of vectors. The experimental results we provide shore that the approach is feasible to be applied even to the largest benchmark circuits and that it compares well to other known approaches in terms of initialized flip flops and sequence length.", "num_citations": "16\n", "authors": ["502"]}
{"title": "On-line Testing of an Off-the-shelf Microprocessor Board for Safety-critical Applications\n", "abstract": " The paper describes the strategy adopted to implement on-line test procedures for a commercial microprocessor board used in an automated light-metro control system. Special care has been devoted to chose the most effective test strategy for memory elements, processors, and caches, while guaranteeing a minimum impact on the normal behavior of the whole system. Implementation of the described techniques will significantly improve the system ability to safely react to possible faults. This will be quantitatively determined in the subsequent dependability evaluation phase.", "num_citations": "16\n", "authors": ["502"]}
{"title": "Exploiting competing subpopulations for automatic generation of test sequences for digital circuits\n", "abstract": " The paper describes the application of a Parallel Genetic Algorithm to Automatic Test Pattern Generation (ATPG) for digital circuits. Genetic Algorithms have been already proposed to solve this industrially critical problem, both on mono- and multi-processor architectures. Although preliminary results are very encouraging, there are some obstacles which limit their use: in particular, GAs are often unable to detect some hard to test faults, and require a careful tuning of the algorithm parameters. In this paper, we describe a new parallel version of an existing GA-based ATPG, which exploits competing sub-populations to overcome these problems. The new approach has been implemented in the PVM environment and has been evaluated on a workstation network using standard benchmark circuits. Preliminary results show that it is able to improve the results quality (by testing additional critical faults) at the\u00a0\u2026", "num_citations": "16\n", "authors": ["502"]}
{"title": "A simulation-based approach to test pattern generation for synchronous sequential circuits\n", "abstract": " Particular design environments, e.g., those based on partial scan, may prevent design for testability techniques from reducing testing to a combinational problem: ATPG for sequential devices thus remains a challenge. Random and deterministic structure-oriented techniques are state-of-the-art, but there is a growing interest in methods that resort to the automaton of the circuit. The authors present SETA, a sequential test generator based on automata, an ATPG applicable to synchronous circuits working in the fundamental mode. SETA generates test patterns while trying to disprove the equivalence of two automata. SETA is simulation-based: within the theoretical framework of the product machine, state-of-the-art simulation techniques are used to yield satisfactory experimental results on the ISCAS89 benchmark set.< >", "num_citations": "16\n", "authors": ["502"]}
{"title": "Sequential circuit diagnosis based on formal verification techniques\n", "abstract": " 1-This paper presents a multi-node 2-Dimensional (2D) time-of-arrival (TOA) and direction-of-arrival (DOA) optimal fusion technique. This technique can be applied in ad-hoc networks, especially suitable for the application in the mobile ad-hoc networks (MANETs). In this work, positioning error in MANETs would be optimized via TOA-DOA joint estimation and fusion across multiple nodes. In the proposed MANET, we assume two categories of nodes: Those equipped with antenna arrays (base-nodes) and those equipped with omni-directional antennas (target-nodes). All nodes are capable of communicating with other nodes. Base-nodes are capable of positioning (TOA-DOA estimation) other nodes located in their coverage area. A fusion method is proposed to minimize the mean square of the positioning error of a target-node, when more than one base-node estimates its position. The fusion scheme is derived\u00a0\u2026", "num_citations": "16\n", "authors": ["502"]}
{"title": "Evaluating alpha-induced soft errors in embedded microprocessors\n", "abstract": " This paper presents the results of alpha single event upsets tests of an embedded 8051 microprocessor. Cross sections for the different memory resources (i.e., internal registers, code RAM, and user memory) are reported as well as the error rate for different codes implemented as test benchmarks. Test results are then discussed to find the contribution of each available resource to the overall device error rate.", "num_citations": "15\n", "authors": ["502"]}
{"title": "System-level test and validation of hardware/software systems\n", "abstract": " New manufacturing technologies have made possible the integration of entire systems on a single chip. This new design paradigm, termed system-on-chip (SOC), together with its associated manufacturing problems, represents a real challenge for designers. SOC is also reshaping approaches to test and validation activities. These are beginning to migrate from the traditional register-transfer or gate levels of abstraction to the system level. Until now, test and validation have not been supported by system-level design tools so designers have lacked the infrastructure to exploit all the benefits stemming from the adoption of the system level of abstraction. Research efforts are already addressing this issue. This monograph provides a state-of-the-art overview of the current validation and test techniques by covering all aspects of the subject including: modeling of bugs and defects; stimulus generation for validation and test purposes (including timing errors; design for testability.", "num_citations": "15\n", "authors": ["502"]}
{"title": "System safety through automatic high-level code transformations: an experimental evaluation\n", "abstract": " This paper deals with a software modification strategy allowing the on-line detection of transient errors. Being based on a set of rules for introducing redundancy in the high-level code, the method can be completely automated, and is particularly suited for low-cost safety-critical microprocessor-based applications. Experimental results from software and hardware fault injection campaigns are presented and discussed, demonstrating the effectiveness of the approach in terms of fault detection capabilities.", "num_citations": "15\n", "authors": ["502"]}
{"title": "Evolving effective CA/CSTP: BIST architectures for sequential circuits\n", "abstract": " Circular self-test path, an industrial standard solution for built-in self test of digital circuits, is very effective, involves low hardware overhead, low time overhead, performs an at-speed testing, and can be easily adapted to most design flows. However, in some cases, it fails and provides unacceptably low fault coverage. This paper proposes a modification of this architecture able to remove some of its drawbacks. The new architecture is based on nonlinear, hybrid cellular automata and exploits an evolutionary algorithm for selecting cell rules. Experimental results on standard benchmarks show the effectiveness of the approach.", "num_citations": "15\n", "authors": ["502"]}
{"title": "Evaluating system dependability in a co-design framework\n", "abstract": " The widespread adoption of embedded microprocessor-based systems for safety critical applications mandates the use of co-design tools able to evaluate system dependability at every steps of the design cycle. In this paper, we describe how Fault Injection techniques have been integrated in an existing co-design tool and which advantages come from the availability of such an enhanced tool. The effectiveness of the proposed tool is assessed on a simple case study.", "num_citations": "15\n", "authors": ["502"]}
{"title": "Fault injection for embedded microprocessor-based systems\n", "abstract": " Microprocessor-based embedded systems are increasingly used to control safetycritical systems (eg, air and railway traffic control, nuclear plant control, aircraft and car control). In this case, fault tolerance mechanisms are introduced at the hardware and software level. Debugging and verifying the correct design and implementation of these mechanisms ask for effective environments, and Fault Injection represents a viable solution for their implementation. In this paper we present a Fault Injection environment, named FlexFI, suitable to assess the correctness of the design and implementation of the hardware and software mechanisms existing in embedded microprocessor-based systems, and to compute the fault coverage they provide. The paper describes and analyzes different solutions for implementing the most critical modules, which differ in terms of cost, speed, and intrusiveness in the original system behavior.", "num_citations": "15\n", "authors": ["502"]}
{"title": "About the functional test of the GPGPU scheduler\n", "abstract": " General Purpose Graphical Processing Units (GPGPUs) are increasingly used in safety critical applications such as the automotive ones. Hence, techniques are required to test them during the operational phase with respect to possible permanent faults arising when the device is already deployed in the field. Functional tests adopting Software-based Self-test (SBST) are an effective solution since they provide benefits in terms of intrusiveness, flexibility and test duration. While the development of the functional test code addressing the several computational cores composing a GPGPU can be done resorting to known methods developed for CPUs, for other modules which are typical of a GPGPU we still miss effective solutions. This paper focuses on one of the most relevant module consists on the scheduler core which is in charge of managing different scalar computational cores and the different executed threads\u00a0\u2026", "num_citations": "14\n", "authors": ["502"]}
{"title": "On the automatic generation of test programs for path-delay faults in microprocessor cores\n", "abstract": " Delay testing is mandatory for guaranteeing the correct behavior of today's high-performance microprocessors. Several methodologies have been proposed to tackle this issue resorting to additional hardware or to software self test techniques. Software techniques are particularly promising as they resort to Assembly programs in normal mode of operation, without requiring circuit modifications; however, the problem of generating effective and efficient test programs for path- delay fault detection is still open. This paper presents an innovative approach for the generation of path-delay self-test programs for microprocessors, based on an evolutionary algorithm and on ad-hoc software simulation/hardware emulation heuristic techniques. Experimental results show how the proposed methodology allows generating suitable test programs in reasonable times.", "num_citations": "14\n", "authors": ["502"]}
{"title": "Testing logic cores using a BIST P1500 compliant approach: a case of study\n", "abstract": " In this paper we describe how we applied a BIST-based approach to the test of a logic core to be included in system-on-a-chip (SoC) environments. The approach advantages are the ability to protect the core IP, the simple test interface (thanks also to the adoption of the P1500 standard), the possibility to run the test at-speed, the reduced test time, and the good diagnostic capabilities. The paper reports figures of the achieved fault coverage, the required area overhead, and the performance slowdown, and compares the figures with those for alternative approaches, such as those based on full scan and sequential ATPG.", "num_citations": "14\n", "authors": ["502"]}
{"title": "A multi-level approach to the dependability analysis of networked systems based on the CAN protocol\n", "abstract": " Safety-critical applications are now common where both digital and mechanical components are deployed, as in the automotive fields. The analysis of the dependability of such systems is a particularly complex task that mandates modeling capabilities in both the discrete and in the continuous domains. To tackle this problem a multi-level approach is presented here, which is based on abstract functional models to capture the behavior of the whole system, and on detailed structural models to cope with the details of system components. In this paper we describe how the interaction between the two levels of abstraction is managed to provide accurate analysis of the dependability of the whole system. In particular, the proposed technique is shown to be able to identify faults affecting the CAN network whose effects are most likely to be critical for vehicle's dynamic. Exploiting the information about the effects of these\u00a0\u2026", "num_citations": "14\n", "authors": ["502"]}
{"title": "A BIST-based solution for the diagnosis of embedded memories adopting image processing techniques\n", "abstract": " This paper proposes a new solution for the diagnosis of faults into embedded RAMs, currently under evaluation within STMicroelectronics. The proposed scheme uses dedicated circuitry embedded in a BIST wrapper, and an ATE test program to schedule the data extraction flow and to analyze the gathered information. Testing is performed exploiting a standard IEEE 1149.1 TAP, which allows the access to multiple memory cores with a P1500 compliant solution. The approach aims at implementing a low-cost solution to diagnose embedded RAMs with the goal to minimize the ATE costs and the time required to extract the diagnostic information. In our approach, the ATE drives the diagnostic scheme and performs the classification of faults, allowing the adoption of low-cost equipments. The proposed solution allows a scalable extraction of test data, whose amount is proportional to the available testing time\u00a0\u2026", "num_citations": "14\n", "authors": ["502"]}
{"title": "Dependability analysis of CAN networks: an emulation-based approach\n", "abstract": " Today many safety-critical applications are based on distributed systems where several computing nodes exchange information via suitable network interconnections. An example of this class of applications is the automotive field, where developers are exploiting the CAN protocol for implementing the communication backbone. The capability of accurately evaluating the dependability properties of such a kind of systems is today a major concern. In this paper we present a new environment that can be fruitfully exploited to assess the effects of faults in CAN-based networks. The entire network is emulated via an ad-hoc hardware/software system that allows easily evaluating the effects of faults in all the network components, namely the network nodes, the protocol controllers and the transmission channel. In this paper, we report a detailed description of the environment we set-up and we present some preliminary\u00a0\u2026", "num_citations": "14\n", "authors": ["502"]}
{"title": "FPGA-based fault injection techniques for fast evaluation of fault tolerance in VLSI circuits\n", "abstract": " Designers of safety-critical VLSI systems are asking for effective tools for evaluating and validating their designs. Fault Injection is commonly adopted for this task, and its effectiveness is therefore a key factor. In this paper we propose to exploit FPGAs to speed-up Fault Injection for fault tolerance evaluation of VLSI circuits. A complete Fault Injection environment is described, relying on FPGA-based emulation of the circuit for fault effect analysis. The proposed approach allows combining the efficiency of hardware-based techniques, and the flexibility of simulation-based techniques. Experimental results are provided to support the feasibility and effectiveness of the approach.", "num_citations": "14\n", "authors": ["502"]}
{"title": "A genetic algorithm for the computation of initialization sequences for synchronous sequential circuits\n", "abstract": " Testing circuits which do not include a global reset signal requires either complex ATPG algorithms based on 9- or even 256-valued algebras, or some suitable method to generate initialization sequences. This paper follows the latter approach, and presents a new method to the automated generation of an initialization sequence for synchronous sequential circuits. We propose a Genetic Algorithm providing a sequence that aims at initializing the highest number of flip flops with the lowest number of vectors. The experimental results show that the approach is feasible to be applied even to the largest benchmark circuits and that it compares well to other known approaches in terms of initialized flip flops and sequence length. Finally, this paper shows how the initialization sequences can be fruitfully exploited by simplifying the ATPG process.", "num_citations": "14\n", "authors": ["502"]}
{"title": "Optimizing area loss in flat glass cutting\n", "abstract": " This paper describes GGOAL, a genetic algorithm for the minimization of glass loss in cutting large sheets into several pieces. The algorithm takes into account several industrial constraints, stemming both from the glass cutting technology and from the requirement that the optimization must run in real-time, concurrently with the cutting operation. The algorithm delivers comparable or better results than optimization procedures embedded in comprehensive commercial software systems, and is now distributed with all flat glass cutting machines sold by Bottero SpA.", "num_citations": "14\n", "authors": ["502"]}
{"title": "Testing permanent faults in pipeline registers of GPGPUs: A multi-kernel approach\n", "abstract": " In the last decade, General Purpose Graphics Processing Units (GPGPUs) have been widely employed in high demanding data processing applications including multimedia and high-performance computing due to their parallel processing capabilities. Nowadays, these devices are considered as promising solutions also for high-performance safety-critical applications, such as autonomous and semi-autonomous vehicles. Current GPGPUs are designed targeting challenging execution requirements, e.g., related to performance and power constraints, forcing designers to use aggressive technology scaling solutions. Nevertheless, some implementation technologies are prone to introduce faults in the device during the operative life adding unaffordable effects and errors for the safety-critical domain. Hence, effective in-field test solutions are required to guarantee the target reliability levels. In this paper, we propose\u00a0\u2026", "num_citations": "13\n", "authors": ["502"]}
{"title": "An extended model to support detailed GPGPU reliability analysis\n", "abstract": " General Purpose Graphics Processing Units (GPGPUs) have been used in the last decades as accelerators in high demanding data processing applications, such as multimedia processing and high-performance computing. Nowadays, these devices are becoming popular even in safety-critical applications, such as autonomous and semi-autonomous vehicles. However, these devices can suffer from the effects of transient faults, such as those produced by radiation effects. These effects can be represented in the system as Single Event Upsets (SEUs) and are able to generate intolerable application misbehaviors in safety critical environments. In this work, we extended the capabilities of an open-source VHDL GPGPU model (FlexGrip) in order to study and analyze in a much more detailed manner the effects of SEUs in some critical modules within a GPGPU. Simulation results showed that scheduler controller has\u00a0\u2026", "num_citations": "13\n", "authors": ["502"]}
{"title": "Test of reconfigurable modules in scan networks\n", "abstract": " Modern devices often include several embedded instruments, such as BIST interfaces, sensors, calibration facilities. New standards, such as IEEE Std 1687, provide vehicles to access these instruments. In approaches based on reconfigurable scan networks (RSNs), instruments are coupled with scan registers, connected into chains and interleaved with reconfigurable modules. Such modules embed reconfigurable multiplexers that permit a selective access to different parts of the chain. A similar scenario is also supported by IEEE Std 1149.1-2013. The test of permanent faults affecting an RSN requires to shift test vectors throughout a certain number of network configurations. This paper presents some methodologies to select the list of configurations that perform the complete test of the reconfigurable modules of the RSN. In particular, one method is presented that, by construction, can be proved to be able to apply\u00a0\u2026", "num_citations": "13\n", "authors": ["502"]}
{"title": "An automatic approach to perform the verification of hardware designs according to the ISO26262 functional safety standard\n", "abstract": " With the increasing adoption of embedded systems in critical automotive applications, the verification of hardware designs reliability is becoming a strictly regulated process where the ISO26262 standard plays a key role. Today crucial verification activities such as failure analysis and FMEA are still relying heavily on reliability engineer expertise, as automatic methods supporting them are still lacking. In this paper, we introduce a novel approach that allows to automatically perform failure analysis considering the hardware schematic of the item under analysis, or safety-element-out-of-context, and a behavioral model of the software the hardware executes. An automotive case study is presented to illustrate the approach, and some preliminary results are discussed.", "num_citations": "13\n", "authors": ["502"]}
{"title": "Automatic generation of stimuli for fault diagnosis in IEEE 1687 networks\n", "abstract": " The IEEE 1687 standard describes reconfigurable structures allowing to flexibly access the instruments existing within devices (e.g., to support test, debug, calibration, etc.), by the use of configurable modules acting as controllable switches. The increasing adoption of this standard requires the availability of algorithms and tools to automate its usage. Since the resulting networks could inevitably be affected by defects which may prevent their correct usage, solutions allowing not only to test against these defects, but also to diagnose them (i.e., to identify the location of possible faults) are of uttermost importance. This paper proposes a method to automatically generate suitable test stimuli: by applying them and observing the output of the network one can not only detect possible faults, but also identify the fault responsible for the misbehavior. Experimental results gathered on a set of benchmark networks with a\u00a0\u2026", "num_citations": "13\n", "authors": ["502"]}
{"title": "Automatic test generation for verifying microprocessors\n", "abstract": " A pipelined processor with a high-level behavioral HDL description is presented in this paper. It generates a set of effective test programs by using a simulator, which is able to evaluate with respect to an RTL coverage metric. The proposed optimizer is based on a technique called microGP, an evolutionary system able to automatically device and optimizes the program written in an assembly language. Quantitative coverage measurement presented will guide the test-program generation. The approach is fully automatic and broadly applicable. The minimal test set with the programmable coverage is attained.", "num_citations": "13\n", "authors": ["502"]}
{"title": "High-level test generation for hardware testing and software validation\n", "abstract": " It is now common for design teams to develop systems where hardware and software components cooperate; they are thus facing the challenging task of validating and testing systems where hardware and software parts exist. In this paper a high-level test generation approach is presented, which is able to produce input stimuli that can be fruitfully exploited for test and validation purposes of both hardware and software components. Experimental results are reported showing that the proposed approach produces high quality vectors in terms of the adopted metrics for hardware and software faults.", "num_citations": "13\n", "authors": ["502"]}
{"title": "Accurate dependability analysis of CAN-based networked systems\n", "abstract": " Computer-based systems where several nodes exchange information via suitable network interconnections are today exploited in many safety-critical applications, like those belonging to the automotive field. Accurate dependability analysis of such a kind of systems is thus a major concern for designers. In this paper, we present an environment we developed in order to assess the effects of faults in CAN-based networks. We developed an IP core implementing the CAN protocol controller, and we exploited it to set-up a network composed of several nodes. Thanks to the approach we adopted, we were able to assess via simulation-based fault injection the effects of faults both in the bus used to carry information and inside each CAN controller as well. In this paper, we report a detailed description of the environment we set-up and we present some preliminary results we gathered to assess the soundness of the\u00a0\u2026", "num_citations": "13\n", "authors": ["502"]}
{"title": "Exploiting symbolic techniques for partial scan flip flop selection\n", "abstract": " Partial scan techniques have been widely accepted as an effective solution to improve sequential ATPG performance while keeping acceptable area and performance overheads. Several techniques for flip-flop selection based on structural analysis have been presented in the literature. In this paper we first propose a new testability measure based on the analysis of the circuit State Transition Graph (STG) through symbolic techniques. We then describe a scan flip flop selection algorithm exploiting this measure. We resort to the identification of several circuit macros to address large sequential circuits. When compared to other techniques, our approach shows good results, especially when it is used to optimize a set of flip-flops previously selected by means of structural analysis.", "num_citations": "13\n", "authors": ["502"]}
{"title": "Fault grading techniques of software test libraries for safety-critical applications\n", "abstract": " The adoption of complex and technologically advanced integrated circuits (ICs) in safety-critical applications (e.g., in automotive) forced the introduction of new solutions to guarantee the achievement of the required reliability targets. One of these solutions lies in performing in-field test (i.e., the test performed when the device is already deployed in the mission environment) to detect faults that may arise in this phase of electronic circuit life. In this scenario, one increasingly adopted approach is based on the software test libraries (STLs), i.e., suitable code which is run by the CPU included in the system and is able to detect the existence of possible permanent faults both in the CPU itself and in the rest of the system. In order to assess the effectiveness of the STLs, fault simulation is performed, so that the achieved fault coverage (e.g., in terms of stuck-at faults) can be computed. This paper explains why the fault\u00a0\u2026", "num_citations": "12\n", "authors": ["502"]}
{"title": "About on-line functionally untestable fault identification in microprocessor cores for safety-critical applications\n", "abstract": " When microprocessor cores are used in safety-critical applications, in-field test must be performed to reach the target reliability figures. In turns, the in-field test must be organized so that it achieves a sufficient fault coverage. The fault list to be considered for computing the fault coverage should only include testable faults, i.e., faults which may cause a failure in the operating conditions. Hence, single permanent faults that in the operating conditions cannot be excited, or do not propagate to any output, or both, should be removed from the list. These faults, called on-line functionally untestable faults, require a significant effort to be identified. The contribution of this paper is twofold. From one side, it reports experiments, showing that in typical embedded safety-critical systems their number is often far from being negligible, and depends on many parameters, including the application code run by the processor. Secondly\u00a0\u2026", "num_citations": "12\n", "authors": ["502"]}
{"title": "On the functional test of branch prediction units based on branch history table\n", "abstract": " Branch Prediction Units (BPUs) are highly efficient modules that can significantly decrease the negative impact of branches in superscalar and RISC processors. Traditional test solutions, mainly based on scan test, are often inadequate to tackle the complexity of these architectures, especially when dealing with delay faults that require at-speed stimuli application. Moreover, scan test does not represent a viable solution when Incoming Inspection or on-line test are considered. In this paper a functional approach targeting BPU test is proposed, allowing to generate a suitable test program whose effectiveness is independent on the specific implementation of the BPU. The effectiveness of the approach is validated on a Branch History Table (BHT) resorting to an open-source computer architecture simulator and to an ad hoc developed HDL testbench. Experimental results show that the proposed method is able to\u00a0\u2026", "num_citations": "12\n", "authors": ["502"]}
{"title": "Evaluating the impact of DFM library optimizations on alpha-induced SEU sensitivity in a microprocessor core\n", "abstract": " This paper presents and discusses the results of Alpha Single Event Upset (SEU) tests on an embedded 8051 microprocessor core implemented using three different standard cell libraries. Each library is based on a different Design for Manufacturability (DfM) optimization strategy; our goal is to understand how these strategies may affect the device sensitivity to alpha-induced Soft Errors. The three implementations are tested resorting to advanced Design for Testability (DfT) methodologies and radiation experiments results are compared. Electrical simulations of flip-flops are finally performed to propose physical motivations to the observed phenomena.", "num_citations": "12\n", "authors": ["502"]}
{"title": "An enhanced technique for the automatic generation of effective diagnosis-oriented test programs for processor\n", "abstract": " The ever increasing usage of microprocessor devices is sustained by a high volume production that in turn requires a high production yield, backed by a controlled process. Fault diagnosis is an integral part of the industrial effort towards these goals. This paper presents a new methodology that significantly improves over a previous work. The goal is construction of cost-effective programs sets for software-based diagnosis of microprocessors. The methodology exploits existing post-production test sets, designed for software-based self-test, and may use an already developed infrastructure IP to perform the diagnosis. Experimental results are reported in the paper comparing the new results with existing ones, and showing the effectiveness of the new approach for an Intel i8051 processor core", "num_citations": "12\n", "authors": ["502"]}
{"title": "Efficient analysis of single event transients\n", "abstract": " The effects of charged particles striking VLSI circuits and producing single event transients (SETs) are becoming an issue for designers who exploit deep sub-micron technologies; efficient and accurate techniques for assessing their impact on VLSI designs are thus needed. This paper presents a new approach for generating the list of faults to be addressed during fault injection experiments tackling SET effects, which resorts to static timing analysis. Moreover, it proposes a simplified SET fault model, which is suitable for being adopted within a zero-delay fault simulation tool. Experimental results are reported on both standard benchmarks and real-life circuits assessing the effectiveness of the proposed techniques.", "num_citations": "12\n", "authors": ["502"]}
{"title": "An experimental evaluation of the effectiveness of automatic rule-based transformations for safety-critical applications\n", "abstract": " Over the last years, an increasing number of safety-critical tasks have been demanded of computer systems. In particular, safety-critical computer-based applications are hitting markets where costs is a major issue, and thus solutions are required which conjugate fault tolerance with low costs. In this paper, a software-based approach for developing safety-critical applications is analyzed. By exploiting an ad-hoc tool implementing the proposed technique, several benchmark applications have been hardened against transient errors. Fault injection campaigns have been performed to evaluate the fault detection capability of the hardened applications. Moreover, a comparison of the proposed techniques with the Algorithm-Based Fault Tolerance (ABFT) approach is proposed. Experimental results show that the proposed approach is far more effective than ABFT in terms of fault detection capability when injecting\u00a0\u2026", "num_citations": "12\n", "authors": ["502"]}
{"title": "Evaluating the effectiveness of a Software Fault-Tolerance technique on RISC-and CISC-based architectures\n", "abstract": " This paper deals with a method able to provide a microprocessor-based system with safety capabilities by modifying the source code of the executed application, only. The method exploits a set of transformations which can automatically be applied, thus greatly reducing the cost of designing a safe system, and increasing the confidence in its correctness. Fault Injection experiments have been performed on a sample application using two different systems based on CISC and RISC processors. Results demonstrate that the method effectiveness is rather independent of the adopted platform.", "num_citations": "12\n", "authors": ["502"]}
{"title": "Simulation-based sequential equivalence checking of RTL VHDL\n", "abstract": " This paper presents a novel approach to equivalence verification of RT-level descriptions. The proposed approach sacrifices exactness in favor of applicability: it is not always able to produce an answer, but it is able to check sequential equivalence of large systems. Furthermore, being based on commercial VHDL tools, it does not have arbitrary limitations in the syntax of the descriptions.", "num_citations": "12\n", "authors": ["502"]}
{"title": "Enhancing topological ATPG with high-level information and symbolic techniques\n", "abstract": " This paper proposes a method to enhance topological ATPG algorithms by exploiting some information computed through symbolic techniques. Since symbolic techniques can only be applied to small circuits, suitable circuit portions (named macros) are first selected, and then symbolic techniques are used to analyze their state graphs. The topological ATPG algorithm benefits from this analysis to bound its search tree. Experimental results show that the proposed approach is effective in reducing the required CPU time and increasing both the Fault Coverage and the Fault Efficiency. When high-level information about the circuit behavior and structure is available, it can be fruitfully exploited for macro selection.", "num_citations": "12\n", "authors": ["502"]}
{"title": "Integrating online and offline testing of a switching memory\n", "abstract": " A circuit used in a telephone switching unit features several test techniques, including BIST, partial scan, and boundary scan. By sharing the same circuitry for both online and offline testing, the design minimizes additional logic while achieving very high fault coverage.", "num_citations": "12\n", "authors": ["502"]}
{"title": "GATTO: an intelligent tool for automatic test pattern generation for digital circuits\n", "abstract": " This paper deals with the problem of automated test pattern generation for large digital circuits. A distributed approach based on genetic algorithms is presented, which exploits the computational power of workstation networks to solve the problem even for the largest circuits. A prototypical system named GATTO is presented: the experimental results show that good results can be reached with CPU times much smaller than for previous methods, and that the distributed approach provides a good speed-up with respect to the mono-processor version. Thanks to the adoption of GAs, the method is able to dynamically adapt itself to the circuit it is applied to, and it allows the user to easily trade-off results accuracy and CPU time.< >", "num_citations": "12\n", "authors": ["502"]}
{"title": "An experimental comparison of different approaches to ROM BIST\n", "abstract": " The issue of ROM testing in VLSI circuits is examined. The BIST (built-in-self-test) solution overcomes controllability and observability difficulties which represent the limits of conventional ATPGs (automatic test pattern generators) and becomes particularly suitable for deeply embedded ROMs. The classical approach is studied, and a novel solution which tries to minimize the masking effect of faults in the additional external logic is proposed. Preliminary results are also reported, related to some BIST implementations of ROMs with different dimensions, and the area overhead due to the BIST logic is carefully evaluated. A macro generator which produces the BIST block without any intervention of the IC designer, starting from ROM characteristics, is presented.<>", "num_citations": "12\n", "authors": ["502"]}
{"title": "Random testability analysis: comparing and evaluating existing approaches\n", "abstract": " The authors present a comparative approach to some testability analysis methods for application to VLSI devices. Using a common framework of implementations and test cases, they compared the results between analysis methods and with those provided by fault simulation or exact calculation where possible. The methods dealt with are the weighted averaging algorithm, COP, the cutting algorithm, Stafan, and Predict.<>", "num_citations": "12\n", "authors": ["502"]}
{"title": "An analysis of test solutions for COTS-based systems in space applications\n", "abstract": " One of the current trends in space electronics is towards considering the adoption of COTS components, mainly to widen the spectrum of available products. When substituting space-qualified components with COTS ones a major challenge lies in guaranteeing the same level of reliability. To achieve this goal, a mix of different solutions can be considered, including effective test techniques, able to guarantee a high level of permanent fault coverage while matching several constraints in terms of system accessibility and hardware complexity. In this paper, we describe an approach based on Software-based Self-test, which is currently being adopted within the MaMMoTH-Up project, targeting the development of an innovative COTS-based system to be used on the Ariane5 launcher. The approach aims at testing the OR1200 processor adopted in the system, combined with new and effective techniques for identifying\u00a0\u2026", "num_citations": "11\n", "authors": ["502"]}
{"title": "A hybrid fault-tolerant architecture for highly reliable processing cores\n", "abstract": " Increasing vulnerability of transistors and interconnects due to scaling is continuously challenging the reliability of future microprocessors. Lifetime reliability is gaining attention over performance as a design factor even for lower-end commodity applications. In this work we present a low-power hybrid fault tolerant architecture for reliability improvement of pipelined microprocessors by protecting their combinational logic parts. The architecture can handle a broad spectrum of faults with little impact on performance by combining different types of redundancies. Moreover, it addresses the problem of error propagation in nonlinear pipelines and error detection in pipeline stages with memory interfaces. Our case-study implementation of a fault tolerant MIPS microprocessor highlights four main advantages of the proposed solution. It offers (i) 11.6\u00a0% power saving, (ii) improved transient error detection capability, (iii\u00a0\u2026", "num_citations": "11\n", "authors": ["502"]}
{"title": "On the functional test of the cache coherency logic in multi-core systems\n", "abstract": " Multi-core systems are becoming particularly common, due to the high performance they can deliver. Their performance strongly depends on the availability of effective cache controllers, able to guarantee (among others) the coherence of the caches of the different cores. This paper proposes a method for the test of the cache coherence logic existing within each core in a multi-core system, resorting to a functional approach; this means that the method is based on the generation of a suitable test program, to be run in a coordinated manner on the cores composing the system. The method is able to detect hardware defects affecting this logic. The method was validated on a LEON3 multicore system.", "num_citations": "11\n", "authors": ["502"]}
{"title": "On the functional test of the BTB logic in pipelined and superscalar processors\n", "abstract": " Electronic systems are increasingly used for safety-critical applications, where the effects of faults must be taken under control and hopefully avoided. For this purpose, test of manufactured devices is particularly important, both at the end of the production line and during the operational phase. This paper describes a method to test the logic implementing the Branch Prediction Unit in pipelined and superscalar processors when this follows the Branch Target Buffer (BTB) architecture; the proposed approach is functional, i.e., it is based on forcing the processor to execute a suitably devised test program and observing the produced results. Experimental results are provided on the DLX processor, showing that the method can achieve a high value of stuck-at fault coverage while also testing the memory in the BTB.", "num_citations": "11\n", "authors": ["502"]}
{"title": "Performance analysis of a noncontact plastic fiber optical fiber displacement sensor with compensation of target reflectivity\n", "abstract": " An inexpensive fiber-based noncontact distance sensor specific for monitoring short-range displacements in micromachining applications is presented. To keep the overall costs low, the sensor uses plastic optical fibers and an intensiometric approach based on the received light intensity after the reflection from the target whose displacement has to be measured. A suitable target reflectivity compensation technique is implemented to mitigate the effects due to target surface nonuniformity or ageing. The performances of the sensor are first evaluated for different fiber configurations and target reflectivity profiles and positions using a numerical method based on Monte Carlo simulations. Then, experimental validations on a configuration designed to work up to 1.5\u2009mm have been conducted. The results have confirmed the validity of the proposed sensor architecture, which demonstrated excellent compensation capabilities, with errors below 0.04\u2009mm in the (0-1)\u2009mm range regardless the color and misalignment of the target.", "num_citations": "11\n", "authors": ["502"]}
{"title": "Software-level soft-error mitigation techniques\n", "abstract": " Several application domains exist, where the effects of Soft Errors on processor-based systems cannot be faced by acting on the hardware (either by changing the technology, or the components, or the architecture, or whatever else). In these cases, an attractive solution lies in just modifying the software: the ability to detect and possibly correct errors is obtained by introducing redundancy in the code and in the data, without modifying the underlying hardware. This chapter provides an overview of the methods resorting to this technique, outlining their characteristics and summarizing their advantages and limitations.", "num_citations": "11\n", "authors": ["502"]}
{"title": "An exact and efficient critical path tracing algorithm\n", "abstract": " This paper presents an exact and efficient Critical Path Tracing algorithm targeting fault simulation of both Transition and Stuck-at faults. The complexity of the proposed algorithm is linear in the number of gates traced during the path tracing process. Experimental results show the efficiency of the proposed approach on a set of benchmark circuits.", "num_citations": "11\n", "authors": ["502"]}
{"title": "A hybrid approach to fault detection and correction in SoCs\n", "abstract": " The reliability of Systems-on-Chip (SoCs) is very important with respect to their use in different types of critical applications. Several fault tolerance techniques have been proposed to improve their fault detection and correction capabilities. These approaches can be classified in two basic categories: software-based and hardware-based techniques. In this paper, we propose a hybrid approach to provide fault detection and correction capabilities of transient faults for processor-based SoCs. This solution improves a previous one, aimed at fault detection only, and combines some modifications of the source code at high level with the introduction of an Infrastructure Intellectual Property (TIP). The main advantage of the proposed method lies in the fact that it does not require modifying the microprocessor core. Experimental results are provided to evaluate the effectiveness of the proposed method.", "num_citations": "11\n", "authors": ["502"]}
{"title": "Exploiting an I-IP for both test and silicon debug of microprocessor cores\n", "abstract": " Semiconductor manufacturers aim at delivering new devices within shorter times in order to gain market shares. First silicon debug is an important issue in order to minimize the time-to-market. In this paper we propose an Infrastructure IP (I-IP) intended to be a companion for processor cores. The proposed I-IP is an efficient, low-cost and easy-to-adopt solution for supporting the silicon debug of microprocessor cores and of other cores in a SoC, as it reuses the hardware introduced for implementing processor software-based self test (SBST)", "num_citations": "11\n", "authors": ["502"]}
{"title": "Automatic generation of test sets for sbst of microprocessor ip cores\n", "abstract": " Higher integration densities, smaller feature lengths, and other technology advances, as well as architectural evolution, have made microprocessor cores exceptionally complex. Currently, Software-Based Self-Test (SBST) is becoming an attractive test solution since it guarantees high fault coverage figures, runs at-speed, and matches core test requirements while exploiting low-cost ATEs. However, automatically generating test programs is still an open problem. This paper presents a novel approach for test program generation, that couples evolutionary techniques with hardware acceleration. The methodology was evaluated targeting a 5-stage pipelined processor implementing a SPARCv8 micro-processor core.", "num_citations": "11\n", "authors": ["502"]}
{"title": "On reducing the peak power consumption of test sequences\n", "abstract": " Due to the increased speed at which test sequences are applied today, and to the power consumption constraints defined during the design phase, power consumed during test is becoming a critical parameter. In this paper we propose a test pattern generation technique which integrates a power optimization phase. It produces test sequences having a reduced peak power consumption, while preserving the fault coverage attained by a classical ATPG.", "num_citations": "11\n", "authors": ["502"]}
{"title": "A data parallel algorithm for Boolean function manipulation\n", "abstract": " This paper describes a data-parallel algorithm for boolean function manipulation. The algorithm adopts Binary Decision Diagrams (BDDs), which are the state-of-the-art approach for representing and handling boolean functions. The algorithm is well suited for SIMD architectures and is based on distributing BDD nodes to the available Processing Elements and traversing BDDs in a breadth-first manner. An improved version of the same algorithm is also presented, which does not use virtual processors. A prototypical package has been implemented and its behavior has been studied with two different applications. In both cases the results show that the approach exploits well the parallel hardware by effectively distributing the load; thanks to the limited CPU time required and to the great amount of memory available, it can solve problems that can not be faced with by conventional architectures.< >", "num_citations": "11\n", "authors": ["502"]}
{"title": "Test time minimization in reconfigurable scan networks\n", "abstract": " Modern devices often include several embedded instruments, such as BISTs, sensors, and other analog components. New standards, such as IEEE Std. 1687, provide vehicles to access these instruments. In approaches based on reconfigurable scan networks, instruments are coupled with scan registers, connected into chains and interleaved with reconfigurable multiplexers, permitting a selective access to different parts of the chain. A similar scenario is also supported by IEEE Std. 1149.1-2013, where a test data register can be constructed as a chain of multiple segments, some of which can be excluded or mutually selected. The test of permanent faults affecting a reconfigurable scan network requires to shift test patterns throughout a certain number of network configurations. This paper presents a method to select the list of configurations needed to apply the complete test set in the minimum amount of clock\u00a0\u2026", "num_citations": "10\n", "authors": ["502"]}
{"title": "An efficient method for the test of embedded memory cores during the operational phase\n", "abstract": " System on Chip devices include an increasing number of embedded memory cores, whose test during the operational phase is often a strict requirement, especially for safety-critical applications. This paper proposes a new memory test method combining the characteristics of hardware and software solutions: the test is performed by the microcontroller/processor, while the code of the test instructions to be executed is generated on-the-fly by an ad hoc module, also in charge of checking the memory behavior. The solution is modular and does not require any modification either in the memory cores or in the processor. Moreover, it is well suited to be used for test during the operational phase. Experimental results, gathered by implementing some representative March elements and algorithms, show that the method guarantees higher defect coverage than software BIST and a test time comparable with that of\u00a0\u2026", "num_citations": "10\n", "authors": ["502"]}
{"title": "A SBST strategy to test microprocessors' branch target buffer\n", "abstract": " A Branch Target Buffer (BTB) is a mechanism to support speculative execution in order to overcome the performance penalty caused by branch instructions in pipelined microprocessors. Being an intrinsically fault tolerant unit, it is hard to achieve a good fault coverage resorting to plain functional testing methods. In this paper we analyze the causes for low functional testability and propose some techniques able to effectively face these issues. In particular, we describe a strategy to perform SBST on fully associative BTB units. The unit's general structure is analyzed, a suitable test program is proposed and the strategy to observe the test responses is explained. Feasibility and effectiveness of the proposed approach are shown on a MIPS-like processor.", "num_citations": "10\n", "authors": ["502"]}
{"title": "Optimized embedded memory diagnosis\n", "abstract": " This paper describes an optimized embedded memory diagnosis flow that exploits many levels of knowledge to produce accurate failure hypothesis. The proposed post-processing analysis flow is composed of many steps investigating failure shapes as well as cell fail syndromes, and includes advanced techniques to tackle incomplete data possibly due to tester noise and/or by faults showing intermittent effects. The effectiveness of the technique is demonstrated on an automotive-oriented System-on-Chip (SoC) manufactured in a 90nm technology by STMicroelectronics, which includes embedded SRAM memory cores tested using a programmable BIST. Scrambled BITMAPS gives a visual feedback leading to quick physical defect identification. Such research is relevant to aid on the manufacturing, material and process enhancements raising silicon yield.", "num_citations": "10\n", "authors": ["502"]}
{"title": "Generating power-hungry test programs for power-aware validation of pipelined processors\n", "abstract": " As CMOS technology scaled to nanometer regimes (100nm and below) power dissipation and power density have become major design constraints. The power consumed by active devices is converted into heat, which in turn increases the substrate temperature. Working at high temperatures may affect several figures of merit (eg, frequency and leakage power), as well as the reliability of the entire system. Therefore, considering power consumption during test and design validation procedures has become a testing due for modern SoCs.", "num_citations": "10\n", "authors": ["502"]}
{"title": "On the generation of functional test programs for the cache replacement logic\n", "abstract": " Caches are crucial components in modern processors (both stand-alone or integrated into SoCs) and their test is a challenging task, especially when addressing complex and high-frequency devices. While the test of the memory array within the cache is usually accomplished resorting to BIST circuitry implementing March test inspired solutions, testing the cache controller logic poses some specific issues, mainly stemming from its limited accessibility. One possible solution consists in letting the processor execute suitable test programs, allowing the detection of possible faults by looking at the results they produce. In this paper we face the issue of generating suitable programs for testing the replacement logic in set-associative caches that implement a deterministic replacement policy. A test program generation approach based on modeling the replacement mechanism as a finite state machine (FSM) is proposed\u00a0\u2026", "num_citations": "10\n", "authors": ["502"]}
{"title": "An enhanced FPGA-based low-cost tester platform exploiting effective test data compression for SoCs\n", "abstract": " Reducing the cost of test (in particular by reducing its duration and the cost of the required ATE) is a common goal which has largely been pursued in the past, mainly by introducing suitable on chip Design for Testability (DfT) circuitry. Today, the increasing popularity of sophisticated DfT architectures and the parallel emergence of new ATE families allow the identification of innovative solutions effectively facing that goal. In this paper we face the increasingly common situation of SoCs adopting the IEEE 1149.1 and 1500 standards for the test of the internal cores, and explore the idea of storing the test program on the tester in a compressed form, and decompressing it on-the-fly during test application. This paper proposes an improved version of an data compression/decompression technique which is well suited for reducing the size of test programs stored on the tester; this technique is particularly effective for very\u00a0\u2026", "num_citations": "10\n", "authors": ["502"]}
{"title": "An evolutionary methodology to enhance processor software-based diagnosis\n", "abstract": " The widespread use of cheap processor cores requires the ability to quickly point out the manufacturing process criticalities in an effort to enhance the production yield. Fault diagnosis is an integral part of the industrial effort towards these goals. This paper describes an innovative application of evolutionary algorithms: iterative refinement of a diagnostic test set. Several enhancements in the used evolutionary core are additionally outlined, highlighting their relevance for the specific problem. Experimental results are reported in the paper showing the effectiveness of the approach for a widely-known microcontroller core.", "num_citations": "10\n", "authors": ["502"]}
{"title": "An efficient perfect algorithm for memory repair problems\n", "abstract": " Memory repair by using spare rows/columns to replace faulty rows/columns has been proved to be NP-complete. Traditional perfect algorithms are comparison-based exhaustive search algorithms and are not efficient enough for complex problems. To overcome the deficiency of performance, a new algorithm has been devised and presented in this paper. The algorithm transforms a memory repair problem into Boolean function operations. By using BDD (binary decision diagram) to manipulate Boolean functions, a repair function which encodes all repair solutions of a memory repair problem can be constructed. The optimal solution, if it exists, can be found efficiently by traversing the BDD of a repair function only once. The algorithm is very efficient due to the fact that BDD can remove redundant nodes, combine isomorphic subgraphs together, and have very compact representations of Boolean functions if a good\u00a0\u2026", "num_citations": "10\n", "authors": ["502"]}
{"title": "Automatic generation of validation stimuli for application-specific processors\n", "abstract": " Microprocessor soft cores offer today an effective solution to the problem of rapidly developing new system-on-a-chips. However, all the features they offer are rarely used in embedded applications, and thus designers are often involved in the challenging task of soft-core customization to obtain application-specific processors. Suitable input stimuli are automatically generated while reasoning only on the software application the processor is intended to execute, while all the details concerning the processor hardware are neglected. Experimental results on an 8051 soft core show the effectiveness of the proposed approach.", "num_citations": "10\n", "authors": ["502"]}
{"title": "Approximate equivalence verification of sequential circuits via genetic algorithms\n", "abstract": " Industrial design flows for electronic circuits typically include at least one optimization step. During this step, the circuit is analyzed and modified in order to improve some specific characteristic, such as speed, size, or power consumption, without modifying its logic behavior. Woefully, exact-by-constructions optimization methods are not always applicable, and designers frequently need to validate the correctness of the optimization process by verifying the equivalence between the optimized design and the original one.Today, state-of-the-art sequential optimization techniques can handle designs with up to hundreds of flip-flops. However, traditional [2] OBDD-based techniques easily run into memory explosion when trying to verify the equivalence of such designs and many real cases are practically intractable. Thus, designers are interested in new verification methodologies that are not exact, but are always able to\u00a0\u2026", "num_citations": "10\n", "authors": ["502"]}
{"title": "Boolean function manipulation on a parallel system using BDDs\n", "abstract": " This paper describes a distributed algorithm for Boolean function manipulation. The algorithm is based on Binary Decision Diagrams (BDDs), which are one of the most commonly used data structures for representing and manipulating Boolean functions. A new distributed version of a BDD data structure and a distributed implementation of the basic operator for its manipulation are presented. The algorithm is suitable to work on a MIMD architecture and is based on a message passing master-slave paradigm. A package has been written, which uses the PVM library and is portable on different architectures.             Two applications have been developed using the parallel BDD package. In both cases the results show that the new distributed version of the algorithm is able to manage BDDs much larger than the ones managed by mono-processor tools.", "num_citations": "10\n", "authors": ["502"]}
{"title": "Boolean function manipulation on massively parallel computers\n", "abstract": " A new algorithm for implementing the basic operations on BDDs (binary decision diagrams) on a massively parallel computer is presented. Each node is associated with a processor, and nodes belonging to the same level are evaluated together. An implementation of the algorithm on a Connection Machine CM2 has been done, and the prototype is being tested on a set of benchmark applications. Experimental results, showing the time required to perform the apply operation on BDDs of growing size demonstrate the exactness of the complexity analysis and the effectiveness of the approach.<>", "num_citations": "10\n", "authors": ["502"]}
{"title": "An effective approach for functional test programs compaction\n", "abstract": " Functional test guarantees that the circuit is tested under normal conditions, thus avoiding any over-as well as under-test. This work is based on the use of Software-Based-Self-Test that allows a special application of functional test to the processor-based systems. This strategy applies the so-called functional test programs that are executed by the processor to guarantee a given fault coverage. The main goal of this paper is to investigate the static test compaction of a given set of functional test programs. The investigation aims at understanding and determining how to select the best functional test program candidates to obtain the smallest set having the best fault coverage. Results carried out on two different microprocessors show that a 49% reduction in test length and a 28.7% reduction in test application time can be achieved.", "num_citations": "9\n", "authors": ["502"]}
{"title": "On the development of Software-Based Self-Test methods for VLIW processors\n", "abstract": " Software-Based Self-Test (SBST) approaches are an effective solution for detecting permanent faults; this technique has been widely used with a good success on generic processors and processors-based architectures; however, when VLIW processors are addressed, traditional SBST techniques and algorithms must be adapted to each particular VLIW architecture. In this paper, we present a method that formalizes the development flow to write effective SBST programs for VLIW processors, starting from known algorithms addressing traditional processors. In particular, the method addresses the parallel Functional Units, such as ALUs and MULs, embedded into a VLIW processor. Fault simulation campaigns confirm the validity of the proposed method.", "num_citations": "9\n", "authors": ["502"]}
{"title": "Software-based testing for system peripherals\n", "abstract": " Software-based self-testing strategies have been mainly proposed to tackle microprocessor testing, but may also be applied to peripheral testing. However, testing system peripherals (e.g., DMA controllers, interrupt controllers, and internal counters) is a challenging task, since their observability and controllability are even more reduced when compared to microprocessors and to peripherals devoted to I/O communication (e.g., serial or parallel ports). In this paper an approach to develop functional tests for system peripherals is proposed. The presented methodology requires two correlated phases: module configuration and module operation. The first one prepares the peripheral to work in the different operation modes, whereas the second one is in charge of exciting the whole device and observing its behavior. We propose a methodology that guides the test engineer in building a compact set of test\u00a0\u2026", "num_citations": "9\n", "authors": ["502"]}
{"title": "A new fault injection approach for testing network-on-chips\n", "abstract": " Packet-based on-chip interconnection networks, or Network-on-Chips (NoCs) are progressively replacing global on-chip interconnections in Multi-processor System-on-Chips (MP-SoCs) thanks to better performances and lower power consumption. However, modern generations of MP-SoCs have an increasing sensitivity to faults due to the progressive shrinking technology. Consequently, in order to evaluate the fault sensitivity in NoC architectures, there is the need of accurate test solution which allows to evaluate the fault tolerance capability of NoCs. This paper presents an innovative test architecture based on a dual-processor system which is able to extensively test mesh based NoCs. The proposed solution improves previously developed methods since it is based on a NoC physical implementation which allows to investigate the effects induced by several kind of faults thanks to the execution of on-line fault\u00a0\u2026", "num_citations": "9\n", "authors": ["502"]}
{"title": "Software-based self-test of embedded microprocessors\n", "abstract": " In the recent years, the usage of embedded microprocessors in complex SoCs has become common practice. Their test is often a challenging task, due to their complexity, to the strict constraints coming from the environment and the application, and to the typical SoC design paradigm, where cores (including microprocessors) are often provided by third parties, and thus must be seen as black boxes. An increasingly popular solution to this challenge is based on developing a suitable test program, forcing the processor to execute it, and then checking the produced results (Software-Based Self Test, or SBST). The SBST methodology is particularly suitable for being applied at the end of manufacturing and in the field as well, to detect the occurrence of faults caused by environmental stresses and intrinsic aging (eg, negative bias temperature instability, hot carriers injection) in embedded systems. This chapter provides\u00a0\u2026", "num_citations": "9\n", "authors": ["502"]}
{"title": "Dft reuse for low-cost radiation testing of socs: A case study\n", "abstract": " This paper proposes an efficient low-cost strategy for collecting data during radiation experiments on systems-on-chips (SoCs), exploiting the available on-chip design for testability (DfT) structures devised for manufacturing test.The approach combines hardware test and diagnostic features with suitable software tools, which enable accurate measurements and quick transient effects data collection. Specific flows for radiation testing of different kinds of embedded cores are described. Results are shown for a radiation experiment conducted on an embedded SRAM core included in a 90 nm test-vehicle.", "num_citations": "9\n", "authors": ["502"]}
{"title": "Software-based self-test strategy for data cache memories embedded in SoCs\n", "abstract": " Testing SoC is a challenging task, especially when addressing complex and high- frequency devices. Among the different techniques that can be exploited, software-based selft-test (SBST) emerged as an effective solution, due some advantages it provides (no HW changes, at- speed testing, re-usability); however, the method requires effective techniques for generating suitable test programs. In this paper we face the issue of generating programs to test data caches (in particular their control part): a method is proposed, and some experimental results are provided to assess its effectiveness.", "num_citations": "9\n", "authors": ["502"]}
{"title": "Validation of the dependability of CAN-based networked systems\n", "abstract": " The validation of networked systems is mandatory to guarantee the dependability levels that international standards impose in many safety-critical applications. In this paper we present an environment to study how soft errors affecting the memory elements of network nodes in CAN-based systems may alter the dynamic behavior of a car. The experimental evidence of the effectiveness of the approach is reported on a case study.", "num_citations": "9\n", "authors": ["502"]}
{"title": "Approximate equivalence verification for protocol interface implementation via genetic algorithms\n", "abstract": " This paper describes a new approximate approach for checking the correctness of the implementation of a protocol interface, comparing its low-level implementation with its high-level prototype. The possibility to validate protocol interfaces is extremely useful in many industrial design flows and the proposed methodology does not impose particular requirements and it is able to fit in existing design flows: the proposed approach is based on coupling a commercial simulator with a genetic algorithm that tries to disprove the equivalence of an implementation with its high-level prototype. The use of a commercial simulator guarantees a complete compatibility with current standards and the method is able to fit painlessly in an existing industrial flow. Moreover, the use of a genetic algorithm allows the analysis of large and realistic designs. Experimental results show that the proposed method is effectively able to\u00a0\u2026", "num_citations": "9\n", "authors": ["502"]}
{"title": "Floorplan area optimization using genetic algorithms\n", "abstract": " The paper deals with the problem of Floorplan Area Optimization; an approach based on Genetic Algorithms is proposed. The method produces optimal results with CPU time requirements comparable with the ones of other approaches but presents some advantages: it is simple to implement, it allows the user to easily trade off CPU time with result accuracy, it requires a limited amount of memory to store partial results, it is not sensible to special structures like nested wheels. Experimental results on the biggest problems proposed in the literature are reported.< >", "num_citations": "9\n", "authors": ["502"]}
{"title": "An approach to sequential circuit diagnosis based on formal verification techniques\n", "abstract": " This article deals with the generation of exact diagnostic trees for real-size synchronous sequential circuits. Starting from existing detection-oriented test patterns, a modified fault simulator is used for assessing their diagnostic power, which, in general, is not satisfactory. A diagnostic procedure for improving it is described that successfully exploits symbolic FSM equivalence proof algorithms. In order to resort to costly techniques, such as product machine traversal, only when really needed, special checks are performed to verify combinational identity and identity on reachable states. As all faults are attributed to theirequivalence class, a complete and exact diagnostic tree can be built. Experimental results on ISCAS'89 circuits show the feasibility of the approach and support the claim that, for the first time, diagnosing real-world synchronous sequential circuits has become feasible.", "num_citations": "9\n", "authors": ["502"]}
{"title": "A new technique to generate test sequences for reconfigurable scan networks\n", "abstract": " Nowadays, industries require reliable methods for accessing the instrumentations embedded within semiconductor devices. The situation led to the definition of standards, such as the IEEE 1687, for designing the required infrastructures, and the proposal of techniques to test them. So far, most of the test-generation approaches are either too computationally demanding to be applied in complex cases, or too approximate to yield high-quality tests. This paper exploits a recent idea: the state of a generic reconfigurable scan chain is modeled as a finite state automaton and a low-level fault, as an incorrect transition; it then proposes a new algorithm for generating a functional test sequence able to detect all incorrect transitions far more efficiently than previous ones. Such an algorithm is based on a greedy search, and it is able to postpone costly operations and eventually minimize their number. Experimental results on\u00a0\u2026", "num_citations": "8\n", "authors": ["502"]}
{"title": "A functional approach for testing the reorder buffer memory\n", "abstract": " Superscalar processors have the ability to execute instructions out-of-order to better exploit the internal hardware and to maximize the performance. To maintain the in-order instruction commitment and to guarantee the correctness of the final results (as well as precise exception management), the Reorder Buffer (ROB) may be used. From the architectural point of view, the ROB is a memory array of several thousands of bits that must be tested against hardware faults to ensure a correct behavior of the processor. Since it is deeply embedded within the microprocessor circuitry, the most straightforward approach to test the ROB is through Built-In Self-Test solutions, which are typically adopted by manufacturers for end-of-production test. However, these solutions may not always be used for the test during the operational phase (in-field test) which aims at detecting possible hardware faults arising when the\u00a0\u2026", "num_citations": "8\n", "authors": ["502"]}
{"title": "Neutron sensitivity and hardening strategies for fast fourier transform on gpus\n", "abstract": " In this paper we analyze the neutron sensitivity of GPU devices when executing a Fast Fourier Transform algorithm. The provided experimental results demonstrate that in the majority of cases the output is affected by multiple errors, caused by thread and data dependencies. ECC is experimentally proved not to be sufficient to provide high reliability. Experimental data and analytical studies are employed to design specific software-based hardening strategies, which are validated through fault-injection.", "num_citations": "8\n", "authors": ["502"]}
{"title": "On the optimized generation of software-based self-test programs for VLIW processors\n", "abstract": " Software-Based Self-Test (SBST) approaches have shown to be an effective solution to detect permanent faults, both at the end of the production process, and during the operational phase. However, when Very Long Instruction Word (VLIW) processors are addressed these techniques require some optimization steps in order to properly exploit the parallelism intrinsic in these architectures. In this paper we present a new method that, starting from previously known algorithms, automatically generates an effective test program able to still reach high fault coverage on the VLIW processor under test, while reducing the test duration and the test code size. The method consists of three parametric phases and can deal with different VLIW processor models. The main goal of the proposed method is to automatically obtain a test program able to effectively reduce the test time and the required resources. Experimental results\u00a0\u2026", "num_citations": "8\n", "authors": ["502"]}
{"title": "Implementing a safe embedded computing system in SRAM-based FPGAs using IP cores: A case study based on the Altera NIOS-II soft processor\n", "abstract": " Reconfigurable Field Programmable Gate Arrays (FPGAs) are growing the attention of developers of mission- and safety-critical applications (e.g., aerospace ones), as they allow unprecedented levels of performance, which are making these devices particularly attractive as ASICs replacement, and as they offer the unique feature of in-the-field reconfiguration. However, the sensitivity of reconfigurable FPGAs to ionizing radiation mandates the adoption of fault tolerant mitigation techniques that may impact heavily the FPGA resource usage. In this paper we consider time redundancy, that allows avoiding the high overhead that more traditional approaches like N-modular redundancy introduce, at an affordable cost in terms of application execution-time overhead. A single processor executes two instances of the same software sequentially; the two instances are segregated in their own memory space through a soft IP\u00a0\u2026", "num_citations": "8\n", "authors": ["502"]}
{"title": "A software-based self-test methodology for system peripherals\n", "abstract": " Software-based self-test strategies have been mainly proposed to tackle microprocessor testing issues, but may also be applied to peripheral testing. However, testing highly embedded peripherals (e.g., DMA or Interrupt controllers) is a challenging task, since their observability and controllability are even more reduced compared to microprocessors and to peripherals devoted to I/O communication (e.g., serial or parallel ports). In this paper we describe an approach to develop functional tests for system peripherals embedded in SoCs that can be used for both design validation and testing. The presented methodology requires two correlated phases: module configuration and module operation. The first one prepares the peripheral on the different operation modes, whereas, the second one is in charge of exciting the whole device and observing its behavior. A methodology for generating suitable test programs is\u00a0\u2026", "num_citations": "8\n", "authors": ["502"]}
{"title": "Effective diagnostic pattern generation strategy for transition-delay faults in full-scan SOCs\n", "abstract": " Nanometric circuits and systems are increasingly susceptible to delay defects. This paper describes a strategy for the diagnosis of transition-delay faults in full-scan systems-on-a-chip (SOCs). The proposed methodology takes advantage of a suitably generated software-based self-test test set and of the scan-chains included in the final SOC design. Effectiveness and feasibility of the proposed approach were evaluated on a nanometric SOC test vehicle including an 8-bit microcontroller, some memory blocks and an arithmetic core, manufactured by STMicroelectronics. Results show that the proposed technique can achieve high diagnostic resolution while maintaining a reasonable application time.", "num_citations": "8\n", "authors": ["502"]}
{"title": "Exploiting MOEA to automatically geneate test programs for path-delay faults in microprocessors\n", "abstract": " This paper presents an innovative approach for the generation of test programs detecting path-delay faults in microprocessors. The proposed method takes advantage of the multiobjective implementation of a previously devised evolutionary algorithm and exploits both gate- and RT-level descriptions of the processor: the former is used to build Binary Decision Diagrams (BDDs) for deriving fault excitation conditions; the latter is used for the automatic generation of test programs able to excite and propagate fault effects, based on a fast RTL simulation. Experiments on an 8-bit microcontroller show that the proposed method is able to generate suitable test programs more efficiently compared to existing approaches.", "num_citations": "8\n", "authors": ["502"]}
{"title": "Accurate analysis of single event upsets in a pipelined microprocessor\n", "abstract": " Modern processors embed features such as pipelined execution units and cache memories that can not be directly controlled by programmers through the processor instruction set. As a result, software-based fault injection approaches are even less suitable for assessing the effects of SEUs in modern processors, since they are not able to evaluate the effects of SEUs affecting pipelines and caches. In this paper we report an analysis of a commercial processor core where the effects of SEUs located in the processor pipeline and cache memories are studied. The obtained results are compared with those software-based approaches provide, showing that software-based approaches may lead to significant errors during the error rate estimation. A major novelty of the paper is an extensive analysis of the effects of SEUs in the pipeline of a commercial processor core during the execution of several benchmark\u00a0\u2026", "num_citations": "8\n", "authors": ["502"]}
{"title": "Automatic test program generation from RT-level microprocessor descriptions\n", "abstract": " The paper addresses the issue of microprocessor and microcontroller testing, and follows an approach based on the generation of a test program. The proposed method relies on two phases: in the first, a library of code fragments (named macros) is generated by hand based on the knowledge of the instruction set, only. In the second phase, an optimization algorithm is run to suitably select macros and values for their parameters. The algorithm only relies on RT-level information, and exploits a suitable RT-level fault model to guide the test program generation. A major advantage of the proposed approach lies in the fact that it does not require any knowledge about the low level implementation of the processor. Experimental results gathered on an i8051 model using a prototypical implementation of the approach show that it is able to generate test programs whose gate-level fault coverage is higher than the one\u00a0\u2026", "num_citations": "8\n", "authors": ["502"]}
{"title": "ARPIA: A high-level evolutionary test signal generator\n", "abstract": " The integrated circuits design flow is rapidly moving towards higher description levels. However, test-related activities are lacking behind this trend, mainly since effective fault models and test signals generators are still missing. This paper proposes ARPIA, a new simulation-based evolutionary test generator. ARPIA adopts an innovative high-level fault model that enables efficient fault simulation and guarantees good correlation with gate-level results. The approach exploits an evolutionary algorithm to drive the search of effective patterns within the gigantic space of all possible signal sequences. ARPIA operates on register-transfer level VHDL descriptions and generates effective test patterns. Experimental results show that the achieved results are comparable or better than those obtained by high-level similar approaches or even by gate-level ones.", "num_citations": "8\n", "authors": ["502"]}
{"title": "Speeding-up fault injection campaigns in VHDL models\n", "abstract": " Simulation-based Fault Injection in VHDL descriptions is increasingly common due to the popularity of top-down design flows exploiting this language. This paper presents some techniques for reducing the time to perform the required simulation experiments. Static and dynamic methods are proposed to analyze the list of faults to be injected, removing faults as soon as their behavior is known. Common features available in most VHDL simulation environments are also exploited. Experimental results show that the proposed techniques are able to reduce the time required by a typical Fault Injection campaign by a factor ranging from 43.9% to 96.6%.", "num_citations": "8\n", "authors": ["502"]}
{"title": "High quality test pattern generation for RT-level VHDL descriptions\n", "abstract": " In current microprocessor design, an increasingly high silicon portion is derived through automatic synthesis. Effective test generation procedures working on the HDL before synthesis would therefore be extremely useful to shorten the design cycle and increase the test quality. This paper presents an effective test pattern generator working at the RT-level on the synthesizable VHDL source. The tool is based on an extensive control-and data-flow analysis of the design and on a Genetic Algorithm interacting with a commercial simulator. Experimental results concerning a set of standard benchmarks show that the obtained results, in terms of gate-level stuck-at fault coverage, are much better than a previous version of the tool and are very close to those obtained by state-of-the-art gate-level ATPGs.", "num_citations": "8\n", "authors": ["502"]}
{"title": "SymFony: A hybrid topological-symbolic ATPG exploiting RT-level information\n", "abstract": " Combining different techniques for sequential automated test pattern generation (ATPG) can help overcome their respective limits and exploit their advantages. In this paper, a hybrid technique resulting from mixing topologic and symbolic approaches to the sequential ATPG problem is presented. Macros are first identified within the circuit (possibly resorting to RT-level knowledge of circuit architecture). Information about macro behavior is then computed and efficiently stored resorting to symbolic techniques. A topological tool exploits this information during the ATPG process to speed-up the propagation task and to identify early unsuccessful choices. Experimental results are reported, demonstrating that the method is able to improve the efficiency of a topological ATPG in terms of required CPU time and attained fault coverage, especially on medium-sized control-dominated circuits.", "num_citations": "8\n", "authors": ["502"]}
{"title": "On the identification of optimal cellular automata for built-in self-test of sequential circuits\n", "abstract": " This paper presents a BIST architecture for finite state machines that exploits cellular automata (CA) as pattern generators and signature analyzers. The main advantage of the proposed approach, called C/sup 2/BIST (circular cellular BIST) is that the same CA is used for generation and compaction, thus lowering substantially the area requirements. The configuration of the CA rules is performed through a generic algorithm that is shown to provide good results both in terms of fault coverage and number of reconfigurations. In many cases, no reconfiguration is necessary and the corresponding area occupation is competitive with current BIST approaches.", "num_citations": "8\n", "authors": ["502"]}
{"title": "SAARA: a simulated annealing algorithm for test pattern generation for digital circuits\n", "abstract": " This paper deals with Automated Test Pattern Generation (ATPG) for large synchronous sequential circuits and describes a new approach based on Simulated Annealing. Simulation-based ATPG tools have several advantages with respect to deterministic and symbolic ones, especially because they can deal with large circuits. A prototypical system named SAARA is used to assess the effectiveness of the Simulated Annealing approach in terms of test quality and CPU time requirements. Results are reported, showing that SAARA is able to deal with large sequential circuits. A comparison with a state-of-the-art ATPG tool based on a Genetic Algorithm shows that SAARA generally improves the attained results in terms of fault coverage.", "num_citations": "8\n", "authors": ["502"]}
{"title": "An industrial experience in the built-in self test of embedded RAMs\n", "abstract": " High-quality embedded memory testing is increasingly important and a BIST scheme seems advantageous. Industrial experience at Italtel, a telecom company, confirms it. The scheme implements in hardware the test pattern generation algorithm proposed by R. Nair, S.M. Thatte, and J.A. Abraham /spl lsqb/1978/spl rsqb/, extending it to word-based memories. Several goodness criteria are satisfied, as the experimental results confirm.< >", "num_citations": "8\n", "authors": ["502"]}
{"title": "Improved techniques for multiple stuck-at fault analysis using single stuck-at fault test sets\n", "abstract": " Previous studies have suggested that many multiple stuck-at faults are detected by the test patterns generated to detect single stuck-at faults, but a fault simulation is often impossible, as their number grows and becomes enormous as real-sized circuits are considered. Thus, rules are needed to decrease their number by neglecting those that are surely detected by the patterns generated for single stuck-ats. A procedure is first given for establishing whether a multiple stuck-at fault is detected by a pattern. Then this procedure is used to prove a set of rules that allows the size of the fault list to be decreased significantly. Experimental results on the standard set of combinational benchmark circuits are provided, showing the effectiveness of the approach.< >", "num_citations": "8\n", "authors": ["502"]}
{"title": "An On-Line Testing Technique for the Scheduler Memory of a GPGPU\n", "abstract": " The highly parallel processing capabilities and reduced power performance of General Purpose Graphics Processing Units (GPGPUs) have been crucial factors for their massive use in multiple fields, such as multimedia and high-performance computing applications. Nowadays, more demanding areas, such as automotive, employ GPGPU devices where safety and reliability are mandatory design constraints. Nevertheless, the structural complexity, the transistor density, and the implementation in the latest silicon technologies introduce challenges to match safety and reliability requirements. In these technologies, wear-out and aging are factors that may significantly increase the occurrence of permanent faults during the lifetime operation. Moreover, these faults may generate unacceptable misbehaviors during the execution of an application. These constraints require devising new methods for in-field fault detection\u00a0\u2026", "num_citations": "7\n", "authors": ["502"]}
{"title": "An evolutionary technique for reducing the duration of reconfigurable scan network test\n", "abstract": " The growing need for effectively accessing registers (called instruments) related to non-functional purposes (e.g., test, debug, calibration) in many electronic devices pushed towards the development of new solutions, including the IEEE 1687 standard. The approach supported by these solutions allows a flexible access to embedded instruments through the Boundary Scan interface via a set of reconfigurable scan chains composing a Reconfigurable Scan Network (RSN). Since permanent faults may affect the circuitry implementing them, several works recently proposed techniques to automatically generate a suitable sequence of input stimuli able to detect them. The common approach is based on forcing the IEEE 1687 network to undergo a sequence of test sessions, each composed of a configuration phase and a test phase. By properly selecting the sequence of network configurations to be used, we can\u00a0\u2026", "num_citations": "7\n", "authors": ["502"]}
{"title": "SW-based transparent in-field memory testing\n", "abstract": " With continuous technology scaling, both quality and reliability are becoming major concerns for ICs due to extreme variations, non-ideal voltage scaling, etc. (not to mention the business pressure leading to shorter-time to market). One-time-factory manufacturing test is not sufficient anymore, and in-field testing (e.g., periodically, at power-on, during idle times) is becoming mandatory. Due to the strict constraints of in-field test, transparent BIST is extremely attractive, since it allows to minimize test invasiveness. This paper presents a cheap, high quality and practical SW-based transparent in-field test approach for memories within a system. Instead of using hardware BIST, the proposed scheme re-uses the CPU to perform infield testing for all memories within the system. All quality metrics of the proposed solution (such as defect coverage, test time and code size) are analyzed. Case studies using the ARM instruction\u00a0\u2026", "num_citations": "7\n", "authors": ["502"]}
{"title": "Hardening of serial communication protocols for potentially critical systems in automotive applications: LIN bus\n", "abstract": " Serial communications protocols used in automotive systems must comply with different levels of robustness. Some subsystems in charge of n on-critical tasks are composed of cheaper and non-fault tolerant elements. As Single Event Upsets also affect these sub-systems, a complete analysis of heir robustness could highlight the critical elements and point out the possible solutions, such as selective hardening in a cost effective way. An extensive fault injection campaign has been applied to a LIN bus controller module in order to select the best mitigation techniques to harden it against soft errors. A discussion around how these mitigation techniques could affect on-line testing in the module is also presented.", "num_citations": "7\n", "authors": ["502"]}
{"title": "A new architecture to cross-fertilize on-line and manufacturing testing\n", "abstract": " This paper deals with the on-line test of SoCs including cores equipped with BIST circuitry and IEEE 1500 wrappers. A method is proposed, which exploits an Infrastructure IP named OTC to manage the on-line test, the OTC module activates the test and provides the related results under the software control of the CPU, thus allowing the SoC to autonomously and flexibly support the on-line test, both at startup and during the normal operation phase. The main advantage of the proposed method lies in the fact that the same hardware resources used for manufacturing test can be exploited for on-line test. Experimental results gathered on a case study system show the benefits and costs of the approach.", "num_citations": "7\n", "authors": ["502"]}
{"title": "An effective methodology for on-line testing of embedded microprocessors\n", "abstract": " Testing embedded microprocessors at mission time is nowadays a requirement in many SoC applications. In this paper, we introduce a methodology where the detection of operational faults is performed while the normal operations are temporarily suspended, by means of an ad-hoc HW module connected to the address, data and control buses of the microprocessor. This module behaves as a peripheral towards the microprocessor but is able to gain access to the bus over the system memory during the test. The proposed approach uses the microprocessor interrupt protocol to preserve the system state. Experimental results, gathered on a MIPS core, show the feasibility and effectiveness of the approach.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Automatic functional stress pattern generation for soc reliability characterization\n", "abstract": " Reliability testing is increasingly used not only to reduce Infant Mortality effects, but also for Reliability Characterization. This paper first discusses the characteristics of the stimuli to be used during Reliability Characterization experiments, and outlines the importance of adopting a functional approach. Secondly, the paper describes a novel approach to automatically generate suitable stress patterns to be used during the Reliability characterization process of Systems-on-chip. The generation process uses an evolutionary algorithm driven by suitable state toggling-related metrics purposely defined in the paper. Costs and benefits of the proposed approach are highlighted, supported by the results gathered on a test vehicle released on a 90 nm technology.", "num_citations": "7\n", "authors": ["502"]}
{"title": "An efficient fault simulation technique for transition faults in non-scan sequential circuits\n", "abstract": " This paper proposes an efficient technique for transition delay fault coverage measurement in synchronous sequential circuits. The proposed strategy is based on a combination of multi-valued algebra simulation, critical path tracing and deductive fault simulation. The main advantages of the proposed approach are that it is highly computationally efficient with respect to state-of-the-art fault simulation techniques, and that it encompasses different delay sizes in one simulation pass without resorting to an improved transition fault model. Preliminary results on ITC99 benchmarks show that the gain in terms of CPU time is up to one order of magnitude compared to previous existing techniques.", "num_citations": "7\n", "authors": ["502"]}
{"title": "A deterministic methodology for identifying functionally untestable path-delay faults in microprocessor cores\n", "abstract": " Delay testing is crucial for most microprocessors. Software-based self-test (SBST) methodologies are appealing, but devising effective test programs addressing the true functionally testable paths and assessing their actual coverage are complex tasks. In this paper, we propose a deterministic methodology, based on the analysis of the processor instruction set architecture, for determining rules arbitrating the functional testability of path-delay faults in the data path and control unit of processor cores. Moreover, the performed analysis gives guidelines for generating test programs. A case study on a widely used 8-bit microprocessor is provided.", "num_citations": "7\n", "authors": ["502"]}
{"title": "An novel methodology for reducing SoC test data volume on FPGA-based testers\n", "abstract": " Low-Cost test methodologies for Systems-on-Chip are increasingly popular. They dictate which features have to be included on-chip and which test procedures have to be adopted in order to guarantee high test quality, while minimizing application costs. Consequently, Low-Cost test strategies can be run on testers offering lower performance and/or reduced features with respect to traditional Automatic Test Equipments (ATEs); these equipments are usually referred to as Low-Cost testers.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Hardware-accelerated path-delay fault grading of functional test programs for processor-based systems\n", "abstract": " The path-delay fault simulation of functional tests on complex circuits such as current processor-based systems is a daunting task. The amount of computing power and memory needed for verifying or grading functional test programs capabilities employing traditional techniques is huge and constitutes a serious bottleneck in the test flow. In this paper we propose a new mechanism for grading functional test program path-delay coverage (1) relying on FPGA-based emulation,(2) based on suitable instrumentation of the circuit structure and (3) exploiting ad hoc modules to minimize the host performance requirements stemming from the experiment management. The proposed setup reduces the grading time by several orders of magnitude with respect to software environments. Moreover, the experimented mechanism is capable of pinpointing the clock cycles when path activation arises, thus providing a key for\u00a0\u2026", "num_citations": "7\n", "authors": ["502"]}
{"title": "Hardware-in-the-loop-based dependability analysis of automotive systems\n", "abstract": " Automotive systems embed several electronic control units whose purpose is to help drivers in controlling vehicles, as well as guaranteeing the safety of vehicles' occupants. The occurrence of faults affecting these units can have dramatic impacts, and must be forecasted as earlier as possible during the conception of new vehicles. In this paper we propose a new fault-injection system based on a hardware-in-the-loop vehicle model. The main novelty of our system is the possibility of directly relating fault effects to vehicle's dynamic response without performing long and expensive experiments on a prototype running on a test track", "num_citations": "7\n", "authors": ["502"]}
{"title": "New evolutionary techniques for test-program generation for complex microprocessor cores\n", "abstract": " Checking if microprocessor cores are fully functional at the end of the productive process has become a major issue. Traditional functional approaches are not sufficient when considering modern designs. This paper describes new improvements for an existing evolutionary algorithm, called \u00b5GP, able to generate Turing-complete programs; these are exploited, along with hardware acceleration techniques, to add content to a qualifying test campaign by automatically generating assembly programs. The approach is suitable for medium-sized processor cores. The experimental evaluation performed on a SPARCv8 clearly shows the potentiality of the approach, and the effectiveness of the enhancements to the evolutionary core.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Detailed comparison of dependability analyses performed at RT and gate levels\n", "abstract": " Methods allowing a designer to perform early dependability analyses aim either at classifying the faults according to their main potential effect, or at analyzing more in depth the error propagation paths in the circuit. In the two cases, these methods can be applied at several description levels, starting from the behavioral level down to the gate level with back annotation data. This paper compares results obtained at RT and gate levels. The advantages of combining an error propagation path analysis and a classification are also discussed.", "num_citations": "7\n", "authors": ["502"]}
{"title": "CA-CSTP: a new BIST architecture for sequential circuits\n", "abstract": " Circular Self-Test Path (CSTP) is an attractive technique for implementing BIST in sequential circuits; unfortunately, there are cases in which the fault coverage it attains is unacceptably low. This paper proposes a new architecture, named CA-CSTP, which overcomes these limitations and always reaches a high fault coverage by exploiting a slightly more complex chain cell based on a Cellular Automata architecture. Experimental results show the effectiveness of our proposal.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Evaluating cost and effectiveness of software redundancy techniques for hardware errors detection\n", "abstract": " Redundancy is a common answer to the increasing demand of high dependability in safety-critical applications. In this paper we consider low-cost computer-based systems; in such systems hardware redundancy is unacceptable and software redundancy is the only design modification which can possibly be introduced to improve the system fault tolerance. The technique adopted here is based on variable duplication: as a main advantage, it can be automatically implemented on the high-level code of the program. The paper performs some analysis about the cost and effectiveness of this approach.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Guaranteeing testability in re-encoding for low power\n", "abstract": " This paper considers the testability implications of low power design methodologies. Low power and high testability are shown to be highly contrasting requirements, and an optimization algorithm is proposed, which is able to explore the trade-off between them. The algorithm is based on a newly proposed power estimation function, and on an estimate of the expected rest length of a pseudo-random rest session. Given these estimates a Genetic Algorithm, exploiting some symbolic computations with BDDs, provides a state reencoding for the circuit. The algorithm is experimental shown both to provide good results from the power optimization point of view, and to be able to sacrifice, on the designer's request, some of the power and area optimization in favor of testability improvement.", "num_citations": "7\n", "authors": ["502"]}
{"title": "A cellular genetic algorithm for the floorplan area optimization problem on a SIMD architecture\n", "abstract": " Once an Integrated Circuit has been fully designed at the gate level, its layout must be drawn on the silicon surface, and the modules which compose it must be placed on the silicon area (Floorplan Design). A common practice is to first determine the relative positions of modules and then chose for each module the implementation which optimizes a given cost function.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Testing a switching memory in a telecommunication system\n", "abstract": " The paper describes the approach followed for testing a real circuit produced by Italtel. Both on-line and off-line testing are considered and the performance and area overheads are taken into account to meet the constraints imposed by the circuit customers. BIST is adopted to test some embedded memories, and boundary scan is exploited to activate the test and gather the results. Particular care is taken to minimize the additional logic, by using the same circuitry for both on-line and off-line testing.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Assessing the diagnostic power of test pattern sets\n", "abstract": " The increasing power of the available fault simulators makes it possible to satisfy new requirements such as statistics and other data about both the testability and the diagnosability of circuits. Commercial fault simulators are often unable to provide this kind of information and, even when they provide some diagnostic facilities, they are highly inefficient in implementing them. This paper introduces a general framework to analyze the problem and presents an ad hoc strategy based on efficient techniques for both simulation and fault dropping. A prototype simulator, able to produce full diagnostic information with acceptable CPU time and memory requirements, is described.", "num_citations": "7\n", "authors": ["502"]}
{"title": "Effective screening of automotive SoCs by combining burn-in and system level test\n", "abstract": " Automotive systems must reach a high reliability in their electronic components. This kind of devices must undergo several tests and stress steps discovering all possible defects that could manifest during lifetime. Burn-In (BI) is a manufacturing test phase used for screening the early life latent faults that can naturally affect a population of devices. System Level Test (SLT) is increasingly adopted as one of the final steps in the testing process of complex Systems on Chip (SoCs) mimicking the operational conditions. This paper aims at describing the motivations for and the effectiveness stemming from combining SLT with BI. The key idea leverages on the development of a new step inside the test process, which reproduces the system using SLT and places the system in the worst cases by means of the BI. Moreover, the paper analyses the required tester architecture to merge SLT and BI. Finally, an industrial case by\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "On the evaluation of SEU effects in GPGPUs\n", "abstract": " General Purpose Graphic Processing Units (GPGPUs) are effective solutions for high-demand data applications which involve multi-signal, image and video processing thanks to their powerful parallel architecture. In the last years, GPGPUs have been considered also for safety-critical applications, such as autonomous and semi-autonomous car driving systems. New GPGPU devices include an increasing number of parallel cores in order to increase throughput and performance. This increment in the number of cores and the requirements in terms of power consumption force designers to use aggressive semiconductor technologies. Nevertheless, those new devices can be seriously affected by radiation effects, modeled as Single Event Upsets (SEUs). SEUs could generate unexpected operation effects in the applications which could be unacceptable for the safety-critical ones. This work analyzes the SEU effects\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "Automated test program reordering for efficient sbst\n", "abstract": " Software-based Self-test (SBST) is one of the techniques adopted to detect latent faults in safety-critical applications, thus aiming at preventing them from producing failures. When adopted for in-field test, not only the achieved fault coverage, but also the test duration of SBST test programs become critical parameters. Sometimes, these test programs are created following guidelines allowing to guarantee a given Fault Coverage with reduced test duration. In other cases, existing test programs are re-used. Hence, it is important to devise automatic techniques able to modify them in such a way that the fault coverage is kept unchanged (or increased) while the test duration is reduced. This paper presents a possible approach in this direction. Its effectiveness is evaluated on some test programs targeting the openMSP430 processor. Experimental results show that the proposed method is able not only to significantly\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "Effective generation and evaluation of diagnostic SBST programs\n", "abstract": " Functional test and software-based self-test (SBST) approaches for processors are becoming popular as they enable low-cost production tests and are often the only solution for in-field tests. With the increasing use of volume diagnosis, efficient and cost-effective diagnosis methods are required. A high quality functional or SBST test program can be used to perform logic fault diagnosis with low-cost test equipment and therefore significantly reduce the cost of diagnosis. We present a framework for the automatic generation of functional diagnostic sequences for stuck-at faults. The framework allows a user to specify constraints imposed by the employed test environment and generates diagnostic sequences satisfying these constraints. Furthermore, the framework is able to prove the equivalence of faults under the specified constraints. This enables to compute the best possible diagnostic quality that can be reached\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "Evaluating the radiation sensitivity of GPGPU caches: New algorithms and experimental results\n", "abstract": " Given their high computational power, General Purpose Graphics Processing Units (GPGPUs) are increasingly adopted: GPGPUs have begun to be preferred to CPUs for several computationally intensive applications, not necessarily related to computer graphics. However, their sensitivity to radiation still requires to be fully evaluated. In this context, GPGPU data caches and shared memory have a key role since they allow to increase performance by sharing data between the parallel resources of a GPGPU and minimizing the memory accesses overhead. In this paper we present three new algorithms designed to support radiation experiments aimed at evaluating the radiation sensitivity of GPGPU data caches and shared memory. We also report the cross-section and Failure In Time results from neutron testing experiments performed on a commercial-off-the-shelf GPGPU using the proposed algorithms, with\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "Increasing the fault coverage of processor devices during the operational phase functional test\n", "abstract": " A key issue in many safety-critical applications is the test of the ICs to be performed during the operational phase: regulations and standards often explicitly state the fault coverage figures to be achieved with respect to permanent faults. Functional test (i.e., a test exploiting only functional inputs and outputs, without resorting to any Design for Testability) is often the only viable solution, unless a strict cooperation exists between the system company and the device provider. However, purely functional test often shows several limitations due to the limited accessibility that it can gain on some input/output signals. This paper proposes a hybrid approach, in which a suitable hardware module is added outside a microcontroller to increase its functional testability during the operational phase. Experimental results gathered on several industrial cases-of-study are reported, showing the feasibility of the method.", "num_citations": "6\n", "authors": ["502"]}
{"title": "A design of security module to protect program execution in embedded system\n", "abstract": " This paper presents a security mechanism using hardware monitoring to protect the program's execution on embedded system. Mainly, three types of information are monitored: code's basic block checksum, execution time of code's basic block and the beginning-ending addresses of code's basic block. This information is extracted from the target program through additional tools. And they are integrated into the monitoring model. When the program is running normally, the security module will first calculate real-time status information of the current execution, and then compare this information with the data stored in monitoring model. Once the abnormal situation is detected, it will be processed (at now, just record). Preliminary experimental results show that the designed basic block information extraction tools and the security module can be expected to work properly. Additional performance loss and Additional\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "Fast power evaluation for effective generation of test programs maximizing peak power consumption\n", "abstract": " High power consumption during test may lead to yield loss and premature aging. In particular, excessive peak power consumption during at-speed delay fault testing represents an important issue. In the literature, several techniques have been proposed to reduce peak power consumption during at-speed LOC or LOS delay testing. On the other side, limiting too much the power consumption during test may reduce the defect coverage. Hence, techniques for identifying upper and lower functional power limits are crucial for delay fault testing. Yet, the task of computing the maximum functional peak power achievable by CPU cores is challenging, since the functional patterns with maximum peak power depend on specific instruction execution order and operands. In this paper, we present a methodology combining neural networks and evolutionary computing for quickly estimating peak power consumption. The\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "Exploiting the debug interface to support on-line test of control flow errors\n", "abstract": " Detecting the effects of transient faults is a key point in many safety-critical applications. This paper explores the possibility of using for this purpose the debug interface existing today in several processors/controllers on the market. In this way one can achieve a good detection capability with respect to control flow errors with very small latency, while the cost for adopting the proposed technique is rather limited and does not involve any change either in the processor hardware or in the application software. The method works even if the processor uses caches. Experimental results are reported, showing both the advantages and the costs of the method.", "num_citations": "6\n", "authors": ["502"]}
{"title": "Fault injection analysis of transient faults in clustered VLIW processors\n", "abstract": " VLIW architectures are widely employed in several embedded signal applications mainly because they offer the opportunity to gain high computational performances while maintaining reduced clock rate and power consumption. Recently, VLIW processors became more and more suitable to be employed in various embedded processing systems including safety critical applications such as aerospace, automotive and rail transport. Therefore, techniques to effectively estimate and improve the reliability of VLIW processor are of great interest. Terrestrial safety-critical applications based on newer nano-scale technologies raise increasing concerns about transient errors induced by neutrons. In this paper, we analyze the cross-domain failures affecting redundant mitigation techniques implemented on a statistically scheduled data path VLIW processor and we describe a fault injection analysis of transient faults affecting\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "An enhanced strategy for functional stress pattern generation for system-on-chip reliability characterization\n", "abstract": " Reliability characterization is the industrial process intended to measure the useful life period and failure rate of a component population by exploiting stress mechanisms. The paper describes a methodology for the automatic generation of stress programs to be used during the reliability characterization process of Systems-on-Chip (SoC). The proposed methodology is composed of a two-phase strategy, first an evolutionary algorithm (EA) works on the SoC's description at Register-Transfer-Level (RTL) by evaluating high-level metrics to quickly progress to a sufficient stress quality level, then evolution is continued on the gate-level description towards a better stress quality. The proposed methodology was experimented on a SoC manufactured in a 90nm technology including an 8051 processor. The proposed strategy reduces significantly the generation times and quickly improves the stress quality values with\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "Functional test generation for DMA controllers\n", "abstract": " Today's SoCs are composed of a high variety of modules, such as microprocessor cores, memories, peripherals, and customized blocks directly related to the targeted application. Testing a peripheral core embedded in a SoC requires two correlated phases: module configuration and module operation. The first one prepares the peripheral on the different operation modes, whereas, the second one is in charge of exciting the whole device and observing its behavior. Different testing strategies based on the execution of assembly programs have been proposed by the research community to test the embedded blocks in a SoC, however, testing highly embedded peripherals (e.g., DMA controllers) is still a challenging task, since their observability and controllability are even more reduced compared to peripherals devoted to I/O communication. In this paper we describe an approach to develop functional tests for DMA\u00a0\u2026", "num_citations": "6\n", "authors": ["502"]}
{"title": "An on-board data-handling computer for deep-space exploration built using commercial-off-the-shelf SRAM-based FPGAs\n", "abstract": " The use of commercial-of-the-shelf SRAM-based FPGA devices in space applications is not yet a reality due to concerns still existing about the device reliability; therefore, more conservative approaches based on anti-fuse FPGAs are currently preferred. The major concern about the use of such devices in space stems from their sensitivity to ionizing radiation, which may alter the content of the design the device implements, and which forces to adopt error mitigation techniques that have very high resource overheads. In this paper we analyze a realistic case study taken from a future space mission, and we show how mitigation techniques that combine hardware and software redundancy can provide very good fault tolerance capabilities to designs that include processor cores, while reducing significantly the overhead of the mitigation technique with respect to the hardware redundancy approach that is nowadays used.", "num_citations": "6\n", "authors": ["502"]}
{"title": "A new approach to build a low-level malicious fault list starting from high-level description and alternative graphs\n", "abstract": " In this paper a new approach is presented to build a list of faults to be used by the fault injection environment; the list is built starting from a high-level description of the system. The approach especially aims at identifying malicious faults, i.e. faults having a critical impact on the system reliability. To overcome the complexity problem inherent in low-level descriptions, high-level ones are exploited, and alternative graphs are applied to carry our the cause-effect analysis, to build up a fault tree and to carry out fault collapsing. The reduced high-level malicious fault list is converted so that it can be used together with the low level description for the final fault injection.", "num_citations": "6\n", "authors": ["502"]}
{"title": "A PVM tool for automatic test generation on parallel and distributed systems\n", "abstract": " The use of parallel architectures for the solution of CPU and memory critical problems in the Electronic CAD area has been limited up to now by several factors, like the lack of efficient algorithms, the reduced portability of the code, and the cost of hardware. However, portable message-passing libraries are now available, and the same code runs on high-cost supercomputers, as well as on common workstation networks. The paper presents an effective ATPG system for large sequential circuits developed using the PVM library and based on a Genetic Algorithm. The tool, named GATTO*, runs on a DEC Alpha AXP farm and a CM-5. Experimental results are provided.", "num_citations": "6\n", "authors": ["502"]}
{"title": "A parallel system for test pattern generation\n", "abstract": " The problem of generating test sequences for digital circuits is a crucial one in the area of electronic CAD. While a significant effort has been done to develop new and more powerful algorithms to solve it, the required CPU times are still unacceptable in many cases. A different approach based on the use of general-purpose MIMD architectures is presented in this paper. The attention is devoted to combinational circuits described at the gate level, although the same concepts can be extended to synchronous sequential circuits; faults are modeled as permanent single stuck-ats. A parallelization strategy is proposed, together with an implementation on a transputer-based machine. The resulting system significantly speeds-up the test generation process: its performance is discussed, reporting also the experimental results obtained on the standard set of benchmark combinational circuits.", "num_citations": "6\n", "authors": ["502"]}
{"title": "Finding the Maximum clique in a Graph using BDDs\n", "abstract": " 1 neur approach for the solution of the narimum clique proble m in general undirect \u00abd graphs is presented. The approach is base d on computing the characte ristic function of all the compl tely connected components in the graph, and then finding the marin um cost satisfying assignment of such a function. The mor li y of the me thod is in the use of BDDs for representing and manipulating characte ristic functions. If a BDD tool is a railable (as il is the case in many (\" 1 D systems) it is thus possible to imple me nt the algorithm in a re ry simple way; moreover, any inprore ment in the BDD area can be directly erploited and a gra att r efficiency obtained.", "num_citations": "6\n", "authors": ["502"]}
{"title": "Exploring the Mysteries of System-Level Test\n", "abstract": " System-level test, or SLT, is an increasingly important process step in today\u2019s integrated circuit testing flows. Broadly speaking, SLT aims at executing functional workloads in operational modes. In this paper, we consolidate available knowledge about what SLT is precisely and why it is used despite its considerable costs and complexities. We discuss the types or failures covered by SLT, and outline approaches to quality assessment, test generation and root-cause diagnosis in the context of SLT. Observing that the theoretical understanding for all these questions has not yet reached the level of maturity of the more conventional structural and functional test methods, we outline new and promising directions for methodical developments leveraging on recent findings from software engineering.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Applicative system level test introduction to increase confidence on screening quality\n", "abstract": " The introduction of System Level Test (SLT) about a decade ago aimed to a better sustainability for the achievement of the quality objectives required by high performances GPUs and CPUs. The paper intends giving some quantitative information to support this simplified statement. To do this we will report the results of a study conducted by the industrial application of SLT on a product targeted to a high-quality market segment, such as automotive. The discussion will describe the different SLT solutions developed for this new field of application and how it was possible to isolate SLT-only fails and relative functional root causes and cross-correlate traditional ATE test versus SLT screening capability through manufacturing operations.", "num_citations": "5\n", "authors": ["502"]}
{"title": "A dynamic reconfiguration mechanism to increase the reliability of GPGPUs\n", "abstract": " General Purpose Graphic Processing Units (GPGPUs) are effective solutions for high-demanding data processing applications. Recently, they started to be used even in safety-critical applications, such as autonomous car driving systems. GPGPUs are implemented using the latest semiconductor technologies, which are more prone to faults arising during the lifetime operation. However, until now fault mitigation solutions were not extensively included in GPGPUs, due to the limited reliability requirements of the applications they were originally intended for (e.g., gaming or multimedia). This work proposes a dynamically configurable self- repairing mechanism aimed at mitigating the impact of permanent faults in the Scalar Processor (SP) cores in GPGPUs. The mechanism is based on spare modules that can be used to replace faulty SPs when a fault is detected. A configuration instruction allows dynamically\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "On the in-field test of the GPGPU scheduler memory\n", "abstract": " GPGPUs have been increasingly successful in the past years in many application domains, due to their high parallel processing capabilities and energy performance. More recently, they started to be used in areas (such as automotive) where safety is also an important parameter. However, their architectural complexity and advanced technology level create challenges when matching the required reliability targets. This requires devising solutions to perform in-field test, thus allowing the systematic detection of possible permanent faults. These faults are caused by aging or external factors that affect the application execution and potentially generate critical misbehaviors. Moreover, effective in-field test techniques oriented to verify the integrity of GPGPU modules during in-field operation are still missed. In this work, we propose a method to generate self-test procedures able to detect all static faults affecting the\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "Assessing test procedure effectiveness for power devices\n", "abstract": " The use of power electronics in safety-critical applications requires specific test techniques for these devices. In particular, it is important to adopt some metric for assessing the quality of a given Test Procedure, e.g., by introducing fault models allowing to compute a Fault Coverage (FC) figure for the analog electronics, as already successfully done for digital electronics. In the digital domain the scientific and industrial community has adopted some fault models (e.g., stuck-at) for permanent faults. The use of this model (and others) allows to establish a priori a finite list of possible faults to be considered, to study their effects during the test (i.e., to determine which of these faults are detected) and during the operational phase (e.g., to perform FMEA), and to generate suitable test procedures targeting them.In the analog domain such widely accepted fault models do not exist, although some fault models have been\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "On the optimization of sbst test program compaction\n", "abstract": " Due to the increasing adoption of SBST solutions for both the end-of-manufacturing and the in-field test of SoC devices, the need for effective techniques able to reduce the duration of existing test programs became more pressing. Previous works demonstrated that this task is highly computational intensive and it is beneficial to partition it, e.g., by addressing the test program for one hardware module at a time. However, existing compaction techniques may become completely ineffective when dealing with faults which relate to memory addresses. This paper clarifies this issue and proposes possible solutions. Their effectiveness is experimentally demonstrated on a OR1200 pipelined processor.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Microprocessor testing: functional meets structural test\n", "abstract": " Structural test is widely adopted to ensure high quality for a given product. The availability of many commercial tools and the use of fault models make it very easy to generate and to evaluate. Despite its efficiency, structural test is also known for the risk of over-testing that may lead to yield loss. This problem is mainly due to the fact that structural test does not take into account the functionality of the circuit under test. On the other hand, functional test guarantees that the circuit is tested under normal conditions, thus avoiding any over- as well as under-testing issues. More in particular, for microprocessor testing, functional test is usually applied by exploiting the Software-Based-Self-Test (SBST) technique. SBST applies a set of functional test programs that are executed by the processor to achieve a given fault coverage. SBST fits particularly well for online testing of processor-based systems. In this work, we describe a\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "Observability solutions for in-field functional test of processor-based systems: a survey and quantitative test case evaluation\n", "abstract": " The usage of electronic systems in safety-critical applications requires mechanisms for the early detection of faults affecting the hardware while the system is in the field. When the system includes a processor, one approach is to make use of functional test programs that are run by the processor itself. Such programs exercise the different parts of the system, and eventually expose the difference between a fully functional system and a faulty one. Their effectiveness depends, among other factors, on the mechanism adopted to observe the behavior of the system, which in turn is deeply affected by the constraints imposed by the application environment. This paper describes different mechanisms for supporting the observation of fault effects during such in-field functional test, and it reports and discusses the results of an experimental analysis performed on some representative case studies, which allow drawing some\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "On the robustness of DCT-based compression algorithms for space applications\n", "abstract": " High compression ratio is crucial to cope with the large amounts of data produced by telemetry sensors and the limited transmission bandwidth typical of space applications. A new generation of telemetry units is under development, based on Commercial Off-The-Shelf (COTS) components that may be subject to misbehaviors due to radiation-induced soft errors. The purpose of this paper is to study the impact of soft errors on different configurations of a discrete cosine transform (DCT)-based compression algorithm. This work's main contribution lies in providing some design guidelines.", "num_citations": "5\n", "authors": ["502"]}
{"title": "FPGA-controlled PCBA power-on self-test using processor's debug features\n", "abstract": " When facing in-field board test, the functional approach plays an important role. Often, it corresponds to forcing the processor to execute a test program (which could be an application one), observing the produced results (e.g., by looking at the results written in the memory at the end of the test program execution). However, the fault coverage that can be achieved in this way is often difficult to compute, and limited by the reduced observability. In this paper we propose to use the debug features provided by many processors to enhance the observability, and hence the achieved fault coverage. In the proposed architecture we monitor on-the-fly during the test program execution the information accessible through the debug port using an ad hoc module mapped on an FPGA which is assumed to exist close to the processor. We provide experimental results showing the feasibility and cost of the approach, and\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "Fault-tolerance techniques for soft-core processors using the trace interface\n", "abstract": " As microprocessors are increasingly used in safety-critical applications, there is a growing demand for effective fault-tolerance techniques that can mitigate the effects of soft errors while reducing intrusiveness and minimizing the impact on performance and power consumption. To this purpose, approaches that are based on monitoring the microprocessor operation through an external interface in a non-intrusive manner have recently been proposed. In this paper we focus on the use of the trace interface for on-line monitoring. This interface provides detailed information about the instructions executed by the processor and can be reused to support error detection and correction in several ways, including multi-processors in hardware redundancy, time redundancy and control-flow checking.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Design space exploration and optimization of a hybrid fault-tolerant architecture\n", "abstract": " Fault-tolerant architectures have been widely used in industry to prevent circuit reliability from becoming a bottleneck for the development of robust high-performance and low-power systems. One such solution is a Hybrid Fault-Tolerant Architecture that offers benefits such as low power and lifetime reliability improvement. However, it has been identified that there is room of improvement in efficiency. Thus, in this paper we present design space exploration and optimization of the Hybrid Fault-Tolerant Architecture. The study involves application of four design variants to some ITC benchmark circuits as case study. Experimental results compare the initial and optimized designs and show that the proposed optimizations offer around 65% reduction in terms of area, about 55% power saving and 87% less performance overhead as compared to the initial design without any penalty of the fault tolerance capability.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Rejuvenation of nanoscale logic at NBTI-critical paths using evolutionary TPG\n", "abstract": " One of the main reliability concerns in the nanoscale logic is the time-dependent variation caused by Negative Bias Temperature Instability (NBTI). It increases the threshold voltage of pMOS transistors, which slows down signal propagation along the paths between flip-flops. As a consequence, NBTI may cause transient faults and, ultimately, permanent circuit functional failure. In this paper, we propose an innovative NBTI mitigation approach by rejuvenation of nanoscale logic along NBTI-critical paths. The method is based on hierarchical NBTI-critical paths identification and rejuvenation stimuli generation using an Evolutionary Algorithm. The rejuvenation stimuli are used to drive to the recovery phase the pMOS transistors that are the most significant for the NBTI-induced path delay. This rejuvenation procedure is to be applied to the circuit as an execution overhead at predefined periods. The proposed approach\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "A functional test algorithm for the register forwarding and pipeline interlocking unit in pipelined microprocessors\n", "abstract": " When the result of a previous instruction is needed in the pipeline before it is available, a \u201cdata hazard\u201d occurs. Register Forwarding and Pipeline Interlock (RF&PI) are mechanisms suitable to avoid data corruption and to limit the performance penalty caused by data hazards in pipelined microprocessors. Data hazards handling is part of the microprocessor control logic; its test can hardly be achieved with a functional approach, unless a specific test algorithm is adopted. In this paper we analyze the causes for the low functional testability of the RF&PI logic and propose some techniques able to effectively perform its test. In particular, we describe a strategy to perform Software-Based Self-Test (SBST) on the RF&PI unit. The general structure of the unit is analyzed, a suitable test algorithm is proposed and the strategy to observe the test responses is explained. The method can be exploited for test both at the end of\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "Safe: a self adaptive frame enhancer fpga-based ip-core for real-time space applications\n", "abstract": " Video-based navigation is an increasingly used procedure with hard real-time requirements and high computational effort. In this field, FPGA hardware acceleration supplies low-cost and considerable performances enhancement. Video-based navigation algorithms extrapolate and correlate features from images, relying on their accuracy. Image enhancement provides more defined and contrasted frames, assuring high precision feature extraction. The paper introduces an FPGA-based self-adaptive image enhancer. The IP-core is suitable for hard-real time applications, such as space applications, thanks to the guaranteed high-throughput.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Increasing fault coverage during functional test in the operational phase\n", "abstract": " A key issue in many safety-critical applications is the test of the ICs to be performed during the operational phase: regulations and standards often explicitly describe fault coverage figures to be achieved. Functional test (i.e., a test exploiting only functional inputs and outputs, without resorting to any Design for Testability) is often the only viable solution, unless a strict cooperation exists between the system company and the device provider. However, purely functional test often shows several limitations due to the limited accessibility that it can gain on some input/output signals. This paper proposes a hybrid approach, in which a suitable hardware module is added outside a microcontroller to increase its functional testability during the operational phase. Experimental results gathered on a couple of cases-of-study are reported, showing the feasibility of the method.", "num_citations": "5\n", "authors": ["502"]}
{"title": "On the modeling of gate delay faults by means of transition delay faults\n", "abstract": " This paper describes a novel modeling method for Gate Delay Faults. The methodology considers each Gate Delay Fault as equivalent to a set of Transition Delay Faults in the propagation paths of the affected port. The main advantage of using this model is that it does not need any explicit timing information and it allows to predict the effect of gate delay faults by using classical Transition Delay fault simulators. In this work, we exploit the modeling method to classify the circuit behavior depending on the delay range, the proposed algorithm finally works out the delay size ranges introducing no effect, small delay and gross delay fault effect. Results are carried out on the full scan version of ISCAS85, ISCAS89 and ITC99 benchmarks.", "num_citations": "5\n", "authors": ["502"]}
{"title": "A tester architecture suitable for MEMS calibration and testing\n", "abstract": " This poster outlines the working principle and an implementation of a tester architecture supporting MEMS calibration and testing; the tester works adaptively, providing electrical stimuli at run-time according to the collected results. The tester manages the calibration and testing process by means of a special hardware module, saving time and avoiding tester parallelism limitations due to massive wiring. Feasibility and effectiveness of the proposed method have been evaluated through simulations before being possibly introduced in commercial MEMS accelerometer testers.", "num_citations": "5\n", "authors": ["502"]}
{"title": "A novel scalable and reconfigurable emulation platform for embedded systems verification\n", "abstract": " Modern embedded systems are characterized by a heterogeneous architecture including several modules (e.g., DSPs, memories and mixed-signal IPs) often integrated with one or more microprocessor cores controlling the system functionalities by means of embedded software programs. The verification of such a kind of systems has become a challenge due to their increasing complexity that makes traditional simulation and emulation techniques unaffordable methods for current quality and time-to-market constraints. This paper presents a new platform for the hardware and software verification of modern embedded systems based on a reconfigurable device. The main novelty consists in an infrastructure architecture containing a signal processing IP and a microprocessor core flexibly interfaced with the device under validation, aimed at the overall reduction of the design verification time. It also provides a dynamic\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "Exploiting an infrastructure-intellectual property for systems-on-chip test, diagnosis and silicon debug\n", "abstract": " Semiconductor manufacturers aim at delivering high-quality new devices within shorter times in order to gain market shares. First silicon debug and diagnosis are important issues to be tackled in order to minimise the time-to-market and avoid expensive re-spins, while volume testing is necessary for guaranteeing acceptable quality levels. In this study, the authors propose an infrastructure intellectual property (I-IP) intended to be a companion for embedded processor cores. The proposed I-IP is an efficient, flexible, low cost and easy-to-adopt solution for managing silicon debug, diagnosis and production test of microprocessor cores and of other cores in a system-on-chip (SoC), offering full support to the three domains of test, diagnosis and debug. A key characteristic of the proposed solution is that the requirements from the three domains are faced in an integrated manner, and the interface to the device during test\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "Design validation of multithreaded architectures using concurrent threads evolution\n", "abstract": " Within the design arena of modern devices based on cutting-edge processor cores, the availability of effective verification, validation and test methodologies able to work on high-level descriptions of processor cores represents an interesting advantage, since it can dramatically reduce the overall time for design and manufacturing, while improving yield and quality. In this paper we propose a semi-automatic test program generation technique able to target modules in modern computer architectures that implement the multithreading paradigm. The methodology starts from high level descriptions of processor cores and using an incremental multi-run approach produces, with very limited manual intervention, a test set able to maximize verification metrics. Experimental results gathered on a real complex design, the OpenSPARC\u2122 T2 core, show the effectiveness of the proposed methodology.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Exploiting embedded FPGA in on-line software-based test strategies for microprocessor cores\n", "abstract": " Strategies based on periodic software-based self-test (SBST) represent an effective and cost-efficient solution for the detection of faults in low-cost embedded systems that do not require immediate recognition of error conditions. Today's integrated systems increasingly often include hardwired microprocessor devices and field-programmable gate array (FPGA) cores. We propose to implement a test-support module in the on-chip FPGA to observe critical processor signals and hence increase the observation capabilities in non-concurrent software-based on-line test strategies. Preliminary results are shown on a case study based on the Leon3 processor.", "num_citations": "5\n", "authors": ["502"]}
{"title": "A software-based methodology for the generation of peripheral test sets based on high-level descriptions\n", "abstract": " Nowadays, the use of Systems-on-Chip (SoCs) represents a very interesting solution, but also introduces some testing concerns. Up to now, researchers focused many efforts on the development of new software and hardware techniques for testing processors embedded in SoCs. However, the test of the surrounding peripherals has not been the subject of many research works, even if their importance within the entire system may be considerable.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Software-based on-line test of communication peripherals in processor-based systems for automotive applications\n", "abstract": " The adoption of systems-on-a-chip (SoCs) in automotive systems opens interesting possibilities, but also introduces significant dependability concerns. Up to now, researchers focused most of their efforts in devising new solutions for improving the dependability of the processor-cores embedded in typical SoCs, and several solutions mixing software techniques with hardware ones have been proposed, which result in low-cost dependable systems. Conversely, the peripheral components also typically embedded in SoCs are often designed according to traditional area-demanding hardware-only fault tolerance techniques. In this paper, we propose an experimental evaluation of the effectiveness of a purely software-based approach, which can be easily and inexpensively implemented on existing SoCs. We present results on a case study inspired to a real- life application, which exploits a network of SoCs based on\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "An integrated approach for increasing the soft-error detection capabilities in SoCs processors\n", "abstract": " Software implemented hardware fault tolerance (SIHFT) techniques are able to detect most of the transient and permanent faults during the usual system operations. However, these techniques are not capable to detect some transient faults affecting processor memory elements such as state registers inside the processor control unit, or temporary registers inside the arithmetic and logic unit. In this paper, we propose an integrated (hardware and software) approach to increase the fault detection capabilities of software techniques by introducing a limited hardware redundancy. Experimental results are reported showing the effectiveness of the proposed approach in covering soft-errors affecting the processor memory elements and escaping to purely software approaches.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Evolutionary Simulation-Based Validation\n", "abstract": " This paper describes evolutionary simulation-based validation, a new point in the spectrum of design validation techniques, besides pseudo-random simulation, designer-generated patterns and formal verification. The proposed approach is based on coupling an evolutionary algorithm with a hardware simulator, and it is able to fit painlessly in an existing industrial flow. Prototypical tools were used to validate gate-level designs, comparing them against both their RT-level specifications and different gate-level implementations. Experimental results show that the proposed method is effectively able to deal with realistic designs, discovering potential problems, and, although approximate in nature, it is able to provide a high degree of confidence in the results and it exhibits a natural robustness even when used starting from incomplete information.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Report on benchmark identification and planning of experiments to be performed\n", "abstract": " The document describes the benchmarks we have identified as test cases to be used during the COTEST project. Being the project focused both on the high-level generation of suitable test/validation vectors and on the high-level insertion of design for testability structures, we identified benchmarks of different characteristics and complexity. The document also outlines the experiments that we intend to perform during the project.", "num_citations": "5\n", "authors": ["502"]}
{"title": "System-level test bench generation in a co-design framework\n", "abstract": " Co-design tools represent an effective solution for reducing costs and shortening time-to-market, when System-on-Chip design is considered. In a top-down design flow, designers would greatly benefit from the availability of tools able to automatically generate test benches, which can be used during every design step, from the system-level specification to the gate-level description. This would significantly increase the chance of identifying design bugs early in the design flow, thus reducing the costs and increasing the final product quality. The paper proposes an approach for integrating the ability to generate test benches into an existing co-design tool. Suitable metrics are proposed to guide the generation, and preliminary experimental results are reported, assessing the effectiveness of the proposed technique.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Exploiting logic simulation to improve simulation-based sequential ATPG\n", "abstract": " The constantly increasing circuit size makes the sequential ATPG problem a challenging area even when simulation-based algorithms are exploited. Several techniques have been proposed which mainly resort to logic simulation, reverting to fault simulation only when strictly required. In this paper we present a new Genetic Algorithm-based test generation method which exploits information coming from a logic simulator (e.g., the circuit activity and the reached states) to guide the search process, in particular in the fault excitation phase. Experimental results show the effectiveness of the proposed method when compared with other Genetic Algorithm-based test generators.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Hybrid symbolic-explicit techniques for the graph coloring problem\n", "abstract": " This paper presents an algorithmic technique based on hybridizing Symbolic Manipulation Techniques based on BDDs with more traditional Explicit solving algorithms. To validate the approach, the graph coloring problem has been selected as a hard-to-solve problem, and an optimized solution based on hybrid techniques has been implemented. Experimental results on a set of benchmarks derived from the CAD for VLSI area show the applicability of the approach to graphs with millions of vertices in a limited CPU time.", "num_citations": "5\n", "authors": ["502"]}
{"title": "Using parallel genetic algorithms for solving the min-cut problem\n", "abstract": " The Min-Cut problem is a very well known graph problem, aimed at finding a partition of the graph vertices into two groups by cutting the minimum number of edges, each partition having the same number of vertices. Several real applications of this problem exist in circuits design and networks configuration. The problem has been proved to be NP complete: therefore the classical deterministic algorithms can not be used, because of their exponential complexity. Several methods have been presented in the literature to solve the min-cut problem [KeLi70][FiMa82][Kirp83][YeCL91]. A recent paper [YeCL95] presented an excellent comparison among them.In this paper we present a new approach based on Genetic Algorithms (GAs). A mono-processor version has been first developed, which resulted to be particularly suited when a sub-optimal solution is looked for on large graphs and in a limited CPU time. A binary\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "An improved data parallel algorithm for Boolean function manipulation using BDDs\n", "abstract": " This paper describes a data-parallel algorithm for boolean function manipulation. The algorithm adopts Binary Decision Diagrams (BDDs), which are the state-of-the-art approach for representing and handling boolean functions. The algorithm is well suited for SIMD architectures and is based on distributing BDD nodes among the available Processing Elements and traversing BDDs in a breadth-first manner. An improved version of the same algorithm is also presented, which does not use virtual processors. A prototypical package has been implemented and its behavior has been studied with two different applications. In both cases the results show that the approach exploits well the parallel hardware by effectively, distributing the load; thanks to the limited CPU time required and to the great amount of memory available, it can solve problems that can not be faced with by conventional architectures.< >", "num_citations": "5\n", "authors": ["502"]}
{"title": "A data parallel approach to Boolean function manipulation using BDDs\n", "abstract": " The paper describes an Electronic CAD package exploiting the CM-200 architecture to manipulate boolean functions. The package exploits Binary Decision Diagrams (BDDs) to symbolically operate with boolean functions. The data parallel approach is based on distributing BDD nodes do the available Processing Elements and traversing BDDs in a breadth-first manner. The behaviour of the algorithm is studied and the results which have been obtained obtained for an application developed with the package are reported. They show that the approach exploits well the parallel hardware and is highly scalable; if implemented on state-of-the-art and fully configured systems, it could solve problems which can not be faced with conventional architectures.< >", "num_citations": "5\n", "authors": ["502"]}
{"title": "A BDD Package For A Massively Parallel SIMD Architecture\n", "abstract": " Efficient techniques for boolean function manipulation are a key point in many areas, such as digital logic design and testing, artificial intelligence, combinatorics; the state-of-the-art approach to the problem is based on Binary Decision Diagrams (BDDs)[a]", "num_citations": "5\n", "authors": ["502"]}
{"title": "Proving Finite State Machines correct with an automaton-based method\n", "abstract": " The authors present a method to prove equivalence of a pair of FSMs, described at the gate level with D-type flip-flops and a reset signal available to bring them into the all-zero initial state. This method restricts investigation to that minimum subset of states that can be reached from the reset condition and are necessary to reach the goal. The equivalence condition is expressed in theoretical terms within the framework of the product machine. Without any loss of information, it is possible to reduce the product machine to a deterministic finite automaton (DFA). considerably reducing the number of states. The DFA is dynamically built by an explicit enumeration algorithm and, in general, only a very small part of the automaton is actually considered. The equivalence condition becomes a proof of the reachability of the DFA's final state. Search is performed in breadth-first. Experimental results on some pairs of ISCAS'89\u00a0\u2026", "num_citations": "5\n", "authors": ["502"]}
{"title": "A new algorithm for diagnosis-oriented automatic test pattern generation\n", "abstract": " Production testing does not only aim at detecting faulty devices, but its goals are often to repair the element or to investigate the cause of failure, so as to tune the manufacturing process. Diagnostic testing is thus becoming the object of attention both in industry and academia, thanks also to the increased power of tools like fault simulators, testability analysers, and ATPGs. Diagnostic testing has two aspects: assessing the diagnostic properties of a given test pattern set or generating test patterns having such properties. This paper deals with the latter aspect. An ATPG algorithm, the Delta -algorithm, generating a pattern able to distinguish between two faults, is described and its preliminary results obtained on a set of benchmark circuits are reported.< >", "num_citations": "5\n", "authors": ["502"]}
{"title": "Determined-Safe Faults Identification: A step towards ISO26262 hardware compliant designs\n", "abstract": " The development of Integrated Circuits for the Automotive sector imposes on major challenges. ISO26262 compliance, as part of this process, entails complex analysis for the evaluation of potential random hardware faults. This paper proposes a systematic approach to identify faults that do not disrupt safety-critical functionalities and consequently can be considered Safe. By deploying code coverage and Formal verification techniques, our methodology enables the classification of faults that are unclassified by other technologies, improving ISO26262 compliance. Our results, in combination with Fault Simulation, achieved a Diagnostic Coverage of 93% in a CAN Controller. These figures allow an initial assessment for an ASIL B configuration of the IP.", "num_citations": "4\n", "authors": ["502"]}
{"title": "A dynamic hardware redundancy mechanism for the in-field fault detection in cores of GPGPUs\n", "abstract": " In the past, in most General-Purpose Graphic Processing Units (GPGPUs) application fields (e.g., multimedia and gaming), the reliability features were not so relevant. Nowadays, GPGPUs are used in new domains, such as the automotive one, where reliability plays a significant role. In this work, we describe a dynamic duplication with a comparison (DDWC) mechanism intended to harden the Scalar Processor (SP) units located in the Streaming multiprocessors (SM) of a GPGPU. The proposed mechanism targets the permanent faults that may arise inside the SPs. One additional SP unit is included in the system to compute redundantly the same operations of a selected SP. Results are compared, and possible failures detected. A custom reconfiguration instruction allows the dynamic selection of the target SP to be monitored. Experimental results show that the proposed mechanism introduces a limited area\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "Special Session: AutoSoC-A Suite of Open-Source Automotive SoC Benchmarks\n", "abstract": " The current demands for autonomous driving generated momentum for an increase in research in the different technologies required for these applications. Nonetheless, the limited access to representative designs and industrial methodologies poses a challenge to the research community. Considering this scenario, there is a high demand for an open-source solution that could support development of research targeting automotive applications. This paper presents the current status of AutoSoC, an automotive SoC benchmark suite that includes hardware and software elements and is entirely open-source. The objective is to provide researchers with an industrial-grade automotive SoC that includes all essential components, is fully customizable, and enables analysis of functional safety solutions and automotive SoC configurations. This paper describes the available configurations of the benchmark including an\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "Untestable faults identification in GPGPUs for safety-critical applications\n", "abstract": " Nowadays, General Purpose Graphics Processing Units (GPGPUs) devices are considered as promising solutions for high-performance safety-critical applications, such as those in the automotive field. However, their adoption requires solutions to effectively detect faults arising in the device during the operative life. Hence, effective in-field test solutions are required to guarantee high-reliability levels. In this paper, we leverage the results of Software-Based Self-Test (SBST) based approaches for GPGPUs by deploying new techniques for automating the identification of untestable faults (UF). Our methodology has achieved fault coverage of 82.8% when applied to an open-source implementation of the NVIDIA G80 GPU architecture. The proposed approach combining SBSTs and UFs identification appears as an effective solution for the reliability analysis of GPGPUs.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Challenges of reliability assessment and enhancement in autonomous systems\n", "abstract": " The gigantic complexity and heterogeneity of today's advanced cyber-physical systems and systems of systems is multiplied by the use of avant-garde computing architectures to employ artificial intelligence based autonomy in the system. Here, the overall system's reliability comes along with requirements for fail-safe, fail-operational modes specific to the target applications of the autonomous system and adopted HW architectures. The paper makes an overview of reliability challenges for intelligence implementation in autonomous systems enabled by HW backbones such as neuromorphic architectures, approximate computing architectures, GPUs, tensor processing units (TPUs) and SoC FPGAs.", "num_citations": "4\n", "authors": ["502"]}
{"title": "A Dynamic Greedy Test Scheduler for Optimizing Probe Motion in In-Circuit Testers\n", "abstract": " The test of a printed-circuit board assembly often includes in-circuit test, which mainly aims at checking whether the different components have been correctly soldered. A tester may adopt either the bed of nails, or the flying probes architecture. In the latter case, probes move to contact test points on each side of the board to perform the required tests. In order to minimize the test time, the sequence of movements of the probes should be optimized, taking into account the tester capabilities, the board layout, and the several constraints coming from the environment and the customer. In this paper we describe the approach developed for optimizing tests on the SPEA 4080, which exploits the new hardware available to combine reduced test time with short test-generation time. Experimental results show the effectiveness of the proposed solution.", "num_citations": "4\n", "authors": ["502"]}
{"title": "A semi-formal technique to generate effective test sequences for reconfigurable scan networks\n", "abstract": " The broad need to efficiently access all the instrumentation embedded within a semiconductor device called for a standardization, and the reconfigurable scan networks proposed in IEEE 1687 have been demonstrated effective in handling complex infrastructures. At the same time, different techniques have been proposed to test the new circuitry required; however, most of the automatic approaches are either too computationally demanding to be applied in complex cases, or too approximate to yield high-quality tests. This paper models the state of a reconfigurable scan network with a finite state automaton, using the length of the active path as the output alphabet and the configurations as input symbols. Permanent faults are represented as incorrect transitions, and a greedy algorithm is used to generate a functional test sequence able to detect all these multiple state-transition faults. The automaton's state set and\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "On the test of a COTS-based system for space applications\n", "abstract": " The MaMMoTH-Up project aims at designing and manufacturing a COTS-based system to be used on the Ariane5 launcher. One of the key challenges in the project lies in guaranteeing the same level of reliability of previous systems, which were based on space qualified components. To achieve this goal, new solutions for the test of the system hardware were developed, able to guarantee a high level of fault coverage while matching several constraints in terms of system accessibility and hardware complexity. An approach based on Software-based Self-test is described to test the OR1200 processor adopted in the system, combined with new and effective techniques for identifying the on-line functionally untestable faults. Results including a comparison between a functional and a structural test approach are also reported.", "num_citations": "4\n", "authors": ["502"]}
{"title": "A high-level approach to analyze the effects of soft errors on lossless compression algorithms\n", "abstract": " In space applications, the data logging sub-system often requires compression to cope with large amounts of data as well as with limited storage and communication capabilities. The usage of Commercial off-the-Shelf (COTS) hardware components is becoming more common, since they are particularly suitable to meet high performance requirements and also to cut the cost with respect to space qualified ones. On the other side, given the characteristics of the space environment, the usage of COTS components makes radiation-induced soft errors highly probable. The purpose of this work is to analyze a set of lossless compression algorithms in order to compare their robustness against soft errors. The proposed approach works on the unhardened version of the programs, aiming to estimate their intrinsic robustness. The main contribution of the work lies in the investigation of the possibility of performing an\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "Rejuvenation of NBTI-Impacted Processors Using Evolutionary Generation of Assembler Programs\n", "abstract": " The time-dependent variation caused by Negative Bias Temperature Instability (NBTI) is agreed to be one of the main reliability concerns in integrated circuits implemented with current nanotechnology nodes. NBTI increases the threshold voltage of pMOS transistors: hence, it slows down signal propagation along logic paths between flip-flops. It may cause intermittent faults and, ultimately, permanent functional failures in processor circuits. In this paper, we study an NBTI mitigation approach in processor designs by rejuvenation of pMOS transistors along NBTI-critical paths. The method incorporates hierarchical fast, yet accurate modelling of NBTI-induced delays at transistor, gate and path levels for generation of rejuvenation Assembler programs using an Evolutionary Algorithm. These programs are applied further as an execution overhead to drive those pMOS transistors to the recovery phase, which are the most\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "Scan-chain intra-cell aware testing\n", "abstract": " This paper first presents an evaluation of the effectiveness of different test pattern sets in terms of ability to detect possible intra-cell defects affecting the scan flip-flops. The analysis is then used to develop an effective test solution to improve the overall test quality. As a major result, the paper demonstrates that by combining test vectors generated by a commercial ATPG to detect stuck-at and delay faults, plus a fragment of extra test patterns generated to specifically target the escaped defects, we can obtain a higher intra-cell defect coverage (i.e., 6.46 percent on average) and a shorter test time (i.e., 42.20 percent on average) than by straightforwardly using an ATPG which directly targets these defects.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Analysis of the effects of soft errors on compression algorithms through fault injection inside program variables\n", "abstract": " Data logging applications, such as those deployed in satellite launchers to acquire telemetry data, may require compression algorithms to cope with large amounts of data as well as limited storage and communication capabilities. When commercial-off-the-shelf hardware components are used to implement such applications, radiation-induced soft errors may occur, especially during the last stages of the launcher cruise, potentially affecting the algorithm execution. The purpose of this work is to analyze two compression algorithms using fault injection to evaluate their robustness against soft errors. The main contribution of the work is the analysis of the compression algorithm susceptibility by attacking their data structures (also referred as program variables) rather than the memory elements of the computing platform in charge of the algorithm execution. This approach is agnostic of the downstream implementation\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "Early reliability evaluation of a biomedical system\n", "abstract": " Early reliability evaluation for safety-critical applications is crucial, since it may allow to spot critical parts of the design and to introduce suitable countermeasures. In some domains it is common to adopt a design flow exploiting a high-level description of the system behavior and architecture; out of this description, suitable tools then automatically generate the software (and eventually the hardware) needed to perform the required tasks. This paper describes an enhanced version of such a design flow in which reliability is also considered and evaluated. The model of a pacemaker is developed and used for early estimation of its robustness with respect to a subset of the possible faults. The paper highlights why it is important to take into account the environment the target system is designed to interact with (in this case the heart), thus making possible to identify the most critical faults, based on the severity of their effects.", "num_citations": "4\n", "authors": ["502"]}
{"title": "A new solution to on-line detection of control flow errors\n", "abstract": " Transient faults can affect the behavior of electronic systems, and represent a major issue in many safety-critical applications. This paper focuses on Control Flow Errors (CFEs) and extends a previously proposed method, based on the usage of the debug interface existing in several processors/controllers. The new method achieves a good detection capability with very limited impact on the system development flow and reduced hardware cost: moreover, the proposed technique does not involve any change either in the processor hardware or in the application software, and works even if the processor uses caches. Experimental results are reported, showing both the advantages and the costs of the method.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Validation and robustness assessment of an automotive system\n", "abstract": " Due to the growing complexity of automotive systems, including various modules (e.g., microcontrollers, DSPs, memories and IP cores), validation and debug have become increasingly complex, with consequent impact on time-to-market and quality. In this paper we propose a novel flow for hardware and software validation and debug through the use of an FPGA-based emulation platform, which provides a valuable support for these important phases of the development flow. The same emulation platform is also able to support faults injection in the device under validation. Fault injection is intended not only to provide an evaluation of the system fault tolerance, but also to support the debug of the embedded fault tolerance mechanisms. Experimental results on a real industrial case study allow to evaluate the effectiveness and costs of the proposed solution.", "num_citations": "4\n", "authors": ["502"]}
{"title": "On the on-line functional test of the Reorder Buffer memory in superscalar processors\n", "abstract": " The Reorder Buffer (ROB) is a key component in superscalar processors. It enables both in-order commitment of instructions and precise exception management even in those architectures that support out-of-order execution. The ROB architecture typically includes a memory array whose size may reach several thousands of bits. Testing this array may be important to guarantee the correct behavior of the processor. Proprietary BIST solutions typically adopted by manufacturers for end-of-production test are not always suitable for on-line test. In fact, they require the usage of test infrastructures that may be expensive, or may not be accessible and/or documented. This paper proposes an alternative solution, based on a functional approach, which has been validated resorting to both an architectural and a memory fault simulator.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Peak power estimation: A case study on cpu cores\n", "abstract": " High peak power consumption during test may lead to yield loss. On the other hand, reducing too much test power may lead to test escape. In order to overcome this problem, test power has to mimic the power consumed during functional mode, being as high as possible but not crossing the frontier of over-consumption. Measuring power consumption is a very time consuming activity, therefore many works in the literature focused on the indirect ways to provide power consumption estimation in a fast manner. In this paper we concentrate on a similar issue, concentrating our effort on devising a fast method for the identification and estimation of the peak power produced by test patterns. In particular we provide a detailed discussion on case studies related to peak power estimation of CPU cores when executing functional patterns, the proposed method uses the gate-level description of the CPU to identify a subset of\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "High-reliability fault tolerant digital systems in nanometric technologies: Characterization and design methodologies\n", "abstract": " This paper reports the main contribution of a project devoted to the definition of techniques to design and evaluate fault tolerant systems implemented using the SoPC paradigm, suitable for missionand safety-critical application environments. In particular, the effort of the five involved research units has been devoted to address some of the main issues related to the specific technological aspects introduced by these flexible platforms. The overall target of the research is the development of a design methodology for highly reliable systems realized on reconfigurable platforms based on a System-on-Programmable Chip (SoPC), as discussed in the next section.", "num_citations": "4\n", "authors": ["502"]}
{"title": "On the functional test of L2 caches\n", "abstract": " Caches are crucial components in today's processors (both stand-alone or integrated into SoCs) and they account for a growing percentage of the occupied silicon area. Therefore, their test (both at the end of the manufacturing and on-line) is crucial for the quality and reliability of the whole product. While in many cases cache test is based on Design for Testability (DfT) techniques, there are situations in which the functional approach is the only viable one. Previous papers addressed the issue of developing test programs for testing caches: since the constant trend is to organize them in different levels, in this paper we address the test of second level caches (L2). To the best of our knowledge, the paper presents the first functional test method for L2 caches: some experimental results also are provided to assess its effectiveness on the OpenSPARC T1 processor.", "num_citations": "4\n", "authors": ["502"]}
{"title": "On the functional test of branch prediction units based on the branch history table architecture\n", "abstract": " Branch Prediction Units (BPUs) are commonly used in pipelined processors, since they can significantly decrease the negative impact of branches in superscalar and RISC architectures. Traditional solutions, mainly based on scan, are often inadequate to effectively test these modules: in particular, scan does not represent a viable solution when Incoming Inspection or on-line test are considered. Functional test may stand as an effective solution in these situations, but requires effective algorithms to be available. In this paper we propose a functional approach targeting the test of BPUs based on the Branch History Table (BHT) architecture; the proposed approach is independent on the specific implementation of the BPU, and is thus widely applicable. Its effectiveness has been validated on a BPU resorting to an open-source computer architecture simulator and to an ad hoc developed HDL testbench\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "Functional Verification of DMA Controllers\n", "abstract": " Today\u2019s SoCs are composed of a wide variety of modules, such as microprocessor cores, memories, peripherals, and customized blocks directly related to the targeted application. To effectively perform simulation-based design verification of peripheral cores, it is necessary to stimulate the description in a broad range of behavior possibilities, checking the produced results. Different strategies for generating suitable stimuli have been proposed by the research community to functionally verify these modules and their interconnection when embedded in a SoC: however, their verification often remains a largely manual and unstructured operation. In this paper we describe a general approach to develop concise and effective sets of inputs by modeling the configuration modes of a peripheral with a graph, and creating paths able to cover all of its nodes: proper stimuli for the device are then directly derived from\u00a0\u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "Automotive Microcontroller End-of-Line Test via Software-Based Methodologies\n", "abstract": " The car market shows a clear trend towards an increasing presence of electronic devices in engine control systems, due to a strong market drive towards high-performance control systems. Empirical evidence shows that a conventional screening approach only targeting system functionality is not enough to reach the desired high-quality targets. In this paper a methodology is presented to generate a set of test programs that are able to perform a stress test on a widely known microcontroller core. The obtained test set is then characterized in terms of functional coverage.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Self checking circuit optimization by means of fault injection analysis: A case study on reed solomon decoders\n", "abstract": " This paper shows how the use of exhaustive fault injection campaigns in conjunction with the analysis of the property of a circuit, allows to improve the efficiency of the checker of self checking circuits. Experimental results coming from fault injection campaigns on a Reed-Solomon decoder demonstrated that by observing the occurred errors and the correspondent detection module has been possible to reduce the number of detection module, while paying a small reduction of the percentage of SEUs that can be detected.", "num_citations": "4\n", "authors": ["502"]}
{"title": "A new approach to the analysis of single event transients in VLSI circuits\n", "abstract": " Single event transients (SETs) on combinational gates are becoming an issue in deep sub-micron technologies, thus efficient and accurate techniques for assessing their impact are strongly required. This paper presents a new technique that embeds time-related information in the topology of the analyzed circuit, allowing evaluating the effects of SETs via zero-delay simulation instead of timed simulation. The analysis of complex designs becomes thus possible at a very limited cost in terms of CPU time. Moreover, circuits enriched with time-related information are suitable for hardware emulation thus allowing further reducing the time for SET-effect analysis, while providing the same accuracy of state-of-the-art approaches based on timed simulations. The paper reports results showing how the proposed method can be effectively used to analyze complex designs.", "num_citations": "4\n", "authors": ["502"]}
{"title": "A multi-level approach to the dependability analysis of CAN networks for automotive applications\n", "abstract": " Colibri: A multi-level approach to the dependability analysis of CAN networks for automotive applications Skip navigation Inicio Listar Comunidades Listar Items por: Fecha Publicaci\u00f3n Autor T\u00edtulo Materia Ayuda En pocas palabras Institucional Objetivos Beneficios Recursos Somos Contacto Equipo de Trabajo BiDYA Ingresar Mi Espacio Alertas Editar perfil Buscar english Icono del idioma espa\u00f1ol Icono del idioma 1.Colibri 2.Facultad de Ingenier\u00eda 3.Instituto de Ingenier\u00eda El\u00e9ctrica 4.Publicaciones acad\u00e9micas y cient\u00edficas Por favor, use este identificador para citar o enlazar este \u00edtem: https://hdl.handle.net/20.500.12008/21276 C\u00f3mo citar T\u00edtulo: A multi-level approach to the dependability analysis of CAN networks for automotive applications Autor: Corno, Fulvio P\u00e9rez Acle, Julio Ramasso, M Sonza Reorda, Matteo Violante, M Tipo: Art\u00edculo Descriptores: ELECTR\u00d3NICA Fecha de publicaci\u00f3n: 2004 Citaci\u00f3n: Corno, F, \u2026", "num_citations": "4\n", "authors": ["502"]}
{"title": "A charge correction cell for FGMOS-based circuits\n", "abstract": " This paper describes a novel cell used in circuits with floating gate MOS transistors (FGMOS) to compensate for variations in the device effective threshold voltage caused by the trapped charge at the floating gate. The performance of the circuit is illustrated with experimental results showing a residual error below 1%. This coarse compensation makes it possible to reduce charge effects to the same order of magnitude as the conventional mismatching in normal MOS transistors.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Built-In Self Test of Sequential Circuits\n", "abstract": " This chapter describes an innovative Built-In Self Test architecture based oncellular automata. The architecture is an enhancement of standard Circular Self-Test Path, and increases stuck-at fault coverage while maintaining all advantages, such as low timing intrusiveness, easy integration into design flow, at-speed testing. Cellular automaton rules are devised using the Selfish Gene algorithm, a new evolutionary algorithm based on an unorthodox view of the Darwinian theory, where the basic units of selection are genes rather than individuals. Experimental results show the effectiveness of the approach and the efficacy of the Selfish Gene algorithm.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Reducing test application time through interleaved scan\n", "abstract": " This paper proposes a new method for reducing the test length for digital circuits by adopting an architecture derived from the popular scan approach. An evolutionary optimization algorithm is exploited to find the optimal solution. The proposed approach was tested on the ISCAS89 standard benchmarks and the experimental results show its effectiveness.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Analysis of the Equivalences and Dominances of Transient Faults at the RT Level\n", "abstract": " This work presents a study for tackling transient faults at the RT-level and outlines the techniques devised and implemented to speed-up fault-injection campaigns, detecting the equivalences and dominances between faults in order to collapse them. Experimental results are provided on an industrial case study, demonstrating the effectiveness of the approach.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Experimental analysis of fault models for behavioral-level test generation\n", "abstract": " The diffusion of the System-on-Chip paradigm is making even more important addressing design at the highest levels of abstraction. Behavioral-level design tools are today commercially available, and offer a solution to this problem. Conversely, test issues are usually addressed at the lowest levels of abstraction and, although in recent years many efforts have been devoted to the definition of strategies for addressing test at the high level, a global solution is yet to come. In particular, several high-level fault models have been proposed, most of them working at the RT-level. In this paper we report preliminary experimental results about some of the available high-level fault models working at the behavioral-level. The experimental procedure we adopted is presented and some preliminary results are discussed.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Hardening the software with respect to transient errors: a method and experimental results\n", "abstract": " This paper deals with a software modification strategy allowing on-line detection of transient errors. Preliminary results are presented and discussed pointing out the effectiveness of this approach", "num_citations": "4\n", "authors": ["502"]}
{"title": "Design of a test simulation environment for test program development\n", "abstract": " Analog Devices has developed its own custom test simulation environment. Five pilot projects have been successfully completed to-date using this product. The design of this test simulation environment is presented.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Transformation-based peak power reduction for test sequences\n", "abstract": " This paper describes a new algorithm for transforming an existing test sequence for sequential circuits into a cheaper one from the point of view of peak power consumption. The algorithm exploits both symbolic techniques (to identify sub-sequences performing a given state transition) and heuristic methods. Preliminary experimental results show that the algorithm is able to reduce the peak power consumption of ATPG-generated sequences by up to 54%, while the reduction of the fault coverage is limited to at most 1%.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Exploiting the background debugging mode in a fault injection system\n", "abstract": " This note describes a software-implemented Fault Injection system suited to be used with embedded microprocessor-based boards and based on some features available in the most recent microprocessors and microcontrollers. Although these features were originally introduced to easy code development and debugging, they are very well suited for supporting the implementation of efficient and barely intrusive Fault Injection Systems. In particular, our Fault Injection system exploits the Background Debugging Mode (BDM), available in the last microprocessors and microcontrollers produced by Motorola.", "num_citations": "4\n", "authors": ["502"]}
{"title": "Fast differential fault simulation by dynamic fault ordering\n", "abstract": " A technique that makes it possible to significantly improve the effectiveness of the differential algorithm for the fault simulation of synchronous sequential circuits is presented. The approach is based on dynamically reordering the fault list before the simulation of each input pattern: faults not yet detected are grouped according to a strategy aiming at minimizing the status differences between successive faults. In such a way the activity to be processed while computing each faulty circuit is minimized at a quite low computational cost. Experimental results are provided showing the effectiveness of the proposed method.<>", "num_citations": "4\n", "authors": ["502"]}
{"title": "Improving GPU register file reliability with a comprehensive ISA extension\n", "abstract": " This work proposes a comprehensive ISA extension to improve GPU reliability to transient effects. Three additional instructions are proposed, implemented, and combined with software-based datapath duplication. Modified program codes are compared to state-of-the-art software-based fault tolerance techniques in terms of execution time. The circuit area is evaluated against the original GPU architecture, and a fault injection campaign is performed to assess reliability. Results show that this comprehensive ISA extension improves performance and fault detection capabilities of software-based approaches at negligible costs in terms of circuit area. This work can help engineers in designing more efficient and resilient GPU architectures.", "num_citations": "3\n", "authors": ["502"]}
{"title": "In-field functional test of CAN bus controllers\n", "abstract": " The Controller Area Network (CAN) bus is a serial bus protocol widely used in the automotive domain to allow communication between different Electronic Control Units in the car. Being often part of safety-critical systems, the hardware implementing the CAN network must be constantly tested along the system lifetime, even during the operational phase. CAN controllers are relatively complex modules in charge of managing the sending and the receiving of packages through the CAN bus and defects affecting them can easily compromise the whole CAN network. In this work, the CAN controller is tested by test programs to be executed by the CPU connected to the device under test and by another unit connected to the same CAN bus. A fault grading with respect to structural permanent faults of a functional test based on the execution of a software test library for the CAN bus is presented for the first time. Results show\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "Faults detection in the heatsinks mounted on power electronic transistors\n", "abstract": " Nowadays, power electronics is widely used in many applications, eg, in industrial field, transport field and household appliances used daily. Considering the high temperature reached in power circuits, it is necessary to introduce some heat dissipation systems able to transfer heat to the surrounding environment. The correct mounting of the heatsink or its physical deterioration over time can affect the operation of the system. The overheating may lead to an ageing acceleration of the power devices or to their permanent damage. This greatly influences the reliability and safety of power systems used in safety-critical applications, eg, for automotive, rail or industrial environments. Hence, it is necessary to introduce some test strategies to identify those heatsinks that do not operate correctly, eg, because they are not correctly assembled during the production of the Printed Circuit Board (PCB). This paper proposes a methodology to test the assembly of heatsinks on power devices. The proposed approach relies on an in-circuit test method at the end-production on the final PCB. The test is carried out without resorting to thermal measurements. Generally, the thermal measurements cannot be performed by the modern Automatic Test Equipment (ATE). The proposed test methodology was evaluated experimentally on a power MOSFET in TO220 package; the MOSFET considered is used in a half bridge.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Assessing the effectiveness of the test of power devices at the board level\n", "abstract": " Power devices have an increasing relevance in many applications, including some safety-critical ones. In the latter case, the effectiveness of the test performed at the end of the manufacturing, when the device is already mounted on the final board, is crucial. Unfortunately, assessing such effectiveness is not trivial, since it requires defining a metric that could be measured in an objective manner. Following a trend that is common to the whole world of analog components, in a previous paper we proposed a fault model based on the availability of the electrical model of the power device. Using this fault model, the test engineer can assess and possibly improve the quality of the developed test solution, and optimize the overall test plan so that a given fault coverage is achieved with minimum cost. The proposed fault requires the availability of the electrical model of the power device. In this paper we adopt this approach\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "Software-based self-test for transition faults: a case study\n", "abstract": " Scan chain-based testing is a de facto standard for guaranteeing quality of manufactured digital circuits. However, functional approaches are often used to complement test suites, especially when analog circuitry is integrated in the chip. Software-Based Self-Test (SBST) can be used to increase defect coverage also in digital parts, or to replace part of the scan pattern set to reduce tester requirements, or to complement the defect coverage achieved by structural techniques when advanced semiconductor technologies introduce new defect types. This paper deals with SBST targeting transition delay faults, and describes a case of study based on a peripheral module integrated in a System on Chip (SoC). A method to develop an effective functional test is first described. A comparative analysis of the delay faults detected by scan and SBST is then presented, together with some discussion about the obtained results.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Test-Plan Optimization for Flying-Probes In-Circuit Testers\n", "abstract": " The test of a printed-circuit board assembly often includes in-circuit test, which mainly aims at checking whether the different components have been correctly soldered. A tester may adopt either the bed of nails, or the flying-probes architecture. In the latter case, probes move to contact test points on each side of the board in order to perform the required tests. In order to minimize the test time, the sequence of movements of the probes should be re-arranged, considering the tester capabilities, the board layout, and several constraints coming from the environment and the customer. In this paper we describe the approach developed for optimizing tests on the SPEA 4080, which combines reduced test time with short test-generation time. Experimental results show the effectiveness of the proposed solution.", "num_citations": "3\n", "authors": ["502"]}
{"title": "A Novel Sequence Generation Approach to Diagnose Faults in Reconfigurable Scan Networks\n", "abstract": " With the complexity of nanoelectronic devices rapidly increasing, an efficient way to handle large number of embedded instruments became a necessity. The IEEE 1687 standard was introduced to provide flexibility in accessing and controlling such instrumentation through a reconfigurable scan chain. Nowadays, together with testing the system for defects that may affect the scan chains themselves, the diagnosis of such faults is also important. This article proposes a method for generating stimuli to precisely identify permanent high-level faults in a IEEE 1687 reconfigurable scan chain: the system is modeled as a finite state automaton where faults correspond to multiple incorrect transitions; then, a dynamic greedy algorithm is used to select a sequence of inputs able to distinguish between all possible faults. Experimental results on the widely-adopted ITC'02 and ITC'16 benchmark suites, as well as on synthetically\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "New categories of Safe Faults in a processor-based Embedded System\n", "abstract": " The identification of safe faults (i.e., faults which are guaranteed not to produce any failure) in an electronic system is a crucial step when analyzing its dependability and its test plan development. Unfortunately, safe fault identification is poorly supported by available EDA tools, and thus remains an open problem. The complexity growth of modern systems used in safety-critical applications further complicates their identification. In this article, we identify some classes of safe faults within an embedded system based on a pipelined processor. A new method for automating the safe fault identification is also proposed. The safe faults belonging to each class are identified resorting to Automatic Test Pattern Generation (ATPG) techniques. The proposed methodology is applied to a sample system built around the OpenRisc1200 open source processor.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Fault-independent test-generation for software-based self-testing\n", "abstract": " Software-based self-test (SBST) is being widely used in both manufacturing and in-the-field testing of processor-based devices and systems-on-chips. Unfortunately, the stuck-at fault model is increasingly inadequate to match the new and different types of defects in the most recent semiconductor technologies, while the explicit and separate targeting of every fault model in SBST is cumbersome due to the high complexity of the test-generation process, the lack of automation tools, and the high CPU-intensity of the fault-simulation process. Moreover, defects in advanced semiconductor technologies are not always covered by the most commonly used fault-models, and the probability of defect-escapes increases even more. To overcome these shortcomings, we propose the first fault-independent method for generating SBST procedures. The proposed method is almost fully automated, it offers high coverage of non\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "Towards Making Fault Injection on Abstract Models a More Accurate Tool for Predicting RT-Level Effects\n", "abstract": " Fault injection and fault simulation are a typical approach to analyze the effect of a fault on a hardware/software system. Often fault injection is done on abstract models of the system either to retrieve early results when no implementation is available, yet, or to speed-up the runtime intensive fault simulation on detailed models. The simulation results from the abstract model are typically inaccurate because details of the concrete hardware are missing. Here, we propose an approach to relate faults from an abstract untimed algorithmic model to their counterparts in the concrete register transfer models. This allows to understand which faults are covered on the concrete model and to speed up the fault simulation process. We use a mapping between both models' variables and mapped timing states for fault injection to corresponding variables on both models. After fault simulations the results are compared to check\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "A fault-tolerant ripple-carry adder with controllable-polarity transistors\n", "abstract": " This article first explores the effects of faults on circuits implemented with controllable-polarity transistors. We propose a new fault model that suits the characteristics of these devices, and we report the results of a SPICE-based analysis of the effects of faults on the behavior of some basic gates implemented with them. Hence, we show that the considered devices are able to intrinsically tolerate a rather high number of faults. We finally exploit this property to build a robust and scalable adder whose area, performance, and leakage power characteristics are improved by 15%, 18%, and 12%;, respectively, when compared to an equivalent FinFET solution at 22nm technology node.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Improving the functional test delay fault coverage: a microprocessor case study\n", "abstract": " Functional test guarantees that the circuit is tested under normal conditions, thus avoiding any over-as well as under-test issues. This work is based on the use of Software-Based-Self-Test that allows a special application of functional test to the processor-based systems. This strategy applies the so-called functional test programs that are executed by the processor to guarantee a given fault coverage. The main goal of this paper is to investigate a methodology to improve the delay fault coverage of a given test set of functional test programs. We propose to exploit existing Design-for-Test architecture to apply in a smarter way the functional programs. Then, we combine those programs with the classical at-speed LOC and LOS delay fault testing schemes to further increase the delay fault coverage. Results show that it is possible to achieve a global test solution able to maximize the delay fault coverage.", "num_citations": "3\n", "authors": ["502"]}
{"title": "On the diagnostic analysis of IEEE 1687 networks\n", "abstract": " The IEEE 1687 standard describes reconfigurable structures allowing to flexibly access the instruments existing within devices (e.g., to support test, diagnosis, calibration, etc.), by using configuration modules which act as controllable switches. The increasing adoption of this standard requires the availability of algorithms and tools to automate its usage. The resulting networks might be affected by defects preventing their correct operation. This necessitates the availability of solutions which allow not only to test against defects, but also to identify the location of possible faults via diagnosis. This paper for the first time addresses the problem of the diagnosis of IEEE 1687 networks. Experimental results gathered on a set of benchmark networks show the feasibility of the solution and provide a first idea about the length of the required input stimuli.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Hybrid soft error mitigation techniques for COTS processor-based systems\n", "abstract": " In this paper we combine a set of software-based fault tolerance techniques with a hardware module that monitors the trace port, and explore from an experimental point of view the fault coverage against soft errors in COTS processors that can be achieved. The costs in terms of performance and memory are also evaluated. Fault injection results show fault coverage is superior to the state-of-the-art techniques with lower performance and memory overheads.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Observability solutions for in-field functional test of processor-based systems\n", "abstract": " The growing usage of electronic systems in safety-critical applications requires effective solutions to early identify possible faults affecting the hardware while it is in the operational phase. A possible approach leverages functional programs to be run by the CPU typically existing in such systems. These programs must exercise the different parts of the system, and produce a behavior different than the normal one in case of faults. However, their effectiveness depends on the adopted observation mechanism, which is deeply affected by the constraints imposed by the in-field application environment. This paper first describes different mechanisms for supporting the observation of possible fault effects; then, it reports and discusses the results of an experimental analysis performed on a multicore system, based on a representative pipelined processor. The gathered results allow to quantitatively evaluate the drop in fault\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "On the development of diagnostic test programs for VLIW processors\n", "abstract": " Software-Based Self-Test (SBST) approaches have shown to be an effective solution to detect permanent faults, both at the end of the production process, and during the operational phase. When partial reconfiguration is adopted to deal with permanent faults, we also need to identify the faulty module, which is then substituted with a spare one. Software-based Diagnosis techniques can be exploited for this purpose, too. When Very Long Instruction Word (VLIW) processors are addressed, these techniques can effectively exploit the parallelism intrinsic in these architectures. In this paper we propose a new approach that starting from existing detection-oriented programs generates a diagnosis-oriented test program which in most cases is able to identify the faulty module. Experimental results gathered on a case study show the effectiveness of the proposed approach.", "num_citations": "3\n", "authors": ["502"]}
{"title": "E-learning at Politecnico di Torino: Moving to a sustainable large-scale multi-channel system of services\n", "abstract": " Politecnico di Torino has been actively experimenting distance education scenarios since 1992, through the development of innovative methodologies and tools. The real challenge today, however, is to move from small settings to a large-scale system able to suit the needs of a broad number of users belonging to different categories, from traditional students to part-time or full-time workers, from students living far from Torino to people with participation restriction due to disability. The emphasis then, is not only on the innovation of methodologies and technologies, but on their effective and economically sustainable use in a complex and multi-faceted setting. This chapter describes the services introduced in this direction and gives a preliminary evaluation after the first year of delivery.", "num_citations": "3\n", "authors": ["502"]}
{"title": "An FPGA-emulation-based platform for characterization of digital baseband communication systems\n", "abstract": " Integrated transceivers play a leading role in an ever-increasing range of systems and applications where dependability is a major concern. In order to reduce time-to-market, it is crucial to verify the correctness of the hardware and software implementations and the overall system performance at early stages of the design flow with respect to specifications or a higher-level model. To this purpose, we identify some specific requirements of these systems, and propose a new methodology for early validation and BER characterization taking into consideration channel noise and sampling frequency offset of the digital base band communication subsystems. The proposed methodology is based on a low-cost FPGA platform and enables precise control of noise injection and clocking parameters during emulation, so as to reproduce channel and receiver non-ideality effects, while accelerating the validation process by\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "Challenges in next-generation and resource-constrained networks\n", "abstract": " Network technologies are undergoing rapid advancements. Researchers are currently envisioning different attractive properties, such as the ability to self-organize, self-configure, self-heal, self-manage and self-maintain, of future network technologies, many of which are resource-constrained. Different networks having the potential to offer cost-effective home and enterprise access networking solutions are being researched. Concepts such as dynamic spectrum access, convergence, unified network architectures and seamless service access in heterogeneous networks are gaining widespread popularity. Technologies such as Wireless Mesh Networks (WMNs), WiFi, WiMax, Bluetooth, IEEE 802.20, IEEE 802.22 and software defined radio are gaining popularity. Even though these technologies hold great promises for our future, there are several research challenges that need to be addressed. In this Special Issue, we primarily targeted to publish high quality research papers relating to the recent advances in the challenges faced by next-generation and resource-constrained networks. This Special Issue presents 12 papers addressing different aspects of these network tech-", "num_citations": "3\n", "authors": ["502"]}
{"title": "Cumulative embedded memory failure bitmap display & analysis\n", "abstract": " An effective silicon debug and diagnosis process has to be supported by on-chip hardware structures, stimulation equipments and software tools for analysis. In this paper, the characteristics of a software tool for memory failure analysis are presented; this tool takes into account the memory topology and the executed memory test, and returns both syndrome and shape-based failure statistics. Furthermore, it allows the cumulative analysis over many memory cuts inside a die, a wafer or a lot. The results obtained for embedded SRAMs tested using March test algorithms are presented, demonstrating the capability of the tool in underlining manufacturing process weaknesses and systematic constructive marginalities.", "num_citations": "3\n", "authors": ["502"]}
{"title": "A new hardware/software platform for the soft-error sensitivity evaluation of FPGA devices\n", "abstract": " As commercial-off-the-shelf SRAM-based FPGA devices are becoming attractive for mass-market products and not only for mission-or safety-critical applications, their sensitivity to atmospheric neutrons is raising several concerns. This paper proposes a new platform for studying the effects of atmospheric neutrons (as well as other radiations) on FPGA devices by providing a solution for tracking soft errors from their origin up to the involved logic resource. Thanks to this approach, soft errors can be analyzed from the designer's point of view in terms of device resources, such as gates, memory elements, and interconnections.", "num_citations": "3\n", "authors": ["502"]}
{"title": "A Software-Based methodology for the generation of peripheral test sets Based on high-level descriptions\n", "abstract": " Nowadays, the use of Systems-on-Chip (SoCs) represents a very interesting solution, but also introduces some testing concerns. Up to now, researchers focused many efforts on the development of new software and hardware techniques for testing processors embedded in SoCs. However, the test of the surrounding peripherals has not been the subject of many research works, even if their importance within the entire system may be considerable. In this paper we focus on Software-based Self-Test techniques for testing peripheral components within a SoC and explore the possibility that test generation only relies on high-level metrics. We outline a possible test generation and application flow, and discuss the suitability of different RT-level metrics. By exploiting a sample case study, we quantitatively evaluate the effectiveness of the different metrics and the practical viability of the considered approach. As a major contribution, the paper shows that for peripheral components the relationship between high-level and gate-level metrics is higher than for the general case.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Exploiting an infrastructure IP to reduce memory diagnosis costs in SoCs\n", "abstract": " Discriminating between good and faulty chips is often not enough during IC manufacturing phases, where a complete understanding about failure mechanisms is required to ramp up production yield. When considering embedded memories, information about the whole set of faults needs to be extracted from the IC and processed: this asks for solutions supporting high data volume transfer. We propose an embedded architecture allowing efficient diagnosis of SoCs containing several BISTed memory cores, which minimizes ATE memory requirements for pattern storage and drastically speeds up the complete diagnostic procedure. Experimental results highlight the convenience of the approach with respect to alternative ATE driven procedures, while resorting to negligible area overhead.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Emulation-based analysis of soft errors in deep sub-micron circuits\n", "abstract": " The continuous technology scaling makes soft errors a critical issue in deep sub-micron technologies, and techniques for assessing their impact are strongly required that combine efficiency and accuracy. FPGA-based emulation is a promising solution to tackle this problem when large circuits are considered, provided that suitable techniques are available to support time-accurate simulations via emulation. This paper presents a novel technique that embeds time-related information in the topology of the analyzed circuit, allowing evaluating the effects of the soft errors known as single event transients (SETs) in large circuits via FPGA-based emulation. The analysis of complex designs becomes thus possible at a very limited cost in terms of CPU time, as showed by the case study described in the paper.", "num_citations": "3\n", "authors": ["502"]}
{"title": "SW-Based Fault Handling Mechanisms to Cope with EMI in Embedded Electronics: are they a good remedy? a Case Study on a COTS Microprocessor\n", "abstract": " We present a preliminary study on the effectiveness of two software-based fault-handling mechanisms in terms of detecting conducted electromagnetic interference (EMI) in microprocessors. One of these techniques deals with processor control flow checking. The second one is used to detect errors in code variables. In order to check the effectiveness of such techniques in RF ambient, an IEC 61.000-4-29 Normative-compliant conducted RF-generator was proposed and constructed to inject spurious EMI noise into the supply lines of a commercial off-the-shelf (COTS) microcontroller-based system.Experimental results suggest that the considered techniques present a good effectiveness to detect this type of faults, despite the multiple-fault injection nature of electromagnetic interference in the processor control flow and data, which in most cases results in a complete system functional loss (the system must be reset).", "num_citations": "3\n", "authors": ["502"]}
{"title": "Behavioral-level fault models comparison: An experimental approach\n", "abstract": " While the design practice is quickly moving toward higher levels of abstraction, test issues are still considered only when a detailed description of the design is available, typically at the gate-level for test sequence generation purposes and register transfer (RT)-level for design for testability structures insertion. In the past years, intensive research efforts have been devoted to devise solutions tackling test sequence generation since the early design phases, mainly the RT-level and several approaches have been proposed. In this paper we report preliminary results of an experimental analysis of the available highlevel fault models when test is addressed at the behavioral-level. The experimental procedure we adopted is presented and some preliminary results are discussed.", "num_citations": "3\n", "authors": ["502"]}
{"title": "An interpretation framework for evaluating high-level fault models and ATPG capabilities\n", "abstract": " Most ASICs are automatically generated from RT-(or higher) level de-scriptions. However, most of the test activities are still performed on gate-level. Several research efforts have been performed to devise test methods able to work directly on high-level descriptions. However, to develop indus-trially effective high-level test methodologies the relationship between high-level fault models and traditional gate-level stuck-at faults must be clearly understood. This paper presents an Interpretation Framework for evaluating, analyzing and understanding the results of RT-Level Fault Models and ATPGs. The proposed framework is applied to analyze the results of a specific tool. The investigation discriminates between the deficiencies of the tool algo-rithm and the adopted Fault Model, and shows that it is possible to foresee at the RT-level a large portion of untested gate-level faults.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Verifying the equivalence of sequential circuits with genetic algorithms\n", "abstract": " In the design flow of digital VLSI circuits, modern state-of-the-art computer-aided design techniques implemented in automatic synthesis and optimization tools can handle designs with hundreds of flip-flops. However, many design steps are not guaranteed to be correct, either due to human intervention or to software bugs. The final correctness of the produced circuit, therefore, heavily depends of the existence of an accurate and effective verification phase. This paper presents a new verification methodology suitable for use when the equivalence between two gate-level versions of the same circuit must be verified (e.g., after an optimization step); the approach is based on genetic algorithms and, while sometimes sacrificing exactness, is able to handle large circuits and give designers the opportunity to trade off CPU time with confidence on the result. The proposed methodology is able to fruitfully integrate the results\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "ALPS: A peak power estimation tool for sequential circuits\n", "abstract": " Tools for evaluating the worst-case peak power consumption of sequential circuits are highly useful to designers of low-power circuits. Previously proposed methods search for the initial state and the couple of vectors with maximum consumption, without fully considering the reachability of the initial state. This paper shows that this approach can lead to a significant underestimation of the maximum peak power consumption and proposes a new algorithm that overcomes this drawback. Experimental results show that for many circuits the algorithm is able to provide better results than those known up to now, while an approximate version is able to deal even with the largest benchmark circuits.", "num_citations": "3\n", "authors": ["502"]}
{"title": "The general product machine: a new model for symbolic FSM traversal\n", "abstract": " Proving the equivalence of two Finite State Machines (FSMs) has many applications to synthesis, verification, testing, and diagnosis. Building their product machine is a theoretical framework for equivalence proof. There are some cases where product machine traversal, a necessary and sufficient check, is mandatory. This is much more complex than traversing just one of the component machines. This paper proposes an equivalence-preserving function that transforms the product machine in theGeneral Product Machine (GPM). Using the GPM in symbolic state space traversal reduces the size of the BDDs and makes image computation easier. As a result, GPM traversal is much less expensive than product machine traversal, its cost being close to dealing with a single machine.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Scan chain partitioning and re-ordering based on layout information: An industrial experience\n", "abstract": " Sub-micron technologies require a new approach for managing scan chains in order to avoid problems during the layout phase. The paper describes the approach followed in an industrial environment to deal with the new situation. A solution is reported, which is based on taking into account information coming from the layout and ATPG tools in an integrated manner; a Genetic Algorithm is used to partition the scan flip flops in chains and to find an optimal ordering of flip flops in the chains themselves. The approach is very flexible, as it can be adapted to the case in which additional constraints have to be considered.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Exploiting symbolic techniques within genetic algorithms for power optimization\n", "abstract": " Proposes an optimization algorithm for reducing the power dissipation in a sequential circuit. The encoding of the different states in a finite-state machine is modified to obtain a functionally equivalent circuit that exhibits a reduced power dissipation. The algorithm is based on a newly-proposed power estimation function that is able to quickly give an accurate estimate of the dissipated power without actually synthesizing the circuit. Given this estimate, a genetic algorithm provides a state re-encoding for the circuit. The estimation function is computed in a very efficient way by exploiting some symbolic computations with binary decision diagrams. The algorithm is experimentally shown to provide good results from the power optimization point of view, at a limited cost in terms of area increase, when compared with similar approaches.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Fault Tolerant and BIST design of a FIFO cell\n", "abstract": " This paper presents a BIST design of a parametrized FIFO component. The component is currently being used in the standard library of Italtel, the main Italian telecom circuit maker. Design choices have been strongly influenced by industrial constraints imposed by the Italtel design flow. To achieve the desired fault coverage level for faults in the memory and in the control logic, traditional BIST schemes had to be combined with more advanced testing techniques. Different parts of the circuits are tested with different strategies and algorithms to account for their different nature: critical parts of the design, such as the FIFO control unit and the BIST controller are tested with on-line test techniques. The final implementation shows that a high fault coverage is attained with an acceptable area overhead and no speed penalty.", "num_citations": "3\n", "authors": ["502"]}
{"title": "An Experimental analysis of the effectiveness of the Circular Self-Test Path technique\n", "abstract": " The paper analyzes the effectiveness of the Circular Self-Test Path technique from an experimental point of view. Several fault simulation experiments have been performed on the ISCAS89 benchmark set as well as on a set of real circuits: in contrast to the theoretical analysis proposed in [PKKa92], a very high Fault Coverage has been attained with a limited number of clock cycles, provided that the circuit does not enter a loop. This danger can not be avoided even if clever strategies for Flip-Flops ordering, aimed at reducing the functional adjacency, are adopted; the effects of carefully choosing the initial state are investigated and an approach based on Formal Verification techniques is proposed.", "num_citations": "3\n", "authors": ["502"]}
{"title": "TORSIM: An efficient fault simulator for synchronous sequential circuits\n", "abstract": " The paper describes a new approach to the fault simulation of synchronous sequential circuits. Its novelty comes from combining the event-driven compiled-code simulation technique proposed by H. K. Lee and D. S. Ha (1992) with the single fault propagation fault-parallel fault simulation algorithm used by F. Maamari and J. Rajski (1988). Our approach is particularly suited for those applications requiring the fault simulation of very high numbers of input patterns, like signature computation or fault dictionary construction. A fault simulator named TORSIM has been written to verify the effectiveness of the approach. The results we present show an average speed-up in terms of CPU time of more than one order of magnitude with respect to the ones reported by Maamari and Rajski.< >", "num_citations": "3\n", "authors": ["502"]}
{"title": "Exploiting a Workstation Network for Automatic Generation of Test Patters for Digital Circuits\n", "abstract": " This1 paper deals with the problem of generating test pattern sequences for large synchronous sequential circuits. A workstation network is exploited in order to tame the growth in CPU time requirements caused by the increase in circuit size and complexity. An approach based on Genetic Algorithms is adopted. Experimental results show that the CPU time required to reach a given fault coverage can be greatly reduced with respect to previous methods, and that the approach is able to provide a good speed-up with respect to the mono-processor version.", "num_citations": "3\n", "authors": ["502"]}
{"title": "Comparing ATPGs for synchronous sequential circuits\n", "abstract": " A comparison is made of some of the approaches to sequential automatic test pattern generation (ATPG). The criterion that guided the choice was to select one or more representatives of the topological, simulation-based, or functions-and automata-based methods for which experimental data on the standard benchmark set of F. Brglez et al.(IEEE Int. Symp. on Circ. and Syst., p. 1929, May 1989) were available. The authors consider STG3 (sequential circuit Test Generator) and HITEC as representatives of the topological approach, CONTEST (concurrent testing) for the simulation-based one, and SETA (Sequential Testing based on Automata) for the automaton-based class. A comparison is presented of experimental results in terms of fault coverage and CPU time. Once the most awkward circuits have been identified, the authors show the influence of structural parameters on ATPG performance, specifically in terms\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "C TPDL\u2217: Adapting TPDL\u2217 to concurrent simulation environments\n", "abstract": " This paper presents an extension to TPDL\u2217 in the domain of concurrent simulation of both good and faulty machines. The new language, called C TPDL\u2217 is general purpose and capable of expressing a wide range of logical and temporal relations at different levels of abstraction in concurrent and parallel simulation environments.", "num_citations": "3\n", "authors": ["502"]}
{"title": "On Test Program Generation for Peripheral Components in a SoC Resorting to High-Level Metrics\n", "abstract": " Nowadays, the use of Systems-on-Chip (SoCs) in different types of systems represents a very interesting solution, but also introduces some testing concerns that must be considered. Up to now, researchers focused many efforts on the development of new software and hardware techniques for testing processors embedded in SoCs. However, the test of the surrounding peripherals has not been the subject of many research works, even if their importance within the entire system may be considerable. In this paper we focus on Software-based Self-Test techniques for testing peripheral components within a SoC and explore the possibility that test generation only rely on high-level metrics. We outline a possible test generation and application flow, and discuss the suitability of different RT-level metrics. By exploiting a sample case study, we quantitatively evaluate the effectiveness of the different metrics and the\u00a0\u2026", "num_citations": "3\n", "authors": ["502"]}
{"title": "Combining Architectural Simulation and Software Fault Injection for a Fast and Accurate CNNs Reliability Evaluation on GPUs\n", "abstract": " Graphic Processing Units (GPUs) are commonly used to accelerate Convolutional Neural Networks (CNNs) for object detection and classification. As CNNs are employed in safety-critical applications, such as autonomous vehicles, their reliability must be carefully evaluated. In this work, we combine the accuracy of microarchitectural simulation with the speed of software fault injection to investigate the reliability of CNNs executed in GPUs. First, with a detailed microarchitectural fault injection on a GPU model (FlexGripPlus), we characterize the effects of faults in critical and user-hidden modules (such as the Warp Scheduler and the Pipeline Registers) in the computation of convolution over a suitably selected subset of tiles. Then, with software fault injection, we propagate the fault effects in the CNN. Thanks to our approach we are able, for the first time, to analyze the impact of faults affecting GPUs\u2019 hidden modules\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "New Techniques for the Automatic Identification of Uncontrollable Lines in a CPU Core\n", "abstract": " In several test and reliability problems (from test generation to FMECA and Burn In) it is important to preliminarily identify those lines in a circuit netlist, which can not be controlled, i.e., can not be toggled to both logic values no matter the applied stimuli. Several techniques have been proposed in the past to attack this problem. In this paper we consider the case where the circuit is a pipelined processor, discuss the specific challenges of this scenario and propose some techniques to automatically identify some of the uncontrollable lines. The approach we devised uses SAT solving as underlying technology. We report the results we gathered on the OR1200 processor, showing that our method allows to trade off between the required computational effort and the achieved results. When compared with results produced by a commercial tool, our approach is able to identify a much higher number of uncontrollable lines\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "DYRE: a DYnamic REconfigurable solution to increase GPGPU\u2019s reliability\n", "abstract": " General-purpose graphics processing units (GPGPUs) are extensively used in high-performance computing. However, it is well known that these devices\u2019 reliability may be limited by the rising of faults at the hardware level. This work introduces a flexible solution to detect and mitigate permanent faults affecting the execution units in these parallel devices. The proposed solution is based on adding some spare modules to perform two in-field operations: detecting and mitigating faults. The solution takes advantage of the regularity of the execution units in the device to avoid significant design changes and reduce the overhead. The proposed solution was evaluated in terms of reliability improvement and area, performance, and power overhead costs. For this purpose, we resorted to a micro-architectural open-source GPGPU model (FlexGripPlus). Experimental results show that the proposed solution can extend the\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "Multilevel simulation methodology for FMECA study applied to a complex cyber-physical system\n", "abstract": " Complex systems are composed of numerous interconnected subsystems, each designed to perform specific functions. The different subsystems use many technological items that work together, as for the case of cyber-physical systems. Typically, a cyber-physical system is composed of different mechanical actuators driven by electrical power devices and monitored by sensors. Several approaches are available for designing and validating complex systems, and among them, behavioral-level modeling is becoming one of the most popular. When such cyber-physical systems are employed in mission-or safety-critical applications, it is mandatory to understand the impacts of faults on them and how failures in subsystems can propagate through the overall system. In this paper, we propose a methodology for supporting the failure mode, effects, and criticality analysis (FMECA) aimed at identifying the critical faults and assessing their effects on the overall system. The end goal is to analyze how a fault affecting a single subsystem possibly propagates through the whole cyber-physical system, considering also the embedded software and the mechanical elements. In particular, our approach allows the analysis of the propagation through the whole system (working at high level) of a fault injected at low level. This paper provides a solution to automate the FMECA process (until now mainly performed manually) for complex cyber-physical systems. It improves the failure classification effectiveness: considering our test case, it reduced the number of critical faults from 10 to 6. The remaining four faults are mitigated by the cyber-physical system architecture\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "On the testing of special memories in GPGPUs\n", "abstract": " Nowadays, data-intensive processing applications, such as multimedia, high-performance computing and safety-critical ones (e.g., in automotive) employ General Purpose Graphics Processing Units (GPGPUs) due to their parallel processing capabilities and high performance. In these devices, multiple levels of memories are employed in GPGPUs to hide latency and increase the performance during the operation of a kernel. Moreover, modern GPGPU architectures implement cutting-edge semiconductor technologies, reducing their size and power consumption. However, some studies proved that these technologies are prone to faults during the operative life of a device, so compromising reliability. In this work, we developed functional test techniques based on parallel Software-Based Self-Test routines to test memory structures in the memory hierarchy of a GPGPU (FlexGripPlus) implementing the G80\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "Evaluating the Code Encryption Effects on Memory Fault Resilience\n", "abstract": " In most safety-critical systems, the robustness and the confidentiality of the application code are crucial. Such code is generally stored into Non-Volatile Memories (NVMs) that are prone to faults (e.g., due to radiation effects). Unfortunately, faults affecting the instruction code result very often into Silent Data Corruption (SDC). This condition lets faults remain undetected and it can lead to undesiderable errors that may compromise the system functionality. Thus, it is desirable that the system is able to detect faults affecting the code memory. To overcome this issue, designers often resort to expensive error detection/correction mechanisms. Furthermore, they also adopt memory encryption techniques to prevent unauthorized, hence malicious, access to the code or to protect it from any unauthorized copy. In this paper, we show that the presence of memory encryption alone is able to strongly reduce the probability of SDC\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "Evaluating Software-based Hardening Techniques for General-Purpose Registers on a GPGPU\n", "abstract": " Graphics Processing Units (GPUs) are considered a promising solution for high-performance safety-critical applications, such as self-driving cars. In this application domain, the use of fault tolerance techniques is mandatory to detect or correct faults, since they must work properly even in the presence of faults. GPUs are designed with aggressive technology scaling, which makes them susceptible to faults caused by radiation interference, such as the Single Event Upsets (SEUs), which can lead the system to a failure, and that is unacceptable in safety-critical applications. In this paper, we evaluate different software-based hardening techniques developed to detect SEUs in GPUs general-purpose registers and propose optimizations to improve performance and memory utilization. The techniques are implemented in three case-study applications and evaluated in a general-purpose soft-core GPU based on the\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "Improved test solutions for COTS-based systems in space applications\n", "abstract": " In order to widen the spectrum of available products, companies involved in space electronics are exploring the possible adoption of COTS components instead of space-qualified ones. However, the adoption of COTS devices and boards requires suitable solutions able to guarantee the same level of dependability. A mix of different solutions can be considered for this purpose. Test techniques play a major role, since they must guarantee that a high percentage of permanent faults can be detected (both at the end of the manufacturing and during the mission) while matching several constraints in terms of system accessibility and hardware complexity. In this paper we focus on the test of the electronics used within launchers, and outline an approach based on Software-based Self-test. The proposed solutions are currently being adopted within the MaMMoTH-Up project, targeting the development of an\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "On the in-field test of embedded memories\n", "abstract": " In-field test of electronic devices is becoming increasingly important due to the wide adoption of electronic systems in safety-critical applications. Hence, it is crucial to devise and deploy effective solutions supporting the test during the operational phase of all the components of an electronic system, including the memory modules embedded in a SoC. Some key aspects include the possible reuse of HW infrastructures introduced for end-of-manufacturing test, the need for limited intrusiveness with respect to the application, and the achievable defect coverage. The paper discusses the main challenges in this area and possible solutions, as well as future trends.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Nanotechnologies Testing\n", "abstract": " In an ideal world, the life of any product starts with the definition of the specifications it should match. Then, designers create a model for its implementation, and using this model the product is manufactured and delivered to customers, who will finally use it in the field. In all the above steps (specification, design, manufacturing, usage) faults may arise, that is, the result may differ from what was expected. As a result, the final product may behave differently than expected (misbehavior, or failure).The role of testing is to detect faults as soon as possible, so that they can be removed, hence improving the quality of the final product. A well-known rule of thumb exists, stating that the cost for fixing a fault increases by 10 each time we move to the following step without detecting it. On the other side, cost is always a crucial parameter, and engineers do their best to identify the most convenient way to test a product with the minimum cost/effort. Depending on the specific stage in the product lifetime we are considering, faults correspond to rather different phenomena. In the specification phase, they mainly are incongruent or missed specifications. In the design phase, they may correspond to bugs introduced by designers or by electronic design automation (EDA) tools. When addressing the above faults, test takes the specific name of verification.", "num_citations": "2\n", "authors": ["502"]}
{"title": "BASTION: Board and SoC test instrumentation for ageing and no failure found\n", "abstract": " This is an overview paper that motivates and describes performed work done in the European Commission funded research project BASTION, which focuses on two critical problems of modern electronics: the No-Fault-Found (NFF) and CMOS ageing. New defect classes contributing to NFF have been identified, including timing related faults (TRF) at board level and intermittent resistive faults (IRF) at IC level. BASTION has addressed the mechanisms of ageing and developed several techniques to improve the longevity of electronic products. Embedded Instrumentation, monitors, and IEEE 1687 standard for reconfigurable scan networks (RSN) are seen as an important leverage that helped mitigating the impact of the above listed problems by facilitating a low-latency, scalable online system health monitoring and error localization infrastructure as well as integration of all heterogeneous technologies into a\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "On the detection of board delay faults through the execution of functional programs\n", "abstract": " In the last years, the phenomenon of electronic products passing all tests by the manufacturer but failing in the field (No Fault Found, or NFF) attracted the attention of industries and researchers. Delay faults are supposed to be among the contributors to this phenomenon. Hence, companies are increasingly adopting functional test as a final step, which is expected to detect this kind of defects. This paper investigates the capabilities of detecting delay faults by several types of functional test, and proposes a method to write functional test programs able to detect most of the delay faults on the connections between the CPU and the memory.", "num_citations": "2\n", "authors": ["502"]}
{"title": "A low-cost susceptibility analysis methodology to selectively harden logic circuits\n", "abstract": " Selecting the ideal trade-off between reliability and cost associated with a fault tolerant architecture generally involves an extensive design space exploration. Employing state-of-the-art susceptibility estimation methods makes it unscalable with design complexity. In this paper we introduce a low-cost susceptibility analysis methodology that helps identifying the most vulnerable circuit elements for hardening with less computational effort and orders of magnitude faster. Our experimental results show that the methodology offers huge gain in terms of computational effort (2,500\u00d7 faster) in comparison with a fault-injection based method and produces results within acceptable degree of accuracy.", "num_citations": "2\n", "authors": ["502"]}
{"title": "On the maximization of the sustained switching activity in a processor\n", "abstract": " Recently, several application areas in the test domain (e.g., burn-in and aging monitoring) started to require suitable input stimuli, able to maximize the switching activity of a certain module for a certain period of time. If the module is part of a processor, this turns into identifying a suitable sequence of instructions, able to maximize the switching activity. This paper proposes a method to attack this problem, and reports some experimental results gathered on a MIPS-like pipelined processor.", "num_citations": "2\n", "authors": ["502"]}
{"title": "The use of benchmarks for high-reliability systems\n", "abstract": " LA-UR-15-22303 Page 1 LA-UR-15-22303 Approved for public release; distribution is unlimited. Title: The Use of Benchmarks for High-Reliability Systems Author(s): Quinn, Heather Marie; Robinson, William; Rech, Paolo; Barnard, Arno; Aguirre, Miguel; Desogus, Marco; Entrena, Luis; Garcia-Valderas, Mario; Guertin, Steve Michael; Kaeli, David; Kastensmidt, Fernanda Lima; Kiddie, Bradley; Sanchez-Clemente, Antonio; Reorda, Matteo Sonza; Sterpone, Luca; Wirthlin, Michael Intended for: SELSE, 2015-03-31 (Austin, Texas, United States) Issued: 2015-03-31 Page 2 Disclaimer: Los Alamos National Laboratory, an affirmative action/equal opportunity employer,is operated by the Los Alamos National Security, LLC for the National NuclearSecurity Administration of the US Department of Energy under contract DE-AC52-06NA25396. By approving this article, the publisher recognizes that the US Government retains \u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "In-field test of safety-critical systems: is functional test a feasible solution?\n", "abstract": " The growing usage of electronic systems in safety- and mission-critical applications, together with the increased susceptibility of electronic devices to faults arising during the operational phase mandate for the availability of effective solutions able to face the effects of these faults. When the target system includes a processor, one possible solution is based on running suitable test programs able to detect the occurrence of faults. This solution provides several advantages (e.g., in terms of flexibility, IP protection, and defect coverage), although it is limited by the cost for developing the test programs. This paper overviews the state of the art in the area, and discusses the trends in the area.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Permanent fault detection and diagnosis in the lightweight dual modular redundancy architecture\n", "abstract": " The Lightweight Dual Modular Redundancy (LDMR) is a fault tolerant architecture for low-latency soft error correction. The LDMR introduces a software compilation strategy that enforces error containment inside a basic block, allowing for a simplified error correction policy. This paper evaluates how the LDMR and its architectural components behave in the presence of permanent faults. It also classifies how sensitive the error detection and rollback machinery is to hard faults. By including permanent fault detection and diagnosis, the LDMR becomes a comprehensive fault tolerant architecture for embedded computing, covering a broad range of fault models. This paper also evaluates the LDMR's performance overhead using a MiBench subset, which is currently 1.54 in average.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Advanced technologies for transient faults detection and compensation\n", "abstract": " Transient faults became an increasing issue in the past few years as smaller geometries of newer, highly miniaturized, silicon manufacturing technologies brought to the mass-market failure mechanisms traditionally bound to niche markets as electronic equipments for avionic, space or nuclear applications. This chapter presents the origin of transient faults, it discusses the propagation mechanism, it outlines models devised to represent them and finally it discusses the state-of-the-art design techniques that can be used to detect and correct transient faults. The concepts of hardware, data and time redundancy are presented, and their implementations to cope with transient faults affecting storage elements, combinational logic and IP-cores (eg, processor cores) typically found in a System-on-Chip are discussed.", "num_citations": "2\n", "authors": ["502"]}
{"title": "On the automatic generation of software-based self-test programs for functional test and diagnosis of vliw processors\n", "abstract": " Software-Based Self-Test (SBST) approaches have shown to be an effective solution to detect permanent faults, both at the end of the production process, and during the operational phase. However, when Very Long Instruction Word (VLIW) processors are addressed these techniques require some optimization steps in order to properly exploit the parallelism intrinsic in these architectures. In this chapter we present a new method that, starting from previously known algorithms, automatically generates an effective test program able to still reach high fault coverage on the VLIW processor under test, while minimizing the test duration and the test code size. Moreover, using this method, a set of small SBST programs can be generated aimed at the diagnosis of the VLIW processor. Experimental results gathered on a case study show the effectiveness of the proposed approach.", "num_citations": "2\n", "authors": ["502"]}
{"title": "A Low-Cost Emulation System for Fast Co-verification and Debug\n", "abstract": " A flexible system for SoC co-verification is proposed, built around an Infrastructure Microprocessor (IM), providing improved controllability and observability in a fast self-contained FPGA-based emulation environment. In addition, software debug is supported by enabling observation of critical signals, breakpoint setting and step-by-step execution with total memory accessibility. Experimental results in an industrial case study confirm the effectiveness of the approach for validating and debugging hardware and software.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Design Validation of Multithreaded Processors Using Threads Evolution\n", "abstract": " Within the design arena of modern devices based on cutting-edge processor cores, the availability of effective verification, validation and test methodologies able to work on high-level descriptions of processor cores represents an interesting advantage, since it can dramatically reduce the overall time for design and manufacturing, while improving yield and quality. In this paper we propose a semi-automatic test program generation technique able to target modules in modern computer architectures that implement the multithreading paradigm. The methodology starts from high level descriptions of processor cores and using an incremental multi-run approach produces, with very limited manual intervention, a test set able to maximize verification metrics. Experimental results gathered on a couple of real complex designs (the OpenSPARC\u2122 T1 and T2) show the effectiveness of the proposed methodology.", "num_citations": "2\n", "authors": ["502"]}
{"title": "A system-layer infrastructure for SoC diagnosis\n", "abstract": " During IC manufacturing phase, discriminating between good and faulty chips is not enough. In fact, especially in the first phase of the production of a new device, a complete understanding of the possible failures is quickly required to ramp up production yield. For test engineers, dealing with the manufacturing test of Systems-on-chip (SoCs) means to tackle the extraction of diagnostic data from faulty chips. Another equally important aim of diagnosis, in a later step of a product lifecycle, is to find the real root cause of silicon misbehaviors for field returns. At the core test layer, the adoption of diagnosis-oriented Design-for-Testability structures is almost mandatory and many solutions have been worked out for several types of cores; diagnosis data retrieval often consists in the execution of a set of self-test procedures whose application order and/or customization may depend on the obtained results themselves\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "An Effective Approach for the Diagnosis of Transition-Delay Faults in SoCs, based on SBST and Scan Chains\n", "abstract": " In this paper, a Software-Based Diagnosis (SBD) procedure suitable for SoCs is proposed to tackle the diagnosis of transition-delay faults. The illustrated methodology takes advantage of an initial Software-Based Self-Test (SBST) test set and of the scan-chains included in the final SoC design release. In principle, the proposed methodology consists in partitioning the considered SBST test set in several slices, and then proceeding to the evaluation of the diagnostic ability owned by each slice with the aim of discarding diagnosis-ineffective test programs portions. The proposed methodology is aimed to provide precise feedback to the failure analysis process focusing the systematic timing failures characteristic of new technologies. Experimental results show the effectiveness and feasibility of the proposed approach on a suitable SoC test vehicle including an 8-bit microcontroller, 4 SRAM memories and an arithmetic\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "On the diagnosis of SoCs including multiple memory cores\n", "abstract": " Memory testing aims at detecting the whole set of faults a device contains. For diagnostic purposes, detailed information about all the detected errors have to be extracted. In this paper, a two-layer P1500 SECT-compliant architecture is proposed to allow efficient diagnosis of SoCs containing several BISTed memory cores. Our approach is suited to support the high amount of data to be exchanged between each core and the external ATE when performing diagnosis, exploiting the test machine real-time processing capabilities.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Coupling different methodologies to validate obsolete microprocessors\n", "abstract": " The actual operating life time for many electronic systems turned out to be much longer than originally foreseen, leading to the use of obsolete components in critical projects. To skip microprocessor obsolescence problems, companies should have bought larger stocks of components when still available, or are forced to find parts in secondary markets later. Alternatively, a suitable low-cost solution could be replacing the obsolete component by emulating its functionalities with a programmable logic device. However, design verification of microprocessors is well known as a challenging task. This paper proposes a coupled methodology to generate test-programs, using complementary techniques: one pseudoexhaustive and one driven by an evolutionary optimizer. As a case study, the Motorola 6800 was targeted.", "num_citations": "2\n", "authors": ["502"]}
{"title": "An infrastructure IP for soft error detection\n", "abstract": " High integration levels coupled with the increased sensitivity to soft errors even at ground level make the task of guaranteeing adequate dependability levels more difficult then ever. In this paper we propose to adopt lowcost infrastructure-intellectual-property (I-IP) cores in conjunction with software-based techniques to perform soft error detection. Experimental results are reported that show the effectiveness of the proposed approach.", "num_citations": "2\n", "authors": ["502"]}
{"title": "An efficient algorithm for the extraction of compressed diagnostic information from embedded memory cores\n", "abstract": " This paper addresses the issue of diagnosing a memory core embedded in a complex SOC. The proposed solution is based on a P1500-compliant wrapper. The proposed solution exploits a hardware-implemented compression method that minimizes the amount of data to be transferred from the core to the ATE. The proposed solution takes into account several constraints existing in an industrial environment, such as reducing the time and area overheads required for diagnosis, and minimizing the cost of the external ATE. Experimental results are provided allowing evaluating the benefits and limitations of the adopted solution.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Introducing sw-based fault handling mechanisms to cope with EMI in embedded electronics: Are they a good remedy?\n", "abstract": " We summarize a study on the effectiveness of two software-based fault handling mechanisms in terms of detecting conducted electromagnetic interference (EMI) in microprocessors. One of these techniques deals with processor control flow checking. The second one is used to detect errors in code variables. In order to check the effectiveness of such techniques in RF ambient, an EIC 61.000-4-29 normative-compliant conducted RF-generator was implemented to inject spurious electromagnetic noise into the supply lines of a commercial off-the-shelf (COTS) microcontroller-based system. Experimental results suggest that the considered techniques present a good effectiveness to detect this type of faults, despite the multiple-fault injection nature of EMI in the processor control and data flows, which in most cases result in a complete system functional loss (the system must be reset).", "num_citations": "2\n", "authors": ["502"]}
{"title": "Accurate dependability analysis of CAN-based networked systems\n", "abstract": " Computer-based systems where several nodes exchange information via suitable network interconnections are today exploited in many safety-critical applications, like those belonging to the automotive field. Accurate dependability analysis of such a kind of systems is thus a major concern for designers. In this paper, we present an environment we developed in order to assess the effects of faults in CAN-based networks. We developed an IP core implementing the CAN protocol controller, and we exploited it to set-up a network composed of several nodes. Thanks to the approach we adopted, we were able to assess via simulation-based fault injection the effects of faults both in the bus used to carry information and inside each CAN controller as well. In this paper, we report a detailed description of the environment we set-up and we present some preliminary results we gathered to assess the soundness of the proposed approach.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Dependability Analysis of CAN Networks: an emulation-based approach\n", "abstract": " Today many safety-critical applications are based on distributed systems where several computing nodes exchange information via suitable network interconnections. An example of this class of applications is the automotive field, where developers are exploiting the CAN protocol for implementing the communication backbone. The capability of accurately evaluating the dependability properties of such a kind of systems is today a major concern. In this paper we present a new environment that can be fruitfully exploited to assess the effects of faults in CAN-based networks. The entire network is emulated via an ad-hoc hardware/software system that allows easily evaluating the effects of faults in all the network components, namely the network nodes, the protocol controllers and the transmission channel. In this paper, we report a detailed description of the environment we set-up and we present some preliminary results we gathered to assess the soundness of the proposed approach.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Evolutionary techniques for minimizing test signals application time\n", "abstract": " Reducing production-test application time is a key problem for modern industries. Several different hardware solutions have been proposed in the literature to ease such process. However, each hardware architecture must be coupled with an effective test signals generation algorithm. This paper propose an evolutionary approach for minimizing the application time of a test set by opportunely extending it and exploiting a new hardware architecture, named interleaved scan. The peculiarities of the problem suggest the use of a slightly modified genetic algorithm with concurrent populations. Experimental results show the effectiveness of the approach against the traditional ones.", "num_citations": "2\n", "authors": ["502"]}
{"title": "An evolutionary algorithm for reducing integrated-circuit test application time\n", "abstract": " The cost for testing integrated circuits represents a growing percentage of the total cost for their production. The former strictly depends on the length of the test session, and its reduction has been the target of many efforts in the past. This paper proposes a new method for reducing the test length by adopting a new architecture and exploiting an evolutionary optimization algorithm. A prototype of the proposed approach was tested on ISCAS standard benchmarks and the experimental results show its effectiveness.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Automatic validation of protocol interfaces described in VHDL\n", "abstract": " In present days, most of the design activity is performed at a high level of abstraction, thus designers need to be sure that their designs are syntactically and semantically correct before starting the automatic synthesis process. The goal of this paper is to propose an automatic input pattern generation tool able to assist designers in the generation of a test bench for difficult parts of small- or medium- sized digital protocol interfaces. The proposed approach exploit a Genetic Algorithm connected to a commercial simulator for cultivating a set of input sequence able to execute given statements in the interface description. The proposed approach has been evaluated on the new ITC\u201999 benchmark set, a collection of circuits offering a wide spectrum of complexity. Experimental results show that some portions of the circuits remained uncovered, and the subsequent manual analysis allowed identifying design\u00a0\u2026", "num_citations": "2\n", "authors": ["502"]}
{"title": "The Training Environment for the course on Microprocessor Systems\n", "abstract": " This paper describes the training environment used for the course on Microprocessor Systems held within the curriculum towards the MS degree in Computer Science at the Politecnico di Torino. The environment is composed of inexpensive hardware and software material and allows students to perform a final assignment work based on implementing a stand-alone video-game system. The experience gathered in the past five years is referred to in explaining the advantages of this environment: more effective learning of microprocessor system architecture and programming, practical experience in project management, first contact with complex systems handling.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Self-Checking and Fault Tolerant Approaches Can Help BIST Fault Coverage: A Case Study.\n", "abstract": " 3. ConclusionsThe BIST component has been designed in the Synopsys environment with the SGS-Thomson ISB24000 technology. It is now being used in the Italtel standard library and exploited in industrial designs. The attained fault coverage was quite satisfying.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Exact probabilistic testability measures for multi-output circuits\n", "abstract": " The computation of probabilistic testability measures has become increasingly important and some methods have been proposed, although the exact solution of the problem is NP-hard. An exact analytical method for singleoutput combinational circuits is extended to deal with multi-output circuits. Such circuits are reduced to singleoutput ones by introducing a dummy gate, the \u201cX-gate,\u201d and applying to the resulting graph the analysis based on supergates.", "num_citations": "2\n", "authors": ["502"]}
{"title": "Testability measures with concurrent good simulation\n", "abstract": " Methods designed to speed up the computation of controllability within an event-driven fault-free simulation environment are described. Input generation techniques are presented which reduce the activity of the simulator, thus achieving a considerable speed-up. Concurrent simulation of fault-free devices is shown to be very effective in this domain. Experimental results are reported for benchmark circuits. No general law has been derived, but a rich set of heuristics has been collected which may be useful in many other cases.<>", "num_citations": "2\n", "authors": ["502"]}
{"title": "System-level test: State of the art and challenges\n", "abstract": " System-level test (SLT) is gaining in importance in modern test flows. This paper summarizes recent industrial findings from three companies and discusses some of the still open questions. The first two reports focus on the optimization potentials due to defect coverage overlaps between SLT and other test insertions. Results observed on approximately 20 million manufactured 28nm and 40nm automotive system-on-chip (SoC) designs are reported. Costs and benefits of SLT are discussed and the potentials of a test results analytics platform are identified. The third report explores the role of marginalities among SLT fails. The post-silicon investigation of a CPU block in a 7nm 5G mobile SoC product aims at achieving a better understanding, whose fails are due to random variations versus systematic factors.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Evaluating Data Encryption Effects on the Resilience of an Artificial Neural Network\n", "abstract": " Nowadays, many electronic systems store valuable Intellectual Property (IP) information inside Non-Volatile Memories (NVMs). Therefore, encryption mechanisms are widely used in order to protect such information from being stolen or modified by human attacks. Encryption techniques can be used for protecting the application code, or sensitive sets of data in the NVM. In particular, in machine-learning applications, the weights of an Artificial Neural Network (ANN) represent a highly valuable IP stemming from long time invested in training the system along the development phase. On the other side, systems implementing ANN applications are increasingly used in safety-critical domains (e.g., autonomous driving), where a high reliability level is required. In a previous paper, we have shown that encryption techniques, applied to the application code of generic systems, provide a significantly higher error detection\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Design and Verification of an open-source SFU model for GPGPUs\n", "abstract": " General Purpose Graphic Processing Units (GPGPUs) are widely used in data-intensive applications, such as multimedia and high-performance computing. These technologies are currently used also to support safety-critical applications (e.g., in the automotive and industrial domains) to implement computer vision, sensor fusion, or machine learning algorithms, which often require the processing of complex transcendent or trigonometric functions. In these cases, an integrated special function unit in the GPGPU is utilized, which is intended to increase the performance in parallel operations. However, this complex module is not present in most of the available architectural and micro-architectural open-source models of GPGPUs, so limiting the characterization and analysis of applications using these units. In this work, we report about the design and functional verification of a Special Function Unit to execute\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Testing the Divergence Stack Memory on GPGPUs: A Modular in-Field Test Strategy\n", "abstract": " General Purpose Graphic Processing Units (GPGPUs) are becoming a promising solution in safety-critical applications, e.g., in the automotive domain. In these applications, reliability and functional safety are relevant factors in the selection of devices to build the systems. Nowadays, many challenges are impacting the implementation of high-performance devices, such as GPGPUs. Moreover, there is the need for effective fault detection solutions to guarantee the correct in-field operation of a GPGPU, such as in the branch management unit, which is one of the most critical modules in this parallel architecture. Faults affecting this structure can heavily corrupt or even collapse the execution of an application on the GPGPU. In this work, we propose a non-invasive Software-Based Self-Test (SBST) solution to detect faults affecting the memory in the branch management unit of a GPGPU. We propose a scalar and\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Assessing the effectiveness of different test approaches for power devices in a PCB\n", "abstract": " Power electronic systems employing Printed Circuit Boards (PCBs) are broadly used in many applications, including some safety-critical ones. Several standards (e.g., ISO26262 for the automotive sector and DO-178 for avionics) mandate the adoption of effective test procedures for all electronic systems. However, the metrics to be used to compute the effectiveness of the adopted test procedures are not so clearly defined for power devices and systems. In the last years, some commercial fault simulation tools (e.g., DefectSim by Mentor Graphics and TestMAX by Synopsys) for analog circuits have been introduced, together with some new fault models. With these new tools, systematic analog fault simulation finally became practically feasible. The aim of this paper is twofold: first, we propose a method to extend the usage of the new analog fault models to power devices, thus allowing to compute a Fault Coverage\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "An Enhanced Evolutionary Technique for the Generation of Compact Reconfigurable Scan-Network Tests\n", "abstract": " Nowadays, many Integrated Systems embed auxiliary on-chip instruments whose function is to perform test, debug, calibration, configuration, etc. The growing complexity and the increasing number of these instruments have led to new solutions for their access and control, such as the IEEE 1687 standard. The standard introduces an infrastructure composed of scan chains incorporating configurable elements for accessing the instruments in a flexible manner. Such an infrastructure is known as Reconfigurable Scan Network or RSN.\u00a0Since permanent faults affecting the circuitry can cause malfunction, i.e., inappropriate behavior, detecting them is of utmost importance. This paper addresses the issue of generating effective sequences for testing the reconfigurable elements within RSNs using evolutionary computation. Test configurations are extracted with automatic test pattern generation (ATPG) and used to guide\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Software-Based Self-Test for Delay Faults\n", "abstract": " Digital integrated circuits require thorough testing in order to guarantee product quality. This is usually achieved with the use of scan chains and automatically generated test patterns. However, functional approaches are often used to complement test suites. Software-Based Self-Test (SBST) can be used to increase defect coverage in microcontrollers, to replace part of the scan pattern set to reduce tester requirements, or to complement the defect coverage achieved by structural techniques when advanced semiconductor technologies introduce new defect types. Delay testing has become common practice with VLSI integration, and with the latest technologies, targeting small delay defects (SDDs) has become necessary. This chapter deals with SBST for delay faults and describes a case of study based on a peripheral module integrated in a System on Chip (SoC). A method to develop an effective functional test is\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "VLSI-SoC: Opportunities and Challenges Beyond the Internet of Things: 25th IFIP WG 10.5/IEEE International Conference on Very Large Scale Integration, VLSI-SoC 2017, Abu Dhabi\u00a0\u2026\n", "abstract": " This book contains extended and revised versions of the best papers presented at the 25th IFIP WG 10.5/IEEE International Conference on Very Large Scale Integration, VLSI-SoC 2017, held in Abu Dhabi, United Arab Emirates, in August 2017. The 11 papers included in this book were carefully reviewed and selected from the 33 full papers presented at the conference. The papers cover a wide range of topics in VLSI technology and advanced research. They address the latest scientific and industrial results and developments as well as future trends in the field of System-on-Chip (SoC) Design. On the occasion of the silver jubilee of the VLSI-SoC conference series the book also includes a special chapter that presents the history of the VLSI-SoC series of conferences and its relation with VLSI-SoC evolution since the early 80s up to the present.", "num_citations": "1\n", "authors": ["502"]}
{"title": "About Performance Faults in Microprocessor Core in-field Testing\n", "abstract": " When microprocessor-based devices are used in safety-critical applications (e.g., in automotive systems), it is common to adopt solutions aimed at testing them in-field, so that permanent faults that may affect them are identified before they cause critical consequences. In this way, the required reliability figures can be achieved. A popular solution to perform in-field test (especially when executed concurrently to the application) is based on triggering the execution of proper procedures (composing a Self-Test Library, or STL), which are able to activate faults and make them visible when checking the produced results (e.g., in memory). Unfortunately, a special class of faults exists (named Performance Faults), which do not impact the value of the results, but only the timing behavior of the processor. This paper describes a set of experiments aimed at quantitatively evaluating the number of these faults in a simple\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "RESCUE: Cross-Sectoral PhD Training Concept for Interdependent Reliability, Security and Quality\n", "abstract": " The recently started European Training Network (ETN) RESCUE advances scientific competences in the demanding and mutually dependent aspects of nano-electronic systems design, i.e. reliability, security and quality, as well as related electronic design automation tools. Second, it provides early-stage researchers with innovative cross-sectoral training in the involved disciplines and beyond, preparing them to face today's and future challenges in nano-electronics design. Furthermore, they are also trained to be innovative, creative, and more important - will have an entrepreneurial mentality. The latter will help to compile ideas into products and services for economic and social benefits and creates qualified workforce and knowledge for the industry. The consortium consists of leading European research groups competent to tackle the interdependent challenges in a holistic manner, and is excellently balanced in\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Automatic Tool Integration into an Industrial Electronic Board Production Hardware Fault Coverage Analysis Process\n", "abstract": " In Magneti Marelli SpA Powertrain very high quality is essential. To achieve such a high quality, the production test must be capable of detecting all potential defects introduced during the production process.The adopted test line involves ICT (In-Circuit Test, Bed of Nails), FT (Functional Test) and AOI (Automated Optical Inspection) strategies. The quality of the test line, which is an indicator of how many defects the line is capable of detecting, is resumed into the coverage document, manually produced by the Magneti Marelli test engineers through the use of personal reasoning and know-how.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Test, Validation and Diagnosis of IEEE 1687 Networks\n", "abstract": " Test, Validation and Diagnosis of IEEE 1687 Networks LUND UNIVERSITY LUND UNIVERSITY LIBRARIES Lund University Lund University Publications LUND UNIVERSITY LIBRARIES Register publications | Statistics | Marked list 0 | Saved searches 0 Advanced Home | Publications | Departments Test, Validation and Diagnosis of IEEE 1687 Networks Cantoro, Riccardo ; Sonza-Reorda, Matteo ; Ghani Zadegan, Farrokh LU ; Larsson, Erik LU ; Jutman, Artur and Devadze, Sergei (2016) Test Standards Application Workshop (TESTA) Mark Links Research Portal page Google Scholar find title Please use this url to cite or link to this publication: https://lup.lub.lu.se/record/3206b174-b40c-4d37-8ff4-6a24e18fd0a8 Details BibTeX author Cantoro, Riccardo ; Sonza-Reorda, Matteo ; Ghani Zadegan, Farrokh LU ; Larsson, Erik LU ; Jutman, Artur and Devadze, Sergei organization Department of Electrical and Information \u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "An effective ATPG flow for Gate Delay Faults\n", "abstract": " This paper proposes a novel approach for the generation of test patterns suitable for detecting Gate Delay Faults (GDFs). The key idea lies in associating any single Gate Delay Fault to a set of Transition Delay (TD) Faults, and exploiting this relationship to produce effective patterns. The approach encompasses several steps: once a Gate Delay Fault is translated into a set of equivalent Transition Delay Faults, a traditional ATPG procedure can be used to determine patterns without any explicit timing information. The latter may account for several iterations, and it is returning the minimum delay that is detected for each delay faults. Effectiveness and feasibility of the proposed ATPG flow have been demonstrated on ISCAS'89 and ITC'99 benchmarks.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Scan-chain intra-cell defects grading\n", "abstract": " With the continuous scaling down of the transistor size, the so-called intra-cell defects are more and more frequent. Several works analyze the impact of intra-cell defects w.r.t. the test quality. However, to the best of our knowledge, none of them target intra-cell defects affecting scan flip-flops. This paper presents an evaluation of the effectiveness of the ATPG test patterns in terms of intra-cell defect coverage affecting scan flip-flops. The experimental results show that a meaningful test solution has to be developed to improve the overall defect coverage for the scan chain testing.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Soft error effects analysis and mitigation in VLIW safety-critical applications\n", "abstract": " VLIW architectures are widely employed in several embedded signal applications since they offer the opportunity to obtain high computational performances while maintaining reduced clock rate and power consumption. Recently, VLIW processors are being considered for employment in various embedded processing systems, including safety-critical ones (e.g., in the aerospace, automotive and rail transport domains). Terrestrial safety-critical applications based on newer nano-scale technologies raise increasing concerns about transient errors induced by neutrons. Therefore, techniques to effectively estimate and improve the reliability of VLIW processors are of great interest. In this paper, we present a novel technique aimed to further improve the efficiency of the Triple Modular Redundancy (TMR) hardening-technique applied at the software level on VLIW processors. In particular, we first experimentally\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Permanent faults on LIN networks: On-line test generation\n", "abstract": " Permanent faults (e.g., due to electronic components aging) represent a real problem in nowadays digital systems working in automotive vehicles. Mandatory tests should be done at the vehicle key-on in order to detect damaged elements. Generation and validation of these tests can be improved in a great manner considering the characteristics of the target distributed subsystems. In this work, an approach for in-field detection of permanent faults in a LIN network is proposed.", "num_citations": "1\n", "authors": ["502"]}
{"title": "On the in-field test of Branch Prediction Units using the correlated predictor mechanism\n", "abstract": " Branch Prediction Units (BPUs) are widely used to reduce the performance penalties caused by branch instructions in pipelined processors. BPUs may be implemented in different forms: the Branch History Table (BHT) is an effective solution when the goal is predicting the result of conditional branches. In this paper we propose a method to generate test programs able to detect faults affecting the memory existing within a BHT implementing the correlated predictors approach. Our method is particularly suited to be used for the in-field test of a processor and allows detecting any stuck-at fault in the BPU memory. The method does not require the detailed knowledge of the BPU implementation, but only relies on the key parameters of its architecture. We gathered experimental results using the SimpleScalar environment.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Reducing SEU sensitivity in LIN networks: Selective and collaborative hardening techniques\n", "abstract": " Digital electronic systems in automotive applications are in charge of different tasks, ranging from very critical control functions (e.g., airbag, ABS, ESP) to comfort services (e.g., handling of mirrors, seats, windows, wipers). Hardening these systems involves suitably trading off cost and reliability. Due to standards and regulations in the area, the reliability of subsystems involved even in the least critical applications has to be evaluated, and in most cases hardening has to be performed with very low extra cost. In this work, two approaches are proposed for hardening the LIN bus, which implements a serial communication network typically used in low-throughput and low-cost sub-systems in automotive applications. First, critical elements in LIN nodes are identified and some techniques to harden them are proposed following a selective hardening approach. Secondly, collaborative hardening techniques are proposed\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "On the functional test of MESI controllers\n", "abstract": " This paper proposes a method to identify a functional sequence able to test the circuitry implementing the MESI protocol in a multi-processor or multi-core system. The method is purely functional and does not require any knowledge about the real implementation of the circuitry. It is simply based on forcing the processors to execute a program, while observing the results (both in terms of produced data and performance behavior). Therefore, the method is particularly suitable to be used for at-speed manufacturing test, incoming inspection, and on-line test. Experimental results have been gathered on a compatible MIPS64 multi-processor system.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Functional test generation for the pLRU replacement mechanism of embedded cache memories\n", "abstract": " Testing cache memories is a challenging task, especially when targeting complex and high-frequency devices such as modern processors. While the memory array in a cache is usually tested exploiting BIST circuits that implement March-based solutions, there is no established methodology to tackle the cache controller logic, mainly due to its limited accessibility. One possible approach is Software-Based Self Testing (SBST): however, devising test programs able to thoroughly excite the replacement logic and made the results observable is not trivial. A test program generation approach, based on a Finite State Machine (FSM) model of the replacement mechanism, is proposed in this paper. The effectiveness of the method is assessed on a case study considering a data cache implementing the pLRU replacement policy.", "num_citations": "1\n", "authors": ["502"]}
{"title": "La formazione a distanza al Politecnico di Torino: nuovi modelli e strumenti\n", "abstract": " Il Politecnico di Torino \u00e8 attivo nel campo della formazione a distanza da oltre 10 anni, nel quale ha sempre giocato un ruolo di innovazione e sperimentazione per quanto riguarda sia le tecnologie, sia le metodologie didattiche ed organizzative. Dall\u2019anno accademico 2010/2011, utilizzando una serie di sperimentazioni precedenti, e contestualmente all\u2019adozione di un nuovo modello didattico compatibile con i requisiti del DM270, il Politecnico di Torino ha ripensato la propria offerta in termini di servizi didattici online e corsi di laurea fruibili a distanza. Questo articolo illustra il modello utilizzato, che si basa sulla sinergia tra il servizio di registrazione e streaming delle lezioni in aula, la disponibilit\u00e0 di tutori on-line e di una specifica piattaforma didattica per studenti non frequentanti, e l\u2019esistenza di sedi decentrate sul territorio. L\u2019articolo illustra altres\u00ec i primi risultati ottenuti, dal punto di vista qualitativo e qualitativo, dopo il primo semestre di funzionamento del modello.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Recovery scheme for hardening system on programmable chips\n", "abstract": " The checkpoint and rollback recovery techniques enable a system to survive failures by periodically saving a known good snapshot of the system's state, and rolling back to it in case a failure is detected. The approach is particularly interesting for developing critical systems on programmable chips that today offers multiple embedded processor cores, as well as configurable fabric that can be used to implement error detection and correction mechanisms. This paper presents an approach that aims at developing a safety- or mission-critical systems on programmable chip able to tolerate soft errors by exploiting processor duplication to implement error detection, as well as checkpoint and rollback recovery to correct errors in a cost-efficient manner. We developed a prototypical implementation of the proposed approach targeting the Leon processor core, and we collected preliminary results that outline the capability of\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "On the generation of test programs for chip multi-thread computer architectures\n", "abstract": " Within the design arena of modern devices based on cutting-edge processor cores, such as the OpenSPARC T2, the availability of effective verification, validation and test methodologies able to take advantage of high level descriptions of processor cores represents a particular advantage, since they can dramatically reduce the overall time for design and manufacturing, and improve yield and quality. A crucial role is played in this context by methods to generate effective test benches to be used for validation and test (through the software-based self-test, or SBST, paradigm). In this context, we are currently investigating innovative test set generation techniques starting from high level descriptions of processor cores that implement multi-thread architectural paradigms, such as the OpenSPARC T2 core. One of the key points of the T2 processor is the chip multi-threading and multi-core facilities, which have not been\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Optimization of self checking FIR filters by means of fault injection analysis\n", "abstract": " In this paper the design of a FIR filter with self checking capabilities based on the residue checking is analyzed. Usually the set of residues used to check the consistency of the results of the FIR filter are based of theoretic considerations about the dynamic range available with a chosen set of residues, the arithmetic characteristics of the errors caused by a fault and on the characteristic of the filter implementation. This analysis is often difficult to perform and, to obtain an acceptable fault coverage the set of chosen residues is overestimated, obtained result a and therefore requires that Instead, in this paper we show how using an exhaustive fault injection campaigns allows to efficiently select the best set of residues. Experimental results coming from fault injection campaigns on a 16 taps FIR filter demonstrated that by observing the occurred errors and the detection modules corresponding to different residue has been\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "An optimized hybrid approach to provide fault detection and correction in SOCs\n", "abstract": " An increasing number of safety-critical applications are based on Systems-on-Chip (SoCs), thus pushing a new wave of research aiming at the development of suitable techniques for ensuring their reliability. Several fault tolerance techniques have been proposed to ensure their fault detection capabilities, based either on software-based or hardware-based techniques.", "num_citations": "1\n", "authors": ["502"]}
{"title": "A non-intrusive on-line control flow error detection technique for SoCs\n", "abstract": " The time to market demands of embedded systems make the reuse of software and hardware components a mandatory design approach, while the growing sensitivity of hardware to soft errors requires effective error detection techniques to be used even in general purpose systems. Control flow error detection techniques are usually either hardware or software intrusive, requiring modification of the processor architecture or changes in the application software. This paper proposes a non-intrusive and low cost technique to be used in SoC designs, that is able to detect errors affecting the program counter with very small area and performance overheads, without the need of any changes in the core processor hardware nor in the application software.", "num_citations": "1\n", "authors": ["502"]}
{"title": "System-in-package testing: problems and solutions\n", "abstract": " Editor's note: System-in-package integrates multiple dies in a common package. Therefore, testin SiP technology is different from system-on-chip, which integrates multiple vendor parts. This article provides test strategies for known-good-die and known-good-substrate in the SiP. Case studies prove feasibility using the IEEE 1500 test structure.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Hardening the control flow\n", "abstract": " This chapter presents the main software-implemented techniques for hardening a microprocessor-based system against control flow (CF) errors (CFEs). A CFE is an error that causes a processor to fetch and execute an instruction different than expected.As experiments demonstrate, a significant percentage of transient faults leads to CFEs: in the experiments performed in [42] in average around 78% of faults affecting a system caused CFEs (of course, this figure depends a lot on the processor architecture and on the applications on which experiments are performed). Most of the CFEs cannot be identified by the mechanisms developed for data errors identification presented in chapter 2. These reasons stimulate the development of special mechanisms for CFEs identification, which are presented in this chapter.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Diagnosing faulty functional units in processors by using automatically generated test sets\n", "abstract": " Microprocessor technology is increasingly used for many applications; the large market volumes call for cost containment in the production phase. Process yield for processor production is, however, far from ideal. To increase it fault diagnosis is an important means, since it can allow both process characterization and product repair by the usage of backup resources. This paper presents a novel methodology to discriminate faulty modules, rather than gates, in a microprocessor based on the automatic construction of diagnostic software-based test sets. The approach exploits a post-production test set, designed for software-based self-test, and an infrastructure IP to perform the diagnosis. An initial diagnostic test set is built, and then iteratively refined resorting to an evolutionary method. Experimental results are reported in the paper showing the feasibility and effectiveness of the approach for an Intel i8051 processor\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Automatic completion and refinement of verification sets for microprocessor cores\n", "abstract": " In the design cycle of a microprocessor core, the unit is usually refined through a series of subsequent steps. To deliver a flaw free unit at the end of the process, in each stage a verification step is required. While it would be useful to automatically develop the set of test programs for verification concurrently to the design, in most of the existing approach verification is performed manually and starting from scratch. This paper presented a methodology for the automatic completion and refinement of existing verification programs. It shows a new technique for allowing a Genetic Programming-based framework to import an existing test-program set and assimilate it for further test generation. A case study is considered, in which a sample pipelined processor is used, and new test programs are generated starting from existing functional ones. Different metrics are targeted, and preliminary results are reported\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "A new DFM-proactive technique\n", "abstract": " A Design for Manufacturing (DfM) proactive strategy to isolate technology marginalities is presented in the paper. This strategy relies on manufacturing multiple instances of a highly diagnosable SoC on the same wafer by varying several constructive parameters such as the library set, the circuit routing and the synthesis parameters. The results obtained by adopting a diagnostic flow based on correlation among physical circuit characteristics allow the precise individuation of design criticalities related both to the library features and the production process.", "num_citations": "1\n", "authors": ["502"]}
{"title": "1. On the diagnosis of embedded memory cores through Programmable BIST\n", "abstract": " This paper addresses the issue of testing and diagnosing a memory core embedded in a SOC. The proposed solution is based on a P1500-compliant wrapper that follows the programmable BIST approach. Tasks previously implemented by the ATE are moved to a custom test microprocessor included in the wrapper, reducing the overall test cost. The test program executed by the processor can be uploaded by the ATE before testing, thus matching specific diagnostic requirements. The flexibility of the proposed approach is particularly suited to the diagnosis of embedded memory cores, allowing the execution of different test programs and easing the reuse of the available architectures for the test and diagnosis of different memory types. Preliminary results obtained while diagnosing a test chip implementing the described approach are reported.", "num_citations": "1\n", "authors": ["502"]}
{"title": "An RT-level concurrent error detection technique for data dominated systems\n", "abstract": " In this paper we present a concurrent error detection (CED) technique that can be applied to behavioral RTL models of data dominated systems. The technique obtains CED through duplication of operations with shifted operands. Duplication is performed according to transformation rules that can automatically be applied to a behavioral model. Preliminary experimental results show the viability of the proposed technique.", "num_citations": "1\n", "authors": ["502"]}
{"title": "A programmable BIST approach for the diagnosis of embedded memory cores\n", "abstract": " This paper addresses the issue of testing and diagnosing a memory core embedded in a complex SOC. The proposed solution is based on a P1500-compliant wrapper that follows the programmable BIST approach. Experimental results are provided allowing to evaluate the benefits and limitations of the adopted solution and to compare it with previously proposed ones.", "num_citations": "1\n", "authors": ["502"]}
{"title": "A hierarchical approach for designing dependable systems\n", "abstract": " New constraints, such as the need for reducing the cost and the time-to-market, are forcing designers of safety-critical systems to exploit commercial-off-the-shelf (COTS) components. To effectively deal with the specification and evaluation of such a kind of systems new design methodologies are required. We propose a new approach where COTS components building a safety critical system are first characterized through a detailed analysis process. The extracted information can then be exploited by a high-level analysis environment that allows evaluating a whole system with good accuracy and high efficiency.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Early power estimation for system-on-chip designs\n", "abstract": " Reduction of chip packaging and cooling costs for deep sub-micron System-On-Chip (SOC) designs is an emerging issue. We present a simulationbased methodology able to realistically model the complex environment in which a SOC design operates in order to provide early and accurate power consumption estimation. We show that a rich functional test bench provided by a designer with a deep knowledge of a complex system is very often not appropriate for power analysis and can lead to power estimation errors of some orders of magnitude. To address this issue, we propose an automatic input sequence generation approach based on a heuristic algorithm able to upgrade a set of test vectors provided by the designer. The obtained sequence closely reflects the worst-case power consumption for the chip and allows looking at how the chip is going to work over time.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Prediction of power requirements for high-speed circuits\n", "abstract": " Modern VLSI design methodologies and manufacturing technologies are making circuits increasingly fast. The quest for higher circuit performance and integration density stems from fields such as the telecommunication one where high speed and capability of dealing with large data sets is mandatory. The design of high-speed circuits is a challenging task, and can be carried out only if designers can exploit suitable CAD tools. Among the several aspects of high-speed circuit design, controlling power consumption is today a major issue for ensuring that circuits can operate at full speed without damages. In particular, tools for fast and accurate estimation of power consumption of highspeed circuits are required. In this paper we focus on the problem of predicting the maximum power consumption of sequential circuits. We formulate the problem as a constrained optimization problem, and solve it resorting to\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Evaluating System Dependability in a Co-Design Framwork\n", "abstract": " The widespread adoption of embedded microprocessor-based systems for safety critical applications mandates the use of co-design tools able to evaluate system dependability at every steps of the design cycle. In this paper, we describe how Fault Injection techniques have been integrated in an existing co-design tool and which advantages come from the availability of such an enhanced tool. The effectiveness of the proposed tool is assessed on a simple case study.", "num_citations": "1\n", "authors": ["502"]}
{"title": "A peak-power estimation algorithm for sequential circuits\n", "abstract": " Tools for evaluating the worst-case peak-power consumption of sequential circuits are strongly required by designers of low-power circuits. Previously proposed methods search for the initial state and the couple of vectors with maximum consumption, without exploiting the information on the reachable state set during the power estimation process. This paper shows that this can lead to significant underestimation of the maximum power consumption, and proposes a new algorithm for overcoming this drawback. Experimental results show that for many circuits our algorithm is able to provide better results than those known up to now, even with large circuits.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Exploiting high-level descriptions for circuits fault tolerance assessments\n", "abstract": " The paper proposes a new approach to estimate early the fault detection capability of a safety-critical computer-based system from its high-level description. This paper first aims at verifying the correspondence between dependability measures obtained through simulation-based fault injection experiments at different levels of abstraction. Then, we propose Alternative Graphs (AGs) to create lists of malicious faults without expanding the full data flow, whose size can often explode. Fault trees are exploited to improve the results of the high-level fault analysis. To evaluate the effectiveness of the approach, simulation-based fault injection experiments have been done on some benchmark systems described in VHDL language. The approach demonstrates that fault detection analysis performed at a high-level is less CPU time demanding but approximates well the fault detection measures achievable on a low-level\u00a0\u2026", "num_citations": "1\n", "authors": ["502"]}
{"title": "Partial Scan Flip Flop Selection for Simulation-Based Sequential ATPG\n", "abstract": " The partial scan approach is now widely adopted and several commercial tools support this technique. However, there is no general agreement on how to select the Scan Flip Flops: in general each technique is tailored to a particular ATPG algorithm and results effective when coupled with the right ATPG tool. In this paper, we propose an approach suitable for GA-based ATPGs, which is based on exploiting some information coming from the ATPG itself; we compare the results of our method with the ones of the approach based on cutting the topological loops and use a GA-based ATPG to demonstrate its effectiveness in terms of Fault Coverage and CPU time.", "num_citations": "1\n", "authors": ["502"]}
{"title": "An improved data parallel algorithm for Boolean function manipulation using BDDs\n", "abstract": " * This paper describes a data-parallel algorithm for boolean function manipulation. The algorithm adopts Binary Decision Diagrams (BDDs), which are the state-of-the-art approach for representing and handling boolean functions. The algorithm is well suited for SIMD architectures and is based on distributing BDD nodes among the available Processing Elements and traversing BDDs in a breadth-first manner. An improved version of the same algorithm is also presented, which does not use virtual processors. A prototypical package has been implemented and its behavior has been studied with two different applications. In both cases the results show that the approach exploits well the parallel hardware by effectively distributing the load; thanks to the limited CPU time required and to the great amount of memory available, it can solve problems that can not be faced with by conventional architectures. 1. Introduction Efficient techniques for boolean function manipulation are a key point in man...", "num_citations": "1\n", "authors": ["502"]}
{"title": "Exploiting Massively Parallel Architectures for the Analysis of Growth Phenomena\n", "abstract": " Growth phenomena are important in several fields like physics, chemistry, biology and engineering. Since analytical approaches are often unable to provide a complete description of growth mechanisms, a numerical solution becomes the only viable alternative. The use of massively parallel architectures allows us to obtain a more complete picture of the growth behaviour.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Efficient verification of sequential circuits on a parallel system\n", "abstract": " The paper presents a method to verify functional correctness of FSMs on a parallel system. The equivalence condition is expressed in theoretical terms within the framework of the product machine. It consists in proving that a set of states of the product machine is unreachable from the initial reset state. The algorithm is based on state-of-the-art simulation techniques for explicit enumeration on the inputs and is implemented on a parallel machine. The states of the product machine are partitioned for evaluation among available processors. Experimental results show that the method is applicable to real-world circuits and that the parallel version achieves an almost linear speedup in the number of processors.<>", "num_citations": "1\n", "authors": ["502"]}
{"title": "Cross-fertilizing FSM verification techniques and sequential diagnosis\n", "abstract": " The authors present a technique for assessing the diagnostic power of an existing detection-oriented test pattern by means of diagnostic fault simulation and a procedure to improve it. The procedure successfully exploits enhanced symbolic finite state machine (FSM) equivalence proof algorithms. In order to resort to product machine traversal only when needed, special checks are performed to verify combinational identity and identity on reachable states. As all faults are attributed to their equivalence class, this method may be used to build a complete and exact diagnostic tree. Experimental results support the claim that the diagnosis of real-world synchronous sequential circuits has become feasible for the first time.<>", "num_citations": "1\n", "authors": ["502"]}
{"title": "TPDL: Extended temporal profile description language\n", "abstract": " This paper presents TPDL (extended temporal profile description language), a general\u2010purpose language to observe and condition dynamic systems by means of temporal and logical expressions. It describes how time is modelled in TPDL, gives an overview of the language through its basic types, primitives and conditional constructs, and its use in computer\u2010aided design of digital systems. The paper discusses TPDL's facilities to support the description of hardware behaviour, to define the environment in which devices operate, and to observe and control both circuits and environments. The characteristics of the language are demonstrated through some representative examples.", "num_citations": "1\n", "authors": ["502"]}
{"title": "The use of model checking in ATPG for sequential circuits\n", "abstract": " Some design environments may prevent Design for Testability techniques from reducing testing to a combinational problem: ATPG for sequential devices remains a challenging field. Random and deterministic structure-oriented techniques are the state-of-the-art, but there is a growing interest in methods where the function implemented by the circuit is known. This paper shows how a test pattern may be generated while trying to disprove the equivalence of a good and a faulty machine. The algorithms are derived from Graph Theory and Model Checking. An example is analyzed to discuss the applicability and the cost of such an approach.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Expressing logical and temporal conditions in simulation environments: TPDL\u2217\n", "abstract": " TPDL\u2217 (Extended Temporal Profile Description Language), a general purpose language to express logical and temporal conditions, is the subject of this paper. The main features discussed are: the time model of TPDL\u2217, its types and primitives, its use in a hierarchical, mixed-mode, concurrent fault simulation environment, and its verification by means of CONLAN. Examples are presented and the current implementation of the software tools supporting TPDL\u2217 is described.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Low Power BIST via Hybrid Cellular Automata\n", "abstract": " In the last decade, researchers devoted increasing efforts to reduce the average power consumption in VLSI systems during normal operation mode, while power consumption during test operation mode was usually neglected. However, during test application the circuits are subject to an activity higher than the normal one: the extra power consumption due to test application may thus rise severe hazards to the circuit reliability. Moreover, it can dramatically shorten the battery life when periodic testing of battery-powered systems is considered. In this paper we propose an algorithm to design a Test Pattern Generator for testing of combinational circuits that effectively reduces the power consumption while attaining high Fault Coverage. Experimental results show that our approach saves 30% on the average of the power consumed during test without affecting Fault Coverage and Test Length.", "num_citations": "1\n", "authors": ["502"]}
{"title": "New Techniques for Accelerating Fault Injectioning VHDL Description\n", "abstract": " Simulation-based Fault Injection in VHDL descrip-tions is increasingly common due to the popularity of top-down design flows exploiting this language. However, the large CPU time required to perform VHDL simulations often represents a major drawback stemming from the adoption of this method. This paper presents some tech-niques for reducing the time to perform the Fault Injec-tion experiments. Static and dynamic methods are pro-posed to analyze the list of faults to be injected, and for removing faults as soon as their behavior is known. Common features available in most VHDL simulation environments are also exploited. Experimental results show that the proposed techniques are able to reduce the time required by a typical Fault Injection campaign by a factor ranging from 51% to 96%.", "num_citations": "1\n", "authors": ["502"]}
{"title": "Report on automatic generation of test benches from system-level descriptions\n", "abstract": " This document describes the process we followed for assessing the feasibility and effectiveness of high-level test vector generation. An experimental analysis of the available high-level fault models is first reported, whose purpose is to identify a reference fault model that could be fruitfully used for evaluating testability of circuits by reasoning on their behavior, only. A prototypical high-level fault simulation tool is also described, whose purpose is to support the fault models analysis. Finally, a test generation algorithm is presented that generates high quality test vectors by exploiting the selected fault model and the described high-level fault simulator.", "num_citations": "1\n", "authors": ["502"]}