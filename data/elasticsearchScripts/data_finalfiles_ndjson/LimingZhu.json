{"title": "Adversarial examples on graph data: Deep insights into attack and defense\n", "abstract": " Graph deep learning models, such as graph convolutional networks (GCN) achieve remarkable performance for tasks on graph data. Similar to other types of deep models, graph deep learning models often suffer from adversarial attacks. However, compared with non-graph data, the discrete features, graph connections and different definitions of imperceptible perturbations bring unique challenges and opportunities for the adversarial attacks and defenses for graph data. In this paper, we propose both attack and defense techniques. For attack, we show that the discreteness problem could easily be resolved by introducing integrated gradients which could accurately reflect the effect of perturbing certain features or edges while still benefiting from the parallel computations. For defense, we observe that the adversarially manipulated graph for the targeted attack differs from normal graphs statistically. Based on this observation, we propose a defense approach which inspects the graph and recovers the potential adversarial perturbations. Our experiments on a number of datasets show the effectiveness of the proposed methods.", "num_citations": "114\n", "authors": ["638"]}
{"title": "Iotchain: Establishing trust in the internet of things ecosystem using blockchain\n", "abstract": " The Internet of Things (IoT) has already reshaped and transformed our lives in many ways, ranging from how we communicate with people or manage our health to how we drive our cars and manage our homes. With the rapid development of the IoT ecosystem in a wide range of applications, IoT devices and data are going to be traded as commodities in the marketplace in the near future, similar to cloud services or physical objects. Developing such a trading platform has previously been identified as one of the key grand challenges in the integration of IoT and data science. Deployment of such a platform raises concerns about the security and privacy of data and devices since their ownership is hard to trace and manage without a central trusted authority. A central trusted authority is not a viable solution for a fully decentralized and distributed IoT ecosystem with a large number of distributed device vendors and\u00a0\u2026", "num_citations": "76\n", "authors": ["638"]}
{"title": "Investigating dependencies in software requirements for change propagation analysis\n", "abstract": " ContextThe dependencies between individual requirements have an important influence on software engineering activities e.g., project planning, architecture design, and change impact analysis. Although dozens of requirement dependency types were suggested in the literature from different points of interest, there still lacks an evaluation of the applicability of these dependency types in requirements engineering.ObjectiveUnderstanding the effect of these requirement dependencies to software engineering activities is useful but not trivial. In this study, we aimed to first investigate whether the existing dependency types are useful in practise, in particular for change propagation analysis, and then suggest improvements for dependency classification and definition.MethodWe conducted a case study that evaluated the usefulness and applicability of two well-known generic dependency models covering 25\u00a0\u2026", "num_citations": "75\n", "authors": ["638"]}
{"title": "Securing a deployment pipeline\n", "abstract": " At the RELENG 2014 Q&A, the question was asked, \u201cWhat is your greatest concern?\u201d and the response was \u201csomeone subverting our deployment pipeline\u201d. That is the motivation for this paper. We explore what it means to subvert a pipeline and provide several different scenarios of subversion. We then focus on the issue of securing a pipeline. As a result, we provide an engineering process that is based on having trusted components mediate access to sensitive portions of the pipeline from other components, which can remain untrusted. Applying our process to a pipeline we constructed involving Chef, Jenkins, Docker, Github, and AWS, we find that some aspects of our process result in easy to make changes to the pipeline, whereas others are more difficult. Consequently, we have developed a design that hardens the pipeline, although it does not yet completely secure it.", "num_citations": "53\n", "authors": ["638"]}
{"title": "Composing enterprise mashup components and services using architecture integration patterns\n", "abstract": " Enterprise mashups leverage various source of information to compose new situational applications. The architecture of such applications must address integration issues: it needs to deal with heterogeneous local and/or public data sources, and build value-added applications on existing corporate IT systems. In this paper, we leverage enterprise architecture integration patterns to compose reusable mashup components. We present a service oriented architecture that addresses reusability and integration needs for building enterprise mashup applications. Key techniques to customize this architecture are developed for mashups with themed data on location maps. The usage of this architecture is illustrated by a property valuation application derived from a real-world scenario. We demonstrate and discuss how this state-of-the-art architecture design method can be applied to enhance the design and development\u00a0\u2026", "num_citations": "44\n", "authors": ["638"]}
{"title": "Desiderata for languages to be used in the definition of reference business processes\n", "abstract": " In many modern enterprises, explicit business process definitions facilitate the pursuit of business goals in such ways as best practice reuse, process analysis, process efficiency improvement, and automation. Most real-world business processes are large and complex. Successfully capturing, analysing, and automating these processes requires process definition languages that capture a variety of process aspects with a wealth of details. Most current process modelling languages, such as Business Process Modelling Notation (BPMN), focus on structural control flows among activities while providing inadequate support for other process definition needs. In this paper, we first illustrate these inadequacies through our experiences with a collection of real-world reference business processes from the Australian lending industry. We observe that the most significant inadequacies include lack of resource management, exception handling, process variation, and data flow integration. These identified shortcomings led us to consider the Little-JIL language as a vehicle for defining business processes. Little-JIL addresses the afore-mentioned inadequacies with a number of innovative features. Our investigation concludes that these innovative features are effective in addressing a number of key reference business process definition needs.", "num_citations": "41\n", "authors": ["638"]}
{"title": "Hpdedup: A hybrid prioritized data deduplication mechanism for primary storage in the cloud\n", "abstract": " Eliminating duplicate data in primary storage of clouds increases the cost-efficiency of cloud service providers as well as reduces the cost of users for using cloud services. Existing primary deduplication techniques either use inline caching to exploit locality in primary workloads or use post-processing deduplication running in system idle time to avoid the negative impact on I/O performance. However, neither of them works well in the cloud servers running multiple services or applications for the following two reasons: Firstly, the temporal locality of duplicate data writes may not exist in some primary storage workloads thus inline caching often fails to achieve good deduplication ratio. Secondly, the post-processing deduplication allows duplicate data to be written into disks, therefore does not provide the benefit of I/O deduplication and requires high peak storage capacity. This paper presents HPDedup, a Hybrid Prioritized data Deduplication mechanism to deal with the storage system shared by applications running in co-located virtual machines or containers by fusing an inline and a post-processing process for exact deduplication. In the inline deduplication phase, HPDedup gives a fingerprint caching mechanism that estimates the temporal locality of duplicates in data streams from different VMs or applications and prioritizes the cache allocation for these streams based on the estimation. HPDedup also allows different deduplication threshold for streams based on their spatial locality to reduce the disk fragmentation. The post-processing phase removes duplicates whose fingerprints are not able to be cached due to the weak temporal locality\u00a0\u2026", "num_citations": "36\n", "authors": ["638"]}
{"title": "Visualising architectural dependencies\n", "abstract": " Visibility of technical debt is critical. A lack thereof can lead to significant problems without adequate visibility as part of the system level decision-making processes [2]. Current approaches for analysing and monitoring architecture related debt are based on dependency analysis to detect code level violations of the software architecture [2,3,6]. However, heterogeneous environments with several systems constructed using OTS, and/or several programming languages may not offer sufficient code visibility. Other limiting factors include legal contracts, Intellectual Property Rights, and just very large systems. Secondly, the complexity of a software dependency is often greater than simple structural dependencies, including; multi-dimensional properties (as argued by [10]); behavioural dependencies [5,9]; and `implicit' dependencies (i.e., dependency inter-relatedness [11]). This paper proposes a simple modelling\u00a0\u2026", "num_citations": "35\n", "authors": ["638"]}
{"title": "Non-intrusive anomaly detection with streaming performance metrics and logs for DevOps in public clouds: a case study in AWS\n", "abstract": " Public clouds are a style of computing platforms, where scalable and elastic Information Technology-enabled capabilities are provided as a service to external customers using Internet technologies. Using public cloud services can reduce costs and increase the choices of technologies, but it also implies limited system information for users. Thus, anomaly detection at user end has to be non-intrusive and hence difficult, particularly during DevOps operations because the impacts from both anomalies and these operations are often indistinguishable, and hence, it is hard to detect the anomalies. In this paper, our work is specific to a successful public cloud, Amazon Web Service, and a representative DevOps operation, rolling upgrade, on which we report our anomaly detection that can effectively detect anomalies. Our anomaly detection requires only metrics data and logs supplied by most public clouds officially. We\u00a0\u2026", "num_citations": "31\n", "authors": ["638"]}
{"title": "Impact of process simulation on software practice: An initial report\n", "abstract": " Process simulation has become a powerful technology in support of software project management and process improvement over the past decades. This research, inspired by the Impact Project, intends to investigate the technology transfer of software process simulation to the use in industrial settings, and further identify the best practices to release its full potential in software practice. We collected the reported applications of process simulation in software industry, and identified its wide adoption in the organizations delivering various software intensive systems. This paper, as an initial report of the research, briefs a historical perspective of the impact upon practice based on the documented evidence, and also elaborates the research-practice transition by examining one detailed case study. It is shown that research has a significant impact on practice in this area. The analysis of impact trace also reveals that the\u00a0\u2026", "num_citations": "26\n", "authors": ["638"]}
{"title": "Subsuming the BPM life cycle in an ontological framework of designing\n", "abstract": " This paper proposes a framework to represent life-cycle activities performed in business process management (BPM). It is based on the function-behaviour-structure (FBS) ontology that represents all design entities uniformly, independently of the specific stages in their life cycle. The framework specifies a set of distinct activities that operate on the function, behaviour and structure of a business process, subsuming the different life-cycle stages within a single framework. This provides an explicit description of a number of BPM issues that are inadequately addressed in current life-cycle models. They include design-time analysis, flexibility of tasks and sub-processes, interaction between life-cycle stages, and the use of experience.", "num_citations": "26\n", "authors": ["638"]}
{"title": "Data Governance for Platform Ecosystems: Critical Factors and the State of Practice\n", "abstract": " Recently, platform ecosystem has received attention as a key business concept. Sustainable growth of platform ecosystems is enabled by platform users supplying and/or demanding content from each other: e.g. Facebook, YouTube or Twitter. The importance and value of user data in platform ecosystems is accentuated since platform owners use and sell the data for their business. Serious concern is increasing about data misuse or abuse, privacy issues and revenue sharing between the different stakeholders. Traditional data governance focuses on generic goals and a universal approach to manage the data of an enterprise. It entails limited support for the complicated situation and relationship of a platform ecosystem where multiple participating parties contribute, use data and share profits. This article identifies data governance factors for platform ecosystems through literature review. The study then surveys the data governance state of practice of four platform ecosystems: Facebook, YouTube, EBay and Uber. Finally, 19 governance models in industry and academia are compared against our identified data governance factors for platform ecosystems to reveal the gaps and limitations.", "num_citations": "23\n", "authors": ["638"]}
{"title": "Effects of architecture and technical development process on micro-process\n", "abstract": " Current software development methodologies (such as agile and RUP) are largely management-centred, macro-process life-cycle models. While they may include some fine-grained micro-process development practices, they usually provide little concrete guidance on appropriate micro-process level day-to-day development activities. The major factors that affect such micro-process activities are not well understood. We propose that software architecture and technical development processes are two major factors. We describe how these two factors affect micro-process activities. We validate our claim by mining micro-processes from two commercial projects and investigating relationships with software architecture and technical development processes.", "num_citations": "22\n", "authors": ["638"]}
{"title": "Trust chain: Establishing trust in the iot-based applications ecosystem using blockchain\n", "abstract": " The Internet of Things (IoT) has already reshaped and transformed our lives in many ways, ranging from how we communicate with people or manage our health to how we drive our cars and manage our homes. With the rapid development of the IoT ecosystem in a wide range of applications, IoT devices and data are going to be traded as commodities in the marketplace in the near future, similar to cloud services or physical objects. Developing such a trading platform has previously been identified as one of the key grand challenges in the integration of IoT and data science. Deployment of such a platform raises concerns about the security and privacy of data and devices since their ownership is hard to trace and manage without a central trusted authority. A central trusted authority is not a viable solution for a fully decentralized and distributed IoT ecosystem with a large number of distributed device vendors and consumers. Blockchain, as a decentralized system, removes the requirement for a trusted third-party by allowing participants to verify data correctness and ensure its immutability. IoT devices can use blockchain to register themselves and organize, store, and share streams of data effectively and reliably. We demonstrate the applicability of blockchain to IoT devices and data management with an aim of providing end-to-end trust for trading. We also give a brief introduction to the topics and challenges for future research toward developing a trustworthy trading platform for IoT ecosystems.", "num_citations": "21\n", "authors": ["638"]}
{"title": "Big data programming models\n", "abstract": " Big Data programming models represent the style of programming and present the interfaces paradigm for developers to write big data applications and programs. Programming models normally the core feature of big data frameworks as they implicitly affects the execution model of big data processing engines and also drives the way for users to express and construct the big data applications and programs. In this chapter, we comprehensively investigate different programming models for big data frameworks with comparison and concrete code examples.", "num_citations": "21\n", "authors": ["638"]}
{"title": "Composing patterns to construct secure systems\n", "abstract": " Building secure applications requires significant expertise. Secure platforms and security patterns have been proposed to alleviate this problem. However, correctly applying patterns to use platform features is still highly expertise-dependent. Patterns are informal and there is a gap between them and platform features. We propose the concept of reusable verified design fragments, which package security patterns and platform features and are verified to provide assurance about their security properties. Design fragments can be composed through four primitive tactics. The verification of the composed design against desired security properties is presented in an assurance case. We demonstrate our approach by securing a Continuous Deployment pipeline and show that the tactics are sufficient to compose design fragments into a secure system. Finally, we formally define composition tactics, which are intended to\u00a0\u2026", "num_citations": "21\n", "authors": ["638"]}
{"title": "On creating industry-wide reference architectures\n", "abstract": " Many industries have been developing e-business standards to improve business-to-business interoperability on a mass scale. Most such standards are composed of business data models with some message exchange patterns. Such data-only standards leave a very large interpretation space for the implementation stage at each individual organization. Thus, true industry-wide interoperability is still hard to achieve. In this industry report, we describe our experiences in creating and evaluating reference architectures for the Australian lending industry. To achieve the right level of prescriptiveness, our reference architectures are deliberately non-structural. Instead, they are based on a set of quality-centric architectural rules. We devised new methods for analyzing interoperability and evaluating such industry-level reference architectures. The first reference architecture has now been adopted and achieved positive\u00a0\u2026", "num_citations": "21\n", "authors": ["638"]}
{"title": "A systematic literature review on federated machine learning: From a software engineering perspective\n", "abstract": " Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.", "num_citations": "20\n", "authors": ["638"]}
{"title": "Big data storage and data models\n", "abstract": " Data and storage models are the basis for big data ecosystem stacks. While storage model captures the physical aspects and features for data storage, data model captures the logical representation and structures for data processing and management. Understanding storage and data model together is essential for understanding the built-on big data ecosystems. In this chapter we are going to investigate and compare the key storage and data models in the spectrum of big data frameworks.", "num_citations": "20\n", "authors": ["638"]}
{"title": "Cloud Application HA using SDN to ensure QoS\n", "abstract": " Users expect cloud applications to be highly available with minimum service disruption. Some of the cloud applications also have Quality of Service (QoS) requirements. The High Availability (HA) module needs to consider QoS requirements while placing or failing over the application, application's components and its replicas. In this paper we propose a new QoS module in the SDN controller. This module creates QoS queues with certain minimum bandwidth on Open Flow based switches on the route between components requiring certain minimum network bandwidth. Our approach is similar to 'Aggregation of RSVP for IPv4 and IPv6 Reservations' (RFC 3175). DiffServ based approaches reserve bandwidth for an entire class of flows and do not cater for requirements from individual applications. Our approach reserves bandwidth on a per-flow basis while overcoming the well-known scalability problem of IntServ\u00a0\u2026", "num_citations": "20\n", "authors": ["638"]}
{"title": "Model driven development with non-functional aspects\n", "abstract": " Model Driven Development (MDD) refers to the systematic use of models as primary engineering artifacts throughout a software development life cycle. In recently years, MDD has been increasingly employed to guide development with a focus on system modeling, code generation from models and white-box analysis of models. However, compositional system analysis regarding early Non-Functional Aspects/Properties (NFP) remains difficult. In this paper, we critically review the state-of-the-art of MDD in the context of non-functional aspects and shed some lights on the following two questions: 1) How to model Non-Functional Aspect/Property (NFP). The focus is to understand the different subtypes of a non-functional aspects and its compositional and emergent nature. 2) How models can be used for analyzing Non-functional Aspect/Property (NFP). This focuses on the analysis models in the form of reasoning\u00a0\u2026", "num_citations": "20\n", "authors": ["638"]}
{"title": "Standardization as a business ecosystem enabler\n", "abstract": " This paper reports research-in-progress that considers standardization as an enabler in business ecosystems. Based on issues encountered, we propose that: business model design is also critical for standards bodies; standards can separate competition from cooperation; standards employ rule-centric designs; and requirements specification and compliance checking are essential. The propositions are illustrated using a standards body with which we have been working.", "num_citations": "20\n", "authors": ["638"]}
{"title": "Deepcu: Integrating both common and unique latent information for multimodal sentiment analysis\n", "abstract": " \u00a9 2019 International Joint Conferences on Artificial Intelligence. All rights reserved. Multimodal sentiment analysis combines information available from visual, textual, and acoustic representations for sentiment prediction. The recent multimodal fusion schemes combine multiple modalities as a tensor and obtain either; the common information by utilizing neural networks, or the unique information by modeling low-rank representation of the tensor. However, both of these information are essential as they render inter-modal and intra-modal relationships of the data. In this research, we first propose a novel deep architecture to extract the common information from the multi-mode representations. Furthermore, we propose unique networks to obtain the modality-specific information that enhances the generalization performance of our multimodal system. Finally, we integrate these two aspects of information via a fusion layer and propose a novel multimodal data fusion architecture, which we call DeepCU (Deep network with both Common and Unique latent information). The proposed DeepCU consolidates the two networks for joint utilization and discovery of all-important latent information. Comprehensive experiments are conducted to demonstrate the effectiveness of utilizing both common and unique information discovered by DeepCU on multiple real-world datasets. The source code of proposed DeepCU is available at https://github.com/sverma88/DeepCU-IJCAI19.", "num_citations": "19\n", "authors": ["638"]}
{"title": "A survey of distributed clustering algorithms\n", "abstract": " Clustering is to divide a set of objects into multiple classes, and each class is made up of similar objects. Traditional centralized clustering algorithms cluster objects stored in a single site, but it cannot satisfy the clustering requirements when objects are distributed. Distributed clustering algorithms can satisfy this need, which extracts a classification mode from distributed objects. This paper classifies and analyzes typical distributed clustering algorithms. Two data sets-Iris and Wine are used to compare several distributed clustering algorithms from two metrics: clustering accuracy and clustering time.", "num_citations": "19\n", "authors": ["638"]}
{"title": "Preliminary results of a systematic review on requirements evolution\n", "abstract": " Background: Software systems must evolve in order to adapt in a timely fashion to the rapid changes of stakeholder needs, technologies, business environment and society regulations. Numerous studies have shown that cost, schedule or defect density of a software project may escalate as the requirements evolve. Requirements evolution management has become one important topic in requirements engineering research. Aim: To depict a holistic state-of-the-art of requirement evolution management. Method: We undertook a systematic review on requirements evolution management. Results: 125 relevant studies were identified and reviewed. This paper reports the preliminary results from this review: (1) the terminology and definition of requirements evolution; (2) fourteen key activities in requirements evolution management; (3) twenty-eight metrics of requirements evolution for three measurement goals\u00a0\u2026", "num_citations": "19\n", "authors": ["638"]}
{"title": "Scaling up software architecture evaluation processes\n", "abstract": " As software systems become larger and more decentralized, increasingly cross organizational boundaries and continue to change, traditional structural and prescriptive software architectures are becoming more rule-centric for better accommodating changes and regulating distributed design and development processes. This is particularly true for Ultra-Large-Scale (ULS) systems and industry-wide reference architectures. However, existing architecture design and evaluation processes have mainly been designed for structural architecture and do not scale up to large and complex system of systems. In this paper, we propose a new software architecture evaluation process \u2013 Evaluation Process for Rule-centric Architecture (EPRA). EPRA reuses and tailors existing proven architecture analysis process components and scales up to complex software-intensive system of systems. We exemplify EPRA\u2019s use in\u00a0\u2026", "num_citations": "18\n", "authors": ["638"]}
{"title": "Applying a selection method to choose Quality Attribute Techniques\n", "abstract": " ContextSoftware products have requirements on software quality attributes such as safety and performance. Development teams use various specific techniques to achieve these quality requirements. We call these \u201cQuality Attribute Techniques\u201d (QATs). QATs are used to identify, analyse and control potential product quality problems. Although QATs are widely used in practice, there is no systematic approach to represent, select, and integrate them in existing approaches to software process modelling and tailoring.ObjectiveThis research aims to provide a systematic approach to better select and integrate QATs into tailored software process models for projects that develop products with specific product quality requirements.MethodA selection method is developed to support the choice of appropriate techniques for any quality attribute, across the lifecycle. The selection method is based on three perspectives: (1) risk\u00a0\u2026", "num_citations": "16\n", "authors": ["638"]}
{"title": "An initial evaluation of requirements dependency types in change propagation analysis\n", "abstract": " Background: Change propagation analysis helps predict the parts of the software that may be affected if a change is made. Existing research on change propagation focuses on design and code level changes. However, as a software evolves, the requirements that drive these changes also have intricate dependencies. Understanding the effect of these requirement dependencies on change prorogation is useful but not trivial. More than twenty requirements dependency types have been identified in the literature, however there still lacks an evaluation of the applicability of these dependency types in requirements and change propagation analysis. Aim: We aim to investigate whether these dependency types are useful for change propagation analysis. Method: We conducted a case study in a real-world industry project. This case study evaluates two representative dependency models covering twenty five types of\u00a0\u2026", "num_citations": "15\n", "authors": ["638"]}
{"title": "Sharing deep neural network models with interpretation\n", "abstract": " Despite outperforming humans in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, where the model is developed by a third party. For a supervised machine learning model, sharing training process including training data is a way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight into a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated deep neural network models with high fidelity. We show that data point pairs\u00a0\u2026", "num_citations": "14\n", "authors": ["638"]}
{"title": "Designing Data Governance in platform ecosystems\n", "abstract": " As platform ecosystems such as Facebook or Twitter are rapidly growing through platform users\u2019 data contribution, the importance of data governance has been highlighted. Platform ecosystems, however, face increasing complexity derived from the business context such as multiple parties\u2019 participation. How to share control and decision rights about data assets with platform users is regarded as a significant governance design issue. However, there is a lack of studies on this issue. Existing design models focus on the characteristics of enterprises. Therefore, there is limited support for platform ecosystems where there are different types of context and complicated relationships. To deal with the issue, this paper proposes a novel design approach for data governance in platform ecosystems including design principles, contingency factors and an architecture model. Case studies are performed to illustrate the practical implications of our suggestion.", "num_citations": "14\n", "authors": ["638"]}
{"title": "HDM: A composable framework for big data processing\n", "abstract": " Over the past years, frameworks such as MapReduce and Spark have been introduced to ease the task of developing big data programs and applications. However, the jobs in these frameworks are roughly defined and packaged as executable jars without any functionality being exposed or described. This means that deployed jobs are not natively composable and reusable for subsequent development. Besides, it also hampers the ability for applying optimizations on the data flow of job sequences and pipelines. In this paper, we present the Hierarchically Distributed Data Matrix (HDM) which is a functional, strongly-typed data representation for writing composable big data applications. Along with HDM, a runtime framework is provided to support the execution, integration and management of HDM applications on distributed infrastructures. Based on the functional data dependency graph of HDM, multiple\u00a0\u2026", "num_citations": "14\n", "authors": ["638"]}
{"title": "Towards an architectural viewpoint for systems of software intensive systems\n", "abstract": " An important aspect of architectural knowledge is the capture of software relationships [25]. But current definitions [25, 21, 23] do not adequately capture external system relationships [5], and offer no guidance on implicit relationships [29]. This leaves architects either unaware of critical relationships or, to'roll their own'based on aggregations of code-level call structures, resulting in critical architectural gaps and communication problems within Systems of Software intensive Systems (S3) environments [2]. These environments may also restrict the sharing of architectural knowledge due to either legal, or contractual constraints, or overwhelm due to the size and number of involved systems adding to the challenges of identifying and describing the relationships.", "num_citations": "14\n", "authors": ["638"]}
{"title": "Model-Driven Architecture\n", "abstract": " One problem lurking at the back of the ICDE development team\u2019s mind is related to capacity planning for new ICDE installations. When an ICDE installation supports multiple users, the request load will become high, and the hardware that the platform runs on needs to be powerful enough to support this request load. If the hardware becomes saturated, it will not be able to process all user generated events, and important data may be lost. The situation is exacerbated by the following issues:", "num_citations": "14\n", "authors": ["638"]}
{"title": "Statistically managing cloud operations for latency-tail-tolerance in IoT-enabled smart cities\n", "abstract": " Smart City is typically large scale and IoT-enabled online services for huge amount of streaming data generated by sensors, and the services are often deployed in clouds, where infrastructure and services need to be maintained and optimised to achieve the best quality of service by using various cloud operations that are running in the background of Smart City business. However, on one hand the background operations inevitably press a negative impact on the latency of normal requests to the services, while on the other hand to maintain the short latency of requests usually results in unreasonably long latency of background operations. In this paper, on the architecture of IoT-enabled Smart City services deployed in clouds, our motivation is to find the best management policy of operations for normal request traffic, which is stable and stationary and to which operations are inserted. We focus on the system\u00a0\u2026", "num_citations": "13\n", "authors": ["638"]}
{"title": "Technical software development process in the XML domain\n", "abstract": " Background: A Technical Development Process (TDP) is a development process for a particular technology, such as XML, service orientation, object orientation or a programming language. Unlike software development life-cycle processes, TDPs provide concrete and detailed guidance to software engineers working in a particular technology domain. TDPs are currently not well understood in terms of description, modelling and interactions with life-cycle processes. Aim: In this paper, we investigate what are TDPs in the XML domain and how can TDPs be modelled using existing development process modelling notations and tools. Method: We extracted XML specific TDPs from literatures, interviews and internal documentation within software development organizations and conducted systematic verifications and validations. Results: We identify different types of TDPs in the XML domain and\u00a0\u2026", "num_citations": "12\n", "authors": ["638"]}
{"title": "Situational method quality\n", "abstract": " Some overall method characteristics, such as agility and scalability, have become increasingly important. These characteristics are different from existing method requirements which focus on the functional purposes of individual method chunks and overall methods. Characteristics like agility and scalability are often not embodied in the function of a single method chunk but are instead reflected in constraints over one or more method chunks, connections between method chunks and cross-cutting aspects of the overall method. We propose the concept of method tactics, which are techniques for achieving certain method quality attributes. We identify a list of method tactics focusing on agility and scalability by considering factors that affect these quality attributes. We validate the feasibility of using method tactics by applying them to traditional software development method chunks and deriving practices for\u00a0\u2026", "num_citations": "12\n", "authors": ["638"]}
{"title": "An approach to adaptive distributed execution monitoring for workflows in service-based systems\n", "abstract": " Systems based on service-oriented architecture are called service-based systems (SBS), and comprise of computing services offered by various organizations. Users of SBS often require these services to be composed into complex workflows to perform their high-level tasks. The users usually have certain expectations on the overall QoS of their workflows. Due to the highly dynamic environments of SBS, in which temporary unavailability or quality- degradation of services may occur frequently and unexpectedly, monitoring the execution of workflows in SBS is necessary, and should be done in distributed and proactive manner. In this paper, a virtual machine-based architecture for the execution, monitoring and control of workflows in SBS is presented. Based on this architecture, an approach to automated generation of workflow monitors for adaptive distributed execution monitoring of workflows in SBS is discussed.", "num_citations": "12\n", "authors": ["638"]}
{"title": "A contingency-based approach to data governance design for platform ecosystems\n", "abstract": " Data today is regarded as a new type of fuel of platform ecosystems. It enables sustainable growth of platform ecosystems through users\u2019 data contribution. Data governance is necessary to safely manage platform data and succeed in business. Platform ecosystems, however, encounter a complicated business context and environment since there are multiple participating groups, different strategies, goals, and different levels of market regulations. How to adopt appropriate data governance dealing with the concerns has been neglected, and therefore there is little research on this topic. To respond to the challenge, we introduce a novel design approach which can address the various contingencies, characteristics and governance goals of platform ecosystems. We present a case study and use case to illustrate the implications and support the implementation of the approach in practice.", "num_citations": "11\n", "authors": ["638"]}
{"title": "CF4BDA: A conceptual framework for big data analytics applications in the cloud\n", "abstract": " Building big data analytics (BDA) applications in the cloud introduces inevitable challenges, such as loss of control and uncertainty. To address the existing challenges, numerous efforts have been made on BDA application engineering to optimize the quality of BDA applications in the cloud, such as performance and reliability. However, there is still a lack of systematic view on engineering BDA applications in the cloud. Therefore, in this paper, we present a conceptual framework named CF4BDA to analyze the existing work on BDA applications from two perspectives: 1) the lifecycle of BDA applications and 2) the objects involved in the context of BDA applications in the cloud. The framework can help researchers and practitioners identify the research opportunities in a structured way and guide implementing BDA applications in the cloud. We perform a preliminary evaluation of the usefulness of CF4BDA by\u00a0\u2026", "num_citations": "11\n", "authors": ["638"]}
{"title": "Using architecture integration patterns to compose enterprise mashups\n", "abstract": " Enterprise mashups deal with corporate data and various sources of information to compose new value-added applications. The architecture design of enterprise mashups encompasses integration issues-it needs to integrate heterogeneous data and/or compose new situational applications from existing infrastructure. We envisage that architecture integration patterns can be applied not only as architecture solutions to mashup development, but also to help develop practical mashup techniques. In this paper, we combine several common architecture integration patterns, namely pipes and filters, data federation, and model-view-control to compose enterprise mashups. A number of techniques are also developed to customize these patterns for specific mashup needs. We illustrate our approach with a property valuation service derived from a real-world setting.", "num_citations": "11\n", "authors": ["638"]}
{"title": "Reactive control strategy for wind farm considering reactive power command\n", "abstract": " With more wind power integrated into power grid, its impact on voltage is larger so that the command that wind farm should support voltage and reactive power in wind power integrated region is required by system. Based on this, a reactive power compensation control method is proposed considering voltage and reactive power command in integrated region. The voltage of point of interconnection (POI) is chosen as controlled voltage and the related reactive power control is divided into three sections, namely, normal control, abnormal control and urgent control. Accordingly, three control modes are given, including abnormal control mode, urgent control mode and fault control mode. In implementation, average controlled voltage in a specific period is used to judge reactive power control section. Further, the control mode is confirmed according to average controlled voltage differences between two different periods\u00a0\u2026", "num_citations": "11\n", "authors": ["638"]}
{"title": "An ontologically-based evaluation of software design methods\n", "abstract": " This paper develops an ontological basis for evaluating software design methods, based on the situated function\u2013behaviour\u2013structure framework. This framework accounts for the situatedness of designing, viewing it as a dynamic activity driven by interactions between designers and the artefacts being designed. On the basis of this framework, we derive a general evaluation schema that we apply to five software design methods. The ideas presented in this work contribute to a better understanding of design methods, and uncover opportunities for method integration and development.", "num_citations": "11\n", "authors": ["638"]}
{"title": "Hybrid modeling of test-and-fix processes in incremental development\n", "abstract": " Software process simulation modeling has become an increasingly active research area for managing and improving software development processes since its introduction in the last two decades. Hybrid process simulation models have attracted interest as a possibility to avoid the limitations of applying single modeling method, and more realistically capture complex real-world software processes. This paper presents a hybrid process modeling scheme to build an integrated software process model. It focuses on the particular portion of software process by using different techniques on separate but interconnected phases, while still allows for the integrity of modeling development process. We developed a hybrid simulation model of the test-and-fix process of incremental software development. Results conclude that this approach can support the investigation of portions of software process at different\u00a0\u2026", "num_citations": "11\n", "authors": ["638"]}
{"title": "A differentiated caching mechanism to enable primary storage deduplication in clouds\n", "abstract": " Existing primary deduplication techniques either use inline caching to exploit locality in primary workloads or use post-processing deduplication to avoid the negative impact on I/O performance. However, neither of them works well in the cloud servers running multiple services for the following two reasons: First, the temporal locality of duplicate data writes varies among primary storage workloads, which makes it challenging to efficiently allocate the inline cache space and achieve a good deduplication ratio. Second, the post-processing deduplication does not eliminate duplicate I/O operations that write to the same logical block address as it is performed after duplicate blocks have been written. A hybrid deduplication mechanism is promising to deal with these problems. Inline fingerprint caching is essential to achieving efficient hybrid deduplication. In this paper, we present a detailed analysis of the limitations of\u00a0\u2026", "num_citations": "10\n", "authors": ["638"]}
{"title": "Reference Architecture for Lending Industry in ULS Systems\n", "abstract": " The ecosystem within the lending industry and beyond is a ULS system. Existing business data and process centric standards have very limited governing power over the quality and evolution of such ULS systems. We have helped to develop an initial reference architecture and associated development guidelines for a lending industry e-business standard, to assist in solving current problems and to promote more sophisticated use of the standard in the context of the LIXI ULS system. The nature of such RA should be quality-centric rather than structure-centric. We have proposed a few technical solutions to help achieve this.", "num_citations": "10\n", "authors": ["638"]}
{"title": "Distilling Scenarios from Patterns for Software Architecture Evaluation\n", "abstract": " Software architecture (SA) evaluation is a quality assurance technique that is increasingly attracting significant research and commercial interests. A number of SA evaluation methods have been developed. Most of these methods are scenario-based, which relies on the quality of the scenarios used for the evaluation. Most of the existing techniques for developing scenarios use stakeholders and requirements documents as main sources of collecting scenarios. Recently, architectures of large software systems are usually composed of patterns and styles. One of the purposes of using patterns is to develop systems with predictable quality attributes. Since patterns are documented in a format that requires the inclusion of problem, solution and quality consequences, we observed that scenarios are, though as informal text, pervasive in patterns description, which can be extracted and documented for the SA\u00a0\u2026", "num_citations": "10\n", "authors": ["638"]}
{"title": "Mobile cloud computing for disaster emergency operation: A systematic review\n", "abstract": " The advancement of mobile cloud computing (MCC) has the potential to improve communication and information during disaster emergency operation. Many studies from different countries have addressed different approaches to implement mobile cloud technologies during disaster operation. However, there has been a lack of any consolidated evidence-based study to evaluate the MCC from the dual perspectives of disaster emergency situation and technology. In this paper, we provide an extensive study of mobile cloud computing research as a critical system or application based on the latest literature published from 2010 to 2015, and highlights the specific challenges in implementing mobile cloud computing in disaster emergency operation. A systematic literature review was conducted over 4 prominent journals of computer science engineering (IEEE, ACM, ProQuest, Inspec/Elsevier) and the findings were\u00a0\u2026", "num_citations": "9\n", "authors": ["638"]}
{"title": "Composable and efficient functional big data processing framework\n", "abstract": " Over the past years, frameworks such as MapReduce and Spark have been introduced to ease the task of developing big data programs and applications. However, the jobs in these frameworks are roughly defined and packaged as executable jars without any functionality being exposed or described. This means that deployed jobs are not natively composable and reusable for subsequent development. Besides, it also hampers the ability for applying optimizations on the data flow of job sequences and pipelines. In this paper, we present the Hierarchically Distributed Data Matrix (HDM) which is a functional, strongly-typed data representation for writing composable big data applications. Along with HDM, a runtime framework is provided to support the execution of HDM applications on distributed infrastructures. Based on the functional data dependency graph of HDM, multiple optimizations are applied to improve\u00a0\u2026", "num_citations": "9\n", "authors": ["638"]}
{"title": "Multi-objective optimisation of rolling upgrade allowing for failures in clouds\n", "abstract": " Rolling upgrade is a practical industry technique for online updating of software in distributed systems. This paper focuses on rolling upgrade of software versions in virtual machine instances on cloud computing platforms, when various failures may occur. An operator can choose the number of instances that are updated in one round and system environments to minimise completion time, availability degradation, and monetary cost for entire rolling upgrade, and hence this is a multi-objective optimisation problem. To predict completion time in the presence of failures, we offer a stochastic model that represents the dynamics of rolling upgrade. To reduce the computational effort of decision making for large scale complex systems, we propose a technique that can find a Pareto set quickly via an upper bound of the expected completion time. Then an optimum of the original problem can be chosen from this set of\u00a0\u2026", "num_citations": "9\n", "authors": ["638"]}
{"title": "Application level HA and QoS using SDN\n", "abstract": " Users expect cloud applications to be highly available with minimum service disruption. Most of the current systems use High Availability (HA) solutions that detect host and Virtual Machine (VM) failures. These solutions don\u2019t detect the failure of applications running inside the VMs. Detection of application failure is critical for stateful applications. Some applications come with their own HA logic to handle application failures. There are some systems which provide system level HA to detect application failures. These system level solutions are very generic in nature and don\u2019t cater to individual application\u2019s requirements. Different applications have different HA requirements and the overall HA solution need to cater to these individual applications\u2019 HA requirements. Some of the cloud applications also have Quality of Service (QoS) requirements. The High Availability (HA) module needs to consider QoS requirements\u00a0\u2026", "num_citations": "9\n", "authors": ["638"]}
{"title": "Risks of off-the-shelf-based software acquisition and development: a systematic mapping study and a survey\n", "abstract": " Background- Risks associated with a software project have the potential to affect all stakeholders. Today much software makes use of off-the-shelf (OTS) components. A better understanding of OTS-derived software risks will help to define responsibilities for these risks, and also to avoid them. Aim- Our objective is to identify, classify and compare risks of OTS-based software projects from both a software development and a software acquisition perspective. Method- To identify and classify the risks, we performed a systematic mapping study. In order to compare risks of OTS-based software development and acquisition in the real world setting, we used the mapping study results to survey occurrences of 11 shared risks in OTS-based software, in 35 OTS-based software developments and 34 OT-Sbased software acquisitions of Indonesian background. The survey is a partial replication of a previous study. Results- We\u00a0\u2026", "num_citations": "9\n", "authors": ["638"]}
{"title": "HDM: Optimized Big Data Processing with Data Provenance.\n", "abstract": " Big Data applications are becoming more complex and experiencing frequent changes and updates. In practice, manual optimization of complex big data jobs is time-consuming and error-prone. Maintenance and management of evolving big data applications is a challenging task as well. We demonstrate HDM, Hierarchically Distributed Data Matrix, as a big data processing framework with built-in data flow optimizations and integrated maintenance of data provenance information that supports the management of continuously evolving big data applications. In HDM, the data flow of jobs are automatically optimized based on the functional DAG representation to improve the performance during execution. Additionally, comprehensive meta-data related to explanation, execution and dependency updates of HDM applications are stored and maintained in order to facilitate the debugging, monitoring, tracing and reproducing of HDM jobs and programs.", "num_citations": "8\n", "authors": ["638"]}
{"title": "Runtime recovery actions selection for sporadic operations on public cloud\n", "abstract": " Sporadic operations such as rolling upgrade or machine instance redeployment are prone to unpredictable failures in the public cloud largely because of the inherent high variability nature of public cloud. Previous dependability research has established several recovery methods for cloud failures. In this paper, we first propose eight recovery patterns for sporadic operations on public cloud. We then present the filtering process which filters applicable recovery patterns. We propose an automation mechanism to automatically generate recovery actions for those applicable recovery patterns based on our resource state transition algorithm. We also propose a methodology to evaluate the recovery actions generated for the applicable recovery patterns based on the recovery evaluation metrics of Recovery Time, Recovery Cost, and Recovery Impact. This quantitative evaluation will lead to selection of the acceptable\u00a0\u2026", "num_citations": "8\n", "authors": ["638"]}
{"title": "A Systematic Mapping Study on Off-The-Shelf-based Software Acquisition\n", "abstract": " Acquiring software from external suppliers and developing less software in-house can help software-developing organizations improve operational efficiency by reducing costs, time and reusing current technologies. Software projects increasingly use Off-The-Shelf (OTS) products. From the acquirer perspective, there is a need to understand in more detail OTS-based software acquisition processes, because they are different to and less well-understood than those for the acquisition of custom software. In this paper we have undertaken a systematic mapping study on OTS-based software acquisition. The study compares and contrasts OTS-based software acquisition and non-OTS-based software acquisition, and identifies factors influencing decision making in OTS-based software acquisition. We find that the main difference is that there is a relationship between determining the software requirements and OTS selection in OTS-based software acquisition. For commercial OTS software, the major factors are functionality and quality of the software, but for open-source OTS software, cost was the most important factor.", "num_citations": "8\n", "authors": ["638"]}
{"title": "Representation of quality attribute techniques using SPEM and EPF composer\n", "abstract": " There are many development techniques used to assist development teams to achieve required levels of product quality such as safety, security and performance. These``Quality Attribute Techniques''(QAT) aim to identify, eliminate, reduce, control and minimise potential quality problems in the development of critical systems. Although widely used, these techniques are not normally well represented in software process models. This paper proposes two alternative representations of Quality Attribute Techniques using the SPEM metamodel and Eclipse Process Framework (EPF) Composer and shows how these techniques can be incorporated into software development process models. Safety techniques have been selected as a case example for evaluation. The evaluation identifies advantages and limitations of the SPEM and EPF Composer in terms of their ability to support representation and integration of\u00a0\u2026", "num_citations": "8\n", "authors": ["638"]}
{"title": "Common software-aging-related faults in fault-tolerant systems\n", "abstract": " In recent years, remarkable attention has been focused on software aging phenomena, in which the performance of software systems degrades with time. Fault-tolerant software systems which provide high assurance may suffer from such phenomena. Based on the common software-aging-related faults in fault-tolerant systems, a behavior model of a double-version fault-tolerant software system is established using Markov reward model. The performance of the system such as expected service rate in steady state is evaluated and the sensitivity analysis of some parameters is performed.", "num_citations": "8\n", "authors": ["638"]}
{"title": "Data Governance Decisions for Platform Ecosystems\n", "abstract": " Platform ecosystem has become an information system research subject after many years of industry success. The concept of platform ecosystem facilitates fast and self-growing of a platform by encouraging data contribution/consumption of multiple networks, and thus the importance and value of data in platforms is accentuated. It is essential to understand how data should be managed in platform ecosystems where there is complicated relationships between multiple participating groups. However, this topic has been rarely addressed in industry and academia. Industry governance frameworks focus on organizational data, and prior research on platform ecosystem is still in early-stage. To response to the limitation, we propose critical data governance decisions for platform ecosystems, and discuss how they have to be implemented in practice. This study supports right decision making about data, and facilitates a secure platform ecosystem. We perform a case study to illustrate the practical implications of this study.", "num_citations": "7\n", "authors": ["638"]}
{"title": "Towards big data analytics across multiple clusters\n", "abstract": " Big data are increasingly collected and stored in a highly distributed infrastructures due to the development of sensor network, cloud computing, IoT and mobile computing among many other emerging technologies. In practice, the majority of existing big-data-processing frameworks (e.g., Hadoop and Spark) are designed based on the single-cluster setup with the assumptions of centralized management and homogeneous connectivity which makes them sub-optimal and sometimes infeasible to apply for scenarios that require implementing data analytics jobs on highly distributed data sets (across racks, data centers or multi-organizations). In order to tackle this challenge, we present HDM-MC, a multi-cluster big data processing framework which is designed to enable the capability of performing large scale data analytics across multi-clusters with minimum extra overhead due to additional scheduling requirements\u00a0\u2026", "num_citations": "7\n", "authors": ["638"]}
{"title": "The need for software architecture evaluation in the acquisition of software-intensive sysetms\n", "abstract": " The software architecture for a software-intensive system defines the main elements of the system, their relationships, and the rationale for them in the system. Software architecture is fundamental to whether a system can achieve its quality objectives. Architecture evaluation is an approach for assessing whether a software architecture can support the system needs, especially its non-functional requirements also known as quality requirements. Architecture evaluation can be used at different stages of a project, and is an effective way of ensuring design quality early in the lifecycle to reduce overall project cost and to manage risks. This report describes software architecture and architecture evaluation, and summarises some of the key benefits for software architecture evaluation that have been observed both in industry and in international Defence contexts. We make some general recommendations about architecture evaluation in the context of Australian defence acquisition.Descriptors:", "num_citations": "7\n", "authors": ["638"]}
{"title": "Hybrid networks: Improving deep learning networks via integrating two views of images\n", "abstract": " The principal component analysis network (PCANet) is an unsupervised parsimonious deep network, utilizing principal components as filters in the layers. It creates an amalgamated view of the data by transforming it into column vectors which destroys its spatial structure while obtaining the principal components. In this research, we first propose a tensor-factorization based method referred as the Tensor Factorization Networks (TFNet). The TFNet retains the spatial structure of the data by preserving its individual modes. This presentation provides a minutiae view of the data while extracting matrix factors. However, the above methods are restricted to extract a single representation and thus incurs information loss. To alleviate this information loss with the above methods we propose Hybrid Network (HybridNet) to simultaneously learn filters from both the views of the data. Comprehensive results on multiple\u00a0\u2026", "num_citations": "6\n", "authors": ["638"]}
{"title": "A data governance framework for platform ecosystem process management\n", "abstract": " Platform ecosystem today is regarded as the key business concept of organizations to win market. Platform companies can grow fast through the data contribution of multi-sided networks. Yet, they face difficulties in managing the data resulted from complicated contribution, use and interactions between the multiple parties. The circumstance causes serious concerns about unclear data ownership and invisible use of data, and ultimately leads to data abuse/misuse or privacy violation. To alleviate to this, a particular type of data governance is required. However, there is limited research on data and data governance for platform ecosystems. We introduce a new data governance framework for platform ecosystems which consists of data, role, decisions and due processes. The framework supports organizations in understanding to show how the risks should be dealt in the processes for business success. We\u00a0\u2026", "num_citations": "6\n", "authors": ["638"]}
{"title": "Extracting highly effective features for supervised learning via simultaneous tensor factorization\n", "abstract": " Real world data is usually generated over multiple time periods associated with multiple labels, which can be represented as multiple labeled tensor sequences. These sequences are linked together, sharing some common features while exhibiting their own unique features. Conventional tensor factorization techniques are limited to extract either common or unique features, but not both simultaneously. However, both types of these features are important in many machine learning systems as they inherently affect the systems' performance. In this paper, we propose a novel supervised tensor factorization technique which simultaneously extracts ordered common and unique features. Classification results using features extracted by our method on CIFAR-10 database achieves significantly better performance over other factorization methods, illustrating the effectiveness of the proposed technique.", "num_citations": "6\n", "authors": ["638"]}
{"title": "Understanding software architecture\n", "abstract": " The last 15 years have seen a tremendous rise in the prominence of a software engineering subdiscipline known as software architecture. Technical Architect and Chief Architect are job titles that now abound in the software industry. There\u2019s an International Association of Software Architects, and even a certain well-known wealthiest geek on earth used to have \u201carchitect\u201d in his job title in his prime. It can\u2019t be a bad gig, then?", "num_citations": "6\n", "authors": ["638"]}
{"title": "Quality attribute techniques framework\n", "abstract": " The quality of software is achieved during its development. Development teams use various techniques to investigate, evaluate and control potential quality problems in their systems. These \u201cQuality Attribute Techniques\u201d target specific product qualities such as safety or security. This paper proposes a framework to capture important characteristics of these techniques. The framework is intended to support process tailoring, by facilitating the selection of techniques for inclusion into process models that target specific product qualities. We use risk management as a theory to accommodate techniques for many product qualities and lifecycle phases. Safety techniques have motivated the framework, and safety and performance techniques have been used to evaluate the framework. The evaluation demonstrates the ability of quality risk management to cover the development lifecycle and to accommodate two\u00a0\u2026", "num_citations": "6\n", "authors": ["638"]}
{"title": "Composing adaptive Web services on COTS middleware\n", "abstract": " Composing adaptive and self-managing Web services needs plug-and-play architecture so that the deployment of control components does not require changes made to the Web services and the host middleware platforms. This is especially challenging for Web services running on COTS middleware platforms, such as Microsoft.Net. In this paper, we propose an architectural solution that introduces a management proxy between adaptive control components and Web services. The management proxy can be customized and seamlessly integrated with a COTS middleware platform by leveraging the existing middleware mechanisms. This solution enables dynamically composing adaptive Web services on COTS middleware without stopping its services. We demonstrate this architecture by a realistic Web service application built on .Net Windows Communication Foundation (WCF). The performance overhead\u00a0\u2026", "num_citations": "6\n", "authors": ["638"]}
{"title": "Going deep: Graph convolutional ladder-shape networks\n", "abstract": " Neighborhood aggregation algorithms like spectral graph convolutional networks (GCNs) formulate graph convolutions as a symmetric Laplacian smoothing operation to aggregate the feature information of one node with that of its neighbors. While they have achieved great success in semi-supervised node classification on graphs, current approaches suffer from the over-smoothing problem when the depth of the neural networks increases, which always leads to a noticeable degradation of performance. To solve this problem, we present graph convolutional ladder-shape networks (GCLN), a novel graph neural network architecture that transmits messages from shallow layers to deeper layers to overcome the over-smoothing problem and dramatically extend the scale of the neural networks with improved performance. We have validated the effectiveness of proposed GCLN at a node-wise level with a semi-supervised task (node classification) and an unsupervised task (node clustering), and at a graph-wise level with graph classification by applying a differentiable pooling operation. The proposed GCLN outperforms original GCNs, deep GCNs and other state-of-the-art GCN-based models for all three tasks, which were designed from various perspectives on six real-world benchmark data sets.", "num_citations": "5\n", "authors": ["638"]}
{"title": "Interpreting shared deep learning models via explicable boundary trees\n", "abstract": " Despite outperforming the human in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, when the model is developed by a third party. For a supervised machine learning model, sharing training process including training data provides an effective way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight of a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated model with high fidelity. We show that traversing data points in the tree gives users significantly better understanding of the model and paves the way for trustworthy model sharing.", "num_citations": "5\n", "authors": ["638"]}
{"title": "Design Choices for Data Governance in Platform Ecosystems: A Contingency Model\n", "abstract": " As platform ecosystems are growing by platform users' data, the importance of data governance has been highlighted. In particular, how to share control and decision rights with platform users are regarded as significant design issues since the role of them is increasing. Platform context should be considered when designing data governance in platform ecosystems (i.e. centralized/decentralized governance). However, there is limited research on this issue. Existing models focus on characteristics for enterprises. This results in limited support for platform ecosystems where there are different types of business context such as open strategies or platform maturity. This paper develops a contingency model for platform ecosystems including distinctive contingency factors. The study then discusses which data governance factors should be carefully considered and strengthened for each contingency in order to succeed in governance and to win market. A case study is performed to validate our model and to show its practical implications.", "num_citations": "5\n", "authors": ["638"]}
{"title": "Runtime recovery actions selection for sporadic operations on cloud\n", "abstract": " Sporadic operations such as rolling upgrade or machine instance redeployment are prone to unpredictable failures in the cloud largely due to the inherent high variability nature of cloud. Previous dependability research has established several recovery methods for cloud failures. In this paper, we first propose eight recovery patterns for sporadic operations. We then present the filtering process which filters applicable recovery patterns for a given operational step. We also propose a methodology to evaluate the recovery actions generated for the applicable recovery patterns based on the metrics of Recovery Time, Recovery Cost and Recovery Impact. This quantitative evaluation will lead to selection of optimal recovery actions. We implement a recovery service and illustrate its applicability by recovering from errors occurring in Asgard rolling upgrade operation on cloud. The experimental results show that the\u00a0\u2026", "num_citations": "5\n", "authors": ["638"]}
{"title": "A component-based approach to developing thematic mashups\n", "abstract": " Mashup provides a way of forming new applications from existing Web content using APIs provided by different Web sites. Such a nature makes mashup a promising technology to deliver a Web-based Enterprise application with rich information of various themes, so called thematic mashup. However, the development of thematic mashup is ad-hoc and mostly from the scratch, which can be a barrier for Enterprise applications to leverage mashups. For an Enterprise application, the complexity of business logic and the requirements to adapt to a changing business environment demand a development method that helps to achieve reusability and maintainability. Component-based development method has proved practical to reduce the complexity of software development by reusing modular components. In this paper, we propose a reference architecture with key components identified for developing thematic mashups. We further demonstrate the usage of this reference architecture in a case study-developing a thematic mashup for a Web-based property valuation application. The contribution of this paper is applying a well-established software engineering discipline to emerging mashup applications, and providing insights of the techniques in developing mashups.", "num_citations": "5\n", "authors": ["638"]}
{"title": "eXplainable AI (XAI) an introduction to the XAI landscape with practical examples\n", "abstract": " SIGGRAPH Asia 2020 Courses: eXplainable AI (XAI): an introduction to the XAI landscape with practical examples Page 1 eXplainable AI (XAI): An introduction to the XAI landscape with practical examples Rowan Test Rowan Hughes1, Cameron Edmond1, Lindsay Wells1, Mashhuda Glencross2,3, Liming Zhu3, Tomasz Bednarz1,3 1 UNSW, Sydney, Australia 2 UQ, Brisbane, Australia 3 CSIRO\u2019s Data61, Australia Page 2 Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s). SA '20 Courses, December 04-13, 2020, Virtual Event, Republic of Korea \u2026", "num_citations": "4\n", "authors": ["638"]}
{"title": "Research of government horizon business integration management system based on HRNM and agent\n", "abstract": " Aiming at government horizon business integration that has becoming a focus problem in the field of e-government, the hierarchical role network model (HRNM) is presented in this paper. The HRNM is considered as the management and control model of horizon business integration. Furthermore, the architecture of the government horizon business integration management system (GHBIMS) based on HRNM and Agent is presented. The HRNM and GHBIMS proposed by this paper stress the importance of the role, satisfy the characteristics and requirements of government horizon business integration, and have a wide application prospect in practice", "num_citations": "4\n", "authors": ["638"]}
{"title": "SMINT: Toward interpretable and robust model sharing for deep neural networks\n", "abstract": " Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not\u00a0\u2026", "num_citations": "3\n", "authors": ["638"]}
{"title": "A deadlock detection method for inter-organizational business process based on role network model\n", "abstract": " In the context of inter-organizational business collaboration, structural deadlock detection is an important means of guaranteeing the correctness of an inter-organizational business process model. The existing deadlock detection methods have limitations in handling some aspects of the dynamics and uncertainty arising from inter-organizational nature of the processes. Role Network Model (RNM) was proposed to model knowledge intensive systems and can better represent the dynamic aspects of business processes using roles. But RNM is weak in the explicit description of business processes. Therefore, an extended Directed Acyclic Graph is proposed to better describe the process side complementing RNM. Second, some extended reduction rules targeting process dynamics are proposed to reduce the complexity. Third, a deadlock detection method is proposed to support highly dynamic processes. At both\u00a0\u2026", "num_citations": "3\n", "authors": ["638"]}
{"title": "Towards process-based composition of self-managing service-oriented systems\n", "abstract": " Loose coupling and preserving safe changes are two key criteria for composing self-managing services. Composition of adaptive control components with business services without interfering with the original service operations is complicated by the dynamic and highly distributed nature of service-oriented systems. Essentially, encapsulating control logic into abstract logical models enables a clear separation of concerns, with states and transitions indicating the logical control flow. The challenge is to seamlessly integrate these models with services and their host infrastructure as a unified self-managing environment. In this paper, we present an architectural solution towards process-based composition and coordination of self-managing services. This architecture framework leverages business process models to produce declarative and executable control models. We discuss the problem context and outline the\u00a0\u2026", "num_citations": "3\n", "authors": ["638"]}
{"title": "Investigating test-and-fix processes of incremental development using hybrid process simulation\n", "abstract": " Software process modeling has become an essential technique for managing, investigating and improving software development processes. In this area, hybrid process simulation modeling attracts an increasing research attention. This paper presents a new hybrid software process model to investigate the test-and-fix process of incremental development. Its novelty comes from its flexible model structure that focuses on the particular portion of software process by using different modeling techniques on separate but interconnected phases in incremental development. Simulation results conclude that this model can support the investigation of portions of incremental development life cycle at different granularity levels simultaneously. It also allows the tradeoff analysis and optimization of test-and-fix process, while avoids the limitation caused by incomplete process detail of other phases.", "num_citations": "3\n", "authors": ["638"]}
{"title": "Research on knowledge modeling system oriented to knowledge management for process enterprises\n", "abstract": " Due to its unique technical economic characteristics of process enterprises-collaboration between product units and market demand, it is difficult to implement knowledge management in process enterprises. To resolve this problem, a threelevel process knowledge modeling system for process enterprises composed of dynamic process simulation model, process control model, technical economic evaluation and management decision-making model was analyzed. Then knowledge classification method was proposed which including static knowledge, strategic knowledge and reasoning knowledge. The decision-making model based on market demand and production process was implemented in process enterprises. The model was prerequisite to modern integration process system., Then the object-oriented knowledge model representation based on classes was studied, which was composed of model class, model framework and model instance. Knowledge acquisition method and knowledge release mode, which was characterized by process simulation and data mining, were also discussed considering of its industrial characteristics. Several examples were given to illustrate application of the techniques.", "num_citations": "3\n", "authors": ["638"]}
{"title": "A Systematic Literature Review on Blockchain Governance\n", "abstract": " Blockchain has been increasingly used as a software component to enable decentralisation in software architecture for a variety of applications. Blockchain governance has received considerable attention to ensure the safe and appropriate use and evolution of blockchain, especially after the Ethereum DAO attack in 2016. To understand the state-of-the-art of blockchain governance and provide an actionable guidance for academia and practitioners, in this paper, we conduct a systematic literature review, identifying 34 primary studies. Our study comprehensively investigates blockchain governance via 5W1H questions. The study results reveal several major findings: 1) the adaptation and upgrade of blockchain are the primary purposes of blockchain governance, while both software quality attributes and human value attributes need to be increasingly considered; 2) blockchain governance mainly relies on the project team, node operators, and users of a blockchain platform; and 3) existing governance solutions can be classified into process mechanisms and product mechanisms, which mainly focus on the operation phase over the blockchain platform layer.", "num_citations": "2\n", "authors": ["638"]}
{"title": "Towards Automated Data Sharing in Personal Data Stores\n", "abstract": " The data on the Web is increasingly being centralised towards a few service providers. Personal Data Stores (PDS) have emerged, proposing a fundamental shift from the current service-centric data ecosystem to a decentralised data storage and processing environment by placing the data with users. Users are to assume total self-sovereignty over their data, including opportunities to monetise. While PDS systems enable user empowerment, they also put a greater burden on the users, who may not be technically-savvy, to manage data access, which may increase the chance of unintended mishaps and privacy risks. This research proposes a privacy preference recommender system for privacy-preserving data sharing control that is designed to work with the constraints of user-centric data storage and processing environment for PDS. The outcome contributes towards a user-assisting privacy technology that\u00a0\u2026", "num_citations": "2\n", "authors": ["638"]}
{"title": "Communication-based approach for promoting energy consumer switching: some evidence from ofgem\u2019s database trials in the united kingdom\n", "abstract": " Prompted by rising concern about weak consumer switching and the practice of price discrimination, over the period of 2016\u20132019, the Office of Gas and Electricity Markets (Ofgem) undertook a series of trials on communication-based interventions to encourage consumer switching in the United Kingdom. The main purpose of this paper is to assess the experience of these Ofgem trials with a view to draw some lessons for policy makers. The analytical framework adopted for this purpose is informed by existing literature on the barriers for consumer switching. The results of the analysis suggest that while the Ofgem trials have made positive impacts on consumer switching, these impacts varied significantly across the trials, suggesting that some interventions were more effective than others. Further, the overall impacts of the Ofgem trials were moderate, as around 70% of participants did not switch suppliers even in the most impactful trial. This reflects a general lack of understanding in the literature about the behaviour-influencing factors, their impacts, and their context-connects. By implication, the difficulty in stimulating consumer switching, as demonstrated by the Ofgem trials, suggests that weak consumer switching and the practice of price discrimination may simply reflect significant competition, rather than a lack of it, especially if retail margins are not greater than the competitive level. In this case, the communication-based intervention aimed at encouraging consumer switching may lead to further price discrimination, especially for the most vulnerable consumers, who are more likely to stay with their incumbent suppliers. View Full-Text", "num_citations": "2\n", "authors": ["638"]}
{"title": "One size does not fit all: The case for chunking configuration in backup deduplication\n", "abstract": " Data backup is regularly required by both enterprise and individual users to protect their data from unexpected loss. There are also various commercial data deduplication systems or software that help users to eliminate duplicates in their backup data to save storage space. In data deduplication systems, the data chunking process splits data into small chunks. Duplicate data is identified by comparing the fingerprints of the chunks. The chunk size setting has significant impact on deduplication performance. A variety of chunking algorithms have been proposed in recent studies. In practice, existing systems often set the chunking configuration in an empirical manner. A chunk size of 4KB or 8KB is regarded as the sweet spot for good deduplication performance. However, the data storage and access patterns of users vary and change along time, as a result, the empirical chunk size setting may not lead to a good\u00a0\u2026", "num_citations": "2\n", "authors": ["638"]}
{"title": "Analyzing differences in risk perceptions between developers and acquirers in OTS-based custom software projects using stakeholder analysis\n", "abstract": " Project stakeholders can have different perceptions of risks and how they should be mitigated, but these differences are not always well understood and managed. This general issue occurs in Off-the-shelf (OTS)-based custom software development projects, which use and integrate OTS software in the development of specialized software for an individual customer. We report on a study of risk perceptions for developers and acquirers in OTS-based custom software development projects. The study used an online questionnaire-based survey. We compared stakeholders' perceptions about their level of control over and exposure to 11 shared risks in OTS-based software, in 35 OTS-based software developments and 34 OTS-based software acquisitions of Indonesian background. We found that both stakeholders can best control, and are most impacted by, risks about requirements negotiation. In general\u00a0\u2026", "num_citations": "2\n", "authors": ["638"]}
{"title": "Towards Concise Architectures for Flexible Business Processes\n", "abstract": " This chapter proposes a view of business processes as designed artefacts that are ontologically no different than artefacts in domains such as mechanical and software engineering. This view distinguishes three concerns for designing processes: architecture, implementation and adaptation. We show that current process modelling approaches conflate these aspects, often leading to high complexity and inflexibility of the resulting process models. We use a generalisation of the \u201cfeature\u201d concept in engineering design, represented using the functionbehaviour-structure (FBS) ontology, as the basis of a new approach to concisely specifying business process architectures that allow for more process flexibility.", "num_citations": "2\n", "authors": ["638"]}
{"title": "Rationale in Semi-Structured Processes\n", "abstract": " This paper argues that an explicit account of rationale is essential for the effective management and evolution of semi-structured processes. Our approach is based on a view of semi-structured process models as unfinished products whose design is implicitly completed through their execution by process model users. The resulting refinements and modifications of the process models are instances of user-driven design innovation. Our framework shows how rationale can explain a user\u2019s individual execution decisions, as a basis for process modelers to improve the original process specifications. We propose and illustrate the ontological foundations of a modeling approach.", "num_citations": "2\n", "authors": ["638"]}
{"title": "Systematic selection of quality attribute techniques\n", "abstract": " Various techniques are used to investigate, evaluate, and control product quality risks throughout software development process. These\" Quality Attribute Techniques\" are used during all stages of the software development life cycle to ensure that acceptable levels of product qualities such as safety and performance are in place. In this paper, we propose a method to select from among the alternatives of these techniques. This method is based on Risk Management theory and the Analytic Hierarchy Process (AHP) approach. We apply our method to an example of real-world safety system presented in the literature. We identify advantages and limitations of the method, and discuss future research.", "num_citations": "2\n", "authors": ["638"]}
{"title": "FLRA: A Reference Architecture for Federated Learning Systems\n", "abstract": " Federated learning is an emerging machine learning paradigm that enables multiple devices to train models locally and formulate a global model, without sharing the clients' local data. A federated learning system can be viewed as a large-scale distributed system, involving different components and stakeholders with diverse requirements and constraints. Hence, developing a federated learning system requires both software system design thinking and machine learning knowledge. Although much effort has been put into federated learning from the machine learning perspectives, our previous systematic literature review on the area shows that there is a distinct lack of considerations for software architecture design for federated learning. In this paper, we propose FLRA, a reference architecture for federated learning systems, which provides a template design for federated learning-based solutions. The proposed FLRA reference architecture is based on an extensive review of existing patterns of federated learning systems found in the literature and existing industrial implementation. The FLRA reference architecture consists of a pool of architectural patterns that could address the frequently recurring design problems in federated learning architectures. The FLRA reference architecture can serve as a design guideline to assist architects and developers with practical solutions for their problems, which can be further customised.", "num_citations": "1\n", "authors": ["638"]}
{"title": "Generative Adversarial Networks\u2013Enabled Human\u2013Artificial Intelligence Collaborative Applications for Creative and Design Industries: A Systematic Review of Current Approaches\u00a0\u2026\n", "abstract": " The future of work and the workplace is very much in flux. There has been a vast amount written on the topic of Artificial Intelligence (AI) and its impact on work, with much of it focused on automation and its impact in terms of job losses, etc. In this survey, we will address one area where AI is being added to the toolbox of creative and design practitioners to aid and enhance their creativity, productivity, and design horizons. A designer\u2019s primary purpose is to create, or generate, the most optimal artifact or prototype given a set of constraints. We have seen AI encroaching into this space with the advent generative networks, and generative adversarial networks (GANs) in particular. This area has become one of the most active fields of research in Machine Learning over the past number of years, and a number of these techniques, particularly those around plausible image generation, have garnered considerable media attention. In this review we look beyond automatic techniques and solutions and see how GANs are being incorporated into user pipelines for design practitioners. From 204 search results, 26 studies (including 2 snowball sampled) are reviewed, highlighting key trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples or implementations that are unlikely to scale. Areas for future study are also identified.", "num_citations": "1\n", "authors": ["638"]}
{"title": "Task-oriented api usage examples prompting powered by programming task knowledge graph\n", "abstract": " Programming tutorials are often created to demonstrate programming tasks with code examples. However, our study of Stack Overflow questions reveals the low utilization of high-quality programming tutorials, which is caused task description mismatch and code information overload. Document search can find relevant tutorial documents, but they often cannot find specific programming actions and code solutions relevant to the developers' task needs. The recently proposed activity-centric search over knowledge graph supports direct search of programming actions, but it has limitations in action coverage, natural language based task search, and coarse-grained code example recommendation. In this work, we enhance action coverage in knowledge graph with actions extracted from comments in code examples and more forms of activity sentences. To overcome the task description mismatch problem, we develop a code matching based task search method to find relevant programming actions and code examples to the code under development. We integrate our knowledge graph and task search method in the IDE, and develop an observe-push based tool to prompt developers with task-oriented API usage examples. To alleviate the code information overload problem, our tool highlights programming action and API information in the prompted tutorial task excerpts and code examples based on the underlying knowledge graph. Our evaluation confirms the high quality of the constructed knowledge graph, and show that our code matching based task search can recommend effective code solutions to programming issues asked on Stack\u00a0\u2026", "num_citations": "1\n", "authors": ["638"]}
{"title": "Accommodating information priority model in Cloudlet environment\n", "abstract": " Massive amounts of data during disaster situations require timely collection and analysis for the emergency team to mitigate the impact of the disaster under challenging social-technical conditions. The absence of Internet or its intermittent and bandwidth-constraint connection in disaster areas may exacerbate and disrupt the data collection process which may prevent some vital information to reach the control room in time for immediate response. Regardless the rare connection in the disaster area, there is a need to group information acquired during the response into a specific information model which accommodates different information priority levels. This is to establish a proper mechanism in transmitting higher prioritized information to the control room before other information. The purpose of this paper is to propose an information priority model and system architectures for data collection under\u00a0\u2026", "num_citations": "1\n", "authors": ["638"]}
{"title": "Integrating a market-based model in trust-based service systems\n", "abstract": " The reputation-based trust mechanism is a way to assess the trustworthiness of offered services, based on the feedback obtained from their users. In the absence of appropriate safeguards, service users can still manipulate this feedback. Auction mechanisms have already addressed the problem of manipulation by markettrading participants. When auction mechanisms are applied to trust systems, their interaction with the trust systems and associated overhead need to be quantitatively evaluated. This paper proposes two distributed architectures based on centralized and hybrid computing for integrating an auction mechanism with the trust systems. The empirical evaluation demonstrates how the architectures help to discourage users from giving untruthful feedback and reduce the overhead costs of the auction mechanisms.", "num_citations": "1\n", "authors": ["638"]}
{"title": "Automating web service development using a unified model\n", "abstract": " Web service standards are being developed in a loosely coordinated and constantly evolving manner and there is a lack of Web service modeling approaches that efficiently reflect the status of the standardization. Consequently the development and deployment of Web services tend to be ad-hoc and platform-oriented. This introduces potential interoperability issues and maintenance overhead. This paper proposes a model-driven framework that includes a Web service modeling language describing functionality and non-functional properties of service-oriented applications in unified models. This Web service modeling language is based on a Web service meta-model extracted directly from the WS-* standards. We developed a corresponding tool that generates code stubs, configurations and deployment heuristics, along with standard-based artifacts from models. We conducted a real-world case study to validate\u00a0\u2026", "num_citations": "1\n", "authors": ["638"]}
{"title": "On Combining WS-Policy4MASC and ASF to Support Business-Driven Autonomic Service-Oriented Computing\n", "abstract": " WS-Policy4MASC is an XML language for specification of policies for run-time Web service management. Among its original contributions is specification of diverse business values (e.g., profit, customer satisfaction) and control strategies maximizing different business values. While it was originally developed for the MASC (Manageable and Adaptable Service Compositions) middleware, it can also be used in a broader context of autonomic service-oriented computing. We discuss our research on using WS-Policy4MASC in the Adaptive Server Framework (ASF), which supports composing adaptive applications on Java and .NET platforms. WS-Policy4MASC enriches ASF with policy definition semantics and enables complex policy management. The combination of WS-Policy4MASC and ASF forms a step towards business-driven autonomic management of service-oriented systems.", "num_citations": "1\n", "authors": ["638"]}