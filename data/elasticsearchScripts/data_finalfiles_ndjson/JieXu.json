{"title": "The Evolution of the Recovery Block Concept\n", "abstract": " This chapter reviews the development of the recovery block approach to software fault tolerance and subsequent work based on this approach. It starts with an account of the development and implementations of the basic recovery block scheme in the early 1970s at Newcastle, and then goes on to describe work at Newcastle and elsewhere on extensions to the basic scheme, recovery in concurrent systems, and linguistic support for recovery blocks based on the use of object-oriented programming concepts.", "num_citations": "726\n", "authors": ["510"]}
{"title": "Fog orchestration for internet of things services\n", "abstract": " Large-scale Internet of Things (IoT) services such as healthcare, smart cities, and marine monitoring are pervasive in cyber-physical environments strongly supported by Internet technologies and fog computing. Complex IoT services are increasingly composed of sensors, devices, and compute resources within fog computing infrastructures. The orchestration of such applications can be leveraged to alleviate the difficulties of maintenance and enhance data security and system reliability. However, efficiently dealing with dynamic variations and transient operational behavior is a crucial challenge within the context of choreographing complex services. Furthermore, with the rapid increase of the scale of IoT deployments, the heterogeneity, dynamicity, and uncertainty within fog environments and increased computational complexity further aggravate this challenge. This article gives an overview of the core issues\u00a0\u2026", "num_citations": "273\n", "authors": ["510"]}
{"title": "Survey of target tracking protocols using wireless sensor network\n", "abstract": " Target tracking is one of the non trivial applications of wireless sensor network which is set up in the areas of field surveillance, habitat monitoring, indoor buildings, and intruder tracking. Various approaches have been investigated for tracking the targets, considering diverse metrics like scalability, overheads, energy consumption and target tracking accuracy. This paper for the first time contributes a survey of target tracking protocols for sensor networks and presents their classification in a precise manner. The five main categories explored in this paper are, hierarchical, tree-based, prediction- based, mobicast message-based tracking and hybrid methods. To be more precise, the survey promotes overview of recent research literature along with their performance comparison and evaluation based on simulation with real data. Certainly this task is challenging and not straight forward due to differences in estimations\u00a0\u2026", "num_citations": "179\n", "authors": ["510"]}
{"title": "Analysis, modeling and simulation of workload patterns in a large-scale utility cloud\n", "abstract": " Understanding the characteristics and patterns of workloads within a Cloud computing environment is critical in order to improve resource management and operational conditions while Quality of Service (QoS) guarantees are maintained. Simulation models based on realistic parameters are also urgently needed for investigating the impact of these workload characteristics on new system designs and operation policies. Unfortunately there is a lack of analyses to support the development of workload models that capture the inherent diversity of users and tasks, largely due to the limited availability of Cloud tracelogs as well as the complexity in analyzing such systems. In this paper we present a comprehensive analysis of the workload characteristics derived from a production Cloud data center that features over 900 users submitting approximately 25 million tasks over a time period of a month. Our analysis focuses\u00a0\u2026", "num_citations": "152\n", "authors": ["510"]}
{"title": "Fuxi: a fault-tolerant resource management and job scheduling system at internet scale\n", "abstract": " Scalability and fault-tolerance are two fundamental challenges for all distributed computing at Internet scale. Despite many recent advances from both academia and industry, these two problems are still far from settled. In this paper, we present Fuxi, a resource management and job scheduling system that is capable of handling the kind of workload at Alibaba where hundreds of terabytes of data are generated and analyzed everyday to help optimize the company's business operations and user experiences. We employ several novel techniques to enable Fuxi to perform efficient scheduling of hundreds of thousands of concurrent tasks over large clusters with thousands of nodes: 1) an incremental resource management protocol that supports multi-dimensional resource allocation and data locality; 2) user-transparent failure recovery where failures of any Fuxi components will not impact the execution of user jobs; and 3) an effective detection mechanism and a multi-level blacklisting scheme that prevents them from affecting job execution. Our evaluation results demonstrate that 95% and 91% scheduled CPU/memory utilization can be fulfilled under synthetic workloads, and Fuxi is capable of achieving 2.36T-B/minute throughput in GraySort. Additionally, the same Fuxi job only experiences approximately 16% slowdown under a 5% fault-injection rate. The slowdown only grows to 20% when we double the fault-injection rate to 10%. Fuxi has been deployed in our production environment since 2009, and it now manages hundreds of thousands of server nodes.", "num_citations": "133\n", "authors": ["510"]}
{"title": "An approach for characterizing workloads in google cloud to derive realistic resource utilization models\n", "abstract": " Analyzing behavioral patterns of workloads is critical to understanding Cloud computing environments. However, until now only a limited number of real-world Cloud data center trace logs have been available for analysis. This has led to a lack of methodologies to capture the diversity of patterns that exist in such datasets. This paper presents the first large-scale analysis of real-world Cloud data, using a recently released dataset that features traces from over 12,000 servers over the period of a month. Based on this analysis, we develop a novel approach for characterizing workloads that for the first time considers Cloud workload in the context of both user and task in order to derive a model to capture resource estimation and utilization patterns. The derived model assists in understanding the relationship between users and tasks within workload, and enables further work such as resource optimization, energy\u00a0\u2026", "num_citations": "129\n", "authors": ["510"]}
{"title": "Dynamic data integration using web services\n", "abstract": " We address the problem of large-scale data integration, where the data sources are unknown at design time, are from autonomous organisations, and may evolve. Experiments are described involving a demonstrator system in the field of health services data integration within the UK. Current Web services technology has been used extensively and largely successfully in these distributed prototype systems. The work shows that Web services provide a good infrastructure layer, but integration demands a higher level \"broker\" architectural layer; the paper identifies eight specific requirements for such an architecture that have emerged from the experiments, derived from an analysis of shortcomings which are collectively due to the static nature of the initial prototype. The way in which these are being met in the current version in order to achieve a more dynamic integration is described.", "num_citations": "119\n", "authors": ["510"]}
{"title": "A novel intrusion severity analysis approach for Clouds\n", "abstract": " Cloud computing presents exciting opportunities to foster research for scientific communities; virtual machine technology has a profound role in this. Among other benefits, virtual machine technology enables Clouds to offer large scale and flexible computing infrastructures that are available on demand to address the diverse requirements of scientific research. However, Clouds introduce novel security challenges which need to be addressed to facilitate widespread adoption. This paper is focused on one such challenge\u2014intrusion severity analysis. In particular, we highlight the significance of intrusion severity analysis for the overall security of Clouds. Additionally, we present a novel method to address this challenge in accordance with the specific requirements of Clouds for intrusion severity analysis. We also present rigorous evaluation to assess the effectiveness and feasibility of the proposed method to address\u00a0\u2026", "num_citations": "100\n", "authors": ["510"]}
{"title": "Internet-based virtual computing environment: Beyond the data center as a computer\n", "abstract": " The two dominating characteristics of new and emerging Internet applications are ultra-large scales and utility. Centralized data centers alone are often inadequate for running such applications. In this paper we introduce the concept of an Internet-based Virtual Computing Environment (iVCE), which aims to provide Cloud services by a dynamic combination of data centers and other multi-scale computing resources on the Internet. We present a model that addresses two critical challenges in iVCE: multi-scale resource aggregation and elastic binding. We then describe the design and implementation of our iVCE software platform that embodies the model. Comprehensive experiments show that iVCE provides a novel, promising way to deal with scalability and utility, thereby enabling economical and elastic Cloud Computing.", "num_citations": "98\n", "authors": ["510"]}
{"title": "Multi-tenancy in cloud computing\n", "abstract": " As Cloud Computing becomes the trend of information technology computational model, the Cloud security is becoming a major issue in adopting the Cloud where security is considered one of the most critical concerns for the large customers of Cloud (i.e. governments and enterprises). Such valid concern is mainly driven by the Multi-Tenancy situation which refers to resource sharing in Cloud Computing and its associated risks where confidentiality and/or integrity could be violated. As a result, security concerns may harness the advancement of Cloud Computing in the market. So, in order to propose effective security solutions and strategies a good knowledge of the current Cloud implementations and practices, especially the public Clouds, must be understood by professionals. Such understanding is needed in order to recognize attack vectors and attack surfaces. In this paper we will propose an attack model\u00a0\u2026", "num_citations": "93\n", "authors": ["510"]}
{"title": "Early career researchers and their publishing and authorship practices\n", "abstract": " This study presents findings from the first year of the Harbingers research project, a 3\u2010year longitudinal study of early career researchers (ECRs), which sought to ascertain current and changing habits in scholarly communication. The study recruited 116 science and social science ECRs from seven countries who were subject to in\u2010depth interviews, and this paper reports on findings regarding publishing and authorship practices and attitudes. A major objective was to determine whether ECRs are taking the myriad opportunities proffered by new digital innovations, developing within the context of open science, open access, and social media, to publish their research. The main finding is that these opportunities are generally not taken because ECRs are constrained by convention and the precarious employment environment they inhabit and know what is best for them, which is to publish (in high impact factor\u00a0\u2026", "num_citations": "91\n", "authors": ["510"]}
{"title": "Where and how early career researchers find scholarly information\n", "abstract": " This article presents findings from the first year of the Harbingers research project started in 2015. The project is a 3\u2010year longitudinal study of early career researchers (ECRs) to ascertain their current and changing habits with regard to information searching, use, sharing, and publication. The study recruited 116 researchers from seven countries (UK, USA, China, France, Malaysia, Poland, and Spain) and performed in\u2010depth interviews by telephone, Skype, or face\u2010to\u2010face to discover behaviours and opinions. This paper reports on findings regarding discovery and access to scholarly information. Findings confirm the universal popularity of Google/Google Scholar. Library platforms and web\u2010scale discovery services are largely unmentioned and unnoticed by this user community, although many ECRs pass through them unknowingly on the way to authenticated use of their other preferred sources, such as Web of\u00a0\u2026", "num_citations": "91\n", "authors": ["510"]}
{"title": "Improved energy-efficiency in cloud datacenters with interference-aware virtual machine placement\n", "abstract": " Virtualization is one of the main technologies used for improving resource efficiency in datacenters; it allows the deployment of co-existing computing environments over the same hardware infrastructure. However, the co-existing of environments \u2014 along with management inefficiencies \u2014 often creates scenarios of high-competition for resources between running workloads, leading to performance degradation. This phenomenon is known as Performance Interference, and introduces a non-negligible overhead that affects both a datacenter's Quality of Service and its energy-efficiency. This paper introduces a novel approach to workload allocation that improves energy-efficiency in Cloud datacenters by taking into account their workload heterogeneity. We analyze the impact of performance interference on energy-efficiency using workload characteristics identified from a real Cloud environment, and develop a model\u00a0\u2026", "num_citations": "86\n", "authors": ["510"]}
{"title": "Assessing the dependability of SOAP RPC-based web services by fault injection\n", "abstract": " This paper presents our research on devising a dependability assessment method for SOAP-based Web Services using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing SOAP RPC-based applications and derive a new method and fault model for testing web services. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy within our system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard web service to the stateful environment of an OGSA service.", "num_citations": "85\n", "authors": ["510"]}
{"title": "Increasing web service dependability through consensus voting\n", "abstract": " This paper demonstrates our Web Service based N-Version model, WS-FTM (Web Service-Fault Tolerance Mechanism), which applies this well proven technique to the domain of Web Services to increase system dependability. WS-FTM achieves transparent usage of replicated Web Services by use of a modified stub. The stub is created using tools included in WS-FTM. Our initial implementation includes a simple consensus voter that allows generic result comparison. Finally we show, through the use of a non-trivial example, that WS-FTM can be used to increase the reliability of a Web Service system.", "num_citations": "84\n", "authors": ["510"]}
{"title": "An empirical failure-analysis of a large-scale cloud computing environment\n", "abstract": " Cloud computing research is in great need of statistical parameters derived from the analysis of real-world systems. One aspect of this is the failure characteristics of Cloud environments composed of workloads and servers, currently, few metrics are available that quantify failure and repair times of workloads and servers at a large-scale. Workload metrics in particular are critical for characterizing and modeling accurate workload behavior, enabling more realistic workload simulation and failure scenarios of systems. This paper presents the analysis of failure data of a large-scale production Cloud environment (consisting of over 12,500 servers), and includes a study of failure and repair times and characteristics for both Cloud workloads and servers. Our results show that failure characteristics for workload and servers are highly variable and that production Cloud workloads can be accurately modeled by a Gamma\u00a0\u2026", "num_citations": "83\n", "authors": ["510"]}
{"title": "Fault tolerance within a grid environment\n", "abstract": " Fault tolerance is an important property in Grid computing as the dependability of individual Grid resources may not be able to be guaranteed; also as resources are used outside of organizational boundaries, it becomes increasingly difficult to guarantee that a resource being used is not malicious in some way. As part of the e-Demand project at the University of Durham we are seeking to develop both an improved fault model for Grid computing and a method for providing fault tolerance for Grid applications that will provide protection against both malicious and erroneous services.We have firstly begun to investigate whether the traditional distributed systems fault model can be readily applied to Grid computing, or whether improvements and alterations need to be made. From our initial investigation, we have concluded that timing, omission and interaction faults may become more prevalent in Grid applications than is the case in traditional distributed systems. From this initial fault model, we have begun to develop an approach for fault tolerance based on the idea of job replication, as anomalous results (either maliciously altered or simply wrong) should be caught at the voting stage. This approach combines a replication-based fault tolerance approach with both dynamic prioritization and dynamic scheduling.", "num_citations": "82\n", "authors": ["510"]}
{"title": "Software sustainability: The modern tower of babel\n", "abstract": " The development of sustainable software has been identified as one of the key challenges in the field of computational science and engineering. However, there is currently no agreed definition of the concept. Current definitions range from a composite, non-functional requirement to simply an emergent property. This lack of clarity leads to confusion, and potentially to ineffective and inefficient efforts to develop sustainable software systems. The aim of this paper is to explore the emerging definitions of software sustainability from the field of software engineering in order to contribute to the question, what is software sustainability? The preliminary analysis suggests that the concept of software sustainability is complex and multifaceted with any consensus towards a shared definition within the field of software engineering yet to be achieved.", "num_citations": "81\n", "authors": ["510"]}
{"title": "Holistic virtual machine scheduling in cloud datacenters towards minimizing total energy\n", "abstract": " Energy consumed by Cloud datacenters has dramatically increased, driven by rapid uptake of applications and services globally provisioned through virtualization. By applying energy-aware virtual machine scheduling, Cloud providers are able to achieve enhanced energy efficiency and reduced operation cost. Energy consumption of datacenters consists of computing energy and cooling energy. However, due to the complexity of energy and thermal modeling of realistic Cloud datacenter operation, traditional approaches are unable to provide a comprehensive in-depth solution for virtual machine scheduling which encompasses both computing and cooling energy. This paper addresses this challenge by presenting an elaborate thermal model that analyzes the temperature distribution of airflow and server CPU. We propose GRANITE - a holistic virtual machine scheduling algorithm capable of minimizing total\u00a0\u2026", "num_citations": "80\n", "authors": ["510"]}
{"title": "An empirical validation of object-oriented design metrics for fault prediction\n", "abstract": " Object-oriented design has become a dominant method in software industry and many design metrics of object-oriented programs have been proposed for quality prediction, but there is no well-accepted statement on how significant those metrics are. In this study, empirical analysis is carried out to validate object-oriented design metrics for defects estimation. Approach: The Chidamber and Kemerer metrics suite is adopted to estimate the number of defects in the programs, which are extracted from a public NASA data set. The techniques involved are statistical analysis and neuro-fuzzy approach. Results: The results indicate that SLOC, WMC, CBO and RFC are reliable metrics for defect estimation. Overall, SLOC imposes most significant impact on the number of defects. Conclusions/Recommendations: The design metrics are closely related to the number of defects in OO classes, but we can not jump to a conclusion by using one analysis technique. We recommend using neuro-fuzzy approach together with statistical techniques to reveal the relationship between metrics and dependent variables, and the correlations among those metrics also have to be considered.", "num_citations": "77\n", "authors": ["510"]}
{"title": "Ws-fit: A tool for dependability analysis of web services\n", "abstract": " This work provides an overview of fault injection techniques and their applicability to testing SOAP RPC based Web service systems. We also give a detailed example of the WS-FIT package and use it to detect a problem in a Web service based system.", "num_citations": "77\n", "authors": ["510"]}
{"title": "Straggler root-cause and impact analysis for massive-scale virtualized cloud datacenters\n", "abstract": " Increased complexity and scale of virtualized distributed systems has resulted in the manifestation of emergent phenomena substantially affecting overall system performance. This phenomena is known as \u201cLong Tail\u201d, whereby a small proportion of task stragglers significantly impede job completion time. While work focuses on straggler detection and mitigation, there is limited work that empirically studies straggler root-cause and quantifies its impact upon system operation. Such analysis is critical to ascertain in-depth knowledge of straggler occurrence for focusing developmental and research efforts towards solving the Long Tail challenge. This paper provides an empirical analysis of straggler root-cause within virtualized Cloud datacenters; we analyze two large-scale production systems to quantify the frequency and impact stragglers impose, and propose a method for conducting root-cause analysis. Results\u00a0\u2026", "num_citations": "71\n", "authors": ["510"]}
{"title": "Coordinated atomic actions: from concept to implementation\n", "abstract": " The Co-ordinated Atomic Action (or CA action) concept is a unified scheme for co-ordinating complex concurrent activities and supporting error recovery between multiple interacting objects in a distributed object-oriented system. It provides a conceptual framework for dealing with different kinds of concurrency and achieving fault tolerance by extending and integrating two complementary concepts-conversations and transactions. Conversations (enhanced with concurrent exception handling) are used to control co-operative concurrency and to implement co-ordinated error recovery whilst transactions are used to maintain the consistency of shared resources in the presence of failures and competitive concurrency. This paper explains the CA action concept in detail and then addresses related design issues such as multi-thread co-ordination, exception handling and resolution, co-ordinated access to shared objects and provision of software fault tolerance. Finally, brief details are given of a number of experimental prototype implementations and case studies.", "num_citations": "67\n", "authors": ["510"]}
{"title": "Mobile agent fault tolerance for information retrieval applications: An exception handling approach\n", "abstract": " Maintaining mobile agent availability in the presence of agent server crashes is a challenging issue since developers normally have no control over remote agent servers. A popular technique is that a mobile agent injects a replica into stable storage upon its arrival at each agent server. However, a server crash leaves the replica unavailable, for an unknown time period, until the agent server is back online. This paper uses exception handling to maintain the availability, of mobile agents in the presence of agent server crash failures. Two exception handler designs are proposed. The first exists at the agent server that created the mobile agent. The second operates at the previous agent server visited by the mobile agent. Initial performance results demonstrate that although the second design is slower it offers the smaller trip time increase in the presence of agent server crashes.", "num_citations": "66\n", "authors": ["510"]}
{"title": "An analysis of failure-related energy waste in a large-scale cloud environment\n", "abstract": " Cloud computing providers are under great pressure to reduce operational costs through improved energy utilization while provisioning dependable service to customers; it is therefore extremely important to understand and quantify the explicit impact of failures within a system in terms of energy costs. This paper presents the first comprehensive analysis of the impact of failures on energy consumption in a real-world large-scale cloud system (comprising over 12 500 servers), including the study of failure and energy trends of the spatial and temporal environmental characteristics. Our results show that 88% of task failure events occur in lower priority tasks producing 13% of total energy waste, and 1% of failure events occur in higher priority tasks due to server failures producing 8% of total energy waste. These results highlight an unintuitive but significant impact on energy consumption due to failures, providing a\u00a0\u2026", "num_citations": "65\n", "authors": ["510"]}
{"title": "Dynamic data deduplication in cloud storage\n", "abstract": " Cloud computing plays a major role in the business domain today as computing resources are delivered as a utility on demand to customers over the Internet. Cloud storage is one of the services provided in cloud computing which has been increasing in popularity. The main advantage of using cloud storage from the customers' point of view is that customers can reduce their expenditure in purchasing and maintaining storage infrastructure while only paying for the amount of storage requested, which can be scaled-up and down upon demand. With the growing data size of cloud computing, a reduction in data volumes could help providers reducing the costs of running large storage system and saving energy consumption. So data deduplication techniques have been brought to improve storage efficiency in cloud storages. With the dynamic nature of data in cloud storage, data usage in cloud changes overtime\u00a0\u2026", "num_citations": "61\n", "authors": ["510"]}
{"title": "An analysis of the server characteristics and resource utilization in google cloud\n", "abstract": " Understanding the resource utilization and server characteristics of large-scale systems is crucial if service providers are to optimize their operations whilst maintaining Quality of Service. For large-scale data enters, identifying the characteristics of resource demand and the current availability of such resources, allows system managers to design and deploy mechanisms to improve data enter utilization and meet Service Level Agreements with their customers, as well as facilitating business expansion. In this paper, we present a large-scale analysis of server resource utilization and a characterization of a production Cloud data enter using the most recent data enter trace logs made available by Google. We present their statistical properties, and a comprehensive coarse-grain analysis of the data, including submission rates, server classification, and server resource utilization. Additionally, we perform a fine-grained\u00a0\u2026", "num_citations": "59\n", "authors": ["510"]}
{"title": "The blind men and the elephant: Towards an empirical evaluation framework for software sustainability\n", "abstract": " Software sustainability has been identified as one of the key challenges in the development of scientific and engineering software as we move towards new paradigms of research and computing infrastructures. However, it is suggested that sustainability is not well understood within the software engineering community, which can led to ineffective and inefficient efforts to address the concept or result in its complete omission from the software system. This paper proposes a definition of software sustainability and considers how it can be measured empirically in the design and engineering process of software systems.", "num_citations": "57\n", "authors": ["510"]}
{"title": "Service-oriented reference architecture for smart cities\n", "abstract": " The trend towards turning existing cities into smart cities is growing. Facilitated by advances in computing such as Cloud services and Internet of Things (IoT), smart cities propose to bring integrated, autonomous systems together to improve quality of life for their inhabitants. Systems such as autonomous vehicles, smart grids and intelligent traffic management are in the initial stages of development. However, as of yet there, is no holistic architecture on which to integrate these systems into a smart city. Additionally, the existing systems and infrastructure of cities is extensive and critical to their operation. We cannot simply replace these systems with smarter versions, instead the system intelligence must augment the existing systems. In this paper we propose a service oriented reference architecture for smart cities which can tackle these problems and identify some related open research questions. The abstract\u00a0\u2026", "num_citations": "56\n", "authors": ["510"]}
{"title": "Simulating errors in web services\n", "abstract": " This paper details our research into creating a method and tools to perform dependability analysis of Web Services. Our method is based upon a modified version of Network Level Fault Injection and extends this technique by automatically decoding network messages, based on SOAP, and thus allowing meaningful faults to be injected. This paper also outlines our method for automating the generation of test scripts from our fault model. Our method and tools can be applied to a system to assess a wide range of Quality of Service metrics and in this paper we present a test case to show how this can be applied to a Quality of Service scenario.", "num_citations": "55\n", "authors": ["510"]}
{"title": "Toward an object-oriented approach to software fault tolerance\n", "abstract": " Software fault tolerance is often necessary, but can itself be dangerously error-prone because of the additional effort that must be involved in the programming process. The additional redundancy may increase size and complexity and thus adversely affect software reliability. Object-oriented programming provides an appropriate framework for controlling complexity and enforcing reliability. However, software fault tolerance cannot be achieved merely by implementing the classical fault-tolerance schemes in an object-oriented fashion. New problems arise while integrating software redundancy into object-oriented computing systems. This paper identifies a set of such problems, addresses possible solutions, and proposes an object-oriented architecture for dealing with software design faults. Both linguistic supports for the architecture and implementation issues are discussed in detail.", "num_citations": "54\n", "authors": ["510"]}
{"title": "Investigation of IT security and compliance challenges in security-as-a-service for cloud computing\n", "abstract": " The key security challenges and solutions on the cloud have been investigated in this paper with the help of literature reviews and an experimental model created on OPNET that is simulated to produce useful statistics to establish the approach that the cloud computing service providers should take to provide optimal security and compliance. The literatures recommend the concept of Security-as-a-Service using unified threat management (UTM) for ensuring secured services on the cloud. Through the simulation results, this paper has demonstrated that UTM may not be a feasible approach to security implementation as it may become a bottleneck for the application clouds. The fundamental benefits of cloud computing (resources on demand and high elasticity) may be diluted if UTMs do not scale up effectively as per the traffic loads on the application clouds. Moreover, it is not feasible for application clouds to\u00a0\u2026", "num_citations": "53\n", "authors": ["510"]}
{"title": "Adaptive fog configuration for the industrial internet of things\n", "abstract": " Industrial fog computing deploys various industrial services, such as automatic monitoring/control and imminent failure detection, at the fog nodes (FNs) to improve the performance of industrial systems. Much effort has been made in the literature on the design of fog network architecture and computation offloading. This paper studies an equally important but much less investigated problem of service hosting where FNs are adaptively configured to host services for sensor nodes (SNs), thereby enabling corresponding tasks to be executed by the FNs. The problem of service hosting emerges because of the limited computational and storage resources at FNs, which limit the number of different types of services that can be hosted by an FN at the same time. Considering the variability of service demand in both temporal and spatial dimensions, when, where, and which services to host have to be judiciously decided to\u00a0\u2026", "num_citations": "50\n", "authors": ["510"]}
{"title": "Throughput maximization for laser-powered UAV wireless communication systems\n", "abstract": " Laser power has become a viable solution to provide convenient and sustainable energy supply to unmanned aerial vehicles (UAVs). In this paper, we study a laser-powered UAV wireless communication system, where a laser transmitter sends laser beams to charge a fixed-wing UAV in flight, and the UAV uses the harvested laser energy to communicate with a ground station. To maintain the UAV's sustainable operation, its total energy consumption cannot exceed that harvested from the laser transmitter. Under such a laser energy harvesting constraint, we maximize the downlink communication throughput from the UAV to the ground station over a finite time duration, by jointly optimizing the UAV's trajectory and its transmit power allocation. However, due to the complicated UAV energy consumption model, this problem is non-convex and difficult to be solved. To tackle the problem, we first consider a special case\u00a0\u2026", "num_citations": "48\n", "authors": ["510"]}
{"title": "Service-oriented integration of systems for military capability\n", "abstract": " Service oriented architecture (SOA) is becoming established in computing as a means to integrate processing and data across organisations. This paper proposes that system-level integration can benefit from service oriented architectural descriptions and loose coupling between the problem domain requirements and different system solutions. The problem domain is exemplified as military capability, from the UK Ministry of Defence (MoD), in particular, network enabled capability (NEC). Representations of military capability in the problem domain can be described in terms of processes. The processes are sequences of functions that can be described as services. Then different types of system solutions can implement the described services. Firstly, the paper presents an overview of conceptual SOA and in the context of military capability compares three levels of service integration: business services, systems\u00a0\u2026", "num_citations": "48\n", "authors": ["510"]}
{"title": "Clustering and fault tolerance for target tracking using wireless sensor networks\n", "abstract": " Using wireless sensor networks to track a moving object provided a practical solution to a wide variety of applications including, for example, wild life, military operations, intruder tracking and monitoring in indoor office buildings. While much work has been done in this area, failures are not considered in most of the existing solutions. However, failures have to be handled carefully in target tracking applications because of their unpredictable and dynamic nature of communication, such as sensor energy depletion, severe environment conditions, unstable communication links and malicious attacks. Traditional approaches of fault tolerance are not well suited to address these new challenges. Therefore the authors propose a novel fault-tolerant target tracking (FTTT) protocol based on clustering. Also, the results of an investigation in terms of performance overheads and scalable nature of the FTTT protocol via\u00a0\u2026", "num_citations": "46\n", "authors": ["510"]}
{"title": "A provenance-aware weighted fault tolerance scheme for service-based applications\n", "abstract": " Service-orientation has been proposed as a way of facilitating the development and integration of increasingly complex and heterogeneous system components. However, there are many new challenges to the dependability community in this new paradigm, such as how individual channels within fault-tolerant systems may invoke common services as part of their workflow, thus increasing the potential for common-mode failure. We propose a scheme that - for the first time - links the technique of provenance with that of multi-version fault tolerance. We implement a large test system and perform experiments with a single-version system, a traditional MVD system, and a provenance-aware MVD system, and compare their results. We show that for this experiment, our provenance-aware scheme results in a much more dependable system than either of the other systems tested, whilst imposing a negligible timing overhead.", "num_citations": "45\n", "authors": ["510"]}
{"title": "Safety distance about car-following\n", "abstract": " It is pointed out that the traffic accident mostly result in improper driver's action. The driver's capability of response time and speed estimation are studied, and driving performance by car following in traffic is investigated, in order to calculate the appropriate driving safety distance, which assures that neither the rear end collision occurs nor road capacity is affected. If the following distance is S (m), and the following vehicle's velocity is V (m/s), the study results show that when SV/2, the following is safety under the good condition of driver and vehicle, when S2V, the following is safety under the poor operation of driver and vehicle.", "num_citations": "43\n", "authors": ["510"]}
{"title": "A cost-effective and flexible scheme for software fault tolerance\n", "abstract": " A new software fault tolerance scheme, called the Self-Configuring Optimistic Programming scheme,(SCOP), is proposed. It attempts to reduce the cost of fault tolerant software and to eliminate some inflexibilities and rigidities present in the existing software fault tolerance schemes. For obtaining these goals, it is structured in phases in order to produce acceptable results with the minimum possible effort and to release these results as soon as available, and it can be parameterized with respect to both the desired reliability and the desired response time. SCOP allows a trade-off between various attributes of system services (such as reliability, throughput and response time) as desired by designers and it is thus a flexible and cost-effective redundant component for gracefully degradable systems.", "num_citations": "43\n", "authors": ["510"]}
{"title": "Zest: a hybrid model on predicting passenger demand for chauffeured car service\n", "abstract": " Chauffeured car service based on mobile applications like Uber or Didi suffers from supply-demand disequilibrium, which can be alleviated by proper prediction on the distribution of passenger demand. In this paper, we propose a Zero-Grid Ensemble Spatio Temporal model (ZEST) to predict passenger demand with four predictors: a temporal predictor and a spatial predictor to model the influences of local and spatial factors separately, an ensemble predictor to combine the results of former two predictors comprehensively and a Zero-Grid predictor to predict zero demand areas specifically since any cruising within these areas costs extra waste on energy and time of driver. We demonstrate the performance of ZEST on actual operational data from ride-hailing applications with more than 6 million order records and 500 million GPS points. Experimental results indicate our model outperforms 5 other baseline models\u00a0\u2026", "num_citations": "42\n", "authors": ["510"]}
{"title": "Customer-aware resource overallocation to improve energy efficiency in realtime cloud computing data centers\n", "abstract": " Energy efficiency is becoming a very important concern for Cloud Computing environments. These are normally composed of large and power consuming data centers to provide the required elasticity and scalability to their customers. In this context, many efforts have been developed to balance the loads at host level. However, determining how to maximize the resources utilization at Virtual Machine (VM) level still remains as a big challenge. This is mainly driven by very dynamic workload behaviors and a wide variety of customers' resource utilization patterns. This paper introduces a dynamic resource provisioning mechanism to overallocate the capacity of real-time Cloud data centers based on customer utilization patterns. Furthermore, its impact on the trade-off between energy efficiency and SLA fulfillment is analyzed. The main idea is to exploit the resource utilization patterns of each customer to decrease the\u00a0\u2026", "num_citations": "42\n", "authors": ["510"]}
{"title": "Assessing the dependability of OGSA middleware by fault injection\n", "abstract": " This paper presents our research on devising a dependability assessment method for the upcoming OGSA 3.0 middleware using network level fault injection. We compare existing DCE middleware dependability testing research with the requirements of testing OGSA middleware and derive a new method and fault model. From this we have implemented an extendable fault injector framework and undertaken some proof of concept experiments with a simulated OGSA middleware system based around Apache SOAP and Apache Tomcat. We also present results from our initial experiments, which uncovered a discrepancy with our simulated OGSA system. We finally detail future research, including plans to adapt this fault injector framework from the stateless environment of a standard Web service to the stateful environment of an OGSA service.", "num_citations": "42\n", "authors": ["510"]}
{"title": "Workload estimation for improving resource management decisions in the cloud\n", "abstract": " In cloud computing, good resource management can benefit both cloud users as well as cloud providers. Workload prediction is a crucial step towards achieving good resource management. While it is possible to estimate the workloads of long-running tasks based on the periodicity in their historical workloads, it is difficult to do so for tasks which do not have such recurring workload patterns. In this paper, we present an innovative clustering based resource estimation approach which groups tasks that have similar characteristics into the same cluster. The historical workload data for tasks in a cluster are used to estimate the resources needed by new tasks based on the cluster(s) to which they belong. In particular, for a new task T, we measure T's initial workload and predict to which cluster(s) it may belong. Then, the workload information of the cluster(s) is used to estimate the workload of T. The approach is\u00a0\u2026", "num_citations": "41\n", "authors": ["510"]}
{"title": "Energy-efficiency in cloud computing environments: Towards energy savings without performance degradation\n", "abstract": " Due to all the pollutants generated during its production and the steady increases in its rates, energy consumption is causing serious environmental and economic problems. In this context, the growing use and adoption of ICTs is being highlighted not only as one as the principal problem sources but also as one of the principal areas that could help in the problem\u2019s reduction. Cloud computing is an emerging model for distributed utility computing and is being considered as an attractive opportunity for saving energy through central management of computational resources. To be successful, the design of energy-efficient mechanisms must start playing a mayor role. This paper argues the importance of energy-efficient mechanisms within cloud data centers and remarks on the significance of the \u201cenergy-performance\u201d relationship in boosting the adoption of these mechanisms in real scenarios. It provides an analysis\u00a0\u2026", "num_citations": "40\n", "authors": ["510"]}
{"title": "Roll-forward error recovery in embedded real-time systems\n", "abstract": " Roll-forward checkpointing schemes are developed in order to avoid rollback in the presence of independent faults and to increase the possibility that a task completes within a tight deadline. However, despite of the adoption of roll-forward recovery, these schemes are not necessarily appropriate for time-critical applications because interactions with the external environment and communications between processes must be deferred during checkpoint validation steps (typically, two checkpoint intervals) until the fault-free processors are identified. The deadlines on providing services may thus be violated. In this paper we present and discuss two alternative roll-forward recovery schemes, especially for time-critical and interaction-intensive applications, that deliver correct, timely results even when checkpoint validation is required.", "num_citations": "40\n", "authors": ["510"]}
{"title": "Survey of advances and challenges in intelligent autonomy for distributed cyber\u2010physical systems\n", "abstract": " With the evolution of the Internet of things and smart cities, a new trend of the Internet of simulation has emerged to utilise the technologies of cloud, edge, fog computing, and high\u2010performance computing for design and analysis of complex cyber\u2010physical systems using simulation. These technologies although being applied to the domains of big data and deep learning are not adequate to cope with the scale and complexity of emerging connected, smart, and autonomous systems. This study explores the existing state\u2010of\u2010the\u2010art in automating, augmenting, and integrating systems across the domains of smart cities, autonomous vehicles, energy efficiency, smart manufacturing in Industry 4.0, and healthcare. This is expanded to look at existing computational infrastructure and how it can be used to support these applications. A detailed review is presented of advances in approaches providing and supporting\u00a0\u2026", "num_citations": "36\n", "authors": ["510"]}
{"title": "Adaptive service discovery on service-oriented and spontaneous sensor systems\n", "abstract": " Natural and man-made disasters can significantly impact both people and environments. Enhanced effect can be achieved through dynamic networking of people, systems and procedures and seamless integration of them to fulfil mission objectives with service-oriented sensor systems. However, the benefits of integration of services will not be realised unless we have a dependable method to discover all required services in dynamic environments. In this paper, we propose an Adaptive and Efficient Peer-to-peer Search (AEPS) approach for dependable service integration on service-oriented architecture based on a number of social behaviour patterns. In the AEPS network, the networked nodes can autonomously support and co-operate with each other in a peer-to-peer (P2P) manner to quickly discover and self-configure any services available on the disaster area and deliver a real-time capability by self-organising themselves in spontaneous groups to provide higher flexibility and adaptability for disaster monitoring and relief.", "num_citations": "36\n", "authors": ["510"]}
{"title": "Neural network-based overallocation for improved energy-efficiency in real-time cloud environments\n", "abstract": " This paper introduces a dynamic resource provisioning mechanism for over allocating the capacity of Cloud data centers based on customer resource utilization patterns. The proposed mechanism reduces the impact on Real-Time constraints while improvements on the overall energy-efficiency are sought. The main idea is to exploit the resource utilization patterns of each customer for smartly under allocating resources to the requested Virtual Machines. This reduces the waste produced by frequent overestimations and increases the data center availability. Consequently, it creates the opportunity to host additional Virtual Machines in the same computing infrastructure improving its energy-efficiency. In order to mitigate the negative effect on deadlines, the proposed over allocation service implements a multiplayer Neural Network to anticipate the resource usage patterns based on historical data. Additionally, a\u00a0\u2026", "num_citations": "35\n", "authors": ["510"]}
{"title": "Quantification of security for compute intensive workloads in clouds\n", "abstract": " Cloud computing is a promising technology to facilitate development of large-scale, on-demand, flexible computing infrastructures. However, improving dependability of cloud computing is critical for realization of its potential. In this paper, we describe our efforts to quantify security for Clouds to facilitate provision of assurance for quality of service, one of the factors contributing to dependability. This has profound implications for delivering customized security solutions such as effective intrusion prevention and detection which is the overall objective of our research. In order to demonstrate the applicability of our research, we have incorporated these requirements in the resource acquisition phase for Clouds. We also present experiments to demonstrate the effectiveness of our approach to address the random migration problem for virtualized computing environments.", "num_citations": "35\n", "authors": ["510"]}
{"title": "A comparison of network level fault injection with code insertion\n", "abstract": " This paper describes our research into the application of fault injection to Simple Object Access Protocol (SOAP) based service oriented-architectures (SOA). We show that our previously devised WS-FIT method, when combined with parameter perturbation, gives comparable performance to code insertion techniques with the benefit that it is less invasive. Finally we demonstrate that this technique can be used to compliment certification testing of a production system by strategic instrumentation of selected servers in a system.", "num_citations": "34\n", "authors": ["510"]}
{"title": "Software services and software maintenance\n", "abstract": " Software services are being promoted as the next big step forward in software engineering. Inevitably, both service vendor and service client programs will require maintenance. We present a service architecture that has been motivated by a long term vision for software as something which is used, not owned. This architecture is used to show how evolution of software can be achieved. It uses the marketplace to drive the process incrementally. We summarise a new fault-tolerant private information retrieval scheme for protecting users' privacy and ensuring service provision even in the presence of intentional/unintentional service provider faults (e.g. malicious failures). An implementation on a realistic distributed database suggests only a modest performance overhead.", "num_citations": "34\n", "authors": ["510"]}
{"title": "Dynamic authentication for cross-realm SOA-based business processes\n", "abstract": " Modern distributed applications are embedding an increasing degree of dynamism, from dynamic supply-chain management, enterprise federations, and virtual collaborations to dynamic resource acquisitions and service interactions across organizations. Such dynamism leads to new challenges in security and dependability. Collaborating services in a system with a Service-Oriented Architecture (SOA) may belong to different security realms but often need to be engaged dynamically at runtime. If their security realms do not have a direct cross-realm authentication relationship, it is technically difficult to enable any secure collaboration between the services. A potential solution to this would be to locate intermediate realms at runtime, which serve as an authentication path between the two separate realms. However, the process of generating an authentication path for two distributed services can be highly\u00a0\u2026", "num_citations": "33\n", "authors": ["510"]}
{"title": "Quality contracts for real-time enterprises\n", "abstract": " Real-time enterprises rely on user queries being answered in a timely fashion and using fresh data. This is relatively easy when systems are lightly loaded and both queries and updates can be finished quickly. However, this goal becomes fundamentally hard to achieve due to the high volume of queries and updates in real systems, especially in periods of flash crowds. In such cases, systems typically try to optimize for the average case, treating all users, queries, and data equally. In this paper, we argue that it is more beneficial for real-time enterprises to have the users specify how to balance such a tradeoff between Quality of Service (QoS) and Quality of Data (QoD), in other words, \u201cinstructing\u201d the system on how to best allocate resources to maximize the overall user satisfaction. Specifically, we propose Quality Contracts (QC) which is a framework based on the micro-economic paradigm and provides an\u00a0\u2026", "num_citations": "32\n", "authors": ["510"]}
{"title": "Private information retrieval in the presence of malicious failures\n", "abstract": " In the application domain of online information services such as online census information, health records and real-time stock quotes, there are at least two fundamental challenges: the protection of users' privacy and the assurance of service availability. We present a fault-tolerant scheme for private information retrieval (FT-PIR) that protects users' privacy and ensure service provision in the presence of malicious server failures. An error detection algorithm is introduced into this scheme to detect the corrupted results from servers. The analytical and experimental results show that the FT-PIR scheme can tolerate malicious server failures effectively and prevent any information of users front being leaked to attackers. This new scheme does not rely on any unproven cryptographic premise and the availability of tamperproof hardware. An implementation of the FT-PIR scheme on a distributed database system suggests\u00a0\u2026", "num_citations": "32\n", "authors": ["510"]}
{"title": "Joint task assignment and wireless resource allocation for cooperative mobile-edge computing\n", "abstract": " This paper studies a multi-user cooperative mobile- edge computing (MEC) system, in which a local mobile user can offload intensive computation tasks to multiple nearby edge devices serving as helpers for remote execution. We focus on the scenario where the local user has a number of independent tasks that can be executed in parallel but cannot be further partitioned. We consider a time division multiple access (TDMA) communication protocol, in which the local user can offload computation tasks to the helpers and download results from them over pre- scheduled time slots. Under this setup, we minimize the local user's computation latency by optimizing the task assignment jointly with the time and power allocations, subject to individual energy constraints at the local user and the helpers. However, the joint task assignment and wireless resource allocation problem is a mixed-integer non-linear program\u00a0\u2026", "num_citations": "31\n", "authors": ["510"]}
{"title": "Massive-scale automation in cyber-physical systems: Vision & challenges\n", "abstract": " The next era of computing is the evolution of the Internet of Things (IoT) and Smart Cities with development of the Internet of Simulation (IoS). The existing technologies of Cloud, Edge, and Fog computing as well as HPC being applied to the domains of Big Data and deep learning are not adequate to handle the scale and complexity of the systems required to facilitate a fully integrated and automated smart city. This integration of existing systems will create an explosion of data streams at a scale not yet experienced. The additional data can be combined with simulations as services (SIMaaS) to provide a shared model of reality across all integrated systems, things, devices, and individuals within the city. There are also numerous challenges in managing the security and safety of the integrated systems. This paper presents an overview of the existing state-of-the-art in automating, augmenting, and integrating systems\u00a0\u2026", "num_citations": "31\n", "authors": ["510"]}
{"title": "An automatic intrusion diagnosis approach for clouds\n", "abstract": " Virtual machines have attracted significant attention especially within the high performance computing community. However, there remain problems with respect to security in general and intrusion detection and diagnosis in particular which underpin the realization of the potential offered by this emerging technology. In this paper, one such problem has been highlighted, i.e., intrusion severity analysis for large-scale virtual machine based systems, such as clouds. Furthermore, the paper proposes a solution to this problem for the first time for clouds. The proposed solution achieves virtual machine specific intrusion severity analysis while preserving isolation between the security module and the monitored virtual machine. Furthermore, an automated approach is adopted to significantly reduce the overall intrusion response time. The paper includes a detailed description of the solution and an evaluation of our\u00a0\u2026", "num_citations": "31\n", "authors": ["510"]}
{"title": "Software fault tolerance: t/(n-1)-variant programming\n", "abstract": " This paper describes the software fault tolerance scheme, t/(n-1)-variant programming (t/(n-1)-VP), which is based on a particular system diagnosis technique used in hardware and thereby has some spectral advantages involving a simplified adjudication mechanism and enhanced capability of tolerating faults. The dependability of the t/(n-1)-VP architecture is evaluated and then compared with two similar schemes: N-version programming (NVP) and N self-checking programming (NSCP). The comparison shows that t/(n-1)-VP is a viable addition or alternative to present techniques. Much of the classical dependability-analysis of software fault tolerance approaches has focused on the simplest architectural examples that tolerate only single software faults, without considering tolerance to multiple and/or related faults. The results obtained from such analyses are thus restricted. The dependability evaluation in this\u00a0\u2026", "num_citations": "31\n", "authors": ["510"]}
{"title": "Multilevel redundancy allocation using two dimensional arrays encoding and hybrid genetic algorithm\n", "abstract": " With the popularity of multilevel design in large scale systems, reliability redundancy allocation on multilevel systems is becoming attractive to researchers. Multilevel redundancy allocation problem (MLRAP) is not only NP-hard, but also qualifies as hierarchy optimization problem. Exact method could not tackle MLRAP very well, so heuristic and meta-heuristic methods are often used to solve it. To improve the effectiveness of current algorithms on MLRAP, this paper proposes a hybrid genetic algorithm (HGA) based on the two dimensional redundancy encoding mechanism. Instead of hierarchical genotype representation, a two dimensional array is used to represent the solutions to MLRAP. Each row of the array contains the redundancy information of a certain unit in the system and each element in one row stands for the redundancy value of one element of that unit. The number of rows of this array is fixed and\u00a0\u2026", "num_citations": "30\n", "authors": ["510"]}
{"title": "Efficient and scalable search on scale-free P2P networks\n", "abstract": " Unstructured peer-to-peer (P2P) systems (e.g. Gnutella) are characterized by uneven distributions of node connectivity and file sharing. The existence of \u201chub\u201d nodes that have a large number of connections and \u201cgenerous\u201d nodes that share many files significantly influences performance of information search over P2P file-sharing networks. In this paper, we present a novel Scalable Peer-to-Peer Search (SP2PS) method with low maintenance overhead for resource discovery in scale-free P2P networks. Different from existing search methods which employ one heuristic to direct searches, SP2PS achieves better performance by considering both of the number of shared files and the connectivity of each neighbouring node. SP2PS enables peer nodes to forward queries to the neighbours that are more likely to have the requested files and also can help in finding the requested files in the future hops. The\u00a0\u2026", "num_citations": "30\n", "authors": ["510"]}
{"title": "MoSeS: A Grid-enabled spatial decision support system\n", "abstract": " The authors present an architecture for simulation modeling using the resources of grid computing. The use of the grid provides access to the substantial data storage and processing power, which are necessary to translate such models from computational tools into genuine planning aids. As well as providing access to virtualized compute resources, the architecture allows customized applications to meet the needs of an array of potential user organizations. A number of key obstacles in the deployment and integration of e-Science services are identified. These include the high computational costs of simulation modeling at the microscale for typical \u2018\u2018what if\u2019\u2019 scenario questions in research and policy settings; the management and technical issues relating to security in licensing common data sources; sociocultural, legal, and administrative restrictions on the privacy of individual-level response data; and the slow\u00a0\u2026", "num_citations": "29\n", "authors": ["510"]}
{"title": "Efficient resource discovery in self\u2010organized unstructured peer\u2010to\u2010peer networks\n", "abstract": " In unstructured peer\u2010to\u2010peer (P2P) networks, two autonomous peer nodes can be connected if users in those nodes are interested in each other's data. Owing to the similarity between P2P networks and social networks, where peer nodes can be regarded as people and connections can be regarded as relationships, social strategies are useful for improving the performance of resource discovery by self\u2010organizing autonomous peers on unstructured P2P networks. In this paper, we present an efficient social\u2010like peer\u2010to\u2010peer (ESLP) method for resource discovery by mimicking different human behaviours in social networks. ESLP has been simulated in a dynamic environment with a growing number of peer nodes. From the simulation results and analysis, ESLP achieved better performance than current methods. Copyright \u00a9 2008 John Wiley & Sons, Ltd.", "num_citations": "29\n", "authors": ["510"]}
{"title": "Determining the dependability of service-oriented architectures\n", "abstract": " This paper introduces our novel method for the generation of fault injection test cases and failure detection by the use of our enhanced fault and failure models. Our enhanced fault model is an extension of a standard fault model that is constructed through decomposition to create a framework that facilitates automatic test generation. The failure model is based on a set of failure modes and again decomposed to create a framework to aid in the automatic detection of system failures. We demonstrate the Web Service-Fault Injection Technology (WS-FIT) method and fault injection tools with the use of some simulated real-world system.", "num_citations": "29\n", "authors": ["510"]}
{"title": "An ontology-based approach for determining the dependability of service-oriented architectures\n", "abstract": " This paper introduces our novel ontologies for the generation of fault injection test cases and failure detection. Our first ontology is an extension of a standard fault model that is constructed through decomposition to create a framework that facilitates automatic test generation. The second ontology is based on a set of failure modes and again decomposed to create a framework to aid in the automatic detection of system failures. We demonstrate the WS-FIT method and fault injection tools with the use of a simulated real-world system.", "num_citations": "29\n", "authors": ["510"]}
{"title": "Replication-based fault tolerance in a grid environment\n", "abstract": " The potential size and complexity of service-based applications suggests that many such applications will be very prone to errors and failures. The e-Demand project at the University of Leeds proposes a replication-based fault tolerance scheme that allows voting to be performed on results of functionally-equivalent services, with the aim of detecting any incorrect results whilst bringing about potential improvements in performance. We present our initial implementation, constructed using Java and Apache Axis, consisting of a \u201cco-ordination service\u201d that locates, receives, and votes upon jobs submitted by a client program. We also detail DSS-Net, the range of computing nodes and web hosting environments that we are hoping to migrate our services to shortly. In the next few months, we plan to increase the functionality in our service to allow for more complex voting schemes to be specified and performed, and allowing more sophisticated and stochastic services to be used with the scheme.", "num_citations": "28\n", "authors": ["510"]}
{"title": "The t (n-1)-diagnosability and its applications to fault tolerance\n", "abstract": " A system composed of n units is said to be t/(n-1)-diagnosable if, given any complete collection of test results, the set of faulty units can be isolated to within a set of at most n-1 units provided that the number of faulty units does not exceed t. Based on some recently discovered properties of t/(n-1)-diagnosability, the author examines three canonical classes of systems-chains, loop and H/sub 2r,n/ systems-and presents optimal t/(n-1) diagnosable configurations for these classes. Incorporating these results into the scheme of D.M. Blough and A. Pelc (see 20th Inst. Symp. on Fault-Toler. Computing, pp.316-323 (1990)), the author gives an improved diagnosis and repair algorithm for constant-degree multiprocessor systems. A software fault tolerance scheme that utilizes t(n-1)-diagnosis technique is also proposed.< >", "num_citations": "28\n", "authors": ["510"]}
{"title": "Fundamental rate limits of UAV-enabled multiple access channel with trajectory optimization\n", "abstract": " This paper studies an unmanned aerial vehicle (UAV)-enabled multiple access channel (MAC), in which multiple ground users transmit individual messages to a mobile UAV in the sky. We consider a linear topology scenario, where these users locate in a straight line and the UAV flies at a fixed altitude above the line connecting them. Under this setup, we jointly optimize the one-dimensional (1D) UAV trajectory and wireless resource allocation to reveal the fundamental rate limits of the UAV-enabled MAC, under the users' individual maximum power constraints and the UAV's maximum flight speed constraints. First, we consider the capacity-achieving non-orthogonal multiple access (NOMA) transmission with successive interference cancellation (SIC) at the UAV receiver. In this case, we characterize the capacity region by maximizing the average sum-rate of all users subject to a set of rate profile constraints. To\u00a0\u2026", "num_citations": "27\n", "authors": ["510"]}
{"title": "SEED: A scalable approach for cyber-physical system simulation\n", "abstract": " Simulation is critical when studying real operational behavior of increasingly complex Cyber-Physical Systems, forecasting future behavior, and experimenting with hypothetical scenarios. A critical aspect of simulation is the ability to evaluate large-scale systems within a reasonable time frame while modeling complex interactions between millions of components. However, modern simulations face limitations in provisioning this functionality for CPSs in terms of balancing simulation complexity with performance, resulting in substantial operational costs required for completing simulation execution. Moreover, users are required to have expertise in modeling and configuring simulations to infrastructure which is time consuming. In this paper we present Simulation EnvironmEnt Distributor (SEED), a novel approach for simulating large-scale CPSs across a loosely-coupled distributed system requiring minimal user\u00a0\u2026", "num_citations": "27\n", "authors": ["510"]}
{"title": "Personalized context-aware QoS prediction for web services based on collaborative filtering\n", "abstract": " The emergence of abundant Web Services has enforced rapid evolvement of the Service Oriented Architecture (SOA). To help user selecting and recommending the services appropriate to their needs, both functional and nonfunctional quality of service (QoS) attributes should be taken into account. Before selecting, user should predict the quality of Web Services. A Collaborative Filtering (CF)-based recommendation system is introduced to attack this problem. However, existing CF approaches generally do not consider context, which is an important factor in both recommender system and QoS prediction. Motivated by this, the paper proposes a personalized context-aware QoS prediction method for Web Services recommendations based on the SLOPE ONE approach. Experimental results demonstrate that the suggested approach provides better QoS prediction.", "num_citations": "27\n", "authors": ["510"]}
{"title": "Characterization and design of sequentially t-diagnosable systems\n", "abstract": " In the system-level diagnosis area, FP Preparata, G. Metze, and RT Chien (1967) first presented a formal graph-theoretic model and introduced the concept of sequentially t-diagnosable systems. A system S is called sequentially t-diagnosable if, given any complete collection of test results, at least one faulty unit in S can be identified, provided the number of faulty units does not exceed t. However, until very recently, developing a characterization theorem of sequentially t-diagnosable systems for the PMC model was still an important, open problem. The authors resolve this problem by presenting the first complete characterization. A canonical class of systems, D/sub 1, k/systems, is discussed, and a valuable result on the sequential t-diagnosability is obtained.<>", "num_citations": "27\n", "authors": ["510"]}
{"title": "Straggler detection in parallel computing systems through dynamic threshold calculation\n", "abstract": " Cloud computing systems face the substantial challenge of the Long Tail problem: a small subset of straggling tasks significantly impede parallel jobs completion. This behavior results in longer service response times and degraded system utilization. Speculative execution, which create task replicas at runtime, is a typical method deployed in large-scale distributed systems to tolerate stragglers. This approach defines stragglers by specifying a static threshold value, which calculates the temporal difference between an individual task and the average task progression for a job. However, specifying static threshold debilitates speculation effectiveness as it fails to consider the intrinsic diversity of job timing constraints within modern day Cloud computing systems. Capturing such heterogeneity enables the ability to impose different levels of strictness for replica creation while achieving specified levels of QoS for different\u00a0\u2026", "num_citations": "26\n", "authors": ["510"]}
{"title": "Reliable computing service in massive-scale systems through rapid low-cost failover\n", "abstract": " Large-scale distributed systems deployed as Cloud datacenters are capable of provisioning service to consumers with diverse business requirements. Providers face pressure to provision uninterrupted reliable services while reducing operational costs due to significant software and hardware failures. A widely adopted means to achieve such a goal is using redundant system components to implement user-transparent failover, yet its effectiveness must be balanced carefully without incurring heavy overhead when deployed-an important practical consideration for complex large-scale systems. Failover techniques developed for Cloud systems often suffer serious limitations, including mandatory restart leading to poor cost-effectiveness, as well as solely focusing on crash failures, omitting other important types, such as timing failures and simultaneous failures. This paper addresses these limitations by presenting a\u00a0\u2026", "num_citations": "26\n", "authors": ["510"]}
{"title": "Delivering sustainable capability on evolutionary service-oriented architecture\n", "abstract": " Network enabled capability (NEC) is the U.K. Ministry of Defencepsilas response to the quickly changing conflict environment in which its forces must operate. In NEC, systems need to be integrated in context, to assist in human activity and provide dependable inter-operation. In order to provide reliable and sustainable military capability, fast paced changes must be conducted without halting the operation of a capability. In this paper we present the concept of evolutionary service-oriented architecture for delivering sustainable capability. The reliability of the architecture is evaluated by simulations using a computer-based model. The simulation results indicate that the evolutionary service-oriented architecture can provide higher reliability and sustainability in the provision of capability in a dynamic environment.", "num_citations": "26\n", "authors": ["510"]}
{"title": "Reducing late-timing failure at scale: Straggler root-cause analysis in cloud datacenters\n", "abstract": " Task stragglers hinder effective parallel job execution in Cloud datacenters, resulting in late-timing failures due to the violation of specified timing constraints. Stragglertolerant methods such as speculative execution provide limited effectiveness due to (i) lack of precise straggler root-cause knowledge and (ii) straggler identification occurring too late within a job lifecycle. This paper proposes a method to ascertain underlying straggler root-causes by analyzing key parameters within large-scale distributed systems, and to determine the correlation between straggler occurrence and factors including resource contention, task concurrency, and server failures. Our preliminary study of a production Cloud datacenter indicates that the dominate straggler root-cause is resultant of high temporal resource contention. The result can assist in enhancing straggler prediction and mitigation for tolerating late-timing failures within large-scale distributed systems.", "num_citations": "25\n", "authors": ["510"]}
{"title": "Computing at massive scale: Scalability and dependability challenges\n", "abstract": " Large-scale Cloud systems and big data analytics frameworks are now widely used for practical services and applications. However, with the increase of data volume, together with the heterogeneity of workloads and resources, and the dynamic nature of massive user requests, the uncertainties and complexity of resource management and service provisioning increase dramatically, often resulting in poor resource utilization, vulnerable system dependability, and user-perceived performance degradations. In this paper we report our latest understanding of the current and future challenges in this particular area, and discuss both existing and potential solutions to the problems, especially those concerned with system efficiency, scalability and dependability. We first introduce a data-driven analysis methodology for characterizing the resource and workload patterns and tracing performance bottlenecks in a massive\u00a0\u2026", "num_citations": "25\n", "authors": ["510"]}
{"title": "On the reliability and energy efficiency in cloud computing\n", "abstract": " With the popularity of cloud computing, it becomes crucial to provide on-demand services dynamically according to the user\u2019s requirements. Reliability and energy efficiency are two big challenges in cloud computing systems that need careful attention and investigation. This paper first presents a review of existing techniques for reliability and energy efficiency and then identifies the research gaps to combining these two metrics for resource provisioning in cloud computing environments.", "num_citations": "24\n", "authors": ["510"]}
{"title": "Guide to e-Science: next generation scientific research and discovery\n", "abstract": " The way in which scientific research is carried out is undergoing a series of radical changes, worldwide, as a result of the digital revolution. However, this \u201cScience 2.0\u201d requires a comprehensive supporting cyber-infrastructure. This essential guidebook on e-science presents real-world examples of practices and applications, demonstrating how a range of computational technologies and tools can be employed to build essential infrastructures supporting next-generation scientific research. Each chapter provides introductory material on core concepts and principles, as well as descriptions and discussions of relevant e-science methodologies, architectures, tools, systems, services and frameworks. The guide\u2019s explanations and context present a broad spectrum of different e-science system requirements. Topics and features: includes contributions from an international selection of preeminent e-science experts and practitioners; discusses use of mainstream grid computing and peer-to-peer grid technology for \u201copen\u201d research and resource sharing in scientific research; presents varied methods for data management in data-intensive research; investigates issues of e-infrastructure interoperability, security, trust and privacy for collaborative research; examines workflow technology for the automation of scientific processes, and that ensures the research can be reusable, reproducible and repeatable; describes applications of e-science, highlighting systems used in the fields of biometrics, clinical medicine, and ecology. This highly practical text/reference is a \u201cmust-have, must-use\u201d resource for both IT professionals and academic researchers. Graduate\u00a0\u2026", "num_citations": "24\n", "authors": ["510"]}
{"title": "Intelligent resource scheduling at scale: a machine learning perspective\n", "abstract": " Resource scheduling in a computing system addresses the problem of packing tasks with multi-dimensional resource requirements and non-functional constraints. The exhibited heterogeneity of workload and server characteristics in Cloud-scale or Internet-scale systems is adding further complexity and new challenges to the problem. Compared with,,,, existing solutions based on ad-hoc heuristics, Machine Learning (ML) has the potential to improve further the efficiency of resource management in large-scale systems. In this paper we,,,, will describe and discuss how ML could be used to understand automatically both workloads and environments, and to help to cope with scheduling-related challenges such as consolidating co-located workloads, handling resource requests, guaranteeing application's QoSs, and mitigating tailed stragglers. We will introduce a generalized ML-based solution to large-scale\u00a0\u2026", "num_citations": "23\n", "authors": ["510"]}
{"title": "Crown-c: A high-assurance service-oriented grid middleware system\n", "abstract": " Service-orientation is a highly useful means of developing flexible, agile, and dependable software systems, and is a paradigm that has been increasingly adopted into grid computing middleware. However, service-orientation brings with it new challenges in the fields of dependability and security that need to be addressed by the high assurance systems community in order to provide sufficient support to enable service- based grid applications to offer non-trivial quality of service guarantees. This paper discusses some of the new dependability and security challenges introduced by service-orientation, and for the first time introduces CROWN-C - a grid middleware system that features specific enhancements designed to support the development and assessment of highly secure, dependable, service-oriented grid systems and applications. The architecture of the new middleware is discussed, and the architecture\u00a0\u2026", "num_citations": "23\n", "authors": ["510"]}
{"title": "Implementing software-fault tolerance in c++ and open c++: An object-oriented and reflective approach\n", "abstract": " This paper reports our experience with the use of the C++ language and Open C++(a reflective version of C++) to implement reusable, dependable control structures that support the prevision of software-fault tolerance in the application layer. We first implement the support using an object library approach and then re-design it using a reflective one. We demonstrate through a realistic experiment why reflection and metaobject protocols are particularly suitable for the development of fault-tolerant programs.", "num_citations": "23\n", "authors": ["510"]}
{"title": "Sequentially t-diagnosable systems: A characterization and its applications\n", "abstract": " In the system level fault diagnosis area, the fundamental problem of characterizing sequentially t-diagnosable systems in the PMC model has remained open for more than two decades. We resolve this problem by providing a complete characterization of such systems. Our solution to the characterization problem leads to the correct identification of optimal sequentially t-diagnosable D/sub /spl delta/,k/ systems. Given a set of n units where n=2t+1, an optimal D/sub /spl delta/,k/ system can be constructed with just n([(t+2)/3]) tests, rather than n([t/2]+1) tests-a previously misjudged bound. An efficient algorithm for identifying the set of faulty units in a sequentially t-diagnosable D/sub /spl delta/,k/ system is given along the line of the proposed characterization, which is linear with respect to the number of tests in the system.< >", "num_citations": "23\n", "authors": ["510"]}
{"title": "Dependability assessment of grid middleware\n", "abstract": " Dependability is a key factor in any software system due to the potential costs in both time and money a failure may cause. Given the complexity of grid applications that rely on dependable grid middleware, tools for the assessment of grid middleware are highly desirable. Our past research, based around our fault injection technology (FIT) framework and its implementation, WS-FIT, has demonstrated that network level fault injection can be a valuable tool in assessing the dependability of traditional Web services. Here we apply our FIT framework to globus grid middleware using grid-FIT, our new implementation of the FIT framework, to obtain middleware dependability assessment data. We conclude by demonstrating that grid-FIT can be applied to globus grid systems to assess dependability as part of a fault removal mechanism and thus allow middleware dependability to be increased.", "num_citations": "22\n", "authors": ["510"]}
{"title": "The internet of simulation, a specialisation of the internet of things with simulation and workflow as a service (sim/wfaas)\n", "abstract": " A trend seen in many industries is the increasing reliance on modelling and simulation to facilitate design, decision making and training. Previously, these models would operate in isolation but now there is a growing need to integrate and connect simulations together for co-simulation. In addition, the 21st century has seen the expansion of the Internet of Things (IoT) enabling the interconnectivity of smart devices across the Internet. In this paper we propose that an important, and often overlooked, domain of IoT is that of modelling and simulation. Expanding IoT to encompass interconnected simulations enables the potential for an Internet of Simulation whereby models and simulations are exposed to the wider internet and can be accessed on an \"as-a-service\" basis. The proposed IoS would need to manage simulation across heterogeneous infrastructures, temporal and causal aspects of simulations, as well as\u00a0\u2026", "num_citations": "21\n", "authors": ["510"]}
{"title": "An abstract model for integrated intrusion detection and severity analysis for clouds\n", "abstract": " Cloud computing is an emerging computing paradigm which introduces novel opportunities to establish large scale, flexible computing infrastructures. However, security underpins extensive adoption of Cloud computing. This paper presents efforts to address one of the significant issues with respect to security of Clouds ie intrusion detection and severity analysis. An abstract model for integrated intrusion detection and severity analysis for Clouds is proposed to facilitate minimal intrusion response time while preserving the overall security of the Cloud infrastructures. In order to assess the effectiveness of the proposed model, detailed architectural evaluation using Architectural Trade-off Analysis Model (ATAM) is used. A set of recommendations which can be used as a set of best practice guidelines while implementing the proposed architecture is discussed.", "num_citations": "21\n", "authors": ["510"]}
{"title": "Service oriented architectures in the provision of military capability\n", "abstract": " Service oriented architecture (SOA) is becoming established in computing as a means to integrate processing and access data across organisations. In general architecture terms, services are used to loosely couple assets in systems by describing service interfaces at a high level of abstraction. Network Enabled Capability (NEC) is an initiative from the MoD to react to the rapidly changing conflict environment in which its forces must operate by using dynamic integration of assets to provide dependable military capabilities. The NECTISE (NEC Through Innovative Systems Engineering) project is responding to this need by investigating how loosely coupled services can be used to describe the functions and quality of service for heterogeneous assets and networks. SOA mechanisms for discovery, management and integration can be used along with rich service descriptions to support continuous delivery of capability.", "num_citations": "21\n", "authors": ["510"]}
{"title": "Tower: Practical trust negotiation framework for grids\n", "abstract": " In order to establish trust relationship between service requesters and providers in an open decentralized environment, we propose a novel trust negotiation framework, TOWER, which integrates distributed trust chain construction of trust management and aims to enhance the grid security infrastructure. Our approach leverages attribute-based credentials to support flexible delegation, and dynamically constructs trust chains. A novel TRust chAin based Negotiation Strategy (TRANS) is proposed to establish trust relationship on the fly by gradually disclosing credentials according to various access control policies. Our approach has been successfully implemented as useful components and fundamental security services in the CROWN Grid, and techniques such as trust tickets and policy caching that can greatly increase service efficiency are used. Finally, we evaluate our approach by comprehensive experiments\u00a0\u2026", "num_citations": "21\n", "authors": ["510"]}
{"title": "Ft-grid: A fault-tolerance system for e-science\n", "abstract": " The FT-Grid system introduces a multi-version design-based fault tolerance framework that allows faults occurring in service-based systems to be tolerated, thus increasing the dependability of such systems. This paper details the progress that has been made in the development of FT-Grid, including both a GUI client and also a web service interface. We show empirical evidence of the dependability benefits offered by FT-Grid, by performing a dependability analysis using fault injection testing performed with the WS-FIT tool. We then illustrate a potential problem with voting based fault tolerance approaches in the service-oriented paradigm\u2013namely, that individual channels within fault-tolerant systems may invoke common services as part of their workflow, thus increasing the potential for common-mode failure. We propose a solution to this issue by using the technique of provenance to provide FT-Grid with topological awareness. We implement a large test system, and\u2013with the use of the PreServ provenance system developed as part of the PASOA project at the University of Southampton\u2013perform a large number of experiments which show that a topologically-aware FT-Grid system results in a much more dependable system than any other configuration tested, whilst imposing a negligible timing overhead.", "num_citations": "21\n", "authors": ["510"]}
{"title": "An adaptive approach to achieving hardware and software fault tolerance in a distributed computing environment\n", "abstract": " This paper focuses on the problem of providing tolerance to both hardware and software faults in independent applications running on a distributed computing environment. Several hybrid-fault-tolerant architectures are identified and proposed. Given the highly varying and dynamic characteristics of the operating environment, solutions are developed mainly exploiting the adaptation property. They are based on the adaptive execution of redundant programs so as to minimise hardware resource consumption and to shorten response time, as much as possible, for a required level of fault tolerance. A method is introduced for evaluating the proposed architectures with respect to reliability, resource utilisation and response time. Examples of quantitative evaluations are also given.", "num_citations": "21\n", "authors": ["510"]}
{"title": "Rose: Cluster resource scheduling via speculative over-subscription\n", "abstract": " A long-standing challenge in cluster scheduling is to achieve a high degree of utilization of heterogeneous resources in a cluster. In practice there exists a substantial disparity between perceived and actual resource utilization. A scheduler might regard a cluster as fully utilized if a large resource request queue is present, but the actual resource utilization of the cluster can be in fact very low. This disparity results in the formation of idle resources, leading to inefficient resource usage and incurring high operational costs and an inability to provision services. In this paper we present a new cluster scheduling system, ROSE, that is based on a multi-layered scheduling architecture with an ability to over-subscribe idle resources to accommodate unfulfilled resource requests. ROSE books idle resources in a speculative manner: instead of waiting for resource allocation to be confirmed by the centralized scheduler, it\u00a0\u2026", "num_citations": "20\n", "authors": ["510"]}
{"title": "An empirical study on the procedure to derive software quality estimation models\n", "abstract": " Software quality assurance has been a heated topic for several decades. If factors that influence software quality can be identified, they may provide more insight for better software development management. More precise quality assurance can be achieved by employing resources according to accurate quality estimation at the early stages of a project. In this paper, a general procedure is proposed to derive software quality estimation models and various techniques are presented to accomplish the tasks in respective steps. Several statistical techniques together with machine learning method are utilized to verify the effectiveness of software metrics. Moreover, a neuro-fuzzy approach is adopted to improve the accuracy of the estimation model. This procedure is carried out based on data from the ISBSG repository to present its empirical value.", "num_citations": "20\n", "authors": ["510"]}
{"title": "On dynamic replication strategies in data service grids\n", "abstract": " Service oriented architecture (SOA) allows multiple and heterogeneous data resources to be integrated within a single service while hiding the implementation details and formats of data resources from users of the service. However, data sources for a service are often distributed geographically and connected with long-latency networks; time and bandwidth consumption of data transportation may have an impact on the system performance. Dynamic data replication is a practical solution to this problem. By replicating data copies to appropriate sites, this approach aims to reduce time and bandwidth consumptions over networks. Existing strategies for dynamic replication are typically based on so-called single-location algorithms for identifying a single site for data replication. In this paper we discuss the issues with single-location strategies in large-scale data integration applications, and examine potential multiple\u00a0\u2026", "num_citations": "20\n", "authors": ["510"]}
{"title": "HARTs: High availability cluster architecture with redundant TCP stacks\n", "abstract": " Improving the availability of services of is a key issue for survivability of a cluster system. Lots of schemes are proposed for this purpose. But most of them aim at enhancing only the service-level availability or application specific. In this paper, we propose a scheme called High Availability with Redundant TCP Stacks (HARTs), providing connection-level availability by exploring the redundant TCP stacks for TCP connections at the server side. We present our performance experiment results on our HA cluster prototype. From results, we find the configuration of one primary server with one backup server running on separated 100 Mbps Ethernet has acceptable performance to support the server side applications while delivering high availability.", "num_citations": "20\n", "authors": ["510"]}
{"title": "Dynamic adjustment of dependability and efficiency in fault-tolerant software\n", "abstract": " In this paper we discuss the problem of attaining a dynamic compromise between using redundancy to improve software dependability and limiting the amount of redundancy so as to avoid unnecessary inefficiencies. A scheme, called self-configuring optimal programming (SCOP), is developed. SCOP attempts to reduce the resource cost of fault-tolerant software, both in space and time, by providing designers with a flexible redundancy architecture in which dependability and efficiency can be adjusted dynamically at run time. A design methodology is proposed to introduce support techniques for such dynamic adjustment. Our scheme also suggests a general control framework into which design diversity, data diversity and multiple copies can be incorporated selectively. A detailed dependability and efficiency evaluation shows that SCOP can achieve the same dependability level as those of other existing\u00a0\u2026", "num_citations": "20\n", "authors": ["510"]}
{"title": "Improving data center efficiency through holistic scheduling in kubernetes\n", "abstract": " Data centers are the infrastructure that underpins modern distributed service-oriented systems. They are complex systems-of-systems, with many interacting elements, that consume vast amounts of power. Demand for such facilities is growing rapidly, leading to significant global environmental impact. The data center industry has conducted much research into efficiency improvements, but this has mostly been at the physical infrastructure level. Research into software-based solutions for improving efficiency is greatly needed. However, most current research does not take a holistic view of the data center that considers virtual and physical infrastructures as well as business process. This is crucial if a solution is to be applied in a realistic setting. This paper describes the complex, system-of-systems nature of data centers, and discusses the service models used in the industry. We describe a holistic scheduling system\u00a0\u2026", "num_citations": "19\n", "authors": ["510"]}
{"title": "Enhancing multi-tenancy security in the cloud IaaS model over public deployment\n", "abstract": " Cloud Computing can be seen as an instance of Computing as a Utility, where customers utilize the concept of \"pay-as-you-go\" for applications, computing and storage resources. This concept is utilized particularly heavily in Infrastructure as a Service (IaaS) models. In IaaS Clouds, customers can instantly be allocated - and subsequently release - Virtual Machines (VMs). To achieve easy management and better performance, Cloud providers utilize automated resource allocation techniques, which results in some cases of having two or more VMs belonging to different customers residing in the same physical machine. The situation described above is known as Multi-Tenancy and could lead to confidentiality violation. Since Multi-tenancy heavily depends on resource allocation, we propose a resource allocation technique which considers security as a requirement. In most cases resource allocation techniques\u00a0\u2026", "num_citations": "19\n", "authors": ["510"]}
{"title": "Timely long tail identification through agent based monitoring and analytics\n", "abstract": " The increasing complexity and scale of distributed systems has resulted in the manifestation of emergent behavior which substantially affects overall system performance. A significant emergent property is that of the \"Long Tail\", whereby a small proportion of task stragglers significantly impact job execution completion times. To mitigate such behavior, straggling tasks occurring within the system need to be accurately identified in a timely manner. However, current approaches focus on mitigation rather than identification, which typically identify stragglers too late in the execution lifecycle. This paper presents a method and tool to identify Long Tail behavior within distributed systems in a timely manner, through a combination of online and offline analytics. This is achieved through historical analysis to profile and model task execution patterns, which then inform online analytic agents that monitor task execution at\u00a0\u2026", "num_citations": "18\n", "authors": ["510"]}
{"title": "Modelling and simulation of network enabled capability on service-oriented architecture\n", "abstract": " Network Enabled Capability (NEC) is the UK Ministry of Defence\u2019s response to the quickly changing conflict environment in which its forces must operate. In NEC, systems need to be integrated in context, to assist in human activity and provide dependable inter-operation. In this paper, we present our research work in the NECTISE project with a focus on the modelling and simulation of service-oriented architecture (SOA) for delivering dependable and sustainable military capability. The simulation results indicate that the proposed architectural model can provide a high-level of reliability and sustainability in the provision of capability in a dynamic environment. Moreover, a NEC demonstration system for regional surveillance is introduced in this paper to illustrate the use of SOA to achieve NEC.", "num_citations": "18\n", "authors": ["510"]}
{"title": "Building dependable software for critical applications: Multi-version software versus one good version\n", "abstract": " An increasing range of industries have a growing dependence on software based systems, many of which are safety-critical, real-time applications that require extremely high dependability. Multi-version programming has been proposed as a method for increasing the overall dependability of such systems. We describe an experiment to establish whether or not the multi-version method can offer increased dependability over the traditional single-version development approach when given the same level of resources. Three programs were developed independently to control a real-time, safety-critical system, and were put together to form a decentralized multi-version system. Three functionally equivalent single-version systems. were also implemented, each using the same amount of development resources as the combined resources of the multi-version system. The analytic results from this experiment show that 1\u00a0\u2026", "num_citations": "18\n", "authors": ["510"]}
{"title": "Dynamic data driven application systems for smart cities and urban infrastructures\n", "abstract": " The smart cities vision relies on the use of information and communication technologies to efficiently manage and maximize the utility of urban infrastructures and municipal services in order to improve the quality of life of its inhabitants. Many aspects of smart cities are dynamic data driven application systems (DDDAS) where data from sensors monitoring the system are used to drive computations that in turn can dynamically adapt and improve the monitoring process as the city evolves. Several leading DDDAS researchers offer their views concerning the DDDAS paradigm applied to realizing smart cities and outline research challenges that lie ahead.", "num_citations": "17\n", "authors": ["510"]}
{"title": "An analysis of performance interference effects on energy-efficiency of virtualized cloud environments\n", "abstract": " Co-allocated workloads in a virtualized computing environment often have to compete for resources, thereby suffering from performance interference. While this phenomenon has a direct impact on the Quality of Service provided to customers, it also changes the patterns of resource utilization and reduces the amount of work per Watt consumed. Unfortunately, there has been only limited research into how performance interference affects energy-efficiency of servers in such environments. In reality, there is a highly dynamic and complicated correlation among resource utilization, performance interference and energy-efficiency. This paper presents a comprehensive analysis that quantifies the negative impact of performance interference on the energy-efficiency of virtualized servers. Our analysis methodology takes into account the heterogeneous workload characteristics identified from a real Cloud environment. In\u00a0\u2026", "num_citations": "17\n", "authors": ["510"]}
{"title": "Distributed service integration for disaster monitoring sensor systems\n", "abstract": " Sensor networks have the potential to revolutionise the capture, processing and communication of critical data for use of disaster rescue and relief. In order to provide a dependable rescue capability through dynamically integrating newly developed and legacy sensor systems with other systems and computing, new methodologies are required for the dependable integration of services in heterogeneous environments. In this study, the authors present a new architectural model which can proactively self-adapt to changes and evolution occurring in the provision of search and rescue capabilities in a dynamic environment. This performance and reliability of the approach has been evaluated using simulations in a dynamic environment and demonstrated through developing and testing a demonstration system for a scenario of disaster area monitoring.", "num_citations": "17\n", "authors": ["510"]}
{"title": "Adaptive speculation for efficient internetware application execution in clouds\n", "abstract": " Modern Cloud computing systems are massive in scale, featuring environments that can execute highly dynamic Internetware applications with huge numbers of interacting tasks. This has led to a substantial challenge\u2014the straggler problem, whereby a small subset of slow tasks significantly impede parallel job completion. This problem results in longer service responses, degraded system performance, and late timing failures that can easily threaten Quality of Service (QoS) compliance. Speculative execution (or speculation) is the prominent method deployed in Clouds to tolerate stragglers by creating task replicas at runtime. The method detects stragglers by specifying a predefined threshold to calculate the difference between individual tasks and the average task progression within a job. However, such a static threshold debilitates speculation effectiveness as it fails to capture the intrinsic diversity of timing\u00a0\u2026", "num_citations": "16\n", "authors": ["510"]}
{"title": "MoSeS: Modelling and simulation for e-social science\n", "abstract": " MoSeS (Modelling and Simulation for e-Social Science) is a research node of the National Centre for e-Social Science. MoSeS uses e-Science techniques to execute an events-driven model that simulates discrete demographic processes; this allows us to project the UK population 25 years into the future. This paper describes the architecture, simulation methodology and latest results obtained by MoSeS.", "num_citations": "16\n", "authors": ["510"]}
{"title": "SOA, dependability, and measures and metrics for network enabled capability\n", "abstract": " Network Enabled Capability (NEC) is the UK Ministry of Defence's key response to the rapidly changing conflict environment in which its forces must operate. This paper introduces one part of the EPSRC and BAE Systems jointly funded project NECTISE (NEC Through Innovative Systems Engineering) that aims to address NEC issues using service-oriented architecture (SOA) and enhanced system dependability using quality of service measures and metrics. SOA is a network-enabled solution that has the potential to combine assets (software resources, people, equipment, processes) to provide capability; that is, the ability to achieve a mission objective. This position paper introduces the problems NEC faces that can be addressed by SOA, and highlights some of those outstanding issues, such as dependability, not addressed by SOA that form research directions within the NECTISE programme.", "num_citations": "16\n", "authors": ["510"]}
{"title": "Multiparty interactions in dependable distributed systems\n", "abstract": " With the expansion of computer networks, activities involving computer communication are becoming more and more distributed. Such distribution can include processing, control, data, network management, and security. Although distribution can improve the reliability of a system by replicating components, sometimes an increase in distribution can introduce some undesirable faults. To reduce the risks of introducing, and to improve the chances of removing and tolerating faults when distributing applications, it is important that distributed systems are implemented in an organized way. As in sequential programming, complexity in distributed, in particular parallel, program development can be managed by providing appropriate programming language constructs. Language constructs can help both by supporting encapsulation so as to prevent unwanted interactions between program components and by providing higher-level abstractions that reduce programmer effort by allowing compilers to handle mundane, error-prone aspects of parallel program implementation. A language construct that supports encapsulation of interactions between multiple parties (objects or processes) is referred in the literature as multiparty interaction. In a multiparty interaction, several parties somehow \"come together\" to produce an intermediate and temporary combined state, use this state to execute some activity, and then leave the interaction and continue their normal execution. There has been a lot of work in the past years on multiparty interaction, but most of it has been concerned with synchronisation, or handshaking, between parties rather than the\u00a0\u2026", "num_citations": "16\n", "authors": ["510"]}
{"title": "Cloud computing security: Opportunities and pitfalls\n", "abstract": " The evolution of modern computing systems has lead to the emergence of Cloud computing. Cloud computing facilitates on-demand establishment of dynamic, large scale, flexible, and highly scalable computing infrastructures. However, as with any other emerging technology, security underpins widespread adoption of Cloud computing. This paper presents the state-of-the-art about Cloud computing along with its different deployment models. The authors also describe various security challenges that can affect an organization\u2019s decision to adopt Cloud computing. Finally, the authors list recommendations to mitigate with these challenges. Such review of state-of-the-art about Cloud computing security can serve as a useful barometer for an organization to make an informed decision about Cloud computing adoption.", "num_citations": "15\n", "authors": ["510"]}
{"title": "FT\u2010Grid: a system for achieving fault tolerance in grids\n", "abstract": " The FT\u2010Grid system introduces a fault\u2010tolerance framework that allows faults occurring in service\u2010oriented systems to be tolerated, thus increasing the dependability of such systems. This paper presents the design, development and evaluation of FT\u2010Grid. We show empirical evidence of the dependability benefits offered by FT\u2010Grid by performing an experimental dependability analysis using fault\u2010injection testing performed with the WS\u2010FIT tool. We then illustrate a potential problem with voting\u2010based fault\u2010tolerance schemes in the service\u2010oriented paradigm\u2014namely that individual channels within a fault\u2010tolerant system, supposed to be independent of each other, may in fact invoke common services as part of their workflow, thus increasing the potential for common\u2010mode failure of those channels. We propose a solution to this issue by using the technique of provenance to provide FT\u2010Grid with topological\u00a0\u2026", "num_citations": "15\n", "authors": ["510"]}
{"title": "A dynamic shadow approach for mobile agents to survive crash failures\n", "abstract": " Fault tolerance schemes for mobile agents to survive agent server crash failures are complex since developers normally have no control over remote agent servers. Some solutions inject a replica into stable storage upon its arrival at an agent server. However in the event of an agent server crash the replica is unavailable until the agent server recovers. This paper presents a failure model and a revised exception handling framework for mobile agent systems. An exception handler design is presented for mobile agents to survive agent server crash failures. A replica mobile agent operates at the agent server visited prior to its master's current location. If a master crashes its replica is available as a replacement. Experimental evaluation is performed and performance results are used to suggest some useful design guidelines.", "num_citations": "15\n", "authors": ["510"]}
{"title": "A fault-tolerant approach to secure information retrieval\n", "abstract": " Several private information retrieval (PIR) schemes were proposed to protect users' privacy when sensitive information stored in database servers is retrieved. However, existing PIR schemes assume that any attack to the servers does not change the information stored and any computational results. We present a novel fault-tolerant PIR scheme (called FT-PIR) that protects users' privacy and at the same time ensures service availability in the presence of malicious server faults. Our scheme neither relies on any unproven cryptographic assumptions nor the availability of tamper-proof hardware. A probabilistic verification function is introduced into the scheme to detect corrupted results. Unlike previous PIR research that attempted mainly to demonstrate the theoretical feasibility of PIR, we have actually implemented both a PIR scheme and our FT-PIR scheme in a distributed database environment. The experimental\u00a0\u2026", "num_citations": "15\n", "authors": ["510"]}
{"title": "The Internet of Simulation: Enabling agile model based systems engineering for cyber-physical systems\n", "abstract": " The expansion of the Internet of Things (IoT) has resulted in a complex cyber-physical system of systems that is continually evolving. With ever more complex systems being developed and changed there has been an increasing reliance on simulation as a vital part of the design process. There is also a growing need for simulation integration and co-simulation in order to analyse the complex interactions between system components. To this end we propose that the Internet of Simulation (IoS) as an extension of IoT can be used to meet these needs. The IoS allows for multiple heterogeneous simulations to be integrated together for co-simulation. It's effect on the engineer process is to facilitate agile practices without sacrificing rigour. An Industry 4.0 example case study is provided showing how IoS could be utilized.", "num_citations": "14\n", "authors": ["510"]}
{"title": "To trust or not to trust? Developing trusted digital spaces through timely reliable and personalized provenance\n", "abstract": " Organizations are increasingly dependent on data stored and processed by distributed, heterogeneous services to make critical, high-value decisions. However, these service-orientated computing environments are dynamic in nature and are becoming ever more complex systems of systems. In such evolving and dynamic eco-system infrastructures, knowing how data was derived is of significant importance in determining its validity and reliability. To address this, a number of advocates and theorists postulate that provenance is critical to building trust in data and the services that generated it as it provides evidence for data consumers to judge the integrity of the results. Thi spaper presents a summary of the STRAPP (trusted digital Spaces through Timely Reliable And Personalised Provenance) project, which is designing and engineering mechanisms to achieve a holistic solution to a number of real-world service-based decision-support systems.", "num_citations": "14\n", "authors": ["510"]}
{"title": "An ontology for evaluation of network enabled capability architectures\n", "abstract": " The UK Ministry of Defence\u2019s Network Enabled Capability and USA Department of Defence\u2019s Network-Centric Warfare programmes, are aimed at improving military effectiveness through the networking of existing and new military assets. Research is being carried out into architectures needed to support Network Enabled Capability\u2013including emerging software architectures such as Service Oriented Architectures. This paper presents a framework for the evaluation of architectures that will enable the comparison of different architectures and their use to be made. The framework identifies the elements needed to perform such an evaluation, how those elements are used and how they should be obtained or developed. Further work will develop such evaluations and use Case Studies to show their value.", "num_citations": "14\n", "authors": ["510"]}
{"title": "MoSeS: Modelling and simulation for e-social science\n", "abstract": " This poster describes an e-Social Science research programme at the University of Leeds with a specific focus on Modelling and Simulation (MOSES\u2013Modelling and Simulation for e-Social Science). The cornerstone of the programme is the creation of a dynamic simulation model of the UK population, represented as a series of richly disaggregated individuals and households. We aim to use the power of e-Science Technologies to deliver a complete representation of the population which draws on attributes from a diverse portfolio of databases. The simulation model will be applied to address research questions in three social science domains, relating to healthcare policy and practice, transport & environmental sustainability, and the business impacts of socio-demographic change.", "num_citations": "14\n", "authors": ["510"]}
{"title": "Performance-aware Speculative Resource Oversubscription for Large-scale Clusters\n", "abstract": " It is a long-standing challenge to achieve a high degree of resource utilization in cluster scheduling. Resource oversubscription has become a common practice in improving resource utilization and cost reduction. However, current centralized approaches to oversubscription suffer from the issue with resource mismatch and fail to take into account other performance requirements, e.g., tail latency. In this article we present ROSE, a new resource management platform capable of conducting performance-aware resource oversubscription. ROSE allows latency-sensitive long-running applications (LRAs) to co-exist with computation-intensive batch jobs. Instead of waiting for resource allocation to be confirmed by the centralized scheduler, job managers in ROSE can independently request to launch speculative tasks within specific machines according to their suitability for oversubscription. Node agents of those\u00a0\u2026", "num_citations": "13\n", "authors": ["510"]}
{"title": "Enabling decision support for the delivery of real-time services\n", "abstract": " The domain of high assurance distributed systems has focused greatly on the areas of fault tolerance and dependability. As a result the paradigm of service orientated architectures (SOA) has been commonly applied to realize the significant benefits of loose coupling and dynamic binding. However, there has been limited research addressing the issues of managing real-time constraints in SOAs that are by their very nature dynamic. Although the paradigm itself is derived from fundamental principles of dependability, these same principles appear to not be applied when considering the timed dimension of quality of service. As a result the current state-of-the-art in SOA research only addresses soft real-time and does not seek to provide concrete guarantees about a systems performance. When a distributed system is deployed we do not understand enough the emerging behavior that will occur. This paper therefore\u00a0\u2026", "num_citations": "13\n", "authors": ["510"]}
{"title": "Towards a virtual integration design and analysis enviroment for automotive engineering\n", "abstract": " As the automotive industry moves towards reduced physical prototyping it is becoming more dependent on distributed simulations. However, the current technologies do not fully enable real-time distributed simulations which involve both virtual and physical components. This paper considers the current approaches to real-time distributed simulation and proposes the use of service-orientation. The highlights and current shortfalls of current research in real-time service orientation are then identified. Finally a key area of research is focused upon requiring a fundamental change of the understanding of quality of service and capability that is necessary to enable dependable real-time service orientated architectures.", "num_citations": "13\n", "authors": ["510"]}
{"title": "Enabling dynamic workflow for disaster monitoring and relief through service-oriented sensor networks\n", "abstract": " Natural and man-made disasters can significantly impact both people and environments. Sensor networks have the potential to revolutionize the capture, processing and communication of critical data for use of disaster rescue and relief [1]. In order to provide a dependable rescue capability through dynamically integrating newly developed and legacy sensor systems with other systems and computing, new methodologies are required for the dependable integration of services in heterogeneous environments. In this paper, we present a new architectural model which can proactively self-adapt to changes and evolution occurring in the provision of search and rescue capabilities in a dynamic environment. This approach has been demonstrated through developing and testing a demonstration system for a scenario of disaster area monitoring.", "num_citations": "13\n", "authors": ["510"]}
{"title": "Scenario-based design and evaluation for capability\n", "abstract": " Scenarios are frequently used within techniques for planning and designing systems. They are an especially helpful means of visualizing and understanding the incorporation of new systems within systems of systems. If used as the basis for decisions about candidate designs, then it is important that such decisions can be rationalized and quantitative assessment is particularly important. In this paper, an approach for developing complex scenarios, which incorporates the phases of systems development and deployment, is presented and a quantitative method of comparison is described. This approach is based on the development of measures of merit and measures of performance. The techniques are illustrated using cases that are relevant to Network Enabled Capability.", "num_citations": "13\n", "authors": ["510"]}
{"title": "Dependability in grids\n", "abstract": " The authors discuss why dependability is so important in Grid applications and demonstrate some of the work their team, the Distributed Systems and Services Group at the University of Leeds, is performing.", "num_citations": "13\n", "authors": ["510"]}
{"title": "Byzantine fault-tolerance in federated cloud computing\n", "abstract": " Cloud computing has emerged as popular paradigm that enables the establishment of large scale, flexible computing infrastructures that can offer significant cost savings for both businesses and consumers by allowing compute resources to be scaled dynamically to deal with current or anticipated usage [1]. This concept has been further strengthened with the emergence of federated computing Clouds that allow users to scale applications across multiple domains to meet Quality of Service targets [2]. However, the challenge of building dependable and robust Clouds remains a critical research problem that has not yet been clearly understood [3], and yet is vital for establishing user confidence in Clouds. This is particularly true when considering Byzantine faults that are arbitrary in nature. This paper analyses the application of Byzantine fault-tolerance to federated Clouds in detail, and presents experimentation\u00a0\u2026", "num_citations": "12\n", "authors": ["510"]}
{"title": "Monitoring resources allocation for service composition under different monitoring mechanisms\n", "abstract": " As availability of web service has become a great concern in SOA, monitoring mechanism is often deployed to detect and recover failures for service composition. While monitoring mechanism could improve the availability to an extent, it may cost more resources and increase the response time perceived by end users. To decrease the overall usage of monitoring resources, this paper proposes to select some services bringing the highest availability improvement to the composition and allocate monitors on them while leaving others unmonitored. This paper first researched two common monitoring mechanisms in service composition and analyzed their different impact on the service composition QoS values. Then continuous-time Markov chain and discrete-time Markov chain were employed to build the availability model related to the monitoring rate or service pool size according to different kind of monitoring\u00a0\u2026", "num_citations": "12\n", "authors": ["510"]}
{"title": "A dynamic shadow approach to fault-tolerant mobile agents in an autonomic environment\n", "abstract": " Large-scale distributed applications such as online information retrieval and collaboration over computational elements demand an approach to self-managed computing systems with a minimum of human interference. However, large scales and full distribution often lead to poor system dependability and security, and increase the difficulty in managing and controlling redundancy for fault tolerance. In particular, fault tolerance schemes for mobile agents to survive agent server crash failures in an autonomie environment are complex since developers normally have no control over remote agent servers. Some solutions inject a replica into stable storage upon its arrival at an agent server. But in the event of an agent server crash the replica is unavailable until the agent server recovers. In this paper we present a failure model and an exception handling framework for mobile agent systems. An exception\u00a0\u2026", "num_citations": "12\n", "authors": ["510"]}
{"title": "Order-preserving encryption using approximate integer common divisors\n", "abstract": " We present a new, but simple, randomised order-preserving encryption (OPE) scheme based on the general approximate common divisor problem (GACDP). This appears to be the first OPE scheme to be based on a computational hardness primitive, rather than a security game. This scheme requires only O(1) arithmetic operations for encryption and decryption. We show that the scheme has optimal information leakage under the assumption of uniformly distributed plaintexts, and we indicate that this property extends to some non-uniform distributions. We report on an extensive evaluation of our algorithms. The results clearly demonstrate highly favourable execution times in comparison with existing OPE schemes.", "num_citations": "11\n", "authors": ["510"]}
{"title": "An approach for modeling and ranking node-level stragglers in cloud datacenters\n", "abstract": " The ability of servers to effectively execute tasks within Cloud datacenters varies due to heterogeneous CPU and memory capacities, resource contention situations, network configurations and operational age. Unexpectedly slow server nodes (node-level stragglers) result in assigned tasks becoming task-level stragglers, which dramatically impede parallel job execution. However, it is currently unknown how slow nodes directly correlate to task straggler manifestation. To address this knowledge gap, we propose a method for node performance modeling and ranking in Cloud datacenters based on analyzing parallel job execution tracelog data. By using a production Cloud system as a case study, we demonstrate how node execution performance is driven by temporal changes in node operation as opposed to node hardware capacity. Different sample sets have been filtered in order to evaluate the generality of our\u00a0\u2026", "num_citations": "11\n", "authors": ["510"]}
{"title": "Personalised provenance reasoning models and risk assessment in business systems: A case study\n", "abstract": " As modern information systems become increasingly business- and safety-critical, it is extremely important to improve both the trust that a user places in a system and their understanding of the risks associated with making a decision. This paper presents the STRAPP framework, a generic framework that supports both of these goals through the use of personalised provenance reasoning engines and state-of-art risk assessment techniques. We present the high-level architecture of the framework, and describe the process of systematically modelling system provenance with the W3C PROV provenance data model. We discuss the business drivers behind the concept of personalizing provenance information, and describe an approach to enabling this through a user-adaptive system style. We discuss using data provenance for risk management and treatment in order to evaluate risk levels, and discuss the use of\u00a0\u2026", "num_citations": "11\n", "authors": ["510"]}
{"title": "Context-aware trust negotiation in peer-to-peer service collaborations\n", "abstract": " Service-oriented architecture (SOA) and Software as a Service (SaaS) are the latest hot topics to software manufacturing and delivering, and attempt to provide a dynamic cross-organisational business integration solution. In a dynamic cross-organisational collaboration environment, services involved in a business process are generally provided by different organisations, and lack supports of common security mechanisms and centralized management middleware. On such occasions, services may have to achieve middleware functionalities and achieve business objectives in a pure peer-to-peer fashion. As the participating services involved in a business process may be selected and combined at run time, a participating service may have to collaborate with multiple participating services which it has no pre-existing knowledge in prior. This introduces some new challenges to traditional trust management\u00a0\u2026", "num_citations": "11\n", "authors": ["510"]}
{"title": "Dual-Chord: a more effective distribute hash table\n", "abstract": " Dual-Chord:a More Effective Distribute Hash Table--\u300aJournal of Chinese Computer Systems\u300b2006\u5e74 \u300aJournal of Chinese Computer Systems\u300b 2006-08 Add to Favorite Get Latest Update Dual-Chord:a More Effective Distribute Hash Table ZHANG Hao, JIN Hai, NIE Jiang-wu, XU Jie, ZHANG Qin (Department of Computer Science and Engineering, Huazhong University of Science and Technology, Wuhan 430074, China) In the DHT based peer-to-peer network environment, the efficiency of searching \u3010Fund\u3011\uff1a \u56fd\u5bb6\u81ea\u7136\u79d1\u5b66\u57fa\u91d1\u91cd\u70b9\u9879\u76ee(60433040)\u8d44\u52a9 . \u3010CateGory Index\u3011\uff1a TP301.6 Download(CAJ format) Download(PDF format) CAJViewer7.supports all the CNKI file formats; AdobeReader only supports the PDF format. \u3010References\u3011 Chinese Journal Full-text Database 10 Hits 1 WANG Xing(School of Network Engineering,\u2026", "num_citations": "11\n", "authors": ["510"]}
{"title": "VMSA: a performance preserving online VM splitting and placement algorithm in dynamic cloud environments\n", "abstract": " Server consolidation schemes whereby each server is replaced with a virtual machine (VM) and multiple such VMs are run on a single physical server can reduce the number of physical servers needed, and in turn, both the cost and energy consumption in data centers. However, existing schemes have not fully exploited the flexibility in the usage and allocation of virtualization resources, so as to allow one application originally deployed on a single large VM (LVM) to be split and hosted by multiple smaller VMs (SVM). Using multiple SVMs instead of an LVM enables resource allocation at a smaller granularity and thus may further increase the utilization and reduce the number of physical servers. However, a major challenge to overcome when deploying multiple SVMs for one application is to preserve the performance of the application in terms of response delay. In this paper, we show through theoretical\u00a0\u2026", "num_citations": "10\n", "authors": ["510"]}
{"title": "D^ 2PS: a dependable data provisioning service in multi-tenant cloud environment\n", "abstract": " Software as a Service (SaaS) is a software delivery and business model widely used by Cloud computing. Instead of purchasing and maintaining a software suite permanently, customers only need to lease the software on-demand. The domain of high assurance distributed systems has focused greatly on the areas of fault tolerance and dependability. In a multi-tenant context, it is particularly important to store, manage and provision data services to customers in a highly efficient and dependable manner due to a large number of file operations involved in running such services. It is also desirable to allow a user group to share and cooperate (e.g., co-edit) on some specific data. In this paper we present a dependable data provisioning service in a multi-tenant Cloud environment. We describe a metadata management approach and leverage multiple replicated metadata caching to shorten the file access time, with the\u00a0\u2026", "num_citations": "10\n", "authors": ["510"]}
{"title": "Dynamic service integration for reliable and sustainable capability provision\n", "abstract": " The move towards network enabled capability (NEC) by the UK Ministry of Defence is designed to achieve enhanced military effect through the networking and coherent integration of existing and future resources including sensors, weapon systems and decision-makers to achieve a more flexible and responsive military. This article addresses the existing reliability and sustainability issues of large-scale military systems and proposes new architectural approaches of dynamic service integration for NEC to adapt to evolution occurring in services and capability for constructing next-generation software-intensive military systems. The reliability and performance of the proposed architectural approaches have been verified through modelling and simulation of service-oriented architecture for NEC and demonstrated through developing and testing a NEC system for a region surveillance capability scenario. The\u00a0\u2026", "num_citations": "10\n", "authors": ["510"]}
{"title": "Energy-aware fault-tolerant clustering scheme for target tracking wireless sensor networks\n", "abstract": " Over the last few years, the deployment of wireless sensor networks (WSNs) has been fostered in diverse applications. WSN has great potential for a variety of domains ranging from scientific experiments to commercial applications. Due to the deployment of WSNs in dynamic and unpredictable environments, these applications must have potential to cope with variety of faults. This paper proposes an energy-aware fault-tolerant clustering protocol for target tracking applications termed as the Fault-Tolerant Target Tracking (FTTT) protocol. The identification of redundant nodes (RNs) makes sensor node (SN) fault tolerance plausible and the clustering endorsed recovery of sensors supervised by a faulty cluster head (CH). The FTTT protocol intends two steps of reducing energy consumption: first, by identifying RNs in the network; secondly, by restricting the numbers of SNs sending data to the CH. Simulations validate\u00a0\u2026", "num_citations": "10\n", "authors": ["510"]}
{"title": "Towards building efficient content-based publish/subscribe systems over structured P2P overlays\n", "abstract": " In this paper, we introduce a generic model to deal with the event matching problem of content-based publish/subscribe systems over structured P2P overlays. In this model, we claim that there are three methods (event-oriented, subscription-oriented and hybrid) to make all the matched pairs (event, subscription) meet in a system. By theoretically analyzing the inherent problem of both event-oriented and subscription-oriented methods, we propose PEM (Popularity-based Event Matching), a variant of hybrid method. PEM can achieve better trade-off between event processing load and subscription storage load of a system. PEM has been verified through both mathematical and simulation-based evaluation.", "num_citations": "10\n", "authors": ["510"]}
{"title": "A scenario-based architecture evaluation framework for network enabled capability\n", "abstract": " The vision of service-oriented computing is one of loosely coupled services that create agile applications to encapsulate business objectives and processes. The potential of services to form complex systems of systems, with emergent behaviour, necessitates the need to understand how we can we develop sufficient confidence in their qualities. In this paper we argue that successful development and evolution of service oriented computing is dependent on making informed decisions at the architectural level. This poses a number of challenges concerning how to evaluate such system architectures and the measurements and metrics that are important in their assessment. This paper describes research-in-progress into the development of a scenario-based architectural evaluation framework that allows architectural-level reasoning across systems of systems in the context of Network Enabled Capability: a UK Ministry\u00a0\u2026", "num_citations": "10\n", "authors": ["510"]}
{"title": "Evolutionary service-oriented architecture for network enabled capability\n", "abstract": " The U.K. Ministry of Defence (MoD) aims to significantly enhance military effect through the networking of existing and future military capabilities, under the banner of Network Enabled Capability (NEC). To respond to this need, the EPSRC and BAE Systems are jointly funding the Network Enabled Capability Though Innovative Systems Engineering (NECTISE) project, which involves ten U.K. universities and is addressing the question of how BAE Systems delivers elements that contribute to NEC for its customers. One of the objectives of the NECTISE project is to develop a systematic approach that would lead to flexible service-oriented architectures for through-life evolution by investigating how loosely coupled services can be used to describe the functions and quality of service for heterogeneous systems and networks. In this paper, we present the concept of evolutionary service-oriented architecture (SOA) for NEC using agile methodologies to adapt to changes for the provision of dependable and sustainable military capability.", "num_citations": "10\n", "authors": ["510"]}
{"title": "Scenario Based Evaluation\n", "abstract": " The concept of a scenario has long been utilized in military procurement as a means of evaluating capability in an operational context. With the advent of initiatives such as the USA Department of Defence's Network-Centric Operations and the UK Ministry of Defence's Network Enabled Capability, research into the application of service-oriented architectures (SOA) as a means of delivering capability is being undertaken. A promising output of this research is the definition of scenario based evaluation models that can be applied to SOA. This paper examines the application of a military scenario framework to more conventional software evaluation situations by means of case studies. This demonstrates that similar techniques can be used in their construction and thus provides a basis for the application of scenario based evaluation models in future research.", "num_citations": "10\n", "authors": ["510"]}
{"title": "Towards a Content-Provider-Friendly Web Page Crawler.\n", "abstract": " Search engine quality is impacted by two factors: the quality of the ranking/matching algorithm used and the freshness of the search engine\u2019s index, which maintains a \u201csnapshot\u201d of the Web. Web crawlers capture web pages and refresh the index, but this is always a never-ending quest, as web pages get updated frequently (and thus have to be re-crawled). Knowing when to re-crawl a web page is fundamentally linked to the freshness of the index, given the size of the Web today and the inherent resource constraints: re-crawling too frequently leads to wasted bandwidth, recrawling too infrequently brings down the quality of the search engine.In this work, we address the scheduling problem for web crawlers, with the objective of optimizing the quality of the index (ie, maximize the freshness probability of the local repository as well as of the index). Towards this, we utilize feedback from the users (content providers) on when their web pages are updated and consider the entire spectrum of collaboration, from no feedback to explicit update schedules. We propose a unified online scheduling algorithm which utilizes different levels of collaboration from content providers. Extensive experiments with real web traces demonstrate that cooperation from users plays a major role in improving search engine index quality.", "num_citations": "10\n", "authors": ["510"]}
{"title": "Pedagogic data as a basis for web service fault models\n", "abstract": " This paper outlines our method for deriving fault models for use with our WS-FIT tool that can be used to assess the dependability of SOA. Since one of the major issues with extracting these heuristic rules and fault models is the availability of software systems we examine the use of systems constructed through pedagogic activities to provide one source of information.", "num_citations": "10\n", "authors": ["510"]}
{"title": "Gateway to the Cloud-Case Study: A Privacy-Aware Environment for Electronic Health Records Research\n", "abstract": " We describe a study in the domain of health informatics which includes some novel requirements for patient confidentiality in the context of medical health research. We present a prototype which takes health records from a commercial data provider, anonymises them in an innovative way and makes them available within a secure cloud-based Virtual Research Environment (VRE). Data anonymity is tailored as required for individual researchers' needs and ethics committee approval. VREs are dynamically configured to model each researcher's personal research environment while maintaining data integrity, provenance generation and patient confidentiality.", "num_citations": "9\n", "authors": ["510"]}
{"title": "The blind men and the elephant: Towards a software sustainability architectural evaluation framework\n", "abstract": " Software sustainability has been identified as one of the key challenges in the development of scientific and engineering software as we move towards new paradigms of research and computing infrastructures. However, it is suggested that sustainability is not well understood within the software engineering community and traditional software engineering methods do not support software sustainability, which can led to ineffective and inefficient efforts to address the concept or result in its complete omission from the software system. This paper considers how software sustainability can be addressed in the design and engineering process of software systems. The paper proposes that in order to achieve software sustainability it must be considered a composite, nonfunctional requirement, which can be reasoned over at an architectural level by building and evaluating appropriate architectural models.", "num_citations": "9\n", "authors": ["510"]}
{"title": "Systems Engineering for Business Process Change: new directions: collected papers from the EPSRC research programme\n", "abstract": " Systems Engineering for Business Process Change: New Directions is a collection of papers resulting from an EPSRC managed research programme set up to investigate the relationships between Legacy IT Systems and Business Processes. The papers contained in this volume report the results from the projects funded by the programme, which ran between 1997 and 2001. An earlier volume, published in 2000, reported interim results. Bringing together researchers from diverse backgrounds in Computer Science, Information Systems, Engineering and Business Schools, this book explores the problems experienced by IT-dependent businesses that have to implement changing business processes in the context of their investment in legacy systems. The book presents some of the solutions investigated through the collaborations set up within the research programme. Whether you are a researcher interested in the ideas that were generated by the research programme, or a user trying to understand the nature of the problems and their solutions, you cannot fail to be inspired by the writings contained in this volume.", "num_citations": "9\n", "authors": ["510"]}
{"title": "Real-time fault-tolerance in federated cloud environments\n", "abstract": " Dependability is a critical concern in provisioning services in Cloud Computing environments. This is true when considering reliability, an attribute of dependability that is a critical and challenging problem in a Cloud context [2]. Fault-tolerance is one means to attain reliability, and is typically implemented by using some form of diversity. Federated Cloud, which is an emerging Cloud paradigm that orchestrates multiple Clouds, is able to implement environmental diversity for Cloud applications with relative ease and minimal additional cost to the consumer due to its inherent design. Real-Time Applications (RTAs) can benefit from deploying fault-tolerant schemes to fulfill deadlines in the presence of faults as they enable the provisioning of correct service in the event of a component in the application failing. However, this diversity can potentially become an issue when designing dynamically scalable fault-tolerant\u00a0\u2026", "num_citations": "9\n", "authors": ["510"]}
{"title": "Migrating legacy assets through soa to realize network enabled capability\n", "abstract": " Network Enabled Capability (NEC) is the UK Ministry of Defence\u2019s aspiration to enhance the achievement of military effect through the networking of future and existing military capabilities. The NECTISE (NEC Through Innovative Systems Engineering) program responded to this need by investigating the question \u2018are you ready for NEC?\u2019 on behalf of equipment and service providers. Research work on this project proposed Service Oriented Architectures (SOA) as an architectural approach to delivering dependable and sustainable military capability. Specifically the work looked at how loosely coupled services could be used to expose and reuse functions and databases and how to describe the quality of service for heterogeneous systems and networks. The System of Systems that NEC will be realized from will not be implemented from scratch, but rather will be migrated from legacy assets over time. These\u00a0\u2026", "num_citations": "9\n", "authors": ["510"]}
{"title": "Agile properties of service oriented architectures for network enabled capability.\n", "abstract": " Network Enabled Capability (NEC) is the U.K. Ministry of Defence\u2019s response to the quickly changing conflict environment in which its forces must operate. In NEC, assets need to be integrated in context, to assist in human activity and provide dependable inter-operation. Different from traditional system engineering for the development of hardware equipments or software engineering for the development of software systems, NEC raises new challenges not only for service development on Service-Oriented Architecture (SOA), but also for dynamic discovery and integration of services in order to provide dependable and sustainable military capability. In this paper we introduce and discuss our research work in the NECTISE (NEC Through Innovative Systems Engineering) project with a focus on the use of agile methodologies to adapt to changes occurring in the process of service development, service discovery and service integration. We demonstrate through examples how to enable agile SOA for NEC.", "num_citations": "9\n", "authors": ["510"]}
{"title": "Dynamic data integration: A service-based broker approach\n", "abstract": " We address the problem of large-scale data integration, where the data sources are unknown at design time, are from autonomous organisations and may evolve. Experiments are described involving a demonstrator system in the field of health services data integration within the UK. Current web services technology has been used extensively and largely successfully in these distributed prototype systems. This paper shows that web services provide a good infrastructure layer, but integration demands a high-level 'broker' architectural layer. The first version of the demonstrator is mostly based on static linking. Lessons from this are extracted, and used to design and implement the current version, in which a more dynamic broker-based integration using service-oriented architecture, late binding and domain ontology is described.", "num_citations": "9\n", "authors": ["510"]}
{"title": "A fault-tolerant TCP scheme based on multi-images\n", "abstract": " The fault-tolerance of TCP is a key technology to guarantee the availability of services for servers. A fault-tolerant TCP scheme based on multi-images is discussed in this paper. Each TCP connection in this scheme exists two synchronous connection images and it is no need to backup the status of every TCP connection. It issues a module mechanism in the kernel of Linux and does not affect the software running on both client and server. It also guarantees each TCP connection be taken over seamlessly when it fails, transparent to the user. At the same time, it also tries to reduce the side effect to the system performance on the premise of ensuring the fault tolerance of each connection.", "num_citations": "9\n", "authors": ["510"]}
{"title": "Fiber optic coherent laser radar 3D vision system\n", "abstract": " Recent advances in fiber optic component technology and digital processing components have enabled the development of a new 3D vision system based upon a fiber optic FMCW coherent laser radar. The approach includes a compact scanner with no moving parts capable of randomly addressing all pixels. The system maintains the immunity to lighting and surface shading conditions which is characteristic of coherent laser radar. The random pixel addressability allows concentration of scanning and processing on the active areas of a scene, as is done by the human eye-brain system.", "num_citations": "9\n", "authors": ["510"]}
{"title": "Practical homomorphic encryption over the integers for secure computation in the cloud\n", "abstract": " We present novel homomorphic encryption schemes for integer arithmetic, intended primarily for use in secure single-party computation in the cloud. These schemes are capable of securely computing arbitrary degree polynomials homomorphically. In practice, ciphertext size and running times limit the polynomial degree, but this appears sufficient for most practical applications. We present four schemes, with increasing levels of security, but increasing computational overhead. Two of the schemes provide strong security for high-entropy data. The remaining two schemes provide strong security regardless of this assumption. These four algorithms form the first two levels of a hierarchy of schemes, and we also present the general cases of each scheme. We further elaborate how a fully homomorphic system can be constructed from one of our general cases. In addition, we present a variant based upon\u00a0\u2026", "num_citations": "8\n", "authors": ["510"]}
{"title": "ML-NA: A machine learning based node performance analyzer utilizing straggler statistics\n", "abstract": " Current Cloud clusters often consist of heterogeneous machine nodes, which can trigger performance challenges such as the task straggler problem, whereby a small subset of parallel tasks running abnormally slower than the other sibling ones. The straggler problem leads to extended job response and deteriorates system throughput. Poor performance nodes are more likely to engender stragglers, and can undermine straggler mitigation effectiveness. For example, as the dominant mechanism for straggler alleviation, speculative execution functions by creating redundant task replicas on other machine nodes as soon as a straggler is detected. When speculative copies are assigned onto the poor performance nodes, it is hard for them to catch up with the stragglers compared to replicas run on fast nodes. And due to the fact that the performance heterogeneity is caused not only by static attribute variations such as\u00a0\u2026", "num_citations": "8\n", "authors": ["510"]}
{"title": "Practical homomorphic encryption over the integers\n", "abstract": " We present novel homomorphic encryption schemes for integer arithmetic, intended for use in secure single-party computation in the cloud. These schemes are capable of securely computing only low degree polynomials homomorphically, but this appears sufficient for most practical applications. In this setting, our schemes lead to practical key and ciphertext sizes. We present a sequence of generalisations of our basic schemes, with increasing levels of security, but decreasing practicality. We have evaluated the first four of these algorithms by computing a low-degree inner product. The timings of these computations are extremely favourable. Finally, we use our ideas to derive a fully homomorphic system, which appears impractical, but can homomorphically evaluate arbitrary Boolean circuits.", "num_citations": "8\n", "authors": ["510"]}
{"title": "DIVIDER: modelling and evaluating real-time service-oriented cyberphysical co-simulations\n", "abstract": " The ability to reliably distribute simulations across a distributed system and seamlessly integrate them as a workflow regardless of their level of abstraction is critical to improving the quality of product manufacturing. This paper presents the DIVIDER architecture for managing and maintaining real-time performance simulations integrated through SOAs. The described approach captures features present in complex workflow patterns such as asynchronous arbitrary cycles and estimates the worst case execution time in the context of the interfering execution environment.", "num_citations": "8\n", "authors": ["510"]}
{"title": "Using Byzantine Fault-Tolerance to Improve Dependability in Federated Cloud Computing.\n", "abstract": " Computing Clouds are typically characterized as large scale systems that exhibit dynamic behavior due to variance in workload. However, how exactly these characteristics affect the dependability of Cloud systems remains unclear. Furthermore provisioning reliable service within a Cloud federation, which involves the orchestration of multiple Clouds to provision service, remains an unsolved problem. This is especially true when considering the threat of Byzantine faults. Recently, the feasibility of Byzantine Fault-Tolerance within a single Cloud and federated Cloud environments has been debated. This paper investigates Cloud reliability and the applicability of Byzantine Fault-Tolerance in Cloud computing and introduces a Byzantine fault-tolerance framework that enables the deployment of applications across multiple Cloud administrations. An implementation of this framework has facilitated in-depth experiments producing results comparing the reliability of Cloud applications hosted in a federated Cloud to that of a single Cloud.", "num_citations": "8\n", "authors": ["510"]}
{"title": "QoS driven web services evolution\n", "abstract": " The loose coupling and on-demand integration are the fundamental characteristics of the Service Oriented Architecture (SOA), which have enforced rapid development of Web services. However, nonfunctional quality of service (QoS) attributes may evolve due to the changes of network conditions and locations of the service users. Some real services may update their QoS properties on-the-fly, others may turn to unavailable. Thus, addressing the problem of uninformed QoS evolution of Web services has become a significant research issue. This paper proposes a dynamic evolution framework of Web services, which uses the Collaborative Filtering (CF) to predict the QoS values and enables the evolution of Web services. In this framework, the QoS values of current users can be predicted using the past QoS data of similar users. There is no extra Web services invocation. About 1.5 millions real-world QoS data are\u00a0\u2026", "num_citations": "8\n", "authors": ["510"]}
{"title": "Model checking of a target tracking protocol for wireless sensor networks\n", "abstract": " Dense collection of tiny Sensor Nodes (SNs), equipped with a variety of sensors are able to sense events, compute and communicate the estimations to the end user, form a distributed Wireless Sensor Network (WSN). The advances in WSNs have made it plausible to appraise critical aspects of object tracking WSNs that are continuous monitoring of object track and fault tolerance. This paper presents probabilistic performance evaluation of clustering, redundancy reduction, target tracking and fault tolerance phases of Fault-Tolerant Target Tracking (FTTT) protocol. This type of formal modeling with limited number of SNs helps to plan a scalable network with specific constraints related to the application, for example, object tracking quality, life time of the network, energy consumption, latency, QoS and network throughput.", "num_citations": "8\n", "authors": ["510"]}
{"title": "A P2P e-commerce reputation model based on fuzzy logic\n", "abstract": " Peer-to-Peer (P2P) networking is beneficial when removing a centralized server. On the other hand, new mechanisms are required to compensate for the central authority, especially for network security and dependability. In this paper, we propose a new fuzzy reputation (Fuzzy-Rep) model to improve security and dependability of P2P e-commerce. The model employs fuzzy logic inference rules to assess transactions through evaluating the direct trust, indirect reputation, peer's comprehensive reputation, rewards and penalty mechanisms which can resist vicious trust recommendation and fraud.", "num_citations": "8\n", "authors": ["510"]}
{"title": "Self-organization of autonomous peers with human strategies\n", "abstract": " Similarly to social networks where people are connected by their social relationships, two autonomous peer nodes can be connected in unstructured peer-to-peer (P2P) networks if users in those nodes are interested in each other's data. The similarity between P2P networks and social networks, where peer nodes are people and connections are relationships, leads us to believe that human strategies in social networks are useful for improving the performance of resource discovery by self-organising autonomous peers on unstructured P2P networks. In this paper, we present an efficient social-like peer-to-peer (ESLP) model for resource discovery by mimicking different human behaviours in social networks.", "num_citations": "8\n", "authors": ["510"]}
{"title": "Practical homomorphic encryption over the integers for secure computation in the cloud\n", "abstract": " We present novel homomorphic encryption schemes for integer arithmetic, intended primarily for use in secure single-party computation in the cloud. These schemes are capable of securely computing arbitrary degree polynomials homomorphically. In practice, ciphertext size and running times limit the polynomial degree, but this appears sufficient for most practical applications. We present four schemes, with increasing levels of security, but increasing computational overhead. Two of the schemes provide strong security for high-entropy data. The remaining two schemes provide strong security regardless of this assumption. These four algorithms form the first two levels of a hierarchy of schemes which require linearly decreasing entropy. We have evaluated these four algorithms by computing low-degree polynomials. The timings of these computations are extremely favourable by comparison with even the\u00a0\u2026", "num_citations": "7\n", "authors": ["510"]}
{"title": "Interface refactoring in performance-constrained web services\n", "abstract": " This paper presents the development of REF-WS an approach to enable a Web Service provider to reliably evolve their service through the application of refactoring transformations. REF-WS is intended to aid service providers, particularly in a reliability and performance constrained domain as it permits upgraded 'non-backwards compatible' services to be deployed into a performance constrained network where existing consumers depend on an older version of the service interface. In order for this to be successful, the refactoring and message mediation needs to occur without affecting functional compatibility with the services' consumers, and must operate within the performance overhead expected of the original service, introducing as little latency as possible. Furthermore, compared to a manually programmed solution, the presented approach enables the service developer to apply and parameterize\u00a0\u2026", "num_citations": "7\n", "authors": ["510"]}
{"title": "Data privacy management in a multi-organizational RFID authentication\n", "abstract": " The growing popularity of Radio Frequency Identification (RFID) systems cannot be overlooked due to their wide range of application areas. However, some RFID applications face security and privacy threats due to the exposure of tags over wider distances. There are security methods but these can lack security and scalability when authenticating RF tags in multi-organizational RFID system. RFID systems operating within a single domain have only security and privacy issues due to the security method on tags, but in a multi-organizational system there are even more security and privacy issues in the system architecture rather than just in the security method on tag side. In this paper, we have designed a broker service to EPC Network for secure and confidential data management of the RFID data events. Secure and confidential authentication broker service is an extension to the EPC network architecture for a\u00a0\u2026", "num_citations": "7\n", "authors": ["510"]}
{"title": "Hierarchical neural network based product quality prediction of industrial ethylene pyrolysis process\n", "abstract": " A two-layer hierarchical neural network is proposed to predict the product qualities of an industrial KTI GK-V ethylene pyrolysis process. The first layer of the model is used to classify these changes into different operating conditions. In the second layer, the process under each operating condition is modeled using bootstrap aggregated neural networks (BANN) with sequential training algorithm. The overall output is obtained by combining all the trained networks. Results of application to the actual process show that the proposed soft-sensing model possesses good generalization capability.", "num_citations": "7\n", "authors": ["510"]}
{"title": "Multi-party authentication for web services: Protocols, implementation and evaluation\n", "abstract": " The Web service technology allows the dynamic composition of a workflow (or a business flow) by composing a set of existing Web services scattered across the Internet. While a given Web service may have multiple service instances taking pan in several workflows simultaneously, a workflow often involves a set of service instances that belong to different Web services. In order to establish trust relationships amongst service instances, new security protocols are urgently needed. Hada and Maruyabma [2002] presented a session-oriented, multi-party authentication protocol to resolve this problem. Within a session their protocol provides a commonly shared session secret for all the service instances, thereby distinguishing the instances from those of other sessions. However, individual instances cannot be distinguished and identified using the session secret. This leads to vulnerable session management and poor\u00a0\u2026", "num_citations": "7\n", "authors": ["510"]}
{"title": "A new comparison-based scheme for multiprocessor fault tolerance\n", "abstract": " In this paper a system-level, comparison-based scheme for multiprocessor fault tolerance is described. Unlike other strategies which have been proposed in the literature, the new approach has shorter diagnosis period and response period, and it needs significantly fewer number of tests to attain fault tolerance. The strategy is also shown to be able to sequentially identify the set of faulty processors in a system, provided the system is connected. Several important characterization theorems are developed for t-fault tolerable replicating sets and for the sequential diagnosis goal. Two canonical and concrete system classes are introduced, which suggests an attractive alternative to present fault tolerance techniques in multiprocessor systems.", "num_citations": "7\n", "authors": ["510"]}
{"title": "Mitigating stragglers to avoid QoS violation for time-critical applications through dynamic server blacklisting\n", "abstract": " The straggler problem is one of the most challenging issues toward rapid and predictable response time for applications in cluster infrastructures, leading to potential QoS violation and late-timing failure. Straggler tasks occur due to reasons such as resource contention, hardware heterogeneity, etc., and become severe with increased system scale and complexity. Speculative execution and blacklisting are the major two straggler tolerant techniques, but each has its own limitations. The former creates replica task to catch up with the identified straggler, but normally with no selection toward nodes when deciding where to launch the backup. Ignoring server performance hinders the speculation success rate. The latter typically relies on manual configuration, despite the fact that the ability of nodes to effectively execute tasks changes over time. In addition, the misidentification of weak-performance nodes decreases\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Uncover the ground-truth relations in distant supervision: A neural expectation-maximization framework\n", "abstract": " Distant supervision for relation extraction enables one to effectively acquire structured relations out of very large text corpora with less human efforts. Nevertheless, most of the prior-art models for such tasks assume that the given text can be noisy, but their corresponding labels are clean. Such unrealistic assumption is contradictory with the fact that the given labels are often noisy as well, thus leading to significant performance degradation of those models on real-world data. To cope with this challenge, we propose a novel label-denoising framework that combines neural network with probabilistic modelling, which naturally takes into account the noisy labels during learning. We empirically demonstrate that our approach significantly improves the current art in uncovering the ground-truth relation labels.", "num_citations": "6\n", "authors": ["510"]}
{"title": "A framework and task allocation analysis for infrastructure independent energy-efficient scheduling in cloud data centers\n", "abstract": " Cloud computing represents a paradigm shift in provisioning on-demand computational resources underpinned by data center infrastructure, which now constitutes 1.5% of worldwide energy consumption. Such consumption is not merely limited to operating IT devices, but encompasses cooling systems representing 40% total data center energy usage. Given the substantive complexity and heterogeneity of data center operation spanning both computing and cooling components, obtaining analytical models for optimizing data center energy-efficiency is an inherently difficult challenge. Specifically, difficulties arise pertaining to the non-intuitive relationship between computing and cooling energy in the data center, computationally complex energy modeling, as well as cooling models restricted to a specific class of data center facility geometry - all of which arise from the interdisciplinary nature of this research domain\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Mitigate data skew caused stragglers through ImKP partition in MapReduce\n", "abstract": " Speculative execution is the mechanism adopted by current MapReduce framework when dealing with the straggler problem, and it functions through creating redundant copies for identified stragglers. The result of the quicker task will be adopted to improve the overall job execution performance. Although proved to be effective for contention caused stragglers, speculative execution can easily meet its bottleneck when mitigating data skew caused stragglers due to its replication nature: the identical unbalanced input data will lead to a slow speculative task. The Map inputs are typically even in size according to the HDFS block configuration, therefore the skew caused stragglers happen mainly in the Reduce phase because of the unknown intermediate key distribution. In this paper, we focus on mitigating data skew caused Reduce stragglers, propose ImKP, an Intermediate Key Pre-processing framework that\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "n-dimensional QoS framework for real-time service-oriented architectures\n", "abstract": " Service-Orientation has long provided an effective mechanism to integrate heterogeneous systems in a loosely coupled fashion as services. However, with the emergence of Internet of Things (IoT) there is a growing need to facilitate the integration of real-time services executing in non-controlled, non-real-time, environments such as the Cloud. With the need to integrate both cyberphysical systems as hardware-in-the-loop (HIL) components and also with Simulation as a Service (SIMaaS) the execution performance and response-times of the services must be managed. This paper presents a mathematical framework that captures the relationship between the host execution environment and service performance allowing the estimation of Quality of Service (QoS) under dynamic Cloud workloads. A formal mathematical definition is provided and this is evaluated against existing techniques from both the Cloud and\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "PROV-TE: A provenance-driven diagnostic framework for task eviction in data centers\n", "abstract": " Cloud Computing allows users to control substantial computing power for complex data processing, generating huge and complex data. However, the virtual resources requested by users are rarely utilized to their full capacities. To mitigate this, providers often perform over-commitment to maximize profit, which can result in node overloading and consequent task eviction. This paper presents a novel framework that mines the huge and growing historical usage data generated by Cloud data centers to identify the causes of overloads. Provenance modelling is applied to add contextual meaning to the data, and the PROV-TE diagnostic framework provides algorithms to efficiently identify the causality of task eviction. Using simulation to reflect real world scenarios, our results demonstrate a precision and recall of the diagnostic algorithms of 83% and 90% respectively. This demonstrates a high level of accuracy of the\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Tolerating transient late-timing faults in cloud-based real-time stream processing\n", "abstract": " Real-time stream processing is a frequently deployed application within Cloud datacenters that is required to provision high levels of performance and reliability. Numerous fault-tolerant approaches have been proposed to effectively achieve this objective in the presence of crash failures. However, such systems struggle with transient late-timing faults - a fault classification challenging to effectively tolerate - that manifests increasingly within large-scale distributed systems. Such faults represent a significant threat towards minimizing soft real-time execution of streaming applications in the presence of failures. This work proposes a fault-tolerant approach for QoS-aware data prediction to tolerate transient late-timing faults. The approach is capable of determining the most effective data prediction algorithm for imposed QoS constraints on a failed stream processor at run-time. We integrated our approach into Apache\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "A method for private car transportation dispatching based on a passenger demand model\n", "abstract": " Although the demand for taxis is increasing rapidly with the soaring population in big cities, the number of taxis grows relatively slowly during these years. In this context, private transportation such as Uber is emerging as a flexible business model, supplementary to the regular form of taxis. At present, much work mainly focuses on the reduction or minimization of taxi cruising miles. However, these taxi-based approaches have some limitations in the case of private car transportation because they do not fully utilize the order information available from the new type of business model. In this paper we present a dispatching method that reduces further the cruising mileage of private car transportation, based on a passenger demand model. In particular, we partition an urban area into many separate regions by using a spatial clustering algorithm and divide a day into several time slots according to the statistics\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Restructuring web service interfaces to support evolution\n", "abstract": " This paper presents an overview of a scheme (RESWS) to enable Web Service providers to be able to evolve their service interface in a non-backwards compatible way and still maintain compatibility with existing consumers. The need for a non-passive approach to evolving services while still remaining backwards compatible is highlighted and presented based on a clear need identified within the literature. Based on graph rewriting theory, the set of refactorings was formally represented as transformations through rewriting rules and enables a set of preconditions to be defined for each transformation. A demonstrator has been developed to implement the RES-WS scheme as a message mediator which interprets the chain of primitive refactoring transformations required to carry out a particular complex transformation. Experimental validation was performed to demonstrate the feasibility and effectiveness of the\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Realizing network enabled capability through dependable dynamic systems integration\n", "abstract": " The move towards Network Enabled Capability (NEC) by the UK Ministry of Defence (MoD) is designed to achieve enhanced military effect through the networking and coherent integration of existing and future resources including sensors, weapon systems, and decision makers to achieve greater agility in the prosecution of military operations. One of the many challenges of realizing NEC is engineering systems to cope with change in a highly dynamic mobile environment. This paper presents our experience in realizing NEC through a proof-of-concept software demonstrator. It adopts an architectural approach, based on the principles of Service-Oriented Architecture (SOA), to deliver dependable, dynamic systems integration in the context of surveillance capability coping with variable availability of static and mobile sensors. Consideration of the delivery of NEC indicates that SOA can support evolutionary\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Quality is in the eye of the beholder: towards user-centric web-databases\n", "abstract": " The proliferation of database-driven web sites (or web-databases) has brought upon a plethora of applications where both Quality of Service (QoS) and Quality of Data (QoD) are of paramount importance to the end users. In our previous work, we have proposed Quality Contracts, a comprehensive framework for specifying multiple dimensions of QoS/QoD; we have also developed user-centric admission control and scheduling algorithms in web databases, whose goal is to maximize overall system performance. In this work, we turn our attention to the user side of the equation. Specifically, we propose to demonstrate how the adaptation of Quality Contracts (QCs) by the users can lead to vastly different performance results, both from the user point of view (ie, user satisfaction) and also from the system point of view. Towards this, we propose to structure our demo in the form of an interactive game, where participants\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Service oriented architectures in the delivery of capability\n", "abstract": " Service oriented architecture (SOA) is becoming established in computing as a means to integrate processing and access data across organisations. In general architecture terms, services are used to loosely couple assets in systems by describing service interfaces at a high level of abstraction. The abstract service descriptions are independent of the different service implementations, which are instantiated by the assets. Dynamic integration of assets can be captured using the service interface descriptions, which promotes loose coupling by only \u2018binding\u2019to service implementations when needed. For Network Enabled Capability (NEC), services can be integrated to provide executable capability. Service descriptions need to be rich enough to enable mission planning and evaluation of architecture. The descriptions also need to be high-level enough to give the means to evalute and improve the dependability of supplied services by making assets interchangeable and evolvable to fulfill continuous delivery of capability. This paper proposes that extending service descriptions in SOA in terms of functional and non-functional attributes of assets can improve dependability in achieving delivery of capability.", "num_citations": "6\n", "authors": ["510"]}
{"title": "Virtualising Visualisation: A distributed service based approach to visualisation on the Grid\n", "abstract": " Context: Current visualisation systems are not designed to work with the large quantities of data produced by scientists today, they rely on the abilities of a single resource to perform all of the processing and visualisation of data which limits the problem size that they can investigate. Objectives: The objectives of this research are to address the issues encountered by scientists with current visualisation systems and the deficiencies highlighted in current visualisation systems. The research then addresses the question:\u201d How do you design the ideal service oriented architecture for visualisation that meets the needs of scientists?\u201d Method: A new design for a visualisation system based upon a Service Oriented Architecture is proposed to address the issues identified, the architecture is implemented using Java and web service technology. The implementation of the architecture also realised several case study scenarios as demonstrators. Evaluation: Evaluation was performed using case study scenarios of scientific problems and performance data was conducted through experimentation. The scenarios were assessed against the requirements for the architecture and the performance data against a base case simulating a single resource implementation. Conclusion: The virtualised visualisation architecture shows promise for applications where visualisation can be performed in a highly parallel manner and where the problem can be easily sub-divided into chunks for distributed processing.", "num_citations": "6\n", "authors": ["510"]}
{"title": "Wide angular aperture lithium niobate acousto-optic Bragg cells\n", "abstract": " A new design for a wide angular aperture Lithium Niobate acousto-optic Bragg cell is proposed. Parallel tangents, beam steering, and an acousto-optic interaction in a rotated 60 degree(s)-YZ plane are used simultaneously, which enhances the product of acceptance angle, bandwidth, and diffraction efficiency to be larger than that of isotropic acousto-optic Bragg cells by more than one order of magnitude. The analysis and optimum design procedure for acoustic beam steering in birefringent acousto-optic diffraction with the parallel tangents condition fulfilled is established and is essential for obtaining the maximum value of the above mentioned product. The acousto-optic interaction in the rotated YZ plane is analyzed and is important to obtain high diffraction efficiency for most cases where the parallel tangents condition is used. The optimal design of a wide angular aperture Lithium Niobate device working at 514\u00a0\u2026", "num_citations": "6\n", "authors": ["510"]}
{"title": "Hawk: Rapid android malware detection through heterogeneous graph attention networks\n", "abstract": " Android is undergoing unprecedented malicious threats daily, but the existing methods for malware detection often fail to cope with evolving camouflage in malware. To address this issue, we present Hawk, a new malware detection framework for evolutionary Android applications. We model Android entities and behavioral relationships as a heterogeneous information network (HIN), exploiting its rich semantic meta-structures for specifying implicit higher order relationships. An incremental learning model is created to handle the applications that manifest dynamically, without the need for reconstructing the whole HIN and the subsequent embedding model. The model can pinpoint rapidly the proximity between a new application and existing in-sample applications and aggregate their numerical embeddings under various semantics. Our experiments examine more than 80,860 malicious and 100,375 benign\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Perphon: A ML-based agent for workload co-location via performance prediction and resource inference\n", "abstract": " Cluster administrators are facing great pressures to improve cluster utilization through workload co-location. Guaranteeing performance of long-running applications (LRAs), however, is far from settled as unpredictable interference across applications is catastrophic to QoS [2]. Current solutions such as [1] usually employ sandboxed and offline profiling for different workload combinations and leverage them to predict incoming interference. However, the time complexity restricts the applicability to complex co-locations. Hence, this issue entails a new framework to harness runtime performance and mitigate the time cost with machine intelligence: i) It is desirable to explore a quantitative relationship between allocated resource and consequent workload performance, not relying on analyzing interference derived from different workload combinations. The majority of works, however, depend on offline profiling and\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "A unified framework with multi-source data for predicting passenger demands of ride services\n", "abstract": " Ride-hailing applications have been offering convenient ride services for people in need. However, such applications still suffer from the issue of supply-demand disequilibrium, which is a typical problem for traditional taxi services. With effective predictions on passenger demands, we can alleviate the disequilibrium by pre-dispatching, dynamic pricing or avoiding dispatching cars to zero-demand areas. Existing studies of demand predictions mainly utilize limited data sources, trajectory data, or orders of ride services or both of them, which also lacks a multi-perspective consideration. In this article, we present a unified framework with a new combined model and a road-network-based spatial partition to leverage multi-source data and model the passenger demands from temporal, spatial, and zero-demand-area perspectives. In addition, our framework realizes offline training and online predicting, which can satisfy\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "A Service-Oriented Co-Simulation: Holistic Data Center Modelling Using Thermal, Power and Computational Simulations\n", "abstract": " Holistic modelling of a data center to include both thermodynamics and computational processes has the potential to revolutionize how data centers are designed and managed. Such a model is inherently multi-disciplinary, bringing together the computational elements studied by computer scientists; thermodynamics studied by mechanical engineers; and other aspects in the domain of electrical engineering. This paper proposes the use of the Internet of Simulation to allow engineers to build models of individual complex elements and deploy them as simulation services. These services can then be integrated as simulation system workflows. A proof of concept server simulation is presented, incorporating simulations of CPUs, heat sinks, and fans exposed using the Simulation as a Service (SIMaaS) paradigm. The integrated workflow of the server is then exposed as a service (WFaaS) to facilitate the building of an\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Worldwide universities network (WUN) web observatory: Applying lessons from the web to transform the research data ecosystem\n", "abstract": " The ongoing growth in research data publication supports global intra-disciplinary and inter-disciplinary research collaboration but the current generation of archive-centric research data repositories do not address some of the key practical obstacles to research data sharing and re-use, specifically: discovering relevant data on a global scale is time-consuming; sharinglive'and streaming data is non-trivial; managing secure access to sensitive data is overly complicated; and, researchers are not guaranteed attribution for re-use of their own research data. These issues are keenly felt in an international network like the Worldwide Universities Network (WUN) as it seeks to address major global challenges. In this paper we outline the WUN Web Observatory project's plan to overcome these obstacles and, given that these obstacles are not unique to WUN, we also propose an ambitious, longer-term route to their solution\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "A demonstration of a service oriented virtual environment for complex system analysis\n", "abstract": " Distributed virtual simulation is increasingly in demand within the automotive industry. A distributed and networked approach to system level design and simulation stands to benefit from a unifying relational oriented modeling and simulation framework. This will permit innovative use of existing independent simulations for increased concurrency in design and verification and validation. This paper demonstrates an analysis of the vehicle as a complex system through the combination of a relational framework, high level syntax and semantics for representing models and distributed simulation. This promises to provide a rigorous, traceable and agile approach to conceptual vehicle design and analysis.", "num_citations": "5\n", "authors": ["510"]}
{"title": "Fault-tolerant dynamic deduplication for utility computing\n", "abstract": " Utility computing is an increasingly important paradigm, whereby computing resources are provided on-demand as utilities. An important component of utility computing is storage, data volumes are growing rapidly, and mechanisms to mitigate this growth need to be developed. Data deduplication is a promising technique for drastically reducing the amount of data stored in such system systems, however, current approachs are static in nature, using an amount of redundancy fixed at design time. This is inappropriate for truly dynamic modern systems. We propose a real-time adaptive deduplication system for Cloud and Utility computing that monitors in real-time for changing system, user, and environmental behaviour in order to fulfill a balance between changing storage efficiency, performance, and fault tolerance requirements. We evaluate our system through simulation, with experimental results showing that our\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Enhancing grid security using quantum key distribution\n", "abstract": " Quantum Key Distribution (QKD) is a secure key distribution technology, which provides information theoretic or unconditional security. BBN DARPA quantum network and SECOQC network of secrets are the examples of such networks. Research is also in progress for the integration of QKD with the protocols in different layers of OSI model. Integration of QKD in point-to-point protocol (PPP) OSI layer 2 and with IPSEC at OSI layer-3 are the examples of such research efforts. All these steps are leading towards the utilization of QKD technology for enhancing the security of modern computing applications on the Internet. This paper presents a model for the exploitation of QKD security networks in high performance distributed computing applications, such as grid computing.", "num_citations": "5\n", "authors": ["510"]}
{"title": "A framework for improving trust in dynamic service-oriented systems\n", "abstract": " Large-scale data processing systems frequently require users to make timely and high-value business decisions based upon information that is received from a variety of heterogeneous sources. Such heterogeneity is especially true of service-oriented systems, which are often dynamic in nature and composed of multiple interacting services. However, in order to establish user trust in such systems, there is a need to determine the validity and reliability of all the data sources that go into the making of a decision. This paper analyses the concept of provenance and discusses how the establishment of personalized provenance recording and retrieval systems can be used to increase the utility of data and engender user trust in complex service-based systems. An overview of current provenance research is presented, and a real-world project to address the abstract concepts of trust and data quality in industrial and\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Achieving dependability in service-oriented systems\n", "abstract": " Service-orientation is a useful means of developing highly flexible and adaptive software systems, and it is a paradigm that has been increasingly adopted into Grids and Clouds. However, service-oriented architectures and designs also bring with them new challenges in the fields of dependability and security that need to be addressed carefully in order to provide sufficient support to enable service-oriented systems to offer non-trivial Quality of Service guarantees. In this paper we examine such challenges and introduce several advanced techniques developed at the University of Leeds to achieve dependability and security in service-oriented systems and applications. These techniques include schemes for achieving fault tolerance, privacy protection, and dynamic authentication, as well as a method for assessing dependability based on fault-injection. We describe and discuss each technique alongside\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Evolution of social models in peer-to-peer networking: Towards self-organising networks\n", "abstract": " In social networks, people can directly contact some acquaintances that potentially have knowledge about the resources they are looking for. Similarly to social networks, where people are connected by their social relationships, two autonomous peer nodes can be connected in unstructured peer-to-peer (P2P) networks if users in those nodes are interested in each other's data. The similarity between P2P networks and social networks, where peer nodes can be considered as people and connections can be considered as relationships, makes it possible to use social models to improve the performance of resource discovery in P2P networks. In this paper, evolution of social models in P2P networking is systematically investigated with a focus on utilising self-organisation to improve the performance of resource discovery in large-scale P2P networks.", "num_citations": "5\n", "authors": ["510"]}
{"title": "The long tail of loop distance for broadband over power lines: Finding a new niche for rural telecommunications in brazil\n", "abstract": " Broadband over power lines (BPL) is considered an attractive broadband delivery system because it can reach more homes than coaxial cable systems or telephone lines. In markets not served by DSL or cable, BPL can provide a cost-effective connectivity solution. Also known as \"third wire\" technology, BPL offers applications to rural and remote areas in attractive modes like: (1) high speed backbone over medium voltage power lines; (2) last mile access network through residential power distribution; (3) last inch access to distribute data communication inside home. Considering the fact that network deployment costs can be reduced if signals are transmitted in an existing infrastructure, current status of BPL can be evaluated as an option to ICT provision in rural and remote areas. This study presents a model to position BPL among other access technologies, considering its applications to Brazilian rural areas.", "num_citations": "5\n", "authors": ["510"]}
{"title": "Motivations for change within an enduring architecture\n", "abstract": " Network Enabled Capability (NEC) has presented the systems research community with a number of research problems, both technical and social. Our research into the understanding of systems architecture has driven a more detailed understanding of problems associated with the realization of the architectural aspects of NEC. In this paper, we pay particular attention to evolving architectures in an NEC context, and the properties defined by an enduring architecture. This paper examines the application of a staged life-cycle to the properties identified by earlier work as critical for an enduring architecture. In addition, we use this life-cycle to taxonomize our understanding of the motivations for change in systems. We finally, look at a case study of free and open-source development as an example of a distributed development community for networked systems that can help us understand properties of and problems\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Dynamic Cross-Realm Authentication for Multi-Party Service Interactions\n", "abstract": " Modern distributed applications are embedding an increasing degree of dynamism, from dynamic supply-chain management, enterprise federations, and virtual collaborations to dynamic service interactions across organisations. Such dynamism leads to new security challenges. Collaborating services may belong to different security realms but often have to be engaged dynamically at run time. If their security realms do not have in place a direct cross-realm authentication relationship, it is technically difficult to enable any secure collaboration between the services. A typical solution to this is to locate at run time intermediate realms that serve as an authentication-path between the two separate realms. However, the process of generating an authentication path for two distributed services can be very complex. It could involve a large number of extra operations for credential conversion and require a long chain of\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Replication-Aware Query Processing in Large-Scale Distributed Information Systems.\n", "abstract": " In this work, we address the problem of replica selection in distributed query processing over the Web, in the presence of user preferences for Quality of Service and Quality of Data. In particular, we propose RAQP, which stands for Replication-Aware Query Processing. RAQP uses an initial statically-optimized logical plan, and then selects the execution site for each operator and also selects which replica to use, thus converting the logical plan to an executable plan. Unlike prior work, we do not perform an exhaustive search for the second phase, which allows RAQP to scale significantly better. Extensive experiments show that our scheme can provide improvements in both query response time and overall quality of QoS and QoD as compared to random site allocation with iterative improvement.", "num_citations": "5\n", "authors": ["510"]}
{"title": "Securing instance-level interactions in web services\n", "abstract": " The Web service technology enables dynamic service composition, resource utilisation and application integration in a heterogeneous computing environment. Web services can be used to compose and perform flexible and complex business flows. In practice, a Web service may create multiple service instances working for different business flows or business sessions, whilst the service instances within a business session may be created by different Web services, often designed, implemented and maintained by different organisations across different security domains. This introduces new challenges to existing security systems and solutions. For many applications ensuring security only at the level of Web services is not enough for a fine-grained level of control for multi-party collaborations because interactions amongst Web services in fact happen at the level of service instances. In this paper, we address the\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "Assessing multi-version systems through fault injection\n", "abstract": " Multi-version design (MVD) has been proposed as a method for increasing the dependability, of critical systems beyond current levels. However, a major obstacle to large-scale commercial usage of this approach is the lack of quantitative characterizations available. We seek to help answer this problem using fault injection. This approach has the potential for yielding highly useful metrics with regard to MVD systems, as well as giving developers a greater insight into the behaviour of each channel within the system. In this research, we develop an automatic fault injection system for multi-version systems called FITMVS. We use this si,stem to test a multi-version system, and then analyze the results produced. We conclude that this approach can yield useful metrics, including metrics related to channel sensitivity, code scope sensitivity, and the likelihood of common-mode failure occurring within a system.", "num_citations": "5\n", "authors": ["510"]}
{"title": "Influence of catalyst and exhaust system on particulate deposition and release from an IDI diesel passenger car under real world driving\n", "abstract": " The influence of a diesel oxidation catalyst and a practical exhaust system with two silencers on the storage and release of participates during cold start real world driving was investigated using a Ford 1.8 litre IDI Mondeo diesel passenger car. Particulates were sampled simultaneously at three points in the exhaust using an on-board gravimetric filter paper method. The test was carried out on two different on-road driving cycles: a simulated ECE 15 cycle to represent free moving low power city driving conditions, and a traffic jam and high speed suburban driving cycle. The results showed that the particulate matter was deposited in the oxidation catalyst during cold start and deposited in the exhaust system downstream of the catalyst throughout the test period. The particulate deposition and release downstream of the catalyst were influenced by the previous operational history of the vehicle. The exhaust pipe\u00a0\u2026", "num_citations": "5\n", "authors": ["510"]}
{"title": "HARDWARE AND SOFTWARE FAULT TOLERANCE: DEFINITION AND EVALUATION OF ADAPTIVE ARCHITECTURES IN A\n", "abstract": " This paper discusses the issue of providing tolerance to both hardware and software faults by defining several hybrid-fault-tolerant architectures, which can co-exist and work simultaneously at the top of the supporting environment, and introduces a systematic method for evaluating their dependability, efficiency and response time. To address general-purpose distributed systems where multiple unrelated applications may compete for system resources, our architectural solutions have an important concern with adaptation in the use of redundancy according to system conditions.", "num_citations": "5\n", "authors": ["510"]}
{"title": "Experimental Evaluation of Fault-Tolerant Mechanisms for Object-Oriented Software\n", "abstract": " This paper describes the implementation of reusable fault-tolerant mechanisms in C++(an object-oriented language) and Open C++(a reflective version of C++) that support the realization of dependable software systems. The target environment for the experiment is a distributed system consisting of multiple SPARC workstations connected through TCP/IP. Testing based on software fault injection is performed and performance-related analysis is provided. Both experimental and analytic results in this paper show that the object-oriented approach with the design of metaobject protocols is particularly promising for the development of computing systems with high dependability requirements.", "num_citations": "5\n", "authors": ["510"]}
{"title": "TOPOSCH: Latency-Aware Scheduling Based on Critical Path Analysis on Shared YARN Clusters\n", "abstract": " Balancing resource utilization and application QoS is a long-standing research topic in cluster resource management. Big data YARN clusters need to co-schedule diverse workloads on shared resources including batch processing jobs, streaming jobs, and other long-running applications such as web services, database services, etc. Current resource managers are only responsible for resource allocation among applications/jobs but completely unaware of runtime QoS requirements of interactive and latency-sensitive applications. Prior works to maximize the QoS of monolithic applications ignore inherent dependencies and temporal-spatio performance variability of components, characteristics of distributed applications primarily driven by microservices. In this paper, we present Toposch, a new resource management system to adaptively co-locate batch tasks and microservices by harvesting runtime latency. In\u00a0\u2026", "num_citations": "4\n", "authors": ["510"]}
{"title": "Mechanism for controlled server overallocation in a datacenter\n", "abstract": " A method of controlling a datacenter (1), for example a cloud datacenter, and a computer management system (3) for managing a datacenter comprising a plurality of servers (2) configured to execute a plurality of jobs is provided. The method comprises receiving a request to execute a job from a user (4), determining an allocated server (2) on which to execute the job, and executing the job on the allocated server (2). The determining the allocated server (2) on which to execute the job comprises: classifying the job according to its resource requirement, selecting a subset of the servers (2) that fulfill the resource requirements of the job, determining the allocated server (2) that can execute the job with a favorable energy efficiency, wherein a total resource estimate for all jobs running on the server (2) from time to time exceeds the resources of at least one of the servers (2). Accordingly, the datacenter (1) may over\u00a0\u2026", "num_citations": "4\n", "authors": ["510"]}
{"title": "Cloud Robotics: A Distributed Computing View\n", "abstract": " As an interdiscipline of distributed computing and robots, cloud robotics concerns augmenting robot capabilities by connecting them to the powerful backend cloud computing infrastructure. It is a field of great potential, and most recent discussions on this topic are from the point of view of robotics. In this paper, we discuss this field mainly from the aspect of distributed and cloud computing, i.e., \u201cwhat distributed computing technologies can contribute to cloud robotics?\u201d and \u201cwhat challenges does cloud robotics bring to distributed computing?\u201d This paper also presents our early experience towards a cloud robotic software infrastructure which is based on the newly-emerged edge computing model and supports the direct deployment of existing ROS (Robot Operating System) packages.", "num_citations": "4\n", "authors": ["510"]}
{"title": "Holistic data centres: next generation data and thermal energy infrastructures\n", "abstract": " Digital infrastructure is becoming more distributed and requiring more power for operation. At the same time, many countries are working to de-carbonise their energy, which will require electrical generation of heat for populated areas. What if this heat generation was combined with digital processing?", "num_citations": "4\n", "authors": ["510"]}
{"title": "Exploratory analysis of quality practices in open source domain\n", "abstract": " Software quality assurance has been a heated topic for several decades, but relatively few analyses were performed on open source software (OSS). As OSS has become very popular in our daily life, many researchers have been keen on the quality practices in this area. Although quality management presents distinct patterns compared with those in closed-source software development, some widely used OSS products have been implemented. Therefore, quality assurance of OSS projects has attracted increased research focuses. In this paper, a survey is conducted to reveal the general quality practices in open source communities. Exploratory analysis has been carried out to disclose those quality related activities. The results are compared with those from closed-source environments and the distinguished features of the quality assurance in OSS projects have been confirmed. Moreover, this study suggests potential directions for OSS developers to follow.", "num_citations": "4\n", "authors": ["510"]}
{"title": "An Improved Back Propagation Neural Network Model and Its Application.\n", "abstract": " t-Stroke is one of the most serious disease, and the incidence rate of stroke is confirmed to be related to environmental factors including temperature, pressure and humidity. In order to obtain the relationship between the incidence rate and environmental factors, we research on local daily meteorological data and stroke disease cases from January 2008 to December 2012, which is provided by the administrative department of public health and medical institutions statistics in China, then build the improved BPNN (Back propagation neural network) model to carry out data analysis and processing, obtain the weight matrix between them. It can be seen that the relationship between incidence rate and pressure is the highest degree from the value of weight matrix, and pressure is positive correlation with the incidence rate. The relationship between the temperature and incidence rate is second, and they are negative correlation. The incidence between average relative humidity and correlation is quite small. The results show that the model can be used to predict the future stroke incidence rate under various meteorological conditions, and it can play a certain role in making disease knowledge popular and providing a reference to potential patients.", "num_citations": "4\n", "authors": ["510"]}
{"title": "Provenance: Current directions and future challenges for service oriented computing\n", "abstract": " Modern organizations increasingly depend heavily on information stored and processed in distributed, heterogeneous data sources and services to make critical, high-value decisions. Service-oriented systems are dynamic in nature and are becoming ever more complex systems of systems. In such systems, knowing how data was derived is of significant importance in determining its validity and reliability. To address this, a number of advocates and theorists postulate that provenance is critical to building trust in data and the services that generated it as it provides evidence for data consumers to judge the integrity of the results. This paper provides an overview of provenance research with an emphasis on its application in the domain of service-oriented computing. The goal of this paper is not to provide an exhaustive survey of the provenance literature but rather to highlight key work, themes, challenges and issues\u00a0\u2026", "num_citations": "4\n", "authors": ["510"]}
{"title": "A survey of personal privacy protection in public service mashups\n", "abstract": " Mashups are web application hybrids built from usually independent online services and information sources, commonly for purposes that differ from the original reasons those services were developed in the first place. The intent is usually to make data more useful, often through visualisation, combination, aggregation or the application of distributed expertise. Mashups rely on APIs to leverage content or functionality. APIs come in many varieties and mashup development is often considered an exercise in spaghetti deployment. This means that mashups are not always well-engineered; that they're often brittle; inconsistent; difficult to maintain; and susceptible to unexpected changes at the whim of service and data providers. Mashups are nevertheless important in the light of growing trends in open government and government-community partnership initiatives such as the UK's Big Society, which aim to maximise\u00a0\u2026", "num_citations": "4\n", "authors": ["510"]}
{"title": "A Practical Model For Conceptual Comparison Using A Wiki\n", "abstract": " One of the key concerns in the conceptualisation of a single object is understanding the context under which that object exists (or can exist). This contextual understanding should provide us with clear conceptual identification of an object including implicit situational information and detail of surrounding objects. For example in learning terms, a learner should be aware of concepts related to the context of their field of study and a surrounding cloud of contextually related concepts. This paper explores the use of an evolving community maintained knowledge-base (that of Wikipedia) in order to prioritise concepts that are semantically relevant to the user's interest space.", "num_citations": "4\n", "authors": ["510"]}
{"title": "Investigation of research towards efficient social peer-to-peer networks\n", "abstract": " Peer-to-peer (P2P) networks attract attentions worldwide with their great success in file sharing networks (e.g. Napster, Gnutella, BitTorrent, and Kazaa). In the last decade, numerous studies have been devoted to the problem of resource discovery in P2P networks. However, efficient resource discovery remains a key challenge for large-scale P2P networks. An investigation of existing research on resource discovery towards efficient social P2P networks will be given in this paper.", "num_citations": "4\n", "authors": ["510"]}
{"title": "Modelling and simulation for e-social science: current progress\n", "abstract": " This paper reports on the progress that has been made in the Modelling and Simulation for e-Social Science (MoSeS) project at the University of Leeds. This project seeks to use e-Science techniques to develop a national demographic model and simulation of the UK population specified at the level of individuals and households. The architecture of MoSeS is presented and discussed, with particular attention paid to the security requirements faced by the project. The demographic and forecasting models that are used by the project are introduced and discussed, as are the large storage requirements of the project. As part of this discussion, the initial development of a Storage Resource Broker storage facility is presented. The MoSeS portal is then introduced, and initial results of running the demographic and forecasting models are presented and analysed. The paper concludes with a discussion on the future work that will be undertaken as part of this ongoing e-Science project.", "num_citations": "4\n", "authors": ["510"]}
{"title": "A practical approach to secure Web services\n", "abstract": " Web services provide the potential to offer interoperability of distributed business-to-business application integration between autonomous organisations, regardless of platforms, operating systems or languages. For both user and vendor organisations, this raises immediate problems of trust, security, privacy and prevention of malicious attacks. Until these problems are addressed and solved properly, the use of Web services will be severely restricted because no-one will trust them. We describe in this paper a service-oriented architecture and an attack-tolerant information retrieval (ATIR) service which tackles certain classes of privacy problems. In particular, we address the problem of protecting a user against malicious attacks upon an information service when the user retrieves some information from the service. Although there have been many theoretical solutions to certain aspects of this problem, the results\u00a0\u2026", "num_citations": "4\n", "authors": ["510"]}
{"title": "Centres of Excellence: Research Institute in Software Evolution, University of Durham\n", "abstract": " The term software maintenance is often used simply to refer to fixing bugs in released code. However, the most maintenance is actually about enhancing functionality. Software that is unsuccessful will not require maintenance. Maintenance is associated with success, and is inevitable for successful software. It often represents a substantial revenue stream for the vendor organisation. It was this sort of thinking that encouraged a group of software engineering academics at Durham to set up the Centre for Software Maintenance in March 1987. By 1999, two significant factors were influencing the work of the Centre. Firstly its success was bringing expansion. This inevitably meant that the scope of our interests was broadening. Moreover, our focus began to change to address problems such as how to construct new software that is very easy to enhance. Evolution now expressed much more clearly where the heart of the\u00a0\u2026", "num_citations": "4\n", "authors": ["510"]}
{"title": "Hardware and Software fault tolerance: adaptive architectures in distributed computing environments\n", "abstract": " This paper discusses the issue of providing tolerance to hardware and software faults in distributed computing environments as well as issues related to efficiency and flexibility. A set of new fault-tolerant architectures is presented, and a detailed dependability analysis of these architectures is performed together with an efficiency and response time evaluation. The proposed architectural solutions are designed mainly for general-purpose distributed computing systems where many unrelated applications could compete for both hardware and software resources, thereby exhibiting highly varying and dynamic system characteristics. Stress is thus placed on adaptation\u2014a major feature of the architectures under consideration is to attempt the adaptive execution of redundant components so as to minimize hardware resource consumption and shorten the response time, as much as possible, for a required level of fault tolerance. The analytical results show that adaptive architectures are able to make the efficient use of available resources without compromising dependability, and moreover, for certain application environments they would respond with lower probability of violating given timing constraints than static architectures.", "num_citations": "4\n", "authors": ["510"]}
{"title": "Responsive Roll-Forward Recovery in Embedded Real-Time Systems\n", "abstract": " Roll-forward checkpointing schemes [Long et al. 1990; Pradhan and Vaidya 1992] are developed in order to avoid rollback in the presence of independent faults and increase the possibility that a task completes within a tight deadline. Despite of the adoption of roll-forward recovery, these schemes are not necessarily appropriate for time-critical applications because interactions with the external environment and communications between processes must be deferred during checkpoint validation steps (typically, two checkpoint intervals) until the fault-free processors are identified. The deadlines on providing services may thus be violated. In this paper we present and discuss two alternative roll-forward recovery schemes, especially for time-critical and interaction-intensive applications, that deliver correct, timely results even when checkpoint validation is required.", "num_citations": "4\n", "authors": ["510"]}
{"title": "Synchronous n-step method for independent q-learning in multi-agent deep reinforcement learning\n", "abstract": " Experience replay memory (ERM) is an effective tool for sampling decorrelated data to train the policy network and improving data utilization in off-policy deep reinforcement learning. However, ERM introduces fluctuations in the training processes of independent Q-learning (IQL) in multi-agent deep reinforcement learning (MA-DRL) because its stored experiences may become obsolete as the agents in IQL update their policies in parallel. Reducing the influence of obsolete experiences while extensively exploring the potential of data in training poses a huge challenge in IQL. Therefore, we propose the synchronous n-step method that totally eliminates obsolete experiences but suffers from the low data utilization problem. Then, we propose the ERM-helped synchronous n-step method that makes a balance between reducing the influence of obsolete experience and enhancing data utilization. We apply these\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "A scalable lnternet-of-Vehicles service over joint clouds\n", "abstract": " Since the Internet of Vehicles (IoV) technology has recently attracted huge research attention, IoV services that can collect, process data and further provision services are increasingly becoming the mainstream. Considering the process efficiency, geo-distributed data is typically collected and exploited on different Clouds, making it significantly essential for IoV application to be deployed on multiple Clouds whilst system components still function well and jointly work. In this paper, we provide a scalable IoV system deployment in the joint Cloud environment where cloud vendors collaboratively cooperate as an alliance. In particular, system components are independently deployed in accordance with the data placement and resource capacities etc. A multi-replication mechanism is utilized to achieve the cross-cloud parallel processing, thereby effectively handling the scalability issues in the massive-scale vehicle data\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "FENet: An SDN-based scheme for virtual network management\n", "abstract": " Virtual networking is vital to efficient resource management in Clouds, and it is in fact one of the main services provided by many Cloud Computing platforms. Virtual network management needs to meet specific requirements, including tenant isolation and adaption to virtual machines' lifecycle. Most of the existing schemes for virtual network management are based on the use of overlay networks in order to achieve a desirable degree of flexibility. However, these schemes suffer from a common limit, i.e. relatively high performance penalty due to a complicated forwarding process. We address this performance concern by developing a new management scheme, FENet, which makes use of Software-Defined Networks (SDN) to create virtual networks and manage them via the SDN controller programs. We present the design of an SDN controller, with the definition of flow entry rules based on the OpenFlow protocol\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "Trust and risk relationship analysis on a workflow basis: A use case\n", "abstract": " Trust and risk are often seen in proportion to each other; as such, high trust may induce low risk and vice versa. However, recent research argues that trust and risk relationship is implicit rather than proportional. Considering that trust and risk are implicit, this paper proposes for the first time a novel approach to view trust and risk on a basis of a W3C PROV provenance data model applied in a healthcare domain. We argue that high trust in healthcare domain can be placed in data despite of its high risk, and low trust data can have low risk depending on data quality attributes and its provenance. This is demonstrated by our trust and risk models applied to the BII case study data. The proposed theoretical approach first calculates risk values at each workflow step considering PROV concepts and second, aggregates the final risk score for the whole provenance chain. Different from risk model, trust of a workflow is derived by applying DS/AHP method. The results prove our assumption that trust and risk relationship is implicit.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Risk assessment and trust in services computing: Applications and experience\n", "abstract": " Service-orientation is effective at managing complexity and dynamicity at a programmatic level, but there is still much work to be done in understanding and improving the trust that users place in a system's outputs, and the extent to which they understand the associated risks of decisions recommended by a system. This is crucial if we are to improve the uptake and real-world effectiveness of service-based decision-support systems whilst also reducing the risks (both perceived and actual) of using such systems. This paper presents the current progress of the STRAPP project, which is designing and engineering novel trust and risk assessment mechanisms for services computing and applying these to a number of real-world service-based decision-support systems. A new layered architecture model for trust and risk is introduced and described in detail, and we present our state-of-the-art work in risk-assessment\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "Evaluating the dependability of dynamic binding in web services\n", "abstract": " Service-Oriented Computing (SOC) provides a flexible framework in which applications are built up from services, often distributed across a network. One of the promises of SOC is that of Dynamic Binding where abstract consumer requests are bound to concrete service instances at runtime. What is clear from existing research is that there exist several components that help to provide the necessary behavior for dynamic binding. However, the focus of these works is on the evaluation of the implementation of dynamic binding and does not consider an evaluation of dynamic binding systems themselves. To remedy this, we propose new system and fault models for Dynamic Binding in SOC that incorporate the types of components required for a Dynamic Binding System (DBS) and the types of fault that can affect these components. In addition to these models, we introduce a novel evaluation framework for the testing of\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "Improved Eavesdropping Detection in Quantum Key Distribution\n", "abstract": " Employing the fundamental laws of quantum physics, Quantum Key Distribution (QKD) promises the unconditionally secure distribution of cryptographic keys. However, in practical realisations, a QKD protocol is only secure, when the quantum bit error rate introduced by an eavesdropper unavoidably exceeds the system error rate. This condition guarantees that an eavesdropper cannot disguise his presence by simply replacing the original transmission line with a less faulty one. Unfortunately, this condition also limits the possible distance between the communicating parties, Alice and Bob, to a few hundred kilometers. To overcome this problem, we design a QKD protocol which allows Alice and Bob to distinguish system errors from eavesdropping errors. If they are able to identify the origin of their errors, they can detect eavesdropping even when the system error rate exceeds the eavesdropping error rate. To achieve this, the proposed protocol employs an alternative encoding of information in two-dimensional photon states. Errors manifest themselves as quantum bit and as index transmission errors with a distinct correlation between them in case of intercept-resend eavesdropping. As a result, Alice and Bob can tolerate lower eavesdropping and higher system error rates without compromising their privacy.", "num_citations": "3\n", "authors": ["510"]}
{"title": "A Certificateless and Across Administrative Domains Authenticated Key Exchange Scheme for E-payment.\n", "abstract": " E-payment scheme allows two users to securely exchange e-cash and digital product over an open network. A problem in the across administrative domains E-payment scenarios is how the participants can carry out the exchange between administrative domains. In other words, the participants are administrated by two trusted administrators respectively. How can they verify their identities each other? In this paper, a certificateless cross-domain authenticated key exchange (CL-CD-AKE) scheme was proposed to solve this problem, and the security and the effectiveness of the proposed CL-CD-AKE scheme were analyzed in the extended random oracle model. Following this work, an E-payment scheme, achieving unforgeability and unreusability of e-cash, customer anonymity and fairness, was then proposed, and the CL-CD-AKE scheme was adopted by the E-payment scheme to deal with the problem of cross-domain authentication and key agreement.", "num_citations": "3\n", "authors": ["510"]}
{"title": "A dynamic service pool size configuration mechanism for service-oriented workflow\n", "abstract": " Service-oriented workflows are the fundamental structures in service-oriented applications and changes in the workflow could cause dramatic changes in system reliability. In several ways to re-heal workflows in execution, re-sizing service pools in the workflow is practical and easy to implement. In order to quickly adjust to workflow or environmental changes, this paper presents a dynamic service pool size configuration mechanism from the point of view of maintaining workflow reliability. An architecture-based reliability model is used to evaluate the overall reliability of a workflow with service pools and an optimal method is proposed to get the combination of service pool size aiming at minimizing the sum of service pool size subject to the workflow reliability requirement. A case study is used to explain this method and experiment results show how to change service pool size to meet the workflow reliability\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "An Intrusion Diagnosis Perspective on Cloud Computing\n", "abstract": " Cloud computing is an emerging paradigm with virtual machine as its enabling technology. As with any other Internet-based technology, security underpins widespread success of Cloud computing. However, Cloud computing introduces new challenges with respect to security mainly due to the unique characteristics inherited via virtual machine technology. In this chapter, we focus on the challenges imposed on intrusion diagnosis for Clouds due to these characteristics. In particular, we identify the importance of intrusion diagnosis problem for Clouds and the novel challenges for intrusion diagnosis for Clouds. Also, we propose a solution to address these challenges and demonstrate the effectiveness of the proposed solution with empirical evaluation.", "num_citations": "3\n", "authors": ["510"]}
{"title": "An architecture for social simulation models to support spatial planning\n", "abstract": " We present an architecture for grid-enabled simulation modeling. The use of grid resources provides access to the substantial data storage and processing power which are necessary to translate such models from computational tools into genuine planning aids. As well as providing access to virtualized compute resources, the architecture allows customized applications to meet the needs of an array of potential user organizations.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Research on Strategic Partnering in Construction Project [J]\n", "abstract": " Partnering is a new project management mode. It breaks the adversarial relationship exited in the traditional mode between the parties and tries to establish a circumstance, where the resources can flow freely and be allocated satisfying. The definition and establishment process of the partnering alliance are introduced. Then a further analysis on the definition and establishment are presented. The critical factors that affect the partnering are found out. The advisement to some extent is given.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Dependability analysis of web services\n", "abstract": " Web Services form the basis of the web based eCommerce eScience applications so it is vital that robust services are developed. Traditional validation and verification techniques are centred around the concept of removing all faults to guarantee correct operation whereas Dependability gives an assessment of how dependably a system can deliver the required functionality by assessing attributes, and by eliminating threats via means attempts to improve dependability. Fault injection is a well-proven dependability assessment method. Although much work has been done in the area of fault injection and distributed systems in general, there appears to have been little research carried out on applying this to middleware systems and Web Services in particular. There are additional problems associated with applying existing fault injection technologies to Web Services running in a virtual machine environment since most are either invasive or work at a machine level. The Fault Injection Technology (FIT) method has been devised to address these problems for middleware systems. The Web Service-Fault Injection Technology (WS-FIT) implementation applies the FIT method, based on network level fault injection, to Web Services to create a non-invasive dependability assessment method. It allows targeted perturbation of Web Service RFC parameters as well as more traditional network level fault injection operations. The WS-FIT tool includes taxonomies that define a system under test, fault models to apply and failure modes to be detected, and uses these taxonomies to generate fault injection campaigns. WS-FIT has been applied to a number of\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "The e-Demand project: a summary'\n", "abstract": " The e-Demand project is a recently completed, three year joint collaborative e-Science project between the Universities of Durham and Leeds, together with experts from industry (Sun Microsystems, Sharp, and Sparkle Computer Technology). The goal of e-Demand has been to investigate fundamental dependability issues underlying large-scale coordinated resource sharing and problem solving in dynamic, multi-institutional virtual organisations, as well as to provide support for stereoscopic visualisation amongst e-Science applications. Services have been developed to facilitate attack-tolerant information retrieval systems, the provision of multi-version-design based fault tolerant web services, fault injection testing of Grid software and web services, and stereoscopic visualisation applications. This paper summarises the achievements of the project, and describes the services that have been developed in more detail, together with details of planned future work.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Provenance-Aware Fault Tolerance for Grid Computing\n", "abstract": " \u2022 The three Yorkshire Universities\u2019 project (started in 2001, over\u00a3 10M investment and research projects) http://www. wrgrid. org. uk/\u2022 Involves Leeds (Profs K Brodlie, PM Dew & J Xu), York (Prof J Austin), and Sheffield (Profs G Tomlinson & P Fleming); under the guidance of the Chief Executive of WRUC (Dr Julian White\u2013CEO of WRUC)", "num_citations": "3\n", "authors": ["510"]}
{"title": "Practical Dependability Analysis of SOAP Based Systems\n", "abstract": " This paper provides a detailed example of our FIT (Fault Injection Technology) package. We show how our GUI driven package can be applied to a complex SOAP based system, generate test scripts from WSDL and detect defects in this system. Finally we discuss how our fault injection tools can be used as part of a specification based certification process.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Sharing with Limited Trust: An Attack Tolerance Service in Durham e-Demand Project\n", "abstract": " The unique characteristics of the Grid pose significant new security challenges that demand for new solutions. This paper argues that only limited trust should be placed in the grid environment. The trust relationship among grid nodes may be valid only within the lifetime of a submitted job. We focus on two key security challenges centred on the trust issue: protecting the intention (privacy) of users against untrusted nodes and detecting job tampering against malicious attacks. We propose to use an attack-Tolerant private Information Retrieval (TIR) scheme to address the above problems. A generic implementation framework is presented for implementing a TIR service in a distributed database environment. Experimental results show that incorrect results reconstructed from corrupted data can be detected with a probability arbitrarily close to one, thus masked from the user. Our current implementation exhibits good performance: for example the service takes less than 32% extra processing time to reconstruct a correct result in the presence of two malicious servers from a total of five servers, in comparison with normal situations. The total processing time takes much less than one second even in the presence of malicious attacks.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Object-Oriented Construction of Fault-Tolerant Software\n", "abstract": " Software fault tolerance is often necessary, but itself can be dangerously error-prone because of the additional effort that must be involved in the programming process. Adding redundancy to programs may increase the size and complexity and thus adversely affect software reliability. Object-oriented programming provides a particularly appropriate framework, based on the theory of abstract data types, for enforcing reliability and controlling complexity. This paper introduces an approach to achieving software fault tolerance in object-oriented systems in a disciplined manner and with reduced cost, and discusses the problem of how to incorporate fault tolerance into concurrent object-oriented computing.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Society diagnosis. IV. Local reasoning, global reasoning and diagnosis algorithms\n", "abstract": " For pt. III see IEEE ISMVL-88, p. 113-7 (1988). The diagnosis process is divided into two stages: local reasoning, which transforms the person-event-person model into an ordinary person-to-person model, and global reasoning, which gives the final solution. Theorems for t/(x),(t/(x))/sub 1/, and (t/(x))/s-diagnosability are derived, and an algorithm for generating a syndrome with complexity O (n* mod R mod) and a global diagnosis algorithm with complexity O (n/sup 2/+ n* mod E mod) are introduced. The results indicate that social diagnosis is possible.<>", "num_citations": "3\n", "authors": ["510"]}
{"title": "General solution of multifrequency acousto-optic diffraction in Bragg cells\n", "abstract": " A general method for calculating the scattering amplitude of multifrequency acousto-optic diffraction is established. The method is based on counting allowable Feynman diagrams. It is found that the ratio of the number of Feynman diagrams allowable in the Bragg regime (isotropic, birefringent, and degenerate) to that in the Raman-Nath regime is independent of the total number of different acoustic frequencies, being a function only of the order of the Feynman diagram and the diffraction order of the final state. A general expression for this ratio is obtained. With this as a basis, complete perturbation solutions of the scattering amplitude can be obtained for any final state, any number of acoustic frequencies, and any kind of multifrequency acousto-optic diffraction. The theory is verified by comparing with theoretical results obtained previously and with experiment results.", "num_citations": "3\n", "authors": ["510"]}
{"title": "Calculation of effective acoustooptic coefficient by abbreviated subscript formalism\n", "abstract": " In designing a new type of acoustooptic (AO) device, one must calculate the effective AO coefficient p many times for many different AO interaction configurations (different acoustic wave propagation directions, polarizations, whether longitudinal or shear wave, and different polarization directions of the incident and diffracted light waves). The standard expression for p is in tensor form and is inconvenient for practical calculations. It is well extablished that, for quantities containing a pair of symmetrical tensor subscripts, practical calculations are simplified and can be carried out by matrix\u2013vector (M\u2013V) multiplications if quantities with abbreviated subscripts are used. In this paper a new abbreviated-subscript expression for p is obtained. With this expression, p can be calculated for any AO interaction configuration by two M\u2013V multiplications and one vector\u2013vector multiplication. In our experience, the compact notation\u00a0\u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "Three-valued system diagnosis and parallel recovery\n", "abstract": " A generalized three-valued diagnostic model is proposed. A description is given of t1/t1/2-diagnosability with a parallel recovery strategy. The results show clearly that t-1/t-1/2-diagnosis with parallel recovery can effectively shorten the period of recovery time from faults and improves availability of a class of fault-tolerant computer systems, as compared with t-fault diagnosis under the same structural constraint.<>", "num_citations": "3\n", "authors": ["510"]}
{"title": "University of Southampton\n", "abstract": " CiteSeerX \u2014 University of Southampton, Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA University of Southampton, Cached Download as a PDF Download Links [users.ecs.soton.ac.uk] Save to List Add to Collection Correct Errors Monitor Changes by Paul Townend , Paul Groth , Nik Looker , Jie Xu Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract and Computer Science, Keyphrases computer science Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The Pennsylvania State University \u2026", "num_citations": "3\n", "authors": ["510"]}
{"title": "Parallel Interactive Networks for Multi-Domain Dialogue State Generation\n", "abstract": " The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Software\u2010Defined Fog Orchestration for IoT Services\n", "abstract": " This chapter presents a scalable software\u2010defined orchestration architecture to intelligently compose and orchestrate thousands of heterogeneous Fog appliances (devices, servers). Specifically, it provides a resource filtering\u2010based resource assignment mechanism to optimize the resource utilization and fair resource sharing among multitenant Internet of things (IoT) applications. The chapter also presents a component selection and placement mechanism for containerized IoT microservices to minimize the latency by harnessing the network uncertainty and security while considering different applications\u2019 requirement and capabilities. It describes a fog simulation scheme to simulate the aforementioned procedure by modeling the entities, their attributes, and actions. The chapter also provides the results of practical experiences on the orchestration and simulation. It outlines numerous difficulties and\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Filtering Inconsistent Failure in Robot Collective Decision with Blockchain\n", "abstract": " Robotic swarm is one of typical and significant distributed systems. Fault tolerance is an important and essential ability for such systems because individual robot may malfunction and even maliciously attack, thereby interfering with reaching consensus on a decision at the swarm level. One kind of fault which is difficult to deal with is called inconsistent failure (ie. Byzantine failure). It is shown as that a robot exhibits different behavior at different times to other members of the swarm. Some previous work have proved that its feasible to exclude the interference of consistent failures via blockchain technology. We expanded the work to solve the problem in situation with inconsistent failures. In this paper, we proposed a method to filter inconsistent failures with blockchain in a collective decision task of robot swarm. The basic idea is to use the blockchain to record the behaviors of the robots, and then automatically audit\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Distributed computing in cyber-physical intelligence: Robotic perception as an example\n", "abstract": " Intelligent cyber-physical systems, such as robots, are emerging computing devices that autonomously and directly interact with the physical world. The new characteristics of these devices motivate further research questions different from those addressed in traditional computing technology. Based on an in-depth investigation of the relationship between a typical example, robotic perception, and distributed computing, this paper tries to explore the challenges and opportunities brought by intelligent cyber-physical systems to distributed computing. The preliminary answers to three questions are given: \"Why should we introduce distributed computing into cyber-physical intelligence\", \"What kind of distributed architecture can contribute to cyber-physical intelligence\" and \"What challenges brought by intelligent cyber-physical systems to distributed computing infrastructure?\" A multi-scale hybrid distributed architecture\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Context-aware location annotation on mobility records through user grouping\n", "abstract": " Due to the increasing popularity of location-based services, a massive volume of human mobility records have been generated. At the same time, the growing spatial context data provides us rich semantic information. Associating the mobility records with relevant surrounding contexts, known as the location annotation, enables us to understand the semantics of the mobility records and helps further tasks like advertising. However, the location annotation problem is challenging due to the ambiguity of contexts and the sparsity of personal data. To solve this problem, we propose a Context-Aware location annotation method through User Grouping (CAUG) to annotate locations with venues. This method leverages user grouping and venue categories to alleviate the data sparsity issue and annotates locations according to multi-view information (spatial, temporal and contextual) of multiple granularities\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "A detailed analysis of kmb09 qkd protocol\n", "abstract": " Employing the fundamental laws of quantum physics, Quantum Key Distribution (QKD) achieves the unconditionally secure distribution of cryptographic keys. Its security arises from the fact that an eavesdropper unavoidably introduces the so-called quantum bit errors. If these errors exceed system errors, his presence can be detected and the key transmission can be aborted. Otherwise, an eavesdropper could disguise his presence as system errors after replacing some of the equipments of the communicating parties. This condition for the security of QKD protocols limits the possible communication distance in practical implementations. In this paper, we propose to significantly increase the communication distance of QKD by using protocols with not only one but two different types of eavesdropping errors, such that eavesdropping and system errors do not affect the communication in different ways. This makes it possible to detect eavesdropping, even when the system errors exceed the eavesdropping errors. In order to illustrate this, we analyse the KMB09 protocol [Khan et al., New J Phys. 11 2009 063043] with only two-dimensional photon states and calculate the Quantum Bit Error Rate (QBER), the Index Transmission Error-Rate (ITER) and the efficiencies with and without eavesdropping of this protocol.", "num_citations": "2\n", "authors": ["510"]}
{"title": "SMTP: An optimized storage method for vehicle trajectory data exploiting trajectory patterns\n", "abstract": " Recent advances in location-acquisition and mobile sensing technologies have enabled tracking of vehicle movements (i.e., trajectory data). Massive trajectory datasets are processed routinely (often in real-time) to provide support for many new types of IoV (Internet of Vehicles) applications (e.g., traffic congestion management, and load-coordination across electric vehicle charging stations). High-volume, high-velocity data emitted by IoV applications introduces issues with efficient spatial and temporal queries over massively redundant datasets, typically represented as a collection of longitude-latitude tuples. In this paper we present SMTP, a new storage method based on the recognition of trajectory patterns to reduce the storage space for the trajectory data. An adaptive algorithm for mining trajectory patterns from the data is developed, and it recognizes frequent trajectories as patterns according to the geo-space\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "M-VCR: Multi-View Consensus Recognition for Real-Time Experimentation\n", "abstract": " A major application area in the computer vision domain is gesture recognition, requiring real-time image classification to respond to human interactions. However, current state-of-the-art high-quality algorithms for image classification do not meet many dynamic real-time requirements. This paper presents the development of M-VCR - a novel approach for improving the reliability of real-time image classification. M-VCR increases the quality of classifications under real-time constraints through the adoption of fast classification algorithms, although these algorithms individually produce lower quality results, utilisation under a 'consensus' approach can achieve results equivalent to those of much higher-quality algorithms. The proposed approach also allows for different algorithms to be utilised in parallel, building on the fault tolerance technique of N-versioning. A significant improvement in image classification is\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Application of PROV Model for Modeling a VM Overload Mitigating Strategy: Task Eviction\n", "abstract": " Provenance can be a very important aspect for mass-computation and large-scale computer systems. Cloud environment with its dynamic and scalable nature can benefit from using provenance to better support its management. In this paper, we have demonstrated the application of PROV, a W3C standard, to understand the behavior of Google Cloud from its one-month log data. Its potential contribution to the implementation of task eviction strategy for Virtual Machine overload mitigation will be discussed.", "num_citations": "2\n", "authors": ["510"]}
{"title": "A Graph-based approach to address trust and reputation in ubiquitous networks\n", "abstract": " The increasing popularity of virtual computing environments such as Cloud and Grid computing is helping to drive the realization of ubiquitous and pervasive computing. However, as computing becomes more entrenched in everyday life, the concepts of trust and risk become increasingly important. In this paper, we propose a new graph-based theoretical approach to address trust and reputation in complex ubiquitous networks. We formulate trust as a function of quality of a task and time required to authenticate agent-to-agent relationship based on the Zero-Common Knowledge (ZCK) authentication scheme. This initial representation applies a graph theory concept, accompanied by a mathematical formulation of trust metrics. The approach we propose increases awareness and trustworthiness to agents based on the values estimated for each requested task, we conclude by stating our plans for future work in this\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "e-Science\u2013towards the cloud: infrastructures, applications and research\n", "abstract": " The 2011 e-Science All Hands Meeting (AHM) marked the tenth annual gathering of technologists and scientists first brought together by the UK e-Science Programme\u2014the\u00a3 250 million research venture funded by the UK Research Councils. In the decade since the founding of this programme in 2001, e-science research has evolved from fundamental research focused on computational grids into work involving research from a wide variety of domains, such as bioinformatics, computing, astronomy, physics and medicine. The AHM community is now multinational, interdisciplinary and dynamic, and has recently moved towards addressing the issues and challenges raised by the increasing emergence of cloud computing in daily life. This continuing popularity and success has been highlighted by AHM 2011, hosted at the University of York by the EPSRC White Rose Grid e-Science Centre and attended by over 200\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Generalization of quantum key distribution protocol\n", "abstract": " Quantum Key Distribution (QKD) is a secure key sharing technology with unconditional security. Certain well-known protocols for QKD have been presented, which claim their security by means of higher eavesdropping error-rates. A generalized quantum key distribution protocol that can be optimized for arbitrary number of bases and dimensions of photon states is presented in this paper. The protocol can provide higher eavesdropping error-rates than the well-known existing QKD protocols like BB-84 [4] and B-92 [5]. The higher error-rate makes it possible for Alice and Bob to share secure keys on relatively large distances.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Improved memetic algorithm for multilevel redundancy allocation\n", "abstract": " With the popularity of multilevel design in large scale systems, multilevel redundancy allocation problem (MLRAP) is becoming attractive to many researchers. MLRAP is not only NP-hard, but also qualifies as the hierarchy optimization problem. Exact met\u00eciod couldn't tackle it well, so heuristic and meta-heuristic methods are often used to solve it. To improve the effectiveness of existing genetic algorithms on MLRAP, this paper proposes an improved memetic algorithm based on sensitivity analysis and an iterative local search method. Firstly, for units at any level of die system, the impact of a change in me redundancy on the overall reliability or cost is analyzed. Then two new genetic operators (sensitivity-based mutation and crossover operators) are designed to conduct operations on selected units. System units with higher impact on reliability improvement and lower impact on cost increase will have a larger\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "QoS Prediction of Web Services on Collaborative Filtering\n", "abstract": " With the increasing popularity of the Web services, finding desired Web services for user constitutes some of the most challenging issues of service-oriented architecture (SOA). A collaborative filtering (CF)-based recommendation system is introduced to predict the quality of service (QoS) value of Web services before selecting. However, existing approaches for QoS prediction based on CF tend to compute the weight among different CF methods depending on the experience. Our work is very different from these methods since we design a hybrid prediction method which is more appropriate for QoS prediction in weight computing. Experimental studies are conducted on a well known real-world Web service performance dataset consisting of 1.5 millions invocation results to demonstrate that our approach provides better prediction results than other approaches.[PUBLICATION ABSTRACT]", "num_citations": "2\n", "authors": ["510"]}
{"title": "Near-optimal configuration of service pool size in service composition\n", "abstract": " To improve the reliability of service composition, service pools with redundant services are often registered in advance. Increasing the number of redundant services in a pool could improve the reliability while decreasing the performance or increasing unnecessary cost. Aiming at minimizing the overall cost or response time under certain reliability constraint, this paper proposed a service pool size choosing method from the point of view of service composition. It started with the reliability and performance evaluation of single service pool with multiple services and then employed architecture-based model to obtain the overall reliability/performance model for service composition with multiple service pools. These models were used as the utility function and constraint function to optimize service pool size combination. Two sensitivity-based algorithms were proposed to solve the optimization problem, which assigned\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "A new method for formalizing optimistic fair exchange protocols\n", "abstract": " It is difficult to analyze the timeliness of optimistic fair exchange protocols by using belief logic. For the problem, a new formal model and reasoning logic were proposed. In the new model, channel errors were attackers\u2019 behaviors, the participants were divided into honest and dishonest ones, and the attackers were attributed to two types of intruders. Based on the ideas of the model checking, the protocol was defined as an evolved logic system that has the Kripke structure. The new logic defined the time operators that describe the temporal relations among the participants\u2019 behaviors. By a typical optimistic fair exchange protocol, the article demonstrates the protocol analysis process in the new model. Two flaws were discovered and improved, which shows that the new method can be used to analyze the fairness and timeliness of optimistic fair exchange protocols.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Testing the Effectiveness of Dynamic Binding in Web Services\n", "abstract": " In recent years, Service Oriented Architectures (SOA) have risen in use as an architectural style for distributed systems. They have many desirable features such as flexibility, software reuse and cost benefits. In addition to this, SOA enables and indeed encourages the binding of services at runtime in the form of dynamic binding. Here, services are bound to service requests at runtime and the choice of service is determined with minimal user intervention. Presently, Web Services have risen as the de facto implementation of SOA and existing research for the testing of Web Services have assumed the choice of service at design-time. However, dynamic binding of services raises several additional challenges, such as managing complexity in service compositions using dynamic binding and non-deterministic behaviour in service selection. Few research exists that involve dynamic binding but with limitations as they do\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Moses: an innovative way to model heterogeneity in complex social systems\n", "abstract": " Computer models can provide valuable groundwork for decision making. Complex social systems present great challenges for building such models. New technologies and techniques enable us to attempt more sophistication in complex social models than in the past. This paper presented an innovative approach to model a large number of heterogeneous individuals on a fine spatial scale to assist the demographic planning in UK.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Dependable Dynamic Service Integration on Service-Oriented Peer-to-Peer Networks\n", "abstract": " For provision of dependable search and rescue capability in dynamic and unpredictable disaster areas, the networked nodes should have the ability to autonomously support and co-operate with each other in a peer-to-peer (P2P) manner to quickly configure any services available on the disaster area to deliver a real-time capability. In this paper, we present an innovative architectural approach which is able to proactively self-diagnose and self-adapt evolution occurring in provision of search and rescue capabilities in service-oriented P2P (servP2P) networks. This architecture provides flexibility and agility in handling dynamic changes and evolution encountered in the delivery of rescue capability. The experimental results indicate that the proposed approach provides a high-level of reliability and sustainability to achieve dependable service integration for capability provision.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Guiding personal choices in a quality contracts driven query economy\n", "abstract": " The emergence of Web 2.0 has brought upon a plethora of databasedriven web applications and services where both Quality of Service (QoS) and Quality of Data (QoD) are of paramount importance to end users. In our previous work, we have proposed Quality Contracts, a comprehensive framework for specifying multiple dimensions of QoS/QoD; we have also developed algorithms to maximize overall system performance under Quality Contracts. In this work, we turn our attention to the user side of the equation, on how to choose and adapt Quality Contracts to better serve users\u2019 needs in the presence of other users, who are competing for the same resources, in a virtual \u201ceconomy\u201d of Quality Contracts at the server. Towards this, we propose the Adaptive Quality Contract (AQC) scheme to maximize the success ratio of user queries. AQC switches between its Overbid (aggressive) mode and Deposit (conservative) mode, to allow users to survive through economic downturns and upturns. Extensive experiments with real traces show that our proposed scheme outperforms other competing schemes, under a variety of environments and a spectrum of workloads.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Service-Oriented Architectures for Network Enabled Capability\n", "abstract": " Network Enabled Capability (NEC) is the UK Ministry of Defence's response to the rapidly changing conflict environment in which its forces must operate. This paper introduces a part of the EPSRC and BAE Systems jointly funded project NECTISE (NEC Through Innovative Systems Engineering) that aims to address NEC issues using Service-Oriented Architecture (SOA). SOA is a network-enabled solution that has the potential to combine assets (software resources, people, equipment, processes) to provide capability; that is, the ability to achieve a mission objective. This position paper introduces the problems that NEC faces that can be addressed by SOA, and highlights some of the outstanding issues not address by SOA that form research directions within the NECTISE programme.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Topology-aware fault-tolerance in service-oriented grids\n", "abstract": " A promising means of attaining dependability improvements within service-oriented Grid environments is through the set of methods comprising design-diversity software fault-tolerance. However, applying such methods in a Grid environment requires the resolution of a new problem not faced in traditional systems, namely the common service problem whereby multiple disparate but functionally-equivalent services may share a common service as part of their respective workflows, thus decreasing their independence and increasing the likelihood of a potentially disastrous common-mode failure occurring. By establishing awareness of the topology of each channel/alternate within a design-diversity fault-tolerance scheme, techniques can be employed to avoid the common-service problem and achieve finer-grained control within the voting process. This paper explores a number of different techniques for obtaining topological information, and identifies the derivation of topological data from provenance information to be a particularly attractive technique. The paper concludes by detailing some very encouraging progress with integrating both provenance and topology-aware fault-tolerance applications into the Chinese CROWN Grid middleware system.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Data-intensive ubiquitous computing needs the grid\n", "abstract": " The rapid advances of wireless network technologies together with the undergoing commercial deployment of sensors shed lights on many aspects of the practicability of large scale Ubiquitous computing (UbiCOMP). The implication of this development is that we now have to handle data at an amount and density previously unattainable. Real-time data processing and analysis are two key capacities for realising the full potential of such data. Existing distributed systems, in particular in the commercial domains, cannot currently cope with the data challenges arise in such data intensive ubiquitous computing systems. The Grid, as a technology introduced to handle data intensive applications, is an ideal candidate to harness and utilize the massively available information produced such UbiCOMP applications. We discuss the unique research challenges resulting from such the integration.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Integrating an Attack Tolerant Information Service with Taverna\n", "abstract": " Utilising the Grid for conducting scientific research often involves a certain level of security risk, in particular when limited knowledge about the service provider is known before hand. This paper reports our experience of integrating an Attack-Tolerant Information Retrieval (ATIR) service with Taverna, a popular workflow tool among the UK e-science community, to support secure information query in a biology context. This paper presents the system architecture that has been used for the integration and the corresponding implementation details. Performance studies show that the overhead of ATIR server side processing is trivial (< 5%) compared with the total processing time of the integrated Taverna. Our experimental results also show that the major processing overhead is caused by the Taverna enactor operations which consume no less than 50% of the total processing time.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Novel checkpoint fault-tolerance scheme of metadata in file system [J]\n", "abstract": " Based on the failure problems in the metadata of cluster file system, a novel metadata checkpoint journal system on PVFS was researched and developed. This system was fully implemented in the Linux environment and could solve the bottleneck in metadata manage of file system, so it was provided with fault-tolerance function. This method used the structure of disk journal and memory journal, and adopted a mechanism of transaction, so it could meet the request of high availability in file system metadata.", "num_citations": "2\n", "authors": ["510"]}
{"title": "Dependability Assessment of an OGSA Compliant Middleware Implementation by Fault Injection\n", "abstract": " This paper presents our research on applying our dependability assessment method to an OGSA compliant middleware product. Our initial proof of concept experiment was implemented using a stateless Tomcat web server and Apache SOAP. This research adapts and enhances our existing fault injection software (OGSA-FIT) from the stateless environment of a standard web service to the stateful environment of an OGSA Toolkit (Globus).", "num_citations": "2\n", "authors": ["510"]}
{"title": "Building Embedded Fault-Tolerant Systems for Critical Applications: An Experimental Study\n", "abstract": " An increasing range of industries have a growing dependence on embedded software systems, many of which are safety-critical, real-time applications that require extremely high dependability. Two fundamental approaches \u2014 fault avoidance and fault tolerance \u2014 have been proposed to increase the overall dependability of such systems. However, the increased cost of using the fault tolerance approach may mean that this increase in dependability is not worth the extra expense involved. We describe an experiment undertaken in order to establish whether or not software redundancy (or the multi-version design method) can offer increased dependability over the traditional single-version development approach when given the same level of resources. The results of this and a subsequent follow-up study are then given. The analytic results from these experiments show that despite the poor quality of\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Software Fault Tolerance in Object-Oriented Systems: Approaches, Implementation and Evaluation\n", "abstract": " In this paper we introduce our experience with the use of C++(an object-oriented language) and Open C++(a reflective version of C++) to implement reusable, dependable control structures that can effectively support the provision of software fault tolerance in the application layer of an object-oriented system. Our implementation follows two different approaches-the object-library approach, using inheritance and delegation, and the reflective approach, based on metaobject protocols. The target environment used for our experiment is a distributed system consisting of multiple SPARC workstations connected through TCP/IP. Testing based on software fault injection is performed and performance-related analysis is provided. Both experimental and analytic results in this paper show that the object-oriented approach with the design of metaobject protocols is particularly promising for the development of computing systems with high dependability requirements", "num_citations": "2\n", "authors": ["510"]}
{"title": "Software fault tolerance: dynamic combination of dependability and efficiency\n", "abstract": " The paper discusses the problem of attaining a flexible compromise between using redundancy to improve software dependability and limiting the amount of redundancy for efficiency. A new scheme for facilitating software fault tolerance, called the self-configuring optimal programming scheme (SCOP), is presented. The proposed scheme attempts to reduce the resource cost of software fault tolerance, both in space and time, by providing designers with a flexible redundant architecture in which dependability and efficiency can be combined dynamically at run time. The design methodology for SCOP introduces support techniques for flexibly adjusting various attributes of system services including reliability, throughput, and response time. A detailed dependability and efficiency evaluation shows clearly that SCOP can achieve the same dependability level as those of other existing schemes for software fault\u00a0\u2026", "num_citations": "2\n", "authors": ["510"]}
{"title": "Characterization of (t/sup 1//t/sup 1/2/)/s diagnosability\n", "abstract": " The authors have previously proposed a three-valued model, which admits the transmission of diagnostic data via a fault-free communication unit in a faulty subsystem, making available more test links than the original. Based on this model, a new diagnosability measure,(t/sup 1//t/sup 1/2/)/s diagnosability, is introduced. A t/sup 1//t/sup 1/2//s-diagnosable system ensures the location of any fault set to within a set of no more than s subsystems provided it contains at most t/sup 1/faulty subsystems with faulty communication units and at most t/sup 1/2/faulty ones with fault-free communication units. Symmetric and asymmetric test invalidation are considered. In the light of multiple-fault-set consistency,(t/sup 1//t/sup 1/2/)/s diagnosability, either with symmetric invalidation or with asymmetric invalidation, is fully characterized.<>", "num_citations": "2\n", "authors": ["510"]}
{"title": "Gallat: A Spatiotemporal Graph Attention Network for Passenger Demand Prediction\n", "abstract": " Online ride-hailing services have become an important component of urban transportation in recent years. As a fundamental research problem for such services, the timely prediction of passenger demands in different regions is vital for effective traffic flow control. As both spatial and temporal patterns are indispensable passenger demand prediction, relevant research has evolved from pure time series to graph-structured data for modelling historical passenger demand data, where a snapshot graph is constructed for each time slot by connecting region nodes via different relational edges. Consequently, the spatiotemporal passenger demand records naturally carry dynamic patterns in the constructed graphs, where the edges also encode important information about the directions and volume (i.e., weights) of passenger demands between two connected regions. However, existing graph-based solutions fail to\u00a0\u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "Integrating clustering and regression for workload estimation in the cloud\n", "abstract": " Workload prediction has been widely researched in the literature. However, existing techniques are per\u2010job based and useful for service\u2010like tasks whose workloads exhibit seasonality and trend. But cloud jobs have many different workload patterns and some do not exhibit recurring workload patterns. We consider job\u2010pool\u2010based workload estimation, which analyzes the characteristics of existing tasks' workloads to estimate the currently running tasks' workload. First cluster existing tasks based on their workloads. For a new task J, collect the initial workload of J and determine which cluster J may belong to, then use the cluster's characteristics to estimate J\u2032s workload. Based on the Google dataset, the algorithm is experimentally evaluated and its effectiveness is confirmed. However, the workload patterns of some tasks do have seasonality and trend, and conventional per\u2010job\u2010based regression methods may\u00a0\u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "Starch and Plant Fiber Reinforced Biodegradable Composites with Open Cell Structures\n", "abstract": " In order to figure out the effect of different starches on the properties of starch-based composites, new biodegradable composites with open cell structure were prepared through thermo-cavity foam molding using four different type starches (corn starch (CS), wheat starch (WS), potato starch (PS), and sweet potato starch (SPS)) and sisal fibers as main raw materials. Mechanical properties of the biodegradable composites were tested. The order of tensile and compressive strength of the composites was as follows: SPS-based composite > CS-based composite > PS-based composite > WS-based composite. Following X-ray diffraction, the infrared spectrum analysis, scanning electron microscopy, and viscosity test were employed to gain comprehensive views on the effect of the different starch microstructures on the properties of the biodegradable composites. X-ray diffraction analysis showed that the crystalline index\u00a0\u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "Parallelized synchronous multi-agent deep reinforcement learning with experience replay memory\n", "abstract": " In the field of deep reinforcement learning, reusing training data from different time-steps by introducing the Experience Replay Memory (ERM) can significantly reduce non-stationarity and decorrelate updates. However, in Multi-Agent Deep Reinforcement Learning (MA-DRL), in particular, the independent Q-Learning problems, it may introduce obsolete experiences to the training process, which inevitalbiy leads that the policy network takes more training times to converge. In this paper, we propose an approach which can exploit parallelization in resource-rich environments (e.g., cloud) to generate decorrelated training data instead of purely relying on ERM in MA-DRL. We enhance the synchronous method, a parallel method originally proposed for single-agent DRL, to the multi-agent environment. And to avoid the drawback of synchronous method, we combine it with ERM together and apply the improved method\u00a0\u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "Cap: Exploiting Data Correlations to Improve the Performance and Endurance of SSD RAID\n", "abstract": " Parity-based RAID provides system-level fault tolerance. However, parity updates caused by small writes introduce lots of extra I/Os, degrading I/O performance and wearing SSDs out. It has been proposed to use Non-Volatile Memory (NVM) as a parity cache on an SSD RAID to postpone parity updates until the whole stripe has been updated. However, this often fails because of skewed distribution of hot data chunks within a stripe. In real workloads, it is often difficult to achieve a full-stripe update even after a long delay. In this paper, we propose a Correlation aware parity caching scheme, called Cap, for SSD-based RAIDs. The key idea behind Cap is to periodically reconstruct correlated hot data chunks into a new stripe. Since these data chunks have a strong correlation, they tend to be updated together within a short time span. This co-update within a stripe more efficiently utilizes the parity cache to convert\u00a0\u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "Worldwide Universities Network (WUN) Web Observatory\n", "abstract": " The ongoing growth in research data publication supports global intra-disciplinary and inter-disciplinary research collaboration but the current generation of archive-centric research data repositories do not address some of the key practical obstacles to research data sharing and re-use, specifically: discovering relevant data on a global scale is timeconsuming; sharing \u2018live\u2019and streaming data is non-trivial; managing secure access to sensitive data is overly complicated; and, researchers are not guaranteed attribution for re-use of their own research data. These issues are keenly felt in an international network like the Worldwide Universities Network (WUN) as it seeks to address major global challenges. In this paper we outline the WUN Web Observatory project\u2019s plan to overcome these obstacles and, given that these obstacles are not unique to WUN, we also propose an ambitious, longer-term route to their solution at Web-scale by applying lessons from the Web itself. c 2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW\u201917 Companion, April 3\u20137, 2017, Perth, Australia. ACM 978-1-4503-4914-7/17/04. http://dx. doi. org/10.1145/3041021.3051691", "num_citations": "1\n", "authors": ["510"]}
{"title": "A novel video retrieval algorithm based on coarse-grained and fine-grained\n", "abstract": " In order to solve the balance problems between efficiency and accuracy encountered in content-based video retrieval, in this paper, we propose firstly a video retrieval scheme based on coarse granularity. To achieve the coarse-grained video retrieval, we use inverted index. Further, we expose an improved BLAST algorithm to realize fine-grained retrieval of video fingerprint. Finally, simulation results show the good efficiency and accuracy of the algorithm we have proposed. Therefore, this algorithm provides effective technical support for the security regulation and quick retrieval of Internet video.", "num_citations": "1\n", "authors": ["510"]}
{"title": "A service oriented virtual environment for complex system analysis: preliminary report\n", "abstract": " Distributed virtual simulation is a capability that is increasing in demand within the automotive manufacturing industry. The distributed and networked approach to system level design and simulation stands to benefit from a unifying relational oriented modeling and simulation framework due to the large number of simulation technologies that must be integrated. This will also permit innovative use of existing independent simulations for increased concurrency in design and verification and validation. Through relational orientation, high level syntax and semantics for representing models and simulations have been developed for proof of concept analysis. This paper presents an approach to drive a process of analysis of the vehicle as a complex system through the combination of a relational trade-off analysis framework and a distributed simulation execution delivered through a service-oriented integration architecture\u00a0\u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "Fair E-payment protocol based on certificateless signature and authenticated key exchange\n", "abstract": " E-payment protocol allows two or more users to securely exchange e-cash and digital product among them over an open network. There are some problems in the E-payment applications of cross-domain and cross-organization scenarios because of certificate-based authentication and digital signature, like inconsistent public key certificates and a heavy certificate management burden. ID-based cryptography is adopted to solve those problems, but it suffers the key escrow issue. Certificateless cryptography has been introduced to mitigate those limitations. A certificateless signature and authenticated key exchange scheme (CL-SAKE for short) is proposed, and its security is proved in the extended random oracle model. As an application, an E-payment protocol based on the new CL-SAKE is then proposed, which achieves unforgeability and un-reusability of e-cash, customer anonymity and fair exchange.", "num_citations": "1\n", "authors": ["510"]}
{"title": "Investigation of research on P2P resource discovery for service-oriented computing.\n", "abstract": " Investigation of research on P2P resource discovery for service-oriented computing. - Middlesex University Research Repository Skip to content Middlesex University Middlesex University Research Repository Research work by Middlesex University staff and alumni Home Latest additions Usage statistics Search: simple / advanced Browse by Year Research area Author Theses Help How do I add my work? Repository FAQs Repository policies Accessibility Copyright Contact us Login (Middlesex University staff only) Investigation of research on P2P resource discovery for service-oriented computing. Liu, Lu, Russell, Duncan and Xu, Jie (2010) Investigation of research on P2P resource discovery for service-oriented computing. In: Handbook of Research on P2P and Grid Systems for Service-Oriented Computing. Antonopoulos, Nick, Exarchakos, Georgios, Li, Maozhen and Liotta, Antonio, eds. IGI Global, Hershey, PA.\u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "The t/(n-1)-VP Approach to Fault-Tolerant Software\n", "abstract": " This paper describes a software fault tolerance scheme, called t/(n\u20131)-Variant Programming (or t/(n\u20131)-VP), which is based on a particular system diagnosis technique used in hardware and thereby has some special advantages involving a simplified adjudication mechanism and enhanced capability of tolerating faults. A detailed dependability evaluation of the t/(n\u20131)-VP architecture is conducted, compared with two similar schemes, namely, N-version programming (NVP) and N self-checking programming (NSCP). The results drawn from the comparison clearly show that t/(n\u20131)-VP is a viable addition or alternative to present techniques.Much of classical work on dependability analysis of software fault tolerance approaches has focused on the simplest architectural examples that can only tolerate single software faults, without considering tolerance to multiple and related faults. The results obtained from such analyses are thus restricted. The dependability evaluation carried out in this paper deals with more complicated and general software redundancy, ie, various architectures tolerating two or more faults. It is not a great surprise that we come to new conclusions: both t/(n\u20131)-VP and the NVP scheme have the ability to tolerate some related faults between software variants; in general, t/(n\u20131)-VP has higher reliability, whereas NVP would be better from the safety point of view.", "num_citations": "1\n", "authors": ["510"]}
{"title": "Adaptive Architectures for Hybrid Fault Tolerance in Distributed Computing Systems\n", "abstract": " This paper discusses the issue of hardware and software fault tolerance in distributed computing environments as well as issues related to efficiency and flexibility. A set of new fault-tolerant architectures is presented, and a detailed dependability analysis of these architectures together with an efficiency evaluation is performed. The proposed architectural solutions are based on the assumption that the distributed supporting environments under consideration are highly varying and they would support multiple competing applications associated with different fault-tolerant architectures, thereby exhibiting dynamic service characteristics. Stress is placed on adaptation\u2014the major goal of designs of these new architectures is to attempt the adaptive execution of redundant components so as to minimize hardware resource consumption and shorten the response time, as much as possible, for a required level of fault tolerance.", "num_citations": "1\n", "authors": ["510"]}
{"title": "Book-Review-Acousto-Optic Devices-Principles Design and Applications\n", "abstract": " Book-Review - Acousto-Optic Devices - Principles Design and Applications - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Book-Review - Acousto-Optic Devices - Principles Design and Applications Xu, JP ; Stroud, R. Abstract Publication: Science Pub Date: September 1992 Bibcode: 1992Sci...X No Sources Found \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is ADS down? (or is it just me...) Smithsonian Institution Smithsonian Privacy Notice Smithsonian Terms of Use Smithsonian Astrophysical Observatory NASA \u2026", "num_citations": "1\n", "authors": ["510"]}
{"title": "The Optimum Design of Acoustooptical Devices Using the First Order Beam Steering\n", "abstract": " Through tho datermination of the angular spec-trum of the ultrasenic waves by Feurier transformation, a goneral formula for calculating the acoustaoptic iateractien frequency response of A0 doricoa using the first crder beam steering has bean derived. After a full analysis of the effect of the six parameters an the shape of the frequency respoa-se curve, a systematic method which determines the values of these six parameters according to the re-quirements on the shape of the frequency response curve has been proposed. The optimum values of these parameters have been determined for all interested cases on a computer. Thus we completed the optimum design of bulk wave and thin-film A0 devicea with planar configuration and two different kinds of step configuration. The accuracy which should be reached for these optimun values in fabrication is also discusaed.", "num_citations": "1\n", "authors": ["510"]}