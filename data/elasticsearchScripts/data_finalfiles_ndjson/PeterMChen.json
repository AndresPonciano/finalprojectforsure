{"title": "RAID: High-performance, reliable secondary storage\n", "abstract": " Disk arrays were proposed in the 1980s as a way to use parallelism between multiple disks to improve aggregate I/O performance. Today they appear in the product lines of most major computer manufacturers. This article gives a comprehensive overview of disk arrays and provides a framework in which to organize current and future work. First, the article introduces disk technology and reviews the driving forces that have popularized disk arrays: performance and reliability. It discusses the two architectural techniques used in disk arrays: striping across multiple disks to improve performance and redundancy to improve reliability. Next, the article describes seven disk array architectures, called RAID (Redundant Arrays of Inexpensive Disks) levels 0\u20136 and compares their  performance, cost, and reliability. It goes on to discuss advanced research and implementation topics such as refining the basic RAID levels to\u00a0\u2026", "num_citations": "1934\n", "authors": ["496"]}
{"title": "ReVirt: Enabling intrusion analysis through virtual-machine logging and replay\n", "abstract": " Current system loggers have two problems: they depend on the integrity of the operating system being logged, and they do not save sufficient information to replay and analyze attacks that include any non-deterministic events. ReVirt removes the dependency on the target operating system by moving it into a virtual machine and logging below the virtual machine. This allows ReVirt to replay the system's execution before, during, and after an intruder compromises the system, even if the intruder replaces the target operating system. ReVirt logs enough information to replay a long-term execution of the virtual machine instruction-by-instruction. This enables it to provide arbitrarily detailed observations about what transpired on the system, even in the presence of non-deterministic attacks and executions. ReVirt adds reasonable time and space overhead. Overheads due to virtualization are imperceptible for interactive\u00a0\u2026", "num_citations": "1445\n", "authors": ["496"]}
{"title": "When virtual is better than real [operating system relocation to virtual machines]\n", "abstract": " This paper argues that the operating system and applications currently running on a real machine should relocate into a virtual machine. This structure enables services to be added below the operating system and to do so without trusting or modifying the operating system or applications. To demonstrate the usefulness of this structure, we describe three services that take advantage of it: secure logging, intrusion prevention and detection, and environment migration.", "num_citations": "727\n", "authors": ["496"]}
{"title": "Backtracking intrusions\n", "abstract": " Analyzing intrusions today is an arduous, largely manual task because system administrators lack the information and tools needed to understand easily the sequence of steps that occurred in an attack. The goal of BackTracker is to identify automatically potential sequences of steps that occurred in an intrusion. Starting with a single detection point (eg, a suspicious file), BackTracker identifies files and processes that could have affected that detection point and displays chains of events in a dependency graph. We use BackTracker to analyze several real attacks against computers that we set up as honeypots. In each case, BackTracker is able to highlight effectively the entry point used to gain access to the system and the sequence of steps from that entry point to the point at which we noticed the intrusion. The logging required to support BackTracker added 9% overhead in running time and generated 1.2 GB per\u00a0\u2026", "num_citations": "525\n", "authors": ["496"]}
{"title": "Debugging operating systems with time-traveling virtual machines\n", "abstract": " Operating systems are difficult to debug with traditional cyclic debugging. They are non-deterministic; they run for long periods of time; they interact directly with hardware devices; and their state is easily perturbed by the act of debugging. This paper describes a time-traveling virtual machine that overcomes many of the difficulties associated with debugging operating systems. Time travel enables a programmer to navigate backward and forward arbitrarily through the execution history of a particular run and to replay arbitrary segments of the past execution. We integrate time travel into a general-purpose debugger to enable a programmer to debug an OS in reverse, implementing commands such as reverse breakpoint, reverse watchpoint, and reverse single step. The space and time overheads needed to support time travel are reasonable for debugging, and movements in time are fast enough to support interactive debugging. We demonstrate the value of our time-traveling virtual machine by using it to understand and fix several OS bugs that are difficult to find with standard debugging tools. Reverse debugging is especially helpful in finding bugs that are fragile due to non-determinism, bugs in device drivers, bugs that require long runs to trigger, bugs that corrupt the stack, and bugs that are detected after the relevant stack frame is popped.", "num_citations": "471\n", "authors": ["496"]}
{"title": "Operating System Support for Virtual Machines.\n", "abstract": " A virtual-machine monitor (VMM) is a useful technique for adding functionality below existing operating system and application software. One class of VMMs (called Type II VMMs) builds on the abstractions provided by a host operating system. Type II VMMs are elegant and convenient, but their performance is currently an order of magnitude slower than that achieved when running outside a virtual machine (a standalone system). In this paper, we examine the reasons for this large overhead for Type II VMMs. We find that a few simple extensions to a host operating system can make it a much faster platform for running a VMM. Taking advantage of these extensions reduces virtualization overhead for a Type II VMM to 14-35% overhead, even for workloads that exercise the virtual machine intensively.", "num_citations": "456\n", "authors": ["496"]}
{"title": "Maximizing performance in a striped disk array\n", "abstract": " Improvements in disk speeds have not kept up with improvements in processor and memory speeds. One way to correct the resulting speed mismatch is to stripe data across many disks. In this paper, we address how to stripe data to get maximum performance from the disks. Specifically, we examine how to choose the striping unit, ie the amount of logically contiguous data on each disk. We synthesize rules for determining the best striping unit for a given range of workloads.", "num_citations": "452\n", "authors": ["496"]}
{"title": "Disk scheduling revisited\n", "abstract": " Since the invention of the movable head disk, people have improved I/O performance by intelligent scheduling of disk accesses. We have applied these techniques to systems with large memories and potentially long disk queues. By viewing the entire buffer cache as a write buffer, we can improve disk bandwidth utilization by applying some traditional disk scheduling tech-niques. We have analyzed these techniques, which attempt to optimize head movement and guarantee fairness in response time, in the presence of long disk queues. We then propose two algorithms which take rotational latency into account, achieving disk bandwidth utilizations of nearly four times a simple first come first serve algorithm. One of these two algorithms, a weighted shortest total time first, is particularly applicable to a file server environment because it guarantees that all requests get to disk within a specified time window.", "num_citations": "449\n", "authors": ["496"]}
{"title": "Detecting past and present intrusions through vulnerability-specific predicates\n", "abstract": " Most systems contain software with yet-to-be-discovered security vulnerabilities. When a vulnerability is disclosed, administrators face the grim reality that they have been running software which was open to attack. Sites that value availability may be forced to continue running this vulnerable software until the accompanying patch has been tested. Our goal is to improve security by detecting intrusions that occurred before the vulnerability was disclosed and by detecting and responding to intrusions that are attempted after the vulnerability is disclosed. We detect when a vulnerability is triggered by executing vulnerability-specific predicates as the system runs or replays. This paper describes the design, implementation and evaluation of a system that supports the construction and execution of these vulnerability-specific predicates. Our system, called IntroVirt, uses virtual-machine introspection to monitor the execution\u00a0\u2026", "num_citations": "373\n", "authors": ["496"]}
{"title": "Execution replay of multiprocessor virtual machines\n", "abstract": " Execution replay of virtual machines is a technique which has many important applications, including debugging, fault-tolerance, and security. Execution replay for single processor virtual machines is well-understood, and available commercially. With the advancement of multi-core architectures, however, multiprocessor virtual machines are becoming more important. Our system, SMP-ReVirt, is the first system to log and replay a multiprocessor virtual machine on commodity hardware. We use hardware page protection to detect and accurately replay sharing between virtual cpus of a multi-cpu virtual machine, allowing us to replay the entire operating system and all applications. We have tested our system on a variety of workloads, and find that although sharing under SMP-ReVirt is expensive, for many workloads and applications, including debugging, the overhead is acceptable.", "num_citations": "357\n", "authors": ["496"]}
{"title": "Introduction to redundant arrays of inexpensive disks (RAID)\n", "abstract": " The authors discuss various types of RAIDs (redundant arrays of inexpensive disks), a cost-effective option to meet the challenge of exponential growth in the processor and memory speeds. They argue that the size reduction of personal-computer (PC) disks is the key to the success of disk arrays. While large arrays of mainframe processors are possible, it is certainly easier to construct an array from the same number of microprocessors (or PC drives). With advantages in cost-performance, reliability, power consumption, and floor space, the authors expect RAIDs to replace large drives in future I/O systems.<>", "num_citations": "355\n", "authors": ["496"]}
{"title": "Memory persistency\n", "abstract": " Emerging nonvolatile memory technologies (NVRAM) promise the performance of DRAM with the persistence of disk. However, constraining NVRAM write order, necessary to ensure recovery correctness, limits NVRAM write concurrency and degrades throughput. We require new memory interfaces to minimally describe write constraints and allow high performance and high concurrency data structures. These goals strongly resemble memory consistency. Whereas memory consistency concerns the order that memory operations are observed between numerous processors, persistent memory systems must constrain the order that writes occur with respect to failure. We introduce memory persistency, a new approach to designing persistent memory interfaces, building on memory consistency. Similar to memory consistency, memory persistency models may be relaxed to improve performance. We describe the\u00a0\u2026", "num_citations": "347\n", "authors": ["496"]}
{"title": "The Rio file cache: Surviving operating system crashes\n", "abstract": " One of the fundamental limits to high-performance, high-reliability file systems is memory's vulnerability to system crashes. Because memory is viewed as unsafe, systems periodically write data back to disk. The extra disk traffic lowers performance, and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable memory enables a system to achieve the best of both worlds: reliability equivalent to a write-through file cache, where every write is instantly safe, and performance equivalent to a pure write-back cache, with no reliability-induced writes to disk. To achieve reliability, we protect memory during a crash and restore it during a reboot (a \"warm\" reboot). Extensive crash tests show that even without protection, warm reboot enables memory to achieve\u00a0\u2026", "num_citations": "309\n", "authors": ["496"]}
{"title": "Decoupling dynamic program analysis from execution in virtual environments\n", "abstract": " Analyzing the behavior of running programs has a wide variety of compelling applications, from intrusion detection and prevention to bug discovery. Unfortunately, the high runtime overheads imposed by complex analysis techniques makes their deployment impractical in most settings. We present a virtual machine based architecture called Aftersight ameliorates this, providing a flexible and practical way to run heavyweight analyses on production workloads.Aftersight decouples analysis from normal execution by logging nondeterministic VM inputs and replaying them on a separate analysis platform. VM output can be gated on the results of an analysis for intrusion prevention or analysis can run at its own pace for intrusion detection and best effort prevention. Logs can also be stored for later analysis offline for bug finding or forensics, allowing analyses that would otherwise be unusable to be applied ubiquitously. In all cases, multiple analyses can be run in parallel, added on demand, and are guaranteed not to interfere with the running workload. We present our experience implementing Aftersight as part of the VMware virtual machine platform and using it to develop a realtime intrusion detection and prevention system, as well as an an offline system for bug detection, which we used to detect numerous novel and serious bugs in VMware ESX Server, Linux, and Windows applications.", "num_citations": "255\n", "authors": ["496"]}
{"title": "DoublePlay: Parallelizing sequential logging and replay\n", "abstract": " Deterministic replay systems record and reproduce the execution of a hardware or software system. In contrast to replaying execution on uniprocessors, deterministic replay on multiprocessors is very challenging to implement efficiently because of the need to reproduce the order of or the values read by shared memory operations performed by multiple threads. In this paper, we present DoublePlay, a new way to efficiently guarantee replay on commodity multiprocessors. Our key insight is that one can use the simpler and faster mechanisms of single-processor record and replay, yet still achieve the scalability offered by multiple cores, by using an additional execution to parallelize the record and replay of an application. DoublePlay timeslices multiple threads on a single processor, then runs multiple time intervals (epochs) of the program concurrently on separate processors. This strategy, which we call\u00a0\u2026", "num_citations": "239\n", "authors": ["496"]}
{"title": "High-performance transactions for persistent memories\n", "abstract": " Emerging non-volatile memory (NVRAM) technologies offer the durability of disk with the byte-addressability of DRAM. These devices will allow software to access persistent data structures directly in NVRAM using processor loads and stores, however, ensuring consistency of persistent data across power failures and crashes is difficult. Atomic, durable transactions are a widely used abstraction to enforce such consistency. Implementing transactions on NVRAM requires the ability to constrain the order of NVRAM writes, for example, to ensure that a transaction's log record is complete before it is marked committed. Since NVRAM write latencies are expected to be high, minimizing these ordering constraints is critical for achieving high performance. Recent work has proposed programming interfaces to express NVRAM write ordering constraints to hardware so that NVRAM writes may be coalesced and reordered\u00a0\u2026", "num_citations": "210\n", "authors": ["496"]}
{"title": "Rethink the sync\n", "abstract": " We introduce external synchrony, a new model for local file I/O that provides the reliability and simplicity of synchronous I/O, yet also closely approximates the performance of asynchronous I/O. An external observer cannot distinguish the output of a computer with an externally synchronous file system from the output of a computer with a synchronous file system. No application modification is required to use an externally synchronous file system. In fact, application developers can program to the simpler synchronous I/O abstraction and still receive excellent performance. We have implemented an externally synchronous file system for Linux, called xsyncfs. Xsyncfs provides the same durability and ordering-guarantees as those provided by a synchronously mounted ext3 file system. Yet even for I/O-intensive benchmarks, xsyncfs performance is within 7% of ext3 mounted asynchronously. Compared to ext3 mounted\u00a0\u2026", "num_citations": "207\n", "authors": ["496"]}
{"title": "Respec: efficient online multiprocessor replayvia speculation and external determinism\n", "abstract": " Deterministic replay systems record and reproduce the execution of a hardware or software system. While it is well known how to replay uniprocessor systems, replaying shared memory multiprocessor systems at low overhead on commodity hardware is still an open problem. This paper presents Respec, a new way to support deterministic replay of shared memory multithreaded programs on commodity multiprocessor hardware. Respec targets online replay in which the recorded and replayed processes execute concurrently. Respec uses two strategies to reduce overhead while still ensuring correctness: speculative logging and externally deterministic replay. Speculative logging optimistically logs less information about shared memory dependencies than is needed to guarantee deterministic replay, then recovers and retries if the replayed process diverges from the recorded process. Externally deterministic\u00a0\u2026", "num_citations": "190\n", "authors": ["496"]}
{"title": "An evaluation of redundant arrays of disks using an Amdahl 5890\n", "abstract": " Recently we presented several disk array architectures designed to increase the data rate and I/O rate of supercomputing applications, transaction processing, and file systems [Patterson 88]. In this paper we present a hardware performance measurement of two of these architectures, mirroring and rotated parity. We see how throughput for these two architectures is affected by response time requirements, request sizes, and read to write ratios. We find that for applications with large accesses, such as many supercomputing applications, a rotated parity disk array far outperforms traditional mirroring architecture. For applications dominated by small accesses, such as transaction processing, mirroring architectures have higher performance per disk than rotated parity architectures.", "num_citations": "188\n", "authors": ["496"]}
{"title": "Enriching Intrusion Alerts Through Multi-Host Causality.\n", "abstract": " Current intrusion detection systems point out suspicious states or events but do not show how the suspicious state or events relate to other states or events in the system. We show how to enrich an IDS alert with information about how those alerts causally lead to or result from other events in the system. By enriching IDS alerts with this type of causal information, we can leverage existing IDS alerts to learn more about the suspected attack. Backward causal graphs can be used to find which host allowed a multi-hop attack (such as a worm) to enter a local network; forward causal graphs can be used to find the other hosts that were affected by the multi-hop attack. We demonstrate this use of causality on a local network by tracking the Slapper worm, a manual attack that spreads via several attack vectors, and an e-mail virus. Causality can also be used to correlate distinct network and host IDS alerts. We demonstrate this use of causality by correlating Snort and host IDS alerts to reduce false positives on a testbed system connected to the Internet.", "num_citations": "181\n", "authors": ["496"]}
{"title": "Parallelizing security checks on commodity hardware\n", "abstract": " Speck (Speculative Parallel Check) is a system thataccelerates powerful security checks on commodity hardware by executing them in parallel on multiple cores. Speck provides an infrastructure that allows sequential invocations of a particular security check to run in parallel without sacrificing the safety of the system. Speck creates parallelism in two ways. First, Speck decouples a security check from an application by continuing the application, using speculative execution, while the security check executes in parallel on another core. Second, Speck creates parallelism between sequential invocations of a security check by running later checks in parallel with earlier ones. Speck provides a process-level replay system to deterministically and efficiently synchronize state between a security check and the original process.We use Speck to parallelize three security checks: sensitive data analysis, on-access virus\u00a0\u2026", "num_citations": "167\n", "authors": ["496"]}
{"title": "Backtracking intrusions\n", "abstract": " Analyzing intrusions today is an arduous, largely manual task because system administrators lack the information and tools needed to understand easily the sequence of steps that occurred in an attack. The goal of BackTracker is to identify automatically potential sequences of steps that occurred in an intrusion. Starting with a single detection point (e.g., a suspicious file), BackTracker identifies files and processes that could have affected that detection point and displays chains of events in a dependency graph. We use BackTracker to analyze several real attacks against computers that we set up as honeypots. In each case, BackTracker is able to highlight effectively the entry point used to gain access to the system and the sequence of steps from that entry point to the point at which we noticed the intrusion. The logging required to support BackTracker added 9% overhead in running time and generated 1.2 GB per\u00a0\u2026", "num_citations": "163\n", "authors": ["496"]}
{"title": "Speculative execution in a distributed file system\n", "abstract": " Speculator provides Linux kernel support for speculative execution. It allows multiple processes to share speculative state by tracking causal dependencies propagated through inter-process communication. It guarantees correct execution by preventing speculative processes from externalizing output, e.g., sending a network message or writing to the screen, until the speculations on which that output depends have proven to be correct. Speculator improves the performance of distributed file systems by masking I/O latency and increasing I/O throughput. Rather than block during a remote operation, a file system predicts the operation's result, then uses Speculator to checkpoint the state of the calling process and speculatively continue its execution based on the predicted result. If the prediction is correct, the checkpoint is discarded; if it is incorrect, the calling process is restored to the checkpoint, and the operation is\u00a0\u2026", "num_citations": "161\n", "authors": ["496"]}
{"title": "Free transactions with rio vista\n", "abstract": " A erful mechanisms for handling failures and manipulating persistent data. Unfortunately, standard recoverable memories incur an overhead of several milliseconds per transaction. This paper presents a system that improves transaction overhead by a factor of 2000 for working sets that fit in main memory. Of this factor of 2000, a factor of 20 is due to the Rio file cache, which absorbs synchronous writes to disk without losing data during system crashes. The remaining factor of 100 is due to vista, a 720-line, recoverable-memory library tailored for Rio. vista lowers transaction overhead to 5 psec by using no redo log, no system calls, and only one memory-to-memory copy: This drastic reduction in overhead leads to a overall speedup of 150-556x for benchmarks based on TPC-B and TPC-C.", "num_citations": "145\n", "authors": ["496"]}
{"title": "Race detection for event-driven mobile applications\n", "abstract": " Mobile systems commonly support an event-based model of concurrent programming. This model, used in popular platforms such as Android, naturally supports mobile devices that have a rich array of sensors and user input modalities. Unfortunately, most existing tools for detecting concurrency errors of parallel programs focus on a thread-based model of concurrency. If one applies such tools directly to an event-based program, they work poorly because they infer false dependencies between unrelated events handled sequentially by the same thread. In this paper we present a race detection tool named CAFA for event-driven mobile systems. CAFA uses the causality model that we have developed for the Android event-driven system. A novel contribution of our model is that it accounts for the causal order due to the event queues, which are not accounted for in past data race detectors. Detecting races based on\u00a0\u2026", "num_citations": "141\n", "authors": ["496"]}
{"title": "Whither generic recovery from application faults? A fault study using open-source software\n", "abstract": " We test the hypothesis that generic recovery techniques, such as process pairs, can survive most application faults without using application-specific information. We examine in detail the faults that occur in three, large, open-source applications: the Apache Web server, the GNOME desktop environment and the MySQL database. Using information contained in the bug reports and source code, we classify faults based on how they depend on the operating environment. We find that 72-87% of the faults are independent of the operating environment and are hence deterministic (non-transient). Recovering from the failures caused by these faults requires the use of application-specific knowledge. Half of the remaining faults depend on a condition in the operating environment that is likely to persist on retry, and the failures caused by these faults are also likely to require application-specific recovery. Unfortunately, only\u00a0\u2026", "num_citations": "140\n", "authors": ["496"]}
{"title": "Striping in a RAID level 5 disk array\n", "abstract": " Redundant disk arrays are an increasingly popular way to improve I/O system performance. Past research has studied how to stripe data in non-redundant (RAID Level 0) disk arrays, but none has yet been done on how to stripe data in redundant disk arrays such as RAID Level 5, or on how the choice of striping unit varies with the number of disks. Using synthetic workloads, we derive simple design rules for striping data in RAID Level 5 disk arrays given varying amounts of workload information. We then validate the synthetically derived design rules using real workload traces to show that the design rules apply well to real systems. We find no difference in the optimal striping units for RAID Level 0 and 5 for read-intensive workloads. For write-intensive workloads, in contrast, the overhead of maintaining parity causes full-stripe writes (writes that span the entire error-correction group) to be more efficient than read\u00a0\u2026", "num_citations": "140\n", "authors": ["496"]}
{"title": "Delegated persist ordering\n", "abstract": " Systems featuring a load-store interface to persistent memory (PM) are expected soon, making in-memory persistent data structures feasible. Ensuring persistent data structure recoverability requires constraints on the order PM writes become persistent. But, current memory systems reorder writes, providing no such guarantees. To complement their upcoming 3D XPoint memory, Intel has announced new instructions to enable programmer control of data persistence. We describe the semantics implied by these instructions, an ordering model we call synchronous ordering. Synchronous ordering (SO) enforces order by stalling execution when PM write ordering is required, exposing PM write latency on the execution critical path. It incurs an average slowdown of 7.21x over volatile execution without ordering in PM-write-intensive benchmarks. SO tightly couples enforcing order and flushing writes to PM, but this tight\u00a0\u2026", "num_citations": "135\n", "authors": ["496"]}
{"title": "A new approach to I/O performance evaluation: self-scaling I/O benchmarks, predicted I/O performance\n", "abstract": " Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance. We propose a new approach to I/O performance analysis. First, we propose a self-scaling benchmark that dynamically adjusts aspects of its workload according to the performance characteristic of the system being measured. By doing so, the benchmark automatically scales across current and future systems. The evaluation aids in understanding system performance by reporting how performance varies according to each of fie workload parameters. Second, we propose predicted performance, a technique for using the results from the self-scaling evaluation to quickly estimate the performance for workloads that have not been measured. We show that this technique yields reasonably accurate performance estimates and argue\u00a0\u2026", "num_citations": "135\n", "authors": ["496"]}
{"title": "An analytical model for designing memory hierarchies\n", "abstract": " Memory hierarchies have long been studied by many means: system building, trace driven simulation, and mathematical analysis. Yet little help is available for the system designer wishing to quickly size the different levels in a memory hierarchy to a first order approximation. We present a simple analysis for providing this practical help and some unexpected results and intuition that come out of the analysis. By applying a specific, parameterized model of workload locality, we are able to derive a closed form solution for the optimal size of each hierarchy level. We verify the accuracy of this solution against exhaustive simulation with two case studies: a three level I/O storage hierarchy and a three level processor cache hierarchy. In all but one case, the configuration recommended by the model performs within 5% of optimal. One result of our analysis is that the first place to spend money is the cheapest (rather than the\u00a0\u2026", "num_citations": "113\n", "authors": ["496"]}
{"title": "Pocket hypervisors: Opportunities and challenges\n", "abstract": " In this position paper, we explore the opportunities and challenges of running pocket hypervisors on commodity mobile devices through four proposed applications: secure operating systems, security services, mobile testbeds, and opportunistic sensor networks. We believe that pocket hypervisors can benefit mobile computing, but that mobility presents several important and unique challenges to virtualization.", "num_citations": "110\n", "authors": ["496"]}
{"title": "RAID-II: A high-bandwidth network file server\n", "abstract": " In 1989, the RAID (Redundant Arrays of Inexpensive Disks) group at U.C. Berkeley built a prototype disk array called RAID-I. The bandwidth delivered to clients by RAID-I was severely limited by the memory system bandwidth of the disk array's host workstation. They designed their second prototype, RAID-II, to deliver more of the disk array bandwidth to file server clients. A custom-built crossbar memory system called the XBUS board connects the disks directly to the high-speed network, allowing data for large requests to bypass the server workstation. RAID-II runs Log-Structured File System (LFS) software to optimize performance for bandwidth-intensive applications. The RAID-II hardware with a single XBUS controller board delivers 20 megabytes/second for large, random read operations and up to 31 megabytes/second for sequential read operations. A preliminary implementation of LFS on RAID-II delivers 21\u00a0\u2026", "num_citations": "108\n", "authors": ["496"]}
{"title": "RAID-II: a scalable storage architecture for high-bandwidth network file service\n", "abstract": " RAID-II (RAID the second) is a scalable high-bandwidth network file server for heterogeneous computing environments characterized by a mixture of high-bandwidth scientific, engineering and multi-media applications and low-latency high-transaction-rate UNIX applications. RAID-II is motivated by three observations: applications are becoming more bandwidth intensive, the I/O bandwidth of workstations is decreasing with respect to MIPS, and recent technological developments in high-performance networks and secondary storage systems make it economical to build high-bandwidth network storage systems. Unlike most existing file servers that use a bus as a system backplane, RAID-II achieves scalability by treating the network as the system backplane. RAID-II is notable because it physically separates les service, the management of file metadata, from storage service, the storage and transfer of file data; stripes files over multiple storage servers for improved performance and reliability; provides separate mechanisms for high-bandwidth and low-latency I/O requests; implements a RAID level 5 storage system; and runs LFS, the Log-Structured File System, which is specifically designed to support high-bandwidth I/O and RAID level 5 storage systems.", "num_citations": "102\n", "authors": ["496"]}
{"title": "Eidetic systems\n", "abstract": " The vast majority of state produced by a typical computer is generated, consumed, then lost forever. We argue that a computer system should instead provide the ability to recall any past state that existed on the computer, and further, that it should be able to provide the lineage of any byte in a current or past state. We call a system with this ability an eidetic computer system. To preserve all prior state efficiently, we observe and leverage the synergy between deterministic replay and information flow. By dividing the system into groups of replaying processes and tracking dependencies among groups, we enable the analysis of information flow among groups, make it possible for one group to regenerate the data needed by another, and permit the replay of subsets of processes rather than of the entire system. We use modelbased compression and deduplicated file recording to reduce the space overhead of deterministic replay. We also develop a variety of linkage functions to analyze the lineage of state, and we apply these functions via retrospective binary analysis. In this paper we present Arnold, the first practical eidetic computing platform. Preliminary data from several weeks of continuous use on our workstations shows that Arnold\u2019s storage requirements for 4 or more years of usage can be satisfied by adding a 4 TB hard drive to the system. 1 Further, the performance overhead on almost all workloads we measured was under 8%. We show that Arnold can reconstruct prior state and answer lineage queries, including backward queries (on what did this item depend?) and forward queries (what other state did this item affect?).", "num_citations": "101\n", "authors": ["496"]}
{"title": "Detecting and surviving data races using complementary schedules\n", "abstract": " Data races are a common source of errors in multithreaded programs. In this paper, we show how to protect a program from data race errors at runtime by executing multiple replicas of the program with complementary thread schedules. Complementary schedules are a set of replica thread schedules crafted to ensure that replicas diverge only if a data race occurs and to make it very likely that harmful data races cause divergences. Our system, called Frost, uses complementary schedules to cause at least one replica to avoid the order of racing instructions that leads to incorrect program execution for most harmful data races. Frost introduces outcome-based race detection, which detects data races by comparing the state of replicas executing complementary schedules. We show that this method is substantially faster than existing dynamic race detectors for unmanaged code. To help programs survive bugs in\u00a0\u2026", "num_citations": "96\n", "authors": ["496"]}
{"title": "The systematic improvement of fault tolerance in the Rio file cache\n", "abstract": " Fault injection is typically used to characterize failures and to validate and compare fault-tolerant mechanisms. However fault injection is rarely used for all these purposes to guide the design and implementation of a fault tolerant system. We present a systematic and quantitative approach for using software-implemented fault injection to guide the design and implementation of a fault-tolerant system. Our system design goal is to build a write-back file cache on Intel PCs that is as reliable as a write-through file cache. We follow an iterative approach to improve robustness in the presence of operating system errors. In each iteration, we measure the reliability of the system, analyze the fault symptoms that lead to data con option, and apply fault-tolerant mechanisms that address the fault symptoms. Our initial system is 13 times less reliable than a write-through file cache. The result of several iterations is a design that is\u00a0\u2026", "num_citations": "95\n", "authors": ["496"]}
{"title": "Language-level persistency\n", "abstract": " The commercial release of byte-addressable persistent memories, such as Intel/Micron 3D XPoint memory, is imminent. Ongoing research has sought mechanisms to allow programmers to implement recoverable data structures in these new main memories. Ensuring recoverability requires programmer control of the order of persistent stores; recent work proposes persistency models as an extension to memory consistency to specify such ordering. Prior work has considered persistency models at the abstraction of the instruction set architecture. Instead, we argue for extending the language-level memory model to provide guarantees on the order of persistent writes. We explore a taxonomy of guarantees a language-level persistency model might provide, considering both atomicity and ordering constraints on groups of persistent stores. Then, we propose and evaluate Acquire-Release Persistency (ARP), a\u00a0\u2026", "num_citations": "92\n", "authors": ["496"]}
{"title": "Exploring failure transparency and the limits of generic recovery\n", "abstract": " We explore the abstraction of failure transparency in which the operating system provides the illusion of failure-free operation. To provide failure transparency, an operating system must recover applications after hardware, operating system, and application failures, and must do so without help from the programmer or unduly slowing failure-free performance. We describe two invariants that must be upheld to provide failure transparency: one that ensures sufficient application state is saved to guarantee the user cannot discern failures, and another that ensures sufficient application state is lost to allow recovery from failures affecting application state. We find that several real applications get failure transparency in the presence of simple stop failures with overhead of 0-12%. Less encouragingly, we find that applications violate one invariant in the course of upholding the other for more than 90% of application faults and 3-15% of operating system faults, rendering transparent recovery impossible for these cases.", "num_citations": "85\n", "authors": ["496"]}
{"title": "How fail-stop are faulty programs?\n", "abstract": " Most fault-tolerant systems are designed to stop faulty programs before they write permanent data or communicate with other processes. This property (halt-on-failure) forms the core of the fail-stop model. Unfortunately, little experimental data exists on whether or not program failures follow the fail-stop model. This paper describes a tool, based on the SimOS complete-machine simulator that can trace how faults propagate through memory, disk, and functions. Using this tool on the Postgres database system, we conduct a controlled experiment to measure how often faulty programs violate the fail-stop model. We find that a significant number of faults (7%) violate the fail-stop model by writing incorrect data to stable storage before halting. We then apply Postgres' transaction mechanism to undo recent changes before a crash and find that transactions reduce fail-stop violations by a factor of 3.", "num_citations": "84\n", "authors": ["496"]}
{"title": "Accelerating mobile applications through flip-flop replication\n", "abstract": " Mobile devices have less computational power and poorer Internet connections than other computers. Computation offload, in which some portions of an application are migrated to a server, has been proposed as one way to remedy this deficiency. Yet, partition-based offload is challenging because it requires applications to accurately predict whether mobile or remote computation will be faster, and it requires that the computation be large enough to overcome the cost of shipping state to and from the server. Further, offload does not currently benefit network-intensive applications.", "num_citations": "82\n", "authors": ["496"]}
{"title": "Chimera: Hybrid program analysis for determinism\n", "abstract": " Chimera uses a new hybrid program analysis to provide deterministic replay for commodity multiprocessor systems. Chimera leverages the insight that it is easy to provide deterministic multiprocessor replay for data-race-free programs (one can just record non-deterministic inputs and the order of synchronization operations), so if we can somehow transform an arbitrary program to be data-race-free, then we can provide deterministic replay cheaply for that program. To perform this transformation, Chimera uses a sound static data-race detector to find all potential data-races. It then instruments pairs of potentially racing instructions with a weak-lock, which provides sufficient guarantees to allow deterministic replay but does not guarantee mutual exclusion.", "num_citations": "78\n", "authors": ["496"]}
{"title": "Tolerating Latency in Replicated State Machines Through Client Speculation.\n", "abstract": " Replicated state machines are an important and widely-studied methodology for tolerating a wide range of faults. Unfortunately, while replicas should be distributed geographically for maximum fault tolerance, current replicated state machine protocols tend to magnify the effects of high network latencies caused by geographic distribution. In this paper, we examine how to use speculative execution at the clients of a replicated service to reduce the impact of network and protocol latency. We first give design principles for using client speculation with replicated services, such as generating early replies and prioritizing throughput over latency. We then describe a mechanism that allows speculative clients to make new requests through replica-resolved speculation and predicated writes. We implement a detailed case study that applies this approach to a standard Byzantine fault tolerant protocol (PBFT) for replicated NFS and counter services. Client speculation trades in 18% maximum throughput to decrease the effective latency under light workloads, letting us speed up run time on single-client micro-benchmarks 1.08\u201319\u00d7 when the client is co-located with the primary. On a macro-benchmark, reduced latency gives the client a speedup of up to 5\u00d7.", "num_citations": "73\n", "authors": ["496"]}
{"title": "Operating system extensions to support host based virtual machines\n", "abstract": " This paper presents two new operating system extensions that facilitate the fast execution of host based virtual machines: KTrace and MMA. KTrace provides a convenient and efficient mechanism for writing kernel modules that control the execution of user-level processes. MMA exposes more detail of the underlying memory management hardware, providing applications with access to high performance intraaddress space protection and address space overloading functionality. These extensions are applied to UMLinux, an x86 virtual machine that implements Linux as a user-mode process. As a result, overhead for UMLinux is reduced from 33% for a CPU bound workload and 819%-1240% for system call and I/O intensive workloads, to 6% and 49%-24% respectively.", "num_citations": "73\n", "authors": ["496"]}
{"title": "Persistency for synchronization-free regions\n", "abstract": " Nascent persistent memory (PM) technologies promise the performance of DRAM with the durability of disk, but how best to integrate them into programming systems remains an open question. Recent work extends language memory models with a persistency model prescribing semantics for updates to PM. These semantics enable programmers to design data structures in PM that are accessed like memory and yet are recoverable upon crash or failure. Alas, we find the semantics and performance of existing approaches unsatisfying. Existing approaches require high-overhead mechanisms, are restricted to certain synchronization constructs, provide incomplete semantics, and/or may recover to state that cannot arise in fault-free execution.   We propose persistency semantics that guarantee failure atomicity of synchronization-free regions (SFRs) - program regions delimited by synchronization operations. Our\u00a0\u2026", "num_citations": "65\n", "authors": ["496"]}
{"title": "Storage performance-metrics and benchmarks\n", "abstract": " The metrics and benchmarks used in storage performance evaluation are discussed. The technology trends taking place in storage systems, such as disk and tape evolution, disk arrays, and solid-state disks, are highlighted. The current popular I/O benchmarks are then described, reviewed, and run on three systems: a DECstation 5000/200 running the Sprite Operating System, a SPARCstation 1+ running SunOS, and an HP Series 700 (Model 730) running HP-UX. Two approaches to storage benchmarks-LADDIS and a self-scaling benchmark with predicted performance-are also described.< >", "num_citations": "63\n", "authors": ["496"]}
{"title": "The design and verification of the Rio file cache\n", "abstract": " Today's file systems are limited in speed and reliability by memory's vulnerability to operating system crashes. Because memory is viewed as unsafe, systems periodically write modified file data back to disk. These extra disk writes lower system performance and the delay period before data is safe lowers reliability. The goal of the Rio (RAM I/O) file cache is to make ordinary main memory safe for persistent storage by enabling memory to survive operating system crashes. Reliable main memory enables the Rio file cache to be as reliable as a write-through file cache, where every write is safe instantly, and as fast as a pure write-back file cache, with no reliability-induced writes to disk. This paper describes the systematic, quantitative process we used to design and verify the Rio file cache on Intel PCs running FreeBSD and the reliability and performance of the resulting system.", "num_citations": "61\n", "authors": ["496"]}
{"title": "Speculative execution in a distributed file system\n", "abstract": " Speculator provides Linux kernel support for speculative execution. It allows multiple processes to share speculative state by tracking causal dependencies propagated through interprocess communication. It guarantees correct execution by preventing speculative processes from externalizing output, for example, sending a network message or writing to the screen, until the speculations on which that output depends have proven to be correct. Speculator improves the performance of distributed file systems by masking I/O latency and increasing I/O throughput. Rather than block during a remote operation, a file system predicts the operation's result, then uses Speculator to checkpoint the state of the calling process and speculatively continue its execution based on the predicted result. If the prediction is correct, the checkpoint is discarded; if it is incorrect, the calling process is restored to the checkpoint, and the\u00a0\u2026", "num_citations": "60\n", "authors": ["496"]}
{"title": "Parallelizing data race detection\n", "abstract": " Detecting data races in multithreaded programs is a crucial part of debugging such programs, but traditional data race detectors are too slow to use routinely. This paper shows how to speed up race detection by spreading the work across multiple cores. Our strategy relies on uniparallelism, which executes time intervals of a program (called epochs) in parallel to provide scalability, but executes all threads from a single epoch on a single core to eliminate locking overhead. We use several techniques to make parallelization effective: dividing race detection into three phases, predicting a subset of the analysis state, eliminating sequential work via transitive reduction, and reducing the work needed to maintain multiple versions of analysis via factorization. We demonstrate our strategy by parallelizing a happens-before detector and a lockset-based detector. We find that uniparallelism can significantly speed up data\u00a0\u2026", "num_citations": "57\n", "authors": ["496"]}
{"title": "Global backtracking of anthropogenic radionuclides by means of a receptor oriented ensemble dispersion modelling system in support of Nuclear-Test-Ban Treaty verification\n", "abstract": " In this paper, we introduce a methodology for quality assessment of backtracking models. We present results illustrating the level of agreement between the backtracking models, and the accuracy of each model and the ensemble model in resolving the geo-temporal reference of a single point source. Both assessments are based on an ensemble of 12 different Lagrangian particle dispersion modelling (LPDM) systems utilized in receptor oriented (adjoint) mode during an international numerical experiment dedicated to source region estimation.As major result, we can confirm that the findings of Galmarini et al. [2004b. Ensemble prediction forecasting\u2014Part II: application and evaluation. Atmospheric Environment 38, 4619\u20134632] and Delle Monache and Stull [2003. An ensemble air-quality forecast over Europe during an ozone episode. Atmospheric Environment 37, 3469\u20133474], regarding the superiority of the\u00a0\u2026", "num_citations": "56\n", "authors": ["496"]}
{"title": "Multi-stage replay with crosscut\n", "abstract": " Deterministic record-replay has many useful applications, ranging from fault tolerance and forensics to reproducing and diagnosing bugs. When choosing a record-replay solution, the system administrator must choose a priori how comprehensively to record the execution and at what abstraction level to record it. Unfortunately, these choices may not match well with how the recording is eventually used. A recording may contain too little information to support the end use of replay, or it may contain more sensitive information than is allowed to be shown to the end user of replay. Similarly, fixing the abstraction level at the time of recording often leads to a semantic mismatch with the end use of replay.", "num_citations": "48\n", "authors": ["496"]}
{"title": "An automated feedback system for computer organization projects\n", "abstract": " This paper describes a system, built and refined over the past five years, that automatically analyzes student programs assigned in a computer organization course. The system tests a student's program, then e-mails immediate feedback to the student to assist and encourage the student to continue testing, debugging, and optimizing his or her program. The automated feedback system improves the students' learning experience by allowing and encouraging them to improve their program iteratively until it is correct. The system has also made it possible to add challenging parts to each project, such as optimization and testing, and it has enabled students to meet these challenges. Finally, the system has reduced the grading load of University of Michigan's large classes significantly and helped the instructors handle the rapidly increasing enrollments of the 1990s. Initial experience with the feedback system showed\u00a0\u2026", "num_citations": "46\n", "authors": ["496"]}
{"title": "Integrating reliable memory in databases\n", "abstract": " Recent results in the Rio project at the University of Michigan show that it is possible to create an area of main memory that is as safe as disk from operating system crashes. This paper explores how to integrate the reliable memory provided by the Rio file cache into a database system. Prior studies have analyzed the performance benefits of reliable memory; we focus instead on how different designs affect reliability. We propose three  designs for integrating reliable memory into databases: non-persistent database buffer cache, persistent database buffer cache, and persistent database buffer cache with protection. Non-persistent buffer caches use an I/O interface to reliable memory and require the fewest modifications to existing databases. However, they waste memory capacity and bandwidth due to double buffering. Persistent buffer caches use a memory interface to reliable memory by mapping it  into\u00a0\u2026", "num_citations": "45\n", "authors": ["496"]}
{"title": "Performance and design evaluation of the RAID-II storage server\n", "abstract": " RAID-II is a high-bandwidth, network-attached storage server designed and implemented at the University of California at Berkeley. In this paper, we measure the performance of RAID-II and evaluate various architectural decisions made during the design process. We first measure the end-to-end performance of the system to be approximately 20 MB/s for both disk array reads and writes. We then perform a bottleneck analysis by examining the performance of each individual subsystem and conclude that the disk subsystem limits performance. By adding a custom interconnect board with a high-speed memory and bus system and parity engine, we are able to achieve a performance speedup of 8 to 15 over a comparative system using only off-the-shelf hardware.", "num_citations": "45\n", "authors": ["496"]}
{"title": "Discount checking: Transparent, low-overhead recovery for general applications\n", "abstract": " Checkpointing is a general technique for recovering applications. Unfortunately, current checkpointing systems add many seconds of overhead per checkpoint. Their high overhead prevents them from making failures transparent to users and other external entities, so failures lose visible state. This paper presents a checkpointing system called Discount Checking that is built on reliable main memory and high-speed transactions. Discount Checking can be used to make general-purpose applications recoverable easily and with low overhead. The checkpoints taken by Discount Checking are extremely fast, ranging for our target applications from 50 \u00b5s to a few milliseconds. Discount Checking\u2019s low overhead makes it possible to provide ideal failure transparency by checkpointing each externally visible event. Yet even with this high rate of checkpointing, Discount Checking slows real applications down by less than 0.6%.", "num_citations": "31\n", "authors": ["496"]}
{"title": "Operating system support for application-specific speculation\n", "abstract": " Speculative execution is a technique that allows serial tasks to execute in parallel. An implementation of speculative execution can be divided into two parts:(1) a policy that specifies what operations and values to predict, what actions to allow during speculation, and how to compare results; and (2) the mechanisms that support speculative execution, such as checkpointing, rollback, causality tracking, and output buffering.", "num_citations": "30\n", "authors": ["496"]}
{"title": "Comparing disk and memory's resistance to operating system crashes\n", "abstract": " Memory is commonly viewed as an unreliable place to store permanent data (files) because it is perceived to be vulnerable to system crashes. Yet despite all the negative implications of memory's unreliability, no data exists that quantifies how vulnerable memory actually is to system crashes. This paper quantitatively compares the vulnerability of disk and memory to operating system crashes. We use software fault injection to induce a wide variety of operating system crashes in DEC Alpha workstations running Digital Unix, ranging from bit errors in the kernel stack to deleting branch instructions to C-level allocation management errors. We find that files on disk are rarely corrupted (1.1% corruption rate), which agrees with our intuition. We also find that, surprisingly files in memory are nearly as safe as files on disk. Only 10 of the 650 crashes we observed (1.5%) corrupt any files in memory. Our data contradicts the\u00a0\u2026", "num_citations": "30\n", "authors": ["496"]}
{"title": "ExtraVirt: Detecting and recovering from transient processor faults\n", "abstract": " Reliability is becoming an increasingly important issue in modern processor design. Smaller feature sizes and more numerous transistors are projected to increase the frequency of transient faults [4, 5]. Our project, ExtraVirt, leverages the trend toward multi-core and multi-processor systems to survive these transient faults. Our goals are (1) to add fault tolerance without modifying existing operating systems, applications or hardware,(2) to minimize the time spent executing software that cannot tolerate faults, and (3) to minimize the time and space overhead needed to detect and recover from faults. We accomplish these goals by leveraging virtual-machine technology and by sharing memory and I/O devices across replicas. ExtraVirt extends prior work on VM-level fault tolerance [2] by detecting and recovering from non-fail-stop faults and by running multiple replicas efficiently on a single machine.", "num_citations": "28\n", "authors": ["496"]}
{"title": "The impact of recovery mechanisms on the likelihood of saving corrupted state\n", "abstract": " Recovery systems must save state before a failure occurs to enable the system to recover from the failure. However, recovery will fail if the recovery system saves any state corrupted by the fault. The frequency and comprehensiveness of how a recovery system saves state has a major effect on how often the recovery system inadvertently saves corrupted state. This paper explores and measures that effect. We measure how often software faults in the application and operating system cause real applications to save corrupted state when using different types of recovery systems. We find that generic recovery techniques, such as checkpointing and logging, work well for faults in the operating system. However, we find that they do not work well for faults in the application because the very actions taken to enable recovery often corrupt the state upon which successful recovery depends.", "num_citations": "25\n", "authors": ["496"]}
{"title": "Optimistic hybrid analysis: Accelerating dynamic analysis through predicated static analysis\n", "abstract": " Dynamic analysis tools, such as those that detect data-races, verify memory safety, and identify information flow, have become a vital part of testing and debugging complex software systems. While these tools are powerful, their slow speed often limits how effectively they can be deployed in practice. Hybrid analysis speeds up these tools by using static analysis to decrease the work performed during dynamic analysis. In this paper we argue that current hybrid analysis is needlessly hampered by an incorrect assumption that preserving the soundness of dynamic analysis requires an underlying sound static analysis. We observe that, even with unsound static analysis, it is possible to achieve sound dynamic analysis for the executions which fall within the set of states statically considered. This leads us to a new approach, called optimistic hybrid analysis. We first profile a small set of executions and generate a set of\u00a0\u2026", "num_citations": "24\n", "authors": ["496"]}
{"title": "JetStream: Cluster-scale parallelization of information flow queries\n", "abstract": " Dynamic information flow tracking (DIFT) is an important tool in many domains, such as security, debugging, forensics, provenance, configuration troubleshooting, and privacy tracking. However, the usability of DIFT is currently limited by its high overhead; complex information flow queries can take up to two orders of magnitude longer to execute than the original execution of the program. This precludes interactive uses in which users iteratively refine queries to narrow down bugs, leaks of private data, or performance anomalies.", "num_citations": "24\n", "authors": ["496"]}
{"title": "RAID-II: design and implementation of a large scale disk array controller\n", "abstract": " We describe the implementation of a large scale disk array controller and subsystem incorporating over 100 high performance 3.5\" disk drives. It is designed to provide 40 MB/s sustained performance and 40 GB capacity in three 19\" racks. The array controller forms an integral part of a file server that attaches to a Gb/s local area network. The controller implements a high bandwidth interconnect between an interleaved memory, an XOR calculation engine, the network interface (HIPPI), and the disk interfaces (SCSI). The system is now functionally operational, and we are tuning its performance. We review the design decisions, history, and lessons learned from this three year university implementation effort to construct a truly large scale system assembly.", "num_citations": "23\n", "authors": ["496"]}
{"title": "Software wear management for persistent memories\n", "abstract": " The commercial release of byte-addressable persistent memories (PMs) is imminent. Unfortunately, these devices suffer from limited write endurance\u2014without any wear management, PM lifetime might be as low as 1.1 months. Existing wear-management techniques introduce an additional indirection layer to remap memory across physical frames and require hardware support to track fine-grain wear. These mechanisms incur storage overhead and increase access latency and energy consumption.", "num_citations": "22\n", "authors": ["496"]}
{"title": "Two papers on RAIDs\n", "abstract": " A RAID is a Redundant Array of Inexpensive Disk, a new way to organize small format disk devices to drastically increase I/O bandwidth. In this technical report, we describe the RAID concept, the basic RAID levels, a more detailed analysis of RAID performance on reliability, and our initial prototyping plans.", "num_citations": "21\n", "authors": ["496"]}
{"title": "Foldable massaging mattress\n", "abstract": " A foldable massaging mattress is disclosed. The massaging mattress includes a mattress body having a protruding pillow portion at an end thereof and a track having a first end and a second end. The track is arranged in the mattress body and extends in the longitudinal midway of the mattress body. The track has at least two sections pivotally connected with each other. A gear case is arranged in the protruding pillow portion of the mattress body in a location adjacent to the first end of the track. A sprocket case is arranged in the mattress body in a location adjacent to the second end of the track. A DC motor is included for driving the gear case. A carriage has opposed two ends and is movable along the track. A chain runs between the gear and sprocket cases and has two ends attached to the opposed two ends of the carriage. The chain is driven by the gear case.", "num_citations": "18\n", "authors": ["496"]}
{"title": "Memory persistency: Semantics for byte-addressable nonvolatile memory technologies\n", "abstract": " Emerging nonvolatile memory technologies (NVRAM) promise the performance of DRAM with the persistence of disk. However, constraining the NVRAM write order, necessary to ensure recovery correctness, limits the NVRAM write concurrency and degrades throughput. New memory interfaces are required to efficiently describe write constraints and allow high-performance and high-concurrency data structures. The authors introduce memory persistency, a new approach to designing persistent memory interfaces that builds on the familiar framework of memory consistency to provide an interface for constraining the order in which persistent writes can occur with respect to failure. Similar to memory consistency, memory persistency models may be relaxed to improve performance. The authors describe the design space of memory persistency, introduce several memory persistency models, and evaluate their ability\u00a0\u2026", "num_citations": "17\n", "authors": ["496"]}
{"title": "Optimizing delay in delayed-write file systems\n", "abstract": " Delayed writes are used in most file systems to improve performance over write-through while limiting the amount of data lost in a crash. Delayed writes improve performance in three ways: by allowing the file cache to absorb some writes without ever propagating them to disk (write cancellation), by improving the efficiency of disk writes, and by spreading bursts out over time. For each of these benefits, this paper determines the optimal value for the delay interval: the smallest delay that achieves as good (or nearly as good) performance as an infinite delay. The largest value for optimal delay of any of the three benefits is. This value is 1-10 seconds for current systems, implying that the delay used today (30 seconds) is much too large; a smaller delay would minimize data loss yet maintain the same performance.", "num_citations": "15\n", "authors": ["496"]}
{"title": "Iodine: fast dynamic taint tracking using rollback-free optimistic hybrid analysis\n", "abstract": " Dynamic information-flow tracking (DIFT) is useful for enforcing security policies, but rarely used in practice, as it can slow down a program by an order of magnitude. Static program analyses can be used to prove safe execution states and elide unnecessary DIFT monitors, but the performance improvement from these analyses is limited by their need to maintain soundness. In this paper, we present a novel optimistic hybrid analysis (OHA) to significantly reduce DIFT overhead while still guaranteeing sound results. It consists of a predicated whole-program static taint analysis, which assumes likely invariants gathered from profiles to dramatically improve precision. The optimized DIFT is sound for executions in which those invariants hold true, and recovers to a conservative DIFT for executions in which those invariants are false. We show how to overcome the main problem with using OHA to optimize live executions\u00a0\u2026", "num_citations": "13\n", "authors": ["496"]}
{"title": "Cooperative ReVirt: adapting message logging for intrusion analysis\n", "abstract": " Virtual-machine logging and replay enables system administrators to analyze intrusions more completely and with greater integrity than traditional system loggers. One challenge in these types of systems is the need to log a potentially large volume of network traffic. Cooperative ReVirt adds message-logging techniques to ReVirt to reduce the amount of network traffic that needs to be logged. Cooperative ReVirt adapts message-logging techniques to address the challenges of intrusion analysis, such as the need to work in the presence of network attacks and unreliable networks, the need to support asymmetric trust relationships among computers, and the need to support dynamic trust and traffic patterns. Cooperative ReVirt is able to reduce the log volume needed to replay a computer by an average of 70% for a variety of distributed computing benchmarks, while adding less than 7% overhead. Measurements of a live network indicate that Cooperative ReVirt would be able to avoid logging 85% of the received network data.", "num_citations": "13\n", "authors": ["496"]}
{"title": "Relaxed persist ordering using strand persistency\n", "abstract": " Emerging persistent memory (PM) technologies promise the performance of DRAM with the durability of Flash. Several language-level persistency models have emerged recently to aid programming recoverable data structures in PM. Unfortunately, these persistency models are built upon hardware primitives that impose stricter ordering constraints on PM operations than the persistency models require. Alternative solutions use fixed and inflexible hardware logging techniques to relax ordering constraints on PM operations, but do not readily apply to general synchronization primitives employed by language-level persistency models. Instead, we propose StrandWeaver, a hardware strand persistency model, to minimally constrain ordering on PM operations. StrandWeaver manages PM order within a strand, a logically independent sequence of operations within a thread. PM operations that lie on separate strands are\u00a0\u2026", "num_citations": "12\n", "authors": ["496"]}
{"title": "Persistency programming 101\n", "abstract": " Emerging non-volatile memory (NVRAM) technologies offer the durability of disk with byte-addressability and access latencies similar to DRAM. Future systems will likely attach these NVRAMs to a DRAM-like memory bus [1, 2]. Such systems enable the construction of high performing, in-memory, recoverable data structures (RDS)[1, 2]. The tenets of creating RDSs revolve around ordering writes to the data structure. However, existing architectures do not provide efficient mechanisms to order writes all the way through NVRAM. Pelley, Chen, and Wenisch [3] introduce persistency models (drawing on memory consistency model research) to reason about and order writes to NVRAM. Here, we introduce notation to concisely and precisely define the models to help draw better parallels to existing memory consistency models. These precise definitions make it easier to reason about the interaction between instruction execution and NVRAM write order. Achieving the desired order of NVRAM writes across threads is tricky. We show two generic coding patterns to illustrate how to leverage persistency models to achieve the desired order of writes. In particular, one code pattern (observe) leverages relaxed persistency models and can be used to enforce only the absolute minimum NVRAM write orderings required for correct recovery.", "num_citations": "9\n", "authors": ["496"]}
{"title": "\u2026 and region serializability for all\n", "abstract": " A desirable concurrency semantics to provide for programs is region serializability. This strong semantics guarantees that all program regions between synchronization operations appear to execute in some global and serial order consistent with program order. Unfortunately, this guarantee is currently provided only to programs that are free of data races. For programs with data races, system designers currently face a difficult trade-off between losing all semantic guarantees and hurting performance. In this paper, we argue that region serializability should be guaranteed for all programs, including those with data races. This allows programmers, compilers, and other tools to reason about a program execution as an interleaving of code regions rather than memory instructions. We show one way to provide this guarantee with an execution style called uniparallelism and simple compiler support. The cost of the system is a 100% increase in utilization. However, if there are sufficient spare cores on the computer, the system adds a median overhead of only 15% for 4 threads.", "num_citations": "9\n", "authors": ["496"]}
{"title": "Intraday nonlinear behavior of stock prices\n", "abstract": " The discovery that rates of return to shares of common stock exhibit significant non-linear behavior was first reported in Hinich and Patterson (1985). Subsequently, Ash-ley and Patterson (1986), Scheinkman and LaBaron (1989), and Brockett, Hinich, and Patterson (1988) also reported finding nonlinear behavior in stock market rates of return. Note that rates of return are analyzed rather than share prices because price series are highly nonstationary in the mean. Hsieh (1989) finds evidence of nonlin-ear behavior in daily spot foreign exchange rates. Hinich and Patterson (1985, 1989, 1990) and Brockett, Hinich, and Patterson (1988) applied the Hinich (1982) bi-spectrum based linearity test to daily rates of return of individual common stocks, whereas Ashley and Patterson (1986) used a bootstrap linearity test applied to daily rates of return of individual stocks and two market indices", "num_citations": "8\n", "authors": ["496"]}
{"title": "Language support for memory persistency\n", "abstract": " Memory persistency models enable maintaining recoverable data structures in persistent memories and prior work has proposed ISA-level persistency models. In addition to these models, we argue for extending language-level memory models to provide persistence semantics. We present a taxonomy of guarantees a language-level persistency model could provide and characterize their programmability and performance.", "num_citations": "7\n", "authors": ["496"]}
{"title": "Tarp: Translating acquire-release persistency\n", "abstract": " The commercial release of byte-addressable persistent memories, such as Intel/Micron 3D XPoint memory, is imminent. Ongoing research has sought mechanisms to allow programmers to implement recoverable data structures in these new main memories. Ensuring recoverability requires programmer control on the order of stores to the persistent memory. We refer to the act of writing a store durably as a persist. Recent works [1]\u2013[3] have proposed persistency models as an extension to memory consistency models to specify an order on persists. All these persistency models have been specified at the instruction set architecture (ISA) level. That is, programmers must reason about recovery correctness at the abstraction of assembly instructions, an approach that is error prone and places an unreasonable burden on the programmer. Furthermore, since the ISA mechanisms differ in sometimes subtle ways, it is difficult to write portable recoverable programs. Just as language-level memory models make shared-memory parallel programs portable over different systems, we believe that a language-level persistency model is critical for writing portable recoverable software. In this work, we propose a language-level persistency model that extends the data-race-free (DRF) memory model espoused by high-level programming language such as C++ 11 and Java. The DRF model guarantees a sequentially consistent execution for properly-labeled programs. Moreover, the lack of data races ensures that synchronization-free regions (SFR, code executed between two sync. accesses) executed on one thread become visible to other threads atomically\u00a0\u2026", "num_citations": "6\n", "authors": ["496"]}
{"title": "Tango: accelerating mobile applications through flip-flop replication\n", "abstract": " Mobile devices have less computational power and poorer Internet connections than other computers. Computation offload [1, 2, 3, 4], in which some portions of an application are migrated to a server, has been proposed as one way to remedy this deficiency. Yet, partitionbased offload is challenging because it requires applications to accurately predict whether mobile or remote computation will be faster, and it requires that the computation be large enough to overcome the cost of shipping state to and from the server. Further, offload does not currently benefit network-intensive applications.", "num_citations": "6\n", "authors": ["496"]}
{"title": "Reliability hierarchies\n", "abstract": " Hierarchies of diverse storage levels have been analyzed extensively for their ability to achieve both good performance and low cost. The article argues that we should view hierarchies also as a way to achieve both good reliability and low overhead. After discussing the design of a reliability hierarchy, we suggest two metrics to use in evaluating the overall reliability of a reliability hierarchy: mean time to data loss (MTTDL) and data loss rate. These and other metrics allow researchers to evaluate the reliability/performance tradeoffs quantitatively for new storage devices and hierarchies. We use this framework to evaluate how the Rio file cache affects the mean time to data loss and data loss rate of an existing storage hierarchy. Rio improves MTTDL over a standard delayed-write file cache by an order of magnitude and can be used with a long write-back delay without increasing data loss rate. Rio fills in the reliability\u00a0\u2026", "num_citations": "6\n", "authors": ["496"]}
{"title": "Input-output performance evaluation: self-scaling benchmarks, predicted performance\n", "abstract": " Over the past 20 years, processor performance has been growing much faster than input/output (I/O) performance. As this occurs, overall system speed becomes more and more limited by the speed of I/O systems and hence I/O systems are evolving to keep up with processor performance. This evolution renders current I/O performance evaluation techniques obsolete or irrelevant, despite their increasing importance. This dissertation investigates two new ideas in I/O evaluation, self-scaling benchmarks and predicted performance. This dissertation\u2019s self-scaling benchmark seeks to measure and report relevant workloads for a wide range of input/output systems. To do so, it scales aspects of its workload to account for the differences in I/O systems. For example, it dynamically discovers the size of the system\u2019s file cache and reports how performance varies both in and out of the file cache. The", "num_citations": "6\n", "authors": ["496"]}
{"title": "Controlling persistent writes to non-volatile memory based on persist buffer data and a persist barrier within a sequence of program instructions\n", "abstract": " A data processing system 2 including non-volatile memory 22 manages the ordering of writes to the non-volatile memory and persist barrier instructions using a persist buffer storing persist buffer data. A write controller responds to the persist buffer data to prevent writing to the non-volatile memory for instructions following a given persist barrier instruction within a sequence of program instructions before the writes to the non-volatile memory which precede that given persist barrier instruction have at least been acknowledged as received by the memory system containing the non-volatile memory. In the case of a multi-core system, cache snooping mechanisms are used to pass persistency dependence data between cores such that strong persist atomicity may be tracked and managed between the cores.", "num_citations": "5\n", "authors": ["496"]}
{"title": "Transmission assembly for a massaging chair\n", "abstract": " A transmission assembly for a massaging chair includes an integral transmission casing with a longitudinal passage and two latitudinal passages to respectively receive therein a longitudinal shaft, a chopping shaft and a rubbing shaft so that minimal noise is created during operation of the transmission assembly.", "num_citations": "5\n", "authors": ["496"]}
{"title": "Persistent messages in local transactions\n", "abstract": " We present a new model for handling messages and state in a distributed application that we call Messages in Local Transactions (MLT). Under this model, messages and data are not lost after crashes, and all sends and receives are performed in local transactions. The model is unique in that it guarantees consistent recovery without the complexity or overhead of other recovery techniques. Applications using MLT do not need to coordinate checkpoints, track causal dependencies, or perform distributed commits. We show that MLT can be implemented using any reliable protocol. Finally, we describe our implementation of Vistagrams, a system based on the MLT model. We show that Vistagrams are just as fast as traditional messages, despite the recoverability they offer. The efficiency of our model and our Vistagrams implementation is enabled by the availability of fast stable storage, such as the reliable memory\u00a0\u2026", "num_citations": "5\n", "authors": ["496"]}
{"title": "ShortCut: accelerating mostly-deterministic code regions\n", "abstract": " Applications commonly perform repeated computations that are mostly, but not exactly, similar. If a subsequent computation were identical to the original, the operating system could improve performance via memoization, ie, capturing the differences in program state caused by the computation and applying the differences in lieu of re-executing the computation. However, opportunities for generic memoization are limited by a myriad of differences that arise during execution, eg, timestamps differ and communication yields non-deterministic responses. Such difference cause memoization to produce incorrect state.", "num_citations": "4\n", "authors": ["496"]}
{"title": "Knockoff: Cheap versions in the cloud\n", "abstract": " Cloud-based storage provides reliability and ease-of-management. Unfortunately, it can also incur significant costs for both storing and communicating data, even after using techniques such as chunk-based deduplication and delta compression. The current trend of providing access to past versions of data exacerbates both costs.", "num_citations": "4\n", "authors": ["496"]}
{"title": "CIDS: Causality-based Intrusion Detection System\n", "abstract": " This paper presents a new style of intrusion de-tection system, CIDS, that links network and host based intrusion detection systems together using causal dependencies. This combination provides a number of benefits. First, combining alerts reduces false positives for both network and host based approaches. Second, host based intrusion detection systems have significantly less data to process since they only act on processes and files that are causally linked to a suspicious network packet. Third, alarms are associated with a specific network service, allowing system administrators to act accordingly after detecting a compromise.", "num_citations": "4\n", "authors": ["496"]}
{"title": "Plato: A platform for virtual machine services\n", "abstract": " Virtual machines are being used to add new services to system level software. One challenge these virtual machine services face is the semantic gap between VM services and the machine-level interface exposed by the virtual machine monitor. Using the virtual machine monitor interface, VM services have access to hardware-level events like Ethernet packets or disk I/O. However, virtual machine services also benefit from guest software (software running inside the virtual machine) semantic information, like sockets and files. These abstractions are specific to the guest software context and are not exposed directly by the machine-level virtual machine monitor interface.Existing ways to bridge this semantic gap are either adhoc or use debuggers. Ad-hoc methods often lead to cutting-and-pasting large sections of the guest operating system to reconstruct its interpretation of the hardware level events. Debuggers add too much overhead for production environments. Both ad-hoc methods and debuggers could cause unwanted perturbations to the virtual system.", "num_citations": "4\n", "authors": ["496"]}
{"title": "Theory and Practice of Failure Transparency\n", "abstract": " System and application failures are all too common. In this dissertation we argue that operating systems should provide the fundamental abstraction we call failure transparency\u2014the illusion that systems and applications do not fail. Systems that provide failure transparency attempt to completely mask failures from users, and failure handling from programmers. We construct a theory of consistent recovery that provides the fundamental rules for recovering transparently after a failure. In addition to aiding our quest for failure transparency, the theory unifies all existing recovery protocols: they are all simply variations on the theme of the theory's central invariant. Using the theory as a launching point, we construct a series of systems that get us closer to providing failure transparency. The first such system is Vista, a lightweight transaction library. Vista is built on reliable memory, and as a result realizes remarkable\u00a0\u2026", "num_citations": "4\n", "authors": ["496"]}
{"title": "Rio: Storing Files Reliably in Memory\n", "abstract": " Memory is currently a second-class citizen of the storage hierarchy because of its vulnerability to power failures and software crashes. Designers have traditionally sacrificed either reliability or performance when using memory as a cache for disks; our goal is to do away with this tradeoff by making memory as reliable as disks. The Rio (RAM I/O) project at Michigan is modifying the Digital Unix (formerly OSF/1) kernel to protect the file cache from operating system crashes. If successful, making memory as reliable as disks will 1) improve file cache performance to that of a pure write-back scheme by eliminating all reliability-caused writes to disk; 2) improve reliability to that of a write-through scheme by making memory a reliable place to store files long term; and 3) simplify applications such as file systems and databases by eliminating write-back daemons and complex commit and checkpointing protocols.", "num_citations": "4\n", "authors": ["496"]}
{"title": "12th USENIX Symposium on Operating Systems Design and Implementation\n", "abstract": " Thanks for joining us in Savannah, GA, for the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI'16). As part of our commitment to open access to research, the full Proceedings and the presentation slides are free and open to the public on the OSDI'16 Program page.", "num_citations": "3\n", "authors": ["496"]}
{"title": "A case for custom silicon in enabling low-cost information technology for developing regions\n", "abstract": " Information and communications technology has the potential for deep social impact in developing regions but today's typical ICT devices--laptops, mobile phones, and similar devices--are often still too expensive for many scenarios. In this paper, we argue that custom integrated circuits can enable a new tier of low-cost information access devices with a price point that will make them widely accessible. And, with control over the silicon, these systems can economically address many other challenges. To evaluate our claim, we focus on a deceptively simple problem--low-cost information access for illiterate populations through audio recordings--and show how custom silicon allows us to reduce cost, lower power, leverage conventional infrastructure in unconventional ways, and optimize the interface for usability. In particular, we show how a rural audio computer can be designed around just three chips, use an\u00a0\u2026", "num_citations": "3\n", "authors": ["496"]}
{"title": "Epoch parallelism: one execution is not enough\n", "abstract": " The conventional approach for using multiprocessors requires programmers to write correct, scalable parallel programs. Unfortunately, writing such programs remains a daunting task, despite decades of research on parallel languages, programming models, static and dynamic analysis tools, and synchronization primitives. We argue that it is futile to expect programmers to write a program that is both correct and scalable. Instead, we propose a different style of parallel execution, called epoch parallelism, that separates the goals of correctness and scalability into different executions. One execution (called the epoch-parallel execution) runs a program that is aimed at achieving correctness (or some other desirable property) but need not scale to multiple processors (eg, the program may be single-threaded). The other execution (called the thread-parallel execution) runs a program that is aimed at scaling to multiple processors but need not always be correct. We then combine these two executions to achieve both correctness and scalability. The epoch-parallel execution counts as the \u201creal\u201d execution, and the thread-parallel execution speeds up the epoch-parallel execution by allowing multiple epochs to run at the same time.", "num_citations": "3\n", "authors": ["496"]}
{"title": "the importance of atmospheric transport Modelling: over ten years of cooperation between the world Meteorological organization and the ctbto\n", "abstract": " by peter chen, gerhard wotawa and andreas becker atM is an integral part of radionuclide monitoring, which is carried out by the ctbt\u2019s radionuclide facilities that belong to the International Monitoring System. Radionuclide monitoring technology is complementary to the three waveform verification technologies\u2013seismic, infrasound and hydroacoustic\u2013employed by the Comprehensive Nuclear-Test-Ban Treaty\u2019s (CTBT) verification regime to monitor compliance with the treaty. while waveform monitoring is utilized for event detection and location and could be used to differentiate between an earthquake and an explosion, detecting relevant radionuclides or noble gases is essential for the unambiguous identification of the nuclear origin of an event. Radionuclide technology combined with ATM thus provides the means to identify the \u201csmoking gun\u201d needed to prove a possible violation of the treaty. with its \u201cforensic proof\u201d of nuclear explosions, radionuclide technology is of crucial importance to the entire verification effort. consistent with a hypothesized release from the event in the DprK.", "num_citations": "3\n", "authors": ["496"]}
{"title": "Unix I/O performance in workstations and mainframes\n", "abstract": " Rapid advances in processor performance have shifted the performance bottleneck to I/O systems. The relatively slow rate of improvement in I/O is due in part to a lack of quantitative performance analysis of software and hardware alternatives. Using a new self-scaling I/O benchmark, we provide such an evaluation for 11 hardware configurations using 9 variations of the Unix operating system. In contrast to processor performance comparisons, where factors of 2 are considered large, we find differences of factors of 10 to 100 in I/O systems. The principal performance culprits are the policies of different Unix operating systems; some policies on writes to the file cache will cause processors to run at magnetic disk speeds instead of at main memory speeds. These results suggest a greater emphasis be placed on I/O performance when making operating system policy decisions.", "num_citations": "3\n", "authors": ["496"]}
{"title": "Failure-atomic synchronization-free regions\n", "abstract": " Emerging persistent memory (PM) technologies, such as Intel and Micron\u2019s 3D XPoint, aim to combine the byteaddressability of DRAM with the durability of storage. The promise of PM is to enable data structures that provide the convenience and performance of in-place load-store manipulation, and yet persist across failures, such as power interruptions and OS or program crashes. Following such a crash, volatile program state (DRAM, program counters, registers, etc.) are lost, but PM state is preserved. A recovery process can then examine the PM state, reconstruct required volatile state, and resume program execution.Reasoning about the correctness of recovery code requires precise semantics for the allowable PM state after a failure. Specifying such semantics is complicated by the desire to support concurrent PM accesses from multiple threads and optimizations that reorder or coalesce accesses. The state observed at recovery can be greatly simplified by providing failure atomicity of sets of PM updates. Failure atomicity assures that either all or none of the updates in a set are visible after failure, reducing the state space recovery code might observe.", "num_citations": "2\n", "authors": ["496"]}
{"title": "A low-cost audio computer for information dissemination among illiterate people groups\n", "abstract": " We present Literacy in Technology (LIT), a low power, low cost audio processor for information dissemination among illiterate people groups in developing regions. The 265 K gate, 8 million transistor, 23 mm 2 , ARM Cortex M0 processor uses a novel memory hierarchy consisting of an on chip 128 kB true LRU cache and off-chip NAND Flash. LIT reduces initial acquisition cost through a high-level of integration that results in a low board-level component count. In addition, it also reduces recurring cost through design decisions that lower energy consumption. LIT's multiple power operational modes and power management schemes are specifically designed for efficient operation on Carbon Zinc batteries. These are commonly found in developing regions and allow LIT to be priced at a point that is viable for illiterate people groups in developing regions.", "num_citations": "2\n", "authors": ["496"]}
{"title": "A comment on\" An analytical model for designing memory hierarchies\"\n", "abstract": " In our paper, \"An analytical model for designing memory hierarchies\" (see ibid., vol. 45, no. 10, p. 180-1, 194 (1996)), we made the following statement: \"Failing to apply a specific model of workload locality makes it impossible to provide an easily used, closed-form solution for the optimal cache configuration, and so the results from these papers have contained dependencies on the cache configuration-the number of levels, or the sizes and hit rates of the levels.\" Our description did not accurately reflect the contents of the paper by J.E. MacDonald and K.L. Sigworth (1975), and we regret any false impressions caused by the inaccuracy.", "num_citations": "2\n", "authors": ["496"]}
{"title": "Raid-ii: A scalable storage architecture for high-bandwidth network\n", "abstract": " RAID-II (RAID the second) is a scalable high-bandwidth network le server for heterogeneous computing environments characterized by a mixture of high-bandwidth scientic, engineering and multi-media applications and low-latency high-transaction-rate UNIX applications. RAID-II is motivated by three observations: applications are becoming more bandwidth intensive, the I/O bandwidth of workstations is decreasing with respect to MIPS, and recent technological developments in high-performance networks and secondary storage systems make it economical to build high-bandwidth network storage systems. Unlike most existing le servers that use a bus as a system backplane, RAID-II achieves scalability by treating the network as the system backplane. RAID-II is notable because it phys-ically separates les service, the management of le metadata, from storage service, the storage and transfer of le data; stripes les over multiple storage servers for improved performance and reliability; provides separate mechanisms for high-bandwidth and low-latency I/O requests; im-plements a RAID level 5 storage system; and runs LFS, the Log-Structured File System, which is specically designed to support high-bandwidth I/O and RAID level 5 storage systems.", "num_citations": "2\n", "authors": ["496"]}
{"title": "Sound garbage collection for C using pointer provenance\n", "abstract": " Garbage collection (GC) support for unmanaged languages can reduce programming burden in reasoning about liveness of dynamic objects. It also avoids temporal memory safety violations and memory leaks. Sound GC for weakly-typed languages such as C/C++, however, remains an unsolved problem. Current value-based GC solutions examine values of memory locations to discover the pointers, and the objects they point to. The approach is inherently unsound in the presence of arbitrary type casts and pointer manipulations, which are legal in C/C++. Such language features are regularly used, especially in low-level systems code.  In this paper, we propose Dynamic Pointer Provenance Tracking to realize sound GC. We observe that pointers cannot be created out-of-thin-air, and they must have provenance to at least one valid allocation. Therefore, by tracking pointer provenance from the source (e.g., malloc\u00a0\u2026", "num_citations": "1\n", "authors": ["496"]}
{"title": "Iodine\n", "abstract": " Formal Verification of Multithreaded Software Page 1 IEEE Symposium on Security and Privacy May 2019 UNIVERSITY OF MICHIGAN * \u2020 IODINE Fast Dynamic Taint Tracking Using Rollback-free Optimistic Hybrid Analysis Subarno Banerjee*, David Devecsery\u2020, Peter M. Chen*, Satish Narayanasamy* Page 2 1 Data Leaks In the news\u2026 Sep \u201817 143M \ud83d\udc64 Mar \u201819 passwords stored in readable format 600M \ud83d\udc64 Nov \u201918 500M \ud83d\udc64 1.8B US ~500 companies 2018 Cost of a Data Breach Study www.ibm.com/security/data-breach Data Breaches shutdown after data leaks 0.5M \ud83d\udc64 Apr \u201819 exposed user data 1B \ud83d\udc64 Mar \u201818 Page 3 Dynamic Taint Tacking tracks information flow 2 Taint Tracking is slow! Optimistic Hybrid Analysis with Safe Elisions improves ! name scanf( ); send( ); cc# Page 4 explicit data flows Propagation Associate taints with sensitive data Propagate taints to derived values Check tainted values don\u2019t reach files to \u2026", "num_citations": "1\n", "authors": ["496"]}
{"title": "Decoupling dynamic program analysis from execution in virtual environments\n", "abstract": " Dynamic program analysis is decoupled from execution in virtual computer environments so that program analysis can be performed on a running computer program without affecting or perturbing the workload of the system on which the program is executing. Decoupled dynamic program analysis is enabled by separating execution and analysis into two tasks:(1) recording, where system execution is recorded with minimal interference, and (2) analysis, where the execution is replayed and analyzed.", "num_citations": "1\n", "authors": ["496"]}
{"title": "How fail-stop are faulty processes\n", "abstract": " Most fault-tolerant systems assume that faulty processes stop before writing permanent data or communicating with other processes. This property (halt-on-failure) forms the core of the fail-stop model. Unfortunately, little experimental data exists on whether or not process failures follow the fail-stop model. This paper describes a tool, based on the SimOS complete-machine simulator, that can trace how faults propagate through memory, disk, and functions. Using this tool on the Postgres database system, we conduct a controlled experiment to measure how often faulty processes violate the fail-stop model. We find that a significant number of faults (7%) violate the fail-stop model by writing incorrect data to stable storage before halting. We then apply Postgres\u2019 transaction mechanism to undo recent changes before a crash and find that transactions reduce fail-stop violations by a factor of 3.", "num_citations": "1\n", "authors": ["496"]}
{"title": "RAID: High-performance, reliable secondary storage\n", "abstract": " Disk arrays were proposed in the 1980s as a way to use parallelism between multiple disks to improve aggregate I/O performance. Today they appear in the product lines of most major computer manufacturers. This article gives a comprehensive overview of disk arrays and provides a framework in which to organize current and future work. First, the article introduces disk technology and reviews the driving forces that have popularized disk arrays: performance and reliability. It discusses the two architectural techniques used in disk arrays: striping across multiple disks to improve performance and redundancy to improve reliability. Next, the article describes seven disk array architectures, called RAID (Redundant Arrays of Inexpensive Disks) levels 0-6 and compares their performance, cost, and reliability. It goes on to discuss advanced research and implementation topics such as refining the basic RAID levels to\u00a0\u2026", "num_citations": "1\n", "authors": ["496"]}
{"title": "Strand persistency\n", "abstract": " Persistent memory (PM) technologies, such as Intel and Micron\u2019s 3D XPoint, are here\u2014OEMs are already evaluating engineering samples and volume shipments are expected in early 2019. PMs aim to combine byte-addressability of DRAM and durability of storage devices. Unlike traditional blockbased storage devices, such as hard disks and SSDs, PMs can be accessed using a byte-addressable load-store interface, avoiding the expensive software layers required to access storage, and allowing for fine-grained PM manipulation. As PMs are durable, they retain data across failures such as power interruptions and program crashes. Upon failure, the volatile program state in hardware caches, registers, and DRAM is lost. In contrast, PM retains its contents\u2014a recovery process can inspect these contents, reconstruct required volatile state, and resume program execution. Several persistency models have been proposed in the past to enable writing recoverable software, both in hardware [1],[2] and programming languages [3]\u2013[7]. Like prior works, we refer to the act of completing a store operation to PM as a persist. Persistency models enable two key properties. First, they allow programmers to reason about the order in which persists are made. Similar to memory consistency models, which order visibility of shared memory writes, memory persistency models govern the order of persists to PM. Second, they enable failure-atomicity for a set of persists. In case of failure, either all or none of the updates within a failure-atomic set are visible to recovery.Recent works [3],[4] extend the memory models of highlevel languages, such as C++ and Java, with\u00a0\u2026", "num_citations": "1\n", "authors": ["496"]}