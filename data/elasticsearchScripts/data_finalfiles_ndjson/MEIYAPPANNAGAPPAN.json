{"title": "Curating github for engineered software projects\n", "abstract": " Software forges like GitHub host millions of repositories. Software engineering researchers have been able to take advantage of such a large corpora of potential study subjects with the help of tools like GHTorrent and Boa. However, the simplicity in querying comes with a caveat: there are limited means of separating the signal (e.g. repositories containing engineered software projects) from the noise (e.g. repositories containing home work assignments). The proportion of noise in a random sample of repositories could skew the study and may lead to researchers reaching unrealistic, potentially inaccurate, conclusions. We argue that it is imperative to have the ability to sieve out the noise in such large repository forges. We propose a framework, and present a reference implementation of the framework as a tool called reaper, to enable researchers to select GitHub repositories that contain evidence of an\u00a0\u2026", "num_citations": "217\n", "authors": ["200"]}
{"title": "Abstracting log lines to log event types for mining software system logs\n", "abstract": " Log files contain valuable information about the execution of a system. This information is often used for debugging, operational profiling, finding anomalies, detecting security threats, measuring performance etc. The log files are usually too big for extracting this valuable information manually, even though manual perusal is still one of the more widely used techniques. Recently a variety of data mining and machine learning algorithms are being used to analyze the information in the log files. A major road block for the efficient use of these algorithms is the inherent variability present in every log line of a log file. Each log line is a combination of a static message type field and a variable parameter field. Even though both these fields are required, the analyses algorithm often requires that these be separated out, in order to find correlations in the repeating log event types. This disentangling of the message and\u00a0\u2026", "num_citations": "110\n", "authors": ["200"]}
{"title": "Efficiently extracting operational profiles from execution logs using suffix arrays\n", "abstract": " An important software reliability engineering tool is operational profiles. In this paper we propose a cost effective automated approach for creating second generation operational profiles using execution logs of a software product. Our algorithm parses the execution logs into sequences of events and produces an ordered list of all possible subsequences by constructing a suffix-array of the events. The difficulty in using execution logs is that the amount of data that needs to be analyzed is often extremely large (more than a million records per day in many applications). Our approach is very efficient. We show that our approach requires O(N) in space and time to discover all possible patterns in N events. We discuss a practical implementation of the algorithm in the context of the logs from a large cloud computing system.", "num_citations": "56\n", "authors": ["200"]}
{"title": "Do bugs foreshadow vulnerabilities? a study of the chromium project\n", "abstract": " As developers face ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. Vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. However, conceptually vulnerabilities are different from bugs: they represent abusive functionality as opposed to wrong or insufficient functionality commonly associated with traditional, non-security bugs. In this study, we performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. Using logistic regression analysis, we examined how various categories of pre-release bugs (e.g. Stability, compatibility, etc.) are associated with post\u00a0\u2026", "num_citations": "53\n", "authors": ["200"]}
{"title": "Artificial intelligence and social simulation: Studying group dynamics on a massive scale\n", "abstract": " Recent advances in artificial intelligence and computer science can be used by social scientists in their study of groups and teams. Here, we explain how developments in machine learning and simulations with artificially intelligent agents can help group and team scholars to overcome two major problems they face when studying group dynamics. First, because empirical research on groups relies on manual coding, it is hard to study groups in large numbers (the scaling problem). Second, conventional statistical methods in behavioral science often fail to capture the nonlinear interaction dynamics occurring in small groups (the dynamics problem). Machine learning helps to address the scaling problem, as massive computing power can be harnessed to multiply manual codings of group interactions. Computer simulations with artificially intelligent agents help to address the dynamics problem by implementing social\u00a0\u2026", "num_citations": "37\n", "authors": ["200"]}
{"title": "Do bugs foreshadow vulnerabilities? An in-depth study of the chromium project\n", "abstract": " As developers face an ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. Conceptually, however, vulnerabilities differ from bugs: they represent an abuse of functionality as opposed to insufficient functionality commonly associated with traditional, non-security bugs. We performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. We used logistic regression analysis, ranking analysis, bug type classifications, developer experience, and vulnerability severity metrics to examine the\u00a0\u2026", "num_citations": "35\n", "authors": ["200"]}
{"title": "Towards improving statistical modeling of software engineering data: think locally, act globally!\n", "abstract": " Much research in software engineering (SE) is focused on modeling data collected from software repositories. Insights gained over the last decade suggests that such datasets contain a high amount of variability in the data. Such variability has a detrimental effect on model quality, as suggested by recent research. In this paper, we propose to split the data into smaller homogeneous subsets and learn sets of individual statistical models, one for each subset, as a way around the high variability in such data. Our case study on a variety of SE datasets demonstrates that such local models can significantly outperform traditional models with respect to model fit and predictive performance. However, we find that analysts need to be aware of potential pitfalls when building local models: firstly, the choice of clustering algorithm and its parameters can have a substantial impact on model quality. Secondly, the data\u00a0\u2026", "num_citations": "32\n", "authors": ["200"]}
{"title": "Toward a first-principles integrated simulation of tokamak edge plasmas\n", "abstract": " Performance of the ITER is anticipated to be highly sensitive to the edge plasma condition. The edge pedestal in ITER needs to be predicted from an integrated simulation of the necessary first-principles, multi-scale physics codes. The mission of the SciDAC Fusion Simulation Project (FSP) Prototype Center for Plasma Edge Simulation (CPES) is to deliver such a code integration framework by (1) building new kinetic codes XGC0 and XGC1, which can simulate the edge pedestal buildup;(2) using and improving the existing MHD codes ELITE, M3D-OMP, M3D-MPP and NIMROD, for study of large-scale edge instabilities called Edge Localized Modes (ELMs); and (3) integrating the codes into a framework using cutting-edge computer science technology. Collaborative effort among physics, computer science, and applied mathematics within CPES has created the first working version of the End-to-end Framework for\u00a0\u2026", "num_citations": "24\n", "authors": ["200"]}
{"title": "Modeling cloud failure data: a case study of the virtual computing lab\n", "abstract": " Virtual Computing Lab is a higher education cloud computing environment that on demand, allocates a chosen software stack on the required hardware and gives access to the customers, in this case NCSU students, faculty and staff. VCL has been in operation since 2004. An important component of the quality of the services provided by a cloud is the reliability and availability. For example, typical availability of the system exceeds 0.999, and reservation reliability is in the 0.99 range. VCL provides comprehensive information (provenance, logs, etc.) about its execution, its resources, and its performance. We mined the VCL log files to find out more about its reliability and availability, and the character of its faults and failures. This paper presents some of these results.", "num_citations": "19\n", "authors": ["200"]}
{"title": "Effects of personality traits on pull request acceptance\n", "abstract": " In this paper, we examine the influence of personality traits of developers on the pull request evaluation process in GitHub. We first replicate Tsay et al.'s work that examined the influence of social factors (e.g., \u2018social distance\u2019) and technical factors (e.g., test file inclusion) for evaluating contributions, and then extend it with personality-based factors. In particular, we extract the Big Five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) of developers from their online digital footprints, such as pull request comments. We analyze the personality traits of 16,935 active developers from 1,860 projects and compare their relative importance to other non-personality factors from past research, in the pull request evaluation process. We find that pull requests from authors (requesters) who are more open and conscientious, but less extroverted, have a higher chance of approval\u00a0\u2026", "num_citations": "18\n", "authors": ["200"]}
{"title": "A Large-Scale Study on the Usage of Testing Patterns That Address Maintainability Attributes: Patterns for Ease of Modification, Diagnoses, and Comprehension\n", "abstract": " Test case maintainability is an important concern, especially in open source and distributed development environments where projects typically have high contributor turn-over with varying backgrounds and experience, and where code ownership changes often. Similar to design patterns, patterns for unit testing promote maintainability quality attributes such as ease of diagnoses, modifiability, and comprehension. In this paper, we report the results of a large-scale study on the usage of four xUnit testing patterns which can be used to satisfy these maintainability attributes. This is a first-of-its-kind study which developed automated techniques to investigate these issues across 82,447 open source projects, and the findings provide more insight into testing practices in open source projects. Our results indicate that only 17% of projects had test cases, and from the 251 testing frameworks we studied, 93 of them were\u00a0\u2026", "num_citations": "16\n", "authors": ["200"]}
{"title": "Analysis of execution log files\n", "abstract": " Log analysis can be used to find problems, define operational profiles, and even pro-actively prevent issues. The goal of my dissertation research is to investigate log management and analysis techniques suited for very large and very complex logs, such as those we might expect in a computational cloud system.", "num_citations": "16\n", "authors": ["200"]}
{"title": "Characterizing and predicting blocking bugs in open source projects\n", "abstract": " Software engineering researchers have studied specific types of issues such reopened bugs, performance bugs, dormant bugs, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems. In this paper, we study blocking bugs in eight open source projects and propose a model to predict them early on. We extract 14 different factors (from the bug repositories) that are made available within 24 hours after the initial submission of the bug reports. Then, we build decision trees to predict whether a bug will be a blocking bugs or not. Our results show that our prediction models achieve F-measures of 21%\u201354%, which is a two-fold improvement over the baseline predictors. We also analyze the fixes of these blocking bugs to\u00a0\u2026", "num_citations": "12\n", "authors": ["200"]}
{"title": "A model for sharing of confidential provenance information in a query based system\n", "abstract": " Workflow management systems are increasingly being used to automate scientific discovery. Provenance meta-data is collected about scientific workflows, processes, simulations and data to add value. There is a variety of workflow management tools that cater to this. The provenance information may have as much value as the raw data. Typically, sensitive information produced by a computational processes or experiments is well guarded. However, this may not necessarily be true when it comes to provenance information. The issue is how to share confidential provenance information. We present a model for sharing provenance information when the confidentiality level is decided by the user dynamically. The key feature of this model is the Query Sharing concept. We illustrate the model for workflows implemented using provenance enabled Kepler system.", "num_citations": "11\n", "authors": ["200"]}
{"title": "Provenance in Kepler-based Scientific Workflow Systems\n", "abstract": " Provenance in Kepler-based Scientific Workflow Systems Page 1 Provenance in Kepler-based Scientific Workflow Systems Ilkay Altintas4, George Chin5, Daniel Crawl4, Terence Critchlow5, David Koop2, Jeff Ligon1, Bertram Ludaescher3, Pierre Mouallem1, Meiyappan Nagappan1, Norbert Podhorszki3, Claudio Silva2, Mladen Vouk1 Introduction System Architecture Definition of Terms Architecture \u2022Scientific workflow management systems are used to automate the data management and analysis tasks of scientific discovery. \u2022Increasing complexity of such workflows, and sometimes legal reasons, is fueling a demand for more run-time and historical information about the workflow processes, outputs, environments, etc. \u2022Properly constructed run-time and provenance information collection framework can help manage, integrate and display the needed information. \u2022In this poster we present the current provenance by \u2019. \u2026", "num_citations": "11\n", "authors": ["200"]}
{"title": "Dynamic Task Scheduling Using Parallel Genetic Algorithms For Heterogeneous Distributed Computing.\n", "abstract": " A parallel genetic algorithm has been developed to dynamically schedule heterogeneous tasks to heterogeneous processors in a distributed environment. The scheduling problem is known to be NP complete. Genetic algorithms, a meta-heuristic search technique, have been used successfully in this field. The proposed algorithm uses multiple processors with centralized control for scheduling. Tasks are taken as batches and are scheduled to minimize the execution time and balance the loads of the processors. According to our experimental results, the proposed parallel genetic algorithm (PPGA) considerably decreases the scheduling time without adversely affecting the maxspan of the resulting schedules.", "num_citations": "11\n", "authors": ["200"]}
{"title": "Managing and Monitoring Scientific Workflows through Dashboards\n", "abstract": " This work sponsored in part by the DOE SciDAC grant DE-FC02-01ER25484, the IBM Corp. Shared University Research Program, and NC State DURP funds\u2022 This dashboard subsystem is part of the general environment that supports Kepler. It is LAMP-based and its principal components are, in addition to computational resources (supercomputers), the Kepler orchestration engine, an authentication and authorization system, provenance database with APIs that allow standardized input and output and the dashboard..", "num_citations": "10\n", "authors": ["200"]}
{"title": "Roles and impacts of hands-on software architects in five industrial case studies\n", "abstract": " Whether software architects should also code is an enduring question. In order to satisfy performance, security, reliability and other quality concerns, architects need to compare and carefully choose a combination of architectural patterns, styles or tactics. Then later in the development cycle, these architectural choices must be implemented completely and correctly so there will not be any drift from envisioned design. In this paper, we use data analytics-based techniques to study five large-scale software systems, examining the impact and the role of software architects who write code on software quality. Our quantitative study is augmented with a follow up interview of architects. This paper provides empirical evidence for supporting the pragmatic opinions that architects should write code. Our analysis shows that implementing architectural tactics is more complex than delivering functionality, tactics are more error\u00a0\u2026", "num_citations": "6\n", "authors": ["200"]}
{"title": "Evaluating state-of-the-art free and open source static analysis tools against buffer errors in android apps\n", "abstract": " Modern mobile apps incorporate rich and complex features, opening the doors for different security concerns. Android is the dominant platform in mobile app markets, and enhancing its apps security is a considerable area of research. Android malware (introduced intentionally by developers) has been well studied and many tools are available to detect them. However, little attention has been directed to address vulnerabilities caused unintentionally by developers in Android apps. Static analysis has been one way to detect such vulnerabilities in traditional desktop and server side desktop. Therefore, our research aims at assessing static analysis tools that could be used by Android developers. Our preliminary analysis revealed that Buffer Errors are the most frequent type of vulnerabilities that threaten Android apps. Also, we found that Buffer Errors in Android apps have the highest risk on Android that affects data\u00a0\u2026", "num_citations": "6\n", "authors": ["200"]}
{"title": "Digital narratives of place: Learning about neighborhood sense of place and travel through online responses\n", "abstract": " As the market penetration of mobile information and communication technologies continues to grow, visitor feedback, such as online reviews of locations or sites visited, will continue to grow in parallel at finer temporal and geographic scales. This growth in data opens the opportunity for travel demand analysts to assess location attractiveness on the basis of online reviews and subsequently inform destination choice models. In geography and urban planning, the construct of sense of place (SOP) has emerged as an indicator for visitor association or connection with a place or site. An opportunity exists for examining SOP through the lens of text mining (i.e., extracting information from online text reviews and forming digital narratives of place). Several websites devoted to sharing feedback on experiences and overall perceptions exist, including Yelp and TripAdvisor. With text-mining methods, previously unidentified\u00a0\u2026", "num_citations": "6\n", "authors": ["200"]}
{"title": "Creating operational profiles of software systems by transforming their log files to directed cyclic graphs\n", "abstract": " Most log files are of one format-a flat file with the events of execution recorded one after the other. Each line in the file contains at least a timestamp, a combination of one or more event identifiers, and the actual log message with information of which event was executed and what the values for the dynamic parameters of that event are. Since log files have this trace information, we can use it for many purposes, such as operational profiling and anomalous execution path detection. However the current flat file format of a log file is very unintuitive to detect the existence of a repeating pattern. In this paper we propose a transformation of the current serial order format of a log file to a directed cyclic graph (such as a non-finite state machine) format and how the operational profile of a system can be built from this representation of the log file. We built a tool (in C++), that transforms a log file with a set of log events in a serial\u00a0\u2026", "num_citations": "6\n", "authors": ["200"]}
{"title": "Studying the impact of evolution in r libraries on software engineering research\n", "abstract": " Empirical software engineering has become an integral and important part of software engineering research in both academia and industry. Every year several new theories are empirically validated by mining and analyzing historical data from open source and closed source projects. Researchers rely on statistical libraries in tools like R, Weka, SAS, SPPS, and Matlab for their analysis. However, these libraries like any software library undergo periodic maintenance. Such maintenance can be to improve performance, but can also be to alter the core algorithms behind the library. If indeed the core algorithms are changed, then the empirical results that have been compiled with the previous versions may not be current anymore. However, this problem exists only if (a) statistical libraries are constantly edited and (b) the results they produce are difference from one version to another. Hence in this paper, we first explore\u00a0\u2026", "num_citations": "4\n", "authors": ["200"]}
{"title": "Supervised sentiment classification with cnns for diverse se datasets\n", "abstract": " Sentiment analysis, a popular technique for opinion mining, has been used by the software engineering research community for tasks such as assessing app reviews, developer emotions in issue trackers and developer opinions on APIs. Past research indicates that state-of-the-art sentiment analysis techniques have poor performance on SE data. This is because sentiment analysis tools are often designed to work on non-technical documents such as movie reviews. In this study, we attempt to solve the issues with existing sentiment analysis techniques for SE texts by proposing a hierarchical model based on convolutional neural networks (CNN) and long short-term memory (LSTM) trained on top of pre-trained word vectors. We assessed our model's performance and reliability by comparing it with a number of frequently used sentiment analysis tools on five gold standard datasets. Our results show that our model pushes the state of the art further on all datasets in terms of accuracy. We also show that it is possible to get better accuracy after labelling a small sample of the dataset and re-training our model rather than using an unsupervised classifier.", "num_citations": "3\n", "authors": ["200"]}
{"title": "Big (ger) data in software engineering\n", "abstract": " \"Big Data\" analytics has become the next hot topic for most companies - from financial institutions to technology companies to service providers. Likewise in software engineering, data collected about the development of software, the operation of the software in the field, and the users feedback on software have been used before. However, collecting and analyzing this information across hundreds of thousands or millions of software projects gives us the unique ability to reason about the ecosystem at large, and software in general. At no time in history has there been easier access to extremely powerful computational resources as it is today, thanks to the advances in cloud computing, both from the technology and business perspectives. In this technical briefing, we will present the state-of-the-art with respect to the research carried out in the area of big data analytics in software engineering research.", "num_citations": "3\n", "authors": ["200"]}
{"title": "A framework for analyzing software system log files\n", "abstract": " It is the premise of this work that current log analysis methods are too ad hoc and do not scale well enough to be effective in the domain of large logs (such as those we might expect in a computational cloud system). The more complex the system, the more complex and voluminous its logs are. In this dissertation we investigate, identify and develop components needed for an adaptable end-to-end framework for the analysis of logs. The framework needs to take into consideration that different users look for different kinds of information in the same log files. Required are adaptable techniques and algorithms for efficient and accurate log data collection, log abstraction and log transformations. The techniques or algorithms that are used in each component of the framework will vary according to the application, its logging mechanisms and the information that the stake holder needs to make decisions.", "num_citations": "3\n", "authors": ["200"]}
{"title": "Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits\n", "abstract": " Public vulnerability databases such as CVE and NVD account for only 60% of security vulnerabilities present in open-source projects, and are known to suffer from inconsistent quality. Over the last two years, there has been considerable growth in the number of known vulnerabilities across projects available in various repositories such as NPM and Maven Central. Such an increasing risk calls for a mechanism to infer the presence of security threats in a timely manner. We propose novel hierarchical deep learning models for the identification of security-relevant commits from either the commit diff or the source code for the Java classes. By comparing the performance of our model against code2vec, a state-of-the-art model that learns from path-based representations of code, and a logistic regression baseline, we show that deep learning models show promising results in identifying security-related commits. We also conduct a comparative analysis of how various deep learning models learn across different input representations and the effect of regularization on the generalization of our models.", "num_citations": "2\n", "authors": ["200"]}
{"title": "Affective Dynamics and Control in Group Processes\n", "abstract": " The computational modeling of groups requires models that connect micro-level with macro-level processes and outcomes. Recent research in computational social science has started from simple models of human behaviour, and attempted to link to social structures. However, these models make simplifying assumptions about human understanding of culture that are of ten not realistic and may be limiting in their generality. In this paper, we present work on Bayesian affect control theory as a more comprehensive, yet highly parsimonious model that integrates artificial intelligence, social psychology, and emotions into a single predictive model of human activities in groups. We illustrate these developments with examples from an ongoing research project aimed at computational analysis of virtual software development teams.", "num_citations": "2\n", "authors": ["200"]}
{"title": "Working with workflows: Highlights from 5 years building scientific workflows\n", "abstract": " In 2006, the SciDAC Scientific Data Management (SDM) Center proposed to continue its work deploying leading-edge data management and analysis capabilities to scientific applications. One of three thrust areas within the proposed center was focused on Scientific Process Automation using workflow technology. As a founding member of the Kepler consortium [LAB+ 09], the SDM Center team was well positioned to begin deploying workflows immediately. We were also keenly aware of some of the deficiencies in Kepler when applied to high performance computing workflows, which allowed us to focus our research and development efforts on critical new capabilities that were ultimately integrated into the Kepler open source distribution, benefiting the entire community.Significant work was required to ensure that Kepler was capable of supporting large-scale production runs for SciDAC applications. Our work on generic actors and templates have improved the portability of workflows across machines and provided a higher level of abstraction for workflow developers. Fault tolerance and provenance tracking were obvious areas for improvement within Kepler, given the longevity and complexity of our target workflows. To monitor workflow execution, we developed and deployed a web-based dashboard, initially targeted to a few specific applications. We then generalized this interface and released it so it could be deployed at other locations. Outreach has always been a primary focus of our work, and we had many successful deployments across a number of scientific domains while continually publishing and presenting our work. This short paper\u00a0\u2026", "num_citations": "2\n", "authors": ["200"]}
{"title": "Sustaining a Healthy Ecosystem: Participation, Discussion, and Interaction in Eclipse Forums\n", "abstract": " Although many software development projects have moved their developer discussion forums to Stack Overflow, Eclipse has been steadfast in hosting their self-supported community forums. However, recent studies show that having a forum seem to share similarities to other communication channels. In this paper, we would like to investigate how the Eclipse project has successfully maintained its forum. Using a mixed-methods approach, we investigate the participation, discussions, and interactions between members of the Eclipse project by empirically analyzing over 1 million forum threads and their linkages to four systems with 2,170 connected contributors within the Eclipse ecosystem. Our results show that forum members actively participate in posting and responding to the threads equally. The forums are dominated by question and answer threads, with the status of user is likely relate to topics discussed\u00a0\u2026", "num_citations": "1\n", "authors": ["200"]}
{"title": "Understanding the role of reporting in work item tracking systems for software development: an industrial case study\n", "abstract": " Work item tracking systems such as Visual Studio Team Services, JIRA, BugZilla and GitHub issue tracker are widely used by software engineers. These systems are used to track work items such as features, user stories, bugs, plan sprints, distribute tasks across the team and prioritize the team's work. Such systems can help teams track the progress and manage the shipping of software. While these tracking systems give data about different work items in tabular format, using a reporting tool on top of them can help teams visualize the data related to their projects such as how many bugs are open and closed and which work items are assigned to a team member. While tools like Visual Studio and JIRA provide reporting services, it is important to understand how users leverage them in their projects to help improve the reporting services. In this study, we conduct an empirical investigation on the usage of Analytics\u00a0\u2026", "num_citations": "1\n", "authors": ["200"]}
{"title": "Reconsidering whether goto is harmful\n", "abstract": " Is it always bad to use GOTO statements? An empirical analysis of open source C projects on GitHub suggests otherwise.", "num_citations": "1\n", "authors": ["200"]}
{"title": "Which code construct metrics are symptoms of post release failures?\n", "abstract": " Software metrics, such as code complexity metrics and code churn metrics, are used to predict failures. In this paper we study a specific set of metrics called code construct metrics and relate them to post release failures. We use the values of the code construct metrics for each file to characterize that file. We analyze the code construct metrics along with the post release failure data on the files (that splits the files into two classes: files with post release failures and files without post release failures). In our analysis we compare a file with post release failure to a set of files without post release failures, that have similar characteristics. In our comparison we identify which code construct metric, more often than the others, differs the most between these two classes of files. The goal of our research is to find out which code construct metrics can perhaps be used as symptoms of post release failures. In this paper we analyzed\u00a0\u2026", "num_citations": "1\n", "authors": ["200"]}
{"title": "Efficient operational profiling of systems using suffix arrays on execution logs\n", "abstract": " Operational profiles are an essential part of software reliability engineering. Typically they are created from the software requirements, and through customer reviews. Creation of operational profiles often is laborious and requires human intervention. Our approach builds an operational profile based on the actual usage from execution logs. The difficulty in using execution logs is that the amount of data to be analyzed is extremely large (more than a million records per day in many applications). Our solution constructs operational profiles by identifying all the possible clustered sequences of events (patterns) that exist in the logs. This is done very efficiently using suffix arrays data structure.", "num_citations": "1\n", "authors": ["200"]}