{"title": "Trojaning attack on neural networks\n", "abstract": " With the fast spread of machine learning techniques, sharing and adopting public machine learning models become very popular. This gives attackers many new opportunities. In this paper, we propose a trojaning attack on neuron networks. As the models are not intuitive for human to understand, the attack features stealthiness. Deploying trojaned models can cause various severe consequences including endangering human lives (in applications like auto driving). We first inverse the neuron network to generate a general trojan trigger, and then retrain the model with external datasets to inject malicious behaviors to the model. The malicious behaviors are only activated by inputs stamped with the trojan trigger. In our attack, we do not need to tamper with the original training process, which usually takes weeks to months. Instead, it takes minutes to hours to apply our attack. Also, we do not require the datasets that\u00a0\u2026", "num_citations": "446\n", "authors": ["267"]}
{"title": "Locating faults through automated predicate switching\n", "abstract": " Typically debugging begins when during a program execution a point is reached at which an obviously incorrect value is observed. A general and powerful approach to automated debugging can be based upon identifying modifications to the program state that will bring the execution to a successful conclusion. However, searching for arbitrary changes to the program state is difficult due to the extremely large search space. In this paper we demonstrate that by forcibly switching a predicate's outcome at runtime and altering the control flow, the program state can not only be inexpensively modified, but in addition it is often possible to bring the program execution to a successful completion (ie, program produces the desired output). By examining the switched predicate, also called the critical predicate, the cause of the bug can then be identified. Since the outcome of a branch can only be either true or false, the\u00a0\u2026", "num_citations": "346\n", "authors": ["267"]}
{"title": "Automatic Protocol Format Reverse Engineering through Context-Aware Monitored Execution.\n", "abstract": " Protocol reverse engineering has often been a manual process that is considered time-consuming, tedious and error-prone. To address this limitation, a number of solutions have recently been proposed to allow for automatic protocol reverse engineering. Unfortunately, they are either limited in extracting protocol fields due to lack of program semantics in network traces or primitive in only revealing the flat structure of protocol format. In this paper, we present a system called AutoFormat that aims at not only extracting protocol fields with high accuracy, but also revealing the inherently \u201cnon-flat\u201d, hierarchical structures of protocol messages. AutoFormat is based on the key insight that different protocol fields in the same message are typically handled in different execution contexts (eg, the runtime call stack). As such, by monitoring the program execution, we can collect the execution context information for every message byte (annotated with its offset in the entire message) and cluster them to derive the protocol format. We have evaluated our system with more than 30 protocol messages from seven protocols, including two text-based protocols (HTTP and SIP), three binary-based protocols (DHCP, RIP, and OSPF), one hybrid protocol (CIFS/SMB), as well as one unknown protocol used by a real-world malware. Our results show that AutoFormat can not only identify individual message fields automatically and with high accuracy (an average 93.4% match ratio compared with Wireshark), but also unveil the structure of the protocol format by revealing possible relations (eg, sequential, parallel, and hierarchical) among the message fields.", "num_citations": "299\n", "authors": ["267"]}
{"title": "Precise dynamic slicing algorithms\n", "abstract": " Dynamic slicing algorithms can greatly reduce the debugging effort by focusing the attention of the user on a relevant subset of program statements. In this paper we present the design and evaluation of three precise dynamic slicing algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic slices. Our experiments show that the LP algorithm is a fast and practical precise slicing algorithm. In fact we show that while precise slices can be orders of magnitude smaller than imprecise dynamic slices, for small number of slicing requests, the LP algorithm is faster than an imprecise dynamic slicing algorithm proposed by Agrawal and Horgan.", "num_citations": "279\n", "authors": ["267"]}
{"title": "Automatic reverse engineering of data structures from binary execution\n", "abstract": " With only the binary executable of a program, it is useful to discover the program's data structures and infer their syntactic and semantic definitions. Such knowledge is highly valuable in a variety of security and forensic applications. Although there exist efforts in program data structure inference, the existing solutions are not suitable for our targeted application scenarios. In this paper, we propose a reverse engineering technique to automatically reveal program data structures from binaries. Our technique, called REWARDS, is based on dynamic analysis. More specifically, each memory location accessed by the program is tagged with a timestamped type attribute. Following the program's runtime data flow, this attribute is propagated to other memory locations and registers that share the same type. During the propagation, a variable's type gets resolved if it is involved in a type-revealing execution point or type sink\u00a0\u2026", "num_citations": "259\n", "authors": ["267"]}
{"title": "Locating faulty code using failure-inducing chops\n", "abstract": " Software debugging is the process of locating and correcting faulty code. Prior techniques to locate faulty code either use program analysis techniques such as backward dynamic program slicing or exclusively use delta debugging to analyze the state changes during program execution. In this paper, we present a new approach that integrates the potential of delta debugging algorithm with the benefit of forward and backward dynamic program slicing to narrow down the search for faulty code. Our approach is to use delta debugging algorithm to identify a minimal failure-inducing input, use this input to compute a forward dynamic slice and then intersect the statements in this forward dynamic slice with the statements in the backward dynamic slice of the erroneous output to compute a failure-inducing chop. We implemented our technique and conducted experiments with faulty versions of several programs from the\u00a0\u2026", "num_citations": "240\n", "authors": ["267"]}
{"title": "Z3-str: A z3-based string solver for web application analysis\n", "abstract": " Analyzing web applications requires reasoning about strings and non-strings cohesively. Existing string solvers either ignore non-string program behavior or support limited set of string operations. In this paper, we develop a general purpose string solver, called Z3-str, as an extension of the Z3 SMT solver through its plug-in interface. Z3-str treats strings as a primitive type, thus avoiding the inherent limitations observed in many existing solvers that encode strings in terms of other primitives. The logic of the plug-in has three sorts, namely, bool, int and string. The string-sorted terms include string constants and variables of arbitrary length, with functions such as concatenation, sub-string, and replace. The int-sorted terms are standard, with the exception of the length function over string terms. The atomic formulas are equations over string terms, and (in)-equalities over integer terms. Not only does our solver have\u00a0\u2026", "num_citations": "231\n", "authors": ["267"]}
{"title": "Pruning dynamic slices with confidence\n", "abstract": " Given an incorrect value produced during a failed program run (e.g., a wrong output value or a value that causes the program to crash), the backward dynamic slice of the value very frequently captures the faulty code responsible for producing the incorrect value. Although the dynamic slice often contains only a small percentage of the statements executed during the failed program run, the dynamic slice can still be large and thus considerable effort may be required by the programmer to locate the faulty code.In this paper we develop a strategy for pruning the dynamic slice to identify a subset of statements in the dynamic slice that are likely responsible for producing the incorrect value. We observe that some of the statements used in computing the incorrect value may also have been involved in computing correct values (e.g., a value produced by a statement in the dynamic slice of the incorrect value may also have\u00a0\u2026", "num_citations": "197\n", "authors": ["267"]}
{"title": "Experimental evaluation of using dynamic slices for fault location\n", "abstract": " Dynamic slicing algorithms have been considered to aid in debugging for many years. However, as far as we know, no detailed studies on evaluating the benefits of using dynamic slicing for detecting faulty statements in programs have been carried out. We have developed a dynamic slicing framework that uses dynamic instrumentation to efficiently collect dynamic slices and reduced ordered Binary Decision Diagrams (roBDDs) to compactly store them. We have used the above framework to implement three variants of dynamic slicing algorithms including: data slicing, full slicing, and relevant slicing algorithms. We have carried out detailed experiments to evaluate these algorithms. Our results show that full slices and relevant slices can considerably reduce the subset of program statements that need to be examined to locate faulty statements. We expect that the observations presented here will enable\u00a0\u2026", "num_citations": "180\n", "authors": ["267"]}
{"title": "Cost effective dynamic program slicing\n", "abstract": " Although dynamic program slicing was first introduced to aid in user level debugging, applications aimed at improving software quality, reliability, security, and performance have since been identified as candidates for using dynamic slicing. However, the dynamic dependence graph constructed to compute dynamic slices can easily cause slicing algorithms to run out of memory for realistic program runs. In this paper we present the design and evaluation of a cost effective dynamic program slicing algorithm. This algorithm is based upon a dynamic dependence graph representation that is highly compact and rapidly traversable. Thus, the graph can be held in memory and dynamic slices can be quickly computed. A compact representation is derived by recognizing that all dynamic dependences (data and control) need not be individually represented. We identify sets of dynamic dependence edges between a pair of\u00a0\u2026", "num_citations": "158\n", "authors": ["267"]}
{"title": "ProTracer: Towards Practical Provenance Tracing by Alternating Between Logging and Tainting\n", "abstract": " Provenance tracing is a very important approach to Advanced Persistent Threat (APT) attack detection and investigation. Existing techniques either suffer from the dependence explosion problem or have non-trivial space and runtime overhead, which hinder their application in practice. We propose ProTracer, a lightweight provenance tracing system that alternates between system event logging and unit level taint propagation. The technique is built on an on-the-fly system event processing infrastructure that features a very lightweight kernel module and a sophisticated user space daemon that performs concurrent and out-of-order event processing. The evaluation with different realistic system workloads and a number of attack cases show that ProTracer only produces 13MB log data per day, and 0.84 GB (Server)/2.32 GB (Client) in 3 months without losing any important information. The space consumption is only< 1.28% of the state-of-the-art, 7 times smaller than an off-line garbage collection technique. The run-time overhead averages< 7% for servers and< 5% for regular applications. The generated attack causal graphs are a few times smaller than those by existing techniques while they are equally informative.", "num_citations": "136\n", "authors": ["267"]}
{"title": "SigGraph: Brute Force Scanning of Kernel Data Structure Instances Using Graph-based Signatures.\n", "abstract": " Brute force scanning of kernel memory images for finding kernel data structure instances is an important function in many computer security and forensics applications. Brute force scanning requires effective, robust signatures of kernel data structures. Existing approaches often use the value invariants of certain fields as data structure signatures. However, they do not fully exploit the rich pointsto relations between kernel data structures. In this paper, we show that such points-to relations can be leveraged to generate graph-based structural invariant signatures. More specifically, we develop SigGraph, a framework that systematically generates non-isomorphic signatures for data structures in an OS kernel. Each signature is a graph rooted at a subject data structure with its edges reflecting the points-to relations with other data structures. Our experiments with a range of Linux kernels show that SigGraph-based signatures achieve high accuracy in recognizing kernel data structure instances via brute force scanning. We further show that SigGraph achieves better robustness against pointer value anomalies and corruptions, without requiring global memory mapping and object reachability. We demonstrate that SigGraph can be applied to kernel memory forensics, kernel rootkit detection, and kernel version inference.", "num_citations": "131\n", "authors": ["267"]}
{"title": "NIC: Detecting Adversarial Samples with Neural Network Invariant Checking\n", "abstract": " Deep Neural Networks (DNN) are vulnerable to adversarial samples that are generated by perturbing correctly classified inputs to cause DNN models to misbehave (eg, misclassification). This can potentially lead to disastrous consequences especially in security-sensitive applications. Existing defense and detection techniques work well for specific attacks under various assumptions (eg, the set of possible attacks are known beforehand). However, they are not sufficiently general to protect against a broader range of attacks. In this paper, we analyze the internals of DNN models under various attacks and identify two common exploitation channels: the provenance channel and the activation value distribution channel. We then propose a novel technique to extract DNN invariants and use them to perform runtime adversarial sample detection. Our experimental results of 11 different kinds of attacks on popular datasets including ImageNet and 13 models show that our technique can effectively detect all these attacks (over 90% accuracy) with limited false positives. We also compare it with three state-of-theart techniques including the Local Intrinsic Dimensionality (LID) based method, denoiser based methods (ie, MagNet and HGD), and the prediction inconsistency based approach (ie, feature squeezing). Our experiments show promising results.", "num_citations": "127\n", "authors": ["267"]}
{"title": "Plagiarizing smartphone applications: attack strategies and defense techniques\n", "abstract": " In this paper, we show how an attacker can launch malware onto a large number of smartphone users by plagiarizing Android applications and by using elements of social engineering to increase infection rate. Our analysis of a dataset of 158,000 smartphone applications meta-information indicates that 29.4% of the applications are more likely to be plagiarized. We propose three detection schemes that rely on syntactic fingerprinting to detect plagiarized applications under different levels of obfuscation used by the attacker. Our analysis of 7,600 smartphone application binaries shows that our schemes detect all instances of plagiarism from a set of real-world malware incidents with 0.5% false positives and scale to millions of applications using only commodity servers.", "num_citations": "119\n", "authors": ["267"]}
{"title": "SUPOR: precise and scalable sensitive user input detection for android apps\n", "abstract": " While smartphones and mobile apps have been an essential part of our lives, privacy is a serious concern. Previous mobile privacy related research efforts have largely focused on predefined known sources managed by smartphones. Sensitive user inputs through UI (User Interface), another information source that may contain a lot of sensitive information, have been mostly neglected.", "num_citations": "116\n", "authors": ["267"]}
{"title": "Spider: Stealthy binary program instrumentation and debugging via hardware virtualization\n", "abstract": " The ability to trap the execution of a binary program at desired instructions is essential in many security scenarios such as malware analysis and attack provenance. However, an increasing percent of both malicious and legitimate programs are equipped with anti-debugging and anti-instrumentation techniques, which render existing debuggers and instrumentation tools inadequate. In this paper, we present Spider, a stealthy program instrumentation framework which enables transparent, efficient and flexible instruction-level trapping based on hardware virtualization. Spider uses invisible breakpoint, a novel primitive we develop that inherits the efficiency and flexibility of software breakpoint, and utilizes hardware virtualization to hide its side-effects from the guest. We have implemented a prototype of Spider on KVM. Our evaluation shows that Spider succeeds in remaining transparent against state-of-the-art anti\u00a0\u2026", "num_citations": "116\n", "authors": ["267"]}
{"title": "Towards locating execution omission errors\n", "abstract": " Execution omission errors are known to be difficult to locate using dynamic analysis. These errors lead to a failure at runtime because of the omission of execution of some statements that would have been executed if the program had no errors. Since dynamic analysis is typically designed to focus on dynamic information arising from executed statements, and statements whose execution is omitted do not produce dynamic information, detection of execution omission errors becomes a challenging task. For example, while dynamic slices are very effective in capturing faulty code for other types of errors, they fail to capture faulty code in presence of execution omission errors. To address this issue relevant slices have been defined to consider certain static dependences (called potential dependences) in addition to dynamic dependences. However, due to the conservative nature of static analysis, overly large slices are\u00a0\u2026", "num_citations": "106\n", "authors": ["267"]}
{"title": "ABS: Scanning neural networks for back-doors by artificial brain stimulation\n", "abstract": " This paper presents a technique to scan neural network based AI models to determine if they are trojaned. Pre-trained AI models may contain back-doors that are injected through training or by transforming inner neuron weights. These trojaned models operate normally when regular inputs are provided, and mis-classify to a specific output label when the input is stamped with some special pattern called trojan trigger. We develop a novel technique that analyzes inner neuron behaviors by determining how output activations change when we introduce different levels of stimulation to a neuron. The neurons that substantially elevate the activation of a particular output label regardless of the provided input is considered potentially compromised. Trojan trigger is then reverse-engineered through an optimization procedure using the stimulation analysis results, to confirm that a neuron is truly compromised. We evaluate\u00a0\u2026", "num_citations": "105\n", "authors": ["267"]}
{"title": "Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples\n", "abstract": " Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible features/attributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94% detection accuracy for 7 different kinds of attacks with 9.91% false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55% accuracy with 23.3% false positives.", "num_citations": "105\n", "authors": ["267"]}
{"title": "Efficient forward computation of dynamic slices using reduced ordered binary decision diagrams\n", "abstract": " Dynamic slicing algorithms can greatly reduce the debugging effort by focusing the attention of the user on a relevant subset of program statements. Recently, algorithms for forward computation of dynamic slices have been proposed which maintain dynamic slices of all variables as the program executes. An advantage of this approach is that when a request for a slice is made, it is already available. The main disadvantage of using such an algorithm for slicing realistic programs is that the space and time required to maintain a large set of dynamic slices corresponding to all program variables can be very high. In this paper, we analyze the characteristics of dynamic slices and identify properties that enable space efficient representation of a set of dynamic slices. We show that by using reduced ordered binary decision diagrams (roBDDs) to represent a set of dynamic slices, the space and time requirements of\u00a0\u2026", "num_citations": "97\n", "authors": ["267"]}
{"title": "Hercule: Attack story reconstruction via community discovery on correlated log graph\n", "abstract": " Advanced cyber attacks consist of multiple stages aimed at being stealthy and elusive. Such attack patterns leave their footprints spatio-temporally dispersed across many different logs in victim machines. However, existing log-mining intrusion analysis systems typically target only a single type of log to discover evidence of an attack and therefore fail to exploit fundamental inter-log connections. The output of such single-log analysis can hardly reveal the complete attack story for complex, multi-stage attacks. Additionally, some existing approaches require heavyweight system instrumentation, which makes them impractical to deploy in real production environments. To address these problems, we present HERCULE, an automated multi-stage log-based intrusion analysis system. Inspired by graph analytics research in social network analysis, we model multi-stage intrusion analysis as a community discovery problem\u00a0\u2026", "num_citations": "96\n", "authors": ["267"]}
{"title": "X-force: Force-executing binary programs for security applications\n", "abstract": " This paper introduces X-Force, a novel binary analysis engine. Given a potentially malicious binary executable, X-Force can force the binary to execute requiring no inputs or proper environment. It also explores different execution paths inside the binary by systematically forcing the branch outcomes of a very small set of conditional control transfer instructions. X-Force features a crash-free execution model that can detect and recover from exceptions. In particular, it can fix invalid memory accesses by allocating memory on-demand and setting the offending pointers to the allocated memory. We have applied X-Force to three security applications. The first is to construct control flow graphs and call graphs for stripped binaries. The second is to expose hidden behaviors of malware, including packed and obfuscated APT malware. X-Force is able to reveal hidden malicious behaviors that had been missed by manual inspection. In the third application, X-Force substantially improves analysis coverage in dynamic type reconstruction for stripped binaries.", "num_citations": "91\n", "authors": ["267"]}
{"title": "Path sensitive static analysis of web applications for remote code execution vulnerability detection\n", "abstract": " Remote code execution (RCE) attacks are one of the most prominent security threats for web applications. It is a special kind of cross-site-scripting (XSS) attack that allows client inputs to be stored and executed as server side scripts. RCE attacks often require coordination of multiple requests and manipulation of string and non-string inputs from the client side to nullify the access control protocol and induce unusual execution paths on the server side. We propose a path- and context-sensitive interprocedural analysis to detect RCE vulnerabilities. The analysis features a novel way of analyzing both the string and non-string behavior of a web application in a path sensitive fashion. It thoroughly handles the practical challenges entailed by modeling RCE attacks. We develop a prototype system and evaluate it on ten real-world PHP applications. We have identified 21 true RCE vulnerabilities, with 8 unreported before.", "num_citations": "90\n", "authors": ["267"]}
{"title": "Statically locating web application bugs caused by asynchronous calls\n", "abstract": " Ajax becomes more and more important for web applications that care about client side user experience. It allows sending requests asynchronously, without blocking clients from continuing execution. Callback functions are only executed upon receiving the responses. While such mechanism makes browsing a smooth experience, it may cause severe problems in the presence of unexpected network latency, due to the non-determinism of asynchronism. In this paper, we demonstrate the possible problems caused by the asynchronism and propose a static program analysis to automatically detect such bugs in web applications. As client side Ajax code is often wrapped in server-side scripts, we also develop a technique that extracts client-side JavaScript code from server-side scripts. We evaluate our technique on a number of real-world web applications. Our results show that it can effectively identify real bugs. We\u00a0\u2026", "num_citations": "86\n", "authors": ["267"]}
{"title": "A study of effectiveness of dynamic slicing in locating real faults\n", "abstract": " Dynamic slicing algorithms have been considered to aid in debugging for many years. However, as far as we know, no detailed studies on evaluating the benefits of using dynamic slicing for locating real faults present in programs have been carried out. In this paper we study the effectiveness of fault location using dynamic slicing for a set of real bugs reported in some widely used software programs. Our results show that of the 19 faults studied, 12 faults were captured by data slices, 7 required the use of full slices, and none of them required the use of relevant slices. Moreover, it was observed that dynamic slicing considerably reduced the subset of program statements that needed to be examined to locate faulty statements. Interestingly, we observed that all of the memory bugs in the faulty versions were captured by data slices. The dynamic slices that captured faulty code included 0.45 to 63.18% of\u00a0\u2026", "num_citations": "84\n", "authors": ["267"]}
{"title": "Detecting Attacks Against Robotic Vehicles: A Control Invariant Approach\n", "abstract": " Robotic vehicles (RVs), such as drones and ground rovers, are a type of cyber-physical systems that operate in the physical world under the control of computing components in the cyber world. Despite RVs' robustness against natural disturbances, cyber or physical attacks against RVs may lead to physical malfunction and subsequently disruption or failure of the vehicles' missions. To avoid or mitigate such consequences, it is essential to develop attack detection techniques for RVs. In this paper, we present a novel attack detection framework to identify external, physical attacks against RVs on the fly by deriving and monitoring Control Invariants (CI). More specifically, we propose a method to extract such invariants by jointly modeling a vehicle's physical properties, its control algorithm and the laws of physics. These invariants are represented in a state-space form, which can then be implemented and inserted into\u00a0\u2026", "num_citations": "83\n", "authors": ["267"]}
{"title": "Dynamic slicing long running programs through execution fast forwarding\n", "abstract": " Fixing runtime bugs in long running programs using trace based analyses such as dynamic slicing was believed to be prohibitively expensive. In this paper, we present a novel execution fast forwarding technique that makes this feasible. While a naive solution is to divide the entire execution by checkpoints, and then apply dynamic slicing enabled by tracing to one checkpoint interval at a time, it is still too costly even with state-of-the-art tracing techniques. Our technique is derived from two key observations. The first one is that long running programs are usually driven by events, which has been taken advantage of by checkpointing/replaying techniques to deterministically replay an execution from the event log. The second observation is that all the events are not relevant to replaying a particular part of the execution, in which the programmer suspects an error happened. We develop a slicing-like technique that can\u00a0\u2026", "num_citations": "81\n", "authors": ["267"]}
{"title": "SENSS: Security enhancement to symmetric shared memory multiprocessors\n", "abstract": " With the increasing concern of the security on high performance multiprocessor enterprise servers, more and more effort is being invested into defending against various kinds of attacks. This paper proposes a security enhancement model called SENSS, that allows programs to run securely on a symmetric shared memory multiprocessor (SMP) environment. In SENSS, a program, including both code and data, is stored in the shared memory in encrypted form but is decrypted once it is fetched into any of the processors. In contrast to the traditional uniprocessor XOM model (Lie et al., 2000), the main challenge in developing SENSS lies in the necessity for guarding the clear text communication between processors in a multiprocessor environment. In this paper we propose an inexpensive solution that can effectively protect the shared bus communication. The proposed schemes include both encryption and\u00a0\u2026", "num_citations": "81\n", "authors": ["267"]}
{"title": "MODE: automated neural network model debugging via state differential analysis and input selection\n", "abstract": " Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our\u00a0\u2026", "num_citations": "79\n", "authors": ["267"]}
{"title": "Whole execution traces\n", "abstract": " Different types of program profiles (control flow, value, address, and dependence) have been collected and extensively studied by researchers to identify program characteristics that can then be exploited to develop more effective compilers and architectures. Due to the large amounts of profile data produced by realistic program runs, most work has focused on separately collecting and compressing different types of profiles. In this paper we present a unified representation of profiles called Whole Execution Trace (WET) which includes the complete information contained in each of the above types of traces. Thus WETs provide a basis for a next generation software tool that will enable mining of program profiles to identify program characteristics that require understanding of relationships among various types of profiles. The key features of our WET representation are: WET is constructed by labeling a static program\u00a0\u2026", "num_citations": "78\n", "authors": ["267"]}
{"title": "Matching execution histories of program versions\n", "abstract": " We develop a method for matching dynamic histories of program executions of two program versions. The matches produced can be useful in many applications including software piracy detection and several debugging scenarios. Unlike some static approaches for matching program versions, our approach does not require access to source code of the two program versions because dynamic histories can be collected by running instrumented versions of program binaries. We base our matching algorithm on comparison of rich program execution histories which include: control flow taken, values produced, addresses referenced, as well as data dependences exercised. In developing a matching algorithm we had two goals: producing an accurate match and producing it quickly. By using rich execution history, we are able to compare the program versions across many behavioral dimensions. The result is a fast and\u00a0\u2026", "num_citations": "75\n", "authors": ["267"]}
{"title": "MPI: Multiple perspective attack investigation with semantics aware execution partitioning\n", "abstract": " Traditional auditing techniques generate large and inaccurate causal graphs. To overcome such limitations, researchers proposed to leverage execution partitioning to improve analysis granularity and hence precision. However, these techniques rely on a low level programming paradigm (ie, event handling loops) to partition execution, which often results in low level graphs with a lot of redundancy. This not only leads to space inefficiency and noises in causal graphs, but also makes it difficult to understand attack provenance. Moreover, these techniques require training to detect low level memory dependencies across partitions. Achieving correctness and completeness in the training is highly challenging. In this paper, we propose a semantics aware program annotation and instrumentation technique to partition execution based on the application specific high level task structures. It avoids training, generates execution partitions with rich semantic information and provides multiple perspectives of an attack. We develop a prototype and integrate it with three different provenance systems: the Linux Audit system, ProTracer and the LPM-HiFi system. The evaluation results show that our technique generates cleaner attack graphs with rich high-level semantics and has much lower space and time overheads, when compared with the event loop based partitioning techniques BEEP and ProTracer.", "num_citations": "72\n", "authors": ["267"]}
{"title": "Obfuscation resilient binary code reuse through trace-oriented programming\n", "abstract": " With the wide existence of binary code, it is desirable to reuse it in many security applications, such as malware analysis and software patching. While prior approaches have shown that binary code can be extracted and reused, they are often based on static analysis and face challenges when coping with obfuscated binaries. This paper introduces trace-oriented programming (TOP), a general framework for generating new software from existing binary code by elevating the low-level binary code to C code with templates and inlined assembly. Different from existing work, TOP gains benefits from dynamic analysis such as resilience against obfuscation and avoidance of points-to analysis. Thus, TOP can be used for malware analysis, especially for malware function analysis and identification. We have implemented a proof-of-concept of TOP and our evaluation results with a range of benign and malicious software\u00a0\u2026", "num_citations": "71\n", "authors": ["267"]}
{"title": "Eavesdropping on Fine-Grained User Activities Within Smartphone Apps Over Encrypted Network Traffic.\n", "abstract": " Smartphone apps have changed the way we interact with online services, but highly specialized apps come at a cost to privacy. In this paper we will demonstrate that a passive eavesdropper is capable of identifying finegrained user activities within the wireless network traffic generated by apps. Despite the widespread use of fully encrypted communication, our technique, called NetScope, is based on the intuition that the highly specific implementation of each app leaves a fingerprint on its traffic behavior (eg, transfer rates, packet exchanges, and data movement). By learning the subtle traffic behavioral differences between activities (eg,\u201cbrowsing\u201d versus \u201cchatting\u201d in a dating app), NetScope is able to perform robust inference of users\u2019 activities, for both Android and iOS devices, based solely on inspecting IP headers. Our evaluation with 35 widely popular app activities (ranging from social networking and dating to personal health and presidential campaigns) shows that NetScope yields high detection accuracy (78.04% precision and 76.04% recall on average).", "num_citations": "70\n", "authors": ["267"]}
{"title": "Hiding program slices for software security\n", "abstract": " Given the high cost of producing software, development of technology for prevention of software piracy is important for the software industry. In this paper we present a novel approach for preventing the creation of unauthorized copies of software. Our approach splits software modules into open and hidden components. The open components are installed (executed) on an insecure machine while the hidden components are installed (executed) on a secure machine. We assume that while open components can be stolen, to obtain a fully functioning copy of the software, the hidden components must be recovered. We describe an algorithm that constructs hidden components by slicing the original software components. We argue that recovery of hidden components constructed through slicing, in order to obtain a fully functioning copy of the software, is a complex task. We further develop security analysis to capture\u00a0\u2026", "num_citations": "68\n", "authors": ["267"]}
{"title": "Locating faults using multiple spectra-specific models\n", "abstract": " Spectra-based fault localization (SFL) techniques have brought encouraging results and a variety of program spectra have been proposed to locate faults. Different types of abnormal behaviors may be revealed by different kinds of spectra. Compared to techniques using single spectra type, techniques combining multiple types of spectra try to leverage the strengths of the constituent types. However, in the presence of multiple kinds of spectra, how to select adequate spectra type and build appropriate models need further investigation.", "num_citations": "65\n", "authors": ["267"]}
{"title": "Securing real-time microcontroller systems through customized memory view switching\n", "abstract": " Real-time microcontrollers have been widely adopted in cyber-physical systems that require both real-time and security guarantees. Unfortunately, security is sometimes traded for real-time performance in such systems. Notably, memory isolation, which is one of the most established security features in modern computer systems, is typically not available in many real-time microcontroller systems due to its negative impacts on performance and violation of real-time constraints. As such, the memory space of these systems has created an open, monolithic attack surface that attackers can target to subvert the entire systems. In this paper, we present MINION, a security architecture that intends to virtually partition the memory space and enforce memory access control of a real-time microcontroller. MINION can automatically identify the reachable memory regions of realtime processes through off-line static analysis on the system\u2019s firmware and conduct run-time memory access control through hardware-based enforcement. Our evaluation results demonstrate that, by significantly reducing the memory space that each process can access, MINION can effectively protect a microcontroller from various attacks that were previously viable. In addition, unlike conventional memory isolation mechanisms that might incur substantial performance overhead, the lightweight design of MINION is able to maintain the real-time properties of the microcontroller.", "num_citations": "64\n", "authors": ["267"]}
{"title": "Deriving input syntactic structure from execution\n", "abstract": " Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging and network security. However, such important information is often not available (eg, most malware programs make use of secret protocols to communicate) or not directly usable by machines (eg, many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic structure\u00a0\u2026", "num_citations": "59\n", "authors": ["267"]}
{"title": "Efficient online detection of dynamic control dependence\n", "abstract": " Capturing dynamic control dependence is critical for many dynamic program analysis such as dynamic slicing, dynamic information flow, and data lineage computation. Existing algorithms are mostly a simple runtime translation of the static definition, which fails to capture certain dynamic properties by its nature, leading to inefficiency. In this paper, we propose a novel online detection technique for dynamic control dependence. The technique is based upon a new definition, which is equivalent to the existing one in the intraprocedural case but it enables an efficient detection algorithm. The new algorithm naturally and efficiently handles interprocedural dynamic control dependence even in presence of irregular control flow. Our evaluation shows that the detection algorithm slows down program execution by a factor of 2.57, which is 2.54 times faster than the existing algorithm that was used in prior work.", "num_citations": "58\n", "authors": ["267"]}
{"title": "How Do Developers Fix Cross-Project Correlated Bugs? A Case Study on the GitHub Scientific Python Ecosystem\n", "abstract": " GitHub, a popular social-software-development platform, has fostered a variety of software ecosystems where projects depend on one another and practitioners interact with each other. Projects within an ecosystem often have complex inter-dependencies that impose new challenges in bug reporting and fixing. In this paper, we conduct an empirical study on cross-project correlated bugs, i.e., causally related bugs reported to different projects, focusing on two aspects: 1) how developers track the root causes across projects, and 2) how the downstream developers coordinate to deal with upstream bugs. Through manual inspection of bug reports collected from the scientific Python ecosystem and an online survey with developers, this study reveals the common practices of developers and the various factors in fixing cross-project bugs. These findings provide implications for future software bug analysis in the scope of\u00a0\u2026", "num_citations": "57\n", "authors": ["267"]}
{"title": "Strict control dependence and its effect on dynamic information flow analyses\n", "abstract": " Program control dependence has substantial impact on applications such as dynamic information flow tracking and data lineage tracing (a technique tracking the set of inputs that affects individual outputs). Without considering control dependence, information can leak via implicit channels without being tracked; important inputs may be absent from output lineage. However, considering control dependence may lead to a large volume of false alarms in information flow tracking or undesirably large lineage sets. We identify a special type of control dependence called strict control dependence (SCD). The nature of SCDs highly resembles that of data dependences, reflecting strong correlations between statements and hence should be considered the same way as data dependences in various applications. We formally define the semantics. We also describe a cost-effective design that allows tracing only strict control\u00a0\u2026", "num_citations": "57\n", "authors": ["267"]}
{"title": "ProFuzzer: On-the-fly Input Type Probing for Better Zero-Day Vulnerability Discovery\n", "abstract": " Existing mutation based fuzzers tend to randomly mutate the input of a program without understanding its underlying syntax and semantics. In this paper, we propose a novel on-the-fly probing technique (called ProFuzzer) that automatically recovers and understands input fields of critical importance to vulnerability discovery during a fuzzing process and intelligently adapts the mutation strategy to enhance the chance of hitting zero-day targets. Since such probing is transparently piggybacked to the regular fuzzing, no prior knowledge of the input specification is needed. During fuzzing, individual bytes are first mutated and their fuzzing results are automatically analyzed to link those related together and identify the type for the field connecting them; these bytes are further mutated together following type-specific strategies, which substantially prunes the search space. We define the probe types generally across all\u00a0\u2026", "num_citations": "56\n", "authors": ["267"]}
{"title": "On-the-fly detection of instability problems in floating-point program execution\n", "abstract": " The machine representation of floating point values has limited precision such that errors may be introduced during execution. These errors may get propagated and magnified by the following operations, leading to instability problems, eg, control flow path may be undesirably altered and faulty output may be emitted. In this paper, we develop an on-the-fly efficient monitoring technique that can predict if an execution is stable. The technique does not explicitly compute errors as doing so incurs high overhead. Instead, it detects possible places where an error becomes substantially inflated regarding the corresponding value, and then tags the value with one bit to denote that it has an inflated error. It then tracks inflation bit propagation, taking care of operations that may cut off such propagation. It reports instability if any inflation bit reaches a critical execution point, such as a predicate, where the inflated error may\u00a0\u2026", "num_citations": "55\n", "authors": ["267"]}
{"title": "Whole execution traces and their applications\n", "abstract": " Different types of program profiles (control flow, value, address, and dependence) have been collected and extensively studied by researchers to identify program characteristics that can then be exploited to develop more effective compilers and architectures. Because of the large amounts of profile data produced by realistic program runs, most work has focused on separately collecting and compressing different types of profiles. In this paper, we present a unified representation of profiles called Whole Execution Trace (WET), which includes the complete information contained in each of the above types of traces. Thus, WETs provide a basis for a next-generation software tool that will enable mining of program profiles to identify program characteristics that require understanding of relationships among various types of profiles. The key features of our WET representation are: WET is constructed by labeling a static\u00a0\u2026", "num_citations": "55\n", "authors": ["267"]}
{"title": "Locating faulty code by multiple points slicing\n", "abstract": " Dynamic slicing has long been considered as a useful tool for debugging programs as it effectively identifies a reduced fault candidate set which captures the faulty code in the program. Traditionally, a  backward dynamic slice is computed starting from an incorrect value observed by the programmer during a failed program run. This incorrect value is either an incorrect output value or an incorrect address whose dereferencing causes the program to crash. Recently we proposed two additional types of dynamic slices, a forward dynamic slice  of a minimal failure inducing input difference and a  bidirectional dynamic slice  of a critical predicate. We have built a dynamic slicing tool that computes dynamic slices by instrumenting program binaries and executing them to build dynamic dependence graphs. In this paper, through experiments, we demonstrate that supporting three different types of dynamic slices has the\u00a0\u2026", "num_citations": "53\n", "authors": ["267"]}
{"title": "iRiS: Vetting Private API Abuse in iOS Applications\n", "abstract": " With the booming sale of iOS devices, the number of iOS applications has increased significantly in recent years. To protect the security of iOS users, Apple requires every iOS application to go through a vetting process called App Review to detect uses of private APIs that provide access to sensitive user information. However, recent attacks have shown the feasibility of using private APIs without being detected during App Review. To counter such attacks, we propose a new iOS application vetting system, called iRiS, in this paper. iRiS first applies fast static analysis to resolve API calls. For those that cannot be statically resolved, iRiS uses a novel iterative dynamic analysis approach, which is slower but more powerful compared to static analysis. We have ported Valgrind to iOS and implemented a prototype of iRiS on top of it. We evaluated iRiS with 2019 applications from the official App Store. From these, iRiS\u00a0\u2026", "num_citations": "52\n", "authors": ["267"]}
{"title": "GUITAR: Piecing together android app GUIs from memory images\n", "abstract": " An Android app's graphical user interface (GUI) displays rich semantic and contextual information about the smartphone's owner and app's execution. Such information provides vital clues to the investigation of crimes in both cyber and physical spaces. In real-world digital forensics however, once an electronic device becomes evidence most manual interactions with it are prohibited by criminal investigation protocols. Hence investigators must resort to\" image-and-analyze\" memory forensics (instead of browsing through the subject phone) to recover the apps' GUIs. Unfortunately, GUI reconstruction is still largely impossible with state-of-the-art memory forensics techniques, which tend to focus only on individual in-memory data structures. An Android GUI, however, displays diverse visual elements each built from numerous data structure instances. Furthermore, whenever an app is sent to the background, its GUI\u00a0\u2026", "num_citations": "50\n", "authors": ["267"]}
{"title": "Efficient diagnostic tracing for wireless sensor networks\n", "abstract": " Wireless sensor networks (WSNs) are hard to program due to unconventional programming models used to satisfy stringent resource constraints. The common event-driven concurrent programming model and lack of kernel protection in these systems introduce the possibility of several subtle faults such as race conditions. These faults are often triggered by unexpected interleavings of events in the real world, and can occur long after their causes. Reproducing a fault from the trace of the past events can play a crucial role in debugging such faults. The same tight constraints that motivate the specific programming model however make tracing challenging. This paper proposes an efficient intra-procedural and inter-procedural control-flow tracing algorithm that generates the traces of all interleaving concurrent events. Our approach enables reproducing faults at a later stage, allowing the programmer to identify them\u00a0\u2026", "num_citations": "49\n", "authors": ["267"]}
{"title": "Python probabilistic type inference with natural language support\n", "abstract": " We propose a novel type inference technique for Python programs. Type inference is difficult for Python programs due to their heavy dependence on external APIs and the dynamic language features. We observe that Python source code often contains a lot of type hints such as attribute accesses and variable names. However, such type hints are not reliable. We hence propose to use probabilistic inference to allow the beliefs of individual type hints to be propagated, aggregated, and eventually converge on probabilities of variable types. Our results show that our technique substantially outperforms a state-of-the-art Python type inference engine based on abstract interpretation.", "num_citations": "46\n", "authors": ["267"]}
{"title": "Busmonitor: A hypervisor-based solution for memory bus covert channels\n", "abstract": " Researchers continue to find side channels present in cloud infrastructure which threaten virtual machine (VM) isolation. Specifically, the memory bus on virtualized x86 systems has been targeted as one such channel. Due to its connection to multiple processors, ease of control, and importance to system stability the memory bus could be one of the most powerful cross-VM side channels present in a cloud environment. To ensure that this critical component cannot be misused by an attacker, we have developed BusMonitor, a hypervisor-based protection which prevents a malicious tenant from abusing the memory bus\u2019s operation. In this paper we investigate the dangers of previously known and possible future memory bus based side channel attacks. We then show that BusMonitor is able to fully prevent these attacks with negligible impact to the performance of guest applications.", "num_citations": "46\n", "authors": ["267"]}
{"title": "LEAPS: Detecting camouflaged attacks with statistical learning guided by program analysis\n", "abstract": " Currently cyber infrastructures are facing increasingly stealthy attacks that implant malicious payloads under the cover of benign programs. Existing attack detection approaches based on statistical learning methods may generate misleading decision boundaries when processing noisy data with such a mixture of benign and malicious behaviors. On the other hand, attack detection based on formal program analysis may lack completeness or adaptivity when modelling attack behaviors. In light of these limitations, we have developed LEAPS, an attack detection system based on supervised statistical learning to classify benign and malicious system events. Furthermore, we leverage control flow graphs inferred from the system event logs to enable automatic pruning of the training data, which leads to a more accurate classification model when applied to the testing data. Our extensive evaluation shows that, compared\u00a0\u2026", "num_citations": "45\n", "authors": ["267"]}
{"title": "A systematic study of failure proximity\n", "abstract": " Software end-users are the best testers, who keep revealing bugs in software that has undergone rigorous in-house testing. In order to leverage their testing efforts, failure reporting components have been widely deployed in released software. Many utilities of the collected failure data depend on an effective failure indexing technique, which, at the optimal case, would index all failures due to the same bug together. Unfortunately, the problem of failure proximity, which underpins the effectiveness of an indexing technique, has not been systematically studied. This article presents the first systematic study of failure proximity. A failure proximity consists of two components: a fingerprinting function that extracts signatures from failures, and a distance function that calculates the likelihood of two failures being due to the same bug. By considering different instantiations of the two functions, we study an array of six failure\u00a0\u2026", "num_citations": "45\n", "authors": ["267"]}
{"title": "J-force: Forced execution on javascript\n", "abstract": " Web-based malware equipped with stealthy cloaking and obfuscation techniques is becoming more sophisticated nowadays. In this paper, we propose J-FORCE, a crash-free forced JavaScript execution engine to systematically explore possible execution paths and reveal malicious behaviors in such malware. In particular, J-FORCE records branch outcomes and mutates them for further explorations. J-FORCE inspects function parameter values that may reveal malicious intentions and expose suspicious DOM injections. We addressed a number of technical challenges encountered. For instance, we keep track of missing objects and DOM elements, and create them on demand. To verify the efficacy of our techniques, we apply J-FORCE to detect Exploit Kit (EK) attacks and malicious Chrome extensions. We observe that J-FORCE is more effective compared to the existing tools.", "num_citations": "44\n", "authors": ["267"]}
{"title": "Matching control flow of program versions\n", "abstract": " In many application areas, including piracy detection, software debugging and maintenance, situations arise in which there is a need for comparing two versions of a program that dynamically behave the same even though they statically appear to be different. Recently dynamic matching [18] was proposed by us which uses execution histories to automatically produce mappings between instructions in the two program versions. The mappings then can be used to understand the correspondence between the two versions by a user involved in software piracy detection or a comparison checker involved in debugging of optimized code. However, if a program's control flow is substantially altered, which usually occurs in obfuscation or even manual transformations, mappings at instruction level are not sufficient to enable a good understanding of the correspondence. In this paper, we present a comprehensive dynamic\u00a0\u2026", "num_citations": "44\n", "authors": ["267"]}
{"title": "Tracing lineage beyond relational operators\n", "abstract": " Tracing the lineage of data is an important requirement for establishing the quality and validity of data. Recently, the problem of data provenance has been increasingly addressed in database research. Earlier work has been limited to the lineage of data as it is manipulated using relational operations within an RDBMS. While this captures a very important aspect of scientific data processing, the existing work is incapable of handling the equally important, and prevalent, cases where the data is processed by non-relational operations. This is particularly common in scientific data where sophisticated processing is achieved by programs that are not part of a DBMS. The problem of tracking lineage when non-relational operators are used to process the data is particularly challenging since there is potentially no constraint on the nature of the processing. In this paper we propose a novel technique that overcomes this\u00a0\u2026", "num_citations": "44\n", "authors": ["267"]}
{"title": "Enabling tracing of long-running multithreaded programs via dynamic execution reduction\n", "abstract": " Debugging long running multithreaded programs is a very challenging problem when using tracing-based analyses. Since such programs are non-deterministic, reproducing the bug is non-trivial and generating and inspecting traces for long running programs can be prohibitively expensive. We propose a framework in which, to overcome the problem of bug reproducibility, a lightweight logging technique is used to log the events during the original execution. When a bug is encountered, it is reproduced using the generated log and during the replay, a fine-grained tracing technique is employed to collect control-flow/dependence traces that are then used to locate the root cause of the bug. In this paper, we address the key challenges resulting due to tracing, that is, the prohibitively high expense of collecting traces and the significant burden on the user who must examine the large amount of trace information to\u00a0\u2026", "num_citations": "43\n", "authors": ["267"]}
{"title": "Accentuating the positive: atomicity inference and enforcement using correct executions\n", "abstract": " Concurrency bugs are often due to inadequate synchronization that fail to prevent specific (undesirable) thread interleavings. Such errors, often referred to as Heisenbugs, are difficult to detect, prevent, and repair. In this paper, we present a new technique to increase program robustness against Heisenbugs. We profile correct executions from provided test suites to infer fine-grained atomicity properties. Additional deadlock-free locking is injected into the program to guarantee these properties hold on production runs. Notably, our technique does not rely on witnessing or analyzing erroneous executions. The end result is a scheme that only permits executions which are guaranteed to preserve the atomicity properties derived from the profile. Evaluation results on large, real-world, open-source programs show that our technique can effectively suppress subtle concurrency bugs, with small runtime overheads (typically\u00a0\u2026", "num_citations": "42\n", "authors": ["267"]}
{"title": "Convicting exploitable software vulnerabilities: An efficient input provenance based approach\n", "abstract": " Software vulnerabilities are the root cause of a wide range of attacks. Existing vulnerability scanning tools are able to produce a set of suspects. However, they often suffer from a high false positive rate. Convicting a suspect and vindicating false positives are mostly a highly demanding manual process, requiring a certain level of understanding of the software. This limitation significantly thwarts the application of these tools by system administrators or regular users who are concerned about security but lack of understanding of, or even access to, the source code. It is often the case that even developers are reluctant to inspect/fix these numerous suspects unless they are convicted by evidence. In this paper, we propose a lightweight dynamic approach which generates evidence for various security vulnerabilities in software, with the goal of relieving the manual procedure. It is based on data lineage tracing, a\u00a0\u2026", "num_citations": "42\n", "authors": ["267"]}
{"title": "VCR: App-Agnostic Recovery of Photographic Evidence from Android Device Memory Images\n", "abstract": " The ubiquity of modern smartphones means that nearly everyone has easy access to a camera at all times. In the event of a crime, the photographic evidence that these cameras leave in a smartphone's memory becomes vital pieces of digital evidence, and forensic investigators are tasked with recovering and analyzing this evidence. Unfortunately, few existing forensics tools are capable of systematically recovering and inspecting such in-memory photographic evidence produced by smartphone cameras. In this paper, we present VCR, a memory forensics technique which aims to fill this void by enabling the recovery of all photographic evidence produced by an Android device's cameras. By leveraging key aspects of the Android framework, VCR extends existing memory forensics techniques to improve vendor-customized Android memory image analysis. Based on this, VCR targets application-generic artifacts in\u00a0\u2026", "num_citations": "38\n", "authors": ["267"]}
{"title": "Reverse engineering input syntactic structure from program execution and its applications\n", "abstract": " Program input syntactic structure is essential for a wide range of applications such as test case generation, software debugging, and network security. However, such important information is often not available (e.g., most malware programs make use of secret protocols to communicate) or not directly usable by machines (e.g., many programs specify their inputs in plain text or other random formats). Furthermore, many programs claim they accept inputs with a published format, but their implementations actually support a subset or a variant. Based on the observations that input structure is manifested by the way input symbols are used during execution and most programs take input with top-down or bottom-up grammars, we devise two dynamic analyses, one for each grammar category. Our evaluation on a set of real-world programs shows that our technique is able to precisely reverse engineer input syntactic\u00a0\u2026", "num_citations": "36\n", "authors": ["267"]}
{"title": "DSCRETE: Automatic rendering of forensic information from memory images via application logic reuse\n", "abstract": " State-of-the-art memory forensics involves signature-based scanning of memory images to uncover data structure instances of interest to investigators. A largely unaddressed challenge is that investigators may not be able to interpret the content of data structure fields, even with a deep understanding of the data structure\u2019s syntax and semantics. This is very common for data structures with application-specific encoding, such as those representing images, figures, passwords, and formatted file contents. For example, an investigator may know that a buffer field is holding a photo image, but still cannot display (and hence understand) the image. We call this the data structure content reverse engineering challenge. In this paper, we present DSCRETE, a system that enables automatic interpretation and rendering of in-memory data structure contents. DSCRETE is based on the observation that the application in which a data structure is defined usually contains interpretation and rendering logic to generate human-understandable output for that data structure. Hence DSCRETE aims to identify and reuse such logic in the program\u2019s binary and create a \u201cscanner+ renderer\u201d tool for scanning and rendering instances of the data structure in a memory image. Different from signature-based approaches, DSCRETE avoids reverse engineering data structure signatures. Our evaluation with a wide range of real-world application binaries shows that DSCRETE is able to recover a variety of application data\u2014eg, images, figures, screenshots, user accounts, and formatted files and messages\u2014with high accuracy. The raw contents of such data would otherwise be\u00a0\u2026", "num_citations": "34\n", "authors": ["267"]}
{"title": "Bistro: Binary component extraction and embedding for software security applications\n", "abstract": " In software security and malware analysis, researchers often need to directly manipulate binary program \u2013 benign or malicious \u2013 without source code. A useful pair of binary manipulation primitives are binary functional component extraction and embedding, for extracting a functional component from a binary program and for embedding a functional component in a binary program, respectively. Such primitives are applicable to a wide range of security scenarios such as legacy program hardening, binary semantic patching, and malware function analysis. Unfortunately, existing binary rewriting techniques are inadequate to support binary function carving and embedding. In this paper, we present bistro, a system that supports these primitives without symbolic information, relocation information, or compiler support. Bistro preserves functional correctness of both the extracted functional component and the\u00a0\u2026", "num_citations": "34\n", "authors": ["267"]}
{"title": "Accelerating array constraints in symbolic execution\n", "abstract": " Despite significant recent advances, the effectiveness of symbolic execution is limited when used to test complex, real-world software. One of the main scalability challenges is related to constraint solving: large applications and long exploration paths lead to complex constraints, often involving big arrays indexed by symbolic expressions. In this paper, we propose a set of semantics-preserving transformations for array operations that take advantage of contextual information collected during symbolic execution. Our transformations lead to simpler encodings and hence better performance in constraint solving. The results we obtain are encouraging: we show, through an extensive experimental analysis, that our transformations help to significantly improve the performance of symbolic execution in the presence of arrays. We also show that our transformations enable the analysis of new code, which would be otherwise\u00a0\u2026", "num_citations": "33\n", "authors": ["267"]}
{"title": "Introlib: Efficient and transparent library call introspection for malware forensics\n", "abstract": " Dynamic malware analysis aims at revealing malware's runtime behavior. To evade analysis, advanced malware is able to detect the underlying analysis tool (e.g. one based on emulation.) On the other hand, existing malware-transparent analysis tools incur significant performance overhead, making them unsuitable for live malware monitoring and forensics. In this paper, we present IntroLib, a practical tool that traces user-level library calls made by malware with low overhead and high transparency. IntroLib is based on hardware virtualization and resides outside of the guest virtual machine where the malware runs. Our evaluation of an IntroLib prototype with 93 real-world malware samples shows that IntroLib is immune to emulation and API hooking detection by malware, uncovers more semantic information about malware behavior than system call tracing, and incurs low overhead (<15% in all-but-one test case\u00a0\u2026", "num_citations": "33\n", "authors": ["267"]}
{"title": "Cost and precision tradeoffs of dynamic data slicing algorithms\n", "abstract": " Dynamic slicing algorithms are used to narrow the attention of the user or an algorithm to a relevant subset of executed program statements. Although dynamic slicing was first introduced to aid in user level debugging, increasingly applications aimed at improving software quality, reliability, security, and performance are finding opportunities to make automated use of dynamic slicing. In this paper we present the design and evaluation of three precise dynamic data slicing algorithms called the full preprocessing (FP), no preprocessing (NP) and limited preprocessing (LP) algorithms. The algorithms differ in the relative timing of constructing the dynamic data dependence graph and its traversal for computing requested dynamic data slices. Our experiments show that the LP algorithm is a fast and practical precise data slicing algorithm. In fact we show that while precise data slices can be orders of magnitude smaller\u00a0\u2026", "num_citations": "33\n", "authors": ["267"]}
{"title": "Dimsum: Discovering semantic data of interest from un-mappable memory with confidence\n", "abstract": " Uncovering semantic data of interest in memory pages without memory mapping information is an important capability in computer forensics. Existing memory mappingguided techniques do not work in that scenario as pointers in the un-mappable memory cannot be resolved and navigated. To address this problem, we present a probabilistic inference-based approach called DIMSUM to enable the recognition of data structure instances from un-mappable memory. Given a set of memory pages and the specification of a target data structure, DIMSUM will identify instances of the data structure in those pages with quantifiable confidence. More specifically, it builds graphical models based on boolean constraints generated from the data structure and the memory page contents. Probabilistic inference is performed on the graphical models to generate results ranked with probabilities. Our experiments with realworld applications on both Linux and Android platforms show that DIMSUM achieves higher effectiveness than nonprobabilistic approaches without memory mapping information.", "num_citations": "32\n", "authors": ["267"]}
{"title": "Extending path profiling across loop backedges and procedure boundaries\n", "abstract": " Since their introduction, path profiles have been used to guide the application of aggressive code optimizations and performing instruction scheduling. However, for optimization and scheduling, it is often desirable to obtain frequency counts of paths that extend across loop iterations and cross procedure boundaries. These longer paths, referred to as interesting paths, account for over 75% of the flow in a subset of SPEC benchmarks. Although the frequency counts of interesting paths can be estimated from path profiles, the degree of imprecision of these estimates is very high. We extend Ball Larus (BL) paths to create slightly longer overlapping paths and develop an instrumentation algorithm to collect their frequencies. While these paths are slightly longer than BL paths, they enable very precise estimation of frequencies of potentially much longer interesting paths. Our experiments show that the average cost of\u00a0\u2026", "num_citations": "32\n", "authors": ["267"]}
{"title": "TaintMan: an ART-Compatible Dynamic Taint Analysis Framework on Unmodified and Non-Rooted Android Devices\n", "abstract": " Dynamic taint analysis (DTA), as a mainstream information flow tracking technique, has been widely used in mobile security. On the Android platform, the existing DTA approaches are typically implemented by instrumenting the Dalvik virtual machine (DVM) interpreter or the Android emulator with taint enforcement code. The most prominent problem of the interpreter-based approaches is that they cannot work in the new Android RunTime (ART) environment introduced since the 5.0 release. For the emulator-based approaches, the most prominent problem is that they cannot be deployed on real devices. In addition, almost all the existing Android DTA approaches only concern the explicit information flow caused by data dependence, while completely ignore the impact of implicit information flow caused by control dependence. These problems limit their adoption in the latest Android system and make them ineffective\u00a0\u2026", "num_citations": "30\n", "authors": ["267"]}
{"title": "A path-aware approach to mutant reduction in mutation testing\n", "abstract": " Context: Mutation testing, which systematically generates a set of mutants by seeding various faults into the base program under test, is a popular technique for evaluating the effectiveness of a testing method. However, it normally requires the execution of a large amount of mutants and thus incurs a high cost.Objective: A common way to decrease the cost of mutation testing is mutant reduction, which selects a subset of representative mutants. In this paper, we propose a new mutant reduction approach from the perspective of program structure.Method: Our approach attempts to explore path information of the program under test, and select mutants that are as diverse as possible with respect to the paths they cover. We define two path-aware heuristic rules, namely module-depth and loop-depth rules, and combine them with statement- and operator-based mutation selection to develop four mutant reduction strategies\u00a0\u2026", "num_citations": "30\n", "authors": ["267"]}
{"title": "Apex: automatic programming assignment error explanation\n", "abstract": " This paper presents Apex, a system that can automatically generate explanations for programming assignment bugs, regarding where the bugs are and how the root causes led to the runtime failures. It works by comparing the passing execution of a correct implementation (provided by the instructor) and the failing execution of the buggy implementation (submitted by the student). The technique overcomes a number of technical challenges caused by syntactic and semantic differences of the two implementations. It collects the symbolic traces of the executions and matches assignment statements in the two execution traces by reasoning about symbolic equivalence. It then matches predicates by aligning the control dependences of the matched assignment statements, avoiding direct matching of path conditions which are usually quite different. Our evaluation shows that Apex is every effective for 205 buggy real\u00a0\u2026", "num_citations": "30\n", "authors": ["267"]}
{"title": "Reference hijacking: Patching, protecting and analyzing on unmodified and non-rooted android devices\n", "abstract": " Many efforts have been paid to enhance the security of Android. However, less attention has been given to how to practically adopt the enhancements on off-the-shelf devices. In particular, securing Android devices often requires modifying their write-protected underlying system component files (especially the system libraries) by flashing or rooting devices, which is unacceptable in many realistic cases. In this paper, a novel technique, called reference hijacking, is presented to address the problem. By introducing a specially designed reset procedure, a new execution environment is constructed for the target application, in which the reference to the underlying system libraries will be redirected to the security-enhanced alternatives. The technique can be applicable to both the Dalvik and Android Runtime (ART) environments and to almost all mainstream Android versions (2.x to 5.x). To demonstrate the capability of\u00a0\u2026", "num_citations": "30\n", "authors": ["267"]}
{"title": "Debugging the Internet of Things: The Case of Wireless Sensor Networks\n", "abstract": " The Internet of Things (IoT) has the strong potential to support a human society interacting more symbiotically with its physical environment. Indeed, the emergence of tiny devices that sense environmental cues and trigger actuators after consulting logic and human preferences promises a more environmentally aware and less wasteful society. However, the IoT inherently challenges software development processes, particularly techniques for ensuring software reliability. Researchers have developed debugging tools for wireless sensor networks (WSNs), which can be viewed as the enablers of perception in the IoT. These tools gather run-time information on individual sensor node executions and node interactions and then compress that information.", "num_citations": "27\n", "authors": ["267"]}
{"title": "IntroPerf: transparent context-sensitive multi-layer performance inference using system stack traces\n", "abstract": " Performance bugs are frequently observed in commodity software. While profilers or source code-based tools can be used at development stage where a program is diagnosed in a well-defined environment, many performance bugs survive such a stage and affect production runs. OS kernel-level tracers are commonly used in post-development diagnosis due to their independence from programs and libraries; however, they lack detailed program-specific metrics to reason about performance problems such as function latencies and program contexts. In this paper, we propose a novel performance inference system, called IntroPerf, that generates fine-grained performance information -- like that from application profiling tools -- transparently by leveraging OS tracers that are widely available in most commodity operating systems. With system stack traces as input, IntroPerf enables transparent context-sensitive\u00a0\u2026", "num_citations": "27\n", "authors": ["267"]}
{"title": "Static detection of resource contention problems in server-side scripts\n", "abstract": " With modern multi-core architectures, web applications are usually configured to serve multiple requests simultaneously by spawning multiple instances. These instances may access the same external resources such as database tables and files. Such contentions may become severe during peak time, leading to violations of atomic business logic. In this paper, we propose a novel static analysis that detects atomicity violations of external operations for server side scripts. The analysis differs from traditional atomicity violation detection techniques by focusing on external resources instead of shared memory. It consists of three components. The first one is an interprocedural and path-sensitive resource identity analysis that determines whether multiple operations access the same external resource, which is critical to identifying contentions. The second component infers pairs of external operations that should be\u00a0\u2026", "num_citations": "27\n", "authors": ["267"]}
{"title": "RVFUZZER: finding input validation bugs in robotic vehicles through control-guided testing\n", "abstract": " Robotic vehicles (RVs) are being adopted in a variety of application domains. Despite their increasing deployment, many security issues with RVs have emerged, limiting their wider deployment. In this paper, we address a new type of vulnerability in RV control programs, called input validation bugs, which involve missing or incorrect validation checks on control parameter inputs. Such bugs can be exploited to cause physical disruptions to RVs which may result in mission failures and vehicle damages or crashes. Furthermore, attacks exploiting such bugs have a very small footprint: just one innocent-looking ground control command, requiring no code injection, control flow hijacking or sensor spoofing. To prevent such attacks, we propose RVFuzzer, a vetting system for finding input validation bugs in RV control programs through control-guided input mutation. The key insight behind RVFuzzer is that the RV control model, which is the generic theoretical model for a broad range of RVs, provides helpful semantic guidance to improve bug-discovery accuracy and efficiency. Specifically, RVFuzzer involves a control instability detector that detects control program misbehavior, by observing (simulated) physical operations of the RV based on the control model. In addition, RVFuzzer steers the input generation for finding input validation bugs more efficiently, by leveraging results from the control instability detector as feedback. In our evaluation of RVFuzzer on two popular RV control programs, a total of 89 input validation bugs are found, with 87 of them being zero-day bugs.", "num_citations": "26\n", "authors": ["267"]}
{"title": "Reranz: A light-weight virtual machine to mitigate memory disclosure attacks\n", "abstract": " Recent code reuse attacks are able to circumvent various address space layout randomization (ASLR) techniques by exploiting memory disclosure vulnerabilities. To mitigate sophisticated code reuse attacks, we proposed a light-weight virtual machine, ReRanz, which deployed a novel continuous binary code re-randomization to mitigate memory disclosure oriented attacks. In order to meet security and performance goals, costly code randomization operations were outsourced to a separate process, called the\" shuffling process\". The shuffling process continuously flushed the old code and replaced it with a fine-grained randomized code variant. ReRanz repeated the process each time an adversary might obtain the information and upload a payload. Our performance evaluation shows that ReRanz Virtual Machine incurs a very low performance overhead. The security evaluation shows that ReRanz successfully\u00a0\u2026", "num_citations": "26\n", "authors": ["267"]}
{"title": "Cross-Layer Retrofitting of UAVs Against Cyber-Physical Attacks\n", "abstract": " As a rapidly growing cyber-physical platform, unmanned aerial vehicles are facing more security threats as their capabilities and applications continue to expand. Adversaries with detailed knowledge about the vehicle could orchestrate sophisticated attacks that are not easily detected or handled by the vehicle's control system. In this work, we purpose a generic security framework, termed BlueBox, capable of detecting and handling a variety of cyber-physical attacks. To demonstrate an application of BlueBox in practice, we retrofitted an off-the-shelf quadcopter. A series of attacks were then launched by embedding malicious code in the control software and by altering the vehicle's hardware with the specific targeting of sensors, controller, motors, vehicle dynamics, and operating system. Experimental results verified that BlueBox was capable of both detecting a variety of cyber-physical attacks, while also providing\u00a0\u2026", "num_citations": "25\n", "authors": ["267"]}
{"title": "Screen after Previous Screens: Spatial-Temporal Recreation of Android App Displays from Memory Images.\n", "abstract": " Smartphones are increasingly involved in cyber and real world crime investigations. In this paper, we demonstrate a powerful smartphone memory forensics technique, called RetroScope, which recovers multiple previous screens of an Android app\u2014in the order they were displayed\u2014from the phone\u2019s memory image. Different from traditional memory forensics, RetroScope enables spatial-temporal forensics, revealing the progression of the phone user\u2019s interactions with the app (eg, a banking transaction, online chat, or document editing session). RetroScope achieves near perfect accuracy in both the recreation and ordering of reconstructed screens. Further, RetroScope is app-agnostic, requiring no knowledge about an app\u2019s internal data definitions or rendering logic. RetroScope is inspired by the observations that (1) app-internal data on previous screens exists much longer in memory than the GUI data structures that \u201cpackage\u201d them and (2) each app is able to perform context-free redrawing of its screens upon command from the Android framework. Based on these, RetroScope employs a novel interleaved re-execution engine to selectively reanimate an app\u2019s screen redrawing functionality from within a memory image. Our evaluation shows that RetroScope is able to recover full temporally-ordered sets of screens (each with 3 to 11 screens) for a variety of popular apps on a number of different Android devices.", "num_citations": "25\n", "authors": ["267"]}
{"title": "Prius: generic hybrid trace compression for wireless sensor networks\n", "abstract": " Several diagnostic tracing techniques (eg, event, power, and control-flow tracing) have been proposed for run-time debugging and postmortem analysis of wireless sensor networks (WSNs). Traces generated by such techniques can become large, defying the harsh resource constraints of WSNs. Compression is a straightforward candidate to reduce trace sizes, yet is challenged by the same resource constraints. Established trace compression algorithms perform unsatisfactorily under these constraints.", "num_citations": "25\n", "authors": ["267"]}
{"title": "Lightweight tracing for wireless sensor networks debugging\n", "abstract": " Wireless Sensor Networks (WSNs) are being increasingly deployed in the real world to monitor the environment and large industrial infrastructures. The extreme resource constraints inherent to WSNs, the in situ deployment in harsh environments and the lack of run-time support tools make debugging and maintaining WSN applications very challenging. In particular, run-time debugging tools are required to detect and diagnose complex run-time faults such as race-conditions, which occur due to unexpected interaction with the real-world environment. The ability to repeatedly reproduce the failure by replaying the execution from the trace of events that took place can play a crucial role in debugging such faults. Obtaining such a trace is made difficult due to tight resource constraints. In this paper, we propose a lightweight tracing tool for WSNs which uses a novel control flow tracing and encoding scheme to generate\u00a0\u2026", "num_citations": "25\n", "authors": ["267"]}
{"title": "Reuse-oriented camouflaging trojan: Vulnerability detection and attack construction\n", "abstract": " We introduce the reuse-oriented camouflaging trojan-a new threat to legitimate software binaries. To perform a malicious action, such a trojan identifies and reuses an existing function in a legal binary program instead of implementing the function itself. Furthermore, this trojan is stealthy in that the malicious invocation of a targeted function usually takes place in a location where it is legal to do so, closely mimicking a legal invocation. At the network level, the victim binary can still follow its communication protocol without exhibiting any anomalous behavior. Meanwhile, many close-source shareware binaries are rich in functions that can be maliciously \u201creused\u201d, making them attractive targets of this type of attack. In this paper, we present a framework to determine if a given binary program is vulnerable to this attack and to construct a concrete trojan if so. Our experiments with a number of real-world software binaries\u00a0\u2026", "num_citations": "24\n", "authors": ["267"]}
{"title": "Extended whole program paths\n", "abstract": " We describe the design, generation and compression of the extended whole program path (eWPP) representation that not only captures the control flow history of a program execution but also its data dependence history. This representation is motivated by the observation that typically a significant fraction of data dependence history can be recovered from the control flow trace. To capture the remainder of the data dependence history we introduce disambiguation checks in the program whose control flow signatures capture the results of the checks. The resulting extended control flow trace enables the recovery of otherwise unrecoverable data dependences. The code for the checks is designed to minimize the increase in the program execution time and the extended control flow trace size when compared to directly collecting control flow and dependence traces. Our experiments show that compressed eWPPs are\u00a0\u2026", "num_citations": "24\n", "authors": ["267"]}
{"title": "Probabilistic disassembly\n", "abstract": " Disassembling stripped binaries is a prominent challenge for binary analysis, due to the interleaving of code segments and data, and the difficulties of resolving control transfer targets of indirect calls and jumps. As a result, most existing disassemblers have both false positives (FP) and false negatives (FN). We observe that uncertainty is inevitable in disassembly due to the information loss during compilation and code generation. Therefore, we propose to model such uncertainty using probabilities and propose a novel disassembly technique, which computes a probability for each address in the code space, indicating its likelihood of being a true positive instruction. The probability is computed from a set of features that are reachable to an address, including control flow and data flow features. Our experiments with more than two thousands binaries show that our technique does not have any FN and has only 3.7\u00a0\u2026", "num_citations": "23\n", "authors": ["267"]}
{"title": "Analysis of SEAndroid Policies: Combining MAC and DAC in Android\n", "abstract": " Android has become a dominant computing platform, and its popularity has coincided with a surge of malware. The incorporation of Security-Enhanced Linux in Android (SEAndroid) is an important security enhancement to the platform. While SEAndroid adds the benefits of mandatory protection that SELinux brought to desktops and servers, the protection is only as good as the policy. Existing Android devices contain a wide variety of SEAndroid policies, depending on both the version of Android as well as the device manufacturer. In this paper, we present an approach to analyze SEAndroid policies in conjunction with the underlying Linux/Unix Discretionary Access Control policies. We apply our approach to four different versions of Android Open Source Project (AOSP) as well as devices from seven different manufacturers, and find several forms of unintentional privilege assignments.", "num_citations": "23\n", "authors": ["267"]}
{"title": "RevARM: A Platform-Agnostic ARM Binary Rewriter for Security Applications\n", "abstract": " ARM is the leading processor architecture in the emerging mobile and embedded market. Unfortunately, there has been a myriad of security issues on both mobile and embedded systems. While many countermeasures of such security issues have been proposed in recent years, a majority of applications still cannot be patched or protected due to run-time and space overhead constraints and the unavailability of source code. More importantly, the rapidly evolving mobile and embedded market makes any platform-specific solution ineffective. In this paper, we propose RevARM, a binary rewriting technique capable of instrumenting ARM-based binaries without limitation on the target platform. Unlike many previous binary instrumentation tools that are designed to instrument binaries based on x86, RevARM must resolve a number of new, ARM-specific binary rewriting challenges. Moreover, RevARM is able to handle\u00a0\u2026", "num_citations": "23\n", "authors": ["267"]}
{"title": "Light: replay via tightly bounded recording\n", "abstract": " Reproducing concurrency bugs is a prominent challenge. Existing techniques either rely on recording very fine grained execution information and hence have high runtime overhead, or strive to log as little information as possible but provide no guarantee in reproducing a bug. We present Light, a technique that features much lower overhead compared to techniques based on fine grained recording, and that guarantees to reproduce concurrent bugs. We leverage and formally prove that recording flow dependences is the necessary and sufficient condition to reproduce a concurrent bug. The flow dependences, together with the thread local orders that can be automatically inferred (and hence not logged), are encoded as scheduling constraints. An SMT solver is used to derive a replay schedule, which is guaranteed to exist even though it may be different from the original schedule. Our experiments show that Light\u00a0\u2026", "num_citations": "22\n", "authors": ["267"]}
{"title": "FACE-CHANGE: Application-Driven Dynamic Kernel View Switching in a Virtual Machine\n", "abstract": " Kernel minimization has already been established as a practical approach to reducing the trusted computing base. Existing solutions have largely focused on whole-system profiling - generating a globally minimum kernel image that is being shared by all applications. However, since different applications use only part of the kernel's code base, the minimized kernel still includes an unnecessarily large attack surface. Furthermore, once the static minimized kernel is generated, it is not flexible enough to adapt to an altered execution environment (e.g., new workload). FACE-CHANGE is a virtualization-based system to facilitate dynamic switching at runtime among multiple minimized kernels, each customized for an individual application. Based on precedent profiling results, FACE-CHANGE transparently presents a customized kernel view for each application to confine its reach ability of kernel code. In the event that\u00a0\u2026", "num_citations": "22\n", "authors": ["267"]}
{"title": "White box sampling in uncertain data processing enabled by program analysis\n", "abstract": " Sampling is a very important and low-cost approach to uncertain data processing, in which output variations caused by input errors are sampled. Traditional methods tend to treat a program as a blackbox. In this paper, we show that through program analysis, we can expose the internals of sample executions so that the process can become more selective and focused. In particular, we develop a sampling runtime that can selectively sample in input error bounds to expose discontinuity in output functions. It identifies all the program factors that can potentially lead to discontinuity and hash the values of such factors during execution in a cost-effective way. The hash values are used to guide the sampling process. Our results show that the technique is very effective for real-world programs. It can achieve the precision of a high sampling rate with the cost of a lower sampling rate.", "num_citations": "21\n", "authors": ["267"]}
{"title": "Webranz: web page randomization for better advertisement delivery and web-bot prevention\n", "abstract": " Nowadays, a rapidly increasing number of web users are using Ad-blockers to block online advertisements. Ad-blockers are browser-based software that can block most Ads on the websites, speeding up web browsers and saving bandwidth. Despite these benefits to end users, Ad-blockers could be catastrophic for the economic structure underlying the web, especially considering the rise of Ad-blocking as well as the number of technologies and services that rely exclusively on Ads to compensate their cost. In this paper, we introduce WebRanz that utilizes a randomization mechanism to circumvent Ad-blocking. Using WebRanz, content publishers can constantly mutate the internal HTML elements and element attributes of their web pages, without affecting their visual appearances and functionalities. Randomization invalidates the pre-defined patterns that Ad-blockers use to filter out Ads. Though the design of\u00a0\u2026", "num_citations": "20\n", "authors": ["267"]}
{"title": "Android implicit information flow demystified\n", "abstract": " In this paper, a comprehensive analysis of implicit information flow (IIF) on the Android bytecode is presented to identify all potential IIF forms, determine their exploitability, and mitigate the potential threat. By applying control-transfer-oriented semantic analysis of the bytecode language, we identify five IIF forms, some of which are not studied by existing IIF literature. We develop proof-of-concepts (PoCs) for each IIF form to demonstrate their exploitability. The experimental results show that all these PoCs can effectively and efficiently transmit sensitive data, as well as successfully evade the detection of a state-of-the-art privacy monitor TaintDroid. To mitigate the threat of IIF, we propose a solution to defending against IIF leveraging a special control dependence tracking technique and implement a prototype system. The evaluation shows that the prototype can effectively detect information leak by all the identified IIF\u00a0\u2026", "num_citations": "20\n", "authors": ["267"]}
{"title": "Efficient Dynamic Tracking Technique for Detecting Integer-Overflow-to-Buffer-Overflow Vulnerability\n", "abstract": " Integer-Overflow-to-Buffer-Overflow (IO2BO) vulnerabilities can be exploited by attackers to cause severe damages to computer systems. In this paper, we present the design and implementation of IntTracker, an efficient dynamic tracking technique for detecting IO2BO vulnerabilities in C/C++ programs. IntTracker utilizes a static taint analysis to select potential overflow sites that are integer operations along critical paths, from sources that are program points reading values from users, to sinks that are memory allocation sites. It then instruments overflow checks at the selected sites. Instead of producing warnings once integer overflows occur, IntTracker replaces the overflown value with a very large and rarely used integer value (dirty value), and treats such the value as an overflow tag. Tag propagation is performed by the existing program operations without any instrumentation as operations on dirty values often\u00a0\u2026", "num_citations": "19\n", "authors": ["267"]}
{"title": "Virtual DOM coverage for effective testing of dynamic web applications\n", "abstract": " Test adequacy criteria are fundamental in software testing. Among them, code coverage criterion is widely used due to its simplicity and effectiveness. However, in dynamic web application testing, merely covering server-side script code is inadequate because it neglects client-side execution, which plays an important role in triggering client-server interactions to reach important execution states. Similarly, a criterion aiming at covering the UI elements on client-side pages ignores the server-side execution, leading to insufficiency.", "num_citations": "19\n", "authors": ["267"]}
{"title": "Precise Android API Protection Mapping Derivation and Reasoning\n", "abstract": " The Android research community has long focused on building an Android API permission specification, which can be leveraged by app developers to determine the optimum set of permissions necessary for a correct and safe execution of their app. However, while prominent existing efforts provide a good approximation of the permission specification, they suffer from a few shortcomings. Dynamic approaches cannot generate complete results, although accurate for the particular execution. In contrast, static approaches provide better coverage, but produce imprecise mappings due to their lack of path-sensitivity. In fact, in light of Android's access control complexity, the approximations hardly abstract the actual co-relations between enforced protections. To address this, we propose to precisely derive Android protection specification in a path-sensitive fashion, using a novel graph abstraction technique. We further\u00a0\u2026", "num_citations": "18\n", "authors": ["267"]}
{"title": "LAMP: data provenance for graph based machine learning algorithms through derivative computation\n", "abstract": " Data provenance tracking determines the set of inputs related to a given output. It enables quality control and problem diagnosis in data engineering. Most existing techniques work by tracking program dependencies. They cannot quantitatively assess the importance of related inputs, which is critical to machine learning algorithms, in which an output tends to depend on a huge set of inputs while only some of them are of importance. In this paper, we propose LAMP, a provenance computation system for machine learning algorithms. Inspired by automatic differentiation (AD), LAMP quantifies the importance of an input for an output by computing the partial derivative. LAMP separates the original data processing and the more expensive derivative computation to different processes to achieve cost-effectiveness. In addition, it allows quantifying importance for inputs related to discrete behavior, such as control flow\u00a0\u2026", "num_citations": "18\n", "authors": ["267"]}
{"title": "Tracing message transmissions between communications network devices\n", "abstract": " A packet of data and a packet-identification value are transmitted to a network device having an identifier. The stored packet-identification value and the identifier are recorded. The stored packet-identification value is then increased and the process repeats. To receive data, an expected identification value is stored in association with the identifier. A packet and a packet-identification value are received from the network device. The identifier and an indication of receipt are stored. If the received value does not match the expected value for the identifier, the received value is stored. If the values match, the stored packet-identification value and identifier are recorded. If the received value exceeds the expected value, the stored packet-identification value, the identifier and the received identifier are recorded. Subsequently, the stored expected value is increased. The process repeats. Network devices and systems are\u00a0\u2026", "num_citations": "17\n", "authors": ["267"]}
{"title": "Ipa: Improving predictive analysis with pointer analysis\n", "abstract": " Predictive analysis, recently proposed for race detection, guarantees to report no false positives and achieves good coverage. Predictive analysis starts with the trace of an execution and mutates the schedule order of the trace to``predict''the executions that expose the hidden races. Ideally, the predictive analysis should allow the schedule mutation to change the memory location accessed by the field access, which helps meet the``same memory location''requirement of the data race. However, existing predictive approaches, including causality-preserving approaches and symbolic approaches, lack this capability. We propose the first predictive analysis that allows changing the accessed locations. The key challenge is that modeling of the field accesses relies on the location, which may however become unknown due to schedule mutation. We solve this challenge through a novel combination of predictive analysis\u00a0\u2026", "num_citations": "17\n", "authors": ["267"]}
{"title": "Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features\n", "abstract": " With the prevalent use of Deep Neural Networks (DNNs) in many applications, security of these networks is of importance. Pre-trained DNNs may contain backdoors that are injected through poisoned training. These trojaned models perform well when regular inputs are provided, but misclassify to a target output label when the input is stamped with a unique pattern called trojan trigger. Recently various backdoor detection and mitigation systems for DNN based AI applications have been proposed. However, many of them are limited to trojan attacks that require a specific patch trigger. In this paper, we introduce composite attack, a more flexible and stealthy trojan attack that eludes backdoor scanners using trojan triggers composed from existing benign features of multiple labels. We show that a neural network with a composed backdoor can achieve accuracy comparable to its original version on benign data and\u00a0\u2026", "num_citations": "16\n", "authors": ["267"]}
{"title": "ARROW: Automated repair of races on client-side web pages\n", "abstract": " Modern browsers have a highly concurrent page rendering process in order to be more responsive. However, such a concurrent execution model leads to various race issues. In this paper, we present ARROW, a static technique that can automatically, safely, and cost effectively patch certain race issues on client side pages. It works by statically modeling a web page as a causal graph denoting happens-before relations between page elements, according to the rendering process in browsers. Races are detected by identifying inconsistencies between the graph and the dependence relations intended by the developer. Detected races are fixed by leveraging a constraint solver to add a set of edges with the minimum cost to the causal graph so that it is consistent with the intended dependences. The input page is then transformed to respect the repair edges. ARROW has fixed 151 races from 20 real world commercial\u00a0\u2026", "num_citations": "16\n", "authors": ["267"]}
{"title": "Flint: fixing linearizability violations\n", "abstract": " Writing concurrent software while achieving both correctness and efficiency is a grand challenge. To facilitate this task, concurrent data structures have been introduced into the standard library of popular languages like Java and C#. Unfortunately, while the operations exposed by concurrent data structures are atomic (or linearizable), compositions of these operations are not necessarily atomic. Recent studies have found many erroneous implementations of composed concurrent operations.  We address the problem of fixing nonlinearizable composed operations such that they behave atomically. We introduce Flint, an automated fixing algorithm for composed Map operations. Flint accepts as input a composed operation suffering from atomicity violations. Its output, if fixing succeeds, is a composed operation that behaves equivalently to the original operation in sequential runs and is guaranteed to be atomic. To our\u00a0\u2026", "num_citations": "16\n", "authors": ["267"]}
{"title": "Diagnostic tracing for wireless sensor networks\n", "abstract": " Wireless sensor networks are typically deployed in harsh environments, thus post-deployment failures are not infrequent. An execution trace containing events in their order of execution could play a crucial role in postmortem diagnosis of these failures. Obtaining such a trace however is challenging due to stringent resource constraints. We propose an efficient approach to intraprocedural and interprocedural control-flow tracing that generates traces of all interleaving concurrent events and of the control-flow paths taken inside those events. We demonstrate the effectiveness of our approach with the help of case studies and illustrate its low overhead through measurements and simulations.", "num_citations": "16\n", "authors": ["267"]}
{"title": "Python predictive analysis for bug detection\n", "abstract": " Python is a popular dynamic language that allows quick software development. However, Python program analysis engines are largely lacking. In this paper, we present a Python predictive analysis. It first collects the trace of an execution, and then encodes the trace and unexecuted branches to symbolic constraints. Symbolic variables are introduced to denote input values, their dynamic types, and attribute sets, to reason about their variations. Solving the constraints identifies bugs and their triggering inputs. Our evaluation shows that the technique is highly effective in analyzing real-world complex programs with a lot of dynamic features and external library calls, due to its sophisticated encoding design based on traces. It identifies 46 bugs from 11 real-world projects, with 16 new bugs. All reported bugs are true positives.", "num_citations": "15\n", "authors": ["267"]}
{"title": "Indexing noncrashing failures: A dynamic program slicing-based approach\n", "abstract": " Recent software systems usually feature an automated failure reporting component, with which a huge number of failures are collected from software end-users. With a proper support of failure indexing, which identifies failures due to the same fault, the collected failure data can help developers prioritize failure diagnosis, among other utilities of the failure data. Since crashing failures can be effectively indexed by program crashing venues, current practice has seen great success in prioritizing crashing failures. A recent study of bug characteristics indicates that as excellent memory checking tools are widely adopted, semantic bugs and the resulting noncrashing failures have become dominant. Unfortunately, the problem of how to index non-crashing failures has not been seriously studied before. In previous study, two techniques have been proposed to index noncrashing failures, and they are T-Proximity and R\u00a0\u2026", "num_citations": "15\n", "authors": ["267"]}
{"title": "Compressing trace data\n", "abstract": " Trace data are compressed by storing a compression table in a memory. The table corresponds to results of processing a set of training trace data using a table-driven compression algorithm. The trace data are compressed using the table according to the algorithm. The stored compression table is accessed read-only. The table can be determined by automatically processing a set of training trace data using the algorithm and transforming the compression table produced thereby into a lookup-efficient form. A network device includes a network interface, memory, and a processor that stores the table in the memory, compresses the trace data using the stored compression table according to the table-driven compression algorithm, the stored table being accessed read-only during the compressing, and transmits the compressed trace data via the network interface.", "num_citations": "13\n", "authors": ["267"]}
{"title": "Phys: probabilistic physical unit assignment and inconsistency detection\n", "abstract": " Program variables used in robotic and cyber-physical systems often have implicit physical units that cannot be determined from their variable types. Inferring an abstract physical unit type for variables and checking their physical unit type consistency is of particular importance for validating the correctness of such systems. For instance, a variable with the unit of \u2018meter\u2019should not be assigned to another variable with the unit of \u2018degree-per-second\u2019. Existing solutions have various limitations such as requiring developers to annotate variables with physical units and only handling variables that are directly or transitively used in popular robotic libraries with known physical unit information. We observe that there are a lot of physical unit hints in these softwares such as variable names and specific forms of expressions. These hints have uncertainty as developers may not respect conventions. We propose to model them\u00a0\u2026", "num_citations": "13\n", "authors": ["267"]}
{"title": "Tracing Message Transmissions Between Communicating Network Devices\n", "abstract": " A packet of data and a packet-identification value are transmitted to a network device having an identifier. The stored packet-identification value and the identifier are recorded. The stored packet-identification value is then increased and the process repeats. To receive data, an expected identification value is stored in association with the identifier. A packet and a packet-identification value are received from the network device. The identifier and an indication of receipt are stored. If the received value does not match the expected value for the identifier, the received value is stored. If the values match, the stored packet-identification value and identifier are recorded. If the received value exceeds the expected value, the stored packet-identification value, the identifier and the received identifier are recorded. Subsequently, the stored expected value is increased. The process repeats. Network devices and systems are\u00a0\u2026", "num_citations": "12\n", "authors": ["267"]}
{"title": "RAIVE: runtime assessment of floating-point instability by vectorization\n", "abstract": " Floating point representation has limited precision and inputs to floating point programs may also have errors. Consequently, during execution, errors are introduced, propagated, and accumulated, leading to unreliable outputs. We call this the instability problem. We propose RAIVE, a technique that identifies output variations of a floating point execution in the presence of instability. RAIVE transforms every floating point value to a vector of multiple values\u2013the values added to create the vector are obtained by introducing artifi-cial errors that are upper bounds of actual errors. The propagation of artificial errors models the propagation of actual errors. When values in vectors result in discrete execution differences (eg, following different paths), the execution is forked to capture the resulting output variations. Our evaluation shows that RAIVE can precisely capture output variations. Its overhead (340%) is 2.43 times lower\u00a0\u2026", "num_citations": "12\n", "authors": ["267"]}
{"title": "Memory slicing\n", "abstract": " Traditional dynamic program slicing techniques are code-centric, meaning dependences are introduced between executed statement instances, which gives rise to various problems such as space requirement is decided by execution length; dependence graphs are highly redundant so that inspecting them is labor intensive. In this paper, we propose a data-centric dynamic slicing technique, in which dependences are introduced between memory locations. Doing so, the space complexity is bounded by memory footprint instead of execution length. Moreover, presenting dependences between memory locations is often more desirable for human inspection during debugging as redundant dependences are suppressed. Our evaluation shows that the proposed technique supersedes traditional dynamic slicing techniques in terms of effectiveness and efficiency.", "num_citations": "12\n", "authors": ["267"]}
{"title": "SemCluster: Clustering of Imperative Programming Assignments Based on Quantitative Semantic Features\n", "abstract": " A fundamental challenge in automated reasoning about programming assignments at scale is clustering student submissions based on their underlying algorithms. State-of-the-art clustering techniques are sensitive to control structure variations, cannot cluster buggy solutions with similar correct solutions, and either require expensive pair-wise program analyses or training efforts. We propose a novel technique that can cluster small imperative programs based on their algorithmic essence:(A) how the input space is partitioned into equivalence classes and (B) how the problem is uniquely addressed within individual equivalence classes. We capture these algorithmic aspects as two quantitative semantic program features that are merged into a program's vector representation. Programs are then clustered using their vector representations. The computation of our first semantic feature leverages model counting to\u00a0\u2026", "num_citations": "11\n", "authors": ["267"]}
{"title": "Correlations between deep neural network model coverage criteria and model quality\n", "abstract": " Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (eg, model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.", "num_citations": "10\n", "authors": ["267"]}
{"title": "Software Numerical Instability Detection and Diagnosis by Combining Stochastic and Infinite-Precision Testing\n", "abstract": " Numerical instability is a well-known problem that may cause serious runtime failures. This paper discusses the reason of instability in software development process, and presents a toolchain that not only detects the potential instability in software, but also diagnoses the reason for such instability. We classify the reason of instability into two categories. When it is introduced by software requirements, we call the instability caused by problem . In this case, it cannot be avoided by improving software development, but requires inspecting the requirements, especially the underlying mathematical properties. Otherwise, we call the instability caused by practice. We design our toolchain as four loosely-coupled tools, which combine stochastic arithmetic with infinite-precision testing. Each tool in our toolchain can be configured with different strategies according to the properties of the analyzed software. We evaluate our\u00a0\u2026", "num_citations": "10\n", "authors": ["267"]}
{"title": "IntEQ: recognizing benign integer overflows via equivalence checking across multiple precisions\n", "abstract": " Integer overflow (IO) vulnerabilities can be exploited by attackers to compromise computer systems. In the mean time, IOs can be used intentionally by programmers for benign purposes such as hashing and random number generation. Hence, differentiating exploitable and harmful IOs from intentional and benign ones is an important challenge. It allows reducing the number of false positives produced by IO vulnerability detection techniques, helping developers or security analysts to focus on fixing critical IOs without inspecting the numerous false alarms. The difficulty of recognizing benign IOs mainly lies in inferring the intent of programmers from source code.", "num_citations": "10\n", "authors": ["267"]}
{"title": "Debugging with intelligence via probabilistic inference\n", "abstract": " We aim to debug a single failing execution without the assistance from other passing/failing runs. In our context, debugging is a process with substantial uncertainty-lots of decisions have to be made such as what variables shall be inspected first. To deal with such uncertainty, we propose to equip machines with human-like intelligence. Specifically, we develop a highly automated debugging technique that aims to couple human-like reasoning (eg, dealing with uncertainty and fusing knowledge) with program semantics based analysis, to achieve benefits from the two and mitigate their limitations. We model debugging as a probabilistic inference problem, in which the likelihood of each executed statement instance and variable being correct/faulty is modeled by a random variable. Human knowledge, human-like reasoning rules and program semantics are modeled as conditional probability distributions, also called\u00a0\u2026", "num_citations": "9\n", "authors": ["267"]}
{"title": "Avoiding program failures through safe execution perturbations\n", "abstract": " We present an online framework to capture and recover from program failures and prevent them from occurring in the future through safe execution perturbations. The perturbations are safe as they respect the semantics of the program. We use a checkpointing/logging mechanism to capture a program execution to an event log. If the execution results in a failure, the framework automatically searches for perturbation of the execution by altering the event log and replaying the execution using the altered log to avoid the failure. If found, the perturbation is recorded as a dynamic patch, which is later applied by all future executions of this application to prevent the failure from occurring again. Our experiments show that the proposed framework is very effective in avoiding concurrency faults, heap memory overflow faults, and malicious requests. The entailed overhead for normal execution is very low (2-18%).", "num_citations": "9\n", "authors": ["267"]}
{"title": "Fault location via precise dynamic slicing\n", "abstract": " Developing automated techniques for identifying a fault candidate set (ie, subset of executed statements that contains the faulty code responsible for the failure during a program run), can greatly reduce the effort of debugging. Over 15 years ago precise dynamic slicing was proposed to identify a fault candidate set as consisting of all executed statements that influence the computation of an incorrect value through a chain of data and/or control dependences. However, the challenge of making precise dynamic slicing practical has not been addressed. This dissertation addresses this challenge and makes precise dynamic slicing useful for debugging realistic applications. First, the cost of computing precise dynamic slices is greatly reduced. Second, innovative ways of using precise dynamic slicing are identified to produce small failure candidate sets.", "num_citations": "8\n", "authors": ["267"]}
{"title": "PMP: Cost-effective Forced Execution with Probabilistic Memory Pre-planning\n", "abstract": " Malware is a prominent security threat and exposing malware behavior is a critical challenge. Recent malware often has payload that is only released when certain conditions are satisfied. It is hence difficult to fully disclose the payload by simply executing the malware. In addition, malware samples may be equipped with cloaking techniques such as VM detectors that stop execution once detecting that the malware is being monitored. Forced execution is a highly effective method to penetrate malware self-protection and expose hidden behavior, by forcefully setting certain branch outcomes. However, an existing state-of-the-art forced execution technique X-Force is very heavyweight, requiring tracing individual instructions, reasoning about pointer alias relations on-the-fly, and repairing invalid pointers by on-demand memory allocation. We develop a light-weight and practical forced execution technique. Without\u00a0\u2026", "num_citations": "7\n", "authors": ["267"]}
{"title": "Uiscope: Accurate, instrumentation-free, and visible attack investigation for gui applications\n", "abstract": " Existing attack investigation solutions for GUI applications suffer from a few limitations such as inaccuracy (because of the dependence explosion problem), requiring instrumentation, and providing very low visibility. Such limitations have hindered their widespread and practical deployment. In this paper, we present UISCOPE, a novel accurate, instrumentationfree, and visible attack investigation system for GUI applications. The core idea of UISCOPE is to perform causality analysis on both UI elements/events which represent users\u2019 perspective and low-level system events which provide detailed information of what happens under the hood, and then correlate system events with UI events to provide high accuracy and visibility. Long running processes are partitioned to individual UI transitions, to which low-level system events are attributed, making the results accurate. The produced graphs contain (causally related) UI elements with which users are very familiar, making them easily accessible. We deployed UISCOPE on 7 machines for a week, and also utilized UISCOPE to conduct an investigation of 6 realworld attacks. Our evaluation shows that compared to existing works, UISCOPE introduces neglibible overhead (less than 1% runtime overhead and 3.05 MB event logs per hour on average) while UISCOPE can precisely identify attack provenance while offering users thorough visibility into the attack context.", "num_citations": "7\n", "authors": ["267"]}
{"title": "AdBudgetKiller: Online Advertising Budget Draining Attack\n", "abstract": " In this paper, we present a new ad budget draining attack. By repeatedly pulling ads from targeted advertisers using crafted browsing profiles, we are able to reduce the chance of showing their ads to real-human visitors and trash the ad budget. From the advertiser profiles collected by an automated crawler, we infer advertising strategies, train satisfying browsing profiles and launch large-scale attacks. We evaluate our methods on 291 public advertisers selected from Alexa Top 500, where we successfully reveal the targeting strategies used by 87% of the advertisers we considered. We also executed a series of attacks against a controlled advertiser and 3 real-world advertisers within the ethical and legal boundary. The results show that we are able to fetch 40,958 ads and drain up to $155.89 from the targeted advertisers within an hour.", "num_citations": "7\n", "authors": ["267"]}
{"title": "Lprov: Practical Library-aware Provenance Tracing\n", "abstract": " With the continuing evolution of sophisticated APT attacks, provenance tracking is becoming an important technique for efficient attack investigation in enterprise networks. Most of existing provenance techniques are operating on system event auditing that discloses dependence relationships by scrutinizing syscall traces. Unfortunately, such auditing-based provenance is not able to track the causality of another important dimension in provenance, the shared libraries. Different from other data-only system entities like files and sockets, dynamic libraries are linked at runtime and may get executed, which poses new challenges in provenance tracking. For example, library provenance cannot be tracked by syscalls and mapping; whether a library function is called and how it is called within an execution context is invisible at syscall level; linking a library does not promise their execution at runtime. Addressing these\u00a0\u2026", "num_citations": "7\n", "authors": ["267"]}
{"title": "Scaling up symbolic analysis by removing z-equivalent states\n", "abstract": " Path explosion is a major issue in applying path-sensitive symbolic analysis to large programs. We observe that many symbolic states generated by the symbolic analysis of a procedure are indistinguishable to its callers. It is, therefore, possible to keep only one state from each set of equivalent symbolic states without affecting the analysis result. Based on this observation, we propose an equivalence relation called z-equivalence, which is weaker than logical equivalence, to relate a large number of z-equivalent states. We prove that z-equivalence is strong enough to guarantee that paths to be traversed by the symbolic analysis of two z-equivalent states are identical, giving the same solutions to satisfiability and validity queries. We propose a sound linear algorithm to detect z-equivalence. Our experiments show that the symbolic analysis that leverages z-equivalence is able to achieve more than ten orders of\u00a0\u2026", "num_citations": "7\n", "authors": ["267"]}
{"title": "Dimsum: Discovering semantic data of interest from un-mappable with confidence\n", "abstract": " Uncovering semantic data of interest in memory pages without memory mapping information is an important capability in computer forensics. Existing memory mappingguided techniques do not work in that scenario as pointers in the un-mappable memory cannot be resolved and navigated. To address this problem, we present a probabilistic inference-based approach called DIMSUM to enable the recognition of data structure instances from un-mappable memory. Given a set of memory pages and the specification of a target data structure, DIMSUM will identify instances of the data structure in those pages with quantifiable confidence. More specifically, it builds graphical models based on boolean constraints generated from the data structure and the memory page contents. Probabilistic inference is performed on the graphical models to generate results ranked with probabilities. Our experiments with realworld applications on both Linux and Android platforms show that DIMSUM achieves higher effectiveness than nonprobabilistic approaches without memory mapping information. 1", "num_citations": "7\n", "authors": ["267"]}
{"title": "Software-based Realtime Recovery from Sensor Attacks on Robotic Vehicles\n", "abstract": " We present a novel technique to recover robotic vehicles (RVs) from various sensor attacks with so-called software sensors. Specifically, our technique builds a predictive state-space model based on the generic system identification technique. Sensor measurement prediction based on the state-space model runs as a software backup of the corresponding physical sensor. When physical sensors are under attacks, the corresponding software sensors can isolate and recover the compromised sensors individually to prevent further damage. We apply our prototype to various sensor attacks on six RV systems, including a real quadrotor and a rover. Our evaluation results demonstrate that our technique can practically and safely recover the vehicle from various attacks on multiple sensors under different maneuvers, preventing crashes.", "num_citations": "6\n", "authors": ["267"]}
{"title": "BDA: practical dependence analysis for binary executables by unbiased whole-program path sampling and per-path abstract interpretation\n", "abstract": " Binary program dependence analysis determines dependence between instructions and hence is important for many applications that have to deal with executables without any symbol information. A key challenge is to identify if multiple memory read/write instructions access the same memory location. The state-of-the-art solution is the value set analysis (VSA) that uses abstract interpretation to determine the set of addresses that are possibly accessed by memory instructions. However, VSA is conservative and hence leads to a large number of bogus dependences and then substantial false positives in downstream analyses such as malware behavior analysis. Furthermore, existing public VSA implementations have difficulty scaling to complex binaries. In this paper, we propose a new binary dependence analysis called BDA enabled by a randomized abstract interpretation technique. It features a novel whole\u00a0\u2026", "num_citations": "6\n", "authors": ["267"]}
{"title": "Automated refactoring for stampedlock\n", "abstract": " StampedLock, proposed in JDK 1.8, provides many interesting features, such as optimistic read locks and upgrading/downgrading locks to improve the design of concurrent programs instead of employing pure read/write locks. Existing refactorings have proposed algorithms to convert locks, but there are a few refactorings that use these promising features of StampedLock. To illustrate a possible refactoring, this paper first shows three code transformations based on StampedLock. Then, this paper presents CLOCK, an automated refactoring tool that helps developers convert the synchronized lock into the StampedLock. An algorithm for reentrance analysis is proposed for the precondition validation. The write lock, read lock, optimistic read lock, and upgrading/downgrading lock are inferred and refactored. CLOCK is evaluated with the SPECjbb2005 benchmark and two real-world applications, Xalan and FOP. A total\u00a0\u2026", "num_citations": "6\n", "authors": ["267"]}
{"title": "UI driven Android application reduction\n", "abstract": " While smartphones and mobile apps have been an integral part of our life, modern mobile apps tend to contain a lot of rarely used functionalities. For example, applications contain advertisements and offer extra features such as recommended news stories in weather apps. While these functionalities are not essential to an app, they nonetheless consume power, CPU cycles and bandwidth. In this paper, we design a UI driven approach that allows customizing an Android app by removing its unwanted functionalities. In particular, our technique displays the UI and allows the user to select elements denoting functionalities that she wants to remove. Using this information, our technique automatically removes all the code elements related to the selected functionalities, including all the relevant background tasks. The underlying analysis is a type system, in which each code element is tagged with a type indicating if it\u00a0\u2026", "num_citations": "6\n", "authors": ["267"]}
{"title": "CPR: cross platform binary code reuse via platform independent trace program\n", "abstract": " The rapid growth of Internet of Things (IoT) has been created a number of new platforms recently. Unfortunately, such variety of IoT devices causes platform fragmentation which makes software development on such devices challenging. In particular, existing programs cannot be simply reused on such devices as they rely on certain underlying hardware and software interfaces which we call platform dependencies. In this paper, we present CPR, a novel technique that synthesizes a platform independent program from a platform dependent program. Specifically, we leverage an existing system called PIEtrace which can generate a platform independent trace program. The generated trace program is platform independent while it can only reproduce a specific execution path. Hence, we develop an algorithm to merge a set of platform independent trace programs and synthesize a general program that can take\u00a0\u2026", "num_citations": "6\n", "authors": ["267"]}
{"title": "Automatic failure inducing chain computation through aligned execution comparison\n", "abstract": " In this paper, we propose an automated debugging technique that explains a failure by computing its causal path leading from the root cause to the failure. Given a failing execution, the technique first searches for a dynamic patch. Fine-grained execution comparison between the failing run and the patched run is performed to isolate the causal path. We introduce a formal system, wherein the corrected version of a faulty program is assumed so that the concept of ideal failure inducing chain (FIC) can be defined by comparing the failing run and the run on the corrected program using the same input. Properties of such chains are studied. A product of the formal system is a metric that serves in the objective evaluation of the proposed technique. We identify a key enabling technique called execution indexing, whose goal is to establish a mapping between equivalent points in two different executions so that comparison\u00a0\u2026", "num_citations": "6\n", "authors": ["267"]}
{"title": "White-Box Program Tuning\n", "abstract": " Many programs or algorithms are largely parameterized, especially those based on heuristics. The quality of the results depends on the parameter setting. Different inputs often have different optimal settings. Program tuning is hence of great importance. Existing tuning techniques treat the program as a black-box and hence cannot leverage the internal program states to achieve better tuning. We propose a white-box tuning technique that is implemented as a library. The user can compose complex program tuning tasks by adding a small number of library calls to the original program and providing a few callback functions. Our experiments on 13 widely-used real-world programs show that our technique substantially improves data processing results and outperforms OpenTuner, the state-of-the-art black-box tuning technique.", "num_citations": "5\n", "authors": ["267"]}
{"title": "A Hypervisor Level Provenance System to Reconstruct Attack Story Caused by Kernel Malware\n", "abstract": " Provenance of system subjects (e.g., processes) and objects (e.g., files) are very useful for many forensics tasks. In our analysis and comparison of existing Linux provenance tracing systems, we found that most systems assume the Linux kernel to be in the trust base, making these systems vulnerable to kernel level malware. To address this problem, we present HProve, a hypervisor level provenance tracing system to reconstruct kernel malware attack story. It monitors the execution of kernel functions and sensitive objects, and correlates the system subjects and objects to form the causality dependencies for the attacks. We evaluated our prototype on 12 real world kernel malware samples, and the results show that it can correctly identify the provenance behaviors of the kernel malware.", "num_citations": "5\n", "authors": ["267"]}
{"title": "Backdoor Scanning for Deep Neural Networks through K-Arm Optimization\n", "abstract": " Back-door attack poses a severe threat to deep learning systems. It injects hidden malicious behaviors to a model such that any input stamped with a special pattern can trigger such behaviors. Detecting back-door is hence of pressing need. Many existing defense techniques use optimization to generate the smallest input pattern that forces the model to misclassify a set of benign inputs injected with the pattern to a target label. However, the complexity is quadratic to the number of class labels such that they can hardly handle models with many classes. Inspired by Multi-Arm Bandit in Reinforcement Learning, we propose a K-Arm optimization method for backdoor detection. By iteratively and stochastically selecting the most promising labels for optimization with the guidance of an objective function, we substantially reduce the complexity, allowing to handle models with many classes. Moreover, by iteratively refining the selection of labels to optimize, it substantially mitigates the uncertainty in choosing the right labels, improving detection accuracy. At the time of submission, the evaluation of our method on over 4000 models in the IARPA TrojAI competition from round 1 to the latest round 4 achieves top performance on the leaderboard. Our technique also supersedes three state-of-the-art techniques in terms of accuracy and the scanning time needed.", "num_citations": "4\n", "authors": ["267"]}
{"title": "Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification\n", "abstract": " Trojan (backdoor) attack is a form of adversarial attack on deep neural networks where the attacker provides victims with a model trained/retrained on malicious data. The backdoor can be activated when a normal input is stamped with a certain pattern called trigger, causing misclassification. Many existing trojan attacks have their triggers being input space patches/objects (e.g., a polygon with solid color) or simple input transformations such as Instagram filters. These simple triggers are susceptible to recent backdoor detection algorithms. We propose a novel deep feature space trojan attack with five characteristics: effectiveness, stealthiness, controllability, robustness and reliance on deep features. We conduct extensive experiments on 9 image classifiers on various datasets including ImageNet to demonstrate these properties and show that our attack can evade state-of-the-art defense.", "num_citations": "4\n", "authors": ["267"]}
{"title": "How Android developers handle evolution-induced API compatibility issues: a large-scale study\n", "abstract": " As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them. In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to\u00a0\u2026", "num_citations": "4\n", "authors": ["267"]}
{"title": "Dual-force: understanding WebView malware via cross-language forced execution\n", "abstract": " Modern Android malwares tend to use advanced techniques to cover their malicious behaviors. They usually feature multi-staged, condition-guarded and environment-specific payloads. An increasing number of them utilize WebView, particularly the two-way communications between Java and JavaScript, to evade detection and analysis of existing techniques. We propose Dual-Force, a forced execution technique which simultaneously forces both Java and JavaScript code of WebView applications to execute along various paths without requiring any environment setup or providing any inputs manually. As such, the hidden payloads of WebView malwares are forcefully exposed. The technique features a novel execution model that allows forced execution to suppress exceptions and continue execution. Experimental results show that Dual-Force precisely exposes malicious payload in 119 out of 150 WebView\u00a0\u2026", "num_citations": "4\n", "authors": ["267"]}
{"title": "Parallel execution profiles\n", "abstract": " Observing the relative behavior of an application's threads is critical to identifying performance bottlenecks and understanding their root causes. We present parallel execution profiles (PEPs), which capture the relative behavior of parallel threads in terms of the user selected code regions they execute. The user annotates the program to identify code regions of interest. The PEP divides the execution time of a multithreaded application into time intervals or a sequence of frames during which the code regions being executed in parallel by application threads remain the same. PEPs can be easily analyzed to compute execution times spent by the application in interesting behavior states. This helps user understand the severity of common performance problems such as excessive waiting on events by threads, threads contending for locks, and the presence of straggler threads.", "num_citations": "4\n", "authors": ["267"]}
{"title": "PIEtrace: Platform independent executable trace\n", "abstract": " To improve software dependability, a large number of software engineering tools have been developed over years. Many of them are difficult to apply in practice because their system and library requirements are incompatible with those of the subject software. We propose a technique called platform independent executable trace. Our technique traces and virtualizes a regular program execution that is platform dependent, and generates a stand-alone program called the trace program. Running the trace program re-generates the original execution. More importantly, trace program execution is completely independent of the underlying operating system and libraries such that it can be compiled and executed on arbitrary platforms. As such, it can be analyzed by a third party tool on a platform preferred by the tool. We have implemented the technique on x86 and sensor platforms. We show that buggy executions of 10\u00a0\u2026", "num_citations": "4\n", "authors": ["267"]}
{"title": "Demo Abstract: Diagnostic tracing of wireless sensor networks with TinyTracer\n", "abstract": " Run-time debugging tools are required to detect and diagnose post-deployment failures in wireless sensor networks. Reproducing a failure from the trace of past events can play a crucial role in diagnosis. We describe TinyTracer, an efficient interprocedural control-flow tracing tool that generates the trace of all interleaving concurrent events as well as the control-flow paths taken. TinyTracer enables reproducing failures at a later stage, allowing the programmer to diagnose failures effectively. In this demo, we demonstrate the ease of use of TinyTracer. We see TinyTracer as an important tool for post-deployment diagnosis, which can enable future research on trace-based debugging approaches for wireless sensor networks.", "num_citations": "4\n", "authors": ["267"]}
{"title": "Whole Execution Traces and their use in Debugging\n", "abstract": " Profiling techniques have greatly advanced in recent years. Extensive amounts of dynamic information can be collected (eg, control flow, address and data values, data, and control dependences), and sophisticated dynamic analysis techniques can be employed to assist in improving the performance and reliability of software. In this chapter we describe a novel representation called whole execution traces that can hold a vast amount of dynamic information in a form that provides easy access to this information during dynamic analysis. We demonstrate the use of this representation in locating faulty code in programs through dynamic-slicing-and dynamic-matching-based analysis of dynamic information generated by failing runs of faulty programs.", "num_citations": "4\n", "authors": ["267"]}
{"title": "From Control Model to Program: Investigating Robotic Aerial Vehicle Accidents with {MAYDAY}\n", "abstract": " With wide adoption of robotic aerial vehicles (RAVs), their accidents increasingly occur, calling for in-depth investigation of such accidents. Unfortunately, an inquiry to \u201cwhy did my drone crash\u201d often ends up with nowhere, if the root cause lies in the RAV\u2019s control program, due to the key challenges in evidence and methodology:(1) Current RAVs\u2019 flight log only records high-level vehicle control states and events, without recording control program execution;(2) The capability of \u201cconnecting the dots\u201d\u2013from controller anomaly to program variable corruption to program bug location\u2013is lacking. To address these challenges, we develop MAYDAY, a cross-domain post-accident investigation framework by mapping control model to control program, enabling (1) in-flight logging of control program execution, and (2) traceback to the control-semantic bug that led to an accident, based on control-and program-level logs. We have applied MAYDAY to ArduPilot, a popular open-source RAV control program that runs on a wide range of commodity RAVs. Our investigation of 10 RAV accidents caused by real ArduPilot bugs demonstrates that MAYDAY is able to pinpoint the root causes of these accidents within the program with high accuracy and minimum runtime and storage overhead. We also found 4 recently patched bugs still vulnerable and alerted the ArduPilot team.", "num_citations": "3\n", "authors": ["267"]}
{"title": "A Lightweight Program Dependence Based Approach to Concurrent Mutation Analysis\n", "abstract": " Mutation analysis is a classical software testing approach which attempts to imitate faults using a set of mutants. It has been advocated to be an appropriate technique for evaluating the quality of test suites as well as the effectiveness of a testing method. However, the applicability of mutation analysis, especially in many practical situations, has been hindered due to the high computation cost and the long execution time, which are mainly caused by the large number of mutants. Numerous studies, particularly those based on parallel computing, have been conducted to reduce the overhead of mutation analysis. In this paper, we aim to improve the efficiency of mutation analysis from a different perspective. We make use of lightweight program analysis techniques to identify a group of mutants that share the common execution traces before the mutation location, and then merge them into a synthesized program with the\u00a0\u2026", "num_citations": "3\n", "authors": ["267"]}
{"title": "Integration of cultivated land quality grades and environmental assessment achievements by using searching method\n", "abstract": " On the premise of the necessity of integration of cultivated land quality grades and geochemical assessment achievements and aiming at the weights determination in the integration process, the searching method was put forward to determine weights value range of cultivated land utilizations and environmental health grades by analyzing the changes of errors. Considering the influences the grade numbers on the changes of errors, the equal interval was changed and three gradation projects were designed on the premise of unchanged data base, which aimed to the integration and analysis of errors. The method proposed was verified in a county of west part in Jilin Province. The results indicated that the reasonable value range of cultivated land utilization grades was [0.7, 0.9]. The integration achievements can reflect the distribution of cultivated land quality by using different gradation projects in the integrated\u00a0\u2026", "num_citations": "3\n", "authors": ["267"]}
{"title": "Lightweight task graph inference for distributed applications\n", "abstract": " Recent paradigm shifts in distributed computing such as the advent of cloud computing pose new challenges to the analysis of distributed executions. One important new characteristic is that the management staff of computing platforms and the developers of applications are separated by corporate boundaries. The net result is that once applications go wrong, the most readily available debugging aids for developers are the visible output of the application and any log files collected during their execution. In this paper, we propose the concept of task graphs as a foundation to represent distributed executions, and present a low overhead algorithm to infer task graphs from event log files. Intuitively, a task represents an autonomous segment of computation inside a thread. Edges between tasks represent their interactions and preserve programmers' notion of data and control flows. Our technique leverages existing\u00a0\u2026", "num_citations": "3\n", "authors": ["267"]}
{"title": "Scalable dynamic information flow tracking and its applications\n", "abstract": " We are designing scalable dynamic information flow tracking techniques and employing them to carry out tasks related to debugging (bug location and fault avoidance), security (software attack detection), and data validation (lineage tracing of scientific data). The focus of our ongoing work is on developing online dynamic analysis techniques for long running multithreaded programs that may be executed on a single core or on multiple cores to exploit thread level parallelism.", "num_citations": "3\n", "authors": ["267"]}
{"title": "Cost Effective Forward Tracing Data Lineage\n", "abstract": " Data lineage plays a critical role in verifying data correctness in scientific databases and data warehousing. In this paper, we clearly define forward data lineage in bag semantics and show its properties. We propose a tracing method that piggybacks normal query evaluation. Our method effectively supports aggregation and variable granularity lineage. More importantly, it features cost effective tracing through sophisticated implementation. The implementation, based on roBDDs, exploits the clustering and overlapping characteristics of data lineage to significantly reduce the overhead. Prior lineage tracing techniques compute lineage backward. More specifically, given a query that produces a view, the lineage is computed by executing the reverse query. While such techniques are believed to be very efficient due to the lazy computation, they are indeed inferior to the proposed forward method in most aspects.", "num_citations": "3\n", "authors": ["267"]}
{"title": "Nonlinear compression in an auditory-nerve model\n", "abstract": " A computational model was improved to explore the compressive nonlinearity of auditory-nerve (AN) fibers. The model can produce physiologically realistic compression as well as a wide range of other nonlinear behavior. It is also designed to simulate the responses of a population of AN fibers tuned to different frequencies.", "num_citations": "3\n", "authors": ["267"]}
{"title": "Cyber-Physical Inconsistency Vulnerability Identification for Safety Checks in Robotic Vehicles\n", "abstract": " We propose a new type of vulnerability for Robotic Vehicles (RVs), called Cyber-Physical Inconsistency. These vulnerabilities target safety checks in RVs (eg, crash detection). They can be exploited by setting up malicious environment conditions such as placing an obstacle with a certain weight and a certain angle in the RV's trajectory. Once exploited, the safety checks may fail to report real physical accidents or report false alarms (while the RV is still operating normally). Both situations could lead to life-threatening consequences. The root cause of such vulnerabilities is that existing safety checks are mostly using simple range checks implemented in general-purpose programming languages, which are incapable of describing the complex and delicate physical world. We develop a novel technique that requires the interplay of program analysis, vehicle modeling, and search-based testing to identify such\u00a0\u2026", "num_citations": "2\n", "authors": ["267"]}
{"title": "Finding client-side business flow tampering vulnerabilities\n", "abstract": " The sheer complexity of web applications leaves open a large attack surface of business logic. Particularly, in some scenarios, developers have to expose a portion of the logic to the client-side in order to coordinate multiple parties (eg merchants, client users, and third-party payment services) involved in a business process. However, such client-side code can be tampered with on the fly, leading to business logic perturbations and financial loss. Although developers become familiar with concepts that the client should never be trusted, given the size and the complexity of the client-side code that may be even incorporated from third parties, it is extremely challenging to understand and pinpoint the vulnerability. To this end, we investigate client-side business flow tampering vulnerabilities and develop a dynamic analysis based approach to automatically identifying such vulnerabilities. We evaluate our technique on\u00a0\u2026", "num_citations": "2\n", "authors": ["267"]}
{"title": "Testing Deep Learning Models for Image Analysis Using Object-Relevant Metamorphic Relations\n", "abstract": " Deep learning models are widely used for image analysis. While they offer high performance in terms of accuracy, people are concerned about if these models inappropriately make inferences using irrelevant features that are not encoded from the target object in a given image. To address the concern, we propose a metamorphic testing approach that assesses if a given inference is made based on irrelevant features. Specifically, we propose two novel metamorphic relations to detect such inappropriate inferences. We applied our approach to 10 image classification models and 10 object detection models, with three large datasets, i.e., ImageNet, COCO, and Pascal VOC. Over 5.3% of the top-5 correct predictions made by the image classification models are subject to inappropriate inferences using irrelevant features. The corresponding rate for the object detection models is over 8.5%. Based on the findings, we further designed a new image generation strategy that can effectively attack existing models. Comparing with a baseline approach, our strategy can double the success rate of attacks.", "num_citations": "2\n", "authors": ["267"]}
{"title": "Programming support for autonomizing software\n", "abstract": " Most traditional software systems are not built with the artificial intelligence support (AI) in mind. Among them, some may require human interventions to operate, eg, the manual specification of the parameters in the data processing programs, or otherwise, would behave poorly. We propose a novel framework called Autonomizer to autonomize these systems by installing the AI into the traditional programs. Autonomizeris general so it can be applied to many real-world applications. We provide the primitives and the run-time support, where the primitives abstract common tasks of autonomization and the runtime support realizes them transparently. With the support of Autonomizer, the users can gain the AI support with little engineering efforts. Like many other AI applications, the challenge lies in the feature selection, which we address by proposing multiple automated strategies based on the program analysis. Our\u00a0\u2026", "num_citations": "2\n", "authors": ["267"]}
{"title": "PAD: Programming third-party web advertisement censorship\n", "abstract": " In the current online advertisement delivery, an ad slot on a publisher's website may go through multiple layers of bidding and reselling until the final ad content is delivered. The publishers have little control on the ads being displayed on their web pages. As a result, website visitors may suffer from unwanted ads such as malvertising, intrusive ads, and information disclosure ads. Unfortunately, the visitors often blame the publisher for their unpleasant experience and switch to competitor websites. In this paper, we propose a novel programming support system for ad delivery, called PAD, for publisher programmers, who specify their policies on regulating third-party ads shown on their websites. PAD features an expressive specification language and a novel persistent policy enforcement runtime that can self-install and self-protect throughout the entire ad delegation chain. It also provides an ad-specific memory\u00a0\u2026", "num_citations": "2\n", "authors": ["267"]}
{"title": "Annotation Guided Collection of Context-Sensitive Parallel Execution Profiles\n", "abstract": " Studying the relative behavior of an application\u2019s threads is critical to identifying performance bottlenecks and understanding their root causes. We present context-sensitive parallel (CSP) execution profiles, that capture the relative behavior of threads in terms of the user selected code regions they execute. CSPs can be analyzed to compute execution times spent by the application in interesting behavior states. To capture execution context, code regions of interest can be given static and dynamic names using a versatile set of annotations. The CSP divides the execution time of a multithreaded application into a sequence of time intervals called frames, during which no thread transitions between code regions. By appropriate selection and naming of code regions, the user can obtain a CSP that captures all occurrences of desired behavior states. We provide the user with a powerful query language to facilitate the\u00a0\u2026", "num_citations": "2\n", "authors": ["267"]}
{"title": "Virtual DOM Coverage: Drive an Effective Testing for Dynamic Web Applications\n", "abstract": " Test adequacy criteria are fundamental in software testing. Among them, code coverage criterion is widely used due to its simplicity and effectiveness. However, in dynamic web application testing, merely covering server-side script code is inadequate because it neglects client-side execution, which plays an important role in triggering client-server interactions to reach important execution states. Similarly, a criterion aiming at covering the UI elements on client-side pages ignores the server-side execution, leading to insufficiency. In this paper, we propose Virtual DOM (V-DOM) Coverage, a novel criterion, to drive web application testing. With static analysis, we first aggregate all the DOM objects that may be produced by a piece of server script to construct a V-DOM tree. The tree models execution on both the client-and server-sides such that V-DOM coverage is more effective than existing coverage criteria in web application testing. We conduct an empirical study on five real world dynamic web applications. We find that V-DOM tree can model much more DOM objects than a web crawling based technique. Test selection based on V-DOM tree criterion substantially outperforms the existing code coverage and UI element coverage, by detecting more faults.", "num_citations": "2\n", "authors": ["267"]}
{"title": "Extending path profiling across loop backedges and procedure boundaries\n", "abstract": " Since their introduction, path profiles have been used toguide the application of aggressive code optimizations andperforming instruction scheduling. However, for optimizationand scheduling, it is often desirable to obtain frequencycounts of paths that extend across loop iterations and crossprocedure boundaries. These longer paths, referred to asinteresting paths in this paper, account for over 75% of theflow in a subset of SPEC benchmarks. Although the frequencycounts of interesting paths can be estimated frompath profiles, the degree of imprecision of these estimates isvery high. We extend Ball Larus (BL) paths to create slightlylonger overlapping paths and develop an instrumentationalgorithm to collect their frequencies. While these pathsare slightly longer than BL paths, they enable very preciseestimation of frequencies of potentially much longer interestingpaths. Our experiments show that the average cost\u00a0\u2026", "num_citations": "2\n", "authors": ["267"]}
{"title": "ALchemist: Fusing Application and Audit Logs for Precise Attack Provenance without Instrumentation\n", "abstract": " Cyber-attacks are becoming more persistent and complex. Most state-of-the-art attack forensics techniques either require annotating and instrumenting software applications or rely on high quality execution profiling to serve as the basis for anomaly detection. We propose a novel attack forensics technique ALchemist. It is based on the observations that builtin application logs provide critical high-level semantics and audit logs provide low-level fine-grained information; and the two share a lot of common elements. ALchemist is hence a log fusion technique that couples application logs and audit logs to derive critical attack information invisible in either log. It is based on a relational reasoning engine Datalog and features the capabilities of inferring new relations such as the task structure of execution (eg, tabs in firefox), especially in the presence of complex asynchronous execution models, and high-level dependencies between log events. Our evaluation on 15 popular applications including firefox, Chromium, and OpenOffice, and 14 APT attacks from the literature demonstrates that although ALchemist does not require instrumentation, it is highly effective in partitioning execution to autonomous tasks (in order to avoid bogus dependencies) and deriving precise attack provenance graphs, with very small overhead. It also outperforms NoDoze and OmegaLog, two stateof-the-art techniques that do not require instrumentation.", "num_citations": "1\n", "authors": ["267"]}
{"title": "Redundancy-Free UAV Sensor Fault Isolation And Recovery\n", "abstract": " (UAVs) plays an important role in flight safety. Thus, any sensor fault/failure can have potentially catastrophic effects on vehicle control. The recent advance in adversarial studies demonstrated successful sensing fault generation by targeting the physical vulnerabilities of the sensors. It poses new security challenges for sensor fault detection and isolation (FDI) and fault recovery (FR) research because the conventional redundancy-based faulttolerant design is not effective against such faults. To address these challenges, we present a redundancy-free method for UAV sensor FDI and FR. In the FDI design, we used a basic state estimator for a rough early warning of faults. We then refine the design by considering the unmeasurable actuator state and modeling uncertainties. Under such novel strategies, the proposed method achieves fine-grained fault isolation. Based on this method, we further designed a redundancy-free FR method by using complementary sensor estimations. In particular, position and attitude feedback can provide backup feedback for each other through geometric correlation. The effectiveness of our approach is validated through simulation of several challenging sensor failure scenarios. The recovery performance is experimentally demonstrated by a challenging flight tasks-restoring control after completely losing attitude sensory feedback. With the protection of FDI and FR, flight safety is ensured. This UAV security enhancement method is promising to be generalized for other types of vehicles and can serve as a compensation to other fault-tolerant methodologies.I. I Cyber-Physical Systems (CPSs) hold a great promise in\u00a0\u2026", "num_citations": "1\n", "authors": ["267"]}
{"title": "Gemini: Guest-transparent honey files via hypervisor-level access redirection\n", "abstract": " Data safety has become a critical problem in the face of various cyber-attacks aiming at stealing or divulging sensitive information. In the event that adversaries have gained access to a system storing classified data, such crucial systems should actively protect the integrity of this data. To purposely deceive an attacker, we propose that accesses to sensitive data can be dynamically partitioned to prevent malicious tampering. In this paper, we present Gemini, a virtualization-based system to transparently redirect accesses to classified files based on the context of the access (e.g., process, user, time-of-day, etc.). If an access violates preconfigured data-use policies then it will be rerouted to a honey version of the file, specifically crafted to be manipulated by the adversary. Thus, Gemini transforms static, sensitive files into moving targets and provides strong transparency and tamper-resistance as it is located at the\u00a0\u2026", "num_citations": "1\n", "authors": ["267"]}
{"title": "Graph-based signatures for kernel data structures\n", "abstract": " The signature of a data structure reflects some unique properties of the data structure and therefore can be used to identify instances of the data structure in a memory image. Such signatures are important to many computer forensics applications. Existing approaches propose the use of value invariants of certain fields as data structure signatures. However, they do not fully exploit pointer fields as pointers are more dynamic in their value range. In this paper, we show that pointers and the topological properties induced by the points-to relations between data structures can be used as signatures. To demonstrate the idea, we develop SigGraph, a system that automatically extracts points-to relations from kernel data structure definitions and generates unique graph-based signatures for the data structures. These signatures are further refined by dynamic profiling to improve efficiency and robustness. Our experimental results show that the graph-based signatures achieve high accuracy in kernel data structure recognition, with zero false negative and close-to-zero false positives. We further show that SigGraph achieves strong robustness in the presence of malicious pointer manipulations.", "num_citations": "1\n", "authors": ["267"]}
{"title": "[Microbial reduction and mobilization of adsorbed arsenate on ferric/aluminum hydroxides].\n", "abstract": " Although the mechanisms of arsenic release into groundwater remain poorly characterized, microbial reduction of As (V) adsorbed on the surface of iron oxides and the reductive dissolution of iron oxides are generally considered to play a key role in the mobilization of arsenic. We investigated the impact of bacterial reduction of adsorbed As (V) on a Al: Fe (1: 0, 1: 1, 0: 1) hydroxides on arsenic mobilization using the mixed bacterial culture. After inoculation, the increase of dissolved As (III) concentration was observed, whereas As (V) was negligible in aqueous phase. Arenic release for the Al: Fe (1: 0, 1: 1, 0: 1) hydroxides systems was 60 microg/L, 1.3 mg/L and 7.8 mg/L respectively. On the contrary, neither reduction nor release of arsenic was observed in the uninoculated groups. Furthermore, the introduction of aluminium may be responsible for the release of arsenic owing to its weaker affinity to As (III). In addition, our results showed that Fe reduction occurred far later than arsenic reduction and mobilization and obvious increase was not observed even after Fe reduction occurred. It suggested that in natural systems, the biotic reduction of As (V) adsorbed on ferric oxides or Fe (III) may not the major cause of arsenic release in sediment or groundwater system as previous works proposed. The reduction of As (V) bound to aluminum oxides or other minerals may play a key role.", "num_citations": "1\n", "authors": ["267"]}
{"title": "[Relationship between three neuropsychological tests and cerebral glucose metabolism in Alzheimer's disease].\n", "abstract": " OBJECTIVE: To study the relationship between the scores of 3 neuropsychological scales: MMSE, extended scale of dementia (ESD) and Blessed dementia scale (BDS) and cerebral glucose metabolism in Alzheimer's disease (AD) indicated by positron emission tomography (PET). METHODS: Brain scanning was performed with SIEMENS ECAT 47 PET scanner among 21 AD patients, aged 60-83 (10 mild, 6 moderate, and 5 severe), and 15 healthy persons, aged 57-73. The ratio of mean radioactivity of cerebral lobe to that of cerebellum as semi-quantitative parameters was used to evaluate the cerebral glucose metabolism. Cognitive function was assessed by three neuropsycholigical scales: MMSE, ESD and BDS. RESULTS: The average MMSE score was 13.3+/-6.3 (range 2-25), the average ESD score was 120.5+/-54.8 (range 28-200), and the average BDS score was 4.7+/-3.1 (range 0.5-10) in the AD patients. The decreases of glucose metabolism in parietal, frontal and temporal lobes were significantly positively correlated with MMSE and ESD scores, and negatively correlated with BDS scores in AD patients (P< 0.05). The correlations between MMSE and ESD (r= 0.886), MMSE and BDS (r=-0.763), and ESD and BDS (r=-0.773) were significant in AD patients (all P< 0.01). Taking the ratio of radioactivity of cerebral lobe to that of cerebellum as an independent variable X, the MMSE, ESD and BDS scorsas dependent variable Y, three regression equations were established as follows: Y= 40.11* X-25.32, Y= 309.19* X-180.9, Y= 19.97-16.53* X. The independent variable entering the three regression equations was always the ratio of\u00a0\u2026", "num_citations": "1\n", "authors": ["267"]}
{"title": "Structure and Properties of poly (para phynelyne benzobisoxazole)(PBO)/single wall carbon nano tube composite fibers\n", "abstract": " The liquid crystalline compositions are prepared by the in-situ polycondensation of diamines and diacid monomers in the presence of single wall carbon nano tubes (SWNT). Processing of the new compositions into fibers provide hybrid materials with improved mechanical properties. The in-situ polymerizations were carried out in polyphosphoric acid (PPA). Carbon nano tubes as high as 10 wt. polymer weight have been utilized. Fiber spinning has been carried out using dry jet wet spinning using a piston driven spinning system and the fiber coagulated in water and subsequently vacuum dried and heat treated in nitrogen at 400oC. Structure and properties of these fibers have been studied. Tensile strength of the composite fibers increased by about 50morphology of these fibers have been studied using X-ray diffraction and scanning electron microscopy.", "num_citations": "1\n", "authors": ["267"]}
{"title": "A coincidence\u2010detection model for level discrimination of tones in roving\u2010level noise\n", "abstract": " This study describes a monaural cross\u2010frequency coincidence\u2010detection mechanism to model level discrimination performance for a tone in the presence of noise in a roving\u2010level paradigm. Model coincidence detectors receive population input from model auditory\u2010nerve (AN) fibers from the same ear with different characteristic frequencies (CFs). The response of each model coincidence detector is sensitive to both the rate and the phase of its AN fiber inputs. A set of model coincidence detectors was constructed that received input from a population of model AN fibers. To detect intensity increments of tones in wideband noise, the most sensitive coincidence detector was one that received the inputs from two AN fibers with tone responses that were opposite in phase (phase opponency); this mechanism was robust in the presence of roving\u2010level maskers. For narrow\u2010band noise, detectors that were sensitive to\u00a0\u2026", "num_citations": "1\n", "authors": ["267"]}
{"title": "Synthesis of star-high vinyl polybutadiene\n", "abstract": " On the basis of patent of multilithium initiator, star-high vinyl polybutadiene (S-HVBR) was synthesized by one-step addition process, with cyclohexane as solvent and tet rahydrofuran, diethylene glycol dimethyl ether and ethylene glycol dimethyl ether as additives. The results showed that when S-HVBR was synthesized with multilithium initiator, the content of 1, 2-structure could be cont rolled between 70%~ 80%, the molecular weight distribution was more wide and the species and the content of microst ructure modifying additives as well as the temperature all had a great influence on the microst ructure of S-HVBR.", "num_citations": "1\n", "authors": ["267"]}
{"title": "[Risk factors for Guillain-Barre syndrome in northern China: a case-control study]\n", "abstract": " We conducted a case-control study in northern China to estimate possible risk factors for Guillain-Barre Syndrome (GBS). Forty patients were consecutively seen at the PUMC hospital and the Second Hospital of Hebei Medical College between July and September 1991. The diagnosis was established following the NINCDS criteria for GBS. Among 36 patients with measurements of motor evoked potentials, 34 had evidences of demyelination. Eighty controls chosen from spouses or siblings, and neighbors or work/school mates, were matched by sex and age (+/-3 years). Using the Mantel-Haenszel estimate of the odds ratio, cold rain, overloaded activities, a history of diarrhea, common cold, and exposure to organophosphorus one month before onset, significantly increased, at least six-fold, the risk for development of GBS. Cases and controls did not differ in the number of previous vaccinations. We suggest that a single antigent is less likely of etiological importance in GBS.", "num_citations": "1\n", "authors": ["267"]}
{"title": "[The diagnosis and treatment in 39 cases of sarcoidosis]\n", "abstract": " The diagnosis, treatment and follow-up in 39 cases of sarcoidosis were reported. The authors believed that (1) diagnosis of this disease would be made by analysing clinical and pathological manifestations carefully,(2) treatment was given when the disease was active, and (3) in order to find complications as early as possible, follow-up was very important during treatment.", "num_citations": "1\n", "authors": ["267"]}