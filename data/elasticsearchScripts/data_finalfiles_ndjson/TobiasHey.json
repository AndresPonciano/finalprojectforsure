{"title": "Detection of Conditionals in Spoken Utterances\n", "abstract": " State-of-the-art intelligent assistant systems such as Siri & Co. struggle with conditionals. They reliably react to ordinary commands. However, their architectures are not designed to cope with complex conditional queries. We propose a system to overcome these limitations. Our approach models if-then-else constructs in spoken utterances explicitly. The model bridges the gap between linguistic and programmatic semantics. To proof our concept, we apply a rule-based approach to extract conditionals. For our prototype we use part-of-speech and chunk tags provided by NLP tools. We make use of coreference information to determine the reference frame of a condition. The explicit modeling of conditionals allows us to evaluate the accuracy of our approach independently from other language understanding tasks. The prototype works well in the domain of humanoid robotics. In a user study we achieve F1 scores of 0\u00a0\u2026", "num_citations": "9\n", "authors": ["2263"]}
{"title": "INDIRECT: Intent-Driven Requirements-to-Code Traceability\n", "abstract": " Traceability information is important for software maintenance, change impact analysis, software reusability, and other software engineering tasks. However, manually generating this information is costly. State-of-the-art automation approaches suffer from their imprecision and domain dependence. I propose INDIRECT, an intent-driven approach to automated requirements-to-code traceability. It combines natural language understanding and program analysis to generate intent models for both requirements and source code. Then INDIRECT learns a mapping between the two intent models. I expect that using the two intent models as base for the mapping poses a more precise and general approach. The intent models contain information such as the semantics of the statements, underlying concepts, and relations between them. The generation of the requirements intent model is divided into smaller subtasks by\u00a0\u2026", "num_citations": "8\n", "authors": ["2263"]}
{"title": "Detection of Control Structures in Spoken Utterances\n", "abstract": " State-of-the-art intelligent assistant systems such as Siri and Cortana do not consider control structures in the user input. They reliably react to ordinary commands. However, their architectures are not designed to cope with queries that require complex control flow structuring. We propose a system to overcome these limitations. Our approach models if-then-else, loop, and concurrency constructs in spoken utterances explicitly. The model bridges the gap between linguistic and programmatic semantics. To demonstrate our concept, we apply a rule-based approach. We have implemented three prototypes that use keyphrases to discover potential control structures depending on the type of control structure. However, the full structures are determined differently. For conditionals we use chunk and part-of-speech (POS) tags provided by natural language processing tools; for loops and concurrency we make use of an\u00a0\u2026", "num_citations": "6\n", "authors": ["2263"]}
{"title": "Integrating a Dialog Component into a Framework for Spoken Language Understanding\n", "abstract": " Spoken language interfaces are the latest trend in human computer interaction. Users enjoy the newly found freedom but developers face an unfamiliar and daunting task. Creating reactive spoken language interfaces requires skills in natural language processing. We show how a developer can integrate a dialog component in a natural language processing system by means of software engineering methods. Our research project PARSE that aims at naturalistic end-user programming in spoken natural language serves as an example. We integrate a dialog component with PARSE without affecting its other components: We modularize the dialog management and introduce dialog acts that bundle a trigger for the dialog and the reaction of the system. We implemented three dialog acts to address the following issues: speech recognition uncertainties, coreference ambiguities, and incomplete conditionals. We\u00a0\u2026", "num_citations": "5\n", "authors": ["2263"]}