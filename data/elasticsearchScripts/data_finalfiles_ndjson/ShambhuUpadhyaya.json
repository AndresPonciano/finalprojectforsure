{"title": "Coordination in emergency response management\n", "abstract": " Developing a framework to analyze coordination patterns occurring in the emergency response life cycle.", "num_citations": "410\n", "authors": ["513"]}
{"title": "Factors influencing online health information search: An empirical analysis of a national cancer-related survey\n", "abstract": " People are increasingly using the Internet to access health information and the information obtained has an impact on their healthcare outcomes. This paper examines the impacts of IT enablers and health motivators on peoples' online health information search behavior. We characterize users' online health information search behavior along three dimensions: the frequency of online health information search, the diversity of online health information usage, and the preference of the Internet for initial search. Using the 2003 Health Information National Trends Survey (HINTS) data on cancer, we find that ease of access to Internet services and trust in online health information could affect the three dimensional search behavior listed above. While perceived quality of communication with doctors has an impact on diversity of use and preference of use, we surprisingly do not find an impact on the frequency of search for\u00a0\u2026", "num_citations": "317\n", "authors": ["513"]}
{"title": "Phishing email detection based on structural properties\n", "abstract": " Phishing attacks pose a serious threat to end-users and commercial institutions alike. Majority of the present day phishing attacks employ e-mail as their primary carrier, in order to allure unsuspecting victims to visit the masqueraded website. While the recent defense mechanisms focus on detection by validating the authenticity of the website, very few approaches have been proposed which concentrate on detecting e-mail based phishing attacks based on the structural properties inherently present in the phishing e-mail. Also, phishing attacks growing in ingenuity as well as sophistication render most of existing browser based solutions weak. In this paper, we propose a novel technique to discriminate phishing e-mails from the legitimate e-mails using the distinct structural features present in them. The derived features, together with oneclass Support Vector Machine (SVM), can be used to efficiently classify phishing\u00a0\u2026", "num_citations": "257\n", "authors": ["513"]}
{"title": "Linear circuit fault diagnosis using neuromorphic analyzers\n", "abstract": " This paper presents a method of analog fault diagnosis using neural networks. The primary focus of the paper is to provide robust diagnosis using a simple mechanism for automatic test pattern generation while reducing test time. A new diagnosis framework consisting of a white noise generator and an artificial neural network for response analysis and classification is proposed. This approach moves the diagnosis of analog circuits closer to the goal of built-in test. Networks of reasonable dimension are shown to be capable of robust diagnosis of analog circuits including effects due to tolerances.", "num_citations": "250\n", "authors": ["513"]}
{"title": "Is rssi a reliable parameter in sensor localization algorithms: An experimental study\n", "abstract": " Wireless sensor networks are becoming ubiquitous and their application areas are widening by the day. Localization algorithms play an important role in enhancing the utility of data collected by enabling sensors to determine the location from which each data packet is obtained. Localization can be done by implementing beacon based algorithms or signature based algorithms. Much of the research work in this area assumes received signal strength indicator (RSSI) as a parameter in their localization algorithms. Since RSSI is the key parameter, we conducted practical experiments to assess whether RSSI could indeed be used by localization algorithms to determine distances between sensors. In our experiments, we tried to calibrate and map RSSI to distance under various conditions and concluded that despite promising hypothetical advantages of RSSI, even under ideal conditions it cannot be used to determine inter nodal distances in wireless sensor networks.", "num_citations": "203\n", "authors": ["513"]}
{"title": "A data-centric approach to insider attack detection in database systems\n", "abstract": " The insider threat against database management systems is a dangerous security problem. Authorized users may abuse legitimate privileges to masquerade as other users or to maliciously harvest data. We propose a new direction to address this problem. We model users\u2019 access patterns by profiling the data points that users access, in contrast to analyzing the query expressions in prior approaches. Our data-centric approach is based on the key observation that query syntax alone is a poor discriminator of user intent, which is much better rendered by what is accessed. We present a feature-extraction method to model users\u2019 access patterns. Statistical learning algorithms are trained and tested using data from a real Graduate Admission database. Experimental results indicate that the technique is very effective, accurate, and is promising in complementing existing database security solutions. Practical\u00a0\u2026", "num_citations": "202\n", "authors": ["513"]}
{"title": "Towards a theory of insider threat assessment\n", "abstract": " Insider attacks are a well-known problem acknowledged as a threat as early as 1980s. The threat is attributed to legitimate users who abuse their privileges, and given their familiarity and proximity to the computational environment, can easily cause significant damage or losses. Due to the lack of tools and techniques, security analysts do not correctly perceive the threat, and hence consider the attacks as unpreventable. In this paper, we present a theory of insider threat assessment. First, we describe a modeling methodology which captures several aspects of insider threat, and subsequently, show threat assessment methodologies to reveal possible attack strategies of an insider.", "num_citations": "198\n", "authors": ["513"]}
{"title": "Quantifying trust in mobile ad-hoc networks\n", "abstract": " This paper introduces a trust-domain based security architecture for mobile ad-hoc networks (MANETs). The aim of this architecture is twofold: to use trust as a basis to establish keys between nodes in a MANET, and to utilize trust as a metric for establishing secure distributed control in infrastructure-less MANETs. We define metrics for nodes to establish and manage trust, and use this mutual trust to make decisions on establishing group and pair-wise keys in the network. The impact of mobility of the nodes on trust establishment is considered and further its use as a means of propagating trust through the network is investigated. We introduce the concept of self-organizing trust-based physical-logical domains (PLDs) as a means of grouping nodes for distributed control in the network.", "num_citations": "152\n", "authors": ["513"]}
{"title": "Flow in games\n", "abstract": " Describes the motivation of DDA. Brings up good questions about the challenge in the context of the type of player. Think about categorization of the challenge?(ie if we do a quantitative measure of difficulty, can we also analyze the nature of the difficulty? So \u201cdifficulty rating of x, appeals to gamer y\u201d.", "num_citations": "150\n", "authors": ["513"]}
{"title": "Online shopping intention in the context of data breach in online retail stores: An examination of older and younger adults\n", "abstract": " Data breaches through hacking incidents have become a significant phenomenon in the world of online shopping. These breaches can result in loss of personal data belonging to customers. This study builds a research model to examine people's intention to engage in e-commerce in the context of a significant data breach (the Target breach in December 2013). In addition, this paper focuses on the difference in responses regarding post-breach online shopping intent among younger adults (below 55\u00a0years) and older adults (senior citizens\u2014above 55\u00a0years). Our findings show the importance of internal (self) monitoring of bank transactions in reducing the effect of perceptions of severity of data breaches on post-breach online shopping intent particularly for senior citizens. The study also demonstrates that perceptions of severity of a hacking incident are significant drivers of perceived online shopping risk for both\u00a0\u2026", "num_citations": "139\n", "authors": ["513"]}
{"title": "Internet and online information privacy: An exploratory study of preteens and early teens\n", "abstract": " Information security and privacy on the internet are critical issues in our society. In this research, we examine factors that influence Internet users' private-information-sharing behavior. Based on a survey of 285 preteens and early teens, who are among the most vulnerable groups on the Web, this study provides a research framework that explains an internet user's information privacy protection behavior. According to our study results, internet users' information privacy behaviors are affected by two significant factors: (1) users' perceived importance of information privacy and (2) information privacy self-efficacy. The study also found that users believe in the value of online information privacy and that information privacy protection behavior varies by gender. Our findings indicate that educational opportunities regarding internet privacy and computer security as well as concerns from other reference groups (e.g., peer\u00a0\u2026", "num_citations": "133\n", "authors": ["513"]}
{"title": "On programmable memory built-in self test architectures\n", "abstract": " The design and architectures of a microcode-based memory BIST and programmable FSM-based memory BIST unit are presented. The proposed microcode-based memory BIST unit is more efficient and flexible than existing architectures. Test logic overhead of the proposed programmable versus non-programmable memory BIST architectures is evaluated. The proposed programmable memory BIST architectures could be used to test memories in different stages of their fabrication and therefore result in lower overall memory test logic overhead. We show that the proposed microcode-based memory BIST architecture has better extendibility and flexibility while having less test logic overhead than the programmable FSM-based memory BIST architecture.", "num_citations": "120\n", "authors": ["513"]}
{"title": "Data model development for fire related extreme events: An activity theory approach\n", "abstract": " Post-analyses of major extreme events reveal that information sharing is critical for effective emergency response. The lack of consistent data standards for current emergency management practice, however, hinders efficient critical information flow among incident responders. In this paper, we adopt a third-generation activity theory guided approach to develop a data model that can be used in the response to fire-related extreme events. This data model prescribes the core data standards to reduce information interoperability barriers. The model is validated through a three-step approach including a request for comment (RFC) process, case application, and prototype system test. This study contributes to the literature in the area of interoperability and data modeling; it also informs practice in emergency response system design.", "num_citations": "119\n", "authors": ["513"]}
{"title": "Efficiency of critical incident management systems: Instrument development and validation\n", "abstract": " There is much literature in the area of emergency response management systems. Even so, there is in general a lacuna of literature that deals with the issue of measuring the effectiveness of such systems. The aim of this study is to develop and validate an instrument to measure the critical factors that contribute to the efficiency of decision support in critical incident management systems (CIMS). The instrument presented in this paper has been developed using a CIMS efficiency model that is based on an adaptation of media richness theory, aspects of the national incident management system (NIMS) and interviews with experts on emergency management. The instrument has been validated through a pretest, followed by a pilot test and, finally, a main field test which includes a survey of 76 experts. The final instrument consists of 28 statistically relevant question items, which form eight constructs. The instrument\u00a0\u2026", "num_citations": "110\n", "authors": ["513"]}
{"title": "Emergency response information system interoperability: Development of chemical incident response data model\n", "abstract": " Emergency response requires an efficient information supply chain for the smooth operations of intra-and inter-organizational emergency management processes. However, the breakdown of this information supply chain due to the lack of consistent data standards presents a significant problem. In this paper, we adopt a theory-driven novel approach to develop an XML-based data model that prescribes a comprehensive set of data standards (semantics and internal structures) for emergency management to better address the challenges of information interoperability. Actual documents currently being used in mitigating chemical emergencies from a large number of incidents are used in the analysis stage. The data model development is guided by Activity Theory and is validated through a RFC-like process used in standards development. This paper applies the standards to the real case of a chemical incident\u00a0\u2026", "num_citations": "101\n", "authors": ["513"]}
{"title": "Phoney: Mimicking user response to detect phishing attacks\n", "abstract": " Phishing scams pose a serious threat to end-users and commercial institutions alike. Email continues to be the favorite vehicle to perpetrate such scams mainly due to its widespread use combined with the ability to easily spoof them. Several approaches, both generic and specialized, have been proposed to address this problem. However, phishing techniques, growing in ingenuity as well as sophistication, render these solutions weak. In this paper we propose a novel approach to detect phishing attacks using fake responses which mimic real users, essentially, reversing the role of the victim and the adversary. Our prototype implementation called PHONEY, sits between a user's mail transfer agent (MTA) and mail user agent (MUA) and processes each arriving email for phishing attacks. Using live email data collected over a period of eight months we demonstrate data that our approach is able to detect a wider\u00a0\u2026", "num_citations": "99\n", "authors": ["513"]}
{"title": "Security in grid computing: A review and synthesis\n", "abstract": " This paper provides an extensive survey of the different methods of addressing security issues in the grid computing environment, and specifically contributes to the research environment by developing a comprehensive framework for classification of these research endeavors. The framework presented classifies security literature into System Solutions, Behavioral Solutions, Hybrid Solutions and Related Technologies. Each one of these categories is explained in detail in the paper to provide insight as to their unique methods of accomplishing grid security, the types of grid and security situations they apply best to, and the pros and cons for each type of solution. Further, several areas of research were identified in the course of the literature survey where more study is warranted. These avenues for future research are also discussed in this paper. Several types of grid systems exist currently, and the security needs\u00a0\u2026", "num_citations": "93\n", "authors": ["513"]}
{"title": "Design principles for critical incident response systems\n", "abstract": " The national incident management system (NIMS) was developed so that responders from different jurisdictions and disciplines could work together to respond to natural and manmade disasters and emergencies, including acts of terrorism. The NIMS document provides a set of guidelines about practices but it does not make explicit the design requirements for information systems to support the management of critical incidents. Though there are academic and practitioner papers in the general area of emergency management, there is a lacuna of literature discussing how to design information systems to support critical incident response. In this paper we develop a set of design principles that are grounded in emergency management concepts and also in the insights from the real response managers in the Western New York area. The set of design principles provide a foundation for the development of\u00a0\u2026", "num_citations": "89\n", "authors": ["513"]}
{"title": "Secure distance-based localization in the presence of cheating beacon nodes\n", "abstract": " Secure distance-based localization in the presence of cheating beacon (or anchor) nodes is an important problem in mobile wireless ad hoc and sensor networks. Despite significant research efforts in this direction, some fundamental questions still remain unaddressed: In the presence of cheating beacon nodes, what are the necessary and sufficient conditions to guarantee a bounded error during a two-dimensional distance-based location estimation? Under these necessary and sufficient conditions, what class of localization algorithms can provide this error bound? In this paper, we attempt to answer these and other related questions by following a careful analytical approach. Specifically, we first show that when the number of cheating beacon nodes is greater than or equal to a given threshold, there do not exist any two-dimensional distance-based localization algorithms that can guarantee a bounded error\u00a0\u2026", "num_citations": "74\n", "authors": ["513"]}
{"title": "Towards a theory of robust localization against malicious beacon nodes\n", "abstract": " Localization in the presence of malicious beacon nodes is an important problem in wireless networks. Although significant progress has been made on this problem, some fundamental theoretical questions still remain unanswered: in the presence of malicious beacon nodes, what are the necessary and sufficient conditions to guarantee a bounded error during 2-dimensional location estimation? Under these necessary and sufficient conditions, what class of localization algorithms can provide that error bound? In this paper, we try to answer these questions. Specifically, we show that, when the number of malicious beacons is greater than or equal to some threshold, there is no localization algorithm that can have a bounded error. Furthermore, when the number of malicious beacons is below that threshold, we identify a class of localization algorithms that can ensure that the localization error is bounded. We also\u00a0\u2026", "num_citations": "74\n", "authors": ["513"]}
{"title": "Secure and fault-tolerant voting in distributed systems\n", "abstract": " Concerns about both security and fault-tolerance have had an important impact on the design and use of distributed information systems in the past. As such systems become more prevalent, as well as more pervasive, these concerns will become even more immediately relevant. We focus on integrating security and fault-tolerance into one, general-purpose protocol for secure distributed voting. Distributed voting is a well-known fault-tolerance technique. For the most part, however, security had not been a concern in systems that used voting. More recently, several protocols have been proposed to shore up this lack. These protocols, however, have limitations which make them particularly unsuitable for many aerospace applications, because those applications require very flexible voting schemes (e.g., voting among real-world sensor data). We present a new, more general voting protocol that reduces the\u00a0\u2026", "num_citations": "72\n", "authors": ["513"]}
{"title": "Profiling users in GUI based systems for masquerade detection\n", "abstract": " Masquerading or impersonation attack refers to the illegitimate activity on a computer system when one user impersonates another user. Masquerade attacks are serious in nature due to the fact that they are mostly carried by insiders and thus are extremely difficult to detect. Detection of these attacks is done by monitoring significant changes in user's behavior based on his/her profile. Currently, such profiles are based mostly on the user command line data and do not represent his/her complete behavior in a graphical user interface (GUI) based system and hence are not sufficient to quickly detect such masquerade attacks. In this paper, we present a new framework for creating a unique feature set for user behavior on GUI based systems. We have collected real user behavior data from live systems and extracted parameters to construct these feature vectors. These vectors contain user information such as mouse\u00a0\u2026", "num_citations": "67\n", "authors": ["513"]}
{"title": "A comprehensive reconfiguration scheme for fault-tolerant VLSI/WSI array processors\n", "abstract": " This paper presents an effective reconfiguration scheme consisting of detailed spare replacement, processor placement, routing, and switch programming mechanisms. A new switch programming scheme is proposed to reduce the hardware overhead of reconfiguration. A thorough yield simulation tool has been developed for accurate prediction of yield by considering the effects of defect clusters and switching network failures. This yield simulation tool can also be used to obtain the information on the performance degradation, spare replacement, processor placement, routing and the switch programming algorithm survival probability.", "num_citations": "66\n", "authors": ["513"]}
{"title": "Trust based routing decisions in mobile ad-hoc networks\n", "abstract": " This paper proposes a trust-based framework for improving the security and robustness of ad-hoc network routing protocols. We choose the popular and widely used Ad-hoc On-demand Distance Vector (AODV) Routing protocol as a representative candidate for constructing our trust framework. The goal is to make minimal changes to the functioning of AODV and obtain an increased level of security and reliability. Our schemes are based on incentives & penalties depending on the behavior of network nodes. They allow source nodes to choose more trusted paths rather than just shorter paths during route discovery in adhoc networks and isolate any malicious nodes from the network. Our schemes incur minimal additional overhead and preserve the lightweight nature of AODV. We illustrate the adaptability of our schemes by extending them to AODV\u2019s multi-path variants. Slight modifications will make our schemes applicable to other routing protocols like DSR and ZRP.", "num_citations": "61\n", "authors": ["513"]}
{"title": "Design principles of coordinated multi-incident emergency response systems\n", "abstract": " Emergency response systems play an important role in homeland security nowadays. Despite this, research in the design of emergency response systems is lacking. An effective design of emergency response system involves multi-disciplinary design considerations. On the basis of emergency response system requirement analysis, in this paper, we develop a set of supporting design concepts and strategic principles for an architecture for a coordinated multi-incident emergency response system", "num_citations": "56\n", "authors": ["513"]}
{"title": "Security policies to mitigate insider threat in the document control domain\n", "abstract": " With rapid advances in online technologies, organizations are migrating from paper based resources to digital documents to achieve high responsiveness and ease of management. These digital documents are the most important asset of an organization and are hence the chief target of insider abuse. Security policies provide the first step to prevent abuse by defining proper and improper usage of resources. Coarse grained security policies that operate on the \"principle of least privilege\" [J. H. Saltzer et al., (1974)] alone are not enough to address the insider threat, since the typical insider possesses a wide range of privileges to start with. In this paper, we propose a security policy that is tailored to prevent insider abuse. We define the concept of subject, object, actions, rights, context and information flow as applicable to the document control domain. Access is allowed based on the principles of \"least privilege and\u00a0\u2026", "num_citations": "53\n", "authors": ["513"]}
{"title": "Secure knowledge management and the semantic web\n", "abstract": " Strengthening security within the domain of shared knowledge is a critical issue, and great challenge, to businesses today. A number of different protocols currently available offer an array of benefits and limitations.", "num_citations": "52\n", "authors": ["513"]}
{"title": "Women in cybersecurity: A study of career advancement\n", "abstract": " Although cybersec\u00bfrity is a critical IT area, women continue to be underrepresented among its ranks. This first study of female cybersecurity professionals examines the required skills, existing challenges, and key success factors for women in the field.As our results show, addressing the needs of women at the beginning of their careers-starting at educational institutions-is crucial to their successful entry and success in the field.Currently, the US government has responded to this issue by providing grants to attract and train women in cybersecurity at the university level.", "num_citations": "50\n", "authors": ["513"]}
{"title": "Dynamically partitioned test scheduling with adaptive TAM configuration for power-constrained SoC testing\n", "abstract": " Given a system-on-chip with a set of cores and a set of test resources, and the constraints on the total power consumption during test and the maximum width on the top-level test access mechanism (TAM), it is required to optimize overall testing time of the system. To solve this problem, we first generate a power-constrained test compatibility graph and then construct a set of power-constrained concurrent test sets (PCTSs) to facilitate concurrent testing. We then handle the constrained scheduling by adaptively assigning the cores in parallel to the TAMs with variable width and efficiently utilizing the TAM bandwidth such that the tests in the same PCTS have their lengths close to each other. We concurrently schedule the test sets by dynamically partitioning and allocating the tests, and consequently constructing and updating a set of dynamically partitioned PCTSs. This reduces the test cost in terms of overall test time\u00a0\u2026", "num_citations": "50\n", "authors": ["513"]}
{"title": "Understanding multistage attacks by attack-track based visualization of heterogeneous event streams\n", "abstract": " In this paper, we present a method of handling the visualization of hetereogeneous event traffic that is generated by intrusion detection sensors, log files and other event sources on a computer network from the point of view of detecting multistage attack paths that are of importance. We perform aggregation and correlation of these events based on their semantic content to generate Attack Tracks that are displayed to the analyst in real-time. Our tool, called the Event Correlation for Cyber-Attack Recognition System (EC-CARS) enables the analyst to distinguish and separate an evolving multistage attack from the thousands of events generated on a network. We focus here on presenting the environment and framework for multistage attack detection using ECCARS along with screenshots that demonstrate its capabilities.", "num_citations": "46\n", "authors": ["513"]}
{"title": "An alert fusion framework for situation awareness of coordinated multistage attacks\n", "abstract": " Recent incidents in the cyber world strongly suggest that coordinated multistage cyber attacks are quite feasible and that effective countermeasures need to be developed. Attack detection by correlation and fusion of intrusion alerts has been an active area of current research. However, most of these research efforts focus on ex post facto analysis of alert data to uncover related attacks. In this paper, we present an approach for dynamically calculating 'scenario credibilities' based on the state of a live intrusion alert stream. We also develop a framework for attack scenario representation that facilitates real-time fusion of intrusion alerts and calculation of the scenario credibility values. Our approach provides a usable mechanism for detecting, predicting and reasoning about multistage goal-oriented attacks in real time. The details of the fusion framework and a description of multistage attack detection using this\u00a0\u2026", "num_citations": "44\n", "authors": ["513"]}
{"title": "Assessing roles of people, technology and structure in emergency management systems: A public sector perspective\n", "abstract": " Emergency management systems are a critical factor in successful mitigation of natural and man-made disasters, facilitating responder decision making in complex situations. Based on socio-technical systems, have which four components (people, technology, structure and task), this study develops a research framework of factors affecting effective emergency management. People factors include psychological factors such as responders' self-efficacy, support from family, peers and community, and training. Technology factors are task technology and information sharing. The structure factors are leadership, labour and logistics. Finally, the task factor refers to effective emergency management. This study empirically tests this framework by collecting surveys from emergency responders who participated in the 2006 Buffalo October Storm. The research results demonstrate that training and support positively affect\u00a0\u2026", "num_citations": "43\n", "authors": ["513"]}
{"title": "Data mining for intrusion detection: techniques, applications and systems\n", "abstract": " An intrusion is defined as any set of actions that compromise the integrity, confidentiality or availability of a resource. Intrusion detection is an important task for information infrastructure security. One major challenge in intrusion detection is that we have to identify the camouflaged intrusions from a huge amount of normal communication activities. Data mining is to identify valid, novel, potentially useful, and ultimately understandable patterns in massive data. It is demanding to apply data mining techniques to detect various intrusions.In the last several years, some exciting and important advances have been made in intrusion detection using data mining techniques. Research results have been published and some prototype systems have been established. Inspired by the huge demands from applications, the interactions and collaborations between the communities of security and data mining have been boosted substantially. This seminar will present an interdisciplinary survey of data mining techniques for intrusion detection so that the researchers from computer security and data mining communities can share the experiences and learn from each other. Some data mining based intrusion detection systems will also be reviewed briefly. Moreover, research challenges and problems will be discussed so that future collaborations may be stimulated. For data mining/database researchers and practitioners, the seminar will provide background knowledge and opportunities for applying data mining techniques to intrusion detection and computer security. For computer security researchers and practitioners, it provides knowledge on how data mining\u00a0\u2026", "num_citations": "43\n", "authors": ["513"]}
{"title": "Concurrent process monitoring with no reference signatures\n", "abstract": " A simple, inexpensive and time/space efficient signature technique for process monitoring is presented. In this technique, a known signature function is applied to the instruction stream at compilation phase and when the accumulated signature forms an m-out-of-n code, the corresponding instructions are tagged. Error checking is done at run-time by monitoring the signatures accumulated at the tagged locations to determine whether they form m-out-of-n codes. This approach of signature checking does not require the embedding of reference signatures at compilation, thereby leading to savings in memory as well as in execution time. The m-out-of-n code approach offers high error coverage and controllable latency. The results of the experiments conducted to verify the controllability of the latency are discussed. One of the distinguishing features of the proposed scheme is the elimination of reference signatures\u00a0\u2026", "num_citations": "43\n", "authors": ["513"]}
{"title": "The landscape of domain name typosquatting: Techniques and countermeasures\n", "abstract": " With more than 294 million registered domain names as of late 2015, the domain name ecosystem has evolved to become a cornerstone for the operation of the Internet. Domain names today serve everyone, from individuals for their online presence to big brands for their business operations. Such ecosystem that facilitated legitimate business and personal uses has also fostered \"creative\" cases of misuse, including phishing, spam, hit and traffic stealing, online scams, among others. As a first step towards this misuse, the registration of a legitimately-looking domain is often required. For that, domain typosquatting provides a great avenue to cybercriminals to conduct their crimes. In this paper, we review the landscape of domain name typosquatting, highlighting models and advanced techniques for typosquatted domain names generation, models for their monetization, and the existing literature on countermeasures\u00a0\u2026", "num_citations": "42\n", "authors": ["513"]}
{"title": "RACOON: rapidly generating user command data for anomaly detection from customizable template\n", "abstract": " One of the biggest obstacles faced by user command based anomaly detection techniques is the paucity of data. Gathering command data is a slow process often spanning months or years. In this paper, we propose an approach for data generation based on customizable templates, where each template represents a particular user profile. These templates can either be user-defined or created from known data sets. We have developed an automated tool called RACOON, which rapidly generates large amounts of user command data from a given template. We demonstrate that our technique can produce realistic data by showing that it passes several statistical similarity tests with real data. Our approach offers significant advantages over passive data collection in terms of being nonintrusive and enabling rapid generation of site-specific data. Finally, we report the benchmark results of some well-known algorithms\u00a0\u2026", "num_citations": "42\n", "authors": ["513"]}
{"title": "Shared keystroke dataset for continuous authentication\n", "abstract": " Keystroke dynamics is an effective behavioral biometrics for user authentication at a computer terminal. Continuous or active authentication using keystroke dynamics has raised a lot of interest among researchers. However, there are only a few public datasets available for the research community compared to other biometric modalities primarily because of the difficulty of large scale data collection. Even the existing ones generally suffer from small number of subjects and lack of extensive features. In this paper, we provide the details on the collection of a shared dataset for the study of keystroke dynamics. We have collected raw keystroke data from 157 subjects allowing them to transcribe fixed text and answer questions freely. The dataset is characterized to reflect the temporal variations of typing patterns and the perturbations caused by different keyboard layouts. To show the usability and the quality of our\u00a0\u2026", "num_citations": "41\n", "authors": ["513"]}
{"title": "Power constrained test scheduling with dynamically varied TAM\n", "abstract": " In this paper we present a novel scheduling algorithm for testing embedded core-based SoCs. Given test conflicts, power consumption limitation and top level test access mechanism (TAM) constraint, we handle the constrained scheduling in a unique way that adaptively assigns the cores in parallel to the TAMs with variable width and concurrently executes the test sets by dynamic test partitioning, thus reducing the test cost in terms of the overall test time. Through simulation, we show that up to 30% of SoC testing time reduction can be achieved by using our scheduling approach.", "num_citations": "41\n", "authors": ["513"]}
{"title": "Electronic Banking and Information Assurance Issues: Surveys and Synthesis\n", "abstract": " Information assurance is a key component in e-banking services. This article investigates the information assurance issues and tenets of e-banking security that would be needed for design, development and assessment of an adequate electronic security infrastructure. The technology terminology and frameworks presented in the article are with the view to equip the reader with a glimpse of the state-of-art technologies that may help towards learned and better decisions regarding electronic security.", "num_citations": "40\n", "authors": ["513"]}
{"title": "Deception-based game theoretical approach to mitigate DoS attacks\n", "abstract": " Denial of Service (DoS) attacks prevent legitimate users from accessing resources by compromising availability of a system. Despite advanced prevention mechanisms, DoS attacks continue to exist, and there is no widely-accepted solution. We propose a deception-based protection mechanism that involves game theory to model the interaction between the defender and the attacker. The defender\u2019s challenge is to determine the optimal network configuration to prevent attackers from staging a DoS attack while providing service to legitimate users. In this setting, the defender can employ camouflage by either disguising a normal system as a honeypot, or by disguising a honeypot as a normal system. We use signaling game with perfect Bayesian equilibrium (PBE) to explore the strategies and point out the important implications for this type of dynamic games with incomplete information. Our analysis\u00a0\u2026", "num_citations": "39\n", "authors": ["513"]}
{"title": "A watchdog processor based general rollback technique with multiple retries\n", "abstract": " A common assumption in the existing rollback techniques is that transients, the cause of most failures, subside very quickly, implying that a single story retry of the program from the previous rollback point is sufficient. The authors discuss a general rollback strategy with  n ( n \u22652) retries which takes into consideration multiple transient failures as well as transients of long duration. Ways of deriving practical values of  n  for a given program are also discussed. Furthermore, the authors propose the use of a watchdog processor as an error detection tool to initiate recovery action through rollback, since the watchdog processor offers low error latency. They also discuss the merging of the watchdog processor with rollback recovery technique for enhancing the overall system reliability.", "num_citations": "39\n", "authors": ["513"]}
{"title": "Spycon: Emulating user activities to detect evasive spyware\n", "abstract": " The success of any spyware is determined by its ability to evade detection. Although traditional detection methodologies employing signature and anomaly based systems have had reasonable success, new class of spyware programs emerge which blend in with user activities to avoid detection. One of the latest anti-spyware technologies consists of a local agent that generates honeytokens of known parameters (e.g., network access requests) and tricks spyware into assuming it to be legitimate activity. In this paper, as a first step, we address the deficiencies of static honeytoken generation and present an attack that circumvents such detection techniques. We synthesize the attack by means of data mining algorithms like associative rule mining. Next, we present a randomized honeytoken generation mechanism to address this new class of spyware. Experimental results show that (i) static honeytokens are detected\u00a0\u2026", "num_citations": "36\n", "authors": ["513"]}
{"title": "Smart handling of colluding black hole attacks in MANETs and wireless sensor networks using multipath routing\n", "abstract": " The open medium, dynamic topology and infrastructureless characteristics of MANETs and sensor networks, have found widespread military applications. However, the nature of these networks and the limited processing capabilities of the nodes make them vulnerable to malicious attacks. In this paper we address the problem of colluding and coordinated black hole attacks, one of the major security issues in MANET based defense applications. These attacks are caused by malicious nodes that advertise the availability of the shortest route to the intended destination, thereby exploiting the functioning of the AODV protocol and retaining the data packets. This leads to loss of critical and sensitive information being relayed across the network. We propose a technique that overcomes the shortcomings of this protocol, and makes it less vulnerable to such attacks by identifying the malicious nodes and isolating them from\u00a0\u2026", "num_citations": "36\n", "authors": ["513"]}
{"title": "A new framework for generating optimal march tests for memory arrays\n", "abstract": " Given a set of memory array faults the problem of computing an optimal March test that detects all specified memory array faults is addressed. In this paper, we propose a novel approach in which every memory army fault is modeled by a set of primitive memory faults. A primitive March test is defined for each primitive memory fault. We show that March tests that detect the specified memory array faults are composed of primitive March tests. A method to compute the optimal March tests for the specified memory array faults is described. A set of examples to illustrate the approach is presented.", "num_citations": "36\n", "authors": ["513"]}
{"title": "Similarity metrics for SQL query clustering\n", "abstract": " Database access logs are the starting point for many forms of database administration, from database performance tuning, to security auditing, to benchmark design, and many more. Unfortunately, query logs are also large and unwieldy, and it can be difficult for an analyst to extract broad patterns from the set of queries found therein. Clustering is a natural first step towards understanding the massive query logs. However, many clustering methods rely on the notion of pairwise similarity, which is challenging to compute for SQL queries, especially when the underlying data and database schema is unavailable. We investigate the problem of computing similarity between queries, relying only on the query structure. We conduct a rigorous evaluation of three query similarity heuristics proposed in the literature applied to query clustering on multiple query log datasets, representing different types of query workloads. To\u00a0\u2026", "num_citations": "35\n", "authors": ["513"]}
{"title": "Real-time multistage attack awareness through enhanced intrusion alert clustering\n", "abstract": " Correlation and fusion of intrusion alerts to provide effective situation awareness of cyber-attacks has become an active area of research. Snort is the most widely deployed intrusion detection sensor. For many networks and their system administrators, the alerts generated by Snort are the primary indicators of network misuse and attacker activity. However, the volume of the alerts generated in typical networks makes real-time attack scenario comprehension difficult. In this paper, we present an attack-stage oriented classification of alerts using Snort as an example and demonstrate that this effectively improves real-time situation awareness of multistage attacks. We also incorporate this scheme into a real-time attack detection framework and prototype presented by the authors in previous work and provide some results from testing against multistage attack scenarios", "num_citations": "35\n", "authors": ["513"]}
{"title": "Reliability, reconfiguration, and spare allocation issues in binary-tree architectures based on multiple-level redundancy\n", "abstract": " The locally redundant modular tree (LRMT) schemes offer high yield and reliability for trees of relatively few levels but are less effective for large binary trees due to the imbalance of reliability of different levels. A new multiple-level redundancy tree (MLRT) architecture that combines modular schemes with level-oriented schemes which lead to better yield and reliability is presented. The MLRT structure enhances the wafer yield to significant levels by offering separate layers of protection for random and clustered defects. Unlike most existing techniques, this technique performs a more accurate reliability analysis by taking into account both switch and link failures. A measure called the marginal switch to processing element area ratio (MSR) is introduced to precisely characterize the effect of switch complexity on the reliability of the redundant system. A systematic method for the optimal distribution of spare modules of\u00a0\u2026", "num_citations": "35\n", "authors": ["513"]}
{"title": "Advances in secure knowledge management in the big data era\n", "abstract": " Information is increasingly becoming important in our daily lives. We have become information dependents of the twenty-first century, living in an on-command, on-demand Big data world. In this era, more information is being created by individuals than by business houses. In the past, we had to stand in a queue at a railway reservation counter to book our tickets, had to visit a cash counter in a bank to do our transactions, had to arrange a get together at a physical location within our town to meet and socialize with our friends, had to visit a theater to watch a movie, and so on. We now have Information and Communication Technology (ICT) to help us do all these by sitting in front of a computer and with a few mouse clicks. Also, all these advancements are possible because we are drenched in a flood of data today. It is important to distinguish between data, information and knowledge. Data is a set of facts about events.", "num_citations": "34\n", "authors": ["513"]}
{"title": "User authentication with keystroke dynamics in long-text data\n", "abstract": " Keystroke dynamics is a form of behavioral biometrics that can be used for continuous authentication of users while working at a terminal. In this paper, we extend the use of support vector machine (SVM) for continuous authentication with long-text data, from one-time password based authentication using short text. In result, we show we can authenticate legitimate users and reject impostors with negligible error (close to 0% equal error rate) by setting a one-class SVM for each user using a dataset of 34 users in a controlled environment. Our results show that by standardizing the input and setting the correct kernel scale, one-class SVM can be utilized as a tool to continuously authenticate users, and recognize keystroke dynamics with a high accuracy.", "num_citations": "33\n", "authors": ["513"]}
{"title": "Insider threat analysis using information-centric modeling\n", "abstract": " Capability acquisition graphs (CAGs) provide a powerful framework for modeling insider threats, network attacks and system vulnerabilities. However, CAG-based security modeling systems have yet to be deployed in practice. This paper demonstrates the feasibility of applying CAGs to insider threat analysis. In particular, it describes the design and operation of an information-centric, graphics-oriented tool called ICMAP. ICMAP enables an analyst without any theoretical background to apply CAGs to answer security questions about vulnerabilities and likely attack scenarios, as well as to monitor network nodes. This functionality makes the tool very useful for attack attribution and forensics.", "num_citations": "33\n", "authors": ["513"]}
{"title": "Part 2: emerging issues for secure knowledge management-results of a Delphi study\n", "abstract": " Secure Knowledge Management (SKM) is one of the emerging areas in both knowledge management and information system disciplines. SKM refers to the management of knowledge while adhering to principles of security and privacy. This study identifies key issues on SKM and draws a consensus among domain experts on the key issues. This study is an attempt to accelerate further research and development in the SKM field. In this study, the authors conducted a three-round Delphi study, identifying 21 issues in the SKM area, along with their importance and urgency ratings. Analyses show that participating experts achieved a higher level of consensus on the importance and urgency of the issue as the rounds progressed. The findings will allow both practitioners and researchers to focus and prioritize research needs in the SKM area. The paper also discusses some future-research directions", "num_citations": "33\n", "authors": ["513"]}
{"title": "Information assurance metric development framework for electronic bill presentment and payment systems using transaction and workflow analysis\n", "abstract": " One of the fastest growing applications in the banking arena is Electronic Bill Presentation and Payment (EBPP), driven primarily by a desire to reduce costs associated with issuing and settling physical bills. EBPP is a secure system for companies to electronically present bills and other related information to their customers, and host the secure payment of these bills. This paper puts forth information assurance issues that are analyzed from a workflow and transaction analysis perspective. Various aspects and technologies deployed in EBPP systems are discussed with a view to understand security underpinnings. The paper develops a framework for the measurement of security levels of any EBPP system, which will help security personnel to ensure a higher level of understanding of information assurance issues and proactively engage in elevating security measures and fraud protection in their organizations. A\u00a0\u2026", "num_citations": "33\n", "authors": ["513"]}
{"title": "An analytical framework for reasoning about intrusions\n", "abstract": " Local and wide area network information assurance analysts need current and precise knowledge about their system activities in order to address the challenges of critical infrastructure protection. In particular, the analyst needs to know in real-time that an intrusion has occurred so that an active response and recovery thread can be created rapidly. Existing intrusion detection solutions are basically after-the-fact, thereby offering very little in terms of damage confinement and restoration of service. Quick recovery is only possible if the assessment scheme has low latency and it occurs in real-time. The objective of the paper is to develop a reasoning framework to aid in the real-time detection and assessment task that is based on a novel idea of encapsulation of owner's intent. The theoretical framework developed here will help resolve dubious circumstances that may arise while inferring the premises of operations\u00a0\u2026", "num_citations": "33\n", "authors": ["513"]}
{"title": "Ettu: Analyzing query intents in corporate databases\n", "abstract": " Insider threats to databases in the financial sector have become a very serious and pervasive security problem. This paper proposes a framework to analyze access patterns to databases by clustering SQL queries issued to the database. Our system Ettu works by grouping queries with other similarly structured queries. The small number of intent groups that result can then be efficiently labeled by human operators. We show how our system is designed and how the components of the system work. Our preliminary results show that our system accurately models user intent.", "num_citations": "32\n", "authors": ["513"]}
{"title": "Enhanced recognition of keystroke dynamics using Gaussian mixture models\n", "abstract": " Keystroke dynamics is a form of behavioral biometrics that can be used for continuous authentication of computer users. Many classifiers have been proposed for the analysis of acquired user patterns and verification of users at computer terminals. The underlying machine learning methods that use Gaussian density estimator for outlier detection typically assume that the digraph patterns in keystroke data are generated from a single Gaussian distribution. In this paper, we relax this assumption by allowing digraphs to fit more than one distribution via the Gaussian Mixture Model (GMM). We have conducted an experiment with a public data set collected in a controlled environment. Out of 30 users with dynamic text, we obtain 0.08% Equal Error Rate (EER) with 2 components by using GMM, while pure Gaussian yields 1.3% EER for the same data set (an improvement of EER by 93.8%). Our results show that GMM can\u00a0\u2026", "num_citations": "32\n", "authors": ["513"]}
{"title": "USim: a user behavior simulation framework for training and testing IDSes in GUI based systems\n", "abstract": " Anomaly detection systems largely depend on user profile data to be able to detect deviations from normal activity. Most of this profile data is currently based on command-line instructions/directives executed by users on a system. With the advent and extensive usage of graphical user interfaces (GUIs), command-line data can no longer fully represent user's complete behavior which is essential for effectively detecting the anomalies in these GUI based systems. Collection of user behavior data is a slow and time consuming process. In this paper, we present a new approach to automate the generation of user data by parameterizing user behavior in terms of user intention (malicious/normal), user skill level, set of applications installed on a machine, mouse movement and keyboard activity. The user behavior parameters are used to generate templates, which can be further customized. The framework is called USim\u00a0\u2026", "num_citations": "31\n", "authors": ["513"]}
{"title": "Position: The user is the enemy\n", "abstract": " The Human Factor has long been recognized as the weakest link in computer systems security, yet, nothing technically significant has been done to address this problem in an attack agnostic manner. In this paper, we introduce the mantra of\" The User is the Enemy\" for security designers and developers alike as an underlying current towards addressing the weak human factor. We present different notions of the user and the system and argue from parallel tracks that user actions, both ignorant and non-compliant, are detrimental to the organization. We further show how the paradigm has been applied in a rather unconscious manner and contend that security mechanisms borne out of a conscious application will be more effective towards addressing this systemic problem. Our position is not meant to be a cynical attitude towards users; rather, it is meant to be the focal point of security design attitude, similar to the\u00a0\u2026", "num_citations": "30\n", "authors": ["513"]}
{"title": "Insider abuse comprehension through capability acquisition graphs\n", "abstract": " Insider attacks constitute one of the most potent, yet difficult to detect threats to information security in the cyber-domain. Malicious actions perpetrated by privileged insiders usually circumvent intrusion detection systems (IDS) and other mechanisms designed to detect and prevent unauthorized activity. In this paper, we present an architectural framework and technique to aid in situation awareness of insider threats in a networked computing environment such as a corporate network. Individual actions by users are analyzed using a theoretical model called a Capability Acquisition Graph (CAG) to evaluate their cumulative effect and detect possible violations. Our approach is based on periodic evaluation of the privileges that users accumulate with respect to critical information assets during their workflow. A static analysis tool called Information-Centric Modeler and Auditor Program (ICMAP) is used to periodically\u00a0\u2026", "num_citations": "29\n", "authors": ["513"]}
{"title": "Role of Perceived Importance of Information Security: An Exploratory Study of Middle School Children's Information Security Behavior.\n", "abstract": " Information security and privacy on the Internet are critical issues in our society. Importantly, children and adolescents need to understand the potential risk of using the Internet. In this research, we examine factors that motivate students\u2019 Information Security Behavior on the Internet. A pilot survey of middle and junior high school students is the source of the data. This study provides a model that explains students\u2019 behavior pertaining to Information Security. A significant research finding is that students\u2019 perceived importance of Information Security plays a critical role in influencing their Information Security Behavior.", "num_citations": "29\n", "authors": ["513"]}
{"title": "Towards the scalable implementation of a user level anomaly detection system\n", "abstract": " Traditional intrusion detection systems can be broadly classified as misuse and anomaly detectors. Misuse detectors attempt detection by matching the current system/user activity against known signatures and patterns. As opposed to this, anomaly detection works by developing a reference graph and comparing the ongoing activity against it. Any significant deviation is flagged as an intrusion. Anomaly detection is more promising because of its potential to detect unseen types of attacks. However, both techniques have conventionally relied on audit trails sampled deep inside the system via probes and the sheer size of the data allows only after-the-fact and off line detection. In recent past, there have been efforts to capture the semantics of system activity for more rapid detection and this can typically be done at levels closer to the user. In our earlier works related to this effort, we presented a scheme and a reasoning\u00a0\u2026", "num_citations": "27\n", "authors": ["513"]}
{"title": "Performance evaluation of rollback-recovery techniques in computer programs\n", "abstract": " Rollback recovery is a backward error recovery technique used to recover from temporary faults in database and process control systems. Rollback in process control systems is generally constrained by deadlines, thereby requiring a dynamic insertion of rollback points. This is in contrast to rollback recovery in database systems in which rollback points are inserted at equidistant intervals. A simple model based on a semi-Markov process is developed to study the performance of rollback recovery strategies. Using this model, the mean program completion time is obtained for both database and process control systems when rollback recovery is implemented. The analytic results obtained by the semi-Markov model are compared with the simulation results by means of extensive computer simulations.< >", "num_citations": "27\n", "authors": ["513"]}
{"title": "Sensitivity analysis in keystroke dynamics using convolutional neural networks\n", "abstract": " Biometrics has become ubiquitous and spurred common use in many authentication mechanisms. Keystroke dynamics is a form of behavioral biometrics that can be used for user authentication while actively working at a terminal. The proposed mechanisms involve digraph, trigraph and n-graph analysis as separate solutions or suggest a fusion mechanism with certain limitations. However, deep learning can be used as a unifying machine learning technique that consolidates the power of all different features since it has shown tremendous results in image recognition and natural language processing. In this paper, we investigate the applicability of deep learning on three different datasets by using convolutional neural networks and Gaussian data augmentation technique. We achieve 10% higher accuracy and 7.3% lower equal error rate (EER) than existing methods. Also, our sensitivity analysis indicates that the\u00a0\u2026", "num_citations": "26\n", "authors": ["513"]}
{"title": "Analysis of malware propagation in Twitter\n", "abstract": " Malware propagation in social networks is a potential risk that has not been well-studied yet as there are no formal threat models for social networks. In this paper we investigate the vulnerability and cost of spreading malware via Twitter. Towards this end we present three specific attack scenarios targeted for Twitter and systematically analyze the cost of staging each of these attacks. Our analysis presents the first step for understanding the threats on the security of a class of social networks. We identify the attack related parameters and verify these parameters by testing the attack on a Net Logo based simulator. Our analysis indicates that the cost of staging attacks to infect users of Twitter is low and that the proposed attack scenarios are plausible. Further, even with a low degree of connectivity and a low probability of clicking links, Twitter and its structure can be exploited by such attacks to infect many users with\u00a0\u2026", "num_citations": "26\n", "authors": ["513"]}
{"title": "Information assurance education in two-and four-year institutions\n", "abstract": " The 2011 ITiCSE working group on information assurance (IA) education examined undergraduate curricula at the two-and four-year levels, both within and outside the United States (US). A broad set of two-year IA degree programs were examined in order to get a sense of similarities and differences between them. A broad set of four-year IA degree programs were also examined to explore their similarities and differences. A comparison between the two-year and fourfour-year degree programs revealed that the common challenge of articulation between two-and four-year programs exists in IA as well. The challenge of articulation was explored in some depth in order to understand what remedies might be available. Finally, a number of IA programs at international institutions were examined in order to gain insight into differences between US and non-US IA programs.", "num_citations": "26\n", "authors": ["513"]}
{"title": "Secure knowledge management\n", "abstract": " As the world is getting more and more technology savvy, the collection and distribution of information and knowledge need special attention. Progress has been made on the languages and tools needed for effective knowledge management and on the legal issues concerning the consumption and dissemination of critical knowledge. From a business perspective, a knowledge-management system (KMS) within a firm generally strives to maximize the human-capital utilization and profitability of the firm. However, security is becoming a major issue revolving around KMS; for instance, the KMS must incorporate adequate security features to prevent any unauthorized access or unauthorized dissemination of information. Acquiring the information that one needs to remain competitive while safeguarding the information one already has is a complicated task. Firms must balance the advantages of openness against its\u00a0\u2026", "num_citations": "26\n", "authors": ["513"]}
{"title": "A target-centric formal model for insider threat and more\n", "abstract": " The diversity of cyber threat has grown over time from network-level attacks and passwordcracking to include newer classes such as insider attacks, email worms and social engineering, which are currently recognized as serious security problems. However, attack modeling and threat analysis tools have not evolved at the same rate. Known formal models such as attack graphs perform action-centric vulnerability modeling and analysis. All possible atomic user actions are represented as states, and sequences which lead to the violation of a specified safety property are extracted to indicate possible exploits. While attack graphs are relevant in the context of network level attacks, they are ill-equipped to address complex threats such as insider attacks. The difficulty mainly lies in the fact that adversaries belonging to this threat class use familiarity of and accessibility to their computational environment to discover new ways of launching stealthy, damaging attacks. In this paper, we propose a new target-centric model to address this class of security problems and explain the modeling methodology with specific examples. Finally, we perform quantified vulnerability analyses and prove worst case complexity results on our model.", "num_citations": "26\n", "authors": ["513"]}
{"title": "ARCHERR: Runtime environment driven program safety\n", "abstract": " Parameters of a program\u2019s runtime environment such as the machine architecture and operating system largely determine whether a vulnerability can be exploited. For example, the machine word size is an important factor in an integer overflow attack and likewise the memory layout of a process in a buffer or heap overflow attack. In this paper, we present an analysis of the effects of a runtime environment on a language\u2019s data types. Based on this analysis, we have developed Archerr, an automated one-pass source-to-source transformer that derives appropriate architecture dependent runtime safety error checks and inserts them in C source programs. Our approach achieves comprehensive vulnerability coverage against a wide array of program-level exploits including integer overflows/underflows. We demonstrate the efficacy of our technique on versions of C programs with known vulnerabilities such as\u00a0\u2026", "num_citations": "26\n", "authors": ["513"]}
{"title": "An empirical examination of IT-enabled emergency response: the cases of Hurricane Katrina and Hurricane Rita\n", "abstract": " This paper reports the results of an empirical study that analyzes emergency incident response. The paper studies how information systems (IS) complement other organizational assets to help emergency responders achieve satisfactory response performance. We test the research model using empirical data collected from responses to Hurricane Katrina and Hurricane Rita. The results show that IS-enabled asset allocation support directly improves emergency response performance and also positively interacts with non-IS response assets in achieving response success. The results also confirm the value of dispatch systems, interagency communications, and knowledge repositories in developing asset allocation support for an emergency response organization.", "num_citations": "25\n", "authors": ["513"]}
{"title": "Framework for analyzing critical incident management systems (CIMS)\n", "abstract": " Critical incident management systems (CIMS) are information systems used to deal with day to day occurring emergencies as well as to mitigate major catastrophic events which can be man-made (9/11 attacks, Oklahoma bombings, etc) or simply an act of nature (Category 5 hurricanes Andrews, category 4 hurricane Katrina, Earthquake in San Francisco, etc). There are several CIMS in use today. Unfortunately, these current systems do not regard risk as a crucial part of the decision-making scenario. Existing literature has also not addressed this issue adequately. To effectively assess and analyze CIMS risk factors, it is necessary to identify the factors relating to assessing particular kinds of risks to such systems. This paper develops a framework for analyzing the risks that are important for CIMS to take into account. We use the economic theory of shortage to develop the framework. Furthermore, feedback from\u00a0\u2026", "num_citations": "25\n", "authors": ["513"]}
{"title": "Adaptive Test scheduling in SOC's by dynamic partitioning\n", "abstract": " In this paper, we present a novel adaptive scheduling algorithm for testing embedded core-based SoC's. Tests are scheduled in a way that dynamically partitions and allocates the tests, consequently constructing and updating a set of dynamically partitioned power constrained concurrent test sets, and ultimately reducing the test application time. A simulation study shows the productivity gained by our new approach.", "num_citations": "25\n", "authors": ["513"]}
{"title": "Programmable memory BIST and a new synthesis framework\n", "abstract": " The development of two programmable memory BIST architectures is first reported. A memory synthesis framework which can automatically generate, verify and insert programmable as well as non-programmable BIST units is developed as a vehicle to efficiently integrate BIST architectures in today's memory-intensive systems. Custom memory test algorithms could be loaded in the developed programmable BIST unit and therefore any type of memory test algorithm could be realized. The flexibility and efficiency of the framework are demonstrated by showing that these memory BIST units could be generated, functionally verified and inserted in a short time.", "num_citations": "25\n", "authors": ["513"]}
{"title": "A new approach to the design of built-in self-testing plas for high fault coverage\n", "abstract": " Four critical requirements are identified for the built-in self-testing of programmable logic arrays (BIST PLAs): the test set to test the PLA as well as the output response must be independent of the function of the PLA; the test pattern generator (TPG) and the response evaluator circuits must be simple to keep the extra logic overhead to a minimum; the fault coverage of the PLA must be within acceptable limits; and the speed of the test application must be high. A design that meets all of these goals is proposed. The approach is based on counting crosspoints, as opposed to the conventional parity technique. The TPG and RE circuits are simple and consist of shift registers and counters. The design requires a reorganization of the columns of the PLA on the basis of the number of crosspoints. This design provides extremely high fault coverage: the coverage for multiple faults is higher than that of any BIST design known to\u00a0\u2026", "num_citations": "25\n", "authors": ["513"]}
{"title": "The early (tweet-ing) bird spreads the worm: An assessment of twitter for malware propagation\n", "abstract": " Social Networks have rapidly become one of the most used Internet based applications. The structure and ease of information dissemination provides an opportunity for adversaries to use it for their own malicious purpose. In this paper we investigate a popular social network \u2013 Twitter as a malware propagation medium. We present a basic model for Twitter-based malware propagation using epidemic theory. Our analysis shows that even with a low degree of connectivity and a low probability of clicking links, Twitter and its structure can be exploited to infect many nodes.", "num_citations": "24\n", "authors": ["513"]}
{"title": "Towards modeling trust based decisions: a game theoretic approach\n", "abstract": " Current trust models enable decision support at an implicit level by means of thresholds or constraint satisfiability. Decision support is mostly included only for a single binary action, and does not explicitly consider the purpose of a transaction. In this paper, we present a game theoretic model that is specifically tuned for decision support on a whole host of actions, based on specified thresholds of risk. As opposed to traditional representations on the real number line between 0 and +1, Trust in our model is represented as an index into a set of actions ordered according to the agent\u2019s preference. A base scenario of zero trust is defined by the equilibrium point of a game described in normal form with a certain payoff structure. We then present the blind trust model, where an entity attempts to initiate a trust relationship with another entity for a one-time transaction, without any prior knowledge or\u00a0\u2026", "num_citations": "24\n", "authors": ["513"]}
{"title": "An experimental study to determine task size for rollback recovery systems\n", "abstract": " The effects of using a recovery cache to save the variables of a program are studied. A novel optimization model for rollback is formulated to include the effects of a recovery cache in rollback systems. The parameters of the model proposed are the maximum recovery time, the cache size, and the save and load time associated with the task size. The results are also discussed of an experimental study conducted to estimate the parameters of the programs that are critical for arriving at a suitable task size or cache size to minimize the cost of recovery.< >", "num_citations": "24\n", "authors": ["513"]}
{"title": "Design of a wireless test control network with radio-on-chip technology for nanometer system-on-a-chip\n", "abstract": " The continued push to smaller geometries, higher frequencies, and larger chip sizes rapidly resulted in an incompatibility between interconnect needs and projected interconnect performance. As stated in the 2003 International Technology Roadmap for Semiconductors (ITRS'03) report, revolutionary interconnect methodologies such as radio frequency (RF)/wireless will deliver the foreseen progress in semiconductor technology. Recent advances in silicon integrated circuit technique are making possible tiny low-cost transceivers to be integrated on chip, namely \"radio-on-chip\" (ROC) technology. This paper proposes the idea of using wireless radios to transmit test data and control signals to resolve the acerbated core accessibility problem. Three types of wireless test micronetworks are first presented, i.e., miniature wireless local area network (LAN), multihop wireless test control network (MTCNet), and distributed\u00a0\u2026", "num_citations": "23\n", "authors": ["513"]}
{"title": "Securing information through trust management in wireless networks\n", "abstract": " Wireless networks are prone to certain information security threats that are either unique, or more pronounced for them due to the open air nature of the channel, bandwidth limitations and constantly changing topology. The concept of trust management, ie, establishment of trust combined with trust monitoring, can be useful for mitigating the consequences of a substantial number of these threats. In this paper we introduce schemes for trust based secure information management for wireless networks. We outline trust management for both centralauthority-assisted (base-assisted) and independent adhoc networks. For base-assisted networks, we define a trust based admission control scheme, use intent graphs for trust monitoring, and a peer monitoring scheme based on actual condition review. For ad-hoc networks, we define the concept of logical and physical trust domains and introduce the idea of using domain heads for trust management.", "num_citations": "22\n", "authors": ["513"]}
{"title": "Function-based candidate discrimination during model-based diagnosis\n", "abstract": " We propose function for candidate discrimination, i.e., suspect ordering during model-based diagnosis. Function offers advantages over structure and fault probabilities currently being used for candidate discrimination. It is readily available from device design, unlike fault probabilities, which are hard to obtain. Function-based discrimination is not dependent on the topology of the device, unlike structure-based discrimination. We propose classes as a scheme for representation of function. As part of classes, we define a set of function primitives and provide a framework for identifying the functions of components and subsystems of a device. The representation scheme is domain independent. We propose a function-based technique for candidate discrimination called the default order technique, and outline a diagnosis algorithm that applies the technique to the class model of a device. Function-based diagnosis is in\u00a0\u2026", "num_citations": "21\n", "authors": ["513"]}
{"title": "On-chip test generation for combinational circuits by LFSR modification\n", "abstract": " A new on-chip test generation technique based on the built-in self test (BIST) and deterministic test generation concepts has been proposed. Given a test set, the test patterns can be regenerated on the chip and applied to the circuit under test without the use of any external test equipments. A systematic procedure for the modification of a basic linear feedback shift register (LFSR) to realize the on-chip test generation hardware is given. Since the delay introduced by the modification of the LFSR is only two gate delays, at-speed testing of circuits is feasible. Experiments are conducted and test application time and hardware overhead are compared with a known test technique under the same fault coverage conditions. It is shown that both test cost and test application time can be decreased significantly by using the proposed technique.", "num_citations": "21\n", "authors": ["513"]}
{"title": "An activity theory approach to modeling dispatch-mediated emergency response\n", "abstract": " Emergency response involves multiple local, state, and federal communities of responders. These communities are supported by emergency dispatch agencies that share digital traces of task-critical information. However, the communities of responders often comprise an informal network of people and lack structured mechanisms of information sharing. To standardize the exchange of task-critical information in communities of responders, we develop a conceptual modeling grammar. We base the grammar on an activity-theory perspective and ground it in an analysis of emergency dispatch incident reports. The paper contributes to research in dispatch-mediated emergency response literature by (1) developing a framework of elements and relationships to support critical information flow within emergency communities of responders,(2) developing a conceptual modeling grammar for modeling emergency tasks in dispatch-mediated emergency response, and (3) implementing a prototype system to demonstrate the utility of the conceptual modeling grammar.", "num_citations": "20\n", "authors": ["513"]}
{"title": "A dispatch-mediated communication model for emergency response systems\n", "abstract": " The current state of emergency communication is dispatch-mediated (the messages from the scene are directed towards the responders and agencies through the dispatch agency). These messages are logged in electronic documents called incident reports, which are useful in monitoring the incident, off-site supervision, resource allocation, and post-incident analysis. However, these messages do not adhere to any particular structure, and there is no set format. The lack of standards creates a problem for sharing information among systems and responders and has a detrimental impact on systems interoperability. In this article, we develop a National Information Exchange Model (NIEM) and Universal Core (UCORE) compliant messaging model, considering message structures and formats, to foster message standardization.", "num_citations": "20\n", "authors": ["513"]}
{"title": "A generic resource distribution and test scheduling scheme for embedded core-based SoCs\n", "abstract": " We present a novel test scheduling algorithm for embedded core-based system-on-chips based on a graph-theoretic formulation. Given a system integrated with a set of cores and a set of test resources, we select a test for each core from a set of alternative test sets, and schedule it in a way to evenly balance the resource usage and to ultimately reduce the test application time. Improvements to the basic algorithm are sought by grouping the cores and assigning higher priorities to those with smaller number of alternate test sets. The algorithm is also extended for solving the general test scheduling problem where multiple test sets are selected for each core from a set of alternatives to facilitate the testing for various fault models. A simulation study is performed to quantify the performance of the proposed scheduling approach.", "num_citations": "20\n", "authors": ["513"]}
{"title": "Fault diagnosis of analog circuits using artificial neural networks as signature analyzers\n", "abstract": " Experimental results using neural networks to provide go/no-go testing and fault diagnosis of analog circuits are presented. The primary focus is on reducing test time and providing a simple mechanism for automatic test pattern generation. Networks of reasonable dimension are shown to be capable of robust diagnosis of analog circuits, including effects due to tolerances and nonlinearities. The concepts are extended to include an approach to built-in test of analog or mixed signal ASICs.< >", "num_citations": "20\n", "authors": ["513"]}
{"title": "An analysis of a reconfigurable binary tree architecture based on multiple-level redundancy\n", "abstract": " The analysis of a multiple-level redundant tree (MLRT) structure is presented for the design of a reconfigurable tree architecture. The MLRT scheme tolerates the catastrophic failure of several locally redundant modules in the corresponding locally redundant modular tree (LRMT) structure. This analysis and experimental study establishes the advantages of the MLRT structure over the LRMT structure. The switch failures are taken into account for an accurate analysis of the reliability. A new measure, called the marginal-switch-to-processing-element-area ratio (MSR), is introduced to characterize the effect of switch complexity on the reliability of the redundant system. It can be used as an evaluation criterion in the design of practical fault-tolerant multiprocessor architectures. A technique for obtaining the best spare distribution in the MLRT structure is presented.< >", "num_citations": "20\n", "authors": ["513"]}
{"title": "Situation awareness of multistage cyber attacks by semantic event fusion\n", "abstract": " In this paper, we present strategies for real-time Situation Awareness of multistage cyber-attacks by utilizing heterogeneous sensor event streams. A flexible and practically usable attack modeling approach based on network connectivity and attack progression semantics is used to produce multistage attack templates. Events in live alert streams are correlated based on their semantics and the attack templates to provide analysts with effective perception, comprehension and projection of likely attacks and their progression. The techniques form the basis of the Event Correlation for Cyber Attack Recognition System (ECCARS), which is tested and validated extensively with realistic datasets.", "num_citations": "19\n", "authors": ["513"]}
{"title": "SAWAN: a survivable architecture for wireless LANs\n", "abstract": " This paper describes survivability schemes against access point (AP) failures in wireless LANs. It particularly aims for resiliency and survivability against multistage attacks where the adversary is successful in compromising the AP, and then targets the survived but more vulnerable network. This is true in real life where the adversary knows that survivability is a design consideration built into the network. It then performs a multistage targeted attack that is aimed at compromising the survived network that may have vulnerabilities. We first present a unique infrastructure for an ad-hoc migration scheme (IAMS) where the nodes under a failed AP form an ad-hoc network and reconnect to the network using available neighboring APs. We then present a scheme for isolating and removing any malicious nodes from the ad-hoc network routes in a transparent manner once the malicious nodes have been identified. This will\u00a0\u2026", "num_citations": "19\n", "authors": ["513"]}
{"title": "A decentralized voting algorithm for increasing dependability in distributed systems\n", "abstract": " Replication and majority voting are well-known and widely-used methods for achieving fault-tolerance in distributed systems. An open area of investigation is coordinating the voting in a secure manner, so as to withstand malicious attacks. The Timed-Buffer Distributed Voting Algorithm (TB-DVA), a secure distributed voting protocol, is introduced for this purpose. It is contrasted with several other distributed voting schemes in order to show its unique contribution for both fault-tolerance and security, which are essential ingredients for system dependability.", "num_citations": "19\n", "authors": ["513"]}
{"title": "A novel approach to random pattern testing of sequential circuits\n", "abstract": " Random pattern testing methods are known to result in poor fault coverage for most sequential circuits unless costly circuit modifications are made. In this paper, we propose a novel approach to improve the random pattern testability of sequential circuits. We introduce the concept of holding signals at primary inputs and scan flipflops of a partially scanned sequential circuit for a certain length of time, instead of applying a new random vector at each clock cycle. When a random vector is held at the primary inputs of the circuit under test or at the scan flip-flops, the system clock is applied and the primary outputs of the circuit are observed. Information obtained from a testability analysis or test generator is used to determine the number of clock cycles for which each random vector is to be held constant. The method is low cost and the results of our experiment on the benchmark circuits show that it is very effective in\u00a0\u2026", "num_citations": "19\n", "authors": ["513"]}
{"title": "Anatomy of the information security workforce\n", "abstract": " Survey results indicate that the information security workforce, one of the fastest growing subgroups in IT, is a unique professional niche with distinctive task responsibilities, job market conditions, and training needs. Given the demand for the information security workforce, this paper offers useful insight to various stakeholders, including prospective information security professionals, employers, educational institutions, and industry steering (government) authorities. Specifically, our study results should help companies, prospective information security professionals, and educational institutions alike understand the issues pertaining to this labor niche and fulfill the increasing labor demands.", "num_citations": "18\n", "authors": ["513"]}
{"title": "CUSP: customizable and usable spam filters for detecting phishing emails\n", "abstract": " Phishing attack continues to be a significant threat to the Internet users and commercial organizations worldwide causing billions of dollars in damage. A successful phishing attack depends on the inability of an end user to accurately tell legitimate and spoofed emails apart. However, unlike their legitimate counterpart, as spoofed emails are composed in bulk, they do not contain any user specific data, which relates users with their accounts. In this paper, as a first step, we propose a customizable spam filter that allows the users to store this user specific data on a per organization basis, and then use the stored data to discriminate against fraudulent emails. As a next step, we propose a NLP based technique to generate context sensitive warnings that would help in educating users about the dangers of phishing attack. Lastly, we test and validate our framework on existing phishing corpus and live emails.", "num_citations": "18\n", "authors": ["513"]}
{"title": "An object-oriented testbed for the evaluation of checkpointing and recovery systems\n", "abstract": " The paper presents the design and development of an object-oriented testbed for simulation and analysis of checkpointing and recovery schemes in distributed systems. An important contribution, of the testbed is a unified environment that provides a set of specialized components for easy and detailed simulation of checkpointing and recovery schemes. The testbed allows a designer to mix and match different components either to study the effectiveness of a particular scheme or to freely experiment with hybrid designs before the actual implementation. The testbed also facilitates the evaluation of interdependencies among the various parameters such as communication and application dynamics and their effect on the performance of checkpointing and recovery schemes. The implementation of the testbed as an extension of DEPEND which is an integrated design and fault-injection environment, provides for\u00a0\u2026", "num_citations": "18\n", "authors": ["513"]}
{"title": "Method for rendering usuable a defective raw programmable logic array and a defective programmable logic array rendered usable by this method\n", "abstract": " A method of rendering defective raw programmable logic arrays usable by locating the manufacturing defects within the raw logic array, reconfiguring the columns of a mask logic array, and mapping the mask logic array onto the raw logic array so as to mask the defects. A defective programmable logic array rendered usable by this method.", "num_citations": "18\n", "authors": ["513"]}
{"title": "Effects of text filtering on authentication performance of keystroke biometrics\n", "abstract": " Free text keystroke dynamics is a behavioral biometric that has the strong potential to offer unobtrusive and continuous user authentication. In free-text keystroke biometrics, users are free to type whatever they want to while still being authenticated. However, not all keystrokes from a user exhibit the same quality of stable patterns that can be used to differentiate them from others. The \u201cunstable\u201d keystrokes may originate from such activities as when the user is playing a computer game, or other sources of noisy or \u201cgibberish\u201d text. Our hypothesis is that some forms of text negatively impact keystroke dynamics-based user authentication, and thus, should be filtered out. This study investigates the impact of gibberish text on authentication performance through locating and removing gibberish text, and comparing the difference in authentication performance before and after the removal. We confirm the positive effect of text\u00a0\u2026", "num_citations": "17\n", "authors": ["513"]}
{"title": "Secure and privacy preserving data processing support for active authentication\n", "abstract": " Keystroke dynamics and mouse movements are effective behavioral biometric modalities for active authentication. However, very little is done on the privacy of collection and transmission of keyboard and mouse data. In this paper, we develop a rule based data sanitization scheme to detect and remove personally identifiable and other sensitive information from the collected data set. Preliminary experiments show that our scheme incurs on average 5.69\u00a0% false negative error rate and 0.64\u00a0% false positive error rate. We also develop a data transmission scheme using the Extensible Messaging and Presence Protocol (XMPP) to guarantee privacy during transmission. Using these two schemes as a basis, we develop two distinct architectures for providing secure and privacy preserving data processing support for active authentication. These architectures provide flexibility of use depending upon the\u00a0\u2026", "num_citations": "17\n", "authors": ["513"]}
{"title": "Short term and total life impact analysis of email worms in computer systems\n", "abstract": " This paper develops a methodology for analyzing and predicting the impact category of malicious code, particularly email worms. The current paper develops two frameworks to classify email worms based on their detrimental impact. The first framework, the Total Life Impact (TLI) framework is a descriptive model or classifier to categorize worms in terms of their impact, after the worm has run its course. The second framework, the Short Term Impact (STI) framework, allows for prediction of the impact of the worm utilizing the data available during the early stages in the life of a worm. Given the classification, this study identifies the issue of how well the STI framework allows for prediction of the worm into its final impact category based on data that are available in early stages as well as whether the predicted value from Short Term Impact framework valid statistically and practically.", "num_citations": "17\n", "authors": ["513"]}
{"title": "A tamper-resistant framework for unambiguous detection of attacks in user space using process monitors\n", "abstract": " Replication and redundancy techniques rely on the assumption that a majority of components are always safe and voting is used to resolve any ambiguities. This assumption may be unreasonable in the context of attacks and intrusions. An intruder could compromise any number of the available copies of a service resulting in a false sense of security. The kernel based approaches have proven to be quite effective but they cause performance impacts if any code changes are in the critical path. We provide an alternate user space mechanism consisting of process monitors by which such user space daemons can be unambiguously monitored without causing serious performance impacts. A framework that claims to provide such a feature must itself be tamper-resistant to attacks. We theoretically analyze and compare some relevant schemes and show their fallibility. We propose our own framework that is based on\u00a0\u2026", "num_citations": "17\n", "authors": ["513"]}
{"title": "Component-ontological representation of function for reasoning about devices\n", "abstract": " We propose principles behind component-ontological representation of function. In this representation, the function of a component is expressed in terms of its ports. The representation has many advantages: (i) The function of a component can be represented in isolation of its environment. Therefore, libraries of function models can be built. These models are re-usable. (ii) The function model of a complex device can be built by composing the function models of its components. This helps preserve the fidelity of representation. Further, this process can be automated. (iii) The representation is linear in space complexity: the function model of an atomic component is linear in the number of its ports, and that of a composite device is in most cases linear in the number of its components.We propose Classes as a component-ontological representation of function. We illustrate Classes by using it to model parts of a printer\u00a0\u2026", "num_citations": "17\n", "authors": ["513"]}
{"title": "BIST-PLA: A built-in self-test design of large programmable logic arrays\n", "abstract": " A new method for designing a Built-In Self-Test Programmable Logic Array (BIST-PLA) is presented. In the proposed design, the Test Pattern Generator and the Response Evaluator circuits are very simple. The design requires a re-arrangement of the AND (OR) planes on the basis of number of crosspoints in the product (output) lines in the PLA.", "num_citations": "17\n", "authors": ["513"]}
{"title": "Adaptive techniques for intra-user variability in keystroke dynamics\n", "abstract": " Conventional machine learning algorithms based on keystroke dynamics build a classifier from labeled data in one or more sessions but assume that the dataset at the time of verification exhibits the same distribution. A user's typing characteristics may gradually change over time and space. Therefore, a traditional classifier may perform poorly on another dataset that is acquired under different environmental conditions. In this paper, we investigate the applicability of transfer learning to update a classifier according to the changing environmental conditions with minimum amount of re-training. We show that by using adaptive techniques, it is possible to identify an individual at a different time by acquiring only a few samples from another session, and at the same time obtain up to 13% higher accuracy. We make a comparative analysis among the proposed algorithms and conclude that adaptive classifiers exhibit a\u00a0\u2026", "num_citations": "16\n", "authors": ["513"]}
{"title": "Managing information assurance in financial services\n", "abstract": " While advances in information technology and the adoption of Internet as service delivery channels have enabled financial service institutions to provide more convenient assistance, they have diversified and complicated the nature of risks involved. Managing Information Assurance in Financial Services provides high-quality research papers and industrial practice articles in the areas of information security in the financial service industry. Managing Information Assurance in Financial Services provides insight into current information security measures, including: technology, processes, and compliance from some of the leading researchers and practitioners in the field. This book provides immense scholarly value and contribution in the areas of information technology, security, finance, and service.", "num_citations": "16\n", "authors": ["513"]}
{"title": "Design principles for emergency response management systems\n", "abstract": " The National Incident Management System (NIMS) was developed so that responders from different jurisdictions and disciplines could work together to respond to natural disasters and emergencies, including acts of terrorism. The NIMS document provides a set of guidelines about practices but it does not spell of the design requirements for information systems to support the management of critical incidents. Though there are academic and practitioner papers in the general area of emergency management, there is a lacuna of literature discussing how to design information systems to support critical incident response. In this paper we develop a set of design principles that are grounded in emergency management literature and also on the insights from the real response managers in Western New York area. The set of design principles provide a foundation for the development of critical incident response systems.", "num_citations": "16\n", "authors": ["513"]}
{"title": "Detecting masquerading users in a document management system\n", "abstract": " A Document Management System (DMS) is a repository of digital documents that provides functionality for check-in, check-out and shared editing. In a DMS, security mechanisms like encryption of documents and enforcement of policies are implemented to protect from information leakage. These security schemes, essentially applications of Digital Rights Management technologies, while effective against external attacks, are ineffective against insider attacks. The typical insider in a DMS already has access to documents and hence, his capabilities for information leakage are much higher. In this work, we address an important, yet unexplored problem of masquerading users in a DMS, a threat for which the DMS inherently has no protection. We approach the problem by monitoring the pattern and mannerism of user actions on documents and building a profile of each user using the resulting logs. In order to illustrate\u00a0\u2026", "num_citations": "16\n", "authors": ["513"]}
{"title": "A distributed concurrent intrusion detection scheme based on assertions\n", "abstract": " This paper1 presents a new technique for intrusion detection based on concurrent monitoring of user operations. In this scheme, prior to starting a session on a computer, an auxiliary process called watchdog rst queries users for a scope le and then generates a table called a sprint-plan. The sprint-plan is composed of carefully derived assertions that can be used as a basis for concurrent monitoring of user commands. The plan is general enough to allow a normal user to perform his task without much interference from the watchdog or system administrator and is speci c enough to detect intrusions, both external and internal. A distributed watchdog process architecture based on the notion of veri able assertions is presented. This scheme is a signi cant enhancement over the traditional approaches that rely on audit trail analysis in that the intrusion detection latency could be much shorter.", "num_citations": "16\n", "authors": ["513"]}
{"title": "Smartdevices Enabled Secure Access to Multiple Entities (SESAME)\n", "abstract": " This invention proposes novel systems, methods and apparatus that utilize smart devices (eg, smartphones) capable of reading/processing biometric inputs, and wireless communications over secure, short-range wireless channels (eg, near field communications (NFC)) to securely access websites and cyber-physical system (CPS) entities such as vehicles, rooms and control knobs as well as sensors and smart meters. A user accesses a website on a display terminal or CPS entity by using her smart device to send her biometric credentials to request access for a service, and communicates with either the said terminal or the said CPS entity which is also capable of short-range wireless communications, using secure and short-range wireless channels to ensure the authenticity of the user when using the service. This system also protects the stored credentials of the user against loss or theft of the smart device since\u00a0\u2026", "num_citations": "15\n", "authors": ["513"]}
{"title": "A deception framework for survivability against next generation cyber attacks\n", "abstract": " Over the years, malicious entities in cyber-space have grown smarter and resourceful. For defenders to stay abreast of the increasingly sophisticated attacks, the need is to understand these attacks. In this paper, we study the current trends in security attacks and present a threat model that encapsulates their sophistication. Survivability is difficult to achieve because of its contradictory requirements. It requires that a critical system survives all attacks (including zero-day attacks), while still conserving the timeliness property of its mission. We recognize deception as an important tool to resolve this conflict. The proposed deception-based framework predicts an attacker\u2019s intent in order to design a stronger and more effective recovery; hence strengthening system survivability. Each design choice is supported by evidence and a detailed review of existing literature. Finally, we discuss the challenges in implementing such a framework and the directions that can be taken to overcome them.", "num_citations": "15\n", "authors": ["513"]}
{"title": "Watchdog processor-assisted fast recovery in distributed systems\n", "abstract": " A major concern in implementing a checkpointbased recovery protocol for distributed systems is the performance degradation resulting from process rollbacks. In critical systems, it is highly desirable to contain the rollback distance as well as the number of processes involved in the rollback so that timely recovery is possible. One popular approach to accomplish such goals is to control the communication of messages which are the main cause of error propagation. In this paper, we show that watchdog processor-based concurrent error detection can be merged with recovery so that quick recovery from errors is possible without restricting the communications. The low cost and low latency characteristic of an m-out-of-n code-based error detection scheme is exploited to develop a novel message validation technique which helps in curtailing the excessive rollback during recovery. A simulation analysis is conducted to demonstrate the bene ts of combining detection and recovery| an approach that has not been looked into as a means to address recovery in critical systems.", "num_citations": "15\n", "authors": ["513"]}
{"title": "Exploration of attacks on current generation smartphones\n", "abstract": " Due to the ever increasing capabilities of current generation smartphones, they are quickly becoming more attractive targets for malicious attackers. The potential of porting attacks and malware from modern computers to these mobile devices is becoming a reality. In this paper, we explore the possibility of staging some attacks on the 802.11 network interface which is common to all smartphones. We begin by explaining and carrying out the exploitation of the SSH vulnerability on jailbroken iPhones that was discovered in late 2009. This paper then looks at simple network flooding attacks with the intention of causing a simple denial of service by depleting the battery life of the device. It is also our intention to show that these flooding attacks can be carried out utilizing a smartphone as the aggressor in order to attack other mobile devices and that the procedure for such attacks is not difficult. A simple tool is developed\u00a0\u2026", "num_citations": "14\n", "authors": ["513"]}
{"title": "A local/global strategy based on signal strength for message routing in wireless mobile ad-hoc networks\n", "abstract": " Route switching is an important issue to connection-oriented communication that needs to maintain a route for a certain period of time. This chapter presents a new approach to ease the route switching problems in a network with a high rate of unit migration. By monitoring the signal strength of messages, a unit in a route that receives an incoming message can detect possible route fluctuations locally. As the average signal strength declines into a dangerous level, the unit that receives the message will send an advance-warning message to the route source unit. If the source unit can find more stable routes locally, it will adapt a substitute route and will complete the process of adaptation before the breakdown of the original route. If the route source unit cannot adapt a new route locally, the source unit will be forced to search for a new route by considering the entire network. The evaluation conducted\u00a0\u2026", "num_citations": "14\n", "authors": ["513"]}
{"title": "Insider threat assessment: Model, analysis and tool\n", "abstract": " Insider threat is typically attributed to legitimate users who maliciously leverage their system privileges, and familiarity and proximity to their computational environment to compromise valuable information or inflict damage. According to the annual CSI/FBI surveys conducted since 1996, internal attacks and insider abuse form a significant portion of reported incidents. The strongest indication yet that insider threat is very real is given by the recent study [2] jointly conducted by CERT and the US Secret Service; the first of its kind, which provides an in-depth insight into the problem in a real-world setting. However, there is no known body of work which addresses this problem effectively. There are several challenges, beginning with understanding the threat.", "num_citations": "13\n", "authors": ["513"]}
{"title": "System-on-chip testability using LSSD scan structures\n", "abstract": " A technology-independent test synthesis tool extends the basic level-sensitive scan design (LSSD) boundary scan methodology. It reuses functional storage elements wherever possible and introduces minimal test logic overhead and delay.", "num_citations": "13\n", "authors": ["513"]}
{"title": "Signature techniques in fault detection and location\n", "abstract": " SIGNATURE TECHNIQUES IN FAULT DETECTION AND LOCATION Shambhu J. Upadhyaya Kewal. K. Saluja Department of Electrical and Computer Engineering University of Newcastle New South Wales, Australia I. INTRODUCTION A. Definitions and Assumptions B. Testing at Various Levels C. Design For Testability II. FAULT DETECTION PROBLEM III. DISCUSSION OF THE DATA COMPACTION TECHNIQUES A. Transition Count And Checksum Testing B. Syndrome Testing C. Spectral Techniques D. Linear Feedback Shift Register (LFSR) E. Parallel Signature Analyzer (PSA) F. Random Testing IV. LFSR AS SIGNATURE ANALYZER A. LFSR as Data Compactor B. LFSR as Test Pattern Generator C. Error Detection Using LFSR D. Effectiveness of LFSR as Signature Analyzer SPECTRAL TECHNIQUES AN D FAULT DETECTION 421 Copyright\u00a9 1985, by Academic Press, Inc. All rights of reproduction in any\u00a0\u2026", "num_citations": "13\n", "authors": ["513"]}
{"title": "Towards a Cyber Ontology for Insider Threats in the Financial Sector.\n", "abstract": " Insider attack has become a major threat in financial sector. Currently, there is no insider threat ontology in this domain and such an ontology is critical to developing countermeasures against insider attacks which are very serious and pervasive security problems. In this paper, we offer a methodology to categorize insider attack suspicions using an ontology we create, which focuses on insider attacks in the banking domain targeting database systems. The scheme we propose takes a suspicion alert as input that triggers the ontology mechanism to analyze the chronology of the events. Our model formulates the ordinary processes that take place in a financial organization and systematically evaluate events in a sequential order. To create the ontology, we use a top-down analysis approach to define a taxonomy and identify the relationships between the taxonomy classes. The ontology is mapped onto the Suggested Upper Merged Ontology (SUMO), Friend of a Friend (FOAF) and Finance ontologies to make it integrable to the systems that use these ontologies and to create a broad knowledge base. It captures masquerade, privilege elevation, privilege abuse and collusion attacks and can be extended to any other novel attack type that may emerge. It classifies an attack using the knowledge base provided and the missing relationships between classes. We validate the ontology showing how description logic works with a given synthetic scenario which is created by banking experts.", "num_citations": "12\n", "authors": ["513"]}
{"title": "Security protection design for deception and real system regimes: A model and analysis\n", "abstract": " In this paper, we model a possible deception system with the explicit purpose of enticing unauthorized users and restricting their access to the real system. The proposed model represents a system designer\u2019s defensive actions against intruders in a way that maximizes the difference between the intruders\u2019 cost and the system designer\u2019s cost of system protection. Under the assumption of a dual entity system, the proposed model shows that intruders differ in behavior depending on the system\u2019s vulnerability at the time of intrusion as well as depending on their own economic incentives. The optimal results of the proposed model provide the system designer with insights on how to configure the level of protection for the two systems.", "num_citations": "12\n", "authors": ["513"]}
{"title": "The effect of spam and privacy concerns on e-mail users' behavior\n", "abstract": " This study aims to examine the effects of both spam and the resulting lack of privacy on users' behavior with respect to e-mail usage. This study reveals that spam e-mail triggers concerns about privacy and in turn, these privacy concerns influence the way users cope with spam or junk mails. Upon receiving spam e-mail, users predominantly exhibit two different behavioral patterns: usage-oriented (passive) and protection-oriented (proactive) behavior. For the purposes of this study, we used data obtained from Pew Internet Research. Logistic regression analysis was performed on the data (N= 588) with the intention of examining how spam negatively affects e-mail usage and degrades life on the Internet. Our results show that:(1) e-mail users' spam experiences have a profound relationship with their privacy concerns;(2) privacy concerns help to mediate the relationship between the spam experience users' protective behavior; and (3) when concerned about privacy as the result of spam, e-mail users tend to exhibit both passive and proactive behaviors.", "num_citations": "12\n", "authors": ["513"]}
{"title": "SWAN: a secure wireless LAN architecture\n", "abstract": " Existing wireless LAN (WLAN) security schemes are few and product specific. While there exist some schemes for information integrity related problems, there are few standard solutions for quality of service and network health maintenance related problems in wireless networks. In this paper we propose an architecture model for secure WLAN that is generic in its design, so that it can easily be incorporated into existing systems at low cost, thus making it feasible and easy to implement. Our secure wireless LAN (SWAN) architecture first describes an admission control mechanism and deals with intrusion detection, malicious behavior detection, and maintaining quality of service and network health. We then introduce a novel infrastructure for an ad-hoc migration scheme (IAMS) to deal with denial of service (DOS) attacks on WLAN, and describe a unique traffic distribution protocol (TDP) for routing traffic when an\u00a0\u2026", "num_citations": "12\n", "authors": ["513"]}
{"title": "A conceptual approach to information security in financial account aggregation\n", "abstract": " An important dimension of mobile computing is the ubiquitous and location-independent availability of data. Aggregation is the ability to electronically access and display personal account information from disparate sources through a single identity. The client financial data is assembled in an organized format providing meaningful summarization and analysis. The prevalent methods of aggregation pose issues in information security and assurance. Utilizing advances in Internet technology such as web services and SOAP coupled with the best of the present approaches to aggregation we can arrive at better solutions to securing the identity and data of aggregation customers. The paper puts forth conceptual solutions to address issues regarding security of user profile and identifying aggregators masquerading as users through processes such as screen scraping.", "num_citations": "12\n", "authors": ["513"]}
{"title": "Functionality defense by heterogeneity: a new paradigm for securing systems\n", "abstract": " The human race has evolved and thrived despite perpetually being attacked by different kinds of viruses, infections, diseases, etc. The body's natural immune system and self healing mechanism as well as the assisted healing systems in human communities has played a major role in this accomplishment. This model has served as an inspiration for the development of a new paradigm for security that we have proposed in this paper. While most security systems focus on defense of a single machine or a cluster of machines we have taken a radically different approach to security. We look at defense of functionalities rather than systems in the traditional sense. In this paper, we propose a new way to look at security called the functionality (such as e-mail server, Web services server, etc) defense by heterogeneity. Further we present an analysis, architecture and an algorithm for defending functionality.", "num_citations": "12\n", "authors": ["513"]}
{"title": "Secure communication in PCS\n", "abstract": " We present a new method to provide secure communication in PCS. Our method is based on Data Encryption Standard (DES) encryption/decryption implemented on the mobile handset. It is shown that the proposed algorithm yields higher security compared to GSM authentication protocol at the expense of using more computational time at call setup, which is still negligible. However, in long run transmissions, the time taken to transmit data using our approach is less than that using GSM approach. Our approach is simple, easy to implement, and it can be very useful in present and future wireless communications.", "num_citations": "12\n", "authors": ["513"]}
{"title": "Modeling the reliability of a class of fault-tolerant VLSI/WSI systems based on multiple-level redundancy\n", "abstract": " A class of fault-tolerant Very Large Scale Integration (VLSI) and Wafer Scale Integration (WSI) schemes, called the multiple-level redundancy, which incorporates both hierarchical and element level redundancy has been proposed for the design of high yield and high reliability large area array processors. The residual redundancy left unused after successfully reconfiguring and eliminating the manufacturing defects can be used to improve the operational reliability of a system. Since existing techniques for the analysis of the effect of residual redundancy on reliability improvement are not applicable, we present a new hierarchical model to estimate the reliability of the systems designed by our approach. Our model emphasizes the effect of support circuit (interconnection) failures on system reliability, leading to more accurate analysis. We discuss two area prediction models, one based on the regular WSI process\u00a0\u2026", "num_citations": "12\n", "authors": ["513"]}
{"title": "Yield analysis of reconfigurable array processors based on multiple-level redundancy\n", "abstract": " Presents and analyzes a new multiple-level redundancy scheme based on hierarchical and element level redundancy for the enhancement of yield and reliability of large area array processors. This scheme can effectively tolerate not only the random defects/faults, but also the clustered defects/faults. The analysis presented here is general in that it takes into account the chip-kill defects occurring in the support circuit area of the array processors and is applicable to a variety of array processors. The authors derive bounds for the support circuit area which will be useful in selecting the most cost-effective redundancy scheme for a given application. The concept of subprocessing element-level redundancy is discussed and it is shown that a combination of subprocessing element-level redundancy with hierarchical redundancy offers significant yield improvements, especially for array processors with large area\u00a0\u2026", "num_citations": "12\n", "authors": ["513"]}
{"title": "Anatomy of secondary features in keystroke dynamics-achieving more with less\n", "abstract": " Keystroke dynamics is an effective behavioral biometric for user authentication at a computer terminal. While many distinctive features have been used for the analysis of acquired user patterns and verification of users transparently, a group of features such as Shift and Comma has always been overlooked and treated as noise. In this paper, we define these normally ignored features as secondary features and investigate their effectiveness in user verification/authentication. By evaluating all the available secondary features, we have found that they contain valuable information that is characteristic of individuals. With a limited number of secondary features, we achieved a promising Equal Error Rate (EER) of 2.94% and Area Under the ROC Curve (AUC) of 0.9940 for classification on a publicly available data set. Surprisingly, this result compares well with the results obtained from primary features by other\u00a0\u2026", "num_citations": "11\n", "authors": ["513"]}
{"title": "A Framework for Understanding Minority Students' Cyber Security Career Interests\n", "abstract": " Recently, a demand toward IT workforce in a cyber security arena is showing an increasing trend. However, underrepresentation of minority workforce in the IT industry is one of reasons for the scarcity of skilled labors in the information security industry. This paper presents various factors that contribute to students\u2019 motivation and interest in a cyber security career by exploring career choice theories. This paper investigates, based on the social cognitive theory, the factors that affect students\u2019 intrinsic motivation to pursue an information security careers. It further suggests a theoretical framework that explains relationships among students\u2019 cyber security career self-efficacy, barriers and cyber security career interests. Finally, this study proposes a research framework that explains minority students\u2019 cyber security career choices.", "num_citations": "11\n", "authors": ["513"]}
{"title": "Mobile computing: Implementing pervasive information and communications technologies\n", "abstract": " Virtual enterprises and mobile computing are emerging as innovative responses to the challenges of doing business in an increasingly mobile and global marketplace. In this rapidly changing environment, it is critical to focus on the fundamental technological aspects that enable the concept of pervasive computing. Mobil Computing: Implementing Pervasive Information and Communication Technologies is designed to address some of the business and technical challenges of pervasive computing that encompass current and emerging technology standards, infrastructures and architectures, and innovative and high impact applications of mobile technologies in virtual enterprises. The various articles examine a host of issues including: the challenges and current solutions in mobile connectivity and coordination; management infrastructures; innovative architectures for fourth generation wireless and Ad-hoc networks; error-free frequency assignments for wireless communication; cost-effective wavelength assignments in optical communication networks; data and transaction modeling in a mobile environment, and bandwidth issues and data routing in mobile Ad-hoc networks. The book is organized around four categories of mobile and pervasive computing and technologies:(1) business and management,(2) architecture,(3) communication, and (4) computing. The first three chapters focus on the business aspects of mobile computing and virtual organization. The fourth chapter lays out an architecture for a fourth generation wireless network. Chapters 5 and 6 are geared towards communication technology, both wireless and wireline. Chapter 7 is a\u00a0\u2026", "num_citations": "11\n", "authors": ["513"]}
{"title": "Automatic insertion of scan structure to enhance testability of embedded memories, cores and chips\n", "abstract": " This paper describes a technology independent test synthesis framework to enhance the testability of embedded memories, cores and chips using extended LSSD boundary scan methodology. Extended LSSD boundary scan reuses functional storage elements and therefore introduces minimal test logic overhead and delay. Automatic insertion of this DFT methodology is particularly challenging since it involves identification and reconfiguration of the functional latches and logic transformations of I/O cells. Experimental results demonstrate the productivity gained using the proposed test synthesis framework as well as the overlead induced by the proposed DFT method.", "num_citations": "11\n", "authors": ["513"]}
{"title": "Surviving advanced persistent threats in a distributed environment\u2013Architecture and analysis\n", "abstract": " Designing robust mission-critical systems demands bringing together fault tolerance and security. The emergence of Advanced Persistent Threats (APT) has further added to the challenge of meeting mission assurance goals. Despite the advances in mission survivability, the existing solutions remain ineffective against APTs. In this paper, we propose a novel survivability architecture against APTs in a distributed environment. It involves tamper-resistant and surreptitious detection and node-to-node verification of suspicious events. The solution aims to identify Attacker Intent, Objectives and Strategies (AIOS) and to design targeted recoveries that promote survivability. Its security strength has been theoretically analyzed, while the performance and scalability aspects are measured via simulation. Our simulations demonstrate high scalability with respect to network size and application runtime and the time\u00a0\u2026", "num_citations": "10\n", "authors": ["513"]}
{"title": "A user behavior monitoring and profiling scheme for masquerade detection\n", "abstract": " Masquerading attack refers to conducting malicious activities on a computer system by impersonating another user. Such attacks are difficult to detect with standard intrusion detection sensors when they are carried out by insiders who have the knowledge of the system. One approach to detect masquerading attacks is to build user profiles and monitor for significant changes in user\u2019s behavior at runtime. Intrusion detectors based on this principle typically have used user command line data to build such profiles. This data does not represent user\u2019s complete behavior in a graphical user interface (GUI)-based system and hence is not sufficient to quickly and accurately detect masquerade attacks. In this chapter, we present a new empirically driven framework for creating a unique feature set for user behavior monitoring on GUI-based systems. For proof-of-concept demonstration, we use a small set of real user behavior\u00a0\u2026", "num_citations": "10\n", "authors": ["513"]}
{"title": "Towards a theory for securing time synchronization in wireless sensor networks\n", "abstract": " Time synchronization in highly distributed wireless systems like sensor and ad hoc networks is extremely important in order to maintain a consistent notion of time throughout the network and to support the various timing-based applications. But, cheating behavior by the participating nodes in the network can severely jeopardize the accuracy of the associated time synchronization process. Despite recent advances in this direction, a key fundamental question still remains unanswered: Is it theoretically feasible to secure distributed time synchronization protocols, given complete (or global) time and time difference information in the network?", "num_citations": "10\n", "authors": ["513"]}
{"title": "AVARE: aggregated vulnerability assessment and response against zero-day exploits\n", "abstract": " In this paper we propose an automated approach for determining recently published vulnerabilities pertinent to the current network/system configuration using the information aggregated from different bug tracking communities. Such vulnerability assessment and indication mechanisms significantly alleviate the system administrator's burden of manual content digging for vulnerabilities in his/her own configuration context. Furthermore, we propose an extensible defense oriented representation schema (EDORS) for vulnerability representation, which is consequently used by the policy engine to generate appropriate IDS signatures. As a result, the generated signatures can be viewed as a preventive stop-gap security measure against zero-day exploits until its patch is released. In the absence of precise detection signatures, we extend our framework to perform forensic analysis on the alerts generated, by\u00a0\u2026", "num_citations": "10\n", "authors": ["513"]}
{"title": "An investigation of risk management issues in the context of emergency response systems\n", "abstract": " Since the September 11, 2001 terrorist attacks, efforts to enhance risk management have taken on increased importance both at the national and state levels. Most current incident response systems do not consider risk as part of the decision making scenario. To effectively mitigate multi-incident coordinated terrorist threats, it is important to consider risk in incident management systems. Based on the review of previous literature, this study proposes a theory-based risk framework for an emergency response system. Proof of concept is provided by applying the framework to two separate existing incident management systems-the urban search-and-rescue system and the biological detection systems.", "num_citations": "10\n", "authors": ["513"]}
{"title": "Intrusion countermeasures security model based on prioritization scheme for intranet access security (emerging concepts category)\n", "abstract": " Access controls and perimeter defenses are essential parts of an enterprise's security armory. However, for a comprehensive intranet security strategy, such defenses alone may not be enough. An enterprise needs mechanisms to analyze alerts and detect real attacks, and policies on how to respond to attacks. A framework for effective response to detection of misuse or attack is a central theme. The focus is towards security of corporate intranets. We first discuss the access control models that are currently being deployed and used at most intranet solutions. Using role based access control as a base, we develop a framework of interaction of various entities in the model. We then propose a prioritization scheme based on cost of impact in case of misuse and business criticality of transactions. The scheme is developed based on categorization and prioritization of risk and vulnerability assessment results for an\u00a0\u2026", "num_citations": "10\n", "authors": ["513"]}
{"title": "Building long term trust in vehicular networks\n", "abstract": " In vehicular networks (VN), the response time is critical, whereas, an autonomous and efficient way of preventing hazardous situations on roads plays an important role in the successful deployment of the system. Trust or reputation models often need to be integrated with inter-vehicle communication protocols in use to avoid selfish or malicious behavior by the vehicles exploiting the system. Existing trust and reputation models for VNs lack the capability of fast and accurate trust management suitable for the ephemeral association of vehicles. In this paper, we design an augmented trust model for VNs by assigning each vehicle a long term trust value. Our model eliminates the overhead of repeated bootstrapping and ensures accountability of the vehicles for incident reporting and other critical actions. The two main features of our framework, viz. a three- party authentication and privacy protocol, and a trust\u00a0\u2026", "num_citations": "9\n", "authors": ["513"]}
{"title": "Emergency Response Coordination and IT Support: Contingency and Strategies\n", "abstract": " This article explores the emergency response coordination phenomena and discusses the response contingency that impacts coordination performance. The authors synthesize the prior literature and examine the critical issues of coordination task analysis, coordinator relationship management, response pre-planning, psychological coping, and information system designs. The multi-dimension lens highlights the interactions and dynamics between tasks, actors, and technologies and suggests the importance of contingency in determining the emergency coordination performance. The paper contributes to the theory development and information system design in the research of emergency response.", "num_citations": "9\n", "authors": ["513"]}
{"title": "On the hardness of approximating the Min-Hack problem\n", "abstract": " We show several hardness results for the Minimum Hacking problem, which roughly can be described as the problem of finding the best way to compromise a target node given a few initial compromised nodes in a network. We give several reductions to show that Minimum Hacking is not approximable to within  where \u03b4 = 1\u2212                    c                  n, for any c < 1/2. We also analyze some heuristics on this problem.", "num_citations": "9\n", "authors": ["513"]}
{"title": "A solution architecture for financial institutions to handle illegal activities: a neural networks approach\n", "abstract": " The banking and financial services industry today relies heavily on the use of networked computerized data systems to manage financial accounts and information on a real-time basis for millions of customers. This underlying technology is a source of a large quantity of information that can be used in the identification and prevention of financial fraud involving the illegal/unauthorized transfer of funds by entities external and internal to the victim financial institution. This paper develops a concept involving the use of neural networks to correlate information from a variety of technological and database sources to identify suspicious account activity.", "num_citations": "9\n", "authors": ["513"]}
{"title": "Defect analysis and a new fault model for multi-port SRAMs\n", "abstract": " Semiconductor memory failures depend on the behavior of its components. This paper deals with testing of defects occurring in the memory cells of a multi-port memory. We also consider the resistive shorts between word/bit lines of same and different ports of the memory. The memory is modeled at the transistor level and analyzed for electrical defects by applying a set of patterns. Not only have existing models been taken into account in our simulation but also a new fault model for the multi-port memory is introduced. The boundaries of failure for the proposed defects are identified.", "num_citations": "9\n", "authors": ["513"]}
{"title": "Yield enhancement of field programmable logic arrays by inherent component redundancy\n", "abstract": " A complete technique that does not use any additional components for enhancing the yield of field-programmable logic arrays (FPLAs) is presented. In this approach, the inherent sparsity (absence of devices at crosspoints) of programmable logic arrays (PLAs) is utilized to mask certain types of manufacturing defects within the unprogrammed FPLAs, thus reclaiming chips which are otherwise discarded. Two categories of faults (called type 1 and type 2) are considered. Type-1 faults, which can be diagnosed a priori, are considered first. After diagnosing type 1 faults, the mask can be reconfigured around the faulty crosspoints. A streamlined bipartite matching algorithm is presented to enhance the speed of this reconfiguration. The uniqueness of the approach is that the programming of an FPLA is formulated as a graph theoretic problem for which a polynomial time solution exists. Type-2 faults in general cannot be\u00a0\u2026", "num_citations": "9\n", "authors": ["513"]}
{"title": "A unified approach to designing fault-tolerant processor ensembles\n", "abstract": " Processor ensembles (abbrev. PEN) form part Not all fault-tolerance schemes are area efficient as illustrated by of parallel processing systems. We present a unified approach to de the fault-tolerance scheme for Binary Trees proposed in [6). It is signing fault-tolerant PENs. Our approach is illustrated by present known that there exists a layout of size O (N) for a Binary Tree with ing fault-tolerant schemes for several commonly used interconnection N nodes (4). But, the fault-tolerant N node Binary Tree resulting from topologies. Our fault-tolerance scheme is shown to be\" area-efficient\u201d, the scheme proposed in (6) requires area equal to O (N Log (N))(3). unlike another fault-tolerance scheme viz. the Diogenes approach (7). The Diogenes scheme7] is also not an area efficient fault tolerant Unlike the reliability analysis of fault-tolerant PENs that have ap scheme. For certain N node PENs the area could increase by a factor peared in the literature our reliability analysis takes into account of O (VN) if the Diogenes approach is used. switch failures along with processor and link failures. We also present a different approach to analyzing the reliability of fault-tolerant PENs. In our analysis we take into account the failure 1. Introduction of the switches required for reconfiguring the system, along with linkProcessor ensembles (PEN) form part of parallel processing sys-and processor failures. This is in contrast to the analysis in (3, 5, 6, 8, 9) where switches are assumed to be fault-free. We believe that our tems. The parallel processing system could be a parallel machine or", "num_citations": "9\n", "authors": ["513"]}
{"title": "Transfer learning in long-text keystroke dynamics\n", "abstract": " Conventional machine learning algorithms based on keystroke dynamics build a classifier from labeled data in one or more sessions but assume that the dataset at the time of verification exhibits the same distribution. Ideally, the keystroke data collected at a session is expected to be an invariant representation of an individual's behavioral biometrics. In real applications, however, the data is sensitive to several factors such as emotion, time of the day and keyboard layout. A user's typing characteristics may gradually change over time and space. Therefore, a traditional classifier may perform poorly on another dataset that is acquired under different environmental conditions. In this paper, we apply two transfer learning techniques on long-text data to update a classifier according to the changing environmental conditions with minimum amount of re-training. We show that by using adaptive techniques, it is possible to\u00a0\u2026", "num_citations": "8\n", "authors": ["513"]}
{"title": "Minimum cost blocking problem in multi-path wireless routing protocols\n", "abstract": " We present a class of Minimum Cost Blocking (MCB) problems in Wireless Mesh Networks (WMNs) with multi-path routing protocols. We establish the provable superiority of multi-path routing protocols over conventional protocols against blocking, node-isolation and network-partitioning type attacks. In our attack model, an adversary is considered successful if he is able to capture/isolate a subset of nodes such that no more than a certain amount of traffic from source nodes reaches the gateways. Two scenarios, viz. (a) low mobility for network nodes, and (b) high degree of node mobility, are evaluated. Scenario (a) is proven to be NP-hard and scenario (b) is proven to be #P-hard for the adversary to realize the goal. Further, several approximation algorithms are presented which show that even in the best case scenario it is at least exponentially hard for the adversary to optimally succeed in such blocking-type\u00a0\u2026", "num_citations": "8\n", "authors": ["513"]}
{"title": "Feasibility of Attacks: What is Possible in the Real World-A Framework for Threat Modeling\n", "abstract": " In this paper we present a new method to assess risks of attacks faced by a network. Our methodology approaches these risks from the perspective of an attacker in order to bridge the gap created by traditional security schemes which approach from the defender\u2019s perspective. These dual perspectives of risk analysis can lead to more effective solutions to security. We describe the various parameters that affect an attack in the real world and use these parameters to analyze the risks of an attack. We also create a model for formally analyzing the risk of an attack using the above parameters. We finally use a case study of jamming attacks on the MAC Layer of the OSI Stack as an illustration and assess the risks for different MAC protocols.", "num_citations": "8\n", "authors": ["513"]}
{"title": "Information assurance, security and privacy services\n", "abstract": " The dual goal of the\" Handbook in Information Systems\" is to provide a reference for the diversity of research in the field by scholars from many disciplines, as well as to stimulate new research. This volume, focusing on Information Assurance, Security and Privacy Services, consists of six sections. In the first part contributors discuss Program Security, Data Security and Authentication, while the second section covers Internet Scourges and Web Security. Parts two and three concentrate on Usable Security and Human-Centric Aspects, along with Security, Privacy and Access Control whereas the final sections of the book examine Economic Aspects of Security, and Threat Modeling, Intrusion and Response.", "num_citations": "8\n", "authors": ["513"]}
{"title": "Effect of process variation on the performance of phase frequency detector\n", "abstract": " In this paper, the effect of process variation in transistors on the phase noise in a conventional CMOS phase frequency detector (PFD) is investigated. When a phase locked loop (PLL) is locked the logical operations of the NAND gates in a PFD can be modeled on the basis of an inverter. Hence the authors consider a CMOS inverter in the TSMC18RF technology and analytically derive expressions for phase noise. Based on the analytical model, the effects of process parameter variations on the PFD are verified through Monte Carlo simulations. The resulting spread obtained for a cumulative variation of the parameters was 1dBc/Hz, indicating that the PFD is quite robust to process parameter variations. Finally, the gates contributing to the phase noise of the PFD are identified", "num_citations": "8\n", "authors": ["513"]}
{"title": "Automatic generation and compaction of March tests for memory arrays\n", "abstract": " Given a set of memory array faults, the problem of computing a compact March test that detects all specified memory array faults is addressed. In this paper, we propose a novel approach in which every memory array fault is modeled by a set of primitive memory faults. A primitive March test is defined for each primitive memory fault. We show that March tests that detect the specified memory array faults are composed of primitive March tests. A method to compact the March tests for the specified memory array faults is described. A set of examples to illustrate the approach is presented. Experimental results demonstrate the productivity gained using the proposed framework.", "num_citations": "8\n", "authors": ["513"]}
{"title": "A new framework for automatic generation, insertion and verification of memory built-in self test units\n", "abstract": " The design and architecture of a memory test synthesis framework for automatic generation, insertion and verification of memory BIST units is presented. We use a building block architecture which results in full customization of memory BIST units. The flexibility and efficiency of the framework are demonstrated by showing that memory BIST units with different architecture and characteristics could be generated, functionally verified and inserted in a short time. Custom memory test algorithms could be loaded in the supported programmable BIST unit and therefore any type of memory test algorithm could be realized.", "num_citations": "8\n", "authors": ["513"]}
{"title": "Fault diagnosis of mixed signal VLSI systems using artificial neural networks\n", "abstract": " This paper presents an approach to the diagnosis of linear and nonlinear analog circuits. The diagnosis methodology is focused on the soft faults in analog circuits. An on-chip white noise generator provides the test stimulus and an artificial neural net (ANN) is used as the response evaluator. Our analysis shows that the white noise relative to the pole zero locations of the circuit transfer function has a significant impact on the classification efficiency of ANN. White noise based stimulus method works for some nonlinear circuits as long as they are constrained to operate in their small signal region of operation. Circuits with strong nonlinearity are difficult to diagnose using the noise stimulus approach. Our results are demonstrated for a linear filter, Schmidt trigger and the phase lock loop (PLL).", "num_citations": "8\n", "authors": ["513"]}
{"title": "Noise generators\n", "abstract": " Noise is a broadbanded signal generated by environmental e ects, such as lightning, or by man-made electrical devices. Two common categories of noise are thermal noise and shot noise. Looney 1] describes thermal noise as an electromotive force generated at the open terminals of a conductor due to the charges bound to thermally vibrating molecules. This type of noise is often referred to as Johnson noise in recognition of the rst observations of the phenomenon 2]. On the other hand, shot noise is associated with the passage of current across a barrier. For instance, a circuit or an appliance that produces electric arcing produces noise. Shot noise was rst described by Schottky using the analogy of a small shot patterning into a container 3]. Noise can be felt in audio systems as a crackle. Noise appears as white or black spots on a television screen.Noise is generally characterized as a source of corruption of information and therefore is treated as an undesired signal. Noise contaminates informational signals to a certain extent by superimposing extrasignal uctuations that assume unpredictable values at each time instantNoise has been studied extensively in the literature because noise reduction is one of the major goals. A more compelling reason for the study of noise is its potential application in real life. These applications encompass biomedical engineering, electronic circuits, communication systems, cryptography, computers, electroacoustics, geosciences, instrumentation, and reliability engineering. This article addresses the various noise generation techniques and implementing them in analog and digital circuit technology and\u00a0\u2026", "num_citations": "8\n", "authors": ["513"]}
{"title": "Path-Based Fault Injection,\"\n", "abstract": " Hong Zhao, Mei-Chen Hsueh, and Ravishankar K. Iyer Coordinated Science Laboratory University of Illinois Urbana, IL 61801", "num_citations": "8\n", "authors": ["513"]}
{"title": "Utilizing spares in multichip modules for the dual function of fault coverage and fault diagnosis\n", "abstract": " Defining a dual role for spare processing elements (PEs) in reliability-challenged processing arrays is the major focus of the paper. The paper also explores a practical way to include reconfiguration hardware in single-package arrays. The implementation of array processor systems may include spare PE's for fault tolerance. These systems typically require a host for fault diagnosis, while the healthy spares sit idle. It is proposed to utilize the idling spare PEs for purposes of fault diagnosis, giving the array the capability of self diagnosis. Fault tolerance must incorporate additional hardware for reconfiguration, and existing plans have not found widespread use in single-package systems due to the extra cost and extra real estate. Multichip modules (MCMs) have the potential to offer fault tolerance with no increase in primary circuit area. It is proposed to contain the reconfiguration hardware in the active substrate of a\u00a0\u2026", "num_citations": "8\n", "authors": ["513"]}
{"title": "Dynamic techniques for yield enhancement of field programmable logic arrays\n", "abstract": " Two techniques are presented to increase the effective yield of field programmable logic arrays (FPLAs). In the first technique, a reconfiguration scheme is proposed to dynamically alter the product-term allocation of the mask PLA onto the product lines of the raw FPLA once a type-two fault is diagnosed. This technique does not require any extra product lines to obtain a usable destination FPLA. The second technique utilizes the often unused product lines within the FPLA. It is shown that once an error is detected during the programming procedure, a product line can always be desensitized from the rest of the FPLA. The intended product term is then simply reprogrammed onto one of the extra product lines.< >", "num_citations": "8\n", "authors": ["513"]}
{"title": "Psychological profiling of hacking potential\n", "abstract": " This paper investigates the psychological traits of individuals\u2019 attraction to engaging in hacking behaviors (both ethical and illegal/unethical) upon entering the workforce. We examine the role of the Dark Triad, Opposition to Authority and Thrill-Seeking traits as regards the propensity of an individual to be interested in White Hat, Black Hat, and Grey Hat hacking. A new set of scales were developed to assist in the delineation of the three hat categories.  We also developed a scale to measure each subject\u2019s perception of the probability of being apprehended for violating privacy laws. Engaging in criminal activity involves a choice where there are consequences and opportunities, and individuals perceive them differently, but they can be deterred if there is a likelihood of punishment, and the punishment is severe.  The results suggest that individuals that are White Hat, Grey Hat and Black Hat hackers score high on the Machiavellian and Psychopathy scales. We also found evidence that Grey Hatters oppose authority, Black Hatters score high on the thrill-seeking dimension and White Hatters, the good guys, tend to be Narcissists. Thrill-seeking was moderately important for White Hat hacking and Black hat hacking. Opposition to Authority was important for Grey Hat hacking. Narcissism was not statistically significant in any of the models. The probability of being apprehended had a negative effect on Grey Hat and Black Hat hacking. Several suggestions will be made on what organizations can do to address insider threats.", "num_citations": "7\n", "authors": ["513"]}
{"title": "Inside the insider\n", "abstract": " We present an overview of two major research projects on the role of monetary incentives and psychological traits in attracting individuals to hacking behavior. In the first study, scenarios were developed for five situations to determine if monetary incentives could be used to influence subjects to obtain healthcare information and to release that information. Approximately 35% to 46% of the 523 survey participants indicated that there is a price, ranging from $1,000 to over $10 million, acceptable for violating HIPAA laws. In the second study, 439 subjects completed a survey that identified the psychological traits that contribute to an individual's propensity to participate in White Hat, Grey Hat, or Black Hat hacking. Preliminary results suggest that individuals that are White Hat, Grey Hat and Black Hat hackers score high on the Machiavellian and Psychopathy scales. We also found evidence that Gray Hatters oppose\u00a0\u2026", "num_citations": "7\n", "authors": ["513"]}
{"title": "You\u2019ve been tricked! a user study of the effectiveness of typosquatting techniques\n", "abstract": " The deceitful practice of Typosquatting involves deliberately registering Internet domain names containing typographical errors that primarily target popular domain names, in an effort to redirect users to unintended destinations or steal traffic for monetary gain. Typosquatting has existed for well over two decades and continues to be a credible threat to this day. While much of the prior work has examined various typosquatting techniques and how they change over time, none have considered how effective they are in deceiving users. In this paper, we attempt to fill in this gap by conducting a user study that exposes subjects to several uniform resource locators (URLs) in an attempt to determine the effectiveness of several typosquatting techniques that are prevalent in the wild. We also attempt to determine if the security education and awareness of cybercrimes such as typosquatting will affect the behavior of Internet\u00a0\u2026", "num_citations": "7\n", "authors": ["513"]}
{"title": "Guest editorial: Introduction to the special issue on emerging security trends for deeply-embedded computing systems\n", "abstract": " Unlike traditional embedded systems, nowadays, emerging computing systems are embedded in every aspect of human lives. These deeply-embedded computing systems often perform extremely sensitive tasks, and in some cases, such as health-care IT, these are life-saving. Thus, in addition to the security threats to traditional embedded systems, emerging deeply-embedded computing systems exhibit a larger attack surface, prone to more serious or life-threatening malicious attacks. These call for revisiting traditional security mechanisms not only because of the new facets of threats and more adverse effects of breaches, but also due to the resource limitations of these often-battery-powered and extremely-constrained computing systems. As such, new trends for providing security for deeply embedded systems are emerging; many of which abandoning use of cryptographic computations or making use of\u00a0\u2026", "num_citations": "7\n", "authors": ["513"]}
{"title": "SESAME: Smartphone enabled secure access to multiple entities\n", "abstract": " In this paper we present a smartphone based architecture to secure user access to web services which require password entry. Our architecture takes advantage of biometric sensors that are present in today's smartphones when authenticating a smartphone user in order to ensure that her identity cannot be masqueraded by anyone else. The user can then access web services using a complex password stored in her smartphone but without having to manually enter the complex password. As a result, the architecture overcomes many security limitations of today's password based authentication approaches, and in particular, resolves the current dilemma associated with the use of complex passwords. In addition, the proposed architecture not only works seamlessly with today's web services since it requires no changes to the existing authentication mechanisms used by the servers, but also can be extended to\u00a0\u2026", "num_citations": "7\n", "authors": ["513"]}
{"title": "Protecting senior citizens from cyber security attacks in the e-health scenario: an international perspective\n", "abstract": " Senior citizens represent a substantial percentage of population around the world and most of them need health care. Health care is becoming expensive around the world. As one of the cost-reduction measures, most of the health care providers are moving the patient's data into electronic format (Electronic Medical Records). Even though this migration is necessary for efficient health care service, it opens up a big can of worms with respect to security and privacy issues. In particular, when the doctors and patients access this medical information through the Internet, there is a large room for cyber security attacks. Given that the senior citizens have less resources (memory, physical energy, technical skills), developing solutions and processes, that will help them in not becoming a victim to attacks, is essential. The problem becomes more interesting when the international component comes into play--eg, senior\u00a0\u2026", "num_citations": "7\n", "authors": ["513"]}
{"title": "Data Model Development for Fire Related Extreme Events-An Activity Theory and Semiotics Approach\n", "abstract": " Post analyses of major extreme events reveal that information sharing is critical for an effective emergency response. The lack of consistent data standards in the current emergency management practice however serves only to hinder efficient critical information flow among the incident responders. In this paper, we adopt a theory driven approach to develop a XML-based data model that prescribes a comprehensive set of data standards for fire related extreme events to better address the challenges of information interoperability. The data model development is guided by third generation Activity Theory and semiotics theories for requirement analyses. The model validation is achieved using a RFC-like process typical in standards development. This paper applies the standards to the real case of a fire incident scenario. Further, it complies with the national leading initiatives in emergency standards (National\u00a0\u2026", "num_citations": "7\n", "authors": ["513"]}
{"title": "Response information interoperability: a development of data standards in the fire incident context\n", "abstract": " Emergency management requires efficient information sharing and exchange among agencies for the smooth operations of intra-and inter-organizational emergency management processes. However, the lack of consistent data standards presents a challenge and hampers the information interoperability. In this paper, we develop a XML-based data model that prescribes a comprehensive set of data standards (semantics and internal structures) for emergency management that attempt to mitigate the information interoperability challenges. The data model is developed using Activity Theory and it is validated through interviews with domain experts. The paper applies the standards in a real case of a fire incident scenario. Further, it complies with the national leading initiatives in emergency standards (National Information Exchange Model) which leverage its implications on the information sharing in emergency context.", "num_citations": "7\n", "authors": ["513"]}
{"title": "Cost model analysis of DFT based fault tolerant SOC designs\n", "abstract": " A lot of emphasis has been placed on the test cost of chips and a variety of models have been proposed in the literature. However they do not include the fault tolerance consideration. Existing models are incomplete by the fact that most do not take into account the costs involved once the chip reaches the market. This paper addresses these limitations by introducing the cost model for a fault tolerant system taking into account the reliability factor of a system. This model can help designers analyze the need for a fault tolerant system and its feasibility in the industry. This paper models the costs involved during the life cycle of a chip. Two case studies using the proposed model are presented in order to substantiate the need to put fault tolerant designs into chips.", "num_citations": "7\n", "authors": ["513"]}
{"title": "Dynamically partitioned test scheduling for SoCs under power constraints\n", "abstract": " Test scheduling increases parallelism of test application and reduces the test cost. In this paper, we present a novel scheduling algorithm for testing embedded core-based System-on-Chips. Given a system integrated with a set of cores and a set of test resources, we construct a set of power constrained concurrent test sets from a power-constrained test compatibility graph (P-TCG). Furthermore, we schedule the tests in a way that dynamically partitions and allocates the tests, and consequently constructs and updates a set of dynamically partitioned PCTS's, and ultimately reduces the test application time. Simulation results show that the proposed approach achieves better performance than existing comparable scheduling algorithms.", "num_citations": "7\n", "authors": ["513"]}
{"title": "Rollback recovery in real-time systems with dynamic constraints\n", "abstract": " Rollback recovery is a backward error recovery technique for recovering from transient faults in computing systems. Real-time systems employing fault tolerance and reconfiguration generally have time-dependent (dynamic) constraints. The author presents a novel rollback point insertion strategy which evaluates the rollback conditions on-line. The technique minimizes both time and space overhead associated with rollback, thereby making it applicable to real-time systems with dynamic constraints. The algorithm presented attains a near-optimum solution in terms of the time spent in saving the states of the system. Details of the simulation conducted to validate the technique are also given. The simulation study has established that the degradation in performance due to using the proposed algorithms is insignificant and the precomputation time is very small for programs that can be represented by general acyclic\u00a0\u2026", "num_citations": "7\n", "authors": ["513"]}
{"title": "A preliminary cyber ontology for insider threats in the financial sector\n", "abstract": " Insider attack has become a major threat in financial sector and is a very serious and pervasive security problem. Currently, there is no insider threat ontology in this domain and such an ontology is critical to developing countermeasures against insider attacks. In this paper, we create an ontology focusing on insider attacks in the banking domain targeting database systems. We define the taxonomy used in this ontology and identify the relationships between the ontology classes. The resulting structure is a domain ontology mapped onto the Suggested Upper Merged Ontology (SUMO), Friend of a Friend (FOAF) and Finance ontologies to make our work integrable to the systems that use these ontologies and to create a broad knowledge base. The attack types we formulate in the ontology are masquerade, privilege elevation, privilege abuse and collusion attacks. Our model could be used to systematically evaluate\u00a0\u2026", "num_citations": "6\n", "authors": ["513"]}
{"title": "An activity theory approach to specification of access control policies in transitive health workflows\n", "abstract": " Access control models are implemented to mitigate the risks of unauthorized access in Electronic Health Records (EHRs). These models provide authorization with the help of security policies, wherein the protected resource is governed by one or more policies that exactly specify what attributes a requester needs to fulfill in order to obtain access. However, due to the increasing complexity of current healthcare system, defining and implementing policies are becoming more and more difficult. In this research-inprogress paper, we present an Activity Theory driven methodology to formalize access control policies that can be used in enforcing patient\u2019s privacy consent in a healthcare setting. In order to account for the transitivity in health workflows, we extend the Activity Theory to include \u201corganizational interconnectedness\u201d within the health workflows.", "num_citations": "6\n", "authors": ["513"]}
{"title": "Messaging model for emergency communication\n", "abstract": " During an emergency, the on-scene communication takes place in the form of dispatch-mediated emergency messages. These messages, identified from the emergency communication reports, follow no standardized format, which render them useless for several other departments. This paper develops a messaging model, compliant with UCORE messaging framework, as follows: First, it determines the structure of the emergency messages. Second, it translates the emergency messages to a standardized format.", "num_citations": "6\n", "authors": ["513"]}
{"title": "Tamper-resistant monitoring for securing multi-core environments\n", "abstract": " Complex software is not only difficult to secure but is also prone to exploitable software bugs. Hence, an intrusion detection system if deployed in user space is susceptible to security compromises. Thus, this \u2018watcher\u2019of other software processes needs to be \u2018watched.\u2019In this paper, we investigate a tamper-resistant solution to the classic problem of \u2018Who watches the watcher?\u2019In our previous work, we investigated this problem in a unicore environment. In this paper, we design a real-time, lightweight, watchdog framework to monitor an intrusion detection system in a multi-core environment. It leverages the principles of graph theory to implement a cyclic monitoring topology. Since our framework monitors intrusion detection systems, the attack surface it has to deal with is considerably reduced. The proposed framework is implemented and evaluated using AMD SimNow simulator. We show that the framework incurs a negligible memory overhead of only 0.8% while sustaining strong, tamper-resistance properties.", "num_citations": "6\n", "authors": ["513"]}
{"title": "Attack scenario recognition through heterogeneous event stream analysis\n", "abstract": " Stealthy, goal-oriented multistage attacks are difficult to detect since they often consist of specific attack steps that do not cause significant variations in the statistical distributions of data streams. We present an approach for attack scenario detection and recognition that is based on analyzing data streams from multiple heterogeneous sensors. Events captured from these sensors are used to generate high-dimensional state vectors that characterize overall system-wide activity. Monitoring the time series of these state vectors through principal component analysis forms the basis of an anomaly detection technique for real-time scenario detection. Data traffic from a real network that emulates a military intelligence network is used to test and validate this approach. Results indicate that our approach is both effective and has low computational requirements, making it a candidate for practical implementation.", "num_citations": "6\n", "authors": ["513"]}
{"title": "Web 2.0: Issues for the design of social net\n", "abstract": " Social Networks have become part of our daily lives and recently there have been a deluge of social networking sites. People are using social networks to keep in touch with friends, family and community. Newer Web 2.0 technologies are encouraging social networking. With social networking sites surfacing every day, we believe it is worthwhile to draw the attention of the reader towards security and privacy related as well as other market and technological factors that should be considered while developing social networks. We discuss in detail the different elements of security and privacy of information that should to be addressed by the developers.", "num_citations": "6\n", "authors": ["513"]}
{"title": "An Investigation of Lessons Learned from Secondary Information of Katrina and Rita Hurricane Disasters: A First Responder Perspective.\n", "abstract": " This study focuses on a first responder perspective of lessons learned from secondary information of Katrina and Rita hurricane disasters. The paper integrates key lessons enunciated in major national news papers and television broadcasts with opinions from domain experts. It then draws a consensus among a large group of domain experts who are first responders, regarding the urgency and importance of the issues. The paper prioritizes the list of issues for better mitigation of disasters in the future. In addition the paper also discusses some of the actions communities in non-disaster areas have taken based on the way the hurricane mitigation was handled on the gulf coast during the Katrina & Rita disasters.", "num_citations": "6\n", "authors": ["513"]}
{"title": "Infastructure Interdependencies Modeling and Analysis-A Review and Synthesis\n", "abstract": " The events of 9/11 and the occurrence of major natural disasters in recent years has resulted in increased awareness and renewed desire to protect critical infrastructure that are the pillars to maintaining what has become normal life in our economy. The problem has been compounded because the increased connectedness between the various sectors of the economy has resulted in interdependencies that allow for problems and issues with one infrastructure to affect other infrastructures. This area is now being investigated extensively after the Department of Homeland Security (DHS) prioritized this issue. There is now a vast extant of literature in the area of infrastructure interdependencies and the modeling of it. This paper presents a synthesis and survey of the literature in the area of infrastructure interdependency modeling methods and proposes a framework for classification of these studies. The framework classifies infrastructure interdependency modeling and analysis methods into four quadrants in terms of system complexities and risks. The directions of future research are also discussed in this paper.", "num_citations": "6\n", "authors": ["513"]}
{"title": "Information theoretic approach to design of emergency response systems\n", "abstract": " Emergency response information systems provide critical support to the disaster management. Despite of the growing interest in this area, the existing research is scanty. A significant limitation is the lack of sound theoretical foundations for emergency management and the information system development. In this paper, the authors adapt Information Theory to explore the theoretical underpinnings of emergency response and discuss the general system design issues.", "num_citations": "6\n", "authors": ["513"]}
{"title": "On extracting consistent graphs in wireless sensor networks\n", "abstract": " Robustness and security of services like localisation, routing and time synchronisation in Wireless Sensor Networks (WSNs) have been critical issues. Efficient mathematical (graph-theoretic) models for these services exist. Since, these services were not designed with robustness and security in mind, new mathematical models are needed to address these issues. In this paper, we propose a practical approach for modelling these services using weighted undirected graphs called Partially Consistent Grounded Graphs (PCGG). In such graphs, malicious behaviour or inaccurate information reporting is modelled as a function that assigns incorrect or inconsistent values (weights) to the edges connecting the corresponding nodes, called inconsistent edges. We formulate two optimisation problems, namely MAX-CON and LARGEST-CON. Given a PCGG, these are the problems of determining the largest induced\u00a0\u2026", "num_citations": "6\n", "authors": ["513"]}
{"title": "Security and dependability issues in location estimation for emergency sensor networks\n", "abstract": " Security and Dependability Issues in Location Estimation for Emergency Sensor Networks - Infoscience English Fran\u00e7ais login Home > Security and Dependability Issues in Location Estimation for Emergency Sensor Networks Infoscience Information Usage statistics Files Security and Dependability Issues in Location Estimation for Emergency Sensor Networks Jadliwala, Murtuza ; Upadhyaya, Shambhu ; Rao, Raghav ; Sharman, Raj Published in: Proceedings of the 4th Workshop on e-Business (WeB 2005) Presented at: 4th Workshop on e-Business (WeB 2005), Las Vegas, Nevada, USA, December 10, 2005 Year: 2005 Laboratories: LCA LDS Record appears in: Scientific production and competences > Archives > I&C - School of Computer and Communication Sciences > LCA - Laboratory for Computer Communications and Applications Infoscience/Research/Archive/IC/IINFCOM Peer-reviewed publications Work \u2026", "num_citations": "6\n", "authors": ["513"]}
{"title": "Real-time intrusion detection with emphasis on insider attacks\n", "abstract": " Securing the cyberspace from attacks is critical to the economy and well being of any country. During the past few years, threats to cyberspace have risen dramatically. It is impossible to close all security loopholes in a computer system by building firewalls or using cryptographic techniques. As a result, intrusion detection has emerged as a key technique for cyber security. Currently there are more than 100 commercial tools and research prototypes for intrusion detection. These can be largely classified as either misuse or anomaly detection systems. While misuse detection looks for specific signs by comparing the current activity against a database of known activity, anomaly detection works by generating a reference line based on the system model and signaling significant deviations from it as intrusions. Both approaches rely on audit trails, which can be very huge. Moreover, conventionally they are off-line\u00a0\u2026", "num_citations": "6\n", "authors": ["513"]}
{"title": "Insecure programming: how culpable is a language's syntax?\n", "abstract": " Vulnerabilities in software stem from poorly written code. Inadvertent errors may creep in due to programmers not being aware of the security implications of their code. Writing secure code is largely a software engineering issue requiring the education of programmers about safe coding practices. Various projects and efforts such as memory usage profiling, meta-compilation and typing proofs that verify correctness of the code at compile-time and run-time provide additional assistance in this regard. We point out that in the context of security, one aspect that is perhaps underrated or overlooked is that vulnerabilities may be inherent in the syntax and grammar of a programming language itself. We leverage on some well-studied problems to show that small syntactic discrepancies may lead to vast semantic differences in programs and in turn, correlate to hard security errors. This technique will helps caution\u00a0\u2026", "num_citations": "6\n", "authors": ["513"]}
{"title": "A Comprehensive Reasoning Framework for Information Survivability\n", "abstract": " Local and wide area network information assurance analysts need current and precise knowledge about their systems activities in order to address the challenges of critical infrastructure protection. In particular, the analyst needs to know in real-time that an intrusion has occurred so that an active response and recovery thread can be created rapidly. Existing intrusion detection solutions are basically after-the-fact, thereby offering very little in terms of damage confinement and restoration of service. Quick recovery is only possible if the assessment scheme has low latency and it occurs in real-time. The objective of this paper is to develop a reasoning framework to aid in the real-time detection and assessment task that is based on a novel idea of encapsulation of owner\u2019s intent. The theoretical framework developed here will help resolve dubious circumstances that may arise while inferring the premises of operations (encapsulated from owner\u2019s intent) by way of examining the observed conclusions resulting from the actual operations of the owner. This reasoning is significant in view of the fact that intrusion signaling is not a binary decision unlike error detection in traditional fault tolerance. Our reasoning framework has been developed by leveraging the concepts of cost analysis and pricing under uncertainty found in economics and finance. Our main result is the modeling of user activity on a computing system as a martingale and the subsequent quantification of the cost of performing a job to enable decision making.", "num_citations": "6\n", "authors": ["513"]}
{"title": "Wireless Network Security\n", "abstract": " \u2022 Increased mobility has become way of life\u2022 W ireless is at the first and last miles\u2022 Presents itself to security problems\u2022 Proper security must be practiced\u2022 A new security culture needs to emerge across the entire Internet user community\u2022 Hacker ethic\u2014destructiveness is inquisitiveness \u201c\u0153 must be resisted", "num_citations": "6\n", "authors": ["513"]}
{"title": "Detecting data leakage from databases on android apps with concept drift\n", "abstract": " Mobile databases are the statutory backbones of many applications on smartphones, and they store a lot of sensitive information. However, vulnerabilities in the operating system or the app logic can lead to sensitive data leakage by giving the adversaries unauthorized access to the app's database. In this paper, we study such vulnerabilities to define a threat model, and we propose an OS-version independent protection mechanism that app developers can utilize to detect such attacks. To do so, we model the user behavior with the database query workload created by the original apps. Here, we model the drift in behavior by comparing probability distributions of the query workload features over time. We then use this model to determine if the app behavior drift is anomalous. We evaluate our framework on real-world workloads of three different popular Android apps, and we show that our system was able to detect\u00a0\u2026", "num_citations": "5\n", "authors": ["513"]}
{"title": "Kidemonas: the silent guardian\n", "abstract": " Advanced Persistent Threats or APTs are big challenges to the security of government organizations or industry systems. These threats may result in stealth attacks, but if the attack is confronted before the attacker end goal has been achieved, the attackers could become aggressive by changing the mode of attack or by resorting to some form of contingency plan, which might cause unexpected damage. Therefore, the attack detection and the notification to the system administrator should be done surreptitiously. This paper presents an architecture, called Kidemonas, to silently detect the threat and secretly report it to the user or the system administrator. This way the attacker is deceived into carrying out the attack, without sending any clear signal so that the defender can buy time to develop countermeasures to deal with the attack. We consider several attack scenarios and perform a security analysis to demonstrate the features of Kidemonas.", "num_citations": "5\n", "authors": ["513"]}
{"title": "Investigating the antecedents of healthcare workers' perceptions of organizational resilience in hospitals\n", "abstract": " Resilience has been studied extensively in humans but less attention has been paid to organizational resilience in hospital organizations. This research aims to offer solutions for effective management of extreme events in hospitals by investigating the antecedents of healthcare workers' perceptions of organizational resilience. The theory of organizational resilience was used to examine the role of information privacy, information security, information access, leadership during emergencies, employee empowerment, and emergency training. A total of 402 survey responses were analyzed using Partial Least Squares regression. This is a research in progress.", "num_citations": "5\n", "authors": ["513"]}
{"title": "An Activity Theory Approach to Leak Detection and Mitigation in Personal Health Information (PHI)\n", "abstract": " The migration to Electronic Health Records (EHR) has raised issues with respect to security and privacy. One such issue that has become a concern for the healthcare providers, insurance companies and pharmacies is Patient Health Information (PHI) leak. Borrowing from Document Control Domain (DCD) literature, in this paper, we develop a methodology for detection and mitigation of PHI leaks by employing Activity Theory to elucidate the complex activities in the transitive workflow.", "num_citations": "5\n", "authors": ["513"]}
{"title": "Emergency response system design: An examination of emergency communication messages\n", "abstract": " The current state of emergency communication is dispatch-mediated i.e. the messages from the scene are directed to responders and agencies through the dispatch. Emergency dispatch provides essential support to emergency responders during emergencies. However, there are several problems associated with the dispatch-mediated communication. Utilizing IBM\u2019s message modeling concept, we develop a messaging model to provide support for computer-mediated communication (CMC) systems.", "num_citations": "5\n", "authors": ["513"]}
{"title": "A multi-step simulation approach toward secure fault tolerant system evaluation\n", "abstract": " As new techniques of fault tolerance and security emerge, so does the need for suitable tools to evaluate them. Generally, the security of a system can be estimated and verified via logical test cases, but the performance overhead of security algorithms on a system needs to be numerically analyzed. The diversity in security methods and design of fault tolerant systems make it impossible for researchers to come up with a standard, affordable and openly available simulation tool, evaluation framework or an experimental test-bed. Therefore, researchers choose from a wide range of available modeling-based, implementation-based or simulation-based approaches in order to evaluate their designs. All of these approaches have certain merits and several drawbacks. For instance, development of a system prototype provides a more accurate system analysis but unlike simulation, it is not highly scalable. This paper\u00a0\u2026", "num_citations": "5\n", "authors": ["513"]}
{"title": "ASFALT: a simple fault-tolerant signature-based localization technique for emergency sensor networks\n", "abstract": " We consider the problem of robust node deployment and fault-tolerant localization in wireless sensor networks for emergency and first response applications. Signature-based localization algorithms are a popular choice for use in such applications due to the non-uniform nature of the sensor node deployment. But, random destruction/disablement of sensor nodes in such networks adversely affects the deployment strategy as well as the accuracy of the corresponding signature-based localization algorithm. In this paper, we first model the phenomenon of sensor node destruction as a non-homogeneous Poisson process and derive a robust and efficient strategy for sensor node deployment based on this model. Next, we outline a protocol, called Group Selection Protocol, that complements current signature-based algorithms by reducing localization errors even when some nodes in a group are destroyed. Finally, we\u00a0\u2026", "num_citations": "5\n", "authors": ["513"]}
{"title": "A trust assignment model based on alternate actions payoff\n", "abstract": " The human component is a determining factor in the success of the security subsystem. While security policies dictate the set of permissible actions of a user, best practices dictate the efficient mode of execution for these actions. Unfortunately, this efficient mode of execution is not always the easiest to carry out. Users, unaware of the implications of their actions, seek to carry out the easier mode of execution rather than the efficient one, thereby introducing a certain level of uncertainty unacceptable in high assurance information systems. In this paper, we present a dynamic trust assignment model that evaluates the system\u2019s trust on user actions over time. We first discuss the interpretation of trust in the context of the statement \u201cthe system trusts the users\u2019 actions\u201d as opposed to \u201cthe system trusts the user.\u201d We then derive the intuition of our trust assignment framework from a game-theoretic model, where trust\u00a0\u2026", "num_citations": "5\n", "authors": ["513"]}
{"title": "Part 1: Special Issue on Secure Knowledge Management\n", "abstract": " KNOWLEDGE management is the methodology for sys-tematically gathering, organizing, and disseminating information. It essentially consists of processes and tools to effectively capture and share data as well as use the knowledge of individuals within an organization. Knowledge-management systems (KMSs) promote sharing information among employees and should contain security features to prevent any unauthorized access. Security is becoming a major issue revolving around KMS. Security methods may include authentication or passwords, cryptography programs, intrusion-detection systems, or access-control systems. Issues include insider threat (protecting from malicious insiders), infrastructure protection (securing against subversion attacks) and establishing correct policies and refinement and enforcement. Furthermore, KMS content is much more sensitive than a raw data stored in databases and\u00a0\u2026", "num_citations": "5\n", "authors": ["513"]}
{"title": "Malware and antivirus deployment for enterprise security\n", "abstract": " Threats to information security are pervasive, originating from both outside and within an organization. The history of computer security is dotted with the tales of newer methods of identification, detection, and prevention of malware, only to be followed by a new set of threats that circumvent those safeguards. The explosive growth of the Internet and wide availability of toolsets and documentation exacerbates this problem by making malware development easy. As blended threats continue to combine multiple types of attacks into single and more dangerous payloads, newer threats are emerging. Phishing, pharming, spamming, spoofing, spyware, and hacking incidents are increasing at an alarming rate despite the release of breakthrough security defense products. A multi-layered, integrated approach using different security products in conjunction with well-defined security policies and antivirus software will form\u00a0\u2026", "num_citations": "5\n", "authors": ["513"]}
{"title": "GSWLAN: a new architecture model for a generic and secure wireless LAN system\n", "abstract": " Existing WLAN security schemes are few and product specific. While there exist some schemes for dealing with problems relating to information integrity, there are hardly any standard solutions for security problems relating to quality of service and network health maintenance in wireless networks. In the absence of strong standards, the existing approach to general WLAN security is vendor specific. We propose a low cost and generic secure WLAN architecture which can be implemented on WLANs comprising of access points from different vendors. A behavior monitoring scheme which validates the model has been developed and tested.", "num_citations": "5\n", "authors": ["513"]}
{"title": "Minimizing concurrent test time in SoC's by balancing resource usage\n", "abstract": " We present a novel test scheduling algorithm for embedded core-based SoC's. Given a system integrated with a set of cores and a set of test resources, we select a test for each core from a set of alternative test sets, and schedule it in a way that evenly balances the resource usage, and ultimately reduce the test application time. Furthermore, we propose a novel approach that groups the cores and assigns higher priority to those with smaller number of alternate test sets. In addition, we also extend the algorithm to allow multiple test sets selection from a set of alternatives to facilitate testing for various fault models.", "num_citations": "5\n", "authors": ["513"]}
{"title": "Implementing degradable processing arrays\n", "abstract": " Today's burgeoning multimedia and network technology require large high-performance processing arrays. Conventional chip packaging and board-level integration cannot meet operating speeds in these high-performance systems. In addition, large systems require technologies that provide reliable, single packaging. The main focus of this work is to demonstrate the feasibility of building reliable systems using MCMs, with special attention to implementation issues. A simple approach employing the gracefully degradable paradigm facilitates the continued operation of a faulty array that the system would otherwise discard. The scheme identifies and makes provision to extract healthy subarrays that retain original topology, and requires a small number of transistor switches. By exploiting various implementation options of MCMs, the technique poses no increase to primary circuit area. We must identify the topology of\u00a0\u2026", "num_citations": "5\n", "authors": ["513"]}
{"title": "A prediction model of privacy control for online social networking users\n", "abstract": " With the growing popularity of social network sites (SNS), organizations have started to leverage them for encouraging both personal and professional data sharing. However, inherent privacy problems in social networks have become a concern for organizations deploying them. So companies have started investing in systems for evaluating employees\u2019 behaviors on SNSs. In evaluating employees\u2019 behaviors on SNSs, this study aims at developing a mechanism for learning users\u2019 behaviors on SNS and predicting their control of privacy on SNS. Privacy prediction is based on the revelation of actual privacy characteristics of users through the analysis of their SNS usage patterns. Using the Design Science research methodology, this study presents the design and instantiation of a prediction model that is trained using survey data and SNS data of graduate students from a prominent Northeastern University\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "Complexity of insider attacks to databases\n", "abstract": " Insider attacks are one of the most dangerous threats to an organization. Unfortunately, they are very difficult to foresee, detect, and defend against due to the trust and responsibilities placed on the employees. In this paper, we first define the notion of user intent, and construct a model for the most common threat scenario used in the literature that poses a very high risk for sensitive data stored in the organization's database. We show that the complexity of identifying pseudo-intents of a user is coNP-Complete in this domain, and launching a harvester insider attack within the boundaries of the defined threat model takes linear time while a targeted threat model is an NP-Complete problem. We also discuss about the general defense mechanisms against the modeled threats, and show that countering against the harvester insider attack model takes quadratic time while countering against the targeted insider attack\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "Data de-duplication and event processing for security applications on an embedded processor\n", "abstract": " Network security schemes generally deploy sensors and other network devices which generate huge volumes of data, overwhelming the underlying decision making algorithms. An example is corporate networks employing intrusion detection systems where there is a deluge of alert data, confounding the computations involved in sensor information fusion and alert correlation. One way to obtain fast and real-time responses is to preprocess such data to manageable sizes. In this paper, we show that data de-duplication using computationally efficient fingerprinting algorithms can provide real-time results. We present an algorithm which utilizes Rabin Fingerprinting/hashing scheme for the purpose of data de-duplication. We have implemented this algorithm on Intel Atom, which is a powerful, energy efficient embedded processor. Our study is intended to show that the relatively low performing embedded processors\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "Design principles for emergency collaborative systems: A situation awareness study of Buffalo plane crash\n", "abstract": " Due to differences in governance structure, training, applicable polices, legal requirements and culture, the nature of operations vary based on agency, county, population, leadership, etc. This leads to serious challenges during multi-agency response to emergencies. The different agencies are required to work together to effectively and efficiently respond to an emergency incident. The paper contributes to research in areas of inter-agency collaboration and emergency management. With the help of case study, this paper aims to explore factors that impact inter-agency collaboration to generate design principles that are useful to designing better systems to mitigate critical incidents. In addition, with the help of interviews with four experts (two fire chiefs and two dispatchers) and raw incident communication reports, we identify system, communication, information, and interoperability issues.", "num_citations": "4\n", "authors": ["513"]}
{"title": "Detecting cheating aggregators and report dropping attacks in Wireless Sensor Networks\n", "abstract": " This chapter focuses on an important, challenging and yet largely unaddressed problem in Wireless Sensor Networks (WSN) data communication: detecting cheating aggregators and malicious/selfish discarding of data reports en route to the Base Stations (BSs). If undetected, such attacks can significantly affect the performance of applications. The goal is to make the aggregation process tamper-resistant so that the aggregator cannot report arbitrary values, and to ensure that silent discarding of data reports by intermediate en-route nodes is detected in a bounded fashion. In our model, individual node readings are aggregated into data reports by Aggregator Nodes or Cluster Heads and forwarded to the BS. BS performs a two-stage analysis on these reports:(a) Verification through attached proofs,(b) Comparison with Proxy Reports for ensuring arrival accuracy. Proofs are non-interactive verifiers sent with reports\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "On the Two Factors Affecting Information Systems Success in the Extreme Event Context\n", "abstract": " This study examines the impact of perceived risk and organizational resilience on information systems success process in the context of hospitals. The research model was tested with the data collected from three of the hospitals in the Western New York area that were affected by a major snowstorm that was labeled as a federal disaster. The analysis and results appear. Theoretical and practical implications of this study and future research directions are discussed.", "num_citations": "4\n", "authors": ["513"]}
{"title": "Defect analysis and defect tolerant design of multi-port srams\n", "abstract": " Multi-port SRAMs are often implemented using static random access memory (SRAM) due to its fast operation and the ability to support multiple read and write operations simultaneously, thus increasing data throughput in embedded systems and meeting the expected demands of parallel or pipelined microprocessors. With the continuous scaling of transistor feature size, designing low power robust memories and investigating their failure characteristics become critical. In this paper, we study the defects occurring in the multi-port SRAM cells. The memory is modeled at the transistor level and analyzed for electrical defects by applying a set of test patterns. Not only have existing models been taken into account in our simulation but also a new fault model, namely, simultaneous deceptive destructive read fault for the multi-port memory is introduced. In addition, we extend our study to the defect tolerant design\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "Inferring sources of leaks in document management systems\n", "abstract": " A document management system (DMS) provides for secure operations on a distributed repository of digital documents. This paper presents a two-phase approach to address the problem of locating the sources of information leaks in a DMS. The initial monitoring phase treats user interactions in a DMS as a series of transactions, each involving content manipulation by a user; in addition to standard audit logging, relevant contextual information and user-related metrics for transactions are recorded. In the detection phase, leaked information is correlated with the existing document repository and context information to identify the sources of leaks. The monitoring and detecting phases are incorporated in a forensic extension module (FEM) to a DMS to combat the insider threat.", "num_citations": "4\n", "authors": ["513"]}
{"title": "On the hardness of eliminating cheating behavior in time synchronization protocols for sensor networks\n", "abstract": " Wireless Sensor Networks are fast gaining popularity for use in a variety of remote sensing, emergency monitoring and information collection applications. Time synchronization in such highly distributed systems is important in order to maintain a global notion of time throughout the network and to support the underlying applications. But, cheating behavior by malicious nodes can severely jeopardize the accuracy of the time synchronization service associated with the network and the related time-based applications. Existing literature on the problem of detection and elimination of cheating behavior in distributed time synchronization protocols has very little or no fundamental theoretical analysis of the basic problem itself. Absence of sound theoretical models and analysis of time synchronization protocols in such networks has left a number of questions unanswered: How hard is it to detect and eliminate cheating or inconsistent behavior in time synchronization protocols given complete (local time and time difference) information? Do efficient algorithms for doing the same exist and if they do, is there a performance guarantee on the solution quality? In this paper, we attempt to answer these questions. We first present a practical graph-theoretic model, called Time Difference Graphs (TDG), for the network and formulate the time synchronization problem as a Constraint Satisfaction Problem (CSP) in such graphs. We then show that efficiently eliminating cheating behavior (or inconsistency causing nodes) in TDGs is NP-hard. Furthermore, we show that this problem is hard even for a special case of TDG, namely, a completely connected TDG. Moreover\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "On the hardness of minimum cost blocking attacks on multi-path wireless routing protocols\n", "abstract": " This paper demonstrates the provable superiority of multi-path routing protocols over other conventional protocols in Wireless Mesh Networks (WMNs) against blocking, node- isolation and network-partitioning type-attacks. Though the underlying network model is of a WMN with mobile nodes, the results in this paper are equally applicable to other types of wireless data networks. The adversarial objective is to isolate a subset of network nodes through minimal cost optimal blocking of certain number of paths in the network (or partitioning the network). If less than a certain threshold of traffic from such node(s) reaches the routers, the adversary is successful. Two scenarios viz. (a) low mobility for network nodes, and (b) high degree of node mobility, are evaluated. Scenario (a) is proven to be NP-hard and scenario (b) is proven to be #P-hard for the adversary to achieve the goal. Further, several approximation\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "Metrics for information security-a literature review\n", "abstract": " It is important to know how vulnerable systems are for a wide variety of reasons. Information Systems managers have the duty to advise senior management of the level of risks faced by the information systems. Therefore an assessment of the level of risk is necessary. Research work in this area is in its infancy. Further, the efforts are varied and deal with different aspects of the issue. There is no coherent approach. This paper provides a review of the literature and will be presenting a framework for analysis and development of metrics for security at the conference.", "num_citations": "4\n", "authors": ["513"]}
{"title": "Control constrained resource partitioning for complex SoCs [intra-chip wireless interconnects]\n", "abstract": " When moving into the billion-transistor era, the wired interconnects used in conventional SoC test control models are rather restricted in not only system performance, but also signal integrity and transmission with continued scaling of feature size. On the other hand, recent advances in silicon integrated circuit technology are making possible tiny low-cost transceivers to be integrated on chip. Based on the recent development in \"radio-on-chip\" technology, a new distributed multihop wireless test control network has been proposed. Under the multilevel tree structure, the system optimization is performed on control constrained resource partitioning and distribution. Several system design issues such as radio-frequency nodes placement, clustering and routing problems are studied, with the integrated resource distribution including not only the circuit blocks to perform testing, but also the on-chip radio-frequency nodes\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "A resource balancing approach to soc test scheduling\n", "abstract": " We present a novel test scheduling algorithm for embedded core-based SoCs. We formulate the test scheduling as a shortest path problem with the feature of evenly balanced resource usage. Improvements to the basic algorithm are sought by core-grouping and all-permutation scheduling. We also extend the algorithm to allow multiple test sets selection to facilitate the testing for various fault models. A simulation study is performed to quantify the benefits of our new scheduling approach.", "num_citations": "4\n", "authors": ["513"]}
{"title": "Design and analysis of an integrated checkpointing and recovery scheme for distributed applications\n", "abstract": " An integrated checkpointing and recovery scheme which exploits the low latency and high coverage characteristics of a concurrent error detection scheme is presented. Message dependency, which is the main source of multistep rollback in distributed systems, is minimized by using a new message validation technique derived from the notion of concurrent error detection. The concept of a new global state matrix is introduced to track error checking and message dependency in a distributed system and assist in the recovery. The analytical model, algorithms and data structures to support an easy implementation of the new scheme are presented. The completeness and correctness of the algorithms are proved. A number of scenarios and illustrations that give the details of the analytical model are presented. The benefits of the integrated checkpointing scheme are quantified by means of simulation using an object\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "Simulation analysis of a dynamic checkpointing strategy for real-time systems\n", "abstract": " The performance of a fault tolerant real-time system is measured by its ability to meet deadlines in the presence of errors. Checkpointing and rollback recovery is an effective technique to tolerate transient and intermittent faults in real-time systems. Several strategies exist for checkpointing which can be broadly classified as static and dynamic. In static checkpointing, the checkpointing intervals are determined before the program execution and remain fixed until the program terminates. On the other hand, in a dynamic checkpointing strategy, the checkpointing interval is varied dynamically during program execution based on certain criteria. This paper presents a comparative study of the performance of real-time systems adopting various rollback recovery strategies. A new dynamic checkpointing strategy is presented, which integrates the deadline constraints and variable error rate in determining the checkpointing\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "Analysis of a fault-tolerance scheme for processor ensembles\n", "abstract": " The authors analyze a locally redundant scheme (LR scheme) for designing fault-tolerant processor ensembles. A switching structure for reconfiguration is presented, and a detailed model for the yield analysis off the LR scheme that takes into account processor, switch, and link failures is developed. A negative binomial distribution is used for the yield statistics, as it best fits the empirical data. This model is used to compare the yields (with and without fault tolerance) of some architectural topologies. A dynamic analysis of the effect of residual redundancy on the improvement of operational system reliability is presented. The analysis reveals an appreciable improvement in the yield and operational system-reliability when the LR scheme is used. This analysis includes the reliability of switches and links, unlike previous analyses of fault-tolerant schemes. The empirical results show that ignoring switch reliability could\u00a0\u2026", "num_citations": "4\n", "authors": ["513"]}
{"title": "BIST PLAs, pass or fail\u2014a case study\n", "abstract": " Numerous Built-In Self Testing (BIST) designs now exist for the testing of Programmable Logic Arrays (PLA), but their practical usefulness has not been studied. In this paper, we implement and compare several BIST designs using a common methodology of implementation. We also perform an yield analysis to characterize the yield degradation due to the BIST design methodology. Our preliminary findings of this work is that BIST approach results in considerable degradation of yield, and therefore may not be suitable as a test vehicle for PLAs.", "num_citations": "4\n", "authors": ["513"]}
{"title": "Cybersecurity interventions for teens: Two time-based approaches\n", "abstract": " Contribution: Intervention effectiveness is shown to vary in its influence on teenagers' outcomes with cybersecurity problem-solving and engagement. In-depth, high-intensity types of intervention may be more effective for female students. Background: Instructional interventions are being developed to address both the critical shortage in cybersecurity talent and gender gaps in the cyber workforce. These interventions need rigorous evaluation. Specific types of instructional strategies are particularly effective for STEM learning. Also, gender differences are found in the benefit students derive from certain instructional methods. An important question is whether certain instructional methods are particularly effective for cybersecurity learning, and consistent in both male and female students. Research Questions: Do cybersecurity interventions affect problem-solving, cybersecurity engagement, and/or cybersecurity self\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "A comprehensive model for elucidating advanced persistent threats (apt)\n", "abstract": " Advanced Persistent Threats or APTs are not only a matter of concern for the government organizations but also to the industries as well. Recent studies show that many companies have suffered financial damage at the face of an attack by an APT and sustained damage to their reputation and brand value. Detecting the attack and building a defense system against APT is difficult due to the fact that these attacks are very stealthy and targeted. This paper presents a holistic approach via a parameterized model to identify an APT and then to review and assess the vulnerabilities, attacker resources and probable targets. This model would help defenders to build a strong defense mechanism for already seen attacks as well as for future attacks. Several case studies are conducted to support the efficacy of our model.", "num_citations": "3\n", "authors": ["513"]}
{"title": "A user study of the effectiveness of typosquatting techniques\n", "abstract": " The nefarious practice of cyber typosquatting involves deliberately registering Internet domain names containing typographical errors that primarily target popular domain names in an effort to steal their traffic for monetary gain. Typosquatting has existed for well over two decades and continues to be a credible threat to this day. In this work, we discuss the results of a user study that exposes subjects to several uniform resource locators (URLs) in an attempt to determine the effectiveness of several typosquatting techniques that are prevalent in the wild. We also attempt to determine if security education and awareness of cybercrimes such as typosquatting will affect the behavior of Internet users.", "num_citations": "3\n", "authors": ["513"]}
{"title": "Deception-based survivability\n", "abstract": " Critical systems in current threat landscape demand more than just fault-tolerance and security; they demand survivability. Survivability is the ability of a system to continue delivering essential services during attacks, faults or accidents. This is usually accomplished via a four-layered defense setting that consists of prevention, detection, recovery and adaption. As evident by the recent incline in advanced persistent threats, the existing defense systems are in a dire need of evolution. This new generation of defense systems should be smart, adaptive and unlike traditional systems, stay ahead of the malicious actors. Many researchers have started to consider deception as a means to this end. Deception involves misrepresenting or hiding information in order to manipulate a user\u2019s actions. When engrafted in the prevention and detection layers, deception can help trace attacker intent, objective and strategies\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "Emerging issues for education in E-discovery for electronic health records\n", "abstract": " In order to provide a foundation for education on e-discovery and security in Electronic Health Record (EHR) systems, this paper identifies emerging issues in the area. Based on a detailed literature review it details key categories: Development in EHR, E-discovery policy and strategy, and Security and privacy in EHR and also discusses e-discovery issues in cloud computing and big data contexts. This may help to create a framework for potential short course-design on e-discovery and security in the healthcare domain.", "num_citations": "3\n", "authors": ["513"]}
{"title": "An exploration of security and privacy behavior of elders on the internet and comparison with younger adults\n", "abstract": " One of the fastest growing demographics to utilize the Web as part of their everyday life is the group of older adults who are aged 55 and above. The rising adoption of the Internet by older adults has resulted in both security and privacy problems for them. In this paper we develop a model to focus on the behavioral side of the security and privacy discussion among elders. For comparative purposes we also test the model with student subjects. We find considerable differences in the results between the elders and young adults and suggest potential issues that can be tested for understanding the differences.", "num_citations": "3\n", "authors": ["513"]}
{"title": "Secure Proactive Recovery\u2013a Hardware Based Mission Assurance Scheme\n", "abstract": " Mission Assurance in critical systems entails both fault tolerance and security. Since fault tolerance via redundancy or replication is contradictory to the notion of a limited trusted computing base, normal security techniques cannot be applied to fault tolerant systems. Thus, in order to enhance the dependability of mission critical systems, designers employ a multi-phase approach that includes fault/threat avoidance/prevention, detection and recovery. Detection phase is the fallback plan for avoidance/prevention phase, as recovery phase is the fallback plan for detection phase. However, despite this three-stage barrier, a determined adversary can still defeat system security by staging an attack on the recovery phase. Recovery being the final stage of the dependability life-cycle, unless certain security methodologies are used, full assurance to mission critical operations cannot be guaranteed. For this reason, we propose a new methodology, viz. secure proactive recovery that can be built into future mission-critical systems in order to secure the recovery phase at low cost. The solution proposed is realized through a hardware-supported design of a consensus protocol. One of the major strengths of this scheme is that it not only detects abnormal behavior due to system faults or attacks, but also secures the system in case where a smart attacker attempts to camouflage by playing along with the predefined protocols. This sort of adversary may compromise certain system nodes at some earlier stage but remain dormant until the critical phase of the mission is reached. We call such an adversary The Quiet Invader. In an effort to minimize overhead\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "An exploration of unintended online private information disclosure in educational institutions across four countries\n", "abstract": " Advanced Google search queries can be used to extract sensitive information from Websites that can potentially be exploited for malice. The purpose of this paper is to identify the existence of unintended private information disclosure possibilities in educational institutions across four countries through advanced search techniques such as Google Hacking. Google Hacking is a technique of retrieving information that may not be intended for public retrieval using advanced Google search operators. The focus is on the country level comparison of Excel files which contain unintended private information. For this exploratory study, we used relevant Google hacking search queries to retrieve Excel spreadsheet files which contain personally identifiable information from higher education institutions of India, US, South Korea, and Romania. Our analysis of these retrieved files establishes that each country shows (1) a\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "Design considerations for high performance RF cores based on process variation study\n", "abstract": " RF circuits play a vital role in high data rate communication systems. Although at the design stage several considerations are made to ensure that the designed circuit functions as per desired specifications, the effect of process variations on the circuit\u2019s performance is less understood. The parametric variations arising from the various stages of fabrication play a significant role in determining the device characteristics. In this paper, in order to analyze the effect of process variations, we consider a bottom\u2013up approach beginning at the component level for active and passive elements and then move to the circuit level in an RF circuit consisting of both analog and digital components. We take Low Noise Amplifier (LNA) and a Phase Frequency Detector (PFD) which is one of the important building blocks of a Phase Locked Loop (PLL) as case studies for circuit level analysis. In the case of LNA, the performance\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "Environment-A ware Trusted Data Delivery in Multipath Wireless Protocols\n", "abstract": " Current multipath protocols for Multi-Hop Wireless Networks (MWNs) use hop-count as the default route selection criteria. Route selection should also consider network and link conditions. We propose a network-environment-aware trust-based route selection framework for MWNs that makes informed and adaptive route-selection decisions. A node quantifies trust values for its neighboring nodes and for the routes that pass through it. The trust metric adjusts to varying network conditions and quick convergence of the protocol implies it works well in mobility scenarios. Glomosim simulations demonstrate throughput improvement over conventional multipath protocols under congestion, link failure and route unreliability scenarios.", "num_citations": "3\n", "authors": ["513"]}
{"title": "QoS-LI: QoS Loss Inference in Disadvantaged Networks\n", "abstract": " Quality of Service (QoS) of disadvantaged networks is considered from a purely network standpoint in existing works. Adversarial intervention in such networks is not analyzed, nor is it possible to infer if a QoS loss is benign or otherwise. In this paper, we present a QoS loss inference module, where the end nodes can infer the nature of a QoS loss in a non-intrusive manner. The objective of this work is to develop a conceptual framework to model the inference module, and investigate its integration into existing platforms. We abstract the problem of link selection (as opposed to route selection) in disadvantaged networks as a resource selection problem, and apply a game theoretic model to set limits on the rate of convergence. Using this convergence rate, our loss inference model can distinguish between adversarial network manipulation and benign network loss. Such a module will help manage the operation of\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "A framework for examining skill specialization, gender inequity, and career advancement in the information security field\n", "abstract": " This paper presents an ongoing research project that examines career advancement barriers to women information security professionals. The study proposes to identify the skill sets critical for success in the information security area and to examine gender differences in specialized skill sets. The paper provides a brief review on IT workforce studies with a special focus on gender inequity problems. The research design and analytical methods are also presented.", "num_citations": "3\n", "authors": ["513"]}
{"title": "A new SoC test architecture with RF/wireless connectivity\n", "abstract": " When moving into the billion-transistor era, the direct or bus interconnects in conventional SoC test control models are rather restricted in not only system performance, but also signal integrity and transmission with continued scaling of the feature size. Recent advances in silicon integrated circuit technology are making possible tiny low-cost transceivers to be integrated on chip. In this paper, we propose a new distributed multihop wireless test control network based on the recent development in \"radio-on-chip\" technology. Under the multilevel tree structure, the system optimization is performed on control constrained resource partitioning and distribution. Several challenging system design issues, such as RF nodes placement, clustering, and routing are studied, with the integrated resource distribution and system optimization on TAM design and test scheduling. Experimental results show that the proposed algorithm\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "Recovery schemes for mesh arrays utilizing dedicated spares\n", "abstract": " In this paper, new schemes are presented in which the spare nodes of a permanent-fault-tolerant processing array are utilized in their idling state to aid in an online transient-error recovery process. Though spares-based methods are well-known solutions to permanent-fault tolerance, the cost of these solutions and the idling spare capacity during normal operation have limited their widespread use. Manufacturers must be offered fault tolerance solutions which provide useful work at all times. We propose the enhanced utility of spares-based methods by commissioning idling spares (those spares remaining after fabrication and subsequent replacement of faulty units) to perform transient-error recovery tasks. Our scheme will commission idling spares to perform periodic on-line testing (verifying whether system is functioning correctly), and recovery point validation during normal operation. When an error occurs, the\u00a0\u2026", "num_citations": "3\n", "authors": ["513"]}
{"title": "Dynamic document reclassification for preventing insider abuse\n", "abstract": " Digital documents in an organization are usually classified into static secrecy levels such as top-secret, secret, confidential and unclassified. Factors such as changes in the user hierarchy and addition of new projects generally require a change in a document's importance. Enforcing such changes in relative importance (RI) of documents protect the privileged documents from insider abuse. In this paper we propose a new framework for monitoring and dynamically changing the RI of documents over time. The proposed scheme utilizes a reasoning technique which takes into consideration the organization-specific data and the document usage data to make an informed decision on the RI of documents. Various inputs to the reasoning framework are standardized using XML schemas to provide interoperability. We have performed simulations with synthetic document usage data and report the results as proof of concept.", "num_citations": "3\n", "authors": ["513"]}
{"title": "Test scheduling for system-on-a-chip using test resource grouping\n", "abstract": " Test scheduling has been known to be one of the efficient techniques for reducing testing time of system-on-a-chip (SoC). In this paper, a heuristic algorithm, in which test resources are grouped and arranged, based on the size of product of power dissipation and test time of each core together with total power consumption in core-based SoC is proposed. We select test resource groups which have maximum power consumption but do not exceed the constrained power consumption and make the testing time slot of resources aligned at the initial position to time slot of resources aligned at the initial position to minimize the idling test time of test resources.", "num_citations": "3\n", "authors": ["513"]}
{"title": "A new distributed test control architecture with multihop wireless test connectivity and communication for gigahertz system-on-chips\n", "abstract": " With the increase in chip size and complexity, the direct or bus interconnects in conventional SoC test control models are rather restricted. In this paper, we propose a new distributed multihop wireless test control network based on the recent development in \u201cradio-on-chip\u201d technology. The proposed architecture consists of three basic components, the test scheduler, the resource configurators, and the RF nodes which support the communication between the test scheduler and clusters of cores. Under the multilevel tree structure, the resources (including not only the circuit blocks to perform testing, but also the on-chip radio-frequency nodes for intra-chip communication) are properly distributed and system optimization is performed in terms of both test application time and test control cost.", "num_citations": "3\n", "authors": ["513"]}
{"title": "A Comprehensive Simulation Platform for Intrusion Detection in Distributed Systems\n", "abstract": " This paper1 describes the simulation of an attack recognition system in a distributed environment. The underlying technique of attack recognition is based on assertion checking. An auxiliary process called watchdog queries the users for a scope-le, from which an assertable plan called Sprint plan is generated. The sprint plan consists of carefully derived assertions, which forms the basis for attack monitoring. Two environments are simulated for the purpose of testing and evaluation of the intrusion detection system. First, a general academic environment with limited security restrictions is simulated. Second, a virtual banking environment with stringent security requirements is simulated. Di erent attack scenarios are simulated for the purpose of testing the recognition system.", "num_citations": "3\n", "authors": ["513"]}
{"title": "Component ontological representation of function for diagnosis\n", "abstract": " Using function instead of fault probabilities for candidate discrimination during model based diagnosis has the advantages that function is more readily available, and facilitates explanation generation. However, current representations of function have been context dependent and state based, making them inefficient and time consuming. We propose classes as a scheme of representation of function for diagnosis based on component ontology principles, i.e., we define component functions (called classes) with respect to their ports. The scheme is space and time-wise linear in complexity, and hence, efficient. It is also domain-independent and scalable to representation of complex devices. We demonstrate the utility of the representation for the diagnosis of a printer buffer board.< >", "num_citations": "3\n", "authors": ["513"]}
{"title": "Yield and layout issues in fault tolerant VLSI architectures\n", "abstract": " Yield and layout are two important but often ignored issues in the design of fault tolerant VLSI systems. The authors present a framework for the systematic analysis of yield and area-efficient layout of fault-tolerant architectures. A multiple level redundancy tree is considered as a target architecture to demonstrate their analysis technique.<>", "num_citations": "3\n", "authors": ["513"]}
{"title": "Advances in secure knowledge management in the artificial intelligence era\n", "abstract": " Knowledge management and preservation started thousands of years ago from cave paintings, representing words by pictures, later moving to books through the invention of paper and printing in the 15th century (Wallace, 2007). Later with the development of computing systems, knowledge/information was stored in the form of computing documents. In the first decade of the 21st century there was an explosion of volume, velocity, and variety (3V) of data. In addition to the storage cost, extraction of information/knowledge was non-trivial and thus required the evolution of knowledge management tools. In the 2nd", "num_citations": "2\n", "authors": ["513"]}
{"title": "An analysis of complexity of insider attacks to databases\n", "abstract": " Insider attacks are one of the most dangerous threats to an organization. Unfortunately, they are very difficult to foresee, detect, and defend against due to the trust and responsibilities placed on the employees. In this article, we first define the notion of user intent and construct a model for a common scenario that poses a very high risk for sensitive data stored in the organization\u2019s database. We show that the complexity of identifying pseudo-intents of a user in this scenario is coNP-Complete, and launching a harvester insider attack within the boundaries of the defined threat model takes linear time while a targeted threat model is an NP-Complete problem. We also discuss the general defense mechanisms against the modeled threats and show that countering the harvester insider attack takes quadratic time while countering the targeted insider attack can take linear to quadratic time, depending on the strategy\u00a0\u2026", "num_citations": "2\n", "authors": ["513"]}
{"title": "The top 10 research priorities in bleeding disorders: a James Lind Alliance Priority Setting Partnership\n", "abstract": " The Top 10 Research Priorities in Bleeding Disorders: A James Lind Alliance Priority Setting Partnership The Top 10 Research Priorities in Bleeding Disorders: A James Lind Alliance Priority Setting Partnership Br J Haematol. 2019 Aug;186(4):e98-e100. doi: 10.1111/bjh.15928. Epub 2019 Apr 23. Authors Susan Shapiro 1 , David Stephensen 2 , Charlotte Camp 3 , Liz Carroll 4 , Peter Collins 5 , Derek Elston 6 , Patrick Gallagher 7 , Kate Khair 8 , William McKeown 9 , Jamie O'Hara 10 , Simon Stanworth 11 12 , Amanda Waterman 13 , Laurence Woollard 14 , Sheela Upadhyaya 15 , Michael Laffan 16 Affiliations 1 Oxford Haemophilia and Thrombosis Centre, Oxford University Hospitals NHS Foundation Trust, NIHR Oxford Biomedical Research Centre, Oxford, UK. 2 Kent Haemophilia Centre, Kent and Canterbury Hospital, Canterbury, UK. 3 HCD Economics, Daresbury, UK. 4 The Haemophilia Society, London, UK. 5 \u2026", "num_citations": "2\n", "authors": ["513"]}
{"title": "Continuous Authentication Using Behavioral Biometrics.\n", "abstract": " A highly utilized classifier [1]\u2022 Generates a region that separates majority of feature data related to a particular class\u2022 By mapping the input vector into a high-dimensional feature space via the kernel function-linear, polynomial, sigmoid, or radial basis function\u2022 Low energy consumption and high performance", "num_citations": "2\n", "authors": ["513"]}
{"title": "Surviving Advanced Persistent Threats\u2013a Framework and Analysis\n", "abstract": " Designing robust mission-critical systems demands bringing together fault tolerance and security. The emergence of advanced persistent threats (APT) has further added to the challenge of meeting mission assurance goals. Despite the advances in mission survivability, the existing solutions remain ineffective against APTs. In this paper, we propose a novel survivability framework against APTs in a distributed environment. It involves tamper-resistant and surreptitious detection and node-to-node verification of suspicious events. The solution aims to identify attacker intent, objectives and strategies (AIOS) and to design targeted recoveries that promote survivability. Its security strength has been theoretically analyzed, while the performance and scalability aspects are measured via simulation. Our simulations demonstrate high scalability with respect to network size and application runtime and the time overhead for long running applications can be easily kept under 1% of original runtime by carefully adjusting the security strength.", "num_citations": "2\n", "authors": ["513"]}
{"title": "Accelerated Processing of Secure Email by Exploiting Built-in Security Features on the Intel EP80579 Integrated Processor with Intel QuickAssist Technology\n", "abstract": " Domain Keys Identified Mail (DKIM) is one of the widely used mechanisms by which email messages can be cryptographically signed, permitting a signing domain to claim responsibility for the release of an email into the mail stream. As the volume of emails exchanged becomes large, the software implementations of DKIM using OpenSSL library will become a limiting factor of performance due to the heavy computations involved. In this largely empirical work, we identify the computation intensive modules of DKIM and solve the performance issues by implementing their functions on COTS hardware. Our approach makes use of the Intel Embedded processor Tolapai (Intel EP80579) that has several built-in cryptographic functionalities, viz. security accelerators for bulk encryption, authentication, hashing and public/private key generation and digital signing. Experimental results show that an overall 50% acceleration\u00a0\u2026", "num_citations": "2\n", "authors": ["513"]}
{"title": "Coordination of emergency response\n", "abstract": " Coordination management plays an important role in emergency response as it resolves the complex and dynamic interdependences among actors, resources, information, and decision making. Review of current emergency response practice suggests that emergency coordination is an understudied research area and new knowledge in this area is of high importance. In this chapter, we examine the roles of people, process, and information technology and their impacts on emergency coordination. Through a case study of Snowstorm 2006 in western New York, we demonstrate their practical operation and evaluate their individual performance. Further, we summarize the important lessons learned in the management of these factors and propose solutions. Also included in the chapter is a detailed discussion of one cutting-edge emergency response system named DisasterLAN, through which we demonstrate how modern response systems are designed and in what ways they facilitate emergency coordination.", "num_citations": "2\n", "authors": ["513"]}
{"title": "Security of alternative delivery channels in banking: Issues and countermeasures\n", "abstract": " To sustain competitive advantages, financial institutions continuously strive to innovate and offer new banking channels to their customers as technology creates new dimensions to their banking systems. One of the most popular such diversification of channel is electronic banking (e-banking). Information assurance is a key component in e-banking services. This chapter investigates the information assurance issues and tenets of e-banking security that would be needed for design, development and assessment of an adequate electronic security infrastructure. The technology terminology and frameworks presented in the chapter are with the view to equip the reader with a glimpse of the state-of-art technologies that may help towards learned and better decisions regarding electronic security.", "num_citations": "2\n", "authors": ["513"]}
{"title": "A Novel Approach for Security and Robustness in Wireless Embedded Systems\n", "abstract": " Security and robustness are paramount in wireless embedded systems due to the vulnerability of the underlying communication medium. To institute security and reliability, most of the existing schemes perform periodic re-establishment of authentication credentials and share secrets among various participating nodes. However, such measures result in overheads in an energy-constrained wireless environment. To alleviate this problem, we propose a software approach that exploits the features of the underlying communication protocol and uses the concept of steganography and covert channels. The highlight of our approach is that it does not require any changes to the protocol and relies only on the modification of frame contents without degrading the protocol performance. We argue that our covert-channel based communication scheme provides security and robustness at low cost and it neither\u00a0\u2026", "num_citations": "2\n", "authors": ["513"]}
{"title": "\u2018WIRED\u2019SENIOR CITIZENS AND ONLINE INFORMATION PRIVACY\n", "abstract": " The role of information and communication technologies is drawing attentions as an important supporting system for elderly people. While the number of elderly online users is increasing, this group is remaining as one of the most vulnerable one in terms of online privacy protection in the cyberspace. The main objective of this study is to understand online privacy behavior and attitudes differentiated along demographic characteristics as well as psychological factors for the aging society. The study proposes factors contributing wired seniors\u2019 online privacy awareness and their information sharing behavior. Based on sociotechnical approach, this research provides a research framework which explains online privacy issues in senior citizens\u2019 perspective.", "num_citations": "2\n", "authors": ["513"]}
{"title": "Surface transportation and cyber-infrastructure: An exploratory study\n", "abstract": " Using IT applications is essential for the productive, safe and reliable transportation service. However, there are concerns regarding the cybersecurity vulnerability of IT applications. In this study, we investigate the role of IT in the surface transportation security based on responses from mid and high level managers in transportation industries. Our survey results indicate that managers of the transportation industry think IT applications are fundamental components in the surface transportation operation and security. However, they are highly concern about cybersecurity threats on IT applications. Our research findings contribute to provide the understating of the sate of IT application on surface transportation systems and cybersecurity threats. Our research results and analysis are discussed in this paper.", "num_citations": "2\n", "authors": ["513"]}
{"title": "AEGIS: A Proactive Methodology to Shield against Zero-Day Exploits\n", "abstract": " Given the large number of vulnerability instances disclosed in various bug-tracking communities, system administrators face an up-hill task of protecting their system/ network against zero-day exploits. In order to safeguard against such exploits, the present challenges come in two-fold: (i) there exists a compelling need to assimilate configuration specific vulnerability information from various bug-tracking diaspora; also (ii) there is a need to proactively generate policy specific signatures which act as a first line of defense. In this paper we propose an automated approach for determining vulnerabilities pertinent to the current network/ system configuration using the information aggregated from different bug tracking communities. Such vulnerability assessment and indication mechanisms significantly alleviate the system administrator's burden of manual content digging for vulnerabilities in his own configuration context\u00a0\u2026", "num_citations": "2\n", "authors": ["513"]}
{"title": "Organizational Coordination in Extreme Events: A Case Study of October\u201906 Snowstorm in Western New York\n", "abstract": " Increased attention has recently been directed towards extreme events and their response management. Emergency events such as natural disasters and manmade accidents are characterized by their rare occurrence and the high risk of negative consequences if decisions in response to the emergency are slow, uninformed, or inadequate (Ajenstat et al. 2007). As emergency management typically involves complex network of tasks, resources, and actors, coordination emerges as a critical management aspect which should be used to address the embedded interdependencies for smooth and efficient response operations (Turoff 2002).Coordination is a specific form of decision making wherein the problems associated with the different possible responses are interdependent (Malone 1994; Malone et al. 1990). Challenges ranging from limited information, unpredictable development, short time windows, and high risks all threaten the response organizations in their ability to make rapid and sound coordination decisions. The governmental reports of coordination in recent rare events, such as 9/11 attack and Hurricane Katrina, reveal enormous failures, calling for further research to improve coordination practices (Townsend 2006).", "num_citations": "2\n", "authors": ["513"]}
{"title": "Encapsulation of User\u2019s Intent: A New Proactive Intrusion Assessment Paradigm\n", "abstract": " Few practical implementations of anomaly detection systems are currently known. Major hindrances in this regard are poor accuracy of detection and excessive false positives. While some of the reasons may be attributed to theory and technology, a major factor that is overlooked is the user. We propose a novel approach that brings the user into the loop by querying him for his session intent in a proactive manner. This encapsulated intent serves the purpose of a certificate based on which more accurate intrusion detection decisions can be made.", "num_citations": "2\n", "authors": ["513"]}
{"title": "A New Architectural Approach for Self-Verification and Fault-Tolerance of Enterprise Servers\n", "abstract": " In this paper, we propose a new dependability paradigm for the design of self-verifiable enterprise servers. The traditional massive redundancy and replication approach for dependability is unattractive and prohibitive in commercial enterprise and Internet applications. Unlike the processor pair approach, where all the instructions are executed on a duplicate processor, our approach consists of taking a pair of snapshots systematically during the execution of an application. The instructions between the snapshots are redundantly executed on an auxiliary hardware and the results compared at the second snapshot of the pair. A discrepancy of results indicates an anomaly. By controlling the snapshot duration and frequency, a flexible detection latency can be achieved. Several architectural options are presented for executing the code between the snapshots and simulations are performed using the IMPACT compiler\u00a0\u2026", "num_citations": "2\n", "authors": ["513"]}
{"title": "A parallel VLSI implementation of Viterbi algorithm for accelerated word recognition\n", "abstract": " A hardware VLSI implementation of the Viterbi algorithm is presented. The Viterbi algorithm is used to solve a word recognition problem using a hidden Markov model. In order to accelerate the speed of computation for real-time word recognition, the inherent parallelism in the recursion step of the algorithm is exploited. Details of the hardware implementation and experimental results are discussed.< >", "num_citations": "2\n", "authors": ["513"]}
{"title": "Decepticon: a Theoretical Framework to Counter Advanced Persistent Threats\n", "abstract": " Deception has been proposed in the literature as an effective defense mechanism to address Advanced Persistent Threats (APT). However, administering deception in a cost-effective manner requires a good understanding of the attack landscape. The attacks mounted by APT groups are highly diverse and sophisticated in nature and can render traditional signature based intrusion detection systems useless. This necessitates the development of behavior oriented defense mechanisms. In this paper, we develop Decepticon (Deception-based countermeasure), a Hidden Markov Model based framework where the indicators of compromise (IoC) are used as the observable features to aid in detection. This theoretical framework also includes several models to represent the spread of APTs in a computer system. The presented framework can be used to select an appropriate deception script when faced with\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}
{"title": "Decepticon: A Hidden Markov Model Approach to Counter Advanced Persistent Threats\n", "abstract": " Deception has been proposed in the literature as an effective defense mechanism to address Advanced Persistent Threats (APT). However, administering deception in a cost-effective manner requires a good understanding of the attack landscape. The attacks mounted by APT groups are highly diverse and sophisticated in nature and can render traditional signature based intrusion detection systems useless. This necessitates the development of behavior oriented defense mechanisms. In this paper, we develop Decepticon (Deception-based countermeasure) a Hidden Markov Model based framework where the indicators of compromise (IoC) are used as the observable features to aid in detection. This framework would help in selecting an appropriate deception script when faced with APTs or other similar malware and trigger an appropriate defensive response. The effectiveness of the model and the associated framework is demonstrated by considering ransomware as the offending APT in a networked system.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Blockchain Badging-Coursera-to-Credit\n", "abstract": " Blockchain is a disruptive force both in finance (cryptocurrency) and higher education credentialing. UB is working on a four-course Coursera specialization that has broad market appeal for matriculated and non-traditional adult learners, nationally and globally, by providing online access to academically rigorous and specific industry-wide practices. Based on a one credit pilot course attended both by campus students and external executives, we propose developing a process to transform Coursera issued Verified Certificates into UB-branded badges. This will be coupled with a new partnership with Empire State College, where UB Computer Science faculty and external reviewers will then apply a rigorous rubric to ascertain whether an adult learner is eligible for academic credit under a prior learning assessment (PLA) process. The goal is to engage non-traditional learners and guide them into additional Micro-Credential or Degree based programs by awarding transferrable credit through the potential of a new \"Coursera to Credit\" process.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Summarizing large query logs in ettu\n", "abstract": " Database access logs are large, unwieldy, and hard for humans to inspect and summarize. In spite of this, they remain the canonical go-to resource for tasks ranging from performance tuning to security auditing. In this paper, we address the challenge of compactly encoding large sequences of SQL queries for presentation to a human user. Our approach is based on the Weisfeiler-Lehman (WL) approximate graph isomorphism algorithm, which identifies salient features of a graph or in our case of an abstract syntax tree. Our generalization of WL allows us to define a distance metric for SQL queries, which in turn permits automated clustering of queries. We also present two techniques for visualizing query clusters, and an algorithm that allows these visualizations to be constructed at interactive speeds. Finally, we evaluate our algorithms in the context of a motivating example: insider threat detection at a large US bank. We show experimentally on real world query logs that (a) our distance metric captures a meaningful notion of similarity, and (b) the log summarization process is scalable and performant.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Creating a preliminary cyber ontology for insider threats in the financial sector\n", "abstract": " Insider attack has become a major threat in financial sector and is a very serious and pervasive security problem. Currently, there is no insider threat ontology in this domain and such an ontology is critical to developing countermeasures against insider attacks. In this paper, we create an ontology focusing on insider attacks in the banking domain targeting database systems. We define the taxonomy used in this ontology and identify the relationships between the ontology classes. The resulting structure is a domain ontology mapped onto SUMO, FOAF and Finance ontologies to make the our work integrable to the systems that use these ontologies and to create a broad knowledge base. The attack types we formulate in the ontology are masquerade, privilege elevation, privilege abuse and collusion attacks. Our model could be used to systematically evaluate any insider threat detection schemes in a realistic way and\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}
{"title": "Security considerations and reference architecture of a cyber computing infrastructure for online education\n", "abstract": " The world of online education is gaining momentum as evidenced by the stamp of approval from accreditation agencies and increasing number of online course offerings from top-tier universities. Learning Management Systems have kept up with the needs of online education by providing an online experience comparable to face-to-face lectures. Besides lectures, one of the major services offered by a traditional brick and mortar university is computing resources for students and faculty. To keep parity between the computing services offered by traditional educational campus environments and online educational environments, it is necessary to offer sufficient computing resources to the online students & cyber-faculty. The reference architecture and security considerations for such a Cyber Computing Infrastructure is laid out in this paper. We present an implementation of a Cyber Computing Infrastructure based on\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}
{"title": "Information Systems Security\n", "abstract": " This volume contains the papers presented at the 11th International Conference on Information Systems Security (ICISS 2015), held December 16\u201320, 2015, in Kolkata. The conference initiated in 2005 to cater to cyber security research in India successfully entered its 11th edition and has been providing an attractive international forum on information system security for academics, industry, business, and government. This year, the conference attracted 133 submissions from 17 countries. Given the high quality of the submissions, the Program Committee (PC) accepted 24 full papers and eight short papers after a rigorous review process with multiple reviews for each paper. We thank all the expert reviewers for their invaluable support. We are grateful to the PC members who put in enormous efforts in reviewing and selecting the papers. Without the untiring efforts of the PC members/reviewers and the contributions\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}
{"title": "Accelerating Techniques for Rapid Mitigation of Phishing and Spam Emails\n", "abstract": " Spam filters that are implemented using Na\u00efve Bayesian learning techniques are widely deployed worldwide with email clients such as Outlook\u00ae. These filters that are deployed on end user\u2019s computers and typically used to filter out spam for individual users are effective when the spam load is around 400-500 spam emails per day per user. However, when the spam load increases, these solutions prove to be slow and hence insufficient for practical use. In this paper, we identify the computation intensive functions of such machine learning algorithms and solve the performance issues by implementing these functions on hardware. Earlier similar approaches made use of specialized hardware chips or coprocessors to achieve such acceleration. These chips being dedicated hardware represent a cost and scalability limitation. Our approach makes use of a more generic Intel desktop processor, viz. Tolapai (Intel EP80579) that has several built-in cryptographic functionalities, viz. security accelerators for bulk encryption, authentication, hashing and public/private key generation. Experimental results show that significant acceleration can be achieved by migrating some of the functionalities to hardware in a transparent way.", "num_citations": "1\n", "authors": ["513"]}
{"title": "A Multistage Framework to Defend Against Phishing Attacks\n", "abstract": " Phishing scams pose a serious threat to end-users and commercial institutions alike. E-mail continues to be the favorite vehicle to perpetrate such scams, mainly due to its widespread use combined with the ability to easily spoof them. Several approaches, both generic and specialized, have been proposed to address this growing problem. However, phishing techniques, growing in ingenuity as well as sophistication, render these solutions weak. To overcome these limitations, we propose a multistage framework\u2013the first stage aims at detecting phishing based on their semantic and structural properties, whereas in the second stage we propose a proactive technique based on a challenge-response technique to establish the authenticity of a Web site. Using live e-mail data, we demonstrate that our approach with these two stages is able to detect a wider range of phishing attacks than existing schemes. Also, our\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}
{"title": "QoS-LI: QoS loss inference in disadvantaged networks--part II\n", "abstract": " The Quality-of-Service (QoS) in disadvantaged networks is a function of many parameters, including the nature of the physical link over which the disadvantaged network operates. While there exist mechanisms (like specialized versions of TCP) to account for their nature and operate optimally over disadvantaged networks, their operation under adversarial conditions have not been investigated. In our prior work [17], we presented a game theoretic framework to infer the nature of a QoS loss in disadvantaged networks. In this work, we present the translation of the theoretical framework to a satellite based disadvantaged network. We show the feasibility of the game theoretic formulations in satellite networks through simulations in Opnet.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Mediated Internet Experience for Senior Citizens\n", "abstract": " Survey results from the Pew Internet American Life Project indicate that the demography of senior citizens that conduct activities on the Internet is rapidly growing and vulnerable to online privacy violations. Their enhanced vulnerability stems from their trusting nature, a characteristic of their times and culture. Since the Internet related threats such as phishing, spamming, etc., are still research issues, a purely technical solution, although desirable, is currently not viable. In this paper, we view the problem through the trust prism and argue for a socio-technical solution to help senior citizens conduct their online activities safely. We propose the conceptual outlines of a trust based framework for a Mediated Internet Experience (MINE) for senior citizens residing in a community center. The framework uses a trust model based on Bayesian Network modeling to help assign trust levels to Internet sites and recommend them\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}
{"title": "Perceived Risk and Resilience in the Face of Natural Disasters: A Study of Hospital\n", "abstract": " Though hospital information systems have been extensively studied as a technology and there is now a growing body of literature in the area of infrastructure interdependencies, the dependencies of civil and built infrastructure on the health care information infrastructure (HII) is understudied. In particular, there is no study to our knowledge that addresses the issue of Hospital Information Infrastructure in the context of disasters. This study explores how an organization\u2019s information systems infrastructure is affected by disasters and examines the relationship between organizational resilience and information infrastructure effectiveness by using conceptual model.", "num_citations": "1\n", "authors": ["513"]}
{"title": "A Comprehensive Reasoning Framework for Information Survivability (User Intent Encapsulation and Reasoning About Intrusion: Implementation and Performance Assessment)\n", "abstract": " This effort approaches the problem of user-level intrusion detection by investigating the design and implementation of a practical online user-level intrusion detection system. The outcome of this research is a Dynamic Reasoning based User Intent Driven DRUID intrusion detection system. It is important to pay attention to deployment-time issues such as usability and evasion, otherwise it may lead to a situation where the security system is deployed but is either unusable or is deliberately bypassed. A variation of sequential hypothesis testing is proposed to address these issues. Data plays a very important role in the validation of any new approaches or models that are proposed. Unfortunately, in the user-level intrusion detection domain, due to concerns of privacy, there are too few datasets available to the research community. This issue is addressed by devising a data generation algorithm called RACOON based on a model used to profile users.Descriptors:", "num_citations": "1\n", "authors": ["513"]}
{"title": "A Delphi Study on Priorities\u2013Lessons Learned from the Gulf Coast\n", "abstract": " As a nation we did learn a lot of lessons from the events on the gulf coast. Since budgets in local economies are limited, there is a need to prioritize what gets done. This paper presents the results of a Delphi study on the importance and urgency of the issues, based on the perception of first responders. The paper informs public policy in the area of disaster preparedness.", "num_citations": "1\n", "authors": ["513"]}
{"title": "SIMS: A Modeling and Simulation Platform for Intrusion Monitoring/Detection Systems\n", "abstract": " Computer security is becoming an ever-growing concern in the current world. Intrusion monitoring and detection systems form a large class of tools that are deployed to combat the scourge of attacks. While there are many such systems available, the typical development cycle of such a system involves significant time and effort. In this paper, we propose and develop a modeling and simulation platform constructed over a popular operating system simulator that can aid in the modeling and rapid testing of intrusion detection systems. In order to achieve this goal, we have studied a number of intrusion detection systems and identified the various features that are essential to the successful construction of such a platform. We demonstrate the capabilities of this system, called Modeling and Simulation Platform for Intrusion Monitoring/Detection Systems (SIMS), by developing and simulating two intrusion detection system models. In the long run, we also hope that this would become a widely used academic and commercial tool.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Time slot specification based approach to analog fault diagnosis using built-in current sensors and test point insertion\n", "abstract": " Testing and diagnosis of analog circuits continues to be a hard task for test engineers and efficient test methodologies to tackle these problems are needed. This paper proposes a novel analog test method using time slot specification (TSS) based built-in current sensors. A technique for location of a fault site and fault type, based on TSS, is presented. The proposed built-in current sense and decision module (BSDM), in association with TSS analysis, has high testability and good fault coverage, and a capability to diagnose catastrophic faults and parametric faults in analog circuits. The digital output of the BSDM can be easily combined with built-in digital test modules for mixed-signal IC testing. The general heuristics for test point placement are also described.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Defect analysis and realistic fault model extensions for multi-port SRAMs\n", "abstract": " 1.1 Introduction\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026 1.2 Thesis Objective and Organization\u2026\u2026\u2026\u2026\u2026\u2026\u2026..\u2026\u2026\u2026\u2026\u2026.. 1.2. 1 Thesis Objective\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\u2026\u2026. 1.2. 2 Thesis Organization\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.", "num_citations": "1\n", "authors": ["513"]}
{"title": "A design for test perspective on memory synthesis\n", "abstract": " A novel approach based on a branch and bound technique is presented for transformation of logical memories to mappable physical memories while specified parameters such as area, delay, memory test time and test strategy are satisfied. The proposed algorithm uses the new test parameters to realize a logical memory with a set of physical memories which could be tested within the specified test time using the specified memory test strategy. The proposed framework generates and stores the realized logical memory in structural VHDL.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Design and analysis of a hardware-assisted checkpointing and recovery scheme for distributed applications\n", "abstract": " A checkpointing and recovery scheme which exploits the low latency and high coverage characteristics of a hardware error detection scheme is presented. Message dependency which is the main source of multi-step rollback in distributed systems is minimized by using a new message validation technique derived from hardware-assisted error detection. The main contribution of this paper is the development of an analytical model to establish the completeness and correctness of the new scheme. A novel concept of global state matrix is defined to keep track of the global state in a distributed system and assist in recovery. An illustration is given to show the distinction between conventional and the new recovery schemes.", "num_citations": "1\n", "authors": ["513"]}
{"title": "A New Approach to Programmable Memory Built-In Self Test Scheme\n", "abstract": " The design and architecture of a recon gurable memory BIST unit is presented. The proposed memory BIST unit could accommodate changes in the test algorithm with no impact to the hardware. Di erent types of march test algorithms could be realized using the proposed memory BIST unit and the proposed architecture allows addition and elimination of the memory BIST components. Therefore memories with di erent characteristics and test requirements could use the same memory BIST architecture. By integrating the diagnostics with this memory BIST unit, further reduction in the cost of test was achieved. Our experimental results show that the proposed memory BIST methodolgy leads to lower logic overhead than other programmable methods.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Implementation of a gracefully degradable binary tree in programmable multi-chip modules\n", "abstract": " A multi-chip module is proposed as a packaging scheme for a binary tree processing array in order to contain the whole system in a single package. Fault tolerance is provided to the array by a pass transistor switching network in the MCM silicon substrate. The benefits of an active substrate base can offset the expense and complexity of an MCM design when it has application to many circuits. The standard interconnect pattern of binary trees would allow many binary tree applications to share such a pre-fabricated substrate. The switching network provides reconfiguration of the original tree to a gracefully degraded binary tree. The largest functioning binary tree which can be extracted from the original tree will be connected to the existing I/O pads. The algorithm used to obtain this subtree is complete and also contains a heuristic approach in its search. A simple procedure is provided to control the reconfiguration\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}
{"title": "A new efficient signature technique for process monitoring in critical systems\n", "abstract": " A new simple, inexpensive and time/space efficient signature technique for process monitoring is presented. In this technique, signatures are accumulated to form an m-out-of-n code and the corresponding locations in the memory are tagged. During the run-time, the generated signatures at the tagged locations are monitored using a simple hardware to determine whether they form m-out-of-n codes. This approach offers flexible error latency and high coverage. Results of an experiment conducted to verify the latency are presented. The main advantage of our approach is the requirement of no reference signatures unlike existing techniques leading to both memory and run-time execution time savings. The flexible latency and the low cost hardware will make this technique suitable for critical applications where quick detection of errors is deemed important.", "num_citations": "1\n", "authors": ["513"]}
{"title": "A new approach to modeling the performance of a class of fault tolerant VLSI/WSI systems based on multiple-level redundancy\n", "abstract": " The on-chip redundancy left unused in a fault tolerant system after successfully reconfiguring and eliminating the manufacturing defects is called residual redundancy. This redundancy can be used to improve the operational reliability of the system. The authors present a new hierarchical model to analyze the effect of residual redundancy on performance improvement of a class of fault tolerant VLSI/WSI systems based on multiple-level redundancy. Their model emphasizes the effect of support circuit (interconnection) failures on system reliability, a practical issue of great concern in WSI technology. Results of a simulation conducted to validate their model are discussed.< >", "num_citations": "1\n", "authors": ["513"]}
{"title": "Test Time versus Design-for-Test Resources in Mixed Signal Systems\n", "abstract": " Functional macro testing of analog subsystems is providing a reasonable approach to testing in mixed signal systems. This allows the partitioning of the analog from digital components. Two approaches emerge: analog test bus and Built-In-Self-Test (BIST). Design-for-Test (DFT) issues arise in both approaches which include additional circuitry for control and observation. This additional circuit cost is balanced against test scheduling since the cost of testing can be directly related to test time. This paper explores some of the DFT issues regulating the balance of circuitry versus concurrent test using a new cost model.", "num_citations": "1\n", "authors": ["513"]}
{"title": "Report Dropping and Tampering Detection in Sensor Networks: Enhancing Data Reliability\n", "abstract": " Achieving communication dependability in Sensor networks is difficult due to limitations on node power and wireless channel unreliability. This problem is further compounded by hard-to-detect attacks such as data-report tampering by malicious Cluster Heads, and dropping of data-reports by intermediate nodes en route from the CH to the Base Station (BS). Detecting these attacks coupled with node failure detection may result in achieving enhanced reliability (semi-reliability) in sensor networks data communication, depending on the granularity and accuracy of diagnosis. In this paper, we introduce consistency checking-based proxy-report schemes rooted at the BS for detecting such data report dropping and tampering. The merit of the solution methodology lies in its simplicity, comprehensive nature and practical usability. We consider a cluster-based sensor network model where sensor nodes within a cluster send their data to the CH which aggregates and forwards this data to the Base Station (BS) in the form of data reports. Our schemes use proxy reports to verify the accuracy and receipt of data reports sent by a CH to the BS. Proxy reports are periodically sent along non-primary paths, enabling the BS to detect any tampering or dropping of the data reports by malicious nodes on the primary paths with a certain probability. Proxy report transmission involves path-reuse and piggybacking on data reports from other clusters, making the scheme lightweight and resulting in minimal additional control and energy overhead for the energy constrained sensor nodes. Simulation results show the robustness of our schemes against random and\u00a0\u2026", "num_citations": "1\n", "authors": ["513"]}