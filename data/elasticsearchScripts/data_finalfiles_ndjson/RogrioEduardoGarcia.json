{"title": "Ensino de l\u00f3gica de programa\u00e7ao e estruturas de dados para alunos do ensino m\u00e9dio\n", "abstract": " This paper reports the experience on teaching computer programming techniques to studients at high school level aimed to develop their abilities to create algorithms and encourage them to follow their studies on Computer Science.Resumo. Este artigo relata a experi\u00eancia de um Projeto de Extens\u00e3o Universit\u00e1ria desenvolvido pela FCT-Unesp, cujo objetivo \u00e9 capacitar alunos regularmente matriculados no ensino a resolver problemas de l\u00f3gica de programa\u00e7\u00e3o e estruturas de dados, despertando o interesse dos jovens para a \u00e1rea de Computa\u00e7\u00e3o.", "num_citations": "30\n", "authors": ["942"]}
{"title": "Analysis of document pre-processing effects in text and opinion mining\n", "abstract": " Typically, textual information is available as unstructured data, which require processing so that data mining algorithms can handle such data; this processing is known as the pre-processing step in the overall text mining process. This paper aims at analyzing the strong impact that the pre-processing step has on most mining tasks. Therefore, we propose a methodology to vary distinct combinations of pre-processing steps and to analyze which pre-processing combination allows high precision. In order to show different combinations of pre-processing methods, experiments were performed by comparing some combinations such as stemming, term weighting, term elimination based on low frequency cut and stop words elimination. These combinations were applied in text and opinion mining tasks, from which correct classification rates were computed to highlight the strong impact of the pre-processing combinations. Additionally, we provide graphical representations from each pre-processing combination to show how visual approaches are useful to show the processing effects on document similarities and group formation (ie, cohesion and separation). View Full-Text", "num_citations": "27\n", "authors": ["942"]}
{"title": "An Ontology for Controlled Experiments on Software Engineering.\n", "abstract": " Running multiples experiments in Software Engineering introduces the need of recording data as well as transferring knowledge across them, specially considering that several researchers are involved on replicating experiments. In this work we explore ontologies to support knowledge transfer, helping to elucidate the associated concepts of controlled experiments and their relationships. Based on our expertise on conducting controlled experiments, we have proposed an ontology to experimental studies, named EXPEROntology. The ontology proposed is intended to be used as a tool for knowledge transfer, assisting researchers, reviewers, and meta-analysts in designing, conducting, and evaluating controlled experiments. In order to validate our ontology we have instantiated it in to a controlled experiment.", "num_citations": "19\n", "authors": ["942"]}
{"title": "Comparison of shape analysis methods for Guinardia citricarpa ascospore characterization\n", "abstract": " Among the diseases affecting the commercial citrus production, the citrus black spot (CBS) is considered to cause substantial losses. The analyses of particles in suspension in the orchards and collected into a disc have been applied as a preventive action trying to identify the presence of fungus spores before symptom appearance. In this paper, we show the results of several shape analysis methods applied to the fungus, the first step to the aimed computer aided vision system, capable to assist the identification process. Experiments and comparative results among the methods are presented in this paper, showing that better results were obtained applying the curvature method", "num_citations": "16\n", "authors": ["942"]}
{"title": "Simplified stress and simplified silhouette coefficient to a faster quality evaluation of multidimensional projection techniques and feature spaces\n", "abstract": " Several multidimensional projection techniques have been proposed in literature over the last years. The quality of those techniques can be evaluated based on the dimensionality reduction or the clusters quality. The first evaluation aim to verify if the similarities from multidimensional space are preserved in projected space. While the second evaluation aim to verify if instances from a same class are placed in a same cluster in projected space. Respectively, Stress and Silhouette Coefficient are the main measures to quality evaluations. In this paper we present two new approaches -- named Simplified Stress and Simplified Silhouette Coefficient -- to speed up the computation of measures, enabling a faster evaluation of multidimensional projection techniques and feature spaces. We present experiments showing the high correlation between results obtained using original approaches and results obtained with\u00a0\u2026", "num_citations": "15\n", "authors": ["942"]}
{"title": "Numerical simulation of the liquid phase in SnO 2 thin film deposition by sol-gel-dip-coating\n", "abstract": " The fluid flow of the liquid phase in the sol-gel-dip-coating process for SnO2 thin film deposition is numerically simulated. This calculation yields useful information on the velocity distribution close to the substrate, where the film is deposited. The fluid modeling is done by assuming Newtonian behavior, since the linear relation between shear stress and velocity gradient is observed. Besides, very low viscosities are used. The fluid governing equations are the Navier\u2013Stokes in the two dimensional form, discretized by the finite difference technique. Results of optical transmittance and X-ray diffraction on films obtained from colloidal suspensions with regular viscosity, confirm the substrate base as the thickest part of the film, as inferred from the numerical simulation. In addition, as the viscosity increases, the fluid acquires more uniform velocity distribution close to the substrate, leading to more homogenous and\u00a0\u2026", "num_citations": "14\n", "authors": ["942"]}
{"title": "Teaching software quality via source code inspection tool\n", "abstract": " Software Quality Assurance is a sub-process that ensures that developed software meets and complies with defined or standardized quality specifications. Focusing on source code, there are characteristics that can be used to evaluate the quality. Introductory courses must encourage freshmen students to improve internal quality of their source code, but only as sophomore they have contact with Software Engineering concepts, including Quality Assurance. In this paper we present a tool to source code quality evaluation aimed at supporting students to improve their source code and, consequently, their programming skills. The proposed tool uses quality reports (available to professional environment integrate with software repositories) to analyze students' source code and provide a feedback about the student coding. The proposed tool run locally, with few computational resources. In addition, we proposed the\u00a0\u2026", "num_citations": "13\n", "authors": ["942"]}
{"title": "Visual analysis of data from empirical studies\n", "abstract": " Exploratory visualization techniques may complement statistical data analysis, helping users to understand and treat data from empirical studies. Visualization becomes particularly interesting as data sets grow large and more diverse. This paper discusses how visual representations and exploratory data visualization may be applied to support and enhance analysis of data sets produced in experimental Software Engineering. Information Visualization techniques are used to analyse data from an experiment comparing different reading techniques, allowing us to identify advantages and limitations of visual approaches as compared with traditional statistical techniques.", "num_citations": "13\n", "authors": ["942"]}
{"title": "Teaching and learning software project management: A hands-on approach\n", "abstract": " Project management is an essential activity across several areas, including Software Engineering. Through good management it is possible to achieve deadlines, budgets goals and mainly delivering a product that meets customer expectations. Project management activity encompasses: measurement and metrics; estimation; risk analysis; schedules; tracking and control. Considering the importance of managing projects, it is necessary that courses related to Information Technology and Computer Science present to students concepts, techniques and methodology necessary to cover all project management activities. Software project management courses aim at preparing students to apply management techniques required to plan, organize, monitor and control software projects. In a nutshell, software project management focuses on process, problem and people. In this paper we proposed an approach to\u00a0\u2026", "num_citations": "12\n", "authors": ["942"]}
{"title": "Genetic algorithms to support software engineering experimentation\n", "abstract": " Empirical software engineering is concerned with running experimental studies in order to establish a broad knowledge base to assist software developers in evaluating models, methods and techniques. Running multiple experimental studies is mandatory, but complex and the cost is high. Besides, replications may impose constraints difficult to meet in real contexts. Researchers face additional problems and cost restrictions when conducting meta-analysis on combined data from multiple experiments. In this paper we are concerned with both issues, of assisting users in carrying out meta-analysis tasks and gathering a meaningful body of data from experimental studies. We show how the genetic algorithms optimization model can effectively handle a specific meta-analysis problem that is not amenable to standard statistical approaches. We also introduce an approach to expand the universe of data by mapping the\u00a0\u2026", "num_citations": "12\n", "authors": ["942"]}
{"title": "Packaging Controlled Experiments Using an Evolutionary Approach Based on Ontology (S).\n", "abstract": " A body of knowledge in Software Engineering requires experiments replications. The knowledge generated by a study is registered in the so-called lab package, which must be reviewed by an eventual research group with the intention to replicate it. However, researchers face difficulties reviewing the lab package, what leads to problems in share knowledge among research groups. Besides that, the lack of standardization is an obstacle to the integration of the knowledge from an isolated study in a common body of knowledge. In this sense, ontologies can be applied, since they can be seen as a standard that promotes the shared understanding of the experiment information structure. In this paper, we present a workflow to generate lab packages based on EXPEROntology, an ontology of controlled experiments domain. In addition, by means of lab packages instantiation, it is possible to evolve the ontology, in order to deal with new concepts that may appear in different lab packages. The iterative ontology evolution aims at achieve a standard that is able to accommodate different lab packages and, hence, facilitate to review and understand their content.", "num_citations": "10\n", "authors": ["942"]}
{"title": "A survey on graduates\u2019 curriculum-based knowledge gaps in software testing\n", "abstract": " This research full paper presents a study on graduates' knowledge gaps in software testing according to industry needs. Several studies indicate that students graduate from computing programs with a knowledge gap in software testing. In this sense, we aimed to investigate in details this broader testing gap, by considering gaps in the level of testing topics. We conducted a survey with Brazilian practitioners in order to collect data (N=90). For each testing topic, knowledge gaps were calculated as the difference between what respondents' learned/practiced in undergraduate courses and what they actually applied in industry after graduating. Results provide evidence on points that could be improved in software testing education. Firstly, for all testing topics there was a negative gap on practice activities. This means that students could benefit from more testing assignments throughout the curriculum. Regarding gaps\u00a0\u2026", "num_citations": "9\n", "authors": ["942"]}
{"title": "Teaching and Learning of Data Structures Supported by Computer: An Experience with the CADILAG tool\n", "abstract": " Teaching and Learning of Data Structures Supported by Computer: An Experience with the CADILAG tool Toggle navigation Reposit\u00f3rio Institucional UNESP portugu\u00eas (Brasil) English espa\u00f1ol portugu\u00eas (Brasil) portugu\u00eas (Brasil) English espa\u00f1ol Entrar Chat Sobre Toggle navigation Item Reposit\u00f3rio Institucional UNESP Produ\u00e7\u00e3o cient\u00edfica Faculdade de Ci\u00eancias e Tecnologia (FCT) - Presidente Prudente Departamento de Matem\u00e1tica e Computa\u00e7\u00e3o Artigos - Matem\u00e1tica e Computa\u00e7\u00e3o - FCT Item Reposit\u00f3rio Institucional UNESP Produ\u00e7\u00e3o cient\u00edfica Faculdade de Ci\u00eancias e Tecnologia (FCT) - Presidente Prudente Departamento de Matem\u00e1tica e Computa\u00e7\u00e3o Artigos - Matem\u00e1tica e Computa\u00e7\u00e3o - FCT Item Buscar no Reposit\u00f3rio Buscar nesta cole\u00e7\u00e3o Navegar Em todo o Reposit\u00f3rioTipo de Produ\u00e7\u00e3oData do documentoAutorT\u00edtuloPalavra-chaveNesta cole\u00e7\u00e3oData do documentoAutorT\u00edtuloPalavra-chave Minha \u2026", "num_citations": "9\n", "authors": ["942"]}
{"title": "Handwritten feature descriptor methods applied to fruit classification\n", "abstract": " Several works have presented distinct ways to compute feature descriptor from different applications and domains. A main issue in Computer Vision systems is how to choose the best descriptor for specific domains. Usually, Computer Vision experts try several combination of descriptor until reach a good result of classification, clustering or retrieving \u2013 for instance, the best descriptor is that capable of discriminating the dataset images and reach high correct classification rates. In this paper, we used feature descriptors commonly applied in handwritten images to improve the image classification from fruit datasets. We present distinct combinations of Zoning and Character-Edge Distance methods to generate feature descriptor from fruits. The combination of these two descriptor with Discrete Fourier Transform led us to a new approach for acquire features from fruit images. In the experiments, the new\u00a0\u2026", "num_citations": "8\n", "authors": ["942"]}
{"title": "Challenges to integrate software testing into introductory programming courses\n", "abstract": " Several studies suggest that the teaching of software testing should begin as early as possible, since introductory programming courses. In this way, the teaching of both subjects, programming and testing, becomes an integrated teaching approach. Testing practices in this context can provide a timely feedback to students while they are still working on programming assignments and, as a result, increase the quality of their code. Besides, developing students' testing skills earlier is useful to improve their programming skills as well, since both kinds of skills are complementary. However, this integration is not straightforward, because lecture hours and the coverage of programming topics must remain the same, while testing concepts and practices are introduced. For this reason, when designing an educational approach to introduce testing practices to novice programmers, there is also the need to address potential\u00a0\u2026", "num_citations": "8\n", "authors": ["942"]}
{"title": "A methodological approach to use technological support on teaching and learning data structures\n", "abstract": " The Data Structure discipline consists on studying data organization in computer memory, primary and secondary ones. Data structures are organized using conceptual and behavioral issues that require high degree of abstract representation. It has been observed significant difficulty, by students, to understand the abstract concepts related, which is observed during practical tasks. In fact, practical tasks are essential to consolidate knowledge about data structures, but the students need support to understand the concepts before practical tasks. There are several works in literature showing tools to support conceptual and behavior comprehension, each one with specific features. However, the tools by themselves are not enough, dealing with specific data structure, with severe constraint. In literature, some tools are presented using case studies specific to the data structures supported. And we observed that the tools\u00a0\u2026", "num_citations": "8\n", "authors": ["942"]}
{"title": "Using otsu's threshold selection method for eliminating terms in vector space model computation\n", "abstract": " Visualization techniques have proved to be valuable tools to support textual data exploration. Dimensionality reduction techniques have been widely used to produce visual representation of document collections. Focusing on multidimensional projection techniques, good visual results are produced depending on how representative terms to discriminate the documents are chosen to compose the vector space model (VSM). To define a good VSM it is necessary to apply filters during the preprocessing in order to eliminate terms using their frequency. For that, the user must evaluate the term frequency histogram based on his/her expertise in the text subject and decide the threshold value for frequency cut. Usually it is a trial and error approach that requires the user to verify the quality of visual representation after each trial. In this paper, we propose an automatic approach that applies the Otsu's Threshold Selection\u00a0\u2026", "num_citations": "8\n", "authors": ["942"]}
{"title": "Supporting technical debt cataloging with TD-Tracker tool\n", "abstract": " Technical debt (TD) is an emergent area that has stimulated academic concern. Managers must have information about debt in order to balance time-to-market advantages and issues of TD. In addition, managers must have information about TD to plan payments. Development tasks such as designing, coding, and testing generate different sorts of TD, each one with specific information. Moreover, literature review pointed out a gap in identifying and accurately cataloging technical debt. It is possible to find tools that can identify technical debt, but there is not a described solution that supports cataloging all types of debt. This paper presents an approach to create an integrated catalog of technical debts from different software development tasks. The approach allows tabulating and managing TD properties in order to support managers in the decision process. It also allows managers to track TD. The approach is implemented by TD-Tracker tool, which can integrate different TD identification tools and import identified debts. We present integrations between TD-Tracker and two external tools, used to identify potential technical debts. As part of the approach, we describe how to map the relationship between TD-Tracker and the external tools. We also show how to manage external information within TD-Tracker.", "num_citations": "7\n", "authors": ["942"]}
{"title": "An approach to perform local analysis on multidimensional projection\n", "abstract": " In the context of Visualization, Multidimensional Projection techniques are employed to show similarity relations among instances of a multidimensional dataset. Distinct projection techniques use different approaches to perform the dimensionality reduction and, consequently, different metrics are employed to assess projection quality according to similarity and structures preservation. Usually, quality measures are computed from the whole projection, what can impair a specific evaluation. This work presents a novel approach to perform evaluation on multidimensional projections, in which clusters of instances are selectively evaluated and compared to the whole projection. The proposed approach has shown to be effective on evaluating projections and it offers a way to apply techniques to enhance poor projected areas.", "num_citations": "6\n", "authors": ["942"]}
{"title": "Exptool: a tool to conduct, package and replicate controlled experiments in software engineering\n", "abstract": " Running multiples experiments in Software Engineering introduces the need of recording data as well as transferring knowledge across them, especially considering that several researchers are involved on replicating experiments. For that, experimental evaluations generate knowledge that must be registered into a so-called lab package. Researches have reported difficulties on sharing lab packages due to lack of standardization. In this paper we present a tool to support experimenters to conduct controlled experiments, packaging experimental data. In this first version, experimental data are kept into XML file organized based on an ontology proposed to controlled experiment. The tool support aims at organizing lab packages focusing on facilitating creating and sharing lab packages.", "num_citations": "6\n", "authors": ["942"]}
{"title": "SPML: A Visual Approach for Modeling Firewall Configurations 1\n", "abstract": " This paper describes a graphical notation for modeling security policy, currently focused on firewalls, named SPML. Using SPML, it is possible to specify graphically the security policy to be implemented by firewalls and to configure firewalls at high level, since the rules can be translated to native configuration. To present the approach proposed, we show how to translate SPML models into firewall configuration. 1", "num_citations": "6\n", "authors": ["942"]}
{"title": "Prediction of winners in MOBA games\n", "abstract": " Multiplayer Online Battle Arena (MOBA) games are very popular in the current eSport scenario, being highlighted in several competitions around the world. However, the domain of knowledge contained in these games is large, which makes it difficult to discover and predict the course of a match. The present work proposes the application of classification algorithms to determine the team with more chances to win a match. Two classifications procedures were used, one based on the composition of heroes in each team and another considering the duration of the match. The experiments were performed on data collected from 123,326 matches of Dota 2, showing that it was possible to achieve approximately 77% accuracy. The results demonstrate the effectiveness of the application when using techniques assisted by computers, and when using the methodology described in championships or other similar games\u00a0\u2026", "num_citations": "5\n", "authors": ["942"]}
{"title": "Engenharia de Software I\n", "abstract": " Engenharia de Software I Page 1 \u25cfFCT-UNESP \u25cf04/07/2017 \u25cfProf. Dr. Rog\u00e9rio E. Garcia \u25cf1 Bacharelado em Ci\u00eancia da Computa\u00e7\u00e3o 19/04/2016 Engenharia de Software I Rog\u00e9rio Eduardo Garcia (rogerio@fct.unesp.br) Aula 05 Material preparado por Fernanda Madeiral Delfim BCC 19/04/2016 T\u00f3picos \u2013 Aula 5 \u25cf Contextualiza\u00e7\u00e3o \u25cf UML \u25cf Astah \u25cf Diagramas no Astah 04/07/2017 Ci\u00eancia da Computa\u00e7\u00e3o - Engenharia de Software I - Rog\u00e9rio Eduardo Garcia 2 Page 2 \u25cfFCT-UNESP \u25cf04/07/2017 \u25cfProf. Dr. Rog\u00e9rio E. Garcia \u25cf2 BCC 19/04/2016 An\u00e1lise Vs. Projeto 04/07/2017 Ci\u00eancia da Computa\u00e7\u00e3o - Engenharia de Software I - Rog\u00e9rio Eduardo Garcia 3 BCC 19/04/2016 A/POO \u25cf Como as responsabilidades devem ser atribu\u00eddas a classes de objetos? \u25cf Como os objetos devem interagir? \u25cf Quais classes devem fazer o qu\u00ea? 04/07/2017 Ci\u00eancia da Computa\u00e7\u00e3o - Engenharia de Software I - \u2026", "num_citations": "5\n", "authors": ["942"]}
{"title": "Performance evaluation of data migration methods between the host and the device in CUDA-based programming\n", "abstract": " CUDA-based programming model is heterogeneous \u2013 composed of two components: host (CPU) and device (GPU). Both components have separated memory spaces and processing units. A great challenge to increase GPU-based application performance is the data migration between these memory spaces. Currently, the CUDA platform supports the following data migration methods: UMA, zero-copy, pageable and pinned memory. In this paper, we compare the zero-copy performance method with the other methods by considering the overall application runtime. Additionally, we investigated the aspects of data migration process to enunciate causes of the performance variations. The obtained results demonstrated in some cases the zero-copy memory can provide an average performance on  higher than the pinned memory transfer. In the studied situation, this method was the second most efficient\u00a0\u2026", "num_citations": "5\n", "authors": ["942"]}
{"title": "Supportive environment for teaching and learning digital image processing\n", "abstract": " Digital Image Processing (DIP) consists of a set of techniques to acquire, represent and transform digital images. Through these techniques, it is possible to extract and identify information of images and improve the visual quality by facilitating human perception and interpretation by computer systems. However, the Digital Image Processing teaching is hindered by the complexity of implementation of many techniques, on several occasions is not possible that student visualize the results of DIP techniques when are discussed during the course. This paper presents an experiment over a web platform to support the teaching of digital image processing. The goal of this platform is not replace coding techniques, but allow rapid visualization and experimentation of studied methods, without be necessary to install software or previous knowledge of a specific programming language.", "num_citations": "5\n", "authors": ["942"]}
{"title": "Teaching-learning methodology for formal languages and automata theory\n", "abstract": " Formal languages and automata (FLA) theory have fundamental relevance to the base of knowledge in the computer science area, especially focusing on scientific education. Usually presented by a discipline, the teaching-learning process of FLA is characterized by the high level of abstraction, and it is considered difficult due to the complexity of language formalisms. As support for the learning process, tools have been used to simulate language formalisms. However, the simulation is not enough to reinforce the construction of an abstract concept. In this paper, we present an FLA teaching-learning methodology based on the development of simulators as an approach to clarify the formalism for the students. Through developing their simulators, students are exposed to the data structure and algorithms to handle the formalism. Consequently, students have the opportunity to make the concept concrete.", "num_citations": "5\n", "authors": ["942"]}
{"title": "Simulation and analysis applied on virtualization to build Hadoop clusters\n", "abstract": " The data growth enhances the need of a method and paradigms responsible to deal with high scalability, reliability and fault tolerance in large amounts of data. Big Data is a framework capable of dealing with this need. This research makes usage of Apache Hadoop, and a Virtual Private Server (VPS) to analyze the performance through benchmark tests executed on locally, geographically distributed, and centralized Hadoop computational layout. The result from the simulations metrics, and performance analyses are compared to real servers and introduce an alternative model implemented with a tunnel protocol that enhance the processing power of the cluster.", "num_citations": "5\n", "authors": ["942"]}
{"title": "Multiple Coordinated Views to Support Aspect Mining Using Program Slicing (S).\n", "abstract": " Aspect Mining and Refactoring to Aspects aim to identify crosscutting concerns and encapsulate them in aspects, respectively. Aspect Mining remains as non-automatic process, ie, the user needs to analyze and understand the results generated by techniques/tools, and confirm crosscutting concerns to refactor them to aspects. In this paper we propose a visual approach that deals with results generated by two aspect mining techniques proposed in the literature. By coordinating visual mappings, different levels of detail to explore software artifacts support aspect mining facilitating their interpretation for further refactoring to aspects. The proposed visual approach was implemented (Sof tV iz4AspectMining tool) and in this paper are presented the obtained visualizations, how to interpret them and lessons learned.", "num_citations": "5\n", "authors": ["942"]}
{"title": "A hybrid visualization approach to perform analysis of feature spaces\n", "abstract": " In this paper, we propose a hybrid visualization by combining a projection based approach with star plot visualization to inspect feature spaces. While the projection based visualization is used to depict the instances similarities from high-dimensional spaces onto a bi-dimensional space, the star plot visual metaphor enables inspection of features (attributes) relationship. By inspecting feature spaces, analysts can assess their quality and analyze which features contribute for the formation of clusters. To validate our proposal, we demonstrate how to improve feature spaces to generate more cohesive clusters, as well as how to analyze deep learning features of distinct Convolutional Neural Network (CNN) architectures.", "num_citations": "4\n", "authors": ["942"]}
{"title": "Evaluation of approaches proposed to avoid overlap of markers in visualizations based on multidimensional projection techniques\n", "abstract": " Multidimensional projection techniques provide graphical representations computed based on instance similarities to enable the analysis of abstract and possibly large data sets. However, when the data set size grows these graphical representations can hardly avoid overlap among markers. To overcome this issue, while some techniques attempt to remove overlap after multidimensional projection, some projection techniques were developed considering non-overlapping constraints. In this work, we present an analysis of four overlap removal techniques and two projection techniques considering non-overlapping. The evaluation was performed according to five metrics that consider structural and similarity relations, and based on the results we provide a guide to use a technique according to the data set and analysis goals.", "num_citations": "4\n", "authors": ["942"]}
{"title": "Coordinated multiple views to support image retrieval\n", "abstract": " The number of images available has grown over the years, as well as the number of techniques to aid to organizing and retrieving from image collections. Techniques and systems have been proposed to recover images based on query, in which an image (or words) is used as input parameter and a list of similar images (or images with related text content) is recovered. However, understanding how the retrieved images are related to each other remains as a problem. This paper proposes an approach based on multidimensional visualization and coordination techniques to show the relationship from retrieved images. In this approach, coordination techniques are employed to perform image retrieval methods and highlight the results in visual representations, showing how retrieved images are relate. To evaluate our proposal image collections with and without textual annotations related to each image were used\u00a0\u2026", "num_citations": "4\n", "authors": ["942"]}
{"title": "Coordinated Visualization of Aspect-Oriented Programs\n", "abstract": " Software Visualization provides support to program analysis by visual presentation, improving cognition to comprehension tasks. Focusing specifically on aspect-oriented source code, it\u2019s needed to map features of Aspect-Oriented Program (aspects and advices) into visual presentations, providing suitable visualization for understanding how aspects crosscut usual classes. In this paper we present a proposal of visual mapping for aspect-oriented programs aimed to support Program Comprehension by improving cognition.", "num_citations": "4\n", "authors": ["942"]}
{"title": "Visual analytics of COVID-19 dissemination in S\u00e3o Paulo state, Brazil\n", "abstract": " Visual analytics techniques are useful tools to support decision-making and cope with increasing data, particularly to monitor natural or artificial phenomena. When monitoring disease progression, visual analytics approaches help decision-makers to understand or even prevent dissemination paths. In this paper, we propose a new visual analytics tool for monitoring COVID-19 dissemination. We use k-nearest neighbors of cities to mimic neighboring cities and analyze COVID-19 dissemination based on comparing a city under consideration and its neighborhood. Moreover, such analysis is performed within periods, which facilitates the assessment of isolation policies. We validate our tool by analyzing the progression of COVID-19 in neighboring cities of S\u00e3o Paulo state, Brazil.", "num_citations": "3\n", "authors": ["942"]}
{"title": "Teaching Practices of Software Testing in Programming Education\n", "abstract": " This Research Full Paper presents an overview of the practices that have been used to integrate software testing into programming education. Introductory programming courses compose the core of several undergraduate programs, since programming is a crucial technical skill for professionals in many areas. Given the subject importance, researchers have been conducting several studies to investigate teaching approaches that can help overcoming students\u2019 learning difficulties. In particular, studies on introducing software testing into this context present evidence that testing practices can improve students\u2019 programming performance and habits. There are many teaching approaches in programming education, which involve different choices of programming paradigm and language, support tools and development practices, such as version control. Likewise, the integration of software testing into such diverse\u00a0\u2026", "num_citations": "3\n", "authors": ["942"]}
{"title": "Small private online courses in Computing Learning: evidence, trends and challenges\n", "abstract": " SPOCs (Small Private Online Courses) are considered a variation of MOOCs (Massive Open Online Courses) and intend to facilitate the combination of online resources and technologies with personal engagement between disciplines and learners. The goal of this paper is to provide both an overview and a discussion about SPOCs' trends, benefits and challenges in the scope of Computing teaching. For that, a systematic mapping of literature (SML) was performed. The benefits and challenges, pointed out, by selected papers are grouped and analysed in four different perspectives: students, industry, teachers and Higher Education Institutions. The results show how SPOCs have been applied and it is discussed how these resources can be helpful in Computing teaching.", "num_citations": "3\n", "authors": ["942"]}
{"title": "Visualizing the document pre-processing effects in text mining process\n", "abstract": " Text mining is an important step to categorize textual data by using data mining techniques. As most obtained textual data is unstructured, it needs to be processed before applying mining algorithms \u2013 that process is known as pre-processing step in overall text mining process. Pre-processing step has important impact on mining. This paper aims at providing detailed analysis of the document pre-processing when employing multidimensional projection techniques to generate graphical representations of vector space models, which are computed from eight combinations of three steps: stemming, term weighting and term elimination based on low frequency cut. Experiments were made to show that the visual approach is useful to perceive the processing effects on document similarities and group formation (i.e., cohesion and separation). Additionally, quality measures were computed from graphical\u00a0\u2026", "num_citations": "3\n", "authors": ["942"]}
{"title": "Combined methodology for theoretical computing\n", "abstract": " Theoretical Computer Science area (TCS) stands out by being an important study field, and it is composed by Formal Languages and Automata Theory (FLA), Computer Science Theory (CST), and Theory of Compilers (TC). This area is responsible for introducing the beginnings of the Computer Science through formalisms - which represent a set of methods, techniques, or rules that describe the solution to a problem with restrictions - and it has a substantial impact on the student's knowledge. Computer science theory is based on the understanding of computability and techniques to solve challenges, and to improve the teaching-learning process used to introduce these concepts we proposed a Combined Methodology for Theoretical Computing (CMTC). Our methodology is based on formalism development to ground the knowledge acquired during classes of FLA, CST, and TC, where students are introduced to\u00a0\u2026", "num_citations": "3\n", "authors": ["942"]}
{"title": "Techniques for the identification of crosscutting concerns: A systematic literature review\n", "abstract": " Modularization is a goal difficult to achieve in software development. Some requirements, named crosscutting concerns, cannot be clearly mapped into isolated source code units, and their implementations tend to cut across multiple units. Although several researches propose new approaches to identify crosscutting concerns, few works aim to provide analysis, synthesis and documentation of the aspect mining literature. To address this research gap, we conducted a systematic literature review to provide researchers with a state-of-the-art of the existing aspect mining techniques. We point out challenges and open issues on most of the techniques analyzed that could be improved in further researches.", "num_citations": "3\n", "authors": ["942"]}
{"title": "Scalable information system using event oriented programming and NoSQL\n", "abstract": " One of many challenges in a web information systems is the capability of staying stable and available as the number of users and requests increase rapidly, what we call scalability. This paper provides a model based on development of events-oriented applications, non-blocking in the environment Node.JS with the non-relational database model (MongoDB), preparing to establish scalability and a comparison with a traditional model (relational database (MySQL), HTTP Apache server and PHP language). The results obtained reveal that the proposed model is more efficient (response time of six to eight faster) than the comparative model, can be used in production environments with development quality, performance improvement and scalability.", "num_citations": "3\n", "authors": ["942"]}
{"title": "Validation of user interface model: A systematic literature review\n", "abstract": " Models have been used along software development process. Their utilization to user interface (UI) development aims to deal with dependency of technology, facilitating the understanding of functionalities and behavior, for example. Besides, models have been used to support test activities of UI. However, as usual to any software artifact, we need to assure the confidence of UI models, considering their particularities. In this paper we focused on searching about Which techniques and methods have been used to validate UI models? We are aware of the risk at handling a broad research question. So, we conducted two systematic literature review (SLR). The first one focusing on models (Which models are used for modeling UI?), and the second one focusing on validation (Which techniques and methods are used to validate the models?). The results obtained from first SLR were used as parameters to the second\u00a0\u2026", "num_citations": "3\n", "authors": ["942"]}
{"title": "Computational support for the process of software requirement specification\n", "abstract": " The software industry has become more and more concerned with the appropriate application of activities that composes requirement engineering as a way to improve the quality of its products. In order to support these activities, several computational tools have been available in the market, although it is still possible to find a lack of resources related to some activities. In this context, this paper proposes the inclusion of a module to aid in the requirements specification to a tool called Requirements Elicitation Support Tool. This module allows to specify requirements in accordance with IEEE 830 standard, thus contributing to the documentation of the requirements established for a software system, besides supporting the learning of concepts related to the requirements specification, which improves the skills of users of the tool.", "num_citations": "3\n", "authors": ["942"]}
{"title": "A Strategy to Enhance Computer Science Teaching Material Using Topic Modelling: Towards Overcoming The Gap Between College And Workplace Skills\n", "abstract": " Computer Science teaching materials are biased towards concepts and theoretical aspects. One may consider it difficult to relate concepts to concrete problems. Consequently, it increases the chances of a student not recognizing the relevance of the subject, becoming unmotivated and unprepared to solve practical problems or coping with workplace needs after college. This paper shows the use of social media data as an alternative to minimize the skill gap between what the student learns in college and the skills required in the workplace. The proposed strategy consists of extracting topics from Stack Overflow questions to identify concepts generally unknown or misunderstood and concepts that their practical application represents a challenge. The concepts covered in Stack Overflow questions provide strong cues about how professors and instructors can improve teaching material with useful content for their\u00a0\u2026", "num_citations": "2\n", "authors": ["942"]}
{"title": "Towards the role of test design in programming assignments\n", "abstract": " Software testing can be very helpful to students if adopted in programming assignments throughout the Computer Science curriculum. Many testing practices involve students writing their own test cases. This approach implies that students are responsible for the test design task while performing the test activity. On the other hand, some testing practices follow the opposite approach of providing ready-made test cases, so students only need to execute and evaluate test results for their solution code. In this paper, we investigated the effect of test design in student programming performance. We conducted an experiment comparing two different testing approaches during programming assignments: student-written and instructor-provided test cases. We also assessed students' perceptions of this subject by means of a survey. Results suggest that when students are responsible for test design, i.e. when they write their\u00a0\u2026", "num_citations": "2\n", "authors": ["942"]}
{"title": "Data mining on LinkedIn data to define professional profile via MineraSkill methodology\n", "abstract": " Social networks are of significant analytical interest. This is because their data are generated in great quantity, and intermittently, besides that, the data are from a wide variety, and it is widely available to users. Through such data, it is desired to extract knowledge or information that can be used in decision-making activities. In this context, we have identified the lack of methods that apply data mining techniques to the task of analyzing the professional profile of employees. The aim of such analyses is to detect competencies that are of greater interest by being more required and also, to identify their associative relations. Thus, this work introduces MineraSkill methodology that deals with methods to infer the desired profile of a candidate for a job vacancy. In order to do so, we use keyword detection via natural language processing techniques; which are related to others by inferring their association rules. The results\u00a0\u2026", "num_citations": "2\n", "authors": ["942"]}
{"title": "Teaching-learning firewall configuration using a visual modeling web based tool: The SP2Model and its application to Computer Science course\n", "abstract": " The traffic between computer networks must be controlled to prevent unauthorized access. Firewall is responsible for filtering data packets between networks, applying rules to select data packets that can get in/out to access the network. The firewall must be configured accordingly network access policy. Therefore, the students have to be trained to acquire skills not only to understand network access policy, but also to translate it into firewall native language. The use of a graphical representation (high level) to model the network access policy consist in a resource to facilitate understanding and to minimize defects on firewall native language. For that, we have proposed an extension to Security Policy Modeling Language (SPML), the SPML2, which aims to create a visual representation of the network access policy using graphical notation. Also, in this paper we present SP2Model, a web based tool to support SPML2\u00a0\u2026", "num_citations": "2\n", "authors": ["942"]}
{"title": "Health information system for medical survey analysis\n", "abstract": " The process of data collection involves the organization of a group of questions, where its answers will be analyzed to obtain results. This process is faced with difficulty of standardization and obtaining accurate results, where usually the collect, storage and computation of results are manually realized. Several factors may cause unwanted results, like poorly completed forms, difficulty of interpretation of what was informed and disinterest of who fills it. Not all problems can be simply solved, but HIS (Health Information System) aids the process and data analysis, ensuring consistency and integrity. This work presents WebSISLapam, a web application to collect, organize and analyze data related to medical survey responses, supporting the creation and reuse of forms, questions and quantitative and qualitative variables.", "num_citations": "2\n", "authors": ["942"]}
{"title": "MySQLite Sync: Middleware for stored data synchronization in mobile devices and DBMSs\n", "abstract": " Data synchronization is a process of establishing equivalence among two data collections. It is essential to distribute databases and several techniques have been implemented in Database Management Systems (DBMS), but it is not a feature for free use versions. Focusing on mobile computing, applications that perform data synchronization have become increasingly popular. However, due to lack of internet connection, synchronization process requires specific features to take place in an asynchrony way. The MySQLite Sync middleware, presented in this paper, perform data synchronization among databases stored in mobile devices and DBMS. To validate our proposal we adopted metrics based on time and number of primary and foreign keys mapped after synchronization, in order to maintain data integrity. The results obtained are presented and discussed showing positive contributions.", "num_citations": "2\n", "authors": ["942"]}
{"title": "Uma Proposta de M\u00faltiplas Vis\u00f5es Coordenadas para Apoiar An\u00e1lise de Impacto de Mudan\u00e7a\n", "abstract": " Program Comprehension is an essential task in Software Maintenance. To make a modification, it is necessary to understand the existing program to be maintained not only to make the modification itself, but also to analyze and to identify code fragments that can be impacted by modification. In this paper we present a visual approach that includes six coordinated views to support program comprehension and change impact analysis by visual exploration of different levels of detail. In addition, is shown the tool in which the proposed visual approach was implemented, with their results (visual presentations), and a discussion of its use.Resumo. Compreensao de Programa \u00e9 uma tarefa fundamental na Manuten\u00e7 ao de Software. Ao realizar uma manuten\u00e7ao, \u00e9 necess\u00e1rio entender o programa existente nao somente para efetuar as altera\u00e7 oes em si, mas tamb\u00e9m para analisar e identificar trechos de c\u00f3digo que podem ser impactados pela altera\u00e7ao. Neste trabalho \u00e9 apresentada uma abordagem visual que inclui seis visoes coordenadas para apoiar compreensao de programa e an\u00e1lise de impacto de mudan\u00e7a pela explora\u00e7ao visual de diferentes n\u0131veis de detalhe. Al\u00e9m disso, \u00e9 apresentada a ferramenta na qual foi implementada a abordagem visual proposta, juntamente com os seus resultados (representa\u00e7oes visuais), e uma discussao sobre seu uso.", "num_citations": "2\n", "authors": ["942"]}
{"title": "SLEad: Electronic tongue data analysis system\n", "abstract": " It was developed an \u201celectronic tongue\u201d\u2013Electronic Tongue System (ETS), in Portuguese Sistema de L\u00edngua Eletr\u00f4nica (SLE)\u2013, instrument employing an array of capacitive sensors. Experimental data collected from several sensors are analyzed by Principal Component Analysis (PCA) and Artificial Neural Networks (ANN) techniques using different tools that support each technique separately. This work aims mainly to describe a software tool to perform the data acquisition and the computational tasks involved in data analysis. Software modules were implemented for the PCA and ANN analysis and they were integrated to the SLE instrument. Results obtained here are compared to the ones obtained using other tools, such as WEKA and MatLab, in order to validate the software developed by us.", "num_citations": "2\n", "authors": ["942"]}
{"title": "Contrastive analysis for scatter plot-based representations of dimensionality reduction\n", "abstract": " Exploring multidimensional datasets is a ubiquitous part of the ones working with data, where interpreting clusters is one of the main tasks. These multidimensional datasets are usually encoded using scatter-plots representations, where spatial proximity encodes similarity among data samples. In the literature, techniques try to understand the scatter plot organization by visualizing the importance of the features for clusters definition with interaction and layout enrichment strategies. However, the approaches used to interpret dimensionality reduction usually do not differentiate clusters well, which hampers analysis where the focus is to understand the differences among clusters. This paper introduces a methodology to visually explore multidimensional datasets and interpret clusters' formation based on the contrastive analysis. We also introduce a bipartite graph to visually interpret and explore the relationship between the statistical variables used to understand how the attributes influenced cluster formation. Our methodology is validated through case studies. We explore a multivariate dataset of patients with vertebral problems and two document collections, one related to news articles and other related to tweets about COVID-19 symptoms. Finally, we also validate our approach through quantitative results to demonstrate how it can be robust enough to support multidimensional analysis.", "num_citations": "1\n", "authors": ["942"]}
{"title": "Teacher Mate: A Support Tool for Teaching Code Quality\n", "abstract": " In introductory courses of technology, students with difficulty in programming subjects have led to high dropout rates due to difficulties to understand algorithms, programming language, and paradigm concepts. Furthermore, teachers encounter challenges to support students individually, when adequate feedback is essential to the learning process. This paper presents the Teacher Mate tool to support source code analysis to identify the lack of good programming practices and to evidence students\u2019 difficulties. We performed a case study focusing on identifying the difficulties among six classes of a object-oriented programming course. Our findings show that difficulties are similar in all classes and the students applied some concepts accurately (such as the encapsulation) and fewer violations remain on the source code. Moreover, we show an intervention and its result: based on analysis, the teacher modifies the approach, leading to positive results to support learning both objectoriented concepts and source code quality.", "num_citations": "1\n", "authors": ["942"]}
{"title": "On the Use of Support Mechanisms to Perform Experimental Variables Selection.\n", "abstract": " The selection of variables in a given experiment is crucial, since it is the theoretical foundation that guides how data should be collected and analyzed. However, selecting variables is an intricate activity, especially considering areas such as Software Engineering and Education, whose studies should also consider human-related variables in the design. In this scenario, we aim to investigate how a support mechanism helps on the variables selection activity of the experiment process. To do so, we conducted a preliminary study on the use of an experimental framework composed by a catalog of variables. We explored the domain of the integration of software testing into programming education. Participants were divided into two groups (ad hoc and framework support) and asked to select variables for a given experiment goal. We analyzed the results by identifying threats to validity in their experimental design drafts. Results show a significant number of threats of type inadequate explication of constructs for both groups. Nonetheless, the framework helped to increase the clarity of concepts selected as variables. The cause of most raised threats, even with the framework support, was an inaccuracy in selecting the values of such variables (ie treatments and fixed values).", "num_citations": "1\n", "authors": ["942"]}
{"title": "MOOCs on the Context of Software Engineering Teaching and Training: Trends and Challenges\n", "abstract": " This Research Full Paper presents an analysis of the challenges and advantages on applying MOOCs in software engineering teaching and training contexts. Software engineering is a constantly evolving discipline in which educators are involved with a constant flow of new tools, resources and techniques in software development. This scenario makes the act of teaching and contributing to the students' academic education more complex. The insertion of educational technologies brings contributions in this context, causing a transformation in the current scenario of teaching. An example of these new technologies are the MOOCs (Massive Open Online Courses) - open and online courses that are available in providers in partnership with reputable universities. Considering this scenario, this paper aims at identifying the challenges and trends of MOOCs application in software engineering domain, by means of a\u00a0\u2026", "num_citations": "1\n", "authors": ["942"]}
{"title": "Hadoop Cluster Deployment: A Methodological Approach\n", "abstract": " For a long time, data has been treated as a general problem because it just represents fractions of an event without any relevant purpose. However, the last decade has been just about information and how to get it. Seeking meaning in data and trying to solve scalability problems, many frameworks have been developed to improve data storage and its analysis. As a framework, Hadoop was presented as a powerful tool to deal with large amounts of data. However, it still causes doubts about how to deal with its deployment and if there is any reliable method to compare the performance of distinct Hadoop clusters. This paper presents a methodology based on benchmark analysis to guide the Hadoop cluster deployment. The experiments employed The Apache Hadoop and the Hadoop distributions of Cloudera, Hortonworks, and MapR, analyzing the architectures on local and on clouding\u2014using centralized and geographically distributed servers. The results show the methodology can be dynamically applied on a reliable comparison among different architectures. Additionally, the study suggests that the knowledge acquired can be used to improve the data analysis process by understanding the Hadoop architecture. View Full-Text", "num_citations": "1\n", "authors": ["942"]}
{"title": "Teaching Distributed Systems Using Hadoop\n", "abstract": " Databases and Distributed Systems have a fundamental relevance in Computer Science; they are usually presented in courses where the high-level of abstraction characterizes the teaching and learning processes. Consequently, the teaching method needs to evolve to fulfill the present requirements. Therefore, grounded in these concepts, the main goal of this paper is to introduce a teaching methodology via benchmark tests. Our methodology was conducted using the Hadoop framework, and it is innovative and proved effective. Our methods allow students to be exposed to complex data, system architecture, network infrastructure, trending technologies and algorithms. During the courses, students analyzed the performance of some computational architectures through benchmark tests on local and on the cloud. Along with this scenario, they evaluate the processing time of each architecture. As a result\u00a0\u2026", "num_citations": "1\n", "authors": ["942"]}
{"title": "Using Information Visualization to comprehend user interface layer: an application to web-based systems\n", "abstract": " Web applications have a complex user interface (UI) structure. The lack of formal documentation for web applications has been noticed in literature. If updated, formal documentation (and its models) would be helpful both to development and maintenance tasks. However, UI models are difficult to understand because they must have all information about UI structure, with levels of abstraction and scattered information across multiple diagrams. We propose an UI presentation using Information Visualization techniques, named ModelUIVIZ, to support understanding UI implementation. For that, we implemented two tools: a reverse engineering tool, named WebModelUI Data, to extract data from web applications; and the WebModelUI Tool, to create visual presentations. In order to evaluate ModelUIVIZ, we also conducted a controlled experiment focusing on evaluating the ModelUIVIZ on supporting UI comprehension\u00a0\u2026", "num_citations": "1\n", "authors": ["942"]}
{"title": "Motivating engineering students to math classes: Practical experience teaching ordinary differential equations\n", "abstract": " Teaching mathematics for engineering students is very important. New undergraduate students are aware that they will need mathematical concepts in professional activities. It is still usual to observe not only the difficulty of students in mathematical disciplines, but also the lack of motivation for those disciplines. Motivate and keep motivated the engineering students to mathematical disciplines are a challenge for the teacher. In the literature, it is possible to observe the use of modeling tools of mathematical problems, some of them able to simulate models. Those tools have been used to try to motivate students, but with a technological bias that may introduce difficulties. In this paper we propose an approach to teaching-learning process of mathematical concepts to engineering students with a motivational factor: a practical problem to be modeled in which students must apply concepts seen in class, in particular using\u00a0\u2026", "num_citations": "1\n", "authors": ["942"]}
{"title": "Banco de Dados dispon\u00edvel por APIs usando Restify: Caracter\u00edsticas, Modelos de Programa\u00e7ao e Analise de Desempenho\n", "abstract": " The volume of data exchanged by computer networks is gradually increasing over time, which provides the need for performance and interoperability between different platforms and systems. In this line, there are several studies dedicated to service-oriented software architectures and resource consumption models. However, a few of them are focused on the development of generic tools for the dynamic creation of data provisioning services. This article presents the analysis of a tool called Restify, which is able to dynamically create web services to provide an online database as a service. Restify achieved the system interoperability requirements regarding heterogeneous operations, programming languages, and server infrastructures. As a result, we observed that the performance of this tool was comparable, if not better, than other evaluated web services, such as REST and SOAP. Finally, Restify excels by behaving like an interface tool, allowing the management and integration of multiple online system tools with various relational databases.", "num_citations": "1\n", "authors": ["942"]}
{"title": "Data bases available through APIs using Restify: Characteristics, programming models, and benchmarks\n", "abstract": " The volume of data exchanged by computer networks is gradually increasing over time, which provides the need for performance and interoperability between different platforms and systems. In this line, there are several studies dedicated to service-oriented software architectures and resource consumption models. However, a few of them are focused on the development of generic tools for the dynamic creation of data provisioning services. This article presents the analysis of a tool called Restify, which is able to dynamically create web services to provide an online database as a service. Restify achieved the system interoperability requirements regarding heterogeneous operations, programming languages, and server infrastructures. As a result, we observed that the performance of this tool was comparable, if not better, than other evaluated web services, such as REST and SOAP. Finally, Restify excels by\u00a0\u2026", "num_citations": "1\n", "authors": ["942"]}
{"title": "KDD processes in non-relational data: The case of the MineraMongo tool\n", "abstract": " The process of Knowledge Discovery in Databases, or KDD for short, have been intensively used in tasks focused on searching useful information based on data. The reason is that such data is generated in significant volume, high speed and with a large variety, which makes it require accurate, efficient and scalable methods to handle them. Due to this scenario, several tools and methodologies have been developed to enable data analysis and mining processes. However, there is still a lack of KDD methods based on non-relational data. Considering this scenario, this work aims to present a tool capable of providing selection, preprocessing, transformation, mining and data analysis through databases. Our results consist in a case study, which is used to demonstrate the potential of the MineraMongo tool. This research contributes in a framework of analytical and computational techniques.", "num_citations": "1\n", "authors": ["942"]}
{"title": "Visual approach for change impact analysis: a controlled experiment\n", "abstract": " In the context of Software Maintenance, when a source code element must be changed, there is the need to identify if other elements will be affected by the change, in order to keep the code consistent. This identification is performed during the activity of change impact analysis. Aiming to support maintainers during this activity, software visualization tools allow a visual exploration of source code elements. In this paper, we present a study aimed at evaluating the support provided to change impact analysis by visual representations of Java program elements and their associations. To this end, we conducted a controlled experiment involving 24 undergraduate students, comparing the visual support approach and an ad hoc approach, where only the source code is analyzed to estimate impact change. Results showed that the effectiveness obtained by using the visual approach is significantly superior. This is an\u00a0\u2026", "num_citations": "1\n", "authors": ["942"]}
{"title": "Simula\u00e7\u00e3o e an\u00e1lise aplicada a virtualiza\u00e7\u00e3o para constru\u00e7\u00e3o de Hadoop clusters\n", "abstract": " The data growth enhances the need of a method and paradigms responsible to deal with high scalability, reliability and fault tolerance in large amounts of data. Big Data is a framework capable of dealing with this need. This research makes usage of Apache Hadoop, and a Virtual Private Server (VPS) to analyze the performance through benchmark tests executed on locally, geographically distributed, and centralized Hadoop computational layout. The result from the simulations metrics, and performance analyses are compared to real servers and introduce an alternative model implemented with a tunnel protocol that enhance the processing power of the cluster.", "num_citations": "1\n", "authors": ["942"]}
{"title": "Uma ferramenta para minera\u00e7\u00e3o de aspectos\n", "abstract": " A Minera\u00e7\u00e3o de Aspectos visa a identificar potenciais interesses transversais em c\u00f3digo fonte de programa e a Refatora\u00e7\u00e3o para Aspectos visa a encapsul\u00e1-los em aspectos. A Minera\u00e7\u00e3o de Aspectos \u00e9 um processo n\u00e3o-autom\u00e1tico, pois o usu\u00e1rio precisa analisar e compreender os resultados gerados por t\u00e9cnicas/ferramentas e confirmar interesses transversais para refator\u00e1-los em aspectos. Neste trabalho \u00e9 proposta uma abordagem visual que lida com resultados gerados por duas t\u00e9cnicas de minera\u00e7\u00e3o de aspectos propostas na literatura. Por meio de m\u00faltiplas vis\u00f5es coordenadas, diferentes n\u00edveis de detalhe para explorar sistemas de software ap\u00f3iam a an\u00e1lise e a compreens\u00e3o de tais resultados para futura refatora\u00e7\u00e3o. O modelo de coordena\u00e7\u00e3o, implementado na ferramenta SoftVis4CA, \u00e9 apresentado neste trabalho, juntamente com as visualiza\u00e7\u00f5es e com os resultados preliminares obtidos.", "num_citations": "1\n", "authors": ["942"]}
{"title": "Coordinating multiple views using an ontology-based semantic mapping\n", "abstract": " Multiple views of data sets from the same domain can support to discover unforeseen associations among data elements, but requires users to interact with them. The coordination mechanism must relate elements across multiple views. The mapping among data elements are constrained by using data attributes, and such mapping influences on how multiple views are coordinated. We propose the application of ontology to link data elements based on semantic for specific context. Representing the underlying data into ontology, semantic representation to create the mappings can benefit exploratory visualization. In this paper we show how to use ontology on coordinating multiple views, the initial results using document collections are presented and discussed, in comparison with traditional techniques.", "num_citations": "1\n", "authors": ["942"]}
{"title": "Introdu\u00e7\u00e3o a ontologias e suas aplica\u00e7\u00f5es\n", "abstract": " Diferentes vocabul\u00e1rios e contextos s\u00e3o obst\u00e1culos para a comunica\u00e7\u00e3o entre pessoas ou sistemas de software. \u00c9 preciso um entendimento comum sobre o dom\u00ednio que se conversa, a fim de se obter uma interpreta\u00e7\u00e3o correta das informa\u00e7\u00f5es. Uma ontologia modela formalmente a estrutura de um dom\u00ednio e explicita o seu entendimento compartilhado sob a forma de conceitos e rela\u00e7\u00f5es que emergem de sua observa\u00e7\u00e3o. Constitui-se em uma esp\u00e9cie de arcabou\u00e7o para o mapeamento ao significado da informa\u00e7\u00e3o sobre a qual se conversa. O rigor formal com que s\u00e3o definidas, por meio de axiomas, permite que sejam process\u00e1veis por m\u00e1quina, implicando na interoperabilidade de sistemas. Estruturado dessa forma, o conhecimento \u00e9 facilmente transferido entre pessoas ou sistemas de diferentes contextos. Ontologias apresentam diversas aplica\u00e7\u00f5es atualmente. S\u00e3o consideradas a infraestrutura da Web Sem\u00e2ntica, a qual \u00e9 composta por recursos Web com significado embutido. Dessa forma, permite que sejam executadas tarefas complexas automaticamente, tirando proveito da comunica\u00e7\u00e3o efetiva entre agentes de software da Web. Entre outras aplica\u00e7\u00f5es, tamb\u00e9m vem sendo utilizadas para estruturar o conhecimento gerado por diversas \u00e1reas, como Biologia e Engenharia de Software.", "num_citations": "1\n", "authors": ["942"]}
{"title": "Empirical Software Engineering using Visual Data Analysis\n", "abstract": " Visualization techniques may complement statistical data analysis, helping users to understand and treat data from empirical studies. However, the use of Visual Data Analysis affects the whole experimental process, not only the final analysis stage. In this paper we revisit an experimental software engineering process, indicating how it must be changed to effectively benefit from the inclusion of visual aids.", "num_citations": "1\n", "authors": ["942"]}