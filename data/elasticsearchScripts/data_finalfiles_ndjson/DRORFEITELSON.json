{"title": "Utilization, predictability, workloads, and user runtime estimates in scheduling the IBM SP2 with backfilling\n", "abstract": " Scheduling jobs on the IBM SP2 system and many other distributed-memory MPPs is usually done by giving each job a partition of the machine for its exclusive use. Allocating such partitions in the order in which the jobs arrive (FCFS scheduling) is fair and predictable, but suffers from severe fragmentation, leading to low utilization. This situation led to the development of the EASY scheduler which uses aggressive backfilling: Small jobs are moved ahead to fill in holes in the schedule, provided they do not delay the first job in the queue. We compare this approach with a more conservative approach in which small jobs move ahead only if they do not delay any job in the queue and show that the relative performance of the two schemes depends on the workload. For workloads typical on SP2 systems, the aggressive approach is indeed better, but, for other workloads, both algorithms are similar. In addition, we study\u00a0\u2026", "num_citations": "840\n", "authors": ["150"]}
{"title": "Theory and practice in parallel job scheduling\n", "abstract": " The scheduling of jobs on parallel supercomputer is becoming the subject of much research. However, there is concern about the divergence of theory and practice. We review theoretical research in this area, and recommendations based on recent results. This is contrasted with a proposal for standard interfaces among the components of a scheduling system, that has grown from requirements in the field.", "num_citations": "683\n", "authors": ["150"]}
{"title": "The workload on parallel supercomputers: modeling the characteristics of rigid jobs\n", "abstract": " The analysis of workloads is important for understanding how systems are used. In addition, workload models are needed as input for the evaluation of new system designs, and for the comparison of system designs. This is especially important in costly large-scale parallel systems. Luckily, workload data are available in the form of accounting logs. Using such logs from three different sites, we analyze and model the job-level workloads with an emphasis on those aspects that are universal to all sites. As many distributions turn out to span a large range, we typically first apply a logarithmic transformation to the data, and then fit it to a novel hyper-Gamma distribution or one of its special cases. This is a generalization of distributions proposed previously, and leads to good goodness-of-fit scores. The parameters for the distribution are found using the iterative EM algorithm. The results of the analysis have been codified\u00a0\u2026", "num_citations": "630\n", "authors": ["150"]}
{"title": "Backfilling using system-generated predictions rather than user runtime estimates\n", "abstract": " The most commonly used scheduling algorithm for parallel supercomputers is FCFS with backfilling, as originally introduced in the EASY scheduler. Backfilling means that short jobs are allowed to run ahead of their time provided they do not delay previously queued jobs (or at least the first queued job). However, predictions have not been incorporated into production schedulers, partially due to a misconception (that we resolve) claiming inaccuracy actually improves performance, but mainly because underprediction is technically unacceptable: users will not tolerate jobs being killed just because system predictions were too short. We solve this problem by divorcing kill-time from the runtime prediction and correcting predictions adaptively as needed if they are proved wrong. The end result is a surprisingly simple scheduler, which requires minimal deviations from current practices (e.g., using FCFS as the basis) and\u00a0\u2026", "num_citations": "464\n", "authors": ["150"]}
{"title": "Gang scheduling performance benefits for fine-grain synchronization\n", "abstract": " Multiprogrammed multiprocessors executing fine-grain parallel programs appear to require new scheduling policies. A promising new idea is gang scheduling, where a set of threads are scheduled to execute simultaneously on a set of processors. This has the intuitive appeal of supplying the threads with an environment that is very similar to a dedicated machine. It allows the threads to interact efficiently by using busy waiting, without the risk of waiting for a thread that currently is not running. Without gang scheduling, threads have to block in order to synchronize, thus suffering the overhead of a context switch. While this is tolerable in coarse-grain computations, and might even lead to performance benefits if the threads are highly unbalanced, it causes severe performance degradation in the fine-grain case. We have developed a model to evaluate the performance of different combinations of synchronization\u00a0\u2026", "num_citations": "439\n", "authors": ["150"]}
{"title": "Parallel job scheduling\u2014a status report\n", "abstract": " The popularity of research on the scheduling of parallel jobs demands a periodic review of the status of the field. Indeed, several surveys have been written on this topic in the context of parallel supercomputers [17, 20]. The purpose of the present paper is to update that material, and to extend it to include work concerning clusters and the grid.", "num_citations": "391\n", "authors": ["150"]}
{"title": "Job scheduling in multiprogrammed parallel systems\n", "abstract": " Scheduling in the context of parallel systems is often thought of in terms of assigning tasks in a program to processors, so as to minimize the makespan. This formulation assumes that the processors are dedicated to the program in question. But when the parallel system is shared by a number of users, this is not necessarily the case. In the context of multiprogrammed parallel machines, scheduling refers to the execution of threads from competing programs. This is an operating system issue, involved with resource allocation, not a program development issue. Scheduling schemes for multiprogrammed parallel systems can be classi ed as one or two leveled. Single-level scheduling combines the allocation of processing power with the decision of which thread will use it. Two level scheduling decouples the two issues: rst, processors are allocated to the job, and then the job's threads are scheduled using this pool of processors. The processors of a parallel system can be shared in two basic ways, which are relevant for both one-level and two-level scheduling. One approach is to use time slicing, eg when all the processors in the system (or all the processors in the pool) service a global queue of ready threads. The other approach is to use space slicing, and partition the processors statically or dynamically among the di erent jobs. As these approaches are orthogonal to each other, it is also possible to combine them in various ways; for example, this is often done in gang scheduling. Systems using the various approaches are described, and the implications of the di erent mechanisms are discussed. The goals of this survey are to describe the many di\u00a0\u2026", "num_citations": "386\n", "authors": ["150"]}
{"title": "Packing schemes for gang scheduling\n", "abstract": " Jobs that do not require all processors in the system can be packed together for gang scheduling. We examine accounting traces from several parallel computers to show that indeed many jobs have small sizes and can be packed together. We then formulate a number of such packing algorithms, and evaluate their effectiveness using simulations based on our workload study. The results are that two algorithms are the best: either perform the mapping based on a buddy system of processors, or use migration to re-map the jobs more tightly whenever a job arrives or terminates. Other approaches, such as mapping to the least loaded PEs, proved to be counterproductive. The buddy system approach depends on the capability to gang-schedule jobs in multiple slots, if there is space. The migration algorithm is more robust, but is expected to suffer greatly due to the overhead of the migration itself. In either case\u00a0\u2026", "num_citations": "349\n", "authors": ["150"]}
{"title": "The Vesta parallel file system\n", "abstract": " The Vesta parallel file system is designed to provide parallel file access to application programs running on multicomputers with parallel I/O subsystems. Vesta uses a new abstraction of files: a file is not a sequence of bytes, but rather it can be partitioned into multiple disjoint sequences that are accessed in parallel. The partitioning\u2014which can also be changed dynamically\u2014reduces the need for synchronization and coordination during the access. Some control over the layout of data is also provided, so the layout can be matched with the anticipated access patterns. The system is fully implemented and forms the basis for the AIX Parallel I/O File System on the IBM SP2. The implementation does not compromise scalability or parallelism. In fact, all data accesses are  done directly to the I/O node that contains the requested data, without any indirection or access to shared metadata. Disk mapping and caching\u00a0\u2026", "num_citations": "340\n", "authors": ["150"]}
{"title": "Utilization and predictability in scheduling the IBM SP2 with backfilling\n", "abstract": " Scheduling jobs on the IBM SP2 system is usually done by giving each job a partition of the machine for its exclusive use. Allocating such partitions in the order that the jobs arrive (FCFS scheduling) is fair and predictable, but suffers from severe fragmentation, leading to low utilization. An alternative is to use the EASY scheduler, which uses aggressive backfilling: small jobs are moved ahead to fill in holes in the schedule, provided they do not delay the first job in the queue. The authors show that a more conservative approach, in which small jobs move ahead only if they do not delay any job in the queue, produces essentially the same benefits in terms of utilization. The conservative scheme has the added advantage that queueing times can be predicted in advance, whereas in EASY the queueing time is unbounded.", "num_citations": "335\n", "authors": ["150"]}
{"title": "Workload modeling for computer systems performance evaluation\n", "abstract": " Reliable performance evaluations require the use of representative workloads. This is no easy task since modern computer systems and their workloads are complex, with many interrelated attributes and complicated structures. Experts often use sophisticated mathematics to analyze and describe workload models, making these models difficult for practitioners to grasp. This book aims to close this gap by emphasizing the intuition and the reasoning behind the definitions and derivations related to the workload models. It provides numerous examples from real production systems, with hundreds of graphs. Using this book, readers will be able to analyze collected workload data and clean it if necessary, derive statistical models that include skewed marginal distributions and correlations, and consider the need for generative models and feedback from the system. The descriptive statistics techniques covered are also useful for other domains.", "num_citations": "328\n", "authors": ["150"]}
{"title": "Improved utilization and responsiveness with gang scheduling\n", "abstract": " Most commercial multicomputers use space-slicing schemes in which each scheduling decision has an unknown impact on the future: should a job be scheduled, risking that it will block other larger jobs later, or should the processors be left idle for now in anticipation of future arrivals? This dilemma is solved by using gang scheduling, because then the impact of each decision is limited to its time slice, and future arrivals can be accommodated in other time slices. This added flexibility is shown to improve overall system utilization and responsiveness. Empirical evidence from using gang scheduling on a Cray T3D installed at Lawrence Livermore National Lab corroborates these results, and shows conclusively that gang scheduling can be very effective with current technology.", "num_citations": "319\n", "authors": ["150"]}
{"title": "Optical computing: a survey for computer scientists\n", "abstract": " An in-depth review of the possibilities and limitations of optical data processing is presented, focusing on applications and connections to computer science rather than optical devices or phenomena. Special purpose optical processing systems are addressed, including those involved in optical image and signal processing, optical numerical processing, and hybrid optical/electronic systems. Attempts to realize a general purpose digital optical computer are examined. The possible connections between optical computers and computer science are surveyed, including devices that operate in conjunction with contemporary electronic computers.", "num_citations": "293\n", "authors": ["150"]}
{"title": "Development and Deployment at Facebook\n", "abstract": " Internet companies such as Facebook operate in a \"perpetual development\" mindset. This means that the website continues to undergo development with no predefined final objective, and that new developments are deployed so that users can enjoy them as soon as they're ready. To support this, Facebook uses both technical approaches such as peer review and extensive automated testing, and a culture of personal responsibility.", "num_citations": "286\n", "authors": ["150"]}
{"title": "Job characteristics of a production parallel scientific workload on the NASA Ames iPSC/860\n", "abstract": " Statistics of a parallel workload on a 128-node iPSC/860 located at NASA Ames are presented. It is shown that while the number of sequential jobs dominates the number of parallel jobs, most of the resources (measured in node-seconds) were consumed by parallel jobs. Moreover, most of the sequential jobs were for system administration. The average runtime of jobs grew with the number of nodes used, so the total resource requirements of large parallel jobs were larger by more than the number of nodes they used. The job submission rate during peak day activity was somewhat lower than one every two minutes, and the average job size was small. At night, submission rate was low but job sizes and system utilization were high, mainly due to NQS. Submission rate and utilization over the weekend were lower than on weekdays. The overall utilization was 50%, after accounting for downtime. About 2/3 of\u00a0\u2026", "num_citations": "275\n", "authors": ["150"]}
{"title": "Toward convergence in job schedulers for parallel supercomputers\n", "abstract": " The space of job schedulers for parallel supercomputers is rather fragmented, because different researchers tend to make different assumptions about the goals of the scheduler, the information that is available about the workload, and the operations that the scheduler may perform. We argue that by identifying these assumptions explicitly, it is possible to reach a level of convergence. For example, it is possible to unite most of the different assumptions into a common framework by associating a suitable cost function with the execution of each job. The cost function reflects knowledge about the job and the degree to which it fits the goals of the system. Given such cost functions, scheduling is done to maximize the system's profit.", "num_citations": "271\n", "authors": ["150"]}
{"title": "Parallel job scheduling: Issues and approaches\n", "abstract": " Parallel job scheduling is beginning to gain recognition as an important topic that is distinct from the scheduling of tasks within a parallel job by the programmer or runtime system. The main issue is how to share the resources of the parallel machine among a number of competing jobs, giving each the required level of service. This level of scheduling is done by the operating system. The four most commonly used or advocated techniques are to use a global queue, use variable partitioning, use dynamic partitioning, and use gang scheduling. These techniques are surveyed, and the benefits and shortcomings of each are identified. Then additional requirements that are not addressed by current systems are outlined, followed by considerations for evaluating various scheduling schemes.", "num_citations": "263\n", "authors": ["150"]}
{"title": "Metrics and benchmarking for parallel job scheduling\n", "abstract": " The evaluation of parallel job schedulers hinges on two things: the use of appropriate metrics, and the use of appropriate workloads on which the scheduler can operate. We argue that the focus should be on on-line open systems, and propose that a standard workload should be used as a benchmark for schedulers. This benchmark will specify distributions of parallelism and runtime, as found by analyzing accounting traces, and also internal structures that create different speedup and synchronization characteristics. As for metrics, we present some problems with slowdown and bounded slowdown that have been proposed recently.", "num_citations": "243\n", "authors": ["150"]}
{"title": "Experience with Using the Parallel Workloads Archive\n", "abstract": " Science is based upon observation. The scientific study of complex computer systems should therefore be based on observation of how they are used in practice, as opposed to how they are assumed to be used or how they were designed to be used. In particular, detailed workload logs from real computer systems are invaluable for research on performance evaluation and for designing new systems.Regrettably, workload data may suffer from quality issues that might distort the study results, just as scientific observations in other fields may suffer from measurement errors. The cumulative experience with the Parallel Workloads Archive, a repository of job-level usage data from large-scale parallel supercomputers, clusters, and grids, has exposed many such issues. Importantly, these issues were not anticipated when the data was collected, and uncovering them was not trivial. As the data in this archive is used in\u00a0\u2026", "num_citations": "215\n", "authors": ["150"]}
{"title": "Distributed hierarchical control for parallel processing\n", "abstract": " A description is given of a novel design, using a hierarchy of controllers, that effectively controls a multiuser, multiprogrammed parallel system. Such a structure allows dynamic repartitioning according to changing job requirements. The design goals are examined, and the principles of distributed hierarchical control are presented. Control over processors is discussed. Mapping and load balancing with distributed hierarchical control are considered. Support for gang scheduling as well as availability and fault tolerance is addressed. The use of distributed hierarchical control in memory management and I/O is discussed.< >", "num_citations": "215\n", "authors": ["150"]}
{"title": "Benchmarks and standards for the evaluation of parallel job schedulers\n", "abstract": " The evaluation of parallel job schedulers hinges on the workloads used. It is suggested that this be standardized, in terms of both format and content, so as to ease the evaluation and comparison of different systems. The question remains whether this can encompass both traditional parallel systems and metacomputing systems.             This paper is based on a panel on this subject that was held at the workshop, and the ensuing discussion; its authors are both the panel members and participants from the audience. Naturally, not all of us agree with all the opinions expressed here...", "num_citations": "205\n", "authors": ["150"]}
{"title": "Workload modeling for performance evaluation\n", "abstract": " The performance of a computer system depends on the characteristics of the workload it must serve: for example, if work is evenly distributed performance will be better than if it comes in unpredictable bursts that lead to congestion. Thus performance evaluations require the use of representative workloads in order to produce dependable results. This can be achieved by collecting data about real workloads, and creating statistical models that capture their salient features. This survey covers methodologies for doing so. Emphasis is placed on problematic issues such as dealing with correlations between workload parameters and dealing with heavy-tailed distributions and rare events. These considerations lead to the notion of structural modeling, in which the general statistical model of the workload is replaced by a model of the process generating the workload.", "num_citations": "200\n", "authors": ["150"]}
{"title": "Supporting priorities and improving utilization of the IBM SP scheduler using slack-based backfilling\n", "abstract": " Distributed memory parallel systems such as the IBM SP2 execute jobs using variable partitioning. Scheduling jobs in FCFS order leads to severe fragmentation and utilization loss, which lead to the development of backfilling schedulers such as EASY. This paper presents a backfilling scheduler that improves EAST in two ways: It supports both user selected and administrative priorities, and guarantees a bounded wait time for all jobs. The scheduler gives each waiting job a slack, which determines how long it may have to wait before running: 'important' and 'heavy' jobs will have little slack in comparison with others. Experimental results show that the priority scheduler reduces the average wait time by about 15% relative to EASY in an equal priorities scenario, and is responsive to differential priorities as well.", "num_citations": "190\n", "authors": ["150"]}
{"title": "Paired gang scheduling\n", "abstract": " Conventional gang scheduling has the disadvantage that when processes perform I/O or blocking communication, their processors remain idle because alternative processes cannot be run independently of their own gangs. To alleviate this problem, we suggest a slight relaxation of this rule: match gangs that make heavy use of the CPU with gangs that make light use of the CPU (presumably due to I/O or communication activity), and schedule such pairs together, allowing the local scheduler on each node to select either of the two processes at any instant. As I/O-intensive gangs make light use of the CPU, this only causes a minor degradation in the service to compute-bound jobs. This degradation is more than offset by the overall improvement in system performance due to the better utilization of the resources.", "num_citations": "189\n", "authors": ["150"]}
{"title": "System noise, OS clock ticks, and fine-grained parallel applications\n", "abstract": " As parallel jobs get bigger in size and finer in granularity,\" system noise\" is increasingly becoming a problem. In fact, fine-grained jobs on clusters with thousands of SMP nodes run faster if a processor is intentionally left idle (per node), thus enabling a separation of\" system noise\" from the computation. Paying a cost in average processing speed at a node for the sake of eliminating occasional processes delays is (unfortunately) beneficial, as such delays are enormously magnified when one late process holds up thousands of peers with which it synchronizes. We provide a probabilistic argument showing that, under certain conditions, the effect of such noise is linearly proportional to the size of the cluster (as is often empirically observed). We then identify a major source of noise to be indirect overhead of periodic OS clock interrupts (\" ticks\"), that are used by all general-purpose OSs as a means of maintaining control\u00a0\u2026", "num_citations": "188\n", "authors": ["150"]}
{"title": "Parallel access to files in the Vesta file system\n", "abstract": " The Vesta parallel file system is intended to solve the I/O problems of massively parallel multicomputers executing numerically intensive scientific applications. It provides parallel access from the applications to files distributed across multiple storage nodes in the multicomputer, thereby exposing an opportunity for high-bandwidth data transfer across the multicomputer's low-latency network. The Vesta interface provides a user-defined parallel view of file data, which gives users some control over the layout of data. This is useful for tailoring data layout to much common access patterns. The interface also allows user-defined partitioning and repartitioning of files without moving data among storage nodes. Libraries with higher-level interfaces that hide the layout details, while exploiting the power of parallel access, may be implemented above the basic interface. It is shown how collective I/O operations can be\u00a0\u2026", "num_citations": "172\n", "authors": ["150"]}
{"title": "The elusive goal of workload characterization\n", "abstract": " The study and design of computer systems requires good models of the workload to which these systems are subjected. Until recently, the data necessary to build these models---observations from production installations---were not available, especially for parallel computers. Instead, most models were based on assumptions and mathematical attributes that facilitate analysis. Recently a number of supercomputer sites have made accounting data available that make it possible to build realistic workload models. It is not clear, however, how to generalize from specific observations to an abstract model of the workload. This paper presents observations of workloads from several parallel supercomputers and discusses modeling issues that have caused problems for researchers in this area.", "num_citations": "165\n", "authors": ["150"]}
{"title": "Modeling user runtime estimates\n", "abstract": " User estimates of job runtimes have emerged as an important component of the workload on parallel machines, and can have a significant impact on how a scheduler treats different jobs, and thus on overall performance. It is therefore highly desirable to have a good model of the relationship between parallel jobs and their associated estimates. We construct such a model based on a detailed analysis of several workload traces. The model incorporates those features that are consistent in all of the logs, most notably the inherently modal nature of estimates (e.g. only 20 different values are used as estimates for about 90% of the jobs). We find that the behavior of users, as manifested through the estimate distributions, is remarkably similar across the different workload traces. Indeed, providing our model with only the maximal allowed estimate value, along with the percentage of jobs that have used it, yields\u00a0\u2026", "num_citations": "159\n", "authors": ["150"]}
{"title": "C. elegans multi-dendritic sensory neurons: Morphology and function\n", "abstract": " PVD and FLP sensory neurons envelope the body of the C. elegans adult with a highly branched network of thin sensory processes. Both PVD and FLP neurons are mechanosensors. PVD is known to mediate the response to high threshold mechanical stimuli. Thus PVD and FLP neurons are similar in both morphology and function to mammalian nociceptors. To better understand the function of these neurons we generated strains lacking them. Behavioral analysis shows that PVD and FLP regulate movement under normal growth conditions, as animals lacking these neurons demonstrate higher dwelling behavior. In addition, PVD\u2014whose thin branches project across the body-wall muscles\u2014may have a role in proprioception, as ablation of PVD leads to defective posture. Moreover, movement-dependent calcium transients are seen in PVD, a response that requires MEC-10, a subunit of the mechanosensory DEG\u00a0\u2026", "num_citations": "138\n", "authors": ["150"]}
{"title": "Metrics for parallel job scheduling and their convergence\n", "abstract": " The arrival process of jobs submitted to a parallel system is bursty, leading to fluctuations in the load at many time scales. In particular, rare events of extreme load may occur. Such events lead to an increase in the standard deviation of performance metrics, and thus delay the convergence of simulations used to evaluate the scheduling. Different performance metrics have been proposed in an effort to reduce this variability, and indeed display different rates of convergence. However, there is no single metric that outperforms the others under all conditions. Rather, the convergence of different metrics depends on the system being studied.", "num_citations": "137\n", "authors": ["150"]}
{"title": "The Linux kernel as a case study in software evolution\n", "abstract": " We use 810 versions of the Linux kernel, released over a period of 14\u00a0years, to characterize the system\u2019s evolution, using Lehman\u2019s laws of software evolution as a basis. We investigate different possible interpretations of these laws, as reflected by different metrics that can be used to quantify them. For example, system growth has traditionally been quantified using lines of code or number of functions, but functional growth of an operating system like Linux can also be quantified using the number of system calls. In addition we use the availability of the source code to track metrics, such as McCabe\u2019s cyclomatic complexity, that have not been tracked across so many versions previously. We find that the data supports several of Lehman\u2019s laws, mainly those concerned with growth and with the stability of the process. We also make some novel observations, e.g. that the average complexity of functions is decreasing with\u00a0\u2026", "num_citations": "135\n", "authors": ["150"]}
{"title": "Overview of the MPI-IO parallel I/O interface\n", "abstract": " Thanks to MPI, writing portable message passing parallel programs is almost a reality. One of the remaining problems is file I/O. Although parallel file systems support similar interfaces, the lack of a standard makes developing a truly portable program impossible. It is not feasible to develop large scientific applications from scratch for each generation of parallel machine, and, in the scientific world, a program is not considered truly portable unless it not only compiles, but also runs e ciently. The MPI-IO interface is being proposed as an extension to the MPI standard to fill this need. MPI-IO supports a high-level interface to describe the partitioning of file data among processes, a collective interface describing complete transfers of global data structures between process memories and files, asynchronous I/O operations, allowing computation to be overlapped with I/O, and optimization of physical file layout on storage devices (disks).", "num_citations": "134\n", "authors": ["150"]}
{"title": "Parallel file systems for the IBM SP computers\n", "abstract": " Parallel computer architectures require innovative software solutions to utilize their capabilities. This statement is true for system software no less than for application programs. File system development for the IBM SP product line of computers started with the Vesta research project, which introduced the ideas of parallel access to partitioned files. This technology was then integrated with a conventional Advanced Interactive Executive\u2122 (AIX\u2122) environment to create the IBM AIX Parallel I/O File System product. We describe the design and implementation of Vesta, including user interfaces and enhancements to the control environment needed to run the system. Changes to the basic design that were made as part of the AIX Parallel I/O File System are identified and justified.", "num_citations": "134\n", "authors": ["150"]}
{"title": "Gang scheduling with memory considerations\n", "abstract": " A major problem with time slicing on parallel machines is memory pressure, as the resulting paging activity damages the synchronism among a job's processes. An alternative is to impose admission controls, and only admit jobs that fit into the available memory. Despite suffering from delayed execution, this leads to better overall performance by preventing the harmful effects of paging and thrashing.", "num_citations": "133\n", "authors": ["150"]}
{"title": "Overview of the Vesta parallel file system\n", "abstract": " The Vesta parallel file system provides parallel access from compute nodes to files distributed across I/O nodes in a massively parallel computer. Vesta is intended to solve the I/O problems of massively parallel computers executing numerically intensive scientific applications. Vesta has three interesting characteristics: First, it provides a user defined parallel view of file data, and allows user defined partitioning and repartitioning of files without moving data among I/O nodes. The parallel file access semantics of Vesta directly support the operations required by parallel language I/O libraries. Second, Vesta is scalable to a very large number (many hundreds) of I/O and compute nodes and does not contain any sequential bottlenecks in the data-access path. Third, it provides user-directed checkpointing of files during continuing program execution with very little processing overhead.", "num_citations": "124\n", "authors": ["150"]}
{"title": "Backfilling with lookahead to optimize the performance of parallel job scheduling\n", "abstract": " The utilization of parallel computers depends on how jobs are packed together: if the jobs are not packed tightly, resources are lost due to fragmentation. The problem is that the goal of high utilization may conflict with goals of fairness or even progress for all jobs. The common solution is to use backfilling, which combines a reservation for the first job in the interest of progress with packing of later jobs to fill in holes and increase utilization. However, backfilling considers the queued jobs one at a time, and thus might miss better packing opportunities. We propose the use of dynamic programming to find the best packing possible given the current composition of the queue. Simulation results show that this indeed improves utilization, and thereby reduces the average response time and average slowdown of all jobs.", "num_citations": "112\n", "authors": ["150"]}
{"title": "Backfilling with lookahead to optimize the packing of parallel jobs\n", "abstract": " The utilization of parallel computers depends on how jobs are packed together: if the jobs are not packed tightly, resources are lost due to fragmentation. The problem is that the goal of high utilization may conflict with goals of fairness or even progress for all jobs. The common solution is to use backfilling, which combines a reservation for the first job in the interest of progress with packing of later jobs to fill in holes and increase utilization. However, backfilling considers the queued jobs one at a time, and thus might miss better packing opportunities. We propose the use of dynamic programming to find the best packing possible given the current composition of the queue, thus maximizing the utilization on every scheduling step. Simulations of this algorithm, called lookahead optimizing scheduler (LOS), using trace files from several IBM SP parallel systems, show that LOS indeed improves utilization, and thereby\u00a0\u2026", "num_citations": "110\n", "authors": ["150"]}
{"title": "Design and implementation of the Vesta parallel file system\n", "abstract": " The Vesta parallel file system is designed to provide parallel file access to application programs running on multicomputers with parallel I/O subsystems. Vesta uses a new abstraction of files: a file is not a sequence of bytes, but rather it can be partitioned into multiple disjoint sequences that are accessed in parallel. The partitioning-which can also be changed dynamically-reduces the need for synchronization and coordination during the access. Some control over the layout of data is also provided, so the layout can be marched with the anticipated access patterns. The system is fully implemented, and is beginning to be used by application programmers. The implementation does not compromise scalability or parallelism. In fact, all data accesses are done directly to the I/O node that contains the requested data, without any indirection or access to shared metadata. There are no centralized control points in the\u00a0\u2026", "num_citations": "103\n", "authors": ["150"]}
{"title": "Pitfalls in parallel job scheduling evaluation\n", "abstract": " There are many choices to make when evaluating the performance of a complex system. In the context of parallel job scheduling, one must decide what workload to use and what measurements to take. These decisions sometimes have subtle implications that are easy to overlook. In this paper we document numerous pitfalls one may fall into, with the hope of providing at least some help in avoiding them. Along the way, we also identify topics that could benefit from additional research.", "num_citations": "100\n", "authors": ["150"]}
{"title": "Accelerating multi-media processing by implementing memoing in multiplication and division units\n", "abstract": " This paper proposes a technique that enables performing multi-cycle (multiplication, division, square-root \u2026) computations in a single cycle. The technique is based on the notion of memoing: saving the input and output of previous calculations and using the output if the input is encountered again. This technique is especially suitable for Multi-Media (MM) processing. In MM applications the local entropy of the data tends to be low which results in repeated operations on the same datum. The inputs and outputs of assembly level operations are stored in cache-like lookup tables and accessed in parallel to the conventional computation. A successful lookup gives the result of a multi-cycle computation in a single cycle, and a failed lookup doesn't necessitate a penalty in computation time. Results of simulations have shown that on the average, for a modestly sized memo-table, about 40% of the floating point\u00a0\u2026", "num_citations": "98\n", "authors": ["150"]}
{"title": "Metric and workload effects on computer systems evaluation\n", "abstract": " While a system's performance is a function of its design and implementation, the workload researchers subject the system to and the metrics they use also can affect performance evaluation results. There are two main approaches to performance evaluation: Analysis involves simplifications in the interest of mathematical tractability, and simulation offers a more realistic alternative that directly uses real workload recordings. The problems the author describes appear more frequently during simulation because of its ability to directly reflect complex situations, even those the evaluator does not know about or understand.", "num_citations": "96\n", "authors": ["150"]}
{"title": "MPI-IO: A Parallel File I/O Interface for MPI Version 0.3\n", "abstract": " Thanks to MPI 21], writing portable message passing parallel programs is almost a reality. Paragraph modi edOne of the remaining problems is le I/O. Although parallel le systems support similar interfaces, the lack of a standard makes developing a truly portable program impossible. It is not feasible to develop large scienti c applications from scratch for each generation of parallel machine, and, in the scienti c world, a program is not considered truly portable unless it not only compiles, but also runs e ciently. The signi cant optimizations required for e ciency (eg grouping 25], two-phase Paragraph", "num_citations": "94\n", "authors": ["150"]}
{"title": "Evaluation of design choices for gang scheduling using distributed hierarchical control\n", "abstract": " Gang scheduling\u2014the scheduling of a number of related threads to execute simultaneously on distinct processors\u2014appears to meet the requirements of interactive, multiuser, general-purpose parallel systems.Distributed hierarchical control(DHC) has been proposed as an efficient mechanism for coping with the dynamic processor partitioning necessary to support gang scheduling on massively parallel machines. In this paper, we compare and evaluate different algorithms that can be used within the DHC framework. Regrettably, gang scheduling can leave processors idle if the sizes of the gangs do not match the number of available processors. We show that in DHC this effect can be reduced by reclaiming the leftover processors when the gang size is smaller than the allocated block of processors, and by adjusting the scheduling time quantum to control the adverse effect of badly matched gangs. Consequently\u00a0\u2026", "num_citations": "85\n", "authors": ["150"]}
{"title": "Adaptive parallel job scheduling with flexible coscheduling\n", "abstract": " Many scientific and high-performance computing applications consist of multiple processes running on different processors that communicate frequently. Because of their synchronization needs, these applications can suffer severe performance penalties if their processes are not all coscheduled to run together. Two common approaches to coscheduling jobs are batch scheduling, wherein nodes are dedicated for the duration of the run, and gang scheduling, wherein time slicing is coordinated across processors. Both work well when jobs are load-balanced and make use of the entire parallel machine. However, these conditions are rarely met and most realistic workloads consequently suffer from both internal and external fragmentation, in which resources and processors are left idle because jobs cannot be packed with perfect efficiency. This situation leads to reduced utilization and suboptimal performance\u00a0\u2026", "num_citations": "82\n", "authors": ["150"]}
{"title": "Coscheduling based on runtime identification of activity working sets\n", "abstract": " This paper introduces a method for runtime identification of sets of interacting activities (\u201cworking sets\u201d) with the purpose ofcoscheduling them, i.e., scheduling them so that all the activities in the set execute simultaneously on distinct processors. The identification is done by monitoring access rates to shared communication objects: activities that access the same objects at a high rate thereby interact frequently, and therefore would benefit from coscheduling. Simulation results show that coscheduling with our runtime identification scheme can give better performance than uncoordinated scheduling based on a single global activity queue. The finer-grained the interactions among the activities in a working set, the better the performance differential. Moreover, coscheduling based on automatic runtime identification achieves about the same performance as coscheduling based on manual identification of\u00a0\u2026", "num_citations": "78\n", "authors": ["150"]}
{"title": "Workload sanitation for performance evaluation\n", "abstract": " The performance of computer systems depends, among other things, on the workload. Performance evaluations are therefore often done using logs of workloads on current productions systems, under the assumption that such real workloads are representative and reliable; likewise, workload modeling is typically based on real workloads. We show, however, that real workloads may also contain anomalies that make them non-representative and unreliable. This is a special case of multi-class workloads, where one class is the \"real\" workload which we wish to use in the evaluation, and the other class contaminates the log with \"bogus\" data. We provide several examples of this situation, including a previously unrecognized type of anomaly we call \"workload flurries\": surges of activity with a repetitive nature, caused by a single user, that dominate the workload for a relatively short period. Using a workload with such\u00a0\u2026", "num_citations": "77\n", "authors": ["150"]}
{"title": "From repeatability to reproducibility and corroboration\n", "abstract": " Being able to repeat experiments is considered a hallmark of the scientific method, used to confirm or refute hypotheses and previously obtained results. But this can take many forms, from precise repetition using the original experimental artifacts, to conceptual reproduction of the main experimental idea using new artifacts. Furthermore, the conclusions from previous work can also be corroborated using a different experimental methodology altogether. In order to promote a better understanding and use of such methodologies we propose precise definitions for different terms, and suggest when and why each should be used.", "num_citations": "72\n", "authors": ["150"]}
{"title": "Instability in parallel job scheduling simulation: the role of workload flurries\n", "abstract": " The performance of computer systems depends, among other things, on the workload. This motivates the use of real workloads (as recorded in activity logs) to drive simulations of new designs. Unfortunately, real workloads may contain various anomalies that contaminate the data. A previously unrecognized type of anomaly is workload flurries: rare surges of activity with a repetitive nature, caused by a single user, that dominate the workload for a relatively short period. We find that long workloads often include at least one such event. We show that in the context of parallel job scheduling these events can have a significant effect on performance evaluation results, e.g. a very small perturbation of the simulation conditions might lead to a large and disproportional change in the outcome. This instability is due to jobs in the flurry being effected in unison, a consequence of the flurry's repetitive nature. We therefore\u00a0\u2026", "num_citations": "70\n", "authors": ["150"]}
{"title": "Experimental analysis of the root causes of performance evaluation results: a backfilling case study\n", "abstract": " The complexity of modern computer systems may enable minor variations in performance evaluation procedures to actually determine the outcome. Our case study concerns the comparison of two parallel job schedulers, using different workloads and metrics. It shows that metrics may be sensitive to different job classes, and not measure the performance of the whole workload in an impartial manner. Workload models may implicitly assume that some workload attribute is unimportant and does not warrant modeling; this too can turn out to be wrong. As such effects are hard to predict, a careful experimental methodology is needed in order to find and verify them.", "num_citations": "70\n", "authors": ["150"]}
{"title": "A comparison of workload traces from two production parallel machines\n", "abstract": " The analysis of workload traces from real production parallel machines can aid a wide variety of parallel processing research, providing a realistic basis for experimentation in the management of resources over an entire workload. We analyze a five-month workload trace of an Intel Paragon machine supporting a production parallel workload at the San Diego Supercomputer Center (SDSC), comparing and contrasting it with a similar workload study of an Intel iPSC/860 machine at NASA Ames NAS. Our analysis of workload characteristics takes into account the job scheduling policies of the sites and focuses on characteristics such as job size distribution (job parallelism), resource usage, runtimes, submission patterns, and wait times. Despite fundamental differences in the two machines and their respective usage environments, we observe a number of interesting similarities with respect to job size distribution\u00a0\u2026", "num_citations": "70\n", "authors": ["150"]}
{"title": "Experimental computer science: The need for a cultural change\n", "abstract": " The culture of computer science emphasizes novelty and self-containment, leading to a fragmentation where each research project strives to create its own unique world. This approach is quite distinct from experimentation as it is known in other sciences\u2014ie based on observations, hypothesis testing, and reproducibility\u2014that is based on a presupposed common world. But there are many cases in which such experimental procedures can lead to interesting research results even in computer science. It is therefore proposed that greater acceptance of such activities would be beneficial and should be fostered.", "num_citations": "68\n", "authors": ["150"]}
{"title": "Flexible coscheduling: mitigating load imbalance and improving utilization of heterogeneous resources\n", "abstract": " Fine-grained parallel applications require all their processes to run simultaneously on distinct processors to achieve good efficiency. This is typically accomplished by space slicing, wherein nodes are dedicated for the duration of the run, or by gang scheduling, wherein time slicing is coordinated across processors. Both schemes suffer from fragmentation, where processors are left idle because jobs cannot be packed with perfect efficiency. Obviously, this leads to reduced utilization and sub-optimal performance. Flexible coscheduling (FCS) solves this problem by monitoring each job's granularity and communication activity, and using gang scheduling only for those jobs that require it. Processes from other jobs, which can be scheduled without any constraints, are used as filler to reduce fragmentation. In addition, inefficiencies due to load imbalance and hardware heterogeneity are also reduced because the\u00a0\u2026", "num_citations": "65\n", "authors": ["150"]}
{"title": "Effects of clock resolution on the scheduling of interactive and soft real-time processes\n", "abstract": " It is commonly agreed that scheduling mechanisms in general purpose operating systems do not provide adequate support for modern interactive applications, notably multimedia applications. The common solution to this problem is to devise specialized scheduling mechanisms that take the specific needs of such applications into account. A much simpler alternative is to better tune existing systems. In particular, we show that conventional scheduling algorithms typically only have little and possibly misleading information regarding the CPU usage of processes, because increasing CPU rates have caused the common 100 Hz clock interrupt rate to be coarser than most application time quanta. We therefore conduct an experimental analysis of what happens if this rate is significantly increased. Results indicate that much higher clock interrupt rates are possible with acceptable overheads, and lead to much better\u00a0\u2026", "num_citations": "64\n", "authors": ["150"]}
{"title": "The dynamics of backfilling: solving the mystery of why increased inaccuracy may help\n", "abstract": " Parallel job scheduling with backfilling requires users to provide runtime estimates, used by the scheduler to better pack the jobs. Studies of the impact of such estimates on performance have modeled them using a \"badness factor\" f ges 0 in an attempt to capture their inaccuracy (given a runtime r, the estimate is uniformly distributed in [r, (f + 1) middot r]). Surprisingly, inaccurate estimates (f > 0) yielded better performance than accurate ones (f = 0). We explain this by a \"heel and toe\" dynamics that, with f > 0, cause backfilling to approximate shortest-job first scheduling. We further find the effect of systematically increasing f is V-shaped: average wait time and slowdown initially drop, only to rise later on. This happens because higher fs create bigger \"holes\" in the schedule (longer jobs can backfill) and increase the randomness (more long jobs appear as short), thus overshadowing the initial heel-and-toe preference\u00a0\u2026", "num_citations": "62\n", "authors": ["150"]}
{"title": "Secretly monopolizing the CPU without superuser privileges\n", "abstract": " We describe a \u201ccheat\u201d attack, allowing an ordinary process to hijack any desirable percentage of the CPU cycles without requiring superuser/administrator privileges. Moreover, the nature of the attack is such that, at least in some systems, listing the active processes will erroneously show the cheating process as not using any CPU resources: the \u201cmissing\u201d cycles would either be attributed to some other process or not be reported at all (if the machine is otherwise idle). Thus, certain malicious operations generally believed to have required overcoming the hardships of obtaining root access and installing a rootkit, can actually be launched by non-privileged users in a straightforward manner, thereby making the job of a malicious adversary that much easier. We show that most major general-purpose operating systems are vulnerable to the cheat attack, due to a combination of how they account for CPU usage and how they use this information to prioritize competing processes. Furthermore, recent scheduler changes attempting to better support interactive workloads increase the vulnerability to the attack, and naive steps taken by certain systems to reduce the danger are easily circumvented. We show that the attack can nevertheless be defeated, and we demonstreate this by implementing a patch for Linux that eliminates the problem with negligible overhead.", "num_citations": "59\n", "authors": ["150"]}
{"title": "Overview of the MPI-IO parallel I/O interface\n", "abstract": " Thanks to MPI, writing portable message passing parallel programs is almost a reality. One of the remaining problems is file I/O. Although parallel file systems support similar interfaces, the lack of a standard makes developing a truly portable program impossible. It is not feasible to develop large scientific applications from scratch for each generation of parallel machine, and, in the scientific world, a program is not considered truly portable unless it not only compiles, but also runs efficiently.               The MPI-IO interface is being proposed as an extension to the MPI standard to fill this need. MPI-IO supports a high-level interface to describe the partitioning of file data among processes, a collective interface describing complete transfers of global data structures between process memories and files, asynchronous I/O operations, allowing computation to be overlapped with I/O, and optimization of physical file\u00a0\u2026", "num_citations": "59\n", "authors": ["150"]}
{"title": "On simulation and design of parallel-systems schedulers: are we doing the right thing?\n", "abstract": " It is customary to use open-system trace-driven simulations to evaluate the performance of parallel-system schedulers. As a consequence, all schedulers have evolved to optimize the packing of jobs in the schedule, as a means to improve a number of performance metrics that are conjectured to be correlated with user satisfaction, with the premise that this will result in a higher productivity in reality. We argue that these simulations suffer from severe limitations that lead to suboptimal scheduler designs and to even dismissing potentially good design alternatives. We propose an alternative simulation methodology called site-level simulation, in which the workload for the evaluation is generated dynamically by user models that interact with the system. We present a novel scheduler called CREASY that exploits knowledge on user behavior to directly improve user satisfaction and compare its performance to the original\u00a0\u2026", "num_citations": "58\n", "authors": ["150"]}
{"title": "Memory usage in the LANL CM-5 workload\n", "abstract": " It is generally agreed that memory requirements should be taken into account in the scheduling of parallel jobs. However, so far the work on combined processor and memory scheduling has not been based on detailed information and measurements. To rectify this problem, we present an analysis of memory usage by a production workload on a large parallel machine, the 1024-node CM-5 installed at Los Alamos National Lab. Our main observations are                                    - The distribution of memory requests has strong discrete components, i.e. some sizes are much more popular than others.                                                     - Many jobs use a relatively small fraction of the memory available on each node, so there is some room for time slicing among several memory-resident jobs.                                                     - Larger jobs (using more nodes) tend to use more memory, but it is difficult to characterize the\u00a0\u2026", "num_citations": "53\n", "authors": ["150"]}
{"title": "Parallel job scheduling under dynamic workloads\n", "abstract": " Jobs that run on parallel systems that use gang scheduling for multiprogramming may interact with each other in various ways. These interactions are affected by system parameters such as the level of multiprogramming and the scheduling time quantum. A careful evaluation is therefore required in order to find parameter values that lead to optimal performance. We perform a detailed performance evaluation of three factors affecting scheduling systems running dynamic workloads: multiprogramming level, time quantum, and the use of backfilling for queue management \u2014 and how they depend on offered load. Our evaluation is based on synthetic MPI applications running on a real cluster that actually implements the various scheduling schemes. Our results demonstrate the importance of both components of the gang-scheduling plus backfilling combination: gang scheduling reduces response time and\u00a0\u2026", "num_citations": "49\n", "authors": ["150"]}
{"title": "Graphical user interface for managing text I/O between a user and a parallel program\n", "abstract": " An I/O control window is created on a user's terminal screen when a parallel program is executing. The I/O control window displays an array of graphical elements (preferably small colored squares), which are partitioned into groups of one or more such elements, each partition representing a task (or thread) of the parallel program. In each partition there is one graphical element which represents the I/O status of the task or thread represented by that partition and is called an I/O status indicator. Each I/O status indicator is capable of assuming any one of several different graphical states (each graphical state preferably being a color for the graphical element), one of which indicates that the corresponding task or thread has provided text output that has not been displayed to the user yet and another of which indicates that the corresponding task (or thread) is requesting text input from the user. The user can open a text\u00a0\u2026", "num_citations": "49\n", "authors": ["150"]}
{"title": "On identifying name equivalences in digital libraries\n", "abstract": " The services provided by digital libraries can be much improved by correctly identifying variants of the same name. For example, this will allow for better retrieval of all the works by a certain author. We focus on variants caused by abbreviations of first names, and show that significant achievements are possible by simple lexical analysis and comparison of names. This is done in two steps: first a pairwise matching of names is performed, and then these are used to find cliques of equivalent names. However, these steps can each be performed in a variety of ways. We therefore conduct an experimental analysis using two real datasets to find which approaches actually work well in practice. Interestingly, this depends on the size of the repository, as larger repositories may have many more similar names.", "num_citations": "48\n", "authors": ["150"]}
{"title": "The supercomputer industry in light of the Top500 data\n", "abstract": " The Top500 list, which has been updated semiannually for the past decade (1995-2005), ranks the 500 most powerful computers installed worldwide. Analyzing this data gives an impartial look at the supercomputer industry's current state and development trends, and sheds light on the challenges the industry faces.", "num_citations": "47\n", "authors": ["150"]}
{"title": "ParC\u2014An Extension of C for Shared Memory Parallel Processing\n", "abstract": " ParC is an extension of the C programming language with block\u2010oriented parallel constructs that allow the programmer to express fine\u2010grain parallelism in a shared\u2010memory model. It is suitable for the expression of parallel shared\u2010memory algorithms, and also conducive for the parallelization of sequential C programs. In addition, performance enhancing transformations can be applied within the language, without resorting to low\u2010level programming. The language includes closed constructs to create parallelism, as well as instructions to cause the termination of parallel activities and to enforce synchronization. The parallel constructs are used to define the scope of shared variables, and also to delimit the sets of activities that are influenced by termination or synchronization instructions. The semantics of parallelism are discussed, especially relating to the discrepancy between the limited number of physical\u00a0\u2026", "num_citations": "47\n", "authors": ["150"]}
{"title": "Desktop scheduling: how can we know what the user wants?\n", "abstract": " Current desktop operating systems use CPU utilization (or lack thereof) to prioritize processes for scheduling. This was thought to be beneficial for interactive processes, under the assumption that they spend much of their time waiting for user input. This reasoning fails for modern multimedia applications. For example, playing a movie in parallel with a heavy background job usually leads to poor graphical results, as these jobs are indistinguishable in terms of CPU usage. Suggested solutions involve shifting the burden to the user or programmer, which we claim is unsatisfactory; instead, we seek an automatic solution. Our attempts using new metrics based on CPU usage failed. We therefore propose and implement a novel scheme of identifying interactive and multimedia applications by directly quantifying the I/O between an application and the user (keyboard, mouse, and screen activity). Preliminary results\u00a0\u2026", "num_citations": "46\n", "authors": ["150"]}
{"title": "Parallel I/O subsystems in massively parallel supercomputers\n", "abstract": " Applications executing on massively parallel processors (MPPs) often require a high aggregate bandwidth of lowlatency I/O to secondary storage. In many current MPPs, this requirement has been met by supplying internal parallel I/O subsystems that serve as staging areas for data. Typically, the parallel I/O subsystem is composed of I/O nodes that are linked to the same interconnection network that connects the compute nodes. The I/O nodes each manage their own set of disks. The option of increasing the number of I/O nodes together with the number of compute nodes and with the interconnection network allows for a balanced architecture. We explore the issues motivating the selection of this architecture for secondary storage in MPPs. We survey the designs of some recent and current parallel I/O subsystems, and discuss issues of system configuration, reliability, and file systems.", "num_citations": "45\n", "authors": ["150"]}
{"title": "Improving and stabilizing parallel computer performance using adaptive backfilling\n", "abstract": " The scheduler is a key component in determining the overall performance of a parallel computer, and as we show here, the schedulers in wide use today exhibit large unexplained gaps in performance during their operation. Also, different scheduling algorithms often vary in the gaps they show, suggesting that choosing the correct scheduler for each time frame can improve overall performance. We present two adaptive algorithms that achieve this: One chooses by recent past performance, and the other by the recent average degree of parallelism, which is shown to be correlated to algorithmic superiority. Simulation results for the algorithms on production workloads are analyzed, and illustrate unique features of the chaotic temporal structure of parallel workloads. We provide best parameter configurations for each algorithm, which both achieve average improvements of 10% in performance and 35% in stability for\u00a0\u2026", "num_citations": "43\n", "authors": ["150"]}
{"title": "Self-tuning systems\n", "abstract": " Tuning operating systems helps system administrators improve system performance and efficiency, but it can be time-consuming. These authors propose a mechanism, based on genetic algorithms, to automate this process by running simulations of system performance for various parameter values during the system's idle loop.", "num_citations": "43\n", "authors": ["150"]}
{"title": "Mapping and scheduling in a shared parallel environment using distributed hierarchical control\n", "abstract": " CiNii \u8ad6\u6587 - Mapping and Scheduling in a Shared Parallel Environment Using Distributed Hierarchical Control CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66 \u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 Mapping and Scheduling in a Shared Parallel Environment Using Distributed Hierarchical Control FEITELSON DG \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 FEITELSON DG \u53ce\u9332 \u520a\u884c\u7269 International Conference on Parallel Processing International Conference on Parallel Processing Vol.I, 1-8, 1990 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u6642\u5206\u5272\u7a7a\u9593\u5206\u5272\u30b9\u30b1\u30b8\u30e5\u30fc\u30ea\u30f3\u30b0 \u5800 \u6566\u53f2 , \u77f3\u5ddd \u88d5 , \u5c0f\u4e2d \u88d5\u559c , \u524d\u7530 \u5b97\u5247 , \u53cb\u6e05 \u5b5d\u5fd7 \u60c5\u5831\u51e6\u7406\u5b66\u4f1a\u8ad6\u6587\u8a8c 37(7), 1320-1331, 1996-07-15 \u53c2\u8003\u6587\u732e22\u4ef6 \u88ab\u5f15\u7528\u6587\u732e2\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) \u8cc7\u6599\u96d1\u8a8c\u2026", "num_citations": "41\n", "authors": ["150"]}
{"title": "Distinguishing humans from robots in web search logs: preliminary results using query rates and intervals\n", "abstract": " The workload on web search engines is actually multiclass, being derived from the activities of both human users and automated robots. It is important to distinguish between these two classes in order to reliably characterize human web search behavior, and to study the effect of robot activity. We suggest an approach based on a multi-dimensional characterization of search sessions, and take first steps towards implementing it by studying the interaction between the query submittal rate and the minimal interval of time between different queries.", "num_citations": "39\n", "authors": ["150"]}
{"title": "Effects of variable names on comprehension: an empirical study\n", "abstract": " It is widely accepted that meaningful variable names are important for comprehension. We conducted a controlled experiment in which 9 professional developers try to understand 6 methods from production util classes, either with the original variable names or with names replaced by meaningless single letters. Results show that parameter names are more significant for comprehension than local variables. But, surprisingly, we also found that in 3 of the methods there were no significant differences between the control and experimental groups, due to poor and even misleading variable names. These disturbingly common bad names reflect the subjective nature of naming, and highlight the need for additional research on how variable names are interpreted and how better names can be chosen.", "num_citations": "38\n", "authors": ["150"]}
{"title": "Locality of sampling and diversity in parallel system workloads\n", "abstract": " Observing the workload on a computer system during a short (but not too short) time interval may lead to distributions that are significantly different from those that would be observed over much longer intervals. Rather than describing such phenomena using involved non-stationary models, we propose a simple global distribution coupled with a localized sampling process. We quantify the effect by the maximal deviation between the global distribution and the distribution as observed over a limited slice of time, and find that in real workload data from parallel supercomputers this deviation is significantly larger than would be observed at random. Likewise, we find that the workloads at different sites also differ from each other. These findings motivate the development of adaptive systems, which adjust their parameters as they learn about their workloads, and also the development of parametrized workload models that\u00a0\u2026", "num_citations": "38\n", "authors": ["150"]}
{"title": "Syntax, predicates, idioms\u2014what really affects code complexity?\n", "abstract": " Program comprehension concerns the ability to understand code written by others. But not all code is the same. We use an experimental platform fashioned as an online game-like environment to measure how quickly and accurately 220 professional programmers can interpret code snippets with similar functionality but different structures; snippets that take longer to understand or produce more errors are considered harder. The results indicate, inter alia, that for loops are significantly harder than if s, that some but not all negations make a predicate harder, and that loops counting down are slightly harder than loops counting up. This demonstrates how the effect of syntactic structures, different ways to express predicates, and the use of known idioms can be measured empirically, and that syntactic structures are not necessarily the most important factor. We also found that the metrics of time to understanding\u00a0\u2026", "num_citations": "36\n", "authors": ["150"]}
{"title": "L1 cache filtering through random selection of memory references\n", "abstract": " Distinguishing transient blocks from frequently used blocks enables servicing references to transient blocks from a small fully-associative auxiliary cache structure. By inserting only frequently used blocks into the main cache structure, we can reduce the number of conflict misses, thus achieving higher performance and allowing the use of direct mapped caches which offer lower power consumption and lower access latencies. We suggest using a simple probabilistic filtering mechanism based on random sampling to identify and select the frequently used blocks. Furthermore, by using a small direct-mapped lookup table to cache the most recently accessed blocks in the auxiliary cache, we eliminate the vast majority of the costly fully-associative lookups. Finally, we show that a 16K direct-mapped LI cache, augmented with a fully-associative 2K filter, achieves on average over 10% more instructions per cycle than a\u00a0\u2026", "num_citations": "36\n", "authors": ["150"]}
{"title": "Process prioritization using output production: scheduling for multimedia\n", "abstract": " Desktop operating systems such as Windows and Linux base scheduling decisions on CPU consumption; processes that consume fewer CPU cycles are prioritized, assuming that interactive processes gain from this since they spend most of their time waiting for user input. However, this doesn't work for modern multimedia applications which require significant CPU resources. We therefore suggest a new metric to identify interactive processes by explicitly measuring interactions with the user, and we use it to design and implement a process scheduler. Measurements using a variety of applications indicate that this scheduler is very effective in distinguishing between competing interactive and noninteractive processes.", "num_citations": "36\n", "authors": ["150"]}
{"title": "Probabilistic backfilling\n", "abstract": " Backfilling is a scheduling optimization that requires information about job runtimes to be known. Such information can come from either of two sources: estimates provided by users when the jobs are submitted, or predictions made by the system based on historical data regarding previous executions of jobs. In both cases, each job is assigned a precise prediction of how long it will run. We suggest that instead the whole distribution of the historical data be used. As a result, the whole backfilling framework shifts from a concrete plan for the future schedule to a probabilistic plan where jobs are backfilled based on the probability that they will terminate in time.", "num_citations": "35\n", "authors": ["150"]}
{"title": "Predictive ranking of computer scientists using CiteSeer data\n", "abstract": " The increasing availability of digital libraries with cross\u2010citation data on the Internet enables new studies in bibliometrics. The paper focuses on the list of 10,000 top\u2010cited authors in computer science available as part of CiteSeer. Using data from several consecutive lists a model of how authors accrue citations with time is constructed. By comparing the rate at which individual authors accrue citations with the average rate, predictions are made of how their ranking in the list will change in the future.", "num_citations": "34\n", "authors": ["150"]}
{"title": "Comparing logs and models of parallel workloads using the co-plot method\n", "abstract": " We present a multivariate analysis technique called Co-plot that is especially suitable for samples with many variables and relatively few observations, as the data about workloads often is. Observations and variables are analyzed simultaneously. We find three stable clusters of highly correlated variables, but that the workloads themselves, on the other hand, are rather different from one another. Synthetic models for workload generation are also analyzed, and found to be reasonable; however, each model usually covers well one machine type. This leads us to conclude that a parameterized model of parallel workloads should be built, and we describe guidelines for such a model. Another feature that the models lack is self-similarity: We demonstrate that production logs exhibit this phenomenon in several attributes of the workload, and in contrast that the none of the synthetic models do.", "num_citations": "33\n", "authors": ["150"]}
{"title": "The forgotten factor: facts; on performance evaluation and its dependence on workloads\n", "abstract": " The performance of a computer system depends not only on its design and implementation, but also on the workloads it has to handle. Indeed, in some cases the workload can sway performance evaluation results. It is therefore crucially important that representative workloads be used for performance evaluation. This can be done by analyzing and modeling existing workloads. However, as more sophisticated workload models become necessary, there is an increasing need for the collection of more detailed data about workloads. This has to be done with an eye for those features that are really important.", "num_citations": "30\n", "authors": ["150"]}
{"title": "Parallel I/O systems and interfaces for parallel computers\n", "abstract": " Continued improvements in processor performance have exposed I/O subsystems as a signi cant bottleneck, which prevents applications from achieving full system utilization 33, 54]. This problem is exacerbated in massively parallel processors (MPPs), where multiple processors are used together. As a result, I/O subsystems have become the focus of much research, leading to the design of parallel I/O hardware and matching system software.The requirement driving the work on I/O subsystems is the desire to achieve a balanced system 8]. The degree to which a system is balanced is typically expressed by the F= b ratio, which is de ned as the ratio of the rate of executing oating point operations (F) to the rate of performing I/O, in bits per second (b). A widely accepted rule of thumb, attributed to Amdahl, calls for F= b 1. While this was originally expressed in instructions rather than oating point operations, there is evidence that this requirement holds for computationally intensive numerical codes as well 20]. Given the high rate of increase in performance of processors, and the lower improvement rate of disks, F= b 1 leads to the use of multiple disks in parallel. This has the advantage of being able to use multiple heads at once, increasing throughput, but introduces reliability problems. The common solution is to encode the data with some level of redundancy, so that if one disk fails the data can be reconstructed from the others 33]. The resulting organization is called a RAID, for Redundant Array of Independent Disks. The encoding typically involves calculating the parity of data striped across a set of disks, and storing the parity itself on another disk\u00a0\u2026", "num_citations": "30\n", "authors": ["150"]}
{"title": "The blueprint for life?\n", "abstract": " One of the greatest scientific discoveries of the twentieth century is the structure of DNA and how it encodes proteins. Current genome projects, especially the Human Genome Project, have sparked interest in the information encoded in DNA, which is often referred to as \"the blueprint for life\", implying that it contains all the information needed to create life. This interpretation ignores the complex interactions between DNA and its cellular environment, interactions that regulate and control the spatial and temporal patterns of gene expression. Moreover, the particulars of many cellular structures seem not to be encoded in DNA, and they are never created from scratch, rather, each cell inherits templates for these structures from its parent cell. Thus, it is not clear that DNA directly or indirectly encodes all life processes, casting doubt on the belief that we can understand them solely by studying DNA sequences. The paper\u00a0\u2026", "num_citations": "29\n", "authors": ["150"]}
{"title": "Reducing performance evaluation sensitivity and variability by input shaking\n", "abstract": " Simulations sometimes lead to observed sensitivity to configuration parameters as well as inconsistent performance results. The question is then what is the true effect and what is a coincidental artifact of the evaluation. The shaking methodology answers this by executing multiple simulations under small perturbations to the input workload, and calculating the average performance result; if the effect persists we can be more confident that it is real, whereas if it disappears it was an artifact. We present several examples where the sensitivity that appears in results based on a single evaluation is eliminated or considerably reduced by the shaking methodology. While our examples come from evaluations of scheduling algorithms for supercomputers, we believe the method has wider applicability.", "num_citations": "28\n", "authors": ["150"]}
{"title": "Fine grained kernel logging with klogger: Experience and insights\n", "abstract": " Understanding the detailed behavior of an operating system is crucial for making informed design decisions. But such an understanding is very hard to achieve, due to the increasing complexity of such systems and the fact that they are implemented and maintained by large and diverse groups of developers. Tools like KLogger---presented in this paper---can help by enabling fine-grained logging of system events and the sharing of a logging infrastructure between multiple developers and researchers, facilitating a methodology where design evaluation can be an integral part of kernel development. We demonstrate the need for such methodology by a host of case studies, using KLogger to better understand various subsystems in the Linux kernel, and pinpointing overheads and problems therein.", "num_citations": "28\n", "authors": ["150"]}
{"title": "Perpetual development: A model of the Linux kernel life cycle\n", "abstract": " Software evolution is widely recognized as an important and common phenomenon, whereby the system follows an ever-extending development trajectory with intermittent releases. Nevertheless there have been only few lifecycle models that attempt to portray such evolution. We use the evolution of the Linux kernel as the basis for the formulation of such a model, integrating the progress in time with growth of the codebase, and differentiating between development of new functionality and maintenance of production versions. A unique element of the model is the sequence of activities involved in releasing new production versions, and how this has changed with the growth of Linux. In particular, the release follow-up phase before the forking of a new development version, which was prominent in early releases of production versions, has been eliminated in favor of a concurrent merge window in the release of 2.6.x\u00a0\u2026", "num_citations": "27\n", "authors": ["150"]}
{"title": "Metrics for mass-count disparity\n", "abstract": " Mass-count disparity is the technical underpinning of the \"mice and elephants\" phenomenon - that most samples are small, but a few are huge - which may be the most important attribute of heavy-tailed distributions. We propose to visualize this phenomenon by plotting the conventional distribution and the mass distribution together in the same plot. This then leads to a natural quantification of the effect based on the distance between the two distributions. Such a quantification addresses this important phenomenon directly, taking the full distribution into account, rather than focusing on the mathematical properties of the tail of the distribution. In particular, it shows that the Pareto distribution with tail index 1 \\le a \\le 2 actually has a relatively low mass-count disparity; the effects often observed are the result of combining some other distribution with a Pareto tail.", "num_citations": "27\n", "authors": ["150"]}
{"title": "Wasted resources in gang scheduling\n", "abstract": " Gang scheduling (the scheduling of a number of interacting threads to run simultaneously on distinct processors) can leave processors idle if the sizes of the gangs do not match the number of available processors. Given an optimal offline algorithm it is shown that the wasted processing power can range from 0% to 50%, depending on the distribution of gang sizes. Focusing on the uniform and the harmonic distributions, rather than worst-case distributions, it is shown that if there are no restrictions on how the processors are partitioned, these distributions cause no waste with an offline algorithm but a waste of 20% to 37% for online algorithms. Using the distributed hierarchical control scheme, which is similar to buddy systems for memory allocation, a waste of 10% to 20% should be expected for offline algorithms, and 21% to 51% for online algorithms.< >", "num_citations": "27\n", "authors": ["150"]}
{"title": "Using site-level modeling to evaluate the performance of parallel system schedulers\n", "abstract": " The conventional performance evaluation methodology for parallel system schedulers uses an open model to generate the workloads used in simulations. In many cases recorded workload traces are simply played back, assuming that they are reliable representatives of real workloads, and leading to the expectation that the simulation results actually predict the scheduler\u2019s true performance. We show that the lack of feedback in these workloads results in performance prediction errors, which may reach hundreds of percents. We also show that load scaling, as currently performed, further ruins the representativeness of the workload, by generating conditions which cannot exist in a real environment. As an alternative, we suggest a novel sitelevel modeling evaluation methodology, in which we model not only the actions of the scheduler but also the activity of users who generate the workload dynamically. This\u00a0\u2026", "num_citations": "26\n", "authors": ["150"]}
{"title": "Hardware memoization of mathematical and trigonometric functions\n", "abstract": " Memoization is saving the input and output of previous calculations and using the output if the input is encountered again. memotables are cache-like tables that store the operands and results of calculations that are candidates for memoization. A successful lookup gives the result of a multi-cycle computation in a single cycle, and a failed lookup doesn't necessitate a penalty in computation time. Thus even a modest success rate can entail a computational speedup. This paper shows how the concept of memoization may be successfully used in the hardware implementations of common mathematical and trigonometric functions (log, exp, sin, cos...) on general-purpose or application-speci c oating point processors. Alternatively we introduce two new instructions to lookup and update a generic memotable. The use of these instructions enables the memoization of software implemented functions written by the user or existing in prewritten libraries. Integrating the new instructions into existing code is possible without altering the function code or calling conventions, with small or non-existent compiler changes, and with a minimal computational overhead.", "num_citations": "26\n", "authors": ["150"]}
{"title": "Uncovering the effect of system performance on user behavior from traces of parallel systems\n", "abstract": " Intuitively, it seems that understanding how the performance of a system affects its users requires research in psychology and the conducting of live experiments. We demonstrate that it is possible to uncover the effect from traces of the system. In particular, we found that the behavior of users of parallel systems is correlated with the response time of their jobs, not the slowdown as was previously assumed. We show that response times affect the decision of users to continue or abort their interactive session with the system, and that this may relate to expectations the users develop. Although this research was conducted in the context of parallel systems, we believe our results are more general and may pertain to other types of systems as well.", "num_citations": "24\n", "authors": ["150"]}
{"title": "Using multicast to pre-load jobs on the ParPar cluster\n", "abstract": " The ParPar system is a high-performance cluster environment supporting a multiuser parallel workload. Its design follows a master-nodes structure, where the master controls all aspects of system activity using a dedicated control network. As nearly all control messages are multicast to a set of nodes, we implemented a reliable multicast protocol for this network based on UDP. This was then used to pre-load executable files to the nodes, rather than using demand paging via NFS. Such pre-loading leads to significant reductions in job startup times in most cases. It is also more scalable than an asymmetrical hardware approach giving the master higher bandwidth, which can be used for small clusters.", "num_citations": "24\n", "authors": ["150"]}
{"title": "Performance of the Vesta parallel file system\n", "abstract": " Vesta is an experimental parallel file system implemented on the IBM SPI. Its main features are support for parallel access from multiple application processes to file, and the ability to partition and re-partition the file data among these processes. This paper reports on a set of experiments designed to evaluate Vesta's performance. This includes basic single-node performance, and performance using parallel access with different file partitioning schemes. Results are that bandwidth scales with the number of I/O nodes accessed, and that orthogonal partitioning schemes achieve essentially the same performance. In many cases performance equals the disk hardware limit. This is often attributed to prefetching and write-behind in the I/O nodes.< >", "num_citations": "24\n", "authors": ["150"]}
{"title": "Meaningful identifier names: the case of single-letter variables\n", "abstract": " It is widely accepted that variable names in computer programs should be meaningful, and that this aids program comprehension. \"Meaningful\" is commonly interpreted as favoring long descriptive names. However, there is at least some use of short and even single-letter names: using 'i' in loops is very common, and we show (by extracting variable names from 1000 popular github projects in 5 languages) that some other letters are also widely used. In addition, controlled experiments with different versions of the same functions (specifically, different variable names) failed to show significant differences in ability to modify the code. Finally, an online survey showed that certain letters are strongly associated with certain types and meanings. This implies that a single letter can in fact convey meaning. The conclusion from all this is that single letter variables can indeed be used beneficially in certain cases, leading to\u00a0\u2026", "num_citations": "23\n", "authors": ["150"]}
{"title": "Workload resampling for performance evaluation of parallel job schedulers\n", "abstract": " Evaluating the performance of a computer system is based on using representative workloads. Common practice is to either use real workload traces to drive simulations or use statistical workload models that are based on such traces. Such models allow various workload attributes to be manipulated, thus providing desirable flexibility, but may lose details of the workload's internal structure. To overcome this, we suggest to combine the benefits of real traces and flexible modeling. Focusing on the problem of evaluating the performance of parallel job schedulers, we partition the trace of submitted jobs into independent subtraces representing different users and then recombine them in various ways, while maintaining features such as long\u2010range dependence and the daily and weekly cycles of activity. This facilitates the creation of longer workload traces that enable longer simulations, the creation of multiple\u00a0\u2026", "num_citations": "23\n", "authors": ["150"]}
{"title": "On extracting session data from activity logs\n", "abstract": " Activity logs from large-scale systems facilitate the study of user behavior, which can be used to improve and tune the user experience. However, the available data often lacks important elements such as the identification of user sessions. Previous work typically compensated for this by setting a threshold of around 30 minutes, and assuming that breaks in activity longer than the threshold reflect breaks between sessions. We show that using such a global threshold introduces artifacts that may affect the analysis, because there is a high probability that long sessions are not identified correctly. As an alternative, we suggest that a suitable individual threshold be found for each user, based on that user's activity pattern. Applying this approach to a large dataset from the AOL search engine leads to a distribution of session durations that is free of artifacts like those that appear when using a global threshold.", "num_citations": "23\n", "authors": ["150"]}
{"title": "Revisiting instruction level reuse\n", "abstract": " The concept of instruction reuse states that execution of a dynamic instruction instance that has been executed in the past can be avoided. Data associated with each dynamic instruction (register values, register names,...) is stored in on-chip dedicated lookup tables and before an instruction is executed it is looked up in the table. If a \u201cmatch\u201d occurs the result (which must be stored in the table as well) is extracted from the table, instruction execution is avoided, and the instruction is passed directly to the commit buffer. This paper revisits the concept by repeating and widening the scope of past simulations, in particular the work of Sodani and Sohi on \u201cDynamic Instruction Reuse\u201d and our previous work (Citron, Feitelson, and Rudolph) on \u201cInstruction Memoization\u201d. The former targets all instructions for potential reuse while the latter focuses on long latency instruction only (multiplication and division). This paper will answer the obvious question:\u201cHow can a single-cycle table lookup speedup a single-cycle execution?\u201d In a nutshell the answer is simple: The lookup table is used as an additional functional unit. An instruction that would have met with a structural hazard is \u201cexecuted\u201d by the LUT.", "num_citations": "23\n", "authors": ["150"]}
{"title": "Compact graphical parallel program user output interface controlled directly by the parallel computer program\n", "abstract": " An output window is created on the user's terminal screen when a parallel program is executing. This window displays an array of graphical elements (each preferably a small square area), which are partitioned into groups of one or more graphical elements per group, each partition representing a task or thread of the parallel program. These graphical elements are capable of assuming any one of several (or many) different graphical states (each of these states preferably being a different color for the graphical clement). A task running on a parallel processor system can set its associated graphical elements to different states (eg, colors) during execution of the task generally through a special instruction in the task that specifics which graphical clement (of the graphical elements assigned to that task) should be set and to what state. When such an instruction is executed by a processor running that task, a message is\u00a0\u2026", "num_citations": "23\n", "authors": ["150"]}
{"title": "On the interpretation of Top500 data\n", "abstract": " A novel representation of Top500 data is proposed in which the systems are plotted in                 rank and number-of-processors coordinates. This rendering brings to light various                 trends, such as complete Japanese domination of the range of high rank achieved with                 a small number of processors. In addition, an evolutionary trend whereby the rank of                 a given machine doubles every year is identified. This, in turn, leads to a                 characterization of the distribution of computing power across the whole list.", "num_citations": "22\n", "authors": ["150"]}
{"title": "Locomotion analysis identifies roles of mechanosensory neurons in governing locomotion dynamics of C. elegans\n", "abstract": " The simple and well-characterized nervous system of C. elegans facilitates the analysis of mechanisms controlling behavior. Locomotion is a major behavioral output governed by multiple external and internal signals. Here, we examined the roles of low- and high-threshold mechanosensors in locomotion, using high-resolution and detailed analysis of locomotion and its dynamics. This analysis revealed a new role for touch receptor neurons in suppressing an intrinsic direction bias of locomotion. We also examined the response to noxious mechanical stimuli, which was found to involve several locomotion properties and to last several minutes. Effects on different locomotion properties have different half-lives and depend on different, partly overlapping sets of sensory neurons. PVD and FLP, high-threshold mechanosensors, play a major role in some of these responses. Overall, our results demonstrate the\u00a0\u2026", "num_citations": "21\n", "authors": ["150"]}
{"title": "On Identifying User Session Boundaries in Parallel Workload Logs\n", "abstract": " The stream of jobs submitted to a parallel supercomputer is actually the interleaving of many streams from different users, each of which is composed of sessions. Identifying and characterizing the sessions is important in the context of workload modeling, especially if a user-based workload model is considered. Traditionally, sessions have been delimited by long think times, that is, by intervals of more than, say, 20 minutes from the termination of one job to the submittal of the next job. We show that such a definition is problematic in this context, because jobs may be extremely long. As a result of including each job\u2019s execution in the session, we may get unrealistically long sessions, and indeed, users most probably do not always stay connected and wait for the termination of long jobs. We therefore suggest that sessions be identified based on proven user activity, namely the submittal of new jobs, regardless\u00a0\u2026", "num_citations": "21\n", "authors": ["150"]}
{"title": "A co-plot analysis of logs and models of parallel workloads\n", "abstract": " We present a multivariate analysis technique called Co-Plot that is especially suitable for few samples of many variables. Co-Plot embeds the multidimensional samples in two dimensions, in a way that allows key variables to be identified, and relations between both variables and observations to be analyzed together. When applied to the workloads on parallel supercomputers, we find two stable perpendicular axes of highly correlated variables, one representing individual job attributes and the other representing multijob attributes. The different workloads, on the other hand, are rather different from one another, and may also change over time. Synthetic models for workload generation are also analyzed, and found to be reasonable in the sense that they span the same range of variable combinations as the real workloads. However, the spread of real workloads implies that a single model cannot be similar to all of\u00a0\u2026", "num_citations": "21\n", "authors": ["150"]}
{"title": "An empirically-based criterion for determining the success of an open-source project\n", "abstract": " In order to determine a success criterion for open-source software projects, we analyzed 122,205 projects in the SourceForge database. There were 80,597 projects with no downloads at all. We restricted our analysis to the 41,608 projects that together were downloaded 704,897,520 times. Contrary to what we had expected, the distribution of the number of downloads of each project is not Zipf-like; only a portion of the log-log plot of the number of downloads and their rank appears to be a straight line. We performed least-squares analysis (utilizing the Bayesian information criterion) to divide the plot into three segments. On the basis of the shapes of the corresponding curves and the locations of their boundary points, we categorized the projects as follows: 85 superprojects (highly successful projects with more than 1.1 million downloads); just over 10,000 successful projects (with more than 1680 downloads each\u00a0\u2026", "num_citations": "21\n", "authors": ["150"]}
{"title": "A case for conservative workload modeling: Parallel job scheduling with daily cycles of activity\n", "abstract": " Computer workloads have many attributes. When modeling these workloads it is often difficult to decide which attributes are important, and which can be abstracted away. In many cases, the modeler only includes attributes that are believed to be important, and ignores the rest. We argue, however, that this can lead to impaired workloads and unreliable system evaluations. Using parallel job scheduling as a case study, and daily cycles of activity as the attribute in dispute, we present two schedulers whose simulated performance seems identical without cycles, but then becomes significantly different when daily cycles are included in the workload. We trace this to the ability of one scheduler to prioritize interactive jobs, which leads to implicitly delaying less critical work to nighttime, when it can utilize resources that otherwise would have been left idle. Notably, this was not a design feature of this scheduler, but rather\u00a0\u2026", "num_citations": "20\n", "authors": ["150"]}
{"title": "Comparing Windows NT, Linux, and QNX as the basis for cluster systems\n", "abstract": " Clusters use commodity hardware and software components to provide an environment for high\u2010performance parallel processing. A major issue in the development of a cluster system is the choice of the operating system that will run on each node. We compare three alternatives: Windows NT, Linux, and QNX\u2014a real\u2010time microkernel. The comparison is based on expressive power, performance, and ease\u2010of\u2010use metrics. The result is that none of these systems has a clear advantage over the others in all the metrics, but that each has its strong and weak points. Thus any choice of a base system will involve some technical compromises, but not major ones. Copyright \u00a9 2001 John Wiley & Sons, Ltd.", "num_citations": "20\n", "authors": ["150"]}
{"title": "Heuristics for Resource Matching in Intel\u2019s Compute Farm\n", "abstract": " In this paper we investigate the issue of resource matching between jobs and machines in Intel\u2019s compute farm. We show that common heuristics such as Best-Fit and Worse-Fit may fail to properly utilize the available resources when applied to either cores or memory in isolation. In an attempt to overcome the problem we propose Mix-Fit, a heuristic which attempts to balance usage between resources. While this indeed usually improves upon the single-resource heuristics, it too fails to be optimal in all cases. As a solution we default to Max-Jobs, a meta-heuristic that employs all the other heuristics as sub-routines, and selects the one which matches the highest number of jobs. Extensive simulations that are based on real workload traces from four different Intel sites demonstrate that Max-Jobs is indeed the most robust heuristic for diverse workloads and system configurations, and provides up to 22\u00a0\u2026", "num_citations": "19\n", "authors": ["150"]}
{"title": "Job Scheduling Strategies for Parallel Processing: 9th International Workshop, Jsspp 2003, Seattle, Wa, Usa, June 24, 2003: Revised Papers (Lecture Notes in Computer Science, 2862)\n", "abstract": " This book constitutes the thoroughly refereed postproceedings of the 9th International Workshop on Job Scheduling Strategies for Parallel Processing, JSSPP 2003, held in Seattle, Washington in June 2003 in conjunction with HPDi12 and FFG-8. The 13 revised full papers presented were carefully refereed and selected during two rounds of reviewing and revision. The papers present state-of-the-art research results in the area with particular emphasis on conventional parallel systems (including infrastructure scheduling algorithms, I/O issues, and QoS), on scheduling issues in the context of grid computing, and on methodological aspects of performance evaluation in parallel job scheduling.", "num_citations": "19\n", "authors": ["150"]}
{"title": "The ParPar system: a software MPP\n", "abstract": " To place ParPar1. in context, we must rst review the di erent modes of operation common on clusters. Probably the most common approach is to view the cluster as a Network Of Workstations (NOW). With this approach, each node is owned by a certain individual, and is usually also physically located in his work area. The owner uses his workstation for administrative work, such as e-mail, and also for processing, eg text processing or engineering applications. But such work typically consumes only a fraction of the workstation's resources. The remaining resources are therefore available for general use by others, who need more resources than their local workstations can provide. Examples of projects based on this approach are the Berkeley NOW, Condor, and MOSIX 3]. At the other extreme are clusters dedicated to a single user at a time, with the explicit goal of executing parallel programs. The motivation is to provide resources for the solution of very demanding problems, and use them as e ciently as possible. Clusters, due to their use of commodity components, are cheap enough to make this possible. The best know system of this type is the Beowulf project. ParPar is related to this approach, but attempts to provide an inexpensive approximation of", "num_citations": "19\n", "authors": ["150"]}
{"title": "From obfuscation to comprehension\n", "abstract": " Code obfuscation techniques are widely used in industry to increase protection of source code and intellectual property. The idea is that even if attackers gain hold of source code, it will be hard for them to understand what it does and how. Thus obfuscation techniques are specifically targeted at human comprehension of code. We suggest that the ideas and experience embedded in obfuscations can be used to learn about comprehension. In particular, we survey known obfuscation techniques and use them in an attempt to derive metrics for code (in) comprehensibility. This leads to emphasis on issues such as identifier naming, which are typically left on the sidelines in discussions of code comprehension, and motivates increased efforts to measure their effect.", "num_citations": "18\n", "authors": ["150"]}
{"title": "General-purpose timing: The failure of periodic timers\n", "abstract": " All general-purpose commodity operating systems use periodic clock interrupts to regain control and measure the passage of time. This is ill-suited for desktop settings, as the fine-grained timing requirements of modern multimedia applications require a high clock rate, which may suffer from significant overhead. It is ill-suited for HPC environments, as asynchronous interrupts ruin the coordination among cluster nodes. And it is ill-suited for mobile platforms, as it wastes significant energy, especially when the system is otherwise idle. To be truly general-purpose, systems should therefore switch to a mechanism that is closer to one-shot timers (set only for specific needs) while avoiding the potentially huge overhead they entail. With a careful design it is possible to achieve both high accuracy and low overhead, thus significantly extending the applicability of general-purpose operating systems.", "num_citations": "18\n", "authors": ["150"]}
{"title": "Hierarchical indexing and document matching in BoW\n", "abstract": " BoW is an on-line bibliographical repository based on a hierarchical c oncept index to which entries are linked. Searching in the repository should therefore return matching topics from the hierarchy, rather than just a list of entries. Likewise, when new entries are inserted, a search for relevant topics to which they should be linked is required. We develop a vector-based algorithm that creates keyword vectors for the set of competing topics at each node in the hierarchy, and show how its performance improves when domain-specific features are added (such as special handling of topic titles and author names). The results of a 7-fold cross validation on a corpus of some 3,500 entries with a 5-level index are hit ratios in the range of 89-95%, and most of the misclassifications are indeed ambiguous to begin with.", "num_citations": "18\n", "authors": ["150"]}
{"title": "A run-time algorithm for managing the granularity of parallel functional programs\n", "abstract": " We present an on-line (run-time) algorithm that manages the granularity of parallel functional programs. The algorithm exploits useful parallelism when it exists, and ignores ineffective parallelism in programs that produce many small tasks. The idea is to balance the amount of local work with the cost of distributing the work. This is achieved by ensuring that for every parallel task spawned, an amount of work that equals the cost of the spawn is performed locally. We analyse several cases and compare the algorithm to the optimal execution. In most cases the algorithm competes well with the optimal algorithm, even though the optimal algorithm has information about the future evolution of the computation that is not available to the on-line algorithm. This is quite remarkable considering we have chosen extreme cases that have contradicting optimal executions. Moreover, we show that no other on-line algorithm can be\u00a0\u2026", "num_citations": "18\n", "authors": ["150"]}
{"title": "Looking at data\n", "abstract": " Collecting and analyzing data lies at the basis of the scientific method: findings about nature usher new ideas, and experimental results support or refute theories. All this is not very prevalent in computer science, possibly due to the fact that computer systems are man made, and not perceived as a natural phenomenon. But computer systems and their interactions with their users are actually complex enough to require objective observations and measurements. We'll survey several examples related to parallel and other systems, in which we attempt to further our understanding of architectural choices, system evaluation, and user behavior. In all the cases, the emphasis is not on heroic data collection efforts, but rather on afresh look at existing data, and uncovering surprising, interesting, and useful information. Using such empirical information is necessary in order to ensure that systems and evaluations are relevant\u00a0\u2026", "num_citations": "17\n", "authors": ["150"]}
{"title": "On the definition of \"on-line\" in job scheduling problems\n", "abstract": " The conventional model of on-line scheduling postulates that jobs have non-trivial release dates, and are not known in advance. However, it fails to impose any stability constraints, leading to algorithms and analyses that must deal with unrealistic load conditions arising from trivial release dates as a special case. In an effort to make the model more realistic, we show how stability can be expressed as a simple constraint on release times and processing times. We then give empirical and theoretical justifications that such a constraint can close the gap between the theory and practice. As it turns out, this constraint seems to trivialize the scheduling problem.", "num_citations": "17\n", "authors": ["150"]}
{"title": "On-line fair allocations based on bottlenecks and global priorities\n", "abstract": " System bottlenecks, namely those resources which are subjected to high contention, constrain system performance. Hence effective resource management should be done by focusing on the bottleneck resources and allocating them to the most deserving clients. It has been shown that for any combination of entitlements and requests a fair allocation of bottleneck resources can be found, using an off-line algorithm that is given full information in advance regarding the needs of each client. We extend this result to the on-line case with no prior information. To this end we introduce a simple greedy algorithm. In essence, when a scheduling decision needs to be made, this algorithm selects the client that has the largest minimal gap between its entitlement and its current allocation among all the bottleneck resources. Importantly, this algorithm takes a global view of the system, and assigns each client a single priority\u00a0\u2026", "num_citations": "16\n", "authors": ["150"]}
{"title": "Time Stamp Counters Library-Measurements with Nano Seconds Resolution\n", "abstract": " As current computers become faster, and more applications require very tight time constraints and optimizations, there is a growing need for a simple and robust manner to evaluate performance with extremely low overhead. The best way to this is to directly measure the CPU cycles. The standard method of evaluation is by extracting the system time using operating system calls. This method incures a very substantial overhead since it implicitly includes two context switches-to the system context and back-and thus can only be used in a very coarse grained methodolgy. Intel, makers of the most popular workstation CPUs, has incorporated into its P5 processors family (Pentium and up) an opcode which allows to read the CPU's cycle counter from user level. Our library enables the user to use this opcode and to take these coveted cycle measurements, that can be used for ne grained performance evaluation. These cycles measurements can be easily converted, using the CPU speed, in...", "num_citations": "16\n", "authors": ["150"]}
{"title": "Parallel network communications protocol using token passing\n", "abstract": " A protocol for achieving atomic multicast in a parallel or distributed computing environment. The protocol guarantees concurrency atomicity with a maximum of m-1 message passes among the m server nodes of the system. Under one embodiment of the protocol, an access component message is transferred to the server nodes storing data to be accessed. The first server node of the plurality generates a token to be passed among the accessed nodes. A node can not process its request until it receives the token. A node may pass the token immediately upon ensuring that it is the current expected token.", "num_citations": "16\n", "authors": ["150"]}
{"title": "Exploiting Core Working Sets to Filter the L1 Cache with Random Sampling\n", "abstract": " Locality is often characterized by working sets, defined by Denning as the set of distinct addresses referenced within a certain window of time. This definition ignores the fact that dramatic differences exist between the usage patterns of frequently used data and transient data. We therefore propose to extend Denning's definition with that of core working sets, which identify blocks that are used most frequently and for the longest time. The concept of a core motivates the design of dual-cache structures that provide special treatment for the core. In particular, we present a probabilistic locality predictor for L1 caches that leverages the skewed popularity of blocks to distinguish transient cache insertions from more persistent ones. We further present a dual L1 design that inserts only frequently used blocks into a low-latency, low-power, direct-mapped main cache, while serving others from a small fully associative filter. To\u00a0\u2026", "num_citations": "15\n", "authors": ["150"]}
{"title": "A global scheduling framework for virtualization environments\n", "abstract": " A premier goal of resource allocators in virtualization environments is to control the relative resource consumption of the different virtual machines, and moreover, to be able to change the relative allocations at will. However, it is not clear what it means to provide a certain fraction of the machine when multiple resources are involved. We suggest that a promising interpretation is to identify the system bottleneck at each instant, and to enforce the desired allocation on that device. This in turn induces an efficient allocation of the other devices.", "num_citations": "15\n", "authors": ["150"]}
{"title": "\u201cLook It Up\u201d or \u201cDo the Math\u201d: An Energy, Area, and Timing Analysis of Instruction Reuse and Memoization\n", "abstract": " Instruction reuse and memoization exploit the fact that during a program run there are operations that execute more than once with the same operand values. By saving previous occurrences of instructions (operands and result) in dedicated, on-chip lookup tables, it is possible to avoid re-execution of these instructions. This has been shown to be efficient in a naive model that assumes single-cycle table lookup. We now extend the analysis to consider the energy, area, and timing overheads of maintaining such tables.               We show that reuse opportunities abound in the SPEC CPU2000 benchmark suite, and that by judiciously selecting table configurations it is possible to exploit these opportunities with a minimal penalty. Energy consumption can be further reduced by employing confidence counters, which enable instructions that have a history of failed memoizations to be filtered out. We conclude by\u00a0\u2026", "num_citations": "15\n", "authors": ["150"]}
{"title": "Preserving user behavior characteristics in trace-based simulation of parallel job scheduling\n", "abstract": " Evaluating the performance of a computer system requires the use of representative workloads. Therefore it is customary to use recorded job traces in simulations to evaluate the performance of proposed parallel job schedulers. We argue that this practice retains unimportant attributes of the workload, at the expense of other more important attributes. Specifically, using traces in open-system simulations retains the exact timestamps at which jobs are submitted. But in a real system these times depend on how users react to the performance of previous jobs, and it is more important to preserve the logical structure of dependencies between jobs than the specific timestamps. Using dependency information extracted from traces, we show how a simulation can preserve these dependencies. To do so we also extract user behavior, in terms of sessions and think times between the termination of one batch of jobs and the\u00a0\u2026", "num_citations": "14\n", "authors": ["150"]}
{"title": "Comparing performance heatmaps\n", "abstract": " The performance of parallel job schedulers is often expressed as an average metric value (e.g. response time) for a given average load. An alternative is to acknowledge the wide variability that exists in real systems, and use a heatmap that portrays the distribution of jobs across the performance\u00a0\u00a0load space. Such heatmaps expose a wealth of details regarding the conditions that occurred in production use or during a simulation. However, heatmaps are a visual tool, lending itself to high-resolution analysis of a single system but not conducive for a direct comparison between different schedulers or environments. We propose a number of techniques that allow to compare heatmaps. The first two treat the heatmaps as images, and focus on the differences between them. Two other techniques are based on tracking how specific jobs fare under the compared scenarios, and drawing underlying trends. This\u00a0\u2026", "num_citations": "14\n", "authors": ["150"]}
{"title": "Comparing partitions with spie charts\n", "abstract": " Statistical graphics are important both in exploratory data analysis and in presenting results to non-professionals. Pie charts are often used to show a partition, but are hard to compare with each other. A spie chart is a combination of two pie charts: one sets the angles of the slices, and the other sets their areas, by manipulating the radius of each slice individually. This enables an easy comparison of the partitions represented by the two charts. It is useful for presenting change (eg a new division of a budget compared to a previous devision) or risk (eg effected groups relative to their fraction in the population).", "num_citations": "13\n", "authors": ["150"]}
{"title": "Using Students as Experimental Subjects in Software Engineering Research--A Review and Discussion of the Evidence\n", "abstract": " Should students be used as experimental subjects in software engineering? Given that students are in many cases readily available and cheap it is no surprise that the vast majority of controlled experiments in software engineering use them. But they can be argued to constitute a convenience sample that may not represent the target population (typically \"real\" developers), especially in terms of experience and proficiency. This causes many researchers (and reviewers) to have reservations about the external validity of student-based experiments, and claim that students should not be used. Based on an extensive review of published works that have compared students to professionals, we find that picking on \"students\" is counterproductive for two main reasons. First, classifying experimental subjects by their status is merely a proxy for more important and meaningful classifications, such as classifying them according to their abilities, and effort should be invested in defining and using these more meaningful classifications. Second, in many cases using students is perfectly reasonable, and student subjects can be used to obtain reliable results and further the research goals. In particular, this appears to be the case when the study involves basic programming and comprehension skills, when tools or methodologies that do not require an extensive learning curve are being compared, and in the initial formative stages of large industrial research initiatives -- in other words, in many of the cases that are suitable for controlled experiments of limited scope.", "num_citations": "12\n", "authors": ["150"]}
{"title": "Success of open source projects: Patterns of downloads and releases with time\n", "abstract": " The success of software projects has many different facets: meeting user requirements, being developed within given budget and time constraints, and actually being used. We focus on one of them: the pattern of adoption by users, and its possible relationship with continued development. This is done based on readily available data for open source projects, namely their releases and downloads. Rather than just classifying projects as \"successful\" or \"failure\", we identify six distinct patterns of how the download rate changes with time that illuminate different aspects of successful or failed projects. \"You're not thinking fourth dimensionally!\" Dr. Emmett \"Doc\" Brown, Back To The Future III, scene 8", "num_citations": "12\n", "authors": ["150"]}
{"title": "User-level communication in a system with gang scheduling\n", "abstract": " One of the scarce resources that limits communication performance is buffer space on the network interface card. This becomes even worse when it is partitioned among several time-sliced processes. However, if gang scheduling is used, it is possible to swap buffer contents as part of the context switch, giving each job the full buffer space for the duration of its quantum. This does not suffer undue overhead, as the buffer space is mainly used to allow a larger flow-control window, and typically does not contain many packets that need to be stored.", "num_citations": "12\n", "authors": ["150"]}
{"title": "Resampling with Feedback: A New Paradigm of Using Workload Data for Performance Evaluation\n", "abstract": " Reliable performance evaluations require representative workloads. This has led to the use of accounting logs from production systems as a source for workload data in simulations. But using such logs directly suffers from various deficiencies, such as providing data about only one specific situation, and lack of flexibility, namely the inability to adjust the workload as needed. Creating workload models solves some of these problems but creates others, most notably the danger of missing out on important details that were not recognized in advance, and therefore not included in the model. Resampling solves many of these deficiencies by combining the best of both worlds. It is based on partitioning real workloads into basic components (specifically the job streams contributed by different users), and then generating new workloads by sampling from this pool of basic components. The generated workloads are\u00a0\u2026", "num_citations": "11\n", "authors": ["150"]}
{"title": "Session-based, estimation-less, and information-less runtime prediction algorithms for parallel and Grid job scheduling\n", "abstract": " The default setting of most production parallel job schedulers is FCFS with backfilling. Under this setting, users must supply job runtime estimates, which are known as being highly inaccurate and inferior to system generated predictions. Recent research revealed how to utilize system predictions for backfilling, and this potential performance gain motivates searching for better prediction techniques. We present three prediction techniques using decreasing levels of information as is suitable for the situation at hand. The first is based on\" user sessions\": continuous temporal periods of per-user work. This algorithm exploits the entire long-term historical data of the workload, along with user runtime estimates. The second is\" estimation-less\", that is, uses historical data only, relieving users from the annoying need to supply estimates. The third is completely\" informationless\" and is suitable for cases in which neither historical information nor estimates are available, as happens in some grid environments. We evaluate the algorithms by simulating real data from production systems. We find all of them to be successful in terms of both accuracy and performance.", "num_citations": "11\n", "authors": ["150"]}
{"title": "Barrier synchronization on a loaded SMP using two-phase waiting algorithms\n", "abstract": " Little work has been done on the performance of barrier synchronization using two-phase blocking, as the common wisdom is that it is useless to spin if the total number of threads in the system exceeds the number of processors. We challenge this and show that it may be beneficial to spin-wait even if the number of threads is up to double the number of processors, especially if the waiting time is at least twice the context switch overhead (rather than being equal to it). We also characterize the alternating synchronization pattern that applications based on barriers tend to fall into which is quite different from the patterns typically assumed in theoretical analyses.", "num_citations": "11\n", "authors": ["150"]}
{"title": "Cooperative indexing, classification, and evaluation in BoW\n", "abstract": " BoW is an on-line bibliographic Dynamic Ranked Informationspace (DyRI). It provides the infrastructure for users to add bibliographical information, classify it, index it, and evaluate it. Thus users cooperate by contributing and sharing their experience in order to advance the most problematic aspects of information retrieval: finding the most relevant and high quality information for their needs.", "num_citations": "11\n", "authors": ["150"]}
{"title": "Probabilistic prediction of temporal locality\n", "abstract": " The increasing gap between processor and memory speeds, as well as the introduction of multi-core CPUs, have exacerbated the dependency of CPU performance on the memory subsystem. This trend motivates the search for more efficient caching mechanisms, enabling both faster service of frequently used blocks and decreased power consumption. In this paper we describe a novel, random sampling based predictor that can distinguish transient cache insertions from non-transient ones. We show that this predictor can identify a small set of data cache resident blocks that service most of the memory references, thus serving as a building block for new cache designs and block replacement policies. Although we only discuss the L1 data cache, we have found this predictor to be efficient also when handling L1 instruction caches and shared L2 caches.", "num_citations": "10\n", "authors": ["150"]}
{"title": "Job scheduling for parallel supercomputers\n", "abstract": " CiNii \u8ad6\u6587 - Job scheduling for parallel supercomputers CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u306e\u30b5\u30fc\u30d3\u30b9 \u306b\u95a2\u3059\u308b\u30a2\u30f3\u30b1\u30fc\u30c8\u3092\u5b9f\u65bd\u4e2d\u3067\u3059\uff0811/11(\u6c34)-12/23(\u6c34)\uff09 CiNii Research\u30d7\u30ec\u7248\u306e\u516c\u958b\u306b\u3064\u3044\u3066 Job scheduling for parallel supercomputers FEITELSON DG \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 FEITELSON DG \u53ce\u9332\u520a\u884c\u7269 Encyclopedia of Computer Science and Technology Encyclopedia of Computer Science and Technology, 1998 Marcel Dekker \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Concerning the Length of Time Slots for Efficient Gang Scheduling ZHOU Bing Bing , GOSCINSKI Andrzej M , BRENT Richard P IEICE transactions on information and systems 86(9), 1594-1600, --/\u2026", "num_citations": "10\n", "authors": ["150"]}
{"title": "Vesta File System programmer\u2019s reference, version 1.01\n", "abstract": " The last decade has witnessed considerable advances in file system development. Important milestones are distributed file systems, in which files may reside on different servers, and concurrent file systems, in which single files are interleaved across multiple disks nodes. Vesta improves on previous systems by allowing the user some control over the mapping of file data to I/O nodes, allowing matching of the data layout to the intended use. The system goals are to provide high performance coupled with reliability.This document specifies the Vesta programming interface. This interface was designed to simplify both the usage and the implementation of Vesta by identifying the essential independent functions that are required. This results in a relatively large number of functions, but each has well-defined and straightforward functionality. The interface is also rich enough to allow the user to trade off services for\u00a0\u2026", "num_citations": "10\n", "authors": ["150"]}
{"title": "Optical interconnection network with 3-D layout and distributed control\n", "abstract": " We present an optical, free-space, crossbar-like N x N interconnection network based on N routing switches of the form 1 to N. The network is used to connect Processing Elements (PEs) in a large-scale, parallel computer, utilizing a 3-D space layout. Each PE controls and sets its own switch, using a distributed algorithm. The switch deflects a monochromatic, coherent light beam towards one of N possible spots. This beam, carrying information from the sending PE, is thereafter passively deflected by a system of multifacet holograms, until it reaches the target PE. Unlike other schemes, this layout uniformly fills the 3-D space with PEs. Thus, inter-PE communication can be minimized when algorithms exploit locality of reference.", "num_citations": "10\n", "authors": ["150"]}
{"title": "Understanding large-scale software\u2013a hierarchical view\n", "abstract": " Program comprehension accounts for a large portion of software development costs and effort. The academic literature contains research on program comprehension of short code snippets, but comprehension at the system level is no less important. We claim that comprehending a software system is a distinct activity that differs from code comprehension. We interview experienced developers, architects, and managers in the software industry and open-source community, to uncover the meaning of program comprehension at the system level. The interviews demonstrate, among other things, that system comprehension is detached from code and programming language, and includes scope that is not captured in the code. It focuses on the structure of the system and less on the code itself. This is a continuous, iterative process, which mixes white-box and black-box approaches at different layers of the system, and\u00a0\u2026", "num_citations": "8\n", "authors": ["150"]}
{"title": "Zipf\u2019s law revisited\n", "abstract": " Zipf\u2019s law states that the frequency of occurence of some event as a function of its rank is a power-law function. Using empirical examples from different domains, we demonstrate that at least in some cases, increasingly significant divergences from Zipf\u2019s law are registered as the number of events observed increases. Importantly, one of these cases is word frequency in a corpus of natural language, which is\u2014undoubtedly\u2014the most prominent example of Zipf\u2019s law. We analyze our findings mathematically and attempt a natural explanation of the regularities underlying them.", "num_citations": "8\n", "authors": ["150"]}
{"title": "Sensitivity of parallel job scheduling to fat-tailed distributions\n", "abstract": " Fat tailed distributions have been found to characterize many aspects of computer workloads, mainly with regard to communication and les. Analysis of workload logs from parallel supercomputers shows that the load on such machines can also be thus characterized, but that the source is a bursty arrival process, and not a fat-tailed distribution of runtimes. Nevertheless, fat-tailed loads are shown to affect scheduler performance (in terms of the scheduler's own runtime), and to have a significant effect on the convergence of simulations used to evaluate different scheduling policies.", "num_citations": "8\n", "authors": ["150"]}
{"title": "Satisfying the I/O requirements of massively parallel supercomputers\n", "abstract": " Copyright [\u00a9](1995) by IEEE. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distrubuted for profit. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.Applications executing on massively parallel supercomputers require a high aggregate bandwidth of I/O with low latency. This requirement cannot be satisfied by an external file server. Once solution is to employ an internal parallel I/O subsystem, in which I/O nodes with DASD are linked wo the same interconnection network that connects the compute nodes. The option of increasing the number of I/O nodes together with the number of compute nodes allows for a balanced architecture. Indeed, most multicomputer vendors provide internal parallel I/O subsystems as part of their product\u00a0\u2026", "num_citations": "8\n", "authors": ["150"]}
{"title": "Deadlock detection without wait-for graphs\n", "abstract": " Deadlock detection is an important service that the run-time system of a parallel environment should provide. In parallel programs deadlock can occur when the different processes are waiting for various events, as opposed to concurrent systems, where deadlock occurs when processes wait for resources held by other processes. Therefore classical deadlock detection techniques such as checking for cycles in the wait-for graph are unapplicable. An alternative algorithm that checks whether all the processes are blocked is presented. This algorithm deals with situations in which the state transition from blocked to unblocked is indirect, as may happen when busy-waiting is used.", "num_citations": "8\n", "authors": ["150"]}
{"title": "How Developers Choose Names\n", "abstract": " The names of variables and functions serve as implicit documentation and are instrumental for program comprehension. But choosing good meaningful names is hard. We perform a sequence of experiments in which a total of 334 subjects are required to choose names in given programming scenarios. The first experiment shows that the probability that two developers would select the same name is low: in the 47 instances in our experiments the median probability was only 6.9%. At the same time, given that a specific name is chosen, it is usually understood by the majority of developers. Analysis of the names given in the experiment suggests a model where naming is a (not necessarily cognizant or serial) three-step process: (1) selecting the concepts to include in the name, (2) choosing the words to represent each concept, and (3) constructing a name using these words. A followup experiment, using the same\u00a0\u2026", "num_citations": "7\n", "authors": ["150"]}
{"title": "High-Resolution Analysis of Parallel Job Workloads\n", "abstract": " Conventional evaluations of parallel job schedulers are based on simulating the outcome of using a new scheduler on an existing workload, as recorded in a log file. In order to check the scheduler\u2019s performance under diverse conditions, crude manipulations of the whole log are used. We suggest instead to perform a high-resolution analysis of the natural variability in conditions that occurs within each log. Specifically, we use a heatmap of jobs in the log, where the X axis is the load experienced by each job, and the Y axis is the job\u2019s performance. Such heatmaps show that the conventional reporting of average performance vs. average load is highly oversimplified. Using the heatmaps, we can see the joint distribution of performance and load, and use this to characterize and understand the system performance as recorded in the different logs. The same methodology can be applied to simulation results\u00a0\u2026", "num_citations": "7\n", "authors": ["150"]}
{"title": "Issues in run-time support for tightly-coupled parallel processing\n", "abstract": " Issues in run-time support for tightly-coupled parallel processing | Papers from the symposium on Experiences with distributed and multiprocessor systems ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsSEDMS IIIIssues in run-time support for tightly-coupled parallel processing Article Issues in run-time support for tightly-coupled parallel processing Share on Authors: Dror G. Feitelson profile image Dror G. Feitelson View Profile , Yosi Ben-Asher profile image Yosi Ben-Asher View Profile , Moshe Ben Ezra profile image Moshe Ben Ezra View Profile , Iaakov Exman profile image Iaakov Exman View Profile , Lior Picherski , S \u2026", "num_citations": "7\n", "authors": ["150"]}
{"title": "Data on the distribution of cancer incidence and death across age and sex groups visualized using multilevel spie charts\n", "abstract": " Cancer incidence and death statistics are typically recorded for multiple age and sex brackets, leading to large data tables which are difficult to digest. Effective visualizations of this data would allow practitioners, policy makers, and the general public to comprehend the data more readily and act on it appropriately. We introduce multilevel spie charts to create a combined visualization of cancer incidence and death statistics. Spie charts combine multiple pie charts, where the base pie chart (representing the general population) is used to set the angles of slices, and the superimposed ones use variable radii to portray the cancer data. Spie charts of cancer incidence and death statistics from Israel for 2009\u20132011 are used as an illustration. These charts clearly show various patterns of how cancer incidence and death distribute across age and sex groups, illustrating (1) absolute numbers and (2) rates per 100,000\u00a0\u2026", "num_citations": "6\n", "authors": ["150"]}
{"title": "A critique of ESP\n", "abstract": " This note is an elaboratin of a panel presentation, and is meant as a constructive critique of ESP. It should be remembered that the bottom line is that ESP is a big step in an important direction \u2013  otherwise we wouldn\u2019t bother with this discussion...", "num_citations": "6\n", "authors": ["150"]}
{"title": "The organization of lookup tables in instruction memoization\n", "abstract": " Instruction Memoization is a technique that enables performing multiple-cycle computations in a single cycle by exploiting redundant instructions. The technique is based on the notion of memoing: saving the input and output of previous calculations and using the output if the input is encountered again. Several recent papers have proven that non-trivial speedup (over 10%) can be obtained by using this technique.The speedup can be attributed to three major factors:(i) the percentage of instructions that can bene t from memoization,(ii) the integration of the the Lookup Table (LUT) into the datapath of the processor, and (iii) the percentage of successful lookups. This paper focuses on the latter factor and explores how the organization of a Lookup Table (LUT) in uences the percentage of successful lookups (hit-ratio).", "num_citations": "6\n", "authors": ["150"]}
{"title": "Limitations on optical free-space crossbarlike interconnection networks\n", "abstract": " Providing the required interconnections between the processors of a parallel computer is a difficult problem: latency, switching control, cost, and crosstalk effects have to be taken into account. It is widely believed that the design might be simplified if optical technology is used. However, even optical interconnections cannot cater for an unlimited number of processors. For systems with up to about 1O - 106 processors, it seems that the arrangement of the processors in space so that each can \"see\" the others is the limiting factor; A large part of the volume involved must stay empty for the information-bearing light beams. above that, diffraction effects and power requirements place a limit on the number of processors.", "num_citations": "6\n", "authors": ["150"]}
{"title": "Empirical quantification of opportunities for content adaptation in web servers\n", "abstract": " A basic problem in the management of web servers is capacity planning: you want enough capacity to be able to serve peak loads, but not too much so as to avoid excessive costs. It is therefore important to know the load that web service places on the CPU, disk, and network. We analyze these loads for representative web sites, and find that with normal caching the disk is not expected to be a bottleneck, and that reducing the number of requests made is more important than reducing the total size. We then consider the option of trading off quality for throughput, as may be necessary to handle flash crowds. The suggested approaches include the elimination of graphical decorations and previews, the compression of large images, the consolidation of style sheets and JavaScript code in the main HTML page, and the removal of unimportant blocks from the design.", "num_citations": "5\n", "authors": ["150"]}
{"title": "Asimov's laws of robotics applied to software\n", "abstract": " In 1940, science fiction writer Isaac Asi- mov formulated the now-famous Three Laws of Robotics, which constrain robots to serve their human masters. These laws focused on the physical well-being of robots and humans. But they can also be applied to software in a more abstract manner, by focusing on human presence in cyberspace. In this vein, I suggest the following laws of software. While they are largely a restatement of known principles, uniting them in this way casts a new light on the contract between humanity and technology.", "num_citations": "5\n", "authors": ["150"]}
{"title": "Common coupling and pointer variables, with application to a Linux case study\n", "abstract": " Both common coupling and pointer variables can exert a deleterious effect on the quality of software. The situation is exacerbated when global variables are assigned to pointer variables, that is, when an alias to a global variable is created. When this occurs, the number of global variables increases, and it becomes considerably harder to compute quality metrics correctly. However, unless aliasing is taken into account, variables may incorrectly appear to be unreferenced (neither defined nor used), or to be used without being defined. These ideas are illustrated by means of a case study of common coupling in the Linux kernel.", "num_citations": "5\n", "authors": ["150"]}
{"title": "On the scalability of centralized control\n", "abstract": " Scalability of clusters and MPPs is typically discussed in terms of limits on growth: something which grows at a rate of O(logp) (where p is the number of processors) is said to be more scalable than something whose growth rate is O(p). But in practice p does not grow without limits. We therefore suggest that discussions of scalability should take time into account. System sizes grow with time, so larger systems need to be supported - but only after some time. And in particular, there is no real need to support arbitrarily large systems right now. Surprisingly, when time is thus put into the picture, we find that centralized control is actually quite scalable. The reason is that the capabilities of a centralized control node grow at a fast pace due to Moore's law. This seems to be more than enough in order to manage current growth patterns displayed by parallel systems.", "num_citations": "5\n", "authors": ["150"]}
{"title": "Pitfalls in parallel job scheduling evaulation\n", "abstract": " There are many choices to make when evaluating the perfor-mance of a complex system. In the context of parallel job scheduling, one must decide what workload to use and what measurements to take. These decisions sometimes have subtle implications that are easy to overlook.", "num_citations": "5\n", "authors": ["150"]}
{"title": "Performance and overhead measurements on the Makbilan\n", "abstract": " The Makbilan is a research multiprocessor with 12 single-board computers in a Multibus-II cage. Each single-board computer is a 386 processor running at 20 MHz with 4 MB of memory. The software includes a local kernel (Intel's RMK) on each board, and a parallel run-time library that supports the constructs of the ParC language. We report measurements of bus contention e ects, context-switch overhead, overhead for spawning new tasks, the e ectiveness of load balancing, and synchronization overhead on this system.", "num_citations": "5\n", "authors": ["150"]}
{"title": "Semi-open trace based simulation for reliable evaluation of job throughput and user productivity\n", "abstract": " New scheduling algorithms are first evaluated in simulations. In simulations, the workload has a huge influence on the measured performance of the simulated system. Simulation workloads typically assume an open system model and the workload is unaffected by the system's performance. As a result, the throughput is fixed and only the wait times and slowdown are evaluated. Therefore instead of evaluating the users' productivity, they evaluate the users' convenience.", "num_citations": "4\n", "authors": ["150"]}
{"title": "Trading off quality for throughput using content adaptation in web servers\n", "abstract": " A basic problem in managing web servers is capacity planning. A partial solution is to use content adaptation, where the system automatically trades off quality for throughput, eg by eliminating graphical decorations and adjusting page layout. We evaluate this approach based on a full implementation in Apache and increasing load patterns. The implementation uses two alternative versions of the files, and employs URL rewriting rules to select which version to use. Triggering a switch from one version to the other is done based on readily available load metrics. The experiments show that throughput can be increased by a factor of 2 to 4 at the price of minor to acceptable deterioration in graphical quality. Increasing throughput by an order of magnitude is also possible, but requires larger compromises. Nevertheless, this is still achievable without a real effect on content. Thus content adaptation is a viable tool, but\u00a0\u2026", "num_citations": "4\n", "authors": ["150"]}
{"title": "Fine-grain analysis of common coupling and its application to a Linux case study\n", "abstract": " Common coupling (sharing global variables across modules) is widely accepted as a measure of software quality and maintainability; a low level of common coupling is necessary (but not sufficient) to ensure maintainability. But when the global variables in question are large multi-field data structures, one must decide whether to consider such data structures as single units, or examine each of their fields individually. We explore this issue by re-analyzing a case study based on the Linux operating system. We determine the common coupling at the level of granularity of the component fields of large, complex data structures, rather than at the level of the data structures themselves, as in previous work. We claim that this is the appropriate level of analysis based on how such data structures are used in practice, and also that such a study is required due to concern that coarse-grained analysis leads to false coupling\u00a0\u2026", "num_citations": "4\n", "authors": ["150"]}
{"title": "Automatic alphabet recognition\n", "abstract": " The last step of the Information Retrieval process is to display the found documents to the user. However, some difficulties might occur at that point. English texts are usually written in the ASCII standard. Unlike the English language, many languages have different character sets, and do not have one standard. This plurality of standards causes problems, especially in a web environment, where one may download a document with an unknown standard. This paper suggests a purely automatic way of finding the standard which was used by the document writer based on the statistical letters distribution in the language. We developed a vector-space-based method that creates frequencies vectors for each letter of the language and then matches a new document's vectors to the pre-computed templates. The algorithm was applied on various types of corpora in Hebrew, Russian and English, and provides an\u00a0\u2026", "num_citations": "4\n", "authors": ["150"]}
{"title": "A distributional measure of correlation\n", "abstract": " Common metrics of correlation are based on the degree to which the value of one random variable can be used to predict the value of another random variable. However, there are forms of correlation that do not lead to a high ability to predict actual values. We identify such a correlation, and suggest it can be measured by a certain monotonicity property among distributions of the dependent variable, for different ranges of the independent variable.", "num_citations": "4\n", "authors": ["150"]}
{"title": "Utilization, Predictability, Workloads, and User Runtime Estimates in Scheduling the IBM SP2 with Backfilling\n", "abstract": " \u2014Scheduling jobs on the IBM SP2 system and many other distributed-memory MPPs is usually done by giving each job a partition of the machine for its exclusive use. Allocating such partitions in the order in which the jobs arrive (FCFS scheduling) is fair and predictable, but suffers from severe fragmentation, leading to low utilization. This situation led to the development of the EASY scheduler which uses aggressive backfilling: Small jobs are moved ahead to fill in holes in the schedule, provided they do not delay thefirst job in the queue. We compare this approach with a more conservative approach in which small jobs move ahead only if they do not delayany job in the queue and show that the relative performance of the two schemes depends on the workload: For workloads typical on SP2 systems, the aggressive approach is indeed better, but, for other workloads, both algorithms are similar. In addition, we study\u00a0\u2026", "num_citations": "4\n", "authors": ["150"]}
{"title": "Terminal I/O for massively parallel systems\n", "abstract": " To be useful, terminal I/O on massively parallel MIMD machines must be able to differentiate between the I/O streams from different tasks. This is done in the Vulcan terminal I/O facility by providing a special control panel, which allows an independent window to be opened for each task. The controls look like LEDs, being color coded to indicate status (e.g. output is available or the task is waiting for input). Additional LEDs are provided as a new form of output, allowing the application to report status visually rather than by using text output. This is useful during program development and debugging. A derivative of the devices reported has been incorporated in the AIX Parallel Environment on the new IBM SP1 multicomputer.< >", "num_citations": "4\n", "authors": ["150"]}
{"title": "Which Refactoring Reduces Bug Rate?\n", "abstract": " We present a methodology to identify refactoring operations that reduce the bug rate in the code. The methodology is based on comparing the bug fixing rate in certain time windows before and after the refactoring. We analyzed 61,331 refactor commits from 1,531 large active GitHub projects. When comparing three-month windows, the bug rate is substantially reduced in 17% of the files of analyzed refactors, compared to 12% of the files in random commits. Within this group, implementing'todo's provides the most benefits. Certain operations like reuse, upgrade, and using enum and namespaces are also especially beneficial.", "num_citations": "3\n", "authors": ["150"]}
{"title": "The rise of Chrome\n", "abstract": " Since Chrome\u2019s initial release in 2008 it has grown in market share, and now controls roughly half of the desktop browsers market. In contrast with Internet Explorer, the previous dominant browser, this was not achieved by marketing practices such as bundling the browser with a pre-loaded operating system. This raises the question of how Chrome achieved this remarkable feat, while other browsers such as Firefox and Opera were left behind. We show that both the performance of Chrome and its conformance with relevant standards are typically better than those of the two main contending browsers, Internet Explorer and Firefox. In addition, based on a survey of the importance of 25 major features, Chrome product managers seem to have made somewhat better decisions in selecting where to put effort. Thus the rise of Chrome is consistent with technical superiority over the competition.", "num_citations": "3\n", "authors": ["150"]}
{"title": "Notes on Operating Systems\n", "abstract": " In the simplest scenario, the operating system is the first piece of software to run on a computer when it is booted. Its job is to coordinate the execution of all other software, mainly user applications. It also provides various common services that are needed by users and applications.", "num_citations": "3\n", "authors": ["150"]}
{"title": "Design and implementation of a generic resource sharing virtual time dispatcher\n", "abstract": " Virtual machine monitors, especially when used for server consolidation, need to enforce a predefined sharing of resources among the running virtual machines. We propose a new mechanism for doing so that provides improved pacing in the face of heterogeneous allocations and priorities. This mechanism lends from token-bucket metering and from virtual-time scheduling, and prioritizes the different clients based on the divergence between their desired allocations and the actual consumptions. The ideas are demonstrated by implementations for the CPU and networking subsystems of the Linux kernel. Notably, both use exactly the same basic module; future plans include using it for disk I/O as well.", "num_citations": "3\n", "authors": ["150"]}
{"title": "Bicriteria scheduling for parallel jobs\n", "abstract": " We study off-line and on-line algorithms for parallel settings which simultaneously yield constant approximation factors for both minimizing total weighted completion time and makespan objectives. Introducing preemptions, the list scheduling algorithm introduced by Garey and Graham is extended in a way in which a parallel job completion time is solely dictated by its predecessors in the input instance, as is the case with Graham\u2019s classical list algorithm for the identical parallel machine model. This suggests the applicability of many ordering techniques which have previously been adapted to the single machine and identical parallel machine settings.", "num_citations": "3\n", "authors": ["150"]}
{"title": "Parallel activity roadmaps\n", "abstract": " Parallel Roadmaps are simple visual constructs, useful for displaying the evolution of large-scale parallel programs with dynamic parallelism. Roadmaps remain intelligible and provide invaluable debugging clues even as program complexity increases. A key design feature is the selective omission of details to be displayed, coupled with the option to focus on part of the program to obtain additional details.", "num_citations": "3\n", "authors": ["150"]}
{"title": "Unequal cell division as a driving force during differentiation\n", "abstract": " Unequal cell division, leading to daughter cells of different sizes, is a common event during development. We have formulated a model that shows how different cell sizes can lead to significant differences in the expression of an activator gene. The model is based on well-known aspects of the regulation of gene expression, specifically the existence of multiple regulatory sites, positive autoregulation, and the short half-life of regulatory proteins. Thus the daughter cells may follow distinct differentiation pathways even if there were no localized determinants in the mother cell.", "num_citations": "3\n", "authors": ["150"]}
{"title": "Communicators: Object-based multiparty interactions for parallel programming\n", "abstract": " Contemporary parallel programming languages often provide only few low-level primitives for pairwise communication and synchronization. These primitives are not always suitable for the interactions being programmed. Programming would be easier if it was possible to tailor communication and synchronization mechanisms to t the needs of the application, much as abstract data types are used to create application-speci c data structures and operations. This should also include the possibility of expressing interactions among multiple processes at once. Communicators support this paradigm by creating abstract communication objects that provide a framework for interprocess multiparty interactions. The behavior of these objects is de ned in terms of interactions, in which multiple processes can enrole. Interactions are performed when all the roles are lled by ready processes. Nondeterminism is used when the order of interaction performance is immaterial. Interactions can also be disabled, thereby creating a uniform queueing mechanism where interactions may represent events.", "num_citations": "3\n", "authors": ["150"]}
{"title": "A unified strategy for search and result representation for an online bibliographical catalogue\n", "abstract": " Purpose \u2013 One of the biggest concerns of modern information retrieval systems is reducing the user effort required for manual traversal and filtering of long matching document lists. Thus, the first goal of this research is to propose an improved scheme for representation of search results. Further, it aims to explore the impact of various user information needs on the searching process with the aim of finding a unified searching approach well suited for different query types and retrieval tasks.Design/methodology/approach \u2013 The BoW online bibliographical catalogue is based on a hierarchical concept index to which entries are linked. The key idea is that searching in the hierarchical catalogue should take advantage of the catalogue structure and return matching topics from the hierarchy, rather than just a long list of entries. Likewise, when new entries are inserted, a search for relevant topics to which they should be\u00a0\u2026", "num_citations": "2\n", "authors": ["150"]}
{"title": "Characterizing software maintenance categories using the Linux kernel\n", "abstract": " Software maintenance involves different categories of activities: corrective, adaptive, perfective, and preventive. However, research regarding these distinct activities is hampered by lack of empirical data that is labeled to identify the type of maintenance being performed. A promising dataset is provided by the more than 800 releases of the Linux kernel that have been made since 1994. The Linux release scheme differentiates between development versions (which may be expected to undergo perfective maintenance) and production versions (probably dominated by corrective maintenance). The structure of the codebase also distinguishes code involved in handling different architectures and devices (where additions reflect adaptive maintenance) from the core of the system\u2019s kernel. Assuming that these dissections of the codebase indeed reflect different types of activity, we demonstrate that corrective maintenance does not necessarily lead to code deterioration, that adaptive maintenance may improve some quality metrics, and that growth is largely the result of continued development as part of perfective maintenance.", "num_citations": "2\n", "authors": ["150"]}
{"title": "Teaching TCP/IP hands-on\n", "abstract": " A review of TCP/IP Essentials: A Lab-Based Approach by Shivendra Panwar, Shiwen Mao, Jeong-Dong Ryoo, and Yihan Li, and Computer Networking: Internet Protocols in Action by Jeanna Matthews", "num_citations": "2\n", "authors": ["150"]}
{"title": "Flexible CoScheduling: Dealing with Load Imbalance and Heterogeneous Resources\n", "abstract": " Fine-grained parallel applications require all their processes to run simultaneously on distinct processors to make good progress. This is typically achieved by space slicing with variable partitioning, in which nodes are dedicated for the duration of the run, or by gang scheduling, in which time slicing is coordinated across processors. The problem is that both schemes suffer from fragmentation, where processors are left idle because jobs cannot be packed with 100% efficiency. Naturally, this leads to reduced utilization and sub-optimal performance. Flexible coscheduling (FCS) solves this problem by monitoring each job\u2019s granularity and communication activity, and using gang scheduling only for those jobs that really need it. Processes from other jobs, which can be scheduled without any constraints, are used as filler to reduce fragmentation. In addition, inefficiencies due to load imbalance and hardware heterogeneity are also reduced, because the classification is done on a per-process basis. FCS has been fully implemented as part of the STORM resource manager, and shown to be competitive with gang scheduling and implicit coscheduling.", "num_citations": "2\n", "authors": ["150"]}
{"title": "Flexible coscheduling\n", "abstract": " Flexible Coscheduling Page 1 Flexible Coscheduling Eitan Frachtenberg1,2, Dror Feitelson2, Fabrizio Petrini1, Juan Fernandez1 1CCS-3 Modeling, Algorithms, and Informatics Group Computer and Computational Sciences (CCS) Division Los Alamos National Laboratory {eitanf,fabrizio,juanf}@lanl.gov 2School of Computer Science and Engineering Hebrew University, Jerusalem, Israel feit@cs.huji.ac.il IPDPS 2003 Flexible Coscheduling \u2013 p.1/27 Page 2 Outline Parallel job scheduling Where we are Recent challenges and opportunities Flexible Coscheduling \u2013 p.2/27 Page 3 Outline Parallel job scheduling Where we are Recent challenges and opportunities Flexible coscheduling New job scheduling method Various kinds of applications and workloads Flexible Coscheduling \u2013 p.2/27 Page 4 Outline Parallel job scheduling Where we are Recent challenges and opportunities Flexible coscheduling New job method .\u2026", "num_citations": "2\n", "authors": ["150"]}
{"title": "XML, Hyper-media, and Fortran I/O\n", "abstract": " CiteSeerX \u2014 XML, Hyper-media, and Fortran I/O Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA XML, Hyper-media, and Fortran I/O Cached Download as a PDF Download Links [www.cs.huji.ac.il] Save to List Add to Collection Correct Errors Monitor Changes by Dror G. Feitelson , Tomer Klainer Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract this paper. Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and Technology \u00a9 2007-2019 The Pennsylvania State University \u2026", "num_citations": "2\n", "authors": ["150"]}
{"title": "The parallel break construct, or how to kill an activity tree\n", "abstract": " Most parallel languages provide means to express parallelism, e.g. a parallel-do construct, but no means to terminate the parallel activities spawned by such constructs. We propose three high-level primitives for this purpose, which are defined by analogies with primitives that break out of sequential iterative constructs. The primitives are pcontinue, which terminates the calling activity, pbreak, which terminates all the activities in the construct that spawned the calling activity, and return, which terminates all the activities created in the current function call. These constructs are especially useful in search problems, where an activity that finds a solution can terminate other activities that are investigating inferior approaches. Given that parallel constructs can be nested, activities form a tree rooted at the original activity that started the program. The main challenge in implementing pbreak and return is identifying the subtree\u00a0\u2026", "num_citations": "2\n", "authors": ["150"]}
{"title": "The Promise of Optical Free-Space Interconnections for Concurrent Memory Access\n", "abstract": " The concurrent-read concurrent-write (CRCW) model of shared memory for parallel computation is considered unimplementable in practice, due to limitations of electronic technology. But optical technology opens new possibilities that may overcome these limitations. An optical CRCW memory can be designed, in principle, based on the following ideas. First, optical free-space interconnections eliminate the need for shared switches that su er from contention. Second, practically unlimited fan-in on detectors allows for concurrent writing without serialization. And third, retrore ective optical elements that reverse the propagation of beams of light implement concurrent reads without address decoding and sequential service. It should be noted, however, that the required technology is still immature, and much work is needed before a practical design is reached.", "num_citations": "2\n", "authors": ["150"]}
{"title": "Envelopes in adaptive local queues for MIMD load balancing\n", "abstract": " Envelopes, a run-time mechanism which automatically supports adaptive local queues for MIMD load balancing, are proposed and demonstrated. Envelopes promote generality and language simplicity, while sustaining efficiency.             The local queues, one for each PE, contain a get_work task which pulls activities from a global list. In addition, they contain one or more envelopes within which activities are actually performed. These queues are adaptive because each get_work task competes with its own envelopes. The more load the PE has, the less additional work it will get. Envelopes are reused for successive activities, thus increasing the granularity. New envelopes are only created to cope with program data and synchronization dependencies, thereby avoiding deadlocks.             Experiments with envelopes performed and efficiency results are reported.", "num_citations": "2\n", "authors": ["150"]}
{"title": "Understanding large-scale software systems\u2013structure and flows\n", "abstract": " Program comprehension accounts for a large portion of software development costs and effort. The academic literature contains mainly research on program comprehension of short code snippets, but comprehension at the system level is no less important. We claim that comprehending a software system is a distinct activity that differs from code comprehension. We interviewed experienced developers, architects, and managers in the software industry and open-source community, to uncover the meaning of program comprehension at the system level; later we conducted a survey to verify the findings. The interviews demonstrate, among other things, that system comprehension is largely detached from code and programming language, and includes scope that is not captured in the code. It focuses on one hand on the structure of the system, and on the other hand on the flows in the system, but less on the code itself\u00a0\u2026", "num_citations": "1\n", "authors": ["150"]}
{"title": "Follow Your Nose--Which Code Smells are Worth Chasing?\n", "abstract": " The common use case of code smells assumes causality: Identify a smell, remove it, and by doing so improve the code. We empirically investigate their fitness to this use. We present a list of properties that code smells should have if they indeed cause lower quality. We evaluated the smells in 31,687 Java files from 677 GitHub repositories, all the repositories with 200+ commits in 2019. We measured the influence of smells on four metrics for quality, productivity, and bug detection efficiency. Out of 151 code smells computed by the CheckStyle smell detector, less than 20% were found to be potentially causal, and only a handful are rather robust. The strongest smells deal with simplicity, defensive programming, and abstraction. Files without the potentially causal smells are 50% more likely to be of high quality. Unfortunately, most smells are not removed, and developers tend to remove the easy ones and not the effective ones.", "num_citations": "1\n", "authors": ["150"]}
{"title": "The Corrective Commit Probability Code Quality Metric\n", "abstract": " We present a code quality metric, Corrective Commit Probability (CCP), measuring the probability that a commit reflects corrective maintenance. We show that this metric agrees with developers' concept of quality, informative, and stable. Corrective commits are identified by applying a linguistic model to the commit messages. Corrective commits are identified by applying a linguistic model to the commit messages. We compute the CCP of all large active GitHub projects (7,557 projects with at least 200 commits in 2019). This leads to the creation of a quality scale, suggesting that the bottom 10% of quality projects spend at least 6 times more effort on fixing bugs than the top 10%. Analysis of project attributes shows that lower CCP (higher quality) is associated with smaller files, lower coupling, use of languages like JavaScript and C# as opposed to PHP and C++, fewer developers, lower developer churn, better onboarding, and better productivity. Among other things these results support the \"Quality is Free\" claim, and suggest that achieving higher quality need not require higher expenses.", "num_citations": "1\n", "authors": ["150"]}
{"title": "Tony's Law\n", "abstract": " Seeking to promote regulations for reliable software for the long-term prosperity of the software industry.", "num_citations": "1\n", "authors": ["150"]}
{"title": "Topology and Routing in Clusters: From Theory to Practice\n", "abstract": " Designers of communication subsystems for clusters often present performance data by measuring bandwidth and latency for single point-to-point connections. Such data is not sensitive to routing algorithms and to network topology, and little experimental evidence relating to the impact of these factors on performance has been collected. On the other hand there is abundant theoretical evidence for the use of topologies with high bisection bandwidth and multiple and even randomized routes. We show that indeed these ideas can be applied in practice to achieve signi cant bene ts. In the case of topology, we show that the characteristics of commercially available Myrinet switches allow for the construction of related topologies with relatively few added links that provide much better support for intensive communication patterns. In the case of routing, we show simple mechanisms for implementing multiple paths in FM and randomization by randomized mapping of logical nodes. These mechanisms alleviate congestion due to uneven tra c patterns.", "num_citations": "1\n", "authors": ["150"]}
{"title": "Design, Implementation, and Impact of Multicast in the ParPar Control Network\n", "abstract": " The ParPar system is a high-performance cluster environment supporting a multiuser parallel workload. Its design follows a master-nodes structure, where the master controls all aspects of system activity using a dedicated control network. As nearly all control messages are multicast to a set of nodes, we implemented a reliable multicast protocol for this network based on UDP. This did not have a large impact on performance most of the time, as sending messages is only a small part of the master's activities. However, it did have a large impact when used to pre-load executable les to the nodes, rather than using demand paging via NFS. It is also more scalable than an asymmetrical hardware approach giving the master higher bandwidth, which can be used for small clusters.", "num_citations": "1\n", "authors": ["150"]}
{"title": "Exception Propagation in the ParPar System\n", "abstract": " A growing number of parallel system use Unix as the basis of their system software, rather than proprietary kernels. Moreover, conventional Unix services are used to spawn processes and control them. In many cases this leads to systems that are susceptible to pollution by stray processes that are not cleaned up properly when a job terminates. In the ParPar system, this is avoided by introducing the notion of metasignals, which are used to propagate signalling information among the nodes of the system. Thus when any process terminates abnormally, a metasignal is sent to the other nodes used by the job, where it is translated to a Unix SIGUSR1 and forwarded to the remaining processes of the job. These processes can either catch the signal and re-organize the computation, or do nothing, in which case they are killed. This default mechanism provides the required cleanup, independent of possible support (or lack thereof) from the programming environment.", "num_citations": "1\n", "authors": ["150"]}
{"title": "A three-dimensional optical interconnection network with distributed control\n", "abstract": " We present a design for an optical free-space, crossbar-like interconnection network. Based on a 1 to N routeing switch, we give details of a possible set-up utilizing three-dimensional space. One switch is associated with each processing element (PE), and the routeing is done in a distributed manner by being incorporated in the action of transmission. The light is thereafter directed by multifaceted holograms. Parallel algorithms with communication locality can benefit from the uniform distribution of PEs in space, by minimizing the time latency of their communications.", "num_citations": "1\n", "authors": ["150"]}