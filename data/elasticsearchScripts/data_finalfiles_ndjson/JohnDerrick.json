{"title": "Refinement in Z and Object-Z: foundations and advanced applications\n", "abstract": " Refinement is one of the cornerstones of the formal approach to software engineering, and its use in various domains has led to research on new applications and generalisation. This book brings together this important research in one volume, with the addition of examples drawn from different application areas. It covers four main themes: Data refinement and its application to Z Generalisations of refinement that change the interface and atomicity of operations Refinement in Object-Z Modelling state and behaviour by combining Object-Z with CSP Refinement in Z and Object-Z: Foundations and Advanced Applications provides an invaluable overview of recent research for academic and industrial researchers, lecturers teaching formal specification and development, industrial practitioners using formal methods in their work, and postgraduate and advanced undergraduate students. This second edition is a comprehensive update to the first and includes the following new material: Early chapters have been extended to also include trace refinement, based directly on partial relations rather than through totalisation Provides an updated discussion on divergence, non-atomic refinements and approximate refinement Includes a discussion of the differing semantics of operations and outputs and how they affect the abstraction of models written using Object-Z and CSP Presents a fuller account of the relationship between relational refinement and various models of refinement in CSP Bibliographic notes at the end of each chapter have been extended with the most up to date citations and research", "num_citations": "404\n", "authors": ["1044"]}
{"title": "Inferring extended finite state machine models from software executions\n", "abstract": " The ability to reverse-engineer models of software behaviour is valuable for a wide range of software maintenance, validation and verification tasks. Current reverse-engineering techniques focus either on control-specific behaviour (e.g., in the form of Finite State Machines), or data-specific behaviour (e.g., as pre / post-conditions or invariants). However, typical software behaviour is usually a product of the two; models must combine both aspects to fully represent the software\u2019s operation. Extended Finite State Machines (EFSMs) provide such a model. Although attempts have been made to infer EFSMs, these have been problematic. The models inferred by these techniques can be non-deterministic, the inference algorithms can be inflexible, and only applicable to traces with specific characteristics. This paper presents a novel EFSM inference technique that addresses the problems of inflexibility and non\u00a0\u2026", "num_citations": "162\n", "authors": ["1044"]}
{"title": "Specification, refinement and verification of concurrent systems\u2014an integration of Object-Z and CSP\n", "abstract": " This paper presents a method of formally specifying, refining and verifying concurrent systems which uses the object-oriented state-based specification language Object-Z together with the process algebra CSP. Object-Z provides a convenient way of modelling complex data structures needed to define the component processes of such systems, and CSP enables the concise specification of process interactions. The basis of the integration is a semantics of Object-Z classes identical to that of CSP processes. This allows classes specified in Object-Z to be used directly within the CSP part of the specification.               In addition to specification, we also discuss refinement and verification in this model. The common semantic basis enables a unified method of refinement to be used, based upon CSP refinement. To enable state-based techniques to be used for the Object-Z components of a specification we\u00a0\u2026", "num_citations": "103\n", "authors": ["1044"]}
{"title": "Mechanically verified proof obligations for linearizability\n", "abstract": " Concurrent objects are inherently complex to verify. In the late 80s and early 90s, Herlihy and Wing proposed linearizability as a correctness condition for concurrent objects, which, once proven, allows us to reason about concurrent objects using pre- and postconditions only. A concurrent object is linearizable if all of its operations appear to take effect instantaneously some time between their invocation and return. In this article we define simulation-based proof conditions for linearizability and apply them to two concurrent implementations, a lock-free stack and a set with lock-coupling. Similar to other approaches, we employ a theorem prover (here, KIV) to mechanize our proofs. Contrary to other approaches, we also use the prover to mechanically check that our proof obligations actually guarantee linearizability. This check employs the original ideas of Herlihy and Wing of verifying linearizability via possibilities.", "num_citations": "74\n", "authors": ["1044"]}
{"title": "A formal framework for viewpoint consistency\n", "abstract": " Multiple Viewpoint models of system development are becoming increasingly important. Each viewpoint offers a different perspective on the target system and system development involves parallel refinement of the multiple views. Viewpoints related approaches have been considered in a number of different guises by a spectrum of researchers. Our work particularly focuses on the use of viewpoints in Open Distributed Processing (ODP) which is an ISO/ITU standardisation framework. The requirements of viewpoints modelling in ODP are very broad and, hence, demanding. Multiple viewpoints, though, prompt the issue of consistency between viewpoints. This paper describes a very general interpretation of consistency which we argue is broad enough to meet the requirements of consistency in ODP. We present a formal framework for this general interpretation; highlight basic properties of the interpretation\u00a0\u2026", "num_citations": "74\n", "authors": ["1044"]}
{"title": "Viewpoint consistency in ODP\n", "abstract": " Open Distributed Processing (ODP) is a joint ITU/ISO standardisation framework for constructing distributed systems in a multi-vendor environment. Central to the ODP approach is the use of viewpoints for specification and design. Inherent in any viewpoint approach is the need to check and manage the consistency of viewpoints. In previous work we have described techniques for consistency checking, refinement, and translation between viewpoint specifications, in particular for LOTOS and Z/Object-Z. Here we present an overview of our work, motivated by a case study combining these techniques in order to show consistency between viewpoints specified in LOTOS and Object-Z.", "num_citations": "70\n", "authors": ["1044"]}
{"title": "Refinement and verification of concurrent systems specified in Object-Z and CSP\n", "abstract": " The formal development of large or complex systems can often be facilitated by the use of more than one formal specification language. Such a combination of languages is particularly suited to the specification of concurrent or distributed systems, where both the modelling of processes and state is necessary. This paper presents an approach to refinement and verification of specifications written using a combination of Object-Z and CSP (communicating sequential processes). A common semantic basis for the two languages enables a unified method of refinement to be used, based upon CSP refinement. To enable state-based techniques to be used for the Object-Z components of a specification, we develop state-based refinement relations which are sound and complete with respect to CSP refinement. In addition, a verification method for static and dynamic properties is presented. The method allows us to verify\u00a0\u2026", "num_citations": "69\n", "authors": ["1044"]}
{"title": "FDTs for ODP\n", "abstract": " This paper discusses the use and integration of formal techniques into the Open Distributed Processing (ODP) standardization initiative. The ODP reference model is a natural progression from OSI. Multiple viewpoints are used to specify complex ODP systems. Formal methods are playing an increasing role within ODP. We provide an overview of the ODP reference model, before discussing the ODP requirements on FDTs, and the role such techniques play. Finally we discuss the use of formalisms in the central problem of maintaining cross viewpoint consistency.", "num_citations": "68\n", "authors": ["1044"]}
{"title": "Author obliged to submit paper before 4 july: Policies in an enterprise specification\n", "abstract": " Specifying policies doesn\u2019t occur in splendid isolation but as part of refining an enterprise specification. The roles, the tasks, and the business processes of an ODP community provide the basic alphabet over which we write our policies. We illustrate this through exploring a conference programme committee case study. We discuss how we might formulate policies and show how policies are refined alongside the refinement of the overall system specification, developing notions of sufficiency and necessity. Policy delegation is also discussed and we categorise different forms of delegating an obligation.", "num_citations": "62\n", "authors": ["1044"]}
{"title": "Viewpoint consistency in ODP, a general interpretation\n", "abstract": " Multiple viewpoints are used in Open Distributed Processing (ODP) in order to decompose the complexity inherent in specifying distributed systems. Multiple viewpoints prompt the issue of consistency between viewpoints. The ODP reference model alludes to three different interpretations of consistency. This paper responds to this uncertainty by proposing a single all embracing interpretation of consistency. We show that our interpretation, firstly, satisfies all the basic requirements of a definition of consistency and, secondly, can be specialised to any of the three ODP reference model definitions. The generality of our definition will be illustrated through instantiation in the FDT LOTOS.", "num_citations": "61\n", "authors": ["1044"]}
{"title": "Increasing functional coverage by inductive testing: A case study\n", "abstract": " This paper addresses the challenge of generating test sets that achieve functional coverage, in the absence of a complete specification. The inductive testing technique works by probing the system behaviour with tests, and using the test results to construct an internal model of software behaviour, which is then used to generate further tests. The idea in itself is not new, but prior attempts to implement this idea have been hampered by expense and scalability, and inflexibility with respect to testing strategies. In the past, inductive testing techniques have tended to focus on the inferred models, as opposed to the suitability of the test sets that were generated in the process. This paper presents a flexible implementation of the inductive testing technique, and demonstrates its application with case-study that applies it to the Linux TCP stack implementation. The evaluation shows that the generated test sets achieve a\u00a0\u2026", "num_citations": "57\n", "authors": ["1044"]}
{"title": "Verifying linearisability with potential linearisation points\n", "abstract": " Linearisability is the key correctness criterion for concurrent implementations of data structures shared by multiple processes. In this paper we present a proof of linearisability of the lazy implementation of a set due to Heller et\u00a0al. The lazy set presents one of the most challenging issues in verifying linearisability: a linearisation point of an operation set by a process other than the one executing it. For this we develop a proof strategy based on refinement which uses thread local simulation conditions and the technique of potential linearisation points. The former allows us to prove linearisability for arbitrary numbers of processes by looking at only two processes at a time, the latter permits disposing with reasoning about the past. All proofs have been mechanically carried out using the interactive prover KIV.", "num_citations": "56\n", "authors": ["1044"]}
{"title": "Relational concurrent refinement\n", "abstract": " Refinement in a concurrent context, as typified by a process algebra, takes a number of different forms depending on what is considered observable. Observations record, for example, which events a system is prepared to accept or refuse. Concurrent refinement relations include trace refinement, failures\u2013divergences refinement, readiness refinement and bisimulation. Refinement in a state-based language such as Z, on the other hand, is defined using a relational model in terms of the input\u2013output behaviour of abstract programs. These refinements are normally verified by using two simulation rules which help make the verification tractable. This paper unifies these two standpoints by generalising the standard relational model to include additional observable aspects. These are chosen in such a way that they represent exactly the notions of observation embedded in the various concurrent refinement\u00a0\u2026", "num_citations": "55\n", "authors": ["1044"]}
{"title": "ODP enterprise viewpoint specification\n", "abstract": " The Open Distributed Processing (ODP) standardisation initiative has led to a framework by which distributed systems can be modelled using a number of viewpoints. These include an enterprise viewpoint, which focuses on the objectives and policies of the enterprise that the system is meant to support. Although the ODP reference model provides abstract languages of relevant concepts, it does not prescribe particular techniques that are to be used in the individual viewpoints. In particular, there is a need to develop appropriate notations for ODP enterprise specification, in order to increase the applicability of the ODP framework. In this paper, we tackle this concern and develop a specification language to support the current draft of the enterprise viewpoint. In doing so, we analyse the current definition of the ODP enterprise viewpoint language. Using the Unified Modelling Language (uml), a meta-model of the core\u00a0\u2026", "num_citations": "55\n", "authors": ["1044"]}
{"title": "Formalising ODP enterprise policies\n", "abstract": " The open distributed processing (ODP) standardisation initiative has led to a framework by which distributed systems can be modelled using a number of viewpoints. These include an enterprise viewpoint, which focuses on the objectives and policies of the enterprise that the system is meant to support. Although the ODP reference model provides abstract languages of relevant concepts, it does not prescribe particular techniques that are to be used in the individual viewpoints. In particular, there is a need to develop appropriate notations for ODP enterprise specification, in order to increase the applicability of the ODP framework. In this paper, we tackle this concern and develop a specification language to support the enterprise viewpoint. In doing so, we focus on the expression of enterprise policies that govern the behaviour of enterprise objects. The language we develop is a combination of structured English and\u00a0\u2026", "num_citations": "55\n", "authors": ["1044"]}
{"title": "Constructive consistency checking for partial specification in Z\n", "abstract": " Partial specification is a method of specifying complex systems in which the system is described by a collection of specifications, each approaching the system from a different viewpoint. The specification notation Z is often advocated as a suitable language for this style of specification. For collections of partial specifications to be meaningful, they need to be consistent, i.e. they should not impose contradictory requirements. This paper addresses how consistency between partial specifications in Z can be checked, by constructing unifications, i.e. least common refinements, of viewpoint specifications.", "num_citations": "54\n", "authors": ["1044"]}
{"title": "How to prove algorithms linearisable\n", "abstract": " Linearisability is the standard correctness criterion for concurrent data structures. In this paper, we present a sound and complete proof technique for linearisability based on backward simulations. We exemplify this technique by a linearisability proof of the queue algorithm presented in Herlihy and Wing\u2019s landmark paper. Except for the manual proof by them, none of the many other current approaches to checking linearisability has successfully treated this intricate example. Our approach is grounded on complete mechanisation: the proof obligations for the queue are verified using the interactive prover KIV, and so is the general soundness and completeness result for our proof technique.", "num_citations": "53\n", "authors": ["1044"]}
{"title": "Analysis of a multimedia stream using stochastic process algebra\n", "abstract": " It is now well recognized that the next generation of distributed systems will be distributed multimedia systems. Central to multimedia systems is quality of service, which defines the non-functional requirements on the system. In this paper we investigate how stochastic process algebra can be used in order to determine the quality of service properties of distributed multimedia systems. We use a simple multimedia stream as our basic example. We describe it in the stochastic process algebra PEPA and then we analyse whether the stream satisfies a set of quality of service parameters: throughput, end-to-end latency, jitter and error rates.", "num_citations": "53\n", "authors": ["1044"]}
{"title": "Iterative refinement of reverse-engineered models by model-based testing\n", "abstract": " This paper presents an iterative technique to accurately reverse engineer models of the behaviour of software systems. A key novelty of the approach is the fact that it uses model-based testing to refine the hypothesised model. The process can in principle be entirely automated, and only requires a very small amount of manually generated information to begin with. We have implemented the technique for use in the development of Erlang systems and describe both the methodology as well as our implementation.", "num_citations": "50\n", "authors": ["1044"]}
{"title": "Viewpoint consistency in Z and LOTOS: A case study\n", "abstract": " Specification by viewpoints is advocated as a suitable method of specifying complex systems. Each viewpoint describes the envisaged system from a particular perspective, using concepts and specification languages best suited for that perspective.             Inherent in any viewpoint approach is the need to check or manage the consistency of viewpoints and to show that the different viewpoints do not impose contradictory requirements. In previous work we have described a range of techniques for consistency checking, refinement, and translation between viewpoint specifications, in particular for the languages LOTOS and Z. These two languages are advocated in a particular viewpoint model, viz. that of the Open Distributed Processing (ODP) reference model. In this paper we present a case study which demonstrates how all these techniques can be combined in order to show consistency between a\u00a0\u2026", "num_citations": "47\n", "authors": ["1044"]}
{"title": "Cross-viewpoint consistency in open distributed processing\n", "abstract": " Discusses the use of viewpoints in the ODP (Open Distributed Processing) standardisation initiative. The ODP reference model is a new framework, going beyond OSI (Open Systems Interconnection). Multiple viewpoints are used to specify complex ODP systems. Consistency of viewpoint specifications is clearly a central issue, In addition, formal techniques have an increasingly significant role within ODP, and so mechanisms are needed that support consistency checking of formal specifications. An overview is provided of the ODP reference model and the use of viewpoints within it, before discussing consistency within ODP and how it can be realised using formal notations. Consistency checking is illustrated using the LOTOS formal description technique.", "num_citations": "47\n", "authors": ["1044"]}
{"title": "Model checking stochastic automata\n", "abstract": " Modern distributed systems include a class of applications in which non-functional requirements are important. In particular, these applications include multimedia facilities where real time constraints are crucial to their correct functioning. In order to specify such systems it is necessary to describe that events occur at times given by probability distributions; stochastic automata have emerged as a useful technique by which such systems can be specified and verified.However, stochastic descriptions are very general, in particular they allow the use of general probability distribution functions, and therefore their verification can be complex. In the last few years, model checking has emerged as a useful verification tool for large systems. In this article we describe two model checking algorithms for stochastic automata. These algorithms consider how properties written in a simple probabilistic real-time logic can be checked\u00a0\u2026", "num_citations": "46\n", "authors": ["1044"]}
{"title": "Supporting ODP-Translating LOTOS to Z\n", "abstract": " This paper describes a translation of full LOTOS into Z. A common semantic model is defined and the translation is proved correct with respect to the semantics.               The motivation for such a translation is the use of multiple viewpoints for specifying complex systems defined by the reference model of the Open Distributed Processing (ODP) standardization initiative.", "num_citations": "45\n", "authors": ["1044"]}
{"title": "Testing refinements of state\u2010based formal specifications\n", "abstract": " A specification provides a concise description of a system, and can be used as both the benchmark against which any implementation is tested, and also as a means to generate tests. Formal specifications have potential advantages over informal descriptions because they offer the possibility of reducing the costs of testing by automating part of the testing process. This observation has led to considerable interest in developing test generation techniques from formal specifications, and a number of different methods have been derived for state\u2010based formalisms such as Z, B and VDM. However, after tests have been derived from a formal specification, the specification might be refined further before it is implemented, and therefore a mechanism is needed to relate the abstract tests to the refined implementation. The purpose of this paper is to provide such a method by exploring the relationship between testing and\u00a0\u2026", "num_citations": "44\n", "authors": ["1044"]}
{"title": "Verifying linearisability: A comparative survey\n", "abstract": " Linearisability is a key correctness criterion for concurrent data structures, ensuring that each history of the concurrent object under consideration is consistent with respect to a history of the corresponding abstract data structure. Linearisability allows concurrent (i.e., overlapping) operation calls to take effect in any order, but requires the real-time order of nonoverlapping to be preserved. The sophisticated nature of concurrent objects means that linearisability is difficult to judge, and hence, over the years, numerous techniques for verifying lineasizability have been developed using a variety of formal foundations such as data refinement, shape analysis, reduction, etc. However, because the underlying framework, nomenclature, and terminology for each method is different, it has become difficult for practitioners to evaluate the differences between each approach, and hence, evaluate the methodology most appropriate\u00a0\u2026", "num_citations": "43\n", "authors": ["1044"]}
{"title": "Development of a verified Erlang program for resource locking\n", "abstract": " In this paper, we describe a tool to verify Erlang programs and show, by means of an industrial case study, how this tool is used. The tool includes a number of components, including a translation component, a state space generation component and a model checking component.                 To verify properties of the code, the tool first translates the Erlang code into a process algebraic specification. The outcome of the translation is made more efficient by taking advantage of the fact that software written in Erlang builds upon software design patterns such as client\u2013server behaviours. A labelled transition system is constructed from the specification by use of the \u03bcCRL toolset. The resulting labelled transition system is model checked against a set of properties formulated in the \u03bc-calculus using the Caesar/Ald\u00e9baran toolset.               As a case study we focus on a simplified resource manager modelled on a real\u00a0\u2026", "num_citations": "42\n", "authors": ["1044"]}
{"title": "Consistency and refinement for partial specification in Z\n", "abstract": " This paper discusses theoretical background for the use of Z as a language for partial specification, in particular techniques for checking consistency between viewpoint specifications. The main technique used is unification, i.e. finding a (candidate) least common refinement. The corresponding notion of consistency between specifications turns out to be different from the known notions of consistency for single Z specifications. A key role is played by correspondence relations between the data types used in the various viewpoints.", "num_citations": "42\n", "authors": ["1044"]}
{"title": "Proving linearizability via non-atomic refinement\n", "abstract": " Linearizability is a correctness criterion for concurrent objects. In this paper, we prove linearizability of a concurrent lock-free stack implementation by showing the implementation to be a non-atomic refinement of an abstract stack. To this end, we develop a generalisation of non-atomic refinement allowing one to refine a single (Z) operation into a CSP process. Besides this extension, the definition furthermore embodies a termination condition which permits one to prove starvation freedom for the concurrent processes.", "num_citations": "40\n", "authors": ["1044"]}
{"title": "Using coupled simulations in non-atomic refinement\n", "abstract": " Refinement is one of the most important techniques in formal system design, supporting stepwise development of systems from abstract specifications into more concrete implementations. Non-atomic refinement is employed when the level of granularity changes during a refinement step, i.e., whenever an abstract operation is refined into a sequence of concrete operations, as opposed to a single concrete operation. There has been some limited work on non-atomic refinement in Z, and the purpose of this paper is to extend this existing theory. In particular, we strengthen the proposed definition to exclude certain behaviours which only occur in the concrete specification but have no counterpart on the abstract level. To do this we use coupled simulations: the standard simulation relation is complemented by a second relation which guarantees the exclusion of undesired behaviour of the concrete system. These\u00a0\u2026", "num_citations": "40\n", "authors": ["1044"]}
{"title": "On behavioural subtyping in LOTOS\n", "abstract": " We consider how the OO notion of subtyping relates to lotos testing theory. In particular, we investigate which of the standard lotos preorders is a suitable instantiation of behavioural subtyping and argue that each of the main preorders, trace preorder, trace extension, reduction and extension, is in some way deficient. Then, in the light of pre and post condition based models of OO subtyping, we re-work the basic interpretation applied to lotos behaviour descriptions. We argue that this re-interpretation enables reduction to be used as an instantiation of behavioural subtyping.", "num_citations": "37\n", "authors": ["1044"]}
{"title": "A sound and complete proof technique for linearizability of concurrent data structures\n", "abstract": " Efficient implementations of data structures such as queues, stacks or hash-tables allow for concurrent access by many processes at the same time. To increase concurrency, these algorithms often completely dispose with locking, or only lock small parts of the structure. Linearizability is the standard correctness criterion for such a scenario\u2014where a concurrent object is linearizable if all of its operations appear to take effect instantaneously some time between their invocation and return. The potential concurrent access to the shared data structure tremendously increases the complexity of the verification problem, and thus current proof techniques for showing linearizability are all tailored to specific types of data structures. In previous work, we have shown how simulation-based proof conditions for linearizability can be used to verify a number of subtle concurrent algorithms. In this article, we now show that conditions\u00a0\u2026", "num_citations": "36\n", "authors": ["1044"]}
{"title": "Issues in implementing a model checker for Z\n", "abstract": " In this paper we discuss some issues in implementing a model checker for the Z specification language. In particular, the language design of Z and its semantics, raises some challenges for efficient model checking, and we discuss some of these issues here. Our approach to model checking Z specifications involves implementing a translation from Z into the SAL input language, upon which the SAL toolset can be applied. In this paper we discuss issues in the implementation of this translation algorithm and illustrate them by looking at how the mathematical toolkit is encoded in SAL and the resultant efficiency of the model checking tools.", "num_citations": "33\n", "authors": ["1044"]}
{"title": "Verifying Erlang code: a resource locker case-study\n", "abstract": " In this paper we describe an industrial case-study on the development of formally verified code for Ericsson\u2019s AXD 301 switch. For the formal verification of Erlang software we have developed a tool to apply model checking to communicating Erlang processes. We make effective use of Erlang\u2019s design principles for large software systems to obtain relatively small models of specific Erlang programs. By assuming a correct implementation of the software components and embedding their semantics into our model, we can concentrate on the specific functionality of the components. We constructed a tool to automatically translate the Erlang code to a process algebra with data. Existing tools were used to generate the full state space and to formally verify properties stated in the modal \u03bc-calculus.             As long as the specific functionality of the component has a finite state vector, we can generate a finite state\u00a0\u2026", "num_citations": "33\n", "authors": ["1044"]}
{"title": "Weak refinement in Z\n", "abstract": " An important aspect in the specification of distributed systems is the role of the internal (or unobservable) operation. Such operations are not part of the user interface (i.e. the user cannot invoke them), however, they are essential to our understanding and correct modelling of the system. Various conventions have been employed to model internal operations when specifying distributed systems in Z. If internal operations are distinguished in the specification notation, then refinement needs to deal with internal operations in appropriate ways. However, in the presence of internal operations, standard Z refinement leads to undesirable implementations.             In this paper we present a generalization of Z refinement, called weak refinement, which treats internal operations differently from observable operations when refining a system. We illustrate some of the properties of weak refinement through a specification\u00a0\u2026", "num_citations": "32\n", "authors": ["1044"]}
{"title": "Non-atomic Refinement\n", "abstract": " Another way in which the level of abstraction can change in development is through the granularity of operations, in particular changing one abstract operation into several concrete ones. This chapter considers the consequences of allowing operations to be decomposed in refinement, which involves further generalisation of the theory from Chap.\u00a0                 3                                . This is often known as non-atomic or action refinement in the literature. The final sections discuss applications to verifying concurrent data structures.", "num_citations": "31\n", "authors": ["1044"]}
{"title": "Using UML to specify QoS constraints in ODP\n", "abstract": " This paper is concerned with Quality of Service (QoS) specification in distributed system design. The specification and implementation of QoS is increasingly important in distributed systems due to the need to address questions of performance, particularly for systems involving multimedia. To ensure correct implementation of QoS requirements, statements of QoS need to be introduced early in the design process, and in terms of design we consider the use of the Unified Modelling Language (UML), which has quickly become the de facto standard for object-based designs.The framework we use for distributed system construction is that provided by the open distributed processing reference model, and we focus in particular on its computational viewpoint. The aim of this paper is to construct a UML model of the computational viewpoint focusing on the description of QoS within that viewpoint. To specify the QoS\u00a0\u2026", "num_citations": "31\n", "authors": ["1044"]}
{"title": "Strategies for consistency checking based on unification\n", "abstract": " There is increasing interest in models of system development which use Multiple Viewpoints. Each viewpoint offers a different perspective on the target system and system development involves parallel refinement of the multiple views. Multiple viewpoints though, prompt the issue of consistency between viewpoints. This paper describes an interpretation of consistency which is general enough to meet the requirements of consistency for very general viewpoints models. Furthermore, the paper investigates strategies for checking this consistency definition. Particular emphasis is placed on mechanisms to obtain global consistency (between an arbitrary number of viewpoints) from a series of binary consistency checks. The consistency checking strategies we develop are illustrated using the formal description technique LOTOS.", "num_citations": "31\n", "authors": ["1044"]}
{"title": "A framework for UML consistency\n", "abstract": " In this paper we discuss a framework by which one might approach questions of consistency in UML. The framework derives from work undertaken in the Open Distributing Processing (ODP) standardisation initiative which looked at consistency checking across the ODP viewpoints. This work has resonance with some of the problems facing those using the many different aspects of UML, and the purpose of this paper is to discuss how the existing work could be applied in a UML context.", "num_citations": "30\n", "authors": ["1044"]}
{"title": "Composition of LOTOS specifications\n", "abstract": " Usually formal methods adopt the traditional waterfall model of system design. New design methodologies, such as Open Distributed Processing and Object Oriented Design, allow for incremental and partial specification. In order to support such design methods, the issues of consistency between specifications and composition of (partial) specification become vital. This paper presents a general framework for dealing with partial specification, which is instantiated for the specification language LOTOS. Necessary and sufficient conditions for consistency to hold between LOTOS specifications are given, and an operational semantics for composition is proposed.", "num_citations": "30\n", "authors": ["1044"]}
{"title": "Guards, preconditions, and refinement in Z\n", "abstract": " In the common Z specification style operations are, in general, partial relations. The domains of these partial operations are traditionally called preconditions, and there are two interpretations of the result of applying an operation outside its domain. In the traditional interpretation anything may result whereas in the alternative, guarded, interpretation the operation is blocked outside its precondition.             In fact these two interpretations can be combined, and this allows representation of both refusals and underspecification in the same model. In this paper we explore this issue, and we extend existing work in this area by allowing arbitrary predicates in the guard.             To do so we adopt a non-standard three valued interpretation of an operation by introducing a third truth value. This value corresponds to a situation where we don\u2019t care what effect the operation has, i.e. the guard holds but we may be outside\u00a0\u2026", "num_citations": "29\n", "authors": ["1044"]}
{"title": "A junction between state based and behavioural specification\n", "abstract": " Two of the dominant paradigms for formally describing and analysing OO distributed systems are state based specification, e.g. Object-Z, and behavioural specification, e.g. process algebra. The style of specification embodied by the two paradigms is highly contrasting. With state based techniques the data state is explicitly defined while the temporal ordering of operations is left implicit, in contrast in behavioural techniques, no explicit data state definition is given while the temporal ordering of action offers is focused on. However, in order to support sophisticated software engineering principles, e.g. multi-paradigm specification, viewpoints modelling and subtyping, there is now considerable interest in developing strategies for relating state based and behavioural specification paradigms.               This paper serves two purposes \u2014 firstly, it reviews the existing body of work on relating these two specification\u00a0\u2026", "num_citations": "29\n", "authors": ["1044"]}
{"title": "Viewpoints and objects\n", "abstract": " There have been a number of proposals to split the specification of large and complex systems into a number of inter-related specifications, called viewpoints. Such a model of multiple viewpoints forms the cornerstone of the Open Distributed Processing (ODP) standardisation initiative. We address two of the technical problems concerning the use of formal techniques within multiple viewpoint models: these are unification and consistency checking. We discuss the software engineering implications of using viewpoints, and show that object encapsulation provides the necessary support for such a model. We then consider how this might be supported by using object-oriented variants of Z.", "num_citations": "29\n", "authors": ["1044"]}
{"title": "Comparing LOTOS and Z refinement relations\n", "abstract": " This paper compares the Z refinement relation with various LOTOS refinement relations. The motivation for such a comparison is the use of multiple viewpoints for specifying complex systems defined by the reference model of the Open Distributed Processing (ODP) standardization initiative.               The ODP architectural semantics describes the application of formal description techniques (FDTs) to the specification of ODP systems. Of the available FDTs, Z is likely to be used for at least the information, and possibly other, viewpoints, whilst LOTOS is a strong candidate for use in the computational viewpoint. Mechanisms are clearly needed to support the parallel development, and integration of, viewpoints using these FDTs. We compare the LOTOS bisimulation relations and the reduction relations to the Z refinement relation showing that failure-traces refinement corresponds closely to refinement in Z.", "num_citations": "28\n", "authors": ["1044"]}
{"title": "Unifying concurrent and relational refinement\n", "abstract": " Refinement in a concurrent context, as typified by a process algebra, takes a number of different forms depending on what is considered observable, where observations record, for example, which events a system is prepared to accept or refuse. Examples of concurrent refinement relations include trace refinement, failures-divergences refinement and bisimulation.Refinement in a state-based language such as Z, on the other hand, is defined using a relational model in terms of input/output behaviour of abstract programs. These refinements are verified by using two simulation rules which help make the verification tractable.The purpose of this paper is to unify these two standpoints, and we do so by generalising the standard relational model to include additional observable aspects. The central result of the paper is then to develop simulation rules to verify relations such as failures-divergences refinement in a\u00a0\u2026", "num_citations": "26\n", "authors": ["1044"]}
{"title": "Structural refinement in Object-Z/CSP\n", "abstract": " State-based refinement relations have been developed for use on the Object-Z components in an integrated Object-Z / CSP specification. However this refinement methodology does not allow the structure of a specification to be changed in a refinement, whereas a full methodology would allow concurrency to be introduced during the development life-cycle. In this paper we tackle these concerns and discuss refinements of specifications written using Object-Z and CSP where we change the structure of the specification when performing the refinement. In particular, we develop a set of structural simulation rules which allow a single Object-Z component to be refined to a number of communicating or interleaved classes. We prove soundness of these rules and illustrate them with a small example.", "num_citations": "26\n", "authors": ["1044"]}
{"title": "Calculating upward and downward simulations of state-based specifications\n", "abstract": " This paper concerns calculational methods of refinement in state-based specification languages. Data refinement is a well-established technique for transforming specifications of abstract data types into ones, which are closer to an eventual implementation. The conditions under which a transformation is a correct refinement are encapsulated into two simulation rules: downward and upward simulations.One approach for refining an abstract system is to specify the concrete data type, and then attempt to verify that it is a valid refinement of the abstract type. An alternative approach is to calculate the concrete specification based upon the abstract specification and a retrieve relation, which links the abstract and concrete states. In this paper we generalise existing calculational methods for downward simulations and derive similar results for upward simulations; we also document their use and application in a particular\u00a0\u2026", "num_citations": "26\n", "authors": ["1044"]}
{"title": "Relational concurrent refinement part II: Internal operations and outputs\n", "abstract": " Two styles of description arise naturally in formal specification: state-based and behavioural. In state-based notations, a system is characterised by a collection of variables, and their values determine which actions may occur throughout a system history. Behavioural specifications describe the chronologies of actions\u2014interactions between a system and its environment. The exact nature of such interactions is captured in a variety of semantic models with corresponding notions of refinement; refinement in state based systems is based on the semantics of sequential programs and is modelled relationally. Acknowledging that these viewpoints are complementary, substantial research has gone into combining the paradigms. The purpose of this paper is to do three things. First, we survey recent results linking the relational model of refinement to the process algebraic models. Specifically, we detail how\u00a0\u2026", "num_citations": "25\n", "authors": ["1044"]}
{"title": "Some results on cross viewpoint consistency checking\n", "abstract": " The ODP multiple viewpoints model prompts the very challenging issue of cross viewpoint consistency. This paper considers definitions of consistency arising from the RM-ODP and relates these in a mathematical framework for consistency checking. We place existing FDTs, in particular LOTOS, into this framework. Then we consider the prospects for viewpoint translation. Our conclusions centre on the relationship between the different definitions of consistency and on the requirements for realistic consistency checking.", "num_citations": "25\n", "authors": ["1044"]}
{"title": "Verifying C11 programs operationally\n", "abstract": " This paper develops an operational semantics for a release-acquire fragment of the C11 memory model with relaxed accesses. We show that the semantics is both sound and complete with respect to the axiomatic model of Batty et al. The semantics relies on a per-thread notion of observability, which allows one to reason about a weak memory C11 program in program order. On top of this, we develop a proof calculus for invariant-based reasoning, which we use to verify the release-acquire version of Peterson's mutual exclusion algorithm.", "num_citations": "24\n", "authors": ["1044"]}
{"title": "Addressing computational viewpoint design\n", "abstract": " Distributed system design is a highly complicated and non-trivial task. The problem is the characterised by the need to design multi-threaded, multi-processor, and multimedia systems. Design frameworks such as open distributed processing (ODP), the ITU/ISO standard, define a number of viewpoints from which the design of a distributed system should be approached. To use the framework, a design language for each of these viewpoints must be defined. This paper defines a computational viewpoint language based on the Unified Modelling Language (UML) and Component Quality Modelling Language (CQML). The use of this approach to provide the ODP viewpoint languages enables standard UML tools to be used as part of an ODP compliant design process; and in addition, it will potentially enable the use of Meta Object Facility (MOF) based generation tools for constructing tool support for our language.", "num_citations": "24\n", "authors": ["1044"]}
{"title": "Abstract specification in Object-Z and CSP\n", "abstract": " A number of integrations of the state-based specification language Object-Z and the process algebra CSP have been proposed in recent years. In developing such integrations, a number of semantic decisions have to be made. In particular, what happens when an operation\u2019s precondition is not satisfied? Is the operation blocked, i.e., prevented from occurring, or can it occur with an undefined result? Also, are outputs from operations angelic, satisfying the environment\u2019s constraints on them, or are they demonic and not influenced by the environment at all? In this paper we discuss the differences between the models, and show that by adopting a blocking model of preconditions together with an angelic model of outputs one can specify systems at higher levels of abstraction.", "num_citations": "24\n", "authors": ["1044"]}
{"title": "Specifying and refining internal operations in Z\n", "abstract": " An important aspect in the specification of distributed systems is the  role of the internal (or unobservable) operation. Such operations are not part of the interface to the environment (i.e. the user cannot invoke them), however, they are essential to our understanding and correct modelling of the system. In this paper we are interested in the use of the formal specification notation Z for the description of distributed systems. Various conventions have been employed to model internal operations when specifying such systems in Z. If internal operations are distinguished in the specification notation, then refinement needs to deal with internal operations in appropriate ways.               Using an example of a telecommunications protocol we show that standard Z refinement is inappropriate for refining a system when internal operations are specified explicitly. We present a generalisation of Z refinement, called weak\u00a0\u2026", "num_citations": "24\n", "authors": ["1044"]}
{"title": "Maintaining cross viewpoint consistency using Z\n", "abstract": " This paper discusses the use and integration of formal techniques, in particular Z, into the Open Distributed Processing (ODP) standardization initiative.", "num_citations": "24\n", "authors": ["1044"]}
{"title": "Quiescent consistency: Defining and verifying relaxed linearizability\n", "abstract": " Concurrent data structures like stacks, sets or queues need to be highly optimized to provide large degrees of parallelism with reduced contention. Linearizability, a key consistency condition for concurrent objects, sometimes limits the potential for optimization. Hence algorithm designers have started to build concurrent data structures that are not linearizable but only satisfy relaxed consistency requirements.             In this paper, we study quiescent consistency as proposed by Shavit and Herlihy, which is one such relaxed condition. More precisely, we give the first formal definition of quiescent consistency, investigate its relationship with linearizability, and provide a proof technique for it based on (coupled) simulations. We demonstrate our proof technique by verifying quiescent consistency of a (non-linearizable) FIFO queue built using a diffraction tree.", "num_citations": "23\n", "authors": ["1044"]}
{"title": "Design Support for Distributed Systems: DSE4DS\n", "abstract": " Distributed System design is a highly complicated and non-trivial task. The problem is characterized by the need to design multi-threaded, multi-processor, and multi-media systems. Design frameworks such as Open Distributed Processing (ODP), the ITU/ISO standard, provide significant, theoretical, guidance and aids towards the provision of successful distributed system designs. The purpose of the DSE4DS project is to develop these theoretical aids into practical design support that will assist designers in the construction of distributed system specifications.", "num_citations": "23\n", "authors": ["1044"]}
{"title": "Verifying linearizability on TSO architectures\n", "abstract": " Linearizability is the standard correctness criterion for fine-grained, non-atomic concurrent algorithms, and a variety of methods for verifying linearizability have been developed. However, most approaches assume a sequentially consistent memory model, which is not always realised in practice. In this paper we define linearizability on a weak memory model: the TSO (Total Store Order) memory model, which is implemented in the x86 multicore architecture. We also show how a simulation-based proof method can be adapted to verify linearizability for algorithms running on TSO architectures. We demonstrate our approach on a typical concurrent algorithm, spinlock, and prove it linearizable using our simulation-based approach. Previous approaches to proving linearizabilty on TSO architectures have required a modification to the algorithm\u2019s natural abstract specification. Our proof method is the first, to our\u00a0\u2026", "num_citations": "22\n", "authors": ["1044"]}
{"title": "Model transformations across views\n", "abstract": " Models of software often describe systems by a number of (partially) orthogonal views: a state machine, a class diagram, a scenario might specify different aspects of the one system to be built. Such abstract, multi-view models are the starting point for transformations into platform-specific models and finally the code. However, during these transformations it is usually not possible to keep such a neat separation into different views: the specification language of the target models might not support all such views. The target model, however, still needs to preserve the behaviour of the abstract, multi-view model. Therefore, model transformations have to be capable of moving aspects of the behaviour across views.In this paper, we study model transformations migrating aspects from state-based views (i.e., class specifications with data and methods) to protocol-based views (i.e., process specifications on orderings of\u00a0\u2026", "num_citations": "22\n", "authors": ["1044"]}
{"title": "Formal methods for distributed processing: a survey of object-oriented approaches\n", "abstract": " Issues in distributed systems/PF Linington/-Distributed systems, an ODP perspective/PF Linington/-Issues in formal methods/H. Bowman/-Finite state machine based: SDL/RO sinnott/-Process calculi: E-LOTOS/T. Robles/-State-based approaches: from Z to object-Z/G. Smith/-The unified modeling language/S. Kent/-Actors: a model for reasoning about open distributed systems/GA Agha/-Mobile ambients/L. Cardelli/-Subtyping in distributed systems/J. Indulska/-Behavioural subtyping using invariants and constraints/BH Liskou/-Behavioural typing for objects and process calculi/E. Najm/-Reflection in concurrent object-oriented languages/H. Masuhara/-Multimedia in the E-LOTOS process algebra/G. Leduc/-Specifying and analysing multimedia systems/L. Blair/-PICCOLA-a small composition language/F. Achermann/-Specification architectures/KJ Turner/-Viewpoints modelling/H. Bowman.", "num_citations": "22\n", "authors": ["1044"]}
{"title": "Z2SAL-building a model checker for Z\n", "abstract": " In this paper we discuss our progress towards building a model-checker for Z. The approach we take in our Z2SAL project involves implementing a translation from Z into the SAL input language, upon which the SAL toolset can be applied. The toolset includes a number of model-checkers together with a simulator. In this paper we discuss our progress towards implementing as complete as a translation as possible, the limitations we have reached and the optimizations we have made. We illustrate with a small example.", "num_citations": "21\n", "authors": ["1044"]}
{"title": "Mechanizing a correctness proof for a lock-free concurrent stack\n", "abstract": " Distributed algorithms are inherently complex to verify. In this paper we show how to verify that a concurrent lock-free implementation of a stack is correct by mechanizing the proof that it is linearizable, linearizability being a correctness notion for concurrent objects. Our approach consists of two parts: the first part is independent of the example and derives proof obligations local for one process which imply linearizabilty. The conditions establish a (special sort of non-atomic) refinement relationship between the specification and the concurrent implementation. These are used in the second part to verify the lock-free stack implementation. We use the specification language Z to describe the algorithms and the KIV theorem prover to mechanize the proof.", "num_citations": "21\n", "authors": ["1044"]}
{"title": "special issue on specification\u2010based testing\n", "abstract": " The significance of this problem can be seen by noting that software testing is a complex and expensive process. Therefore it is sensible to utilize any information that might help this process, whether it comes from the specification, the code or some other source. In addition, there are classes of fault, such as missing special cases, that are difficult to find by considering only the code. Consequently, the scope of this Special Issue was not limited to testing through executing the implementation under test (IUT). Instead, in the call for papers the scope was expanded to cover a rather more general notion of testing\u2014one that includes any process that is capable of detecting faults in an implementation. This includes both dynamic testing, in which the IUT is executed and the observed behaviour is compared with that expected, and static testing, which involves comparing the specification and the code without actually\u00a0\u2026", "num_citations": "21\n", "authors": ["1044"]}
{"title": "Viewpoints and consistency: translating LOTOS to Object-Z\n", "abstract": " This paper presents a translation between the formal description technique lotos and the object-oriented specification language Object-z. The need for such a translation lies in the use of formal methods in viewpoint specification, and in particular in the Open Distributed Processing standard. The use of viewpoints as a set of partial interlocking specifications brings an obligation to check the consistency of these partial specifications, and to do so we need to relate specifications written in differing languages. The work presented here aims to support the consistency checking of viewpoints written using formal methods by defining a translation from lotos to Object-z. A lotos specification describes both an ADT component and a behavioural model, the former is translated into the z type system, and the behavioural specification is translated into a collection of Object-z classes where we relate lotos actions to operations in\u00a0\u2026", "num_citations": "21\n", "authors": ["1044"]}
{"title": "Z2SAL: a translation-based model checker for Z\n", "abstract": " Despite being widely known and accepted in industry, the Z formal specification language has not so far been well supported by automated verification tools, mostly because of the challenges in handling the abstraction of the language. In this paper we discuss a novel approach to building a model-checker for Z, which involves implementing a translation from Z into SAL, the input language for the Symbolic Analysis Laboratory, a toolset which includes a number of model-checkers and a simulator. The Z2SAL translation deals with a number of important issues, including: mapping unbounded, abstract specifications into bounded, finite models amenable to a BDD-based symbolic checker; converting a non-constructive and piecemeal style of functional specification into a deterministic, automaton-based style of specification; and supporting the rich set-based vocabulary of the Z mathematical toolkit. This paper\u00a0\u2026", "num_citations": "20\n", "authors": ["1044"]}
{"title": "Verifying data refinements using a model checker\n", "abstract": " In this paper, we consider how refinements between state-based specifications (e.g., written in Z) can be checked by use of a model checker. Specifically, we are interested in the verification of downward and upward simulations which are the standard approach to verifying refinements in state-based notations. We show how downward and upward simulations can be checked using existing temporal logic model checkers.               In particular, we show how the branching time temporal logic CTL can be used to encode the standard simulation conditions. We do this for both a blocking, or guarded, interpretation of operations (often used when specifying reactive systems) as well as the more common non-blocking interpretation of operations used in many state-based specification languages (for modelling sequential systems). The approach is general enough to use with any state-based specification language\u00a0\u2026", "num_citations": "20\n", "authors": ["1044"]}
{"title": "The specification and testing of conformance in ODP systems\n", "abstract": " Open Distributed Processing (ODP) is a joint standardisation activity of the ISO and ITU. A reference model has been defined which describes an architecture for building open distributed systems. This paper introduces the key aspects of the reference model of open distributed processing, including the ODP conformance framework. We discuss how specific formal techniques are used in the ODP viewpoints, along with the implications for conformance assessment using such techniques. Particular attention is given to the role of consistency in the conformance assessment process. Finally, we review the current work on an ODP conformance testing methodology.", "num_citations": "19\n", "authors": ["1044"]}
{"title": "Using behaviour inference to optimise regression test sets\n", "abstract": " Where a software component is updated or replaced regression testing is required. Regression test sets can contain considerable redundancy. This is especially true in the case where no formal regression test set exists and the new component must instead be compared against patterns of behaviour derived from in-use log data from the previous version. Previous work has applied search-based techniques such as Genetic Algorithms to minimise test sets, but these relied on code coverage metrics to select test cases. Recent work has demonstrated the advantage of behaviour inference as a test adequacy metric. This paper presents a multi-objective search-based technique that uses behaviour inference as the fitness metric. The resulting test sets are evaluated using mutation testing and it is demonstrated that a considerably reduced test set can be found that retains all of the fault finding capability of the\u00a0\u2026", "num_citations": "17\n", "authors": ["1044"]}
{"title": "Verifying fault-tolerant erlang programs\n", "abstract": " In this paper we target the verification of fault tolerant aspects of distributed applications written in Erlang. Erlang is unusual in several respects. First, it is one of a few functional languages that is used in industry. Secondly the programming language contains support for concurrency and distribution as well as including constructs for handling fault-tolerance. Erlang programmers, of course, mostly work with ready-made language components. Our approach to verification of fault tolerance is to verify systems built using two central components of most Erlang software, a generic server component with fault tolerance handling, and a supervisor component that restarts failed processes. To verify Erlang programs built using these components we automatically translate them into processes of the \u03bcCRL process algebra, generate their state spaces, and use a model checker to determine whether they satisfy correctness\u00a0\u2026", "num_citations": "17\n", "authors": ["1044"]}
{"title": "Linear temporal logic and Z refinement\n", "abstract": " Since Z, being a state-based language, describes a system in terms of its state and potential state changes, it is natural to want to describe properties of a specified system also in terms of its state. One means of doing this is to use Linear Temporal Logic (LTL) in which properties about the state of a system over time can be captured. This, however, raises the question of whether these properties are preserved under refinement. Refinement is observation preserving and the state of a specified system is regarded as internal and, hence, non-observable.               In this paper, we investigate this issue by addressing the following questions. Given that a Z specification A is refined by a Z specification C, and that P is a temporal logic property which holds for A, what temporal logic property Q can we deduce holds for C? Furthermore, under what circumstances does the property Q preserve the intended meaning of\u00a0\u2026", "num_citations": "17\n", "authors": ["1044"]}
{"title": "Extending LOTOS with time: A true concurrency perspective\n", "abstract": " An ongoing restandardisation activity is currently extending the OSI specification language LOTOS with quantitative time. We give an alternative perspective on this activity. We highlight a very simple but expressive timed LOTOS enhancement which is based on time intervals. The main point at which we depart from the standard approach to extending LOTOS with time, is that we employ a true concurrency semantics. We present a semantics based on a time extended bundle event structures. We give a full semantics for relating our timed LOTOS to timed bundle event structures. A fixed point theory is defined and finally, we describe how urgency can be supported in the language.", "num_citations": "16\n", "authors": ["1044"]}
{"title": "Issues in multiparadigm viewpoint specification\n", "abstract": " This paper discusses the issues of specification style and refinement that arise in connection with viewpoint modelling. In particular, we consider the support needed in order to deal with viewpoints written at different levels of abstraction. The motivation for this work arises from the use of viewpoints in distributed systems design, in particular in the Open Distributed Processing standard.", "num_citations": "16\n", "authors": ["1044"]}
{"title": "Fractional permissions and non-deterministic evaluators in interval temporal logic\n", "abstract": " We propose Interval Temporal Logic as a basis for reasoning about concurrent programs with fine-grained atomicity due to the generality it provides over reasoning with standard pre/post-state relations. To simplify the semantics of parallel composition over intervals, we use fractional permissions, which allows one to ensure that conflicting reads and writes to a variable do not occur simultaneously. Using non-deterministic evaluators over intervals, we enable reasoning about the apparent states over an interval, which may differ from the actual states in the interval. The combination of Interval Temporal Logic, non-deterministic evaluators and fractional permissions results in a generic framework for reasoning about concurrent programs with fine-grained atomicity. We use our logic to develop rely/guarantee-style rules for decomposing a proof of a large system into proofs of its subcomponents, where fractional permissions are used to ensure that the behaviours of a program and its environment do not conflict.", "num_citations": "15\n", "authors": ["1044"]}
{"title": "Formal program development with approximations\n", "abstract": " We describe a method for combining formal program development with a disciplined and documented way of introducing realistic compromises, for example necessitated by resource bounds. Idealistic specifications are identified with the limits of sequences of more \u201crealistic\u201d specifications, and such sequences can then be refined in their entirety. Compromises amount to focusing the attention on a particular element of the sequence instead of the sequence as a whole.             This method addresses the problem that initial formal specifications can be abstract or complete but rarely both. Various potential application areas are sketched, some illustrated with examples. Key research issues are found in identifying metric spaces and properties that make them usable for refinement using approximations.", "num_citations": "15\n", "authors": ["1044"]}
{"title": "Making linearizability compositional for partially ordered executions\n", "abstract": " In the interleaving model of concurrency, where events are totally ordered, linearizability is compositional: the composition of two linearizable objects is guaranteed to be linearizable. However, linearizability is not compositional when events are only partially ordered, as in the weak-memory models that describe multicore memory systems. In this paper, we present a generalisation of linearizability for concurrent objects implemented in weak-memory models. We abstract from the details of specific memory models by defining our condition using Lamport\u2019s execution structures. We apply our condition to the C11 memory model, providing a correctness condition for C11 objects. We develop a proof method for verifying objects implemented in C11 and related models. Our method is an adaptation of simulation-based methods, but in contrast to other such methods, it does not require that the implementation\u00a0\u2026", "num_citations": "14\n", "authors": ["1044"]}
{"title": "Non-atomic refinement in Z and CSP\n", "abstract": " In this paper we discuss the relationship between notions of non-atomic (or action) refinement in a state-based setting with that in a behavioural setting. In particular, we show that the definition of non-atomic coupled downward simulation as defined for Z and Object-Z is sound with respect to an action refinement definition of CSP failures refinement.", "num_citations": "14\n", "authors": ["1044"]}
{"title": "Structural refinement of systems specified in Object-Z and CSP\n", "abstract": " This paper is concerned with methods for refinement of specifications written using a combination of Object-Z and CSP. Such a combination has proved to be a suitable vehicle for specifying complex systems which involve state and behaviour, and several proposals exist for integrating these two languages. The basis of the integration in this paper is a semantics of Object-Z classes identical to CSP processes. This allows classes specified in Object-Z to be combined using CSP operators. It has been shown that this semantic model allows state-based refinement relations to be used on the Object-Z components in an integrated Object-Z/CSP specification. However, the current refinement methodology does not allow the structure of a specification to be changed in a refinement, whereas a full methodology would, for example, allow concurrency to be introduced during the development life-cycle. In this paper\u00a0\u2026", "num_citations": "14\n", "authors": ["1044"]}
{"title": "Translating LOTOS to object-Z\n", "abstract": " This paper presents a translation between the formal description technique LOTOS and the object-oriented specification language Object-Z. The need for such a translation lies in the use of formal methods in viewpoint specification, and in particular in the Open Distributed Processing standard. The use of viewpoints as a set of partial interlocking specifications brings an obligation to check the consistency of these partial specifications, and to do so we need to relate specifications written in differing languages. The work presented here supports the consistency checking of viewpoints written using formal methods by defining a translation from full LOTOS to Object-Z. A common semantic model is provided for the languages, and we verify the translation with respect to this model. The translation is illustrated with a small example.", "num_citations": "14\n", "authors": ["1044"]}
{"title": "Proving opacity of a pessimistic STM\n", "abstract": " Transactional Memory (TM) is a high-level programming abstraction for concurrency control that provides programmers with the illusion of atomically executing blocks of code, called transactions. TMs come in two categories, optimistic and pessimistic, where in the latter transactions never abort. While this simplifies the programming model, high-performing pessimistic TMs can be complex. In this paper, we present the first formal verification of a pessimistic software TM algorithm, namely, an algorithm proposed by Matveev and Shavit. The correctness criterion used is opacity, formalising the transactional atomicity guarantees. We prove that this pessimistic TM is a refinement of an intermediate opaque I/O-automaton, known as TMS2. To this end, we develop a rely-guarantee approach for reducing the complexity of the proof. Proofs are mechanised in the interactive prover Isabelle.", "num_citations": "13\n", "authors": ["1044"]}
{"title": "Verifying opacity of a transactional mutex lock\n", "abstract": " Software transactional memory (STM) provides programmers with a high-level programming abstraction for synchronization of parallel processes, allowing blocks of codes that execute in an interleaved manner to be treated as an atomic block. This atomicity property is captured by a correctness criterion called opacity. Opacity relates histories of a sequential atomic specification with that of STM implementations.                 In this paper we prove opacity of a recently proposed STM implementation (a Transactional Mutex Lock) by Dalessandro et al.. The proof is carried out within the interactive verifier KIV and proceeds via the construction of an intermediate level in between sequential specification and implementation, leveraging existing proof techniques for linearizability.", "num_citations": "13\n", "authors": ["1044"]}
{"title": "Admit your weakness: Verifying correctness on TSO architectures\n", "abstract": " Linearizability has become the standard correctness criterion for fine-grained non-atomic concurrent algorithms, however, most approaches assume a sequentially consistent memory model, which is not always realised in practice. In this paper we study the correctness of concurrent algorithms on a weak memory model: the TSO (Total Store Order) memory model, which is commonly implemented by multicore architectures. Here, linearizability is often too strict, and hence, we prove a weaker criterion, quiescent consistency instead. Like linearizability, quiescent consistency is compositional making it an ideal correctness criterion in a component-based context. We demonstrate how to model a typical concurrent algorithm, seqlock, and prove it quiescent consistent using a simulation-based approach. Previous approaches to proving correctness on TSO architectures have been based on linearizabilty which\u00a0\u2026", "num_citations": "13\n", "authors": ["1044"]}
{"title": "Modelling divergence in relational concurrent refinement\n", "abstract": " An integration of state-based and behavioural formalisms can be obtained by imposing a concurrency semantics on a relational formalism. The data refinement theory for relational languages then provides a method for verifying the concurrent refinement relation. In this paper we investigate how divergence can be modelled relationally, and in particular show how differing process algebraic interpretations of divergence can be embedded in a relational framework. In doing so we derive relational simulation conditions for process algebraic refinement incorporating divergence.", "num_citations": "13\n", "authors": ["1044"]}
{"title": "Disjunction of LOTOS specifications\n", "abstract": " LOTOS is a formal specification language, designed for the precise description of open distributed systems and protocols. The definition of, so called, implementation relations has made it possible also to use LOTOS as a specification technique for the design of such systems. These LOTOS based specification techniques usually (ab)use non-determinism to achieve implementation freedom. Unfortunately, this is unsatisfactory when specifying non-deterministic processes. We, therefore, propose to extend LOTOS with a disjunction operator in order to achieve more implementation freedom while maintaining the possibility to describe non-deterministic processes. In contrast with similar proposals we maintain the operational semantics.", "num_citations": "13\n", "authors": ["1044"]}
{"title": "Verifying correctness of persistent concurrent data structures\n", "abstract": " Non-volatile memory (NVM), aka persistent memory, is a new paradigm for memory preserving its contents even after power loss. The expected ubiquity of NVM has stimulated interest in the design of persistent concurrent data structures, together with associated notions of correctness. In this paper, we present the first formal proof technique for durable linearizability, which is a correctness criterion that extends linearizability to handle crashes and recovery in the context of NVM. Our proofs are based on refinement of IO-automata representations of concurrent data structures. To this end, we develop a generic procedure for transforming any standard sequential data structure into a durable specification. Since the durable specification only exhibits durably linearizable behaviours, it serves as the abstract specification in our refinement proof. We exemplify our technique on a recently proposed persistent memory\u00a0\u2026", "num_citations": "12\n", "authors": ["1044"]}
{"title": "Integrated Formal Methods: 4th International Conference, IFM 2004, Canterbury, UK, April 4-7, 2004, Proceedings\n", "abstract": " The fourth conference in the series of international meetings on Integrated F-mal Methods, IFM, was held in Canterbury, UK, 4\u20137 April 2004. The conference was organized by the Computing Laboratory at the University of Kent, whose main campus is just outside the ancient town of Canterbury, part of the county of Kent. Kent is situated in the southeast of England, and the university sits on a hill overlooking the city of Canterbury and its world-renowned cathedral. The UniversityofKentwasgranteditsRoyalCharterin1965. Todaytherearealmost 10,000 full-time and part-time students, with over 110 nationalities represented. The IFM meetings have proven to be particularly successful. The? rst m-ting was held in York in 1999, and subsequently we held events in Germany in 2000, and then Finland in 2002. The conferences are held every 18 months or so, and attract a wide range of participants from Europe, the Americas, Asia and Australia. The conference is now? rmly part of the formal methods conference calendar. The conference has also evolved in terms of themes and subjects-presented, and this year, in line with the subject as a whole, we saw more work on veri? cation as some of the challenges in this subject are being met. The work reported at IFM conferences can be seen as part of the attempt to manage complexity by combining paradigms of speci? cation and design, so that the most appropriate design tools are used at di? erent points in the life-cycle.", "num_citations": "12\n", "authors": ["1044"]}
{"title": "Testing refinements by refining tests\n", "abstract": " One of the potential benefits of formal methods is that they offer the possibility of reducing the costs of testing. A specification acts as both the benchmark against which any implementation is tested, and also as the means by which tests are generated. There has therefore been interest in developing test generation techniques from formal specifications, and a number of different methods have been derived for state based languages such as Z, B and VDM. However, in addition to deriving tests from a formal specification, we might wish to refine the specification further before its implementation.               The purpose of this paper is to explore the relationship between testing and refinement. As our model for test generation we use a DNF partition analysis for operations written in Z, which produces a number of disjoint test cases for each operation. In this paper we discuss how the partition analysis of an\u00a0\u2026", "num_citations": "12\n", "authors": ["1044"]}
{"title": "Cross viewpoint consistency in Open Distributed Processing (intra language consistency)\n", "abstract": " This document contains the first deliverable of a research project on `Cross Viewpoint Consistency in Open Distributed Processing', carried out at the Computing Laboratory of the University of Kent. Open Distributed Processing (ODP) is recognised as an important area of standardisation activity. The ODP model seeks to provide a standardised architecture for building potentially global distributed systems with components from many vendors. Thus, ODP will realise the open systems ethos in the distributed systems domain. A central concept in ODP is that of a viewpoint. Distributed systems are viewed to be so complex that a process of separation of concerns must be employed when describing such systems. Viewpoints provide such a separation of concerns by presenting five distinct views of a single system; these are the enterprise viewpoint, information viewpoint, computational viewpoint, engineering viewpoint and technology viewpoint. It should be clear that in such viewpoint models it is essential that specifications in different viewpoinmts are related in order to determine whether the muyltiple specifications impose conflicting requirements. The project being reported here responds to this need by investigating how to check that multiple viewpoint specifications are in some sense ''consistent''. The objective of the project is to assess the feasibility of cross viewpoint consistency checking for specifications written in Z and LOTOS. We aim to develop prototype tools and techniques for consistency checking in and between these two specification languages. This deliverable describes the initial phase of the project which has focused on\u00a0\u2026", "num_citations": "12\n", "authors": ["1044"]}
{"title": "A high-level semantics for program execution under total store order memory\n", "abstract": " Processor cores within modern multicore systems often communicate via shared memory and use (local) store buffers to improve performance. A penalty for this improvement is the loss of Sequential Consistency to weaker memory guarantees that increase the number of possible program behaviours, and hence, require a greater amount of programming effort. This paper formalises the effect of Total Store Order (TSO) memory \u2014 a weak memory model that allows a write followed by a read in the program order to be reordered during execution. Although the precise effects of TSO are well-known, a high-level formalisation of programs that execute under TSO has not been developed. We present an interval-based semantics for programs that execute under TSO memory and include methods for fine-grained expression evaluation, capturing the non-determinism of both concurrency and TSO-related reorderings.", "num_citations": "11\n", "authors": ["1044"]}
{"title": "On using data abstractions for model checking refinements\n", "abstract": " In this paper we investigate how standard model checkers can be applied to checking refinement relationships between Z specifications. The major obstacle to such a use are the (potentially) infinite data domains in specifications. Consequently, we examine the application of data abstraction techniques for reducing the infinite to a finite state space. Since data abstractions do, however, decrease the amount of information in a specification, refinement can\u2014in general\u2014not be proven on the abstractions anymore, it can only be disproved. The model checker can thus be used to generate counter examples to a refinement relationship. Here, we show how abstract specifications can be systematically constructed (from a given data abstraction) and how a standard model checker (FDR) can be applied to find counter examples in case when refinement is absent. We especially discuss the applicability of the\u00a0\u2026", "num_citations": "11\n", "authors": ["1044"]}
{"title": "Model transformations incorporating multiple views\n", "abstract": " Model transformations are an integral part of OMG\u2019s standard for Model Driven Architecture. Model transformations are advocated to be behaviour preserving: platform specific models should adhere to platform independent descriptions developed in earlier design stages.               In this paper, we deal with models consisting of several views of a system. Often, in such a scenario, model transformations change just one view, and, although the overall transformation of all views is behaviour preserving, it is not behaviour preserving in isolation. To tackle this problem we develop a proof technique (and show its soundness) that allows one to consider just the view that has changed, and not the entire system. We focus specifically on one particular class of view-crossing transformations, namely on transformations conjunctively adding new constraints to a model.", "num_citations": "11\n", "authors": ["1044"]}
{"title": "A single complete refinement rule for Z\n", "abstract": " Data refinements is a well established technique for transforming specifications of abstract data types into ones which are closer to an eventual implementation. The conditions under which a transformation is a correct refinement can be encapsulated into two simulation rules: downward and upward simulations. These simulations are known to be sound and jointly complete for boundedly-nondeterministic specifications. In this note we derive a single complete refinement method and show how it may be formulated in Z, this is achieved by using possibility mappings. The use of possibility mappings themselves is not new, our aim here is to reformulate them for use within the Z specification language.", "num_citations": "11\n", "authors": ["1044"]}
{"title": "Consistency of partial process specifications\n", "abstract": " The structuring of the specification and development of distributed systems according to viewpoints, as advocated by the Reference Model for Open Distributed Processing, raises the question of when such viewpoint specifications may be considered consistent with one another. In this paper, we analyse the notion of consistency in the context of formal process specification. It turns out that different notions of correctness give rise to different consistency relations. Each notion of consistency is formally characterised and placed in a spectrum of consistency relations. An example illustrates the use of these relations for consistency checking.", "num_citations": "11\n", "authors": ["1044"]}
{"title": "Grey box data refinement\n", "abstract": " We introduce the concepts of grey box and display box data types. These make explicit the idea that state variables in abstract data types are not always hidden. Programming languages have visibility rules which make representations observable and modifiable. Specifications in model-based notations may have implicit assumptions about visible state components, or are used in contexts where the representation does matter. Grey box data types are like the ``standard'' black box data types, except that they contain explicit subspaces of the state which are modifiable and observable. Display boxes indirectly observe the state by adding displays to a black box. Refinement rules for both these alternative data types are given, based on their interpretations as black boxes.", "num_citations": "11\n", "authors": ["1044"]}
{"title": "Coupling schemas: data refinement and view (point) composition\n", "abstract": " We define the notion of a  coupling schema  in Z, and describe the role it plays in data refinement, view composition, and viewpoint unification. In each case coupling schemas relate several state schemas. In data refinement they occur as retrieve relations (abstraction schemas). In specification by views, coupling schemas provide a link between the various views. For viewpoint specification, coupling schemas are closely related to correspondence relations between state schemas in the viewpoints. Simple properties of coupling schemas (e.g. totality, functionality, and consistency) are shown to have important consequences in the techniques listed, and to be very useful for exhibiting the relations between these techniques. It turns out that views and viewpoints can both be seen as variations on, or even as generalisations of, data refinement.", "num_citations": "11\n", "authors": ["1044"]}
{"title": "Linearizability and causality\n", "abstract": " Most work on the verification of concurrent objects for shared memory assumes sequential consistency, but most multicore processors support only weak memory models that do not provide sequential consistency. Furthermore, most verification efforts focus on the linearizability of concurrent objects, but there are existing implementations optimized to run on weak memory models that are not linearizable.                 In this paper, we address these problems by introducing causal linearizability, a correctness condition for concurrent objects running on weak memory models. Like linearizability itself, causal linearizability enables concurrent objects to be composed, under weak constraints on the client\u2019s behaviour. We specify these constraints by introducing a notion of operation-race freedom, where programs that satisfy this property are guaranteed to behave as if their shared objects were in fact linearizable\u00a0\u2026", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Temporal-logic property preservation under Z refinement\n", "abstract": " Formal specification languages such as Z, B and VDM are used in the incremental development of abstract specifications (suitable for establishing required properties) to more concrete specifications (resembling the final implementation). This incremental development process, known as refinement, preserves all observable properties of the original abstract specification. Recent research has looked at applying temporal-logic model checking to such specification languages. While this assists in the establishment of properties of the abstract specification, temporal-logic properties typically refer to state variables which are regarded as non-observable. Hence, such properties are not guaranteed to be preserved by refinement. This paper investigates the classes of temporal-logic properties which are preserved by refinement, and for some of those properties that are not preserved in general, the restrictions on\u00a0\u2026", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Incompleteness of relational simulations in the blocking paradigm\n", "abstract": " Refinement is the notion of development between formal specifications. For specifications given in a relational formalism, downward and upward simulations are the standard method to verify that a refinement holds, their usefulness based upon their soundness and joint completeness. This is known to be true for total relational specifications and has been claimed to hold for partial relational specifications in both the non-blocking and blocking interpretations.In this paper we show that downward and upward simulations in the blocking interpretation, where domains are \u201cguards\u201d, are not jointly complete. This contradicts earlier claims in the literature. We illustrate this with an example (based on one recently constructed by Reeves and Streader) and then construct a proof to show why joint completeness fails in general.", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Model-checking Erlang\u2013a comparison between EtomCRL2 and McErlang\n", "abstract": " Model-checking programs is important in the development of a reliable software system. Two approaches might be applied to model-check a system at a source code level. One is to directly apply model-checking algorithm to the programming language; the other to abstract the program source codes into a formal specification, upon which some standard model-checkers can be used to verify system\u2019s properties. Both methods have recently been investigated for model-checking the functional programming language Erlang. Correspondingly, two Erlang model-checkers McErlang and Etomcrl2 are developed. This paper evaluates the two model-checkers by applying them to verify a a distributed and concurrent example - telecoms implemented in Erlang/OTP. A number of system\u2019s key properties are model-checked with both tool-sets. Advantages and disadvantages upon the uses of Etomcrl2 and McErlang\u00a0\u2026", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Design and verification of distributed multi-media systems\n", "abstract": " Performance analysis of computing systems, in particular distributed computing systems, is a complex process. Analysing the complex flows and interactions between a set of distributed processing nodes is a non-trivial task. The problem is exacerbated by the addition of continuous system functions that are time dependent, such as communication between components in the form of multimedia streams of video and audio data. Quality-of-Service (QoS) specifications define constraints on such communications and describe the required patterns of data transfer. By making use of these specifications as part of the performance analysis process it is possible to add significant confidence to predictions about the correct (required) operation of a distributed system. This paper presents a method for designing distributed multimedia systems, including the specification of QoS, using the ODP framework and UML and\u00a0\u2026", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Timed csp and object-z\n", "abstract": " In this paper we discuss a simple integration of timed CSP and Object-Z. Following existing work, the components in such an integration are written as either Object-Z classes, or timed CSP processes, and are combined together using CSP parallel composition.             Here we discuss the approach in general, and describe how the semantics of timed CSP can be used as the semantics of the integrated notation. We briefly discuss verification and analysis for integrated descriptions, before providing a more in-depth discussion of refinement in this approach. We describe both refinement of individual components, as well as a two-event model which distinguishes between start and end events. The latter allows operation duration to be specified and we show how refinement in this model integrates into traditional state-based simulation rules.", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Handling inconsistencies in z using quasi-classical logic\n", "abstract": " The aim of this paper is to discuss what formal support can be given to the process of living with inconsistencies in Z, rather than eradicating them. Logicians have developed a range of logics to continue to reason in the presence of inconsistencies. We present one representative of such paraconsistent logics, namely Hunter\u2019s quasi-classical logic, and apply it to the analysis of inconsistent Zsc hemas. In the presence of inconsistency quasi-classical logic allows us to derive less, but more \u201cuseful\u201d, information. Consequently, inconsistent Z specifications can be analysed in more depth than at present. Part of the analysis of a Zoperation is the calculation of the precondition. However, in the presence of an inconsistency, information about the intended application of the operation may be lost. It is our aim to regain this information. We introduce a new classification of precondition areas, based on the notions of\u00a0\u2026", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Liberating data refinement\n", "abstract": " Traditional rules for refinement of abstract data types suggest a software development process in which much of the detail has to be present already in the initial specification. In particular, the set of available operations and their interfaces need to be fixed. In contrast, many formal and informal software development methods rely on changes of granularity and require introduction of detail in a gradual way during the development process.             This paper discusses several generalisations and extensions of the traditional refinement rules, which are compatible with each other and, more importantly, with the semantic grounding of data refinement. Together they should provide a semantic justification for a larger spectrum of development steps.             The discussion takes place in the context of the formal specification language Z and its relational underpinnings.", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Stochastic specification and verification\n", "abstract": " Modern distributed systems include a class of applications in which non-functional requirements are important. In particular, these applications include multimedia facilities where real time constraints are crucial to their correct functioning. In order to specify such systems it is necessary to describe that events occur at times given by probability distributions. Stochastic process algebras have emerged as a useful technique by which such systems can be specified and verified. However, stochastic descriptions are very general, in particular they allow the use of general probability distribution functions, and therefore their verification can be complex. In this paper we define a translation from stochastic process algebras to timed automata. By doing so we aim to use the simpler verification methods for timed automata (e.g. reachability properties) for the more complex stochastic descriptions.", "num_citations": "10\n", "authors": ["1044"]}
{"title": "Defining correctness conditions for concurrent objects in multicore architectures\n", "abstract": " Correctness of concurrent objects is defined in terms of conditions that determine allowable relationships between histories of a concurrent object and those of the corresponding sequential object. Numerous correctness conditions have been proposed over the years, and more have been proposed recently as the algorithms implementing concurrent objects have been adapted to cope with multicore processors with relaxed memory architectures. We present a formal framework for defining correctness conditions for multicore architectures, covering both standard conditions for totally ordered memory and newer conditions for relaxed memory, which allows them to be expressed in uniform manner, simplifying comparison. Our framework distinguishes between order and commitment properties, which in turn enables a hierarchy of correctness conditions to be established. We consider the Total Store Order (TSO) memory model in detail, formalise known conditions for TSO using our framework, and develop sequentially consistent variations of these. We present a work-stealing deque for TSO memory that is not linearizable, but is correct with respect to these new conditions. Using our framework, we identify a new non-blocking compositional condition, fence consistency, which lies between known conditions for TSO, and aims to capture the intention of a programmer-specified fence.", "num_citations": "9\n", "authors": ["1044"]}
{"title": "A UML approach to the design of open distributed systems\n", "abstract": " The design of distributed systems is a highly complicated and non-trivial task. Introduction of multiple types of media into distributed systems causes a dramatic increase in the complexity of design. To deal with the inherent complexity of systems, two approaches have received considerable attention; ODP and UML. Open Distributed Processing (ODP) is a joint ITU/ISO standardisation framework for constructing distributed systems. Unified Modelling Language (UML) is a de facto standard for visualising, specifying, designing, and documenting object-oriented systems.               This paper presents a case study using a UML approach for the design and specification of distributed systems based on ODP. The purpose of the case study is to try this approach on a large system containing multiple types of media. The case study is carried out on an Interactive Multimedia Kiosk (IMK) example. IMKs integrate\u00a0\u2026", "num_citations": "9\n", "authors": ["1044"]}
{"title": "Combining component specifications in Object-Z and CSP\n", "abstract": " This paper discusses the separation of components from the contexts in which they are used, and how this separation can be supported whilst using different specification languages. There are a number of ways in which this might be possible and here we show how the technique of promotion in Object-Z can be used to combine components which are specified using process algebras. We outline two approaches. The first is to separate out a single specification into a number of distinct viewpoints (i.e., partial specifications), each possibly written in a different notation. These viewpoints can be developed separately, but combined if necessary by a process of translation and unification. The alternative approach we discuss is to use a single hybrid language which is composed of a combination of notations, which we illustrate here by combining CSP and Object-Z. We illustrate both approaches with a simple\u00a0\u2026", "num_citations": "9\n", "authors": ["1044"]}
{"title": "Refinement of objects and operations in Object-Z\n", "abstract": " In this paper we describe how we can refine both objects and operations in an Object-Z specification. In particular, we will be concerned with changes of granularity of both objects and operations. Objects in that we wish to change the structure of objects in a specification. Operations in that we wish to provide explicit support for action refinement in this language. There are clear advantages in being able to change such levels of granularity when performing a refinement. In this paper we discuss the issues surrounding such refinements and derive general rules to support their use. We illustrate our ideas by looking at a specification of a cash point machine at a bank.", "num_citations": "9\n", "authors": ["1044"]}
{"title": "Applying the UML to the ODP Enterprise Viewpoint\n", "abstract": " Now that the Reference Model for Open Distributed Processing (RM-ODP) has stabilised, attention is shifting towards the definition of specific notations for the ODP viewpoints. The objective of this paper is twofold. Firstly, we analyse the current definition of the ODP enterprise viewpoint language. Using the UML, a meta-model of the core concepts and their relationships is constructed. Secondly, we investigate to what extent the UML can be used for enterprise viewpoint specification by means of a small case study. We conclude by discussing the main open issues with regard to enterprise viewpoint specification.", "num_citations": "9\n", "authors": ["1044"]}
{"title": "Modelling distributed systems using Z\n", "abstract": " The ODP development model is a natural progression from OSI. Multiple viewpoints are used to specify complex ODP systems, Formal methods are playing an increasing role within ODP. There are two technical problems concerning the use of formal techniques within ODP which have yet to be addressed: these are unification and consistency checking. We show how Z can be used to provide a solution for both; and hence provide a mechanism for Z to be used properly in the ODP development process.", "num_citations": "9\n", "authors": ["1044"]}
{"title": "Using model checking to automatically find retrieve relations\n", "abstract": " Downward and upward simulations form a sound and jointly complete methodology for verifying relational data refinement in state-based specification languages such as Z and B. In previous work, we showed how both downward and upward simulation conditions can be discharged using a CTL model checker. The approach was implemented in the SAL tool suite. Given the retrieve relation, each of the simulation conditions can be proven fully automatically. It has been recognised, however, that finding retrieve relations is often very hard. In this paper, we show how it is feasible to use the SAL model checkers to also generate retrieve relations.", "num_citations": "8\n", "authors": ["1044"]}
{"title": "Model checking downward simulations\n", "abstract": " This paper shows how downward simulation can be checked using existing temporal logic model checkers. In particular, we show how the branching time temporal logic CTL can be used to encode the standard downward simulation conditions. We do this for both a blocking, or guarded, interpretation of operations (often used when specifying reactive systems) as well as the more common non-blocking interpretation of operations used in many state-based specification languages (for modelling sequential systems). The approach is general enough to use with any state-based specification language, and any CTL model checker in which the language can be encoded.", "num_citations": "8\n", "authors": ["1044"]}
{"title": "Formalising extended finite state machine transition merging\n", "abstract": " Model inference from system traces, e.g. for analysing legacy components or generating security tests for distributed components, is a common problem. Extended Finite State Machine (EFSM) models, managing an internal data state as a set of registers, are particularly well suited for capturing the behaviour of stateful components however existing inference techniques for (E)FSMs lack the ability to infer the internal state and its update functions.                 In this paper, we present the underpinning formalism for an EFSM inference technique that involves the merging of transitions with updates to the internal data state. Our model is formalised in Isabelle/HOL, allowing for the machine-checked validation of transition merges and system properties.", "num_citations": "7\n", "authors": ["1044"]}
{"title": "Deriving real-time action systems with multiple time bands using algebraic reasoning\n", "abstract": " The verify-while-develop paradigm allows one to incrementally develop programs from their specifications using a series of calculations against the remaining proof obligations. This paper presents a derivation method for real-time systems with realistic constraints on their behaviour. We develop a high-level interval-based logic that provides flexibility in an implementation, yet allows algebraic reasoning over multiple granularities and sampling multiple sensors with delay. The semantics of an action system is given in terms of interval predicates and algebraic operators to unify the logics for an action system and its properties, which in turn simplifies the calculations and derivations.", "num_citations": "7\n", "authors": ["1044"]}
{"title": "From ODP viewpoint consistency to integrated formal methods\n", "abstract": " Questions asked by research into ODP viewpoint consistency led to fundamental questions in refinement and contributed greatly to insights and interest in Integrated Formal Methods; research in those areas is still ongoing, while the answers provided remain largely unincorporated into model driven development.In this paper we survey some of the work done on consistency checking for multiple viewpoints, and subsequent work on generalised notions of refinement, which in turn led to work on integrations of state-based and behavioural formal methods.", "num_citations": "7\n", "authors": ["1044"]}
{"title": "Verifying Erlang Telecommunication Systems with the Process Algebra \u03bcCRL\n", "abstract": " Verification is an important process in the development of Erlang systems. A recent strand of work has studied the verification of Erlang applications using the process algebra \u03bcCRL. The general idea is that Erlang programs are translated into a \u03bcCRL specification, upon which the standard model checkers can be applied for checking the system\u2019s properties. In this paper, we pull together some of the existing work and investigate the verification of an Erlang telecommunication system in \u03bcCRL. This case study uses a server-client structure and incorporates timing restrictions and is designed and implemented using a number of Erlang/OTP components. We show how this system is translated into a \u03bcCRL specification by using the defined rules, after which system properties are checked via the toolset CADP. Through studying the verification of such an application, we aim to validate the effectiveness of the\u00a0\u2026", "num_citations": "7\n", "authors": ["1044"]}
{"title": "Verification of timed Erlang/OTP components using the process algebra \u03bcCRL\n", "abstract": " Recent work has looked at how Erlang programs could be model-checked via translation into the process algebra \u03bcCRL. Rules for translating Erlang programs and OTP components into \u03bcCRL have been defined and investigated. However, in the existing work, no rule is defined for the translation of timeout events into \u03bcCRL. This could degrade the usability of the existing work as in some real applications, timeout events play a significant role in the system development. In this paper, by extending the existing work, we investigate the verification of timed Erlang/OTP components in \u03bcCRL. By using an explicit tick action in the \u03bcCRL specification, a discrete-time timing model is defined to support the translation of timed Erlang functions into \u03bcCRL. Two small examples are presented, which demonstrates the applications of the proposed approach.", "num_citations": "7\n", "authors": ["1044"]}
{"title": "Eliminating overlapping of pattern matching when verifying Erlang programs in \u00b5CRL\n", "abstract": " When verifying Erlang programs in the process algebra \u00b5CRL specification, if there exists overlapping between patterns in the Erlang source codes, the problem of overlapping in pattern matching occurs when translating the Erlang codes into the \u00b5CRL specification. This paper investigates the problem and proposes an approach to overcome it. The proposed method rewrites an Erlang program with overlapping patterns into a counterpart program that has no overlapping patterns. Structure Splitting Trees (SSTs) are defined and applied for pattern evaluation. The use of SSTs guarantees that no overlapping patterns will be introduced into the rewritten Erlang code.", "num_citations": "7\n", "authors": ["1044"]}
{"title": "A viewpoints approach to designing group based applications\n", "abstract": " There is increasing interest in group-based applications for video distribution, multimedia conferencing, publish and subscribe etc. Such applications can use networks effectively by multicasting (supported traditionally at the network layer, but also now at the application layer). Designing such large-scale distributed systems is a complex task that can be aided by using a viewpoint approach to separate out different concerns, for example to separate object interaction from communications support. This paper extends earlier work on the design of distributed systems that use point-to-point communication to propose a framework within which viewpoints can be used to assist the design of complex applications involving groups and multicasting.", "num_citations": "7\n", "authors": ["1044"]}
{"title": "Constraint-oriented style for object-oriented formal specification\n", "abstract": " The authors propose a specification style which combines the features and advantages of object-oriented and constraint-oriented system decomposition. A system description is decomposed into data handling objects, which usually reflect objects and individual operations in the real system, and temporal-ordering constraints, which capture aspects of functionality as behavioural sequences, with a possibility to also introduce entities which blur the distinction between these two extreme cases. Composition is achieved via synchronisation on shared operations: different objects/constraints insisting on an operation express different views on the enabling conditions and effects of that operation. Objects, constraints, and their composition can be formally specified in Object-Z, an object-oriented extension of the Z notation, with pure temporal ordering constraints equivalently expressed as transition graphs. However\u00a0\u2026", "num_citations": "7\n", "authors": ["1044"]}
{"title": "A framework for correctness criteria on weak memory models\n", "abstract": " The implementation of weak (or relaxed) memory models is standard practice in modern multiprocessor hardware. For efficiency, these memory models allow operations to take effect in shared memory in a different order from that which they occur in a program. A number of correctness criteria have been proposed for concurrent objects operating on such memory models, each reflecting different constraints on the objects which can be proved correct. In this paper, we provide a framework in which correctness criteria are defined in terms of two components: the first defining the particular criterion (as it would be defined in the absence of a weak memory model), and the second defining the particular weak memory model. The framework facilitates the definition and comparison of correctness criteria, and encourages reuse of existing definitions. The latter enables properties of the criteria to be proved using\u00a0\u2026", "num_citations": "6\n", "authors": ["1044"]}
{"title": "Relational concurrent refinement part III: traces, partial relations and automata\n", "abstract": " Data refinement in a state-based language such as Z is defined using a relational model in terms of the behaviour of abstract programs. Downward and upward simulation conditions form a sound and jointly complete methodology to verify relational data refinements, which can be checked on an event-by-event basis rather than per trace. In models of concurrency, refinement is often defined in terms of sets of observations, which can include the events a system is prepared to accept or refuse, or depend on explicit properties of states and transitions. By embedding such concurrent semantics into a relational framework, eventwise verification methods for such refinement relations can be derived. In this paper, we continue our program of deriving simulation conditions for process algebraic refinement by defining further embeddings into our relational model: traces, completed traces, failure traces and\u00a0\u2026", "num_citations": "6\n", "authors": ["1044"]}
{"title": "Proving linearisability via coarse-grained abstraction\n", "abstract": " Linearisability has become the standard safety criterion for concurrent data structures ensuring that the effect of a concrete operation takes place after the execution some atomic statement (often referred to as the linearisation point). Identification of linearisation points is a non-trivial task and it is even possible for an operation to be linearised by the execution of other concurrent operations. This paper presents a method for verifying linearisability that does not require identification of linearisation points in the concrete code. Instead, we show that the concrete program is a refinement of some coarse-grained abstraction. The linearisation points in the abstraction are straightforward to identify and the linearisability proof itself is simpler due to the coarse granularity of its atomic statements. The concrete fine-grained program is a refinement of the coarse-grained program, and hence is also linearisable because every behaviour of the concrete program is a possible behaviour its abstraction.", "num_citations": "6\n", "authors": ["1044"]}
{"title": "More relational concurrent refinement: traces and partial relations\n", "abstract": " Data refinement in a state-based language such as Z is defined using a relational model in terms of the behaviour of abstract programs. Downward and upward simulation conditions form a sound and jointly complete methodology to verify relational data refinements. On the other hand, refinement in a process algebra takes a number of different forms depending on the exact notion of observation chosen, which can include the events a system is prepared to accept or refuse.In this paper we continue our program of deriving relational simulation conditions for process algebraic refinement by defining further embeddings into our relational model: traces, completed traces, failure traces and extension.", "num_citations": "6\n", "authors": ["1044"]}
{"title": "A case study in partial specification: consistency and refinement for Object-Z\n", "abstract": " The 'viewpoint' approach, in which a system is described by several partial specifications, has been proposed as a way of making complex computing systems more understandable. The ISO's Open Distributing Processing (ODP) framework is an architecture for open distributed systems, involving five named viewpoints. This paper compares two partial specifications of a lending library-from the ODP's Enterprise and Information Viewpoints-and discusses the relation between them. Both specifications are written in Object-Z, an object-oriented variant of Z. Examining how such partial specifications might be unified raises broader issues of refinement and mutual consistency of partial specifications in Object-Z.", "num_citations": "6\n", "authors": ["1044"]}
{"title": "A model checking algorithm for stochastic systems\n", "abstract": " In this report we present an algorithm for model checking stochastic automata. The algorithm, which is essentially based on discretising time, permits generalised distributions to be used.", "num_citations": "6\n", "authors": ["1044"]}
{"title": "Unification and multiple views of data in Z\n", "abstract": " This paper discusses the unification of Z specifications, in particular specifications that maintain different representations of what is intended to be the same datatype. Essentially this amounts to integrating previously published techniques for combining multiple viewpoints and for combining multiple views. It is shown how the technique proposed in this paper indeed produces unifications, and that it generalises both previous techniques.", "num_citations": "6\n", "authors": ["1044"]}
{"title": "A proof method for linearizability on TSO architectures\n", "abstract": " Linearizability is the standard correctness criterion for fine-grained non-atomic concurrent algorithms, and a variety of methods for verifying linearizability have been developed. However, most approaches to verifying linearizability assume a sequentially consistent memory model, which is not always realised in practice. In this chapter we study the use of linearizability on a weak memory model. Specifically we look at the TSO (Total Store Order) memory model, which is implemented in the x86 multicore architecture. A key component of the TSO architecture is the use of write buffers, which are used to store pending writes to memory. In this chapter, we explain how linearizability is defined on TSO, and how one can adapt a simulation-based proof method for use on TSO. Our central result is a proof method that simplifies simulation-based proofs of linearizability on TSO. The simplification involves constructing\u00a0\u2026", "num_citations": "5\n", "authors": ["1044"]}
{"title": "Simplifying proofs of linearisability using layers of abstraction\n", "abstract": " Linearisability has become the standard correctness criterion for concurrent data structures, ensuring that every history of invocations and responses of concurrent operations has a matching sequential history. Existing proofs of linearisability require one to identify so-called linearisation points within the operations under consideration, which are atomic statements whose execution causes the effect of an operation to be felt. However, identification of linearisation points is a non-trivial task, requiring a high degree of expertise. For sophisticated algorithms such as Heller et al's lazy set, it even is possible for an operation to be linearised by the concurrent execution of a statement outside the operation being verified. This paper proposes an alternative method for verifying linearisability that does not require identification of linearisation points. Instead, using an interval-based logic, we show that every behaviour of each concrete operation over any interval is a possible behaviour of a corresponding abstraction that executes with coarse-grained atomicity. This approach is applied to Heller et al's lazy set to show that verification of linearisability is possible without having to consider linearisation points within the program code.", "num_citations": "5\n", "authors": ["1044"]}
{"title": "Relational concurrent refinement with internal operations\n", "abstract": " Data refinement in a state-based language such as Z is defined using a relational model in terms of the input-output behaviour of abstract programs. Downward and upward simulations form a sound and jointly complete methodology for verifying relational data refinements.Refinement in a concurrent context, for example, as found in a process semantics, takes a number of different forms. Typically this is based on a notion of observation, for example, which events a system is prepared to accept or refuse. Concurrent refinement relations include trace refinement, failures-divergences refinement, readiness refinement and bisimulation.In this paper we survey recent results linking the relational model of refinement to the process algebraic models. Specifically, we detail how variations in the relational framework lead to relational data refinement being in correspondence with traces-divergences, singleton failures and\u00a0\u2026", "num_citations": "5\n", "authors": ["1044"]}
{"title": "Interpreting ODP viewpoint specification: Observations from a case study\n", "abstract": " Open Distributed Processing (ODP) is a viewpoints based ISO framework for specifying open distributed systems. This paper considers an application of ODP to the specification of an air traffic control (ATC) system. The key issues that arise from this are discussed further in the context of the formal specification of a simpler model \u2014 the Information Viewpoint in Z, and the Computational Viewpoint in Object-Z.", "num_citations": "5\n", "authors": ["1044"]}
{"title": "Towards a formal model of consistency in ODP\n", "abstract": " The ODP (Open Distributed Processing) development model is a natural progression from OSI. Multiple viewpoints are used to specify complex ODP systems. The conformance assessment methodology for ODP defines the relationships between specifications and implementations that must be proved in order that conformance to ODP systems can be asserted. These relationships include transformation, refinement, conformance and consistency. This paper develops a formal interpretation of these concepts. In particular the paper defines a framework for checking the consistency of different specifications of the same ODP system. This framework is essential if FDT's are to be successfully employed in the development of ODP systems. In the second part of the paper, we present examples of consistency checking in both LOTOS and RAISE.", "num_citations": "5\n", "authors": ["1044"]}
{"title": "Defining and verifying durable opacity: correctness for persistent software transactional memory\n", "abstract": " Non-volatile memory (NVM), aka persistent memory, is a new paradigm for memory that preserves its contents even after power loss. The expected ubiquity of NVM has stimulated interest in the design of novel concepts ensuring correctness of concurrent programming abstractions in the face of persistency. So far, this has lead to the design of a number of persistent concurrent data structures, built to satisfy an associated notion of correctness: durable linearizability.                 In this paper, we transfer the principle of durable concurrent correctness to the area of software transactional memory (STM). Software transactional memory algorithms allow for concurrent access to shared state. Like linearizability for concurrent data structures, opacity is the established notion of correctness for STMs. First, we provide a novel definition of durable opacity extending opacity to handle crashes and recovery in the context of\u00a0\u2026", "num_citations": "4\n", "authors": ["1044"]}
{"title": "Mechanized proofs of opacity: a comparison of two techniques\n", "abstract": " Software transactional memory (STM) provides programmers with a high-level programming abstraction for synchronization of parallel processes, allowing blocks of codes that execute in an interleaved manner to be treated as atomic blocks. This atomicity property is captured by a correctness criterion called opacity, which relates the behaviour of an STM implementation to those of a sequential atomic specification. In this paper, we prove opacity of a recently proposed STM implementation: the Transactional Mutex Lock (TML) by Dalessandro et\u00a0al. For this, we employ two different methods: the first method directly shows all histories of TML to be opaque (proof by induction), using a linearizability proof of TML as an assistance; the second method shows TML to be a refinement of an existing intermediate specification called TMS2 which is known to be opaque (proof by simulation). Both proofs are carried out\u00a0\u2026", "num_citations": "4\n", "authors": ["1044"]}
{"title": "Observational models for linearizability checking on weak memory models\n", "abstract": " Weak memory models are used to increase the performance of concurrent programs by allowing program instructions to be executed on the hardware in a different order to that specified by the software. This places a challenge on the verification of concurrent programs running on weak memory models since the variations in the executions need to be considered. Many approaches of modelling weak memory behaviour focus on architectural models to capture aspects of the hardware's architecture. In this paper, we investigate observational models of weak memory model behaviour which abstract from the underlying hardware architecture, and are instead derived from instruction reordering rules. This enables existing proof methods and tool support for linearizability to be reused. Specifically, we show how one existing proof method and associated model checking approach can be used to reason about programs\u00a0\u2026", "num_citations": "4\n", "authors": ["1044"]}
{"title": "Formal Methods for Open Object-based Distributed Systems: Volume 2\n", "abstract": " This book presents the leading edge in several related fields, specifically object orientated programming, open distributed systems and formal methods for object oriented systems. With increased support within industry regarding these areas, this book captures the most up-to-date information on the subject. Many topics are discussed, including the following important areas: object oriented design and programming; formal specification of distributed systems; open distributed platforms; types, interfaces and behaviour; formalisation of object oriented methods.", "num_citations": "4\n", "authors": ["1044"]}
{"title": "Automatic inference of erlang module behaviour\n", "abstract": " Previous work has shown the benefits of using grammar inference techniques to infer models of software behaviour for systems whose specifications are not available. However, this inference has required considerable direction from an expert user who needs to have familiarity with the system\u2019s operation, and must be actively involved in the inference process. This paper presents an approach that can be applied automatically to infer a model of the behaviour of Erlang modules with respect to their presented interface. It integrates the automated learning system StateChum with the automated refactoring tool Wrangler to allow both interface discovery and behaviour inference to proceed without human involvement.", "num_citations": "4\n", "authors": ["1044"]}
{"title": "Relational concurrent refinement: Automata\n", "abstract": " Data refinement in a state-based language such as Z is defined using a relational model in terms of the behaviour of abstract programs. Downward and upward simulation conditions form a sound and jointly complete methodology to verify relational data refinements. In models of concurrency, refinement takes a number of different forms depending on the exact notion of observation chosen, which can include the events a system is prepared to accept or refuse, or depend on explicit properties of states and transitions.In this paper we continue our program of deriving relational simulation conditions for behavioural notions of refinement by defining embeddings into the relational model that extend our framework to include various notions of automata based refinement.", "num_citations": "4\n", "authors": ["1044"]}
{"title": "Managing inconsistency and promoting consistency\n", "abstract": " In this paper we describe an approach to (in) consistency management in viewpoint specication using formal description techniques. Our development model consists of two phases. In the rst phase (\\inconsistency management\"), viewpoints are developed independently, and inconsistency is detected and reported back to the viewpoint speci ers by a central inconsistency manager. Once the inconsistency manager detects that all viewpoints are consistent, the second phase of\\consistency propagation\" is entered. In this phase, consistent viewpoint speci cations are combined (uni ed) and developed further in order to obtain an implementation. The consistency management and propagation techniques used in these two phases need to be syntactic in nature and structure preserving. This allows traceability of inconsistencies through precise localisation, and guarantees the possibility of propagation of consistency through compositionality. Formal description techniques provide the required syntax and much of the required compositionality. In addition, as a consequence of using di erent languages in the viewpoints, we need translations between formal description techniques. These should be structure preserving as well, for the same reasons.", "num_citations": "4\n", "authors": ["1044"]}
{"title": "Modularising verification of durable opacity\n", "abstract": " Non-volatile memory (NVM), also known as persistent memory, is an emerging paradigm for memory that preserves its contents even after power loss. NVM is widely expected to become ubiquitous, and hardware architectures are already providing support for NVM programming. This has stimulated interest in the design of novel concepts ensuring correctness of concurrent programming abstractions in the face of persistency and in the development of associated verification approaches. Software transactional memory (STM) is a key programming abstraction that supports concurrent access to shared state. In a fashion similar to linearizability as the correctness condition for concurrent data structures, there is an established notion of correctness for STMs known as opacity. We have recently proposed durable opacity as the natural extension of opacity to a setting with non-volatile memory. Together with this novel correctness condition, we designed a verification technique based on refinement. In this paper, we extend this work in two directions. First, we develop a durably opaque version of NOrec (no ownership records), an existing STM algorithm proven to be opaque. Second, we modularize our existing verification approach by separating the proof of durability of memory accesses from the proof of opacity. For NOrec, this allows us to re-use an existing opacity proof and complement it with a proof of the durability of accesses to shared state.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Choreography-based analysis of distributed message passing programs\n", "abstract": " We report on the analysis of gen_server, a popular Erlang library to build client-server applications. Our analysis uses a tool based on choreographic models. We discuss how, once the library has been modelled in terms of communicating finite state machines, an automated analysis can be used to detect potential communication errors. The results of our analysis suggest how to properly use gen_server in order to guarantee the absence of communication errors.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Using coarse-grained abstractions to verify linearizability on TSO architectures\n", "abstract": " Most approaches to verifying linearizability assume a sequentially consistent memory model, which is not always realised in practice. In this paper we study correctness on a weak memory model: the TSO (Total Store Order) memory model, which is implemented in x86 multicore architectures.             Our central result is a proof method that simplifies proofs of linearizability on TSO. This is necessary since the use of local buffers in TSO adds considerably to the verification overhead on top of the already subtle linearizability proofs. The proof method involves constructing a coarse-grained abstraction as an intermediate layer between an abstract description and the concurrent algorithm. This allows the linearizability proof to be split into two smaller components, where the effect of the local buffers in TSO is dealt with at a higher level of abstraction than it would have been otherwise.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Reasoning algebraically about refinement on TSO architectures\n", "abstract": " The Total Store Order memory model is widely implemented by modern multicore architectures such as x86, where local buffers are used for optimisation, allowing limited forms of instruction reordering. The presence of buffers and hardware-controlled buffer flushes increases the level of non-determinism from the level specified by a program, complicating the already difficult task of concurrent programming. This paper presents a new notion of refinement for weak memory models, based on the observation that pending writes to a process\u2019 local variables may be treated as if the effect of the update has already occurred in shared memory. We develop an interval-based model with algebraic rules for various programming constructs. In this framework, several decomposition rules for our new notion of refinement are developed. We apply our approach to verify the spinlock algorithm from the literature.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Data refinement for true concurrency\n", "abstract": " The majority of modern systems exhibit sophisticated concurrent behaviour, where several system components modify and observe the system state with fine-grained atomicity. Many systems (e.g., multi-core processors, real-time controllers) also exhibit truly concurrent behaviour, where multiple events can occur simultaneously. This paper presents data refinement defined in terms of an interval-based framework, which includes high-level operators that capture non-deterministic expression evaluation. By modifying the type of an interval, our theory may be specialised to cover data refinement of both discrete and continuous systems. We present an interval-based encoding of forward simulation, then prove that our forward simulation rule is sound with respect to our data refinement definition. A number of rules for decomposing forward simulation proofs over both sequential and parallel composition are developed.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Formally based tool support for model checking Erlang applications\n", "abstract": " Model checking as a verification technique has proved effective at the system design and hardware level, and is now beginning to be applied to program code. In this paper, we study the application of model checking techniques in the development of Erlang systems. Erlang is a concurrent functional language with specific support for the development of distributed, fault-tolerant systems with soft real-time requirements. It was designed from the start to support a concurrency-oriented programming paradigm and large distributed implementations that this supports. The methodology we describe in this paper consists of abstracting the behaviour of Erlang and OTP components into a process algebraic specification, specifically an mCRL2 specification, upon which the standard model checker CADP can be used to verify the system\u2019s properties. In addition to rules that model the Erlang syntax, a translation\u00a0\u2026", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Building a refinement checker for Z\n", "abstract": " In previous work we have described how refinements can be checked using a temporal logic based model-checker, and how we have built a model-checker for Z by providing a translation of Z into the SAL input language. In this paper we draw these two strands of work together and discuss how we have implemented refinement checking in our Z2SAL toolset. The net effect of this work is that the SAL toolset can be used to check refinements between Z specifications supplied as input files written in the LaTeX mark-up. Two examples are used to illustrate the approach and compare it with a manual translation and refinement check.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Filtering retrenchments into refinements\n", "abstract": " Retrenchment is a weakening of model based refinement that enables many development steps not expressible by refinement to be formally described nevertheless. The greater flexibility of retrenchment comes at the price of much feebler guarantees as compared with refinement, and so the interplay between retrenchment and refinement can hope to offer the best of both worlds. The paper explores the strategy of filtering the information in a retrenchment to yield a refinement under a suitable notion of observation. A general construction is given that enables a retrenchment, with its intrinsic notion of observability, to be filtered to produce a refinement with its intrinsic notion of observability. A simple running example illustrates the theory", "num_citations": "3\n", "authors": ["1044"]}
{"title": "FORSE: Formally-based tool support for Erlang development\n", "abstract": " Techniques and tools. Functional programming is a different paradigm from OO or procedural programming, but it shares a number of aspects with other paradigms. Principally, the control of complexity is an issue irrespective of the language involved, and work by staff and students at Kent has investigated software engineering aspects of functional programming. Projects have included design [35], software measurement and visualisation [33], and refactoring functional programs [32].", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Recent advances in refinement\n", "abstract": " In this paper we survey recent work on generalising refinement in a state-based setting. Such generalisations challenge a number of assumptions embedded in the standard formalisation of Refinement in a language such as Z, and lead to simulation conditions that allow one to verify a refinement in a number of different contexts.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "A Manual for a ModelChecker for Stochastic Automata\n", "abstract": " This technical report describes a modelchecker for Stochastic Automata, which was built based on the theory described in [BBD00]. The tool is available from: http://www.cs.ukc.ac.uk/people/staff/dha/index.html It accepts a stochastic automaton, a 'timed probabilistic until' formula pattern and a time step parameter. Note that we do not yet allow adversaries, a clock which guards two or more transitions is considered a (run-time) error. Also, we have not yet implemented the full range of the temporal logic in [BBD00]; only the 'timed probabilistic until' queries are allowed, and propositions must be atomic. The algorithm will return one of three results: true, false or undecided. If it returns true, then the automaton models the formula. If it returns false, then the automaton does not model the formula. If it returns undecided, then the algorithm was unable to determine whether the automaton models the formula.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Strategies for consistency checking, the choice of unification\n", "abstract": " There is increasing interest in models of system development which use multiple viewpoints. Each viewpoint offers a different perspective on the target system and system development involves parallel refinement of the multiple views. Our work particularly focuses on the use of viewpoints in Open Distributed Processing (ODP) which is an ISO/ITU standardisation framework. Multiple viewpoints, though, prompt the issue of consistency between viewpoints. This paper describes an interpretation of consistency which is general enough to meet the requirements of consistency in ODP. Furthermore, the paper investigates strategies for checking this consistency definition. Particular emphasis is placed on mechanisms to obtain global consistency (between an arbitrary number of viewpoints) from a series of binary consistency checks. The consistency checking strategies we develop are illustrated using the formal description technique LOTOS. Keywords: Viewpoints, Consistency, ODP, Formal Description Techniques, LOTOS.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Strategies for consistency checking\n", "abstract": " Viewpoint models of system development are becoming increasingly important. A major requirement for viewpoints modelling is to be able to check that the multiple viewpoint specifications are consistent with one another. The work presented in this report makes a contribution to this task. Our work is particularly influenced by the viewpoints model used in the ISO standardisation architecture for Open Distributed Processing. This report focuses on the issue of strategies for consistency checking. In particular, it considers how global consistency (between any arbitrary number of viewpoints) can be obtained from binary consistency (between two viewpoints). The report documents a number of different classes of consistency checking, from those that are very poorly behaved to those that are very well behaved. The report is intended as a companion to the work presented in [1] and it should be read in association with this document. In particular, the body of this report is a single chapter which should be viewed as additional to the chapters included in [1]. This report contains complete proofs of all relevant results, even though some of the results are obvious and some of the proofs are trivial. A much compressed version of the report is being submitted for publication. Thus, the main value of this report is as a reference document for readers who require a complete presentation of the technical. [1] E. Boiten, H. Bowman, J. Derrick and M. Steen ''Cross Viewpoint Consistency in Open Distributed Processing (Intra Language Consistency)'', Technical Report, Computing Laboratory, University of Kent at Canterbury, report No. 8-95, 1995. Phone: +44 1227\u00a0\u2026", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Modelling garbage collection algorithms\n", "abstract": " We show how abstract requirements of garbage collection can be captured using temporal logic. The temporal logic specification can then be used as a basis for process algebra specifications which can involve varying amounts of parallelism. We present two simple CCS specifications as an example, followed by a more complex specification of the cyclic reference counting algorithm. The verification of such algorithms is then briefly discussed.", "num_citations": "3\n", "authors": ["1044"]}
{"title": "Verifying correctness of persistent concurrent data structures: a sound and complete method\n", "abstract": " Non-volatile memory (NVM), aka persistent memory, is a new memory paradigm that preserves its contents even after power loss. The expected ubiquity of NVM has stimulated interest in the design of persistent concurrent data structures, together with associated notions of correctness. In this paper, we present a formal proof technique for durable linearizability, which is a correctness criterion that extends linearizability to handle crashes and recovery in the context ofNVM. Our proofs are based on refinement of Input/Output automata (IOA) representations of concurrent data structures. To this end, we develop a generic procedure for transforming any standard sequential data structure into a durable specification and prove that this transformation is both sound and complete. Since the durable specification only exhibits durably linearizable behaviours, it serves as the abstract specification in our refinement proof. We\u00a0\u2026", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Incorporating data into efsm inference\n", "abstract": " Models are an important way of understanding software systems. If they do not already exist, then we need to infer them from system behaviour. Most current approaches infer classical FSM models that do not consider data, thus limiting applicability. EFSMs provide a way to concisely model systems with an internal state but existing inference techniques either do not infer models which allow outputs to be computed from inputs, or rely heavily on comprehensive white-box traces to reveal the internal program state, which are often unavailable.                 In this paper, we present an approach for inferring EFSM models, including functions that modify the internal state. Our technique uses black-box traces which only contain information visible to an external observer of the system. We implemented our approach as a prototype.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "An observational approach to defining linearizability on weak memory models\n", "abstract": " In this paper we present a framework for defining linearizability on weak memory models. The purpose of the framework is to be able to define the correctness of concurrent algorithms in a uniform way across a variety of memory models. To do so linearizability is defined within the framework in terms of memory order as opposed to program order. Such a generalisation of the original definition of linearizability enables it to be applied to non-sequentially consistent architectures. It also allows the definition to be given in terms of observable effects rather than being dependent on an understanding of the weak memory model architecture. We illustrate the framework on the TSO (Total Store Order) weak memory model, and show that it respects existing definitions of linearizability on TSO.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Invariant generation for linearizability proofs\n", "abstract": " Linearizability is a widely recognised correctness criterion for concurrent objects. A number of proof methods for verifying linearizability exist. In this paper, we simplify one such method with a systematic approach for invariant generation. Although this existing refinement-based method is itself systematic and fully tool-supported, it requires the verifier to provide a specific invariant over the implementation. While a chosen invariant may suffice for some proof obligations of the method, it may not for others resulting in a new, stronger invariant to be chosen and the previously completed proof steps to be redone. Our approach avoids such wasted proof effort by generating an invariant which is guaranteed to be sufficient for all proof obligations.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Interval-based data refinement: A uniform approach to true concurrency in discrete and real-time systems\n", "abstract": " The majority of modern systems exhibit sophisticated concurrent behaviour, where several system components observe and modify the state with fine-grained atomicity. Many systems also exhibit truly concurrent behaviour, where multiple events may occur simultaneously. Data refinement, a correctness criterion to compare an abstract and a concrete implementation, normally admits interleaved models of execution only. In this paper, we present a method of data refinement using a framework that allows one to view a component's evolution over an interval of time, simplifying reasoning about true concurrency. By modifying the type of an interval, our theory may be specialised to cover data refinement of both discrete and real-time systems. We develop a sound interval-based forward simulation rule that enables decomposition of data refinement proofs, and apply this rule to verify data refinement for two examples: a\u00a0\u2026", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Experiences using Z2SAL\n", "abstract": " The Z notation is a language that can be used for writing formal specifications of a system since it is based on mathematical notation and logic. However, there is less tool support for this language that one might wish for. In this paper, Z2SAL, a translator for Z which translates the Z notation into a SAL input language, is explored. The generated SAL file can be used further by an existing model checker, specifically ones provided in the SAL tool suite. This paper describes experiences during conducting several experiments on the Z2SAL translator.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Relational concurrent refinement: timed refinement\n", "abstract": " Data refinement in a state-based language such as Z is defined using a relational model in terms of the behaviour of abstract programs. Downward and upward simulation conditions form a sound and jointly complete methodology to verify relational data refinements, which can be checked on an event-by-event basis rather than per trace. In models of concurrency, refinement is often defined in terms of sets of observations, which can include the events a system is prepared to accept or refuse, or depend on explicit properties of states and transitions. By embedding such concurrent semantics into a relational one, eventwise verification methods for such refinement relations can be derived. In this paper we continue our program of deriving simulation conditions for process algebraic refinement by considering how notions of time should be embedded into a relational model, and thereby deriving relational\u00a0\u2026", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Incrementally discovering testable specifications from program executions\n", "abstract": " The ProTest project is an EU FP7 project to develop techniques that improve the testing and verification of concurrent and distributed software systems. One of the four main work packages is concerned with the automated identification of specifications that could serve as a suitable basis for testing; this is currently a tedious and error-prone manual task that tends to be neglected in practice. This paper describes how this problem has been addressed in the ProTest project. It describes a technique that uses test executions to refine the specification from which they are generated. It shows how the technique has been implemented and applied to real Erlang systems. It also describes in detail the major challenges that remain to be addressed in future work.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Applying Testability Transformations to Achieve Structural Coverage of Erlang Programs\n", "abstract": " This paper studies the structural testing of Erlang applications. A program transformation is proposed that represents the program under test as a binary tree. The challenge of achieving structural coverage can thus be interpreted as a tree-search procedure. We have developed a testing-technique that takes advantage of this tree-structure, which we demonstrate with respect to a small case study of an Erlang telephony system.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Integration of specifications through development relations\n", "abstract": " We sketch a framework for viewpoint specification using formal specification languages. In order to establish consistency and to further develop specifications, specifications need to be integrated (\u201d unified\u201d). This integration is not defined in terms of their semantics, but more abstractly in terms of, so-called, development relations, which represent acceptable \u201cdevelopments\u201d(eg refinements) of each of the viewpoint specifications. The framework is motivated by its instantiations with a number of specification languages (eg, LOTOS and Z) and different development relations.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Viewpoint modelling\n", "abstract": " In this final chapter of this part, we continue with the theme of architecture by looking at an ODP viewpoints modelling case study. The purpose of the case study is to illustrate viewpoints and show how formal methods can be used in modelling and analysing them. In doing so we illustrate how different languages can be used in different viewpoints and introduce cross-viewpoint consistency and its checking. In looking at ODP viewpoints we draw heavily on the material introduce in Chap-ters 1 and 2 of this book, and in focussing on a case study we show how some of the languages introduced in Part Two can be used for viewpoints modelling. The particular case study that we are looking at is an IT system designed to keep track of a library collection and its borrowing process. We describe the enter-prise viewpoint in Section 20.4 using UML (see Chapter 7) together with a policy specification language that has a mapping into Object-Z (see Chapter 6). Section 20.5 extends this Object-Z description to an information viewpoint specification, and the computational and engineering viewpoints are modelled using E-LOTOS, which was introduced in Chapter 5. The consistency of viewpoints written in different languages is clearly an issue, and we begin this chapter with a discussion of the problem of consistency in general terms. Section 20.6. 2 then looks at a specific example of inconsistency in our case study.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Specification and analysis of automata-based designs\n", "abstract": " One of the results of research into formal system specification has been the large number of notations which have been developed. Of these notations, automata have emerged as a promising vehicle for the specification, and particularly the analysis, of systems. This is especially so when the systems under consideration include timing requirements, and timed automata model such systems as a finite set of states with timed transitions between them. However, not all specifications involve deterministic timing, and stochastic automata can be used in these circumstances.               In this paper we consider both timed and stochastic automata, and demonstrate how they can be used in the same design.We will also consider what analysis of the specification can then be performed. In particular, we will describe how to translate stochastic to timed automata, and look at two approaches to model checking the\u00a0\u2026", "num_citations": "2\n", "authors": ["1044"]}
{"title": "Specifying component and context specification using Promotion\n", "abstract": " In this paper we discuss how the specification of components may be separated from the description of the context in which they are used. There are a number of ways in which this might be possible and here we show how to use the technique of promotion in Object-Z to combine components which are specified using process algebras.             We discuss two approaches, the first is to separate out the specification into two distinct viewpoints written in different languages. These viewpoints axe then combined by a process of translation and unification. The second approach will be to use hybrid languages composed of a combination of CSP and Object-Z. We also consider how to refine such component based descriptions and consider issues of compositionality.", "num_citations": "2\n", "authors": ["1044"]}
{"title": "On strong observational refinement and forward simulation\n", "abstract": " Hyperproperties are correctness conditions for labelled transition systems that are more expressive than traditional trace properties, with particular relevance to security. Recently, Attiya and Enea studied a notion of strong observational refinement that preserves all hyperproperties. They analyse the correspondence between forward simulation and strong observational refinement in a setting with finite traces only. We study this correspondence in a setting with both finite and infinite traces. In particular, we show that forward simulation does not preserve hyperliveness properties in this setting. We extend the forward simulation proof obligation with a progress condition, and prove that this progressive forward simulation does imply strong observational refinement.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Inference of Extended Finite State Machines\n", "abstract": " In this AFP entry, we provide a formal implementation of a state-merging technique to infer extended finite state machines (EFSMs), complete with output and update functions, from black-box traces. In particular, we define the subsumption in context relation as a means of determining whether one transition is able to account for the behaviour of another. Building on this, we define the direct subsumption relation, which lifts the subsumption in context relation to EFSM level such that we can use it to determine whether it is safe to merge a given pair of transitions. Key proofs include the conditions necessary for subsumption to occur and that subsumption and direct subsumption are preorder relations. We also provide a number of different heuristics which can be used to abstract away concrete values into registers so that more states and transitions can be merged and provide proofs of the various conditions which must hold for these abstractions to subsume their ungeneralised counterparts. A Code Generator setup to create executable Scala code is also defined.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "A Formal Model of Extended Finite State Machines\n", "abstract": " In this AFP entry, we provide a formalisation of extended finite state machines (EFSMs) where models are represented as finite sets of transitions between states. EFSMs execute traces to produce observable outputs. We also define various simulation and equality metrics for EFSMs in terms of traces and prove their strengths in relation to each other. Another key contribution is a framework of function definitions such that LTL properties can be phrased over EFSMs. Finally, we provide a simple example case study in the form of a drinks machine.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Modelling concurrent objects running on the TSO and ARMv8 memory models\n", "abstract": " Hardware weak memory models, such as TSO and ARM, are used to increase the performance of concurrent programs by allowing program instructions to be executed on the hardware in a different order to that specified by the software. This places a challenge on the verification of concurrent objects used in these programs since the variations in the executions need to be considered.Many approaches exist for verifying concurrent objects along with associated tool support. In particular, we focus on a thread-local approach to checking linearizability, the standard correctness condition for concurrent objects, using a model checker. This approach, like most others, does not support weak memory models. In order to reuse this existing approach, therefore, we show how to use the semantics of a weak memory model to directly derive a transition system of concurrent objects running under it.We do this for both TSO and\u00a0\u2026", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Refinement: Semantics, Languages and Applications\n", "abstract": " Refinement is one of the cornerstones of a formal approach to software engineering. Refinement is all about turning an abstract description (of a soft or hardware system) into something closer to implementation. It provides that essential bridge between higher level requirements and an implementation of those requirements. This book provides a comprehensive introduction to refinement for the researcher or graduate student. It introduces refinement in different semantic models, and shows how refinement is defined and used within some of the major formal methods and languages in use today. It (1) introduces the reader to different ways of looking at refinement, relating refinement to observations (2) shows how these are realised in different semantic models (3) shows how different formal methods use different models of refinement, and (4) how these models of refinement are related.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Causal linearizability: Compositionality for partially ordered executions\n", "abstract": " In the interleaving model of concurrency, where events are totally ordered, linearizability is compositional: the composition of two linearizable objects is guaranteed to be linearizable. However, linearizability is not compositional when events are only partially ordered, as in many weak-memory models that describe multicore memory systems. In this paper, we present causal linearizability, a correctness condition for concurrent objects implemented in weak-memory models. We abstract from the details of specific memory models by defining our condition using Lamport's execution structures. We apply our condition to the C11 memory model, providing a correctness condition for C11 objects. We develop a proof method for verifying objects implemented in C11 and related models. Our method is an adaptation of simulation-based methods, but in contrast to other such methods, it does not require that the implementation totally order its events. We also show that causal linearizability reduces to linearizability in the totally ordered case.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "mu2: A Refactoring-Based Mutation Testing Framework for Erlang\n", "abstract": " We present a mutation testing framework for the Erlang functional programming language. Mutation testing evaluates a test set by mutating the original System Under Test (SUT) and measuring the test set\u2019s ability to detect the change. Designing mutation operators can be difficult, since they must modify the original program in a way that is both semantically significant, and a realistic simulation of a potential fault (either a fault with the system in its real context, or a common programmer error). The principal contribution of this work is the mu2 framework, which leverages the Wrangler refactoring API to allow users to specify their own mutation operators. The framework makes it possible to quickly and clearly define mutation operators that can have complex and subtle effects on the SUT. This allows users to define domain-specific operators that can simulate faults that are of particular relevance to their project\u00a0\u2026", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Smother: an MC/DC analysis tool for Erlang\n", "abstract": " This paper discusses an MC/DC analysis tool built for the Erlang programming language. Code coverage metrics are one way to measure the adequacy of a test suite, however, despite widespread industrial use in business-critical software, the only coverage metric readily available for Erlang is statement coverage, provided by the Cover tool that is distributed by the OTP standard libraries. An alternative to statement cover is Multiple Condition/Decision Coverage (MC/DC)--one of the most extensive coverage metrics, which is mandated by many software safety standards. Here we describe the application of MC/DC analysis to Erlang programs, and include an extension to traditional MC/DC analysis that applies the underlying philosophy to the pattern matching decision structures of functional languages. We have implemented the approach in the Smother tool that we also describe here as well as its use by our\u00a0\u2026", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Using Abstraction in Model Checking Z Specifications\n", "abstract": " The Z notation is a language used for writing formal specifications of a system. However, tool support for this language is lacking. One such tool that is not generally available is a model checker. Model checking is a method used to verify that a system has certain properties, this is important since it can provide full verification of a finite state system without the user having sophisticated knowledge. Originally applied in hardware systems, it is now commonly available for application in software systems. One of the drawbacks of model checking is that it applies to finite state systems, since it works by performing a complete state space exploration. However, the size of the systems that model checkers can now cope with has increased rapidly. In this paper, the use of abstraction as a means to make model checking feasible for arbitrary Z specifications is investigated. Several experiments have shown that the abstract models have fewer states than the concrete ones or have the same number of states as the concrete one.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "A relational framework for the integration of specifications\n", "abstract": " We describe a framework for viewpoint specification using formal specification languages. In order to establish consistency and to further develop specifications, specifications need to be integrated (\" unified\"). This integration is not defined in terms of their semantics, but more abstractly in terms of, so-called, development relations, which represent acceptable\" developments\"(eg, refinements) of each of the viewpoint specifications. The framework is motivated by its instantiations with a number of specification languages (eg, LOTOS and Z) and different development relations.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "ODP computational-to-information viewpoint mappings: a translation of CORBA IDL to Z\n", "abstract": " The reference model of Open Distributed Processing prescribes the use of a number of viewpoints (i.e. partial specifications). Specifications written in these viewpoints are likely to use different notations, e.g. the computational viewpoint is likely to include descriptions given in CORBA IDL, while the information viewpoint might well use a schema-based notation such as Z. To support such a specification scenario a translation from a subset of CORBA IDL to the Z specification notation is described, which has been implemented in a prototype translator based on the IDL parsing tool HaskellDirect. Although the main motivation is to integrate CORBA IDL into an existing multilanguage framework for viewpoint specification and consistency checking, the translation could also serve as the basis for a reverse translation from a subset of Z into IDL. In addition, it will help support a translation into Z from IDL specifications\u00a0\u2026", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Issues in formal methods (chapter 3)\n", "abstract": " The previous two chapters of this book discuss the construction of distributed systems and highlight some of the challenges that they pose. The central problem that these chapters leave is, of course, how to enhance reliability in the context of distribution. For those interested in a formal approach, this gives rise to a number of requirements on both the theoretical framework and particular engineering approaches. In this chapter we draw on some of the themes introduced in Chapters 1 and 2 to discuss implications on the use of formal methods for the specification of distributed systems.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Teaching Communication Protocols\n", "abstract": " Computer Science Education, as a speciality, is sometimes accused of simply addressing broad, low-level issues; of looking no further than \u201ccomputer literacy\u201d or \u201cintroductory programming\u201d. However, examining what we present in technical curriculum areas\u2014and how we present it\u2014is an important part of the computer science education endeavor. The papers collected together here represent some possible approaches to teaching in the area of communication protocols, and they especially emphasize the use of formal methods and techniques to describe their behavior. Most of these papers grew from presentations given at the Educational Case Studies in Protocols (ECASP) workshop held at the FORTE/PSTV\u201998 conference in Paris, November 1998. In this paper we provide background to the context in which these papers were written.Partly because of the specialized nature of the subject area, there has been\u00a0\u2026", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Stochastic Model Checking for Multimedia\n", "abstract": " Modern distributed systems include a class of applications in which non-functional requirements are important. In particular, these applications include multimedia facilities where real time constraints are crucial to their correct functioning. In order to specify such systems it is necessary to describe that events occur at times given by probability distributions and stochastic automata have emerged as a useful technique by which such systems can be specified and verified. However, stochastic descriptions are very general, in particular they allow the use of general probability distribution functions, and therefore their verification can be complex. In the last few years, model checking has emerged as a useful verification tool for large systems. In this paper we describe two model checking algorithms for stochastic automata. These algorithms consider how properties written in a simple probabilistic real-time logic can be checked against a given stochastic automaton.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Towards stochastic model checking with generalised distributions\n", "abstract": " Towards Stochastic Model Checking with Generalised Distributions - Kent Academic Repository Skip to main content kent-logo Logo Login Admin Dashboards Help Simple search | Advanced search Home Browse Latest additions Policies Accessibility Statement Towards Stochastic Model Checking with Generalised Distributions Bowman, Howard, Bryans, Jeremy W., Derrick, John (2000) Towards Stochastic Model Checking with Generalised Distributions. In: UKPEW 2000, 16th United Kingdom Performance Engineering Workshop. . (The full text of this publication is not currently available from this repository. You may be able to access a copy if URLs are provided) (KAR id:21943) The full text of this publication is not currently available from this repository. You may be able to access a copy if URLs are provided. (Contact us about this Publication) Item Type: Conference or workshop item (UNSPECIFIED) Uncontrolled \u2026", "num_citations": "1\n", "authors": ["1044"]}
{"title": "A formal framework for viewpoint consistency (full version)\n", "abstract": " Multiple Viewpoint models of system development are becoming increasingly important. Each viewpoint offers a different perspective on the target system and system development involves parallel refinement of the multiple views. Viewpoints related approaches have been considered in a number of different guises by a spectrum of researchers. Our work particularly focuses on the use of viewpoints in Open Distributed Processing (ODP) which is an ISO/ITU standardisation framework. The requirements of viewpoints modelling in ODP are very broad and, hence, demanding. Multiple viewpoints, though, prompt the issue of consistency between viewpoints. This paper describes a very general interpretation of consistency which we argue is broad enough to meet the requirements of consistency in ODP. We present a formal framework for this general interpretation; highlight basic properties of the interpretation and locate restricted classes of consistency. Strategies for checking consistency are also investigated. Throughout we illustrate our theory using the formal description technique LOTOS. Thus, the paper also characterises the nature of and options for consistency checking in LOTOS.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Formal specification and testing of a management architecture\n", "abstract": " The importance of network and distributed systems management to supply and maintain services required by users has led to a demand for management facilities. Open network management is assisted by representing the system resources to be managed as objects, and providing standard services and protocols for interrogating and manipulating these objects. This paper examines the application of formal description techniques to the specification of managed objects by presenting a case study in the specification and testing of a management architecture. We describe a formal specification of a management architecture suitable for scheduling and distributing services across nodes in a distributed system. In addition, we show how formal specifications can be used to generate conformance tests for the management architecture.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "A True Concurrency Semantics for Quality of Service Specification and Validation\n", "abstract": " This paper considers quality of service specification and validation for distributed multimedia systems. We present a new perspective on the LOTOS/QTL approach to such specification and validation. This approach has been previously presented in the context of a standard interleaving model of concurrency. In this paper we motivate the use of an alternative, truly concurrent, semantic model based on bundle event structures and integrate these semantics into the LOTOS/QTL approach. The applicability of the approach is illustrated with a specification of a multimedia stream.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "Modelling Garbage Collection Algorithms---Extend abstract\n", "abstract": " We show how abstract requirements of garbage collection can be captured using temporal logic. The temporal logic specification can then be used as a basis for process algebra specifications which can involve varying amounts of parallelism. We present two simple CCS specifications as an example, followed by a more complex specification of the cyclic reference counting algorithm. The verification of such algorithms is then briefly discussed.", "num_citations": "1\n", "authors": ["1044"]}
{"title": "An Investigation into the Use of Abstraction in Model Checking Z Specification\n", "abstract": " Z notation is a language used for writing formal specifications of a system. However, there is less tool support for this language. One such tool that is not generally available is a model checker. Model checking is a method used to verify that a system has certain properties, and has been found to be useful since it can provide full verification of a finite state system without the user having sophisticated knowledge. Originally applied in hardware systems, it is now commonly available for application in software systems. One of the drawbacks of model checking is that it applies to finite state systems, since it works by performing a complete state space exploration. However, the size of the systems that model checkers can now cope with has increased rapidly. In this paper, it is investigated the use of abstraction as a means to make model checking feasible for arbitrary Z specifications.", "num_citations": "1\n", "authors": ["1044"]}