{"title": "NII-UIT at MediaEval 2015 Affective Impact of Movies Task.\n", "abstract": " Affective Impact of Movies task aims to detect violent videos and affective impact on viewers of that videos [9]. This is a challenging task not only because of the diversity of video content but also due to the subjectiveness of human emotion. In this paper, we present a unified framework that can be applied to both subtasks:(i) induce affect detection, and (ii) violence detection. This framework is based on our previous year\u2019s Violent Scene Detection (VSD) framework. We extended it to support affect detection by training different valence/arousal classes independently and combine them to make the final decision. Besides using internal features from three different modalities: audio, image, and motion, in this year, we also incorporate deep learning features into our framework. Experimental results show that our unified framework can detect violent videos and its affective impact with a reasonable accuracy. Moreover, using deep features can significantly improve the detection performance of both subtasks.", "num_citations": "86\n", "authors": ["389"]}
{"title": "Evaluation of multiple features for violent scenes detection\n", "abstract": " Violent scenes detection (VSD) is a challenging problem because of the heterogeneous content, large variations in video quality, and complex semantic meanings of the concepts involved. In the last few years, combining multiple features from multi-modalities has proven to be an effective strategy for general multimedia event detection (MED), but the specific event detection like VSD has been comparatively less studied. Here, we evaluated the use of multiple features and their combination in a violent scenes detection system. We rigorously analyzed a set of low-level features and a deep learning feature that captures the appearance, color, texture, motion and audio in video. We also evaluated the utility of mid-level visual information obtained from detecting related violent concepts. Experiments were performed on the publicly available MediaEval VSD 2014 dataset. The results showed that visual and motion features are better than audio features. Moreover, the performance of the mid-level features was nearly as good as that of the low-level visual features. Experiments with a number of fusion methods showed that all single features are complementary", "num_citations": "29\n", "authors": ["389"]}
{"title": "NII-UIT-VBS: A video browsing tool for known item search\n", "abstract": " This paper introduces a video browsing tool for the known item search task. The key idea is to reduce the number of segments to further investigate by several ways such as applying visual filters and skimming representative keyframes. The user interface is optimally designed so as to reduce unnecessary navigations. Furthermore, a coarse-to-fine based approach is employed to quickly find the target clip.", "num_citations": "10\n", "authors": ["389"]}
{"title": "NII-UIT: A tool for known item search by sequential pattern filtering\n", "abstract": " This paper presents an interactive tool for searching a known item in a video or a video archive. To rapidly select the relevant segment, we use query patterns formulated by users for filtering. The patterns can be formulated by drawing color sketches or selecting predefined concepts. Especially, our tool support users to define patterns for sequences of consecutive segments, for instance, sequences of occurrences of concepts. Such patterns are called sequential patterns, which are more powerful to describe users\u2019 search intention. Besides that, the user interface is organized following a coarse-to-fine manner, so that users can quickly scan the set of candidate segments. By using color-based and concept-based filters, our tool can deal with both visual and descriptive known item search.", "num_citations": "7\n", "authors": ["389"]}
{"title": "Computational optimization for violent scenes detection\n", "abstract": " Violent scenes detection (VSD) can be considered as a specific problem of multimedia event detection. One popular approach to this problem is to employ multiple modals for presentation. By combining complementary modals, it has been shown remarkable improvement in accuracy. But, such an approach also requires high computational cost to process all features globally and locally extracted from static frames, video sequences, audio streams, or deep visual features. In this paper, we address the problem of modal selection (i.e. feature selection) when the computing resource (including both CPU and GPU) is limited. We evaluated possible combinations of features with different specifications of the computing resource. Evaluation results can be used to choose the optimal set of features for high accuracy regarding a pre-selected resource. We conducted experiments on the benchmark dataset MedialEval VSD\u00a0\u2026", "num_citations": "5\n", "authors": ["389"]}
{"title": "Evaluation of Low-Level features for detecting Violent Scenes in videos\n", "abstract": " Automatically detecting violent scenes in videos not only has great potential in several applications (such as movie selection or recommendation for children) but also is a very hot academic research topic. Since 2011, violent scene detection task is one of the core tasks of MediaEval, a benchmarking initiative dedicated to evaluating new algorithms for multimedia access and retrieval1. In this paper, we evaluate the performance of low-level audio/visual features for the violent scene detection task using the datasets and evaluation protocol provided by the MediaEval organizers. Our result report can be used as a baseline for comparison of new algorithms in this task.", "num_citations": "4\n", "authors": ["389"]}
{"title": "Violent scene detection using mid-level feature\n", "abstract": " Violent scene detection (VSD) refers to the task of detecting shots containing violent scenes in videos. With a wide range of promising real-world applications (eg movies/films inspection, video on demand, semantic video indexing and retrieval), VSD has been an important research problem. A typical approach for VSD is to learn a violent scene classifier and then apply it to video shots. Finding good feature representation for video shots is therefore essential to achieving high classification accuracy. It has been shown in recent work that using low-level features results in disappointing performance, since low-level features cannot convey high-level semantic information to represent violence concept. In this paper, we propose to use mid-level features to narrow the semantic gap between low-level features and violence concept. The mid-level features of a training (or test) video shots are formulated by concatenating\u00a0\u2026", "num_citations": "4\n", "authors": ["389"]}