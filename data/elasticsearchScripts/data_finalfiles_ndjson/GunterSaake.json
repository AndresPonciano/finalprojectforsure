{"title": "Datenbanken. Konzepte und Sprachen\n", "abstract": " Das Gebiet der Datenbanksysteme geh\u00f6rt zu den klassischen Ausbildungsgebieten der Informatikstudieng\u00e4nge. Datenbanksysteme kommen immer dann zum Einsatz, wenn an die Datenhaltung besondere Anforderungen hinsichtlich der Zuverl\u00e4ssigkeit, des zu speichernden Volumens, der Ausfallsicherheit, des Mehrbenutzerzugriffs, der Komplexit\u00e4t der Datenbeschreibung oder der Datenqualit\u00e4t gestellt werden. Zu Beginn des Informationszeitalters ist es daher nicht verwunderlich, dass der Umgang mit Datenbanksystemen f\u00fcr viele Absolventinnen und Absolventen der Informatikstudieng\u00e4nge zum Berufsalltag geh\u00f6rt.", "num_citations": "312\n", "authors": ["1503"]}
{"title": "Troll: A language for object-oriented specification of information systems\n", "abstract": " TROLL is a language particularly suited for the early stages of information system development, when the universe of discourse must be described. In TROLL the descriptions of the static and dynamic aspects of entities are integrated into object descriptions. Sublanguages for data terms, for first-order and temporal assertions, and for processes, are used to describe respectively the static properties, the behavior, and the evolution over time of objects. TROLL organizes system design through object-orientation and the support of abstractions such as classification, specialization, roles, and aggregation. Language features for state interactions and dependencies among components support the composition of the system from smaller modules, as does the facility of defining interfaces on top of object descriptions.", "num_citations": "184\n", "authors": ["1503"]}
{"title": "Monitoring dynamic integrity constraints based on temporal logic\n", "abstract": " Dynamic integrity constraints are used to specify admissible sequences of database states. We present algorithmic fundamentals of monitoring constraints expressed in temporal logic. The essential means are finite transition graphs which can be constructed from temporal formulae by utilizing an appropriate normalform. To ensure admissibility of a state sequence, the integrity monitor has to follow a corresponding path through the graph and to check certain nontemporal conditions in each state; these conditions are provided as edge labels. Thus, monitoring dynamic integrity is reduced to a controlled checking of static integrity. All errors in present database behaviour are detected as well as many inevitable future errors.", "num_citations": "160\n", "authors": ["1503"]}
{"title": "Object-oriented specification of information systems: The TROLL language\n", "abstract": " In this report we present the language TROLL. It is a language particularly suited to be used in the early stages of information system design where the problem domain or Universe of Discourse must be described. In TROLL, the description of the static and dynamic aspects of entities is integrated in object descriptions. TROLL is based on sublanguages for data terms, for rst-order and temporal assertions and for processes. These sublanguages are the tools to describe static properties, behavior and evolution over time of objects. Object descriptions can be related in many ways. The abstraction mechanisms provided by TROLL are classi cation, specialization, generalization, roles, and aggregation. Abstraction mechanisms together with the basic structuring in objects help in organizing the system design.In order to support the composition of system descriptions from component description TROLL provides language features to state interactions and dependencies between components when put into the global context of the system. Furthermore, interfaces that can be de ned on top of object descriptions help in modularizing the system design and make cooperative and distributed design possible All language features of TROLL are explained in detail. We de ne a formal syntax of TROLL and give some ideas towards the semantics of TROLL language constructs.", "num_citations": "137\n", "authors": ["1503"]}
{"title": "Challenges and perspectives of metaproteomic data analysis\n", "abstract": " In nature microorganisms live in complex microbial communities. Comprehensive taxonomic and functional knowledge about microbial communities supports medical and technical application such as fecal diagnostics as well as operation of biogas plants or waste water treatment plants. Furthermore, microbial communities are crucial for the global carbon and nitrogen cycle in soil and in the ocean. Among the methods available for investigation of microbial communities, metaproteomics can approximate the activity of microorganisms by investigating the protein content of a sample. Although metaproteomics is a very powerful method, issues within the bioinformatic evaluation impede its success. In particular, construction of databases for protein identification, grouping of redundant proteins as well as taxonomic and functional annotation pose big challenges. Furthermore, growing amounts of data within a\u00a0\u2026", "num_citations": "133\n", "authors": ["1503"]}
{"title": "Report on the dagstuhl seminar \u201cData Quality on the Web\u201d\n", "abstract": " Over the past few years, techniques for managing, querying, and integrating data on the Web have significantly matured. Well-founded and practical approaches to assess or even guarantee a required degree of quality of the data in these frameworks, however, are still missing. This can be contributed to the lack of welldefined data quality metrics and assessment techniques, and the difficulty of handling information about data quality during data integration and query processing. Data quality problems arise in many settings, such as the integration of business data, in Web mining, data dissemination, and in querying the Web using search engines. Data quality (DQ) addresses various forms of data, including structured and semistructured data, text documents, multimedia, and streaming data. Different forms of metadata describing the quality of data is becoming increasingly important since they provide applications\u00a0\u2026", "num_citations": "118\n", "authors": ["1503"]}
{"title": "Evolving Logical Specification in Information Systems\n", "abstract": " Traditional logic-based specification approaches fix the structure and the dynamics of an object system at specification time. Information systems are applications with a very long life-time. Therefore, object and specification evolution is needed to react to changing requirements. Hence, this is a relevant aspect of describing information systems as object societies.             We present a logical specification framework for evolving objects. Our framework is based on the concepts of object developed for the languages Troll and Gnome and the underlying temporal logic OSL. The syntactic notion of object descriptions is extended to explicitly manipulate temporal axioms during behaviour evolution. An extension of OSL called dyOSL establishes a logical framework where basic temporal formulae are evaluated by a second logical layer. dyOSL allows to explicitly manipulate state-dependent sets of temporal formulae\u00a0\u2026", "num_citations": "114\n", "authors": ["1503"]}
{"title": "Logics for databases and information systems\n", "abstract": " Time is ubiquitous in information systems. Almost every enterprise faces the problem of its data becoming out of date. However, such data is often valu able, so it should be archived and some means to access it should be provided. Also, some data may be inherently historical, eg, medical, cadastral, or ju dicial records. Temporal databases provide a uniform and systematic way of dealing with historical data. Many languages have been proposed for tem poral databases, among others temporal logic. Temporal logic combines ab stract, formal semantics with the amenability to efficient implementation. This chapter shows how temporal logic can be used in temporal database applica tions. Rather than presenting new results, we report on recent developments and survey the field in a systematic way using a unified formal framework [GHR94; Ch094]. The handbook [GHR94] is a comprehensive reference on mathematical foundations of temporal logic. In this chapter we study how temporal logic is used as a query and integrity constraint language. Consequently, model-theoretic notions, particularly for mula satisfaction, are of primary interest. Axiomatic systems and proof meth ods for temporal logic [GHR94] have found so far relatively few applications in the context of information systems. Moreover, one needs to bear in mind that for the standard linearly-ordered time domains temporal logic is not re cursively axiomatizable [GHR94]'so recursive axiomatizations are by necessity incomplete.", "num_citations": "101\n", "authors": ["1503"]}
{"title": "GPU-accelerated Database Systems: Survey and Open Challenges\n", "abstract": " The vast amount of processing power and memory bandwidth provided by modern graphics cards make them an interesting platform for data-intensive applications. Unsurprisingly, the database research community identified GPUs as effective co-processors for data processing several years ago. In the past years, there were many approaches to make use of GPUs at different levels of a database system. In this paper, we explore the design space of GPU-accelerated database management systems. Based on this survey, we present key properties, important trade-offs and typical challenges of GPU-aware database architectures, and identify major open challenges. Additionally, we survey existing GPU-accelerated DBMSs and classify their architectural properties. Then, we summarize typical optimizations implemented in GPU-accelerated DBMSs. Finally, we propose a reference architecture, indicating how\u00a0\u2026", "num_citations": "93\n", "authors": ["1503"]}
{"title": "XML data clustering: An overview\n", "abstract": " In the last few years we have observed a proliferation of approaches for clustering XML documents and schemas based on their structure and content. The presence of such a huge amount of approaches is due to the different applications requiring the clustering of XML data. These applications need data in the form of similar contents, tags, paths, structures, and semantics. In this article, we first outline the application contexts in which clustering is useful, then we survey approaches so far proposed relying on the abstract representation of data (instances or schema), on the identified similarity measure, and on the clustering algorithm. In this presentation, we aim to draw a taxonomy in which the current approaches can be classified and compared. We aim at introducing an integrated view that is useful when comparing XML data clustering approaches, when developing a new clustering algorithm, and when\u00a0\u2026", "num_citations": "86\n", "authors": ["1503"]}
{"title": "Adding conflict resolution features to a query language for database federations\n", "abstract": " A main problem of data integration is the treatment of conflicts caused by different modeling of realworld entities, different data models or simply by different representations of one and the same object. During the integration phase these conflicts have to be identified and resolved as part of the mapping between local and global schemata. Therefore, conflict resolution affects the definition of the integrated view as well as query transformation and evaluation, in this paper we present a SQL extension for defining and querying database federations. This language addresses in particular the resolution of integration conflicts by providing mechanisms for mapping attributes, restructuring relations as well as extended integration operations. Finally, the application of these resolution strategies is briefly explained by presenting a simple conflict resolution method.", "num_citations": "86\n", "authors": ["1503"]}
{"title": "Tool support for featureoriented software development: featureIDE: an Eclipse-based approach\n", "abstract": " Software program families have a long tradition and will gain momentum in the future. Today's research tries to move software development to a new quality of industrial production. Several solutions concerning different phases of the software development process have been proposed in order to cope with different problems of program family development. A major problem of program family engineering is still the missing tool support. The vision is an IDE that brings all phases of the development process together consistently and in a user-friendly manner. This paper focuses on AHEAD, a prominent design methodology and architectural model for feature-based program families. We present our first results on developing an Eclipse-based IDE that supports building program families following the AHEAD architecture model. Starting from current weaknesses and pitfalls in implementing program families we outline\u00a0\u2026", "num_citations": "85\n", "authors": ["1503"]}
{"title": "Element similarity measures in XML schema matching\n", "abstract": " Schema matching plays a central role in a myriad of XML-based applications. There has been a growing need for developing high-performance matching systems in order to identify and discover semantic correspondences across XML data. XML schema matching methods face several challenges in the form of definition, adoption, utilization, and combination of element similarity measures. In this paper, we classify, review, and experimentally compare major methods of element similarity measures and their combinations. We aim at presenting a unified view which is useful when developing a new element similarity measure, when implementing an XML schema matching component, when using an XML schema matching system, and when comparing XML schema matching systems.", "num_citations": "82\n", "authors": ["1503"]}
{"title": "An enhanced density based spatial clustering of applications with noise\n", "abstract": " DBSCAN is a pioneer density based clustering algorithm. It can find out the clusters of different shapes and sizes from the large amount of data which is containing noise and outliers. But the clusters detected by it contain large amount of density variation within them. It can not handle the local density variation that exists within the cluster. For good clustering a significant density variation may be allowed within the cluster because if we go for homogeneous clustering, a large number of smaller unimportant clusters may be generated. In this paper we propose an Enhanced DBSCAN algorithm which keeps track of local density variation within the cluster. It calculates the density variance for any core object with respect to its e -neighborhood. If density variance of a core object is less than or equal to a threshold value and also satisfying the homogeneity index with respect to its e -neighborhood then it will allow the core\u00a0\u2026", "num_citations": "82\n", "authors": ["1503"]}
{"title": "Merging inheritance hierarchies for database integration\n", "abstract": " Merging inheritance hierarchies with overlapping class extensions and types is an essential task in database design. In the context of view integration and schema integration for federated databases and multidatabases conflicting inheritance hierarchies have to be merged. Inheritance hierarchies often occur explicitly in object-oriented databases as well as implicitly in relational databases. Since a concept lattice can be regarded as an inheritance hierarchy we propose to apply the theory of concept analysis to the problem of merging inheritance hierarchies. After investigating the power and complexity of concept analysis algorithms we provide a new algorithm tailored to our problem. The new algorithm has polynomial complexity and helps to optimize the resulting hierarchy with respect to certain quality criteria, e.g. number of classes and null values. An example demonstrates the practicability of our approach to\u00a0\u2026", "num_citations": "77\n", "authors": ["1503"]}
{"title": "Efficient similarity-based operations for data integration\n", "abstract": " Dealing with discrepancies in data is still a big challenge in data integration systems. The problem occurs both during eliminating duplicates from semantic overlapping sources as well as during combining complementary data from different sources. Though using SQL operations like grouping and join seems to be a viable way, they fail if the attribute values of the potential duplicates or related tuples are not equal but only similar by certain criteria. As a solution to this problem, we present in this paper similarity-based variants of grouping and join operators. The extended grouping operator produces groups of similar tuples, the extended join combines tuples satisfying a given similarity condition. We describe the semantics of this operator, discuss efficient implementations for the edit distance similarity and present evaluation results. Finally, we give examples of application from the context of a data reconciliation\u00a0\u2026", "num_citations": "76\n", "authors": ["1503"]}
{"title": "Efficient co-processor utilization in database query processing\n", "abstract": " Specialized processing units such as GPUs or FPGAs provide great opportunities to speed up database operations by exploiting parallelism and relieving the CPU. However, distributing a workload on suitable (co-)processors is a challenging task, because of the heterogeneous nature of a hybrid processor/co-processor system. In this paper, we present a framework that automatically learns and adapts execution models for arbitrary algorithms on any (co-)processor. Our physical optimizer uses the execution models to distribute a workload of database operators on available (co-)processing devices. We demonstrate its applicability for two common use cases in modern database systems. Additionally, we contribute an overview of GPU-co-processing approaches, an in-depth discussion of our framework's operator model, the required steps for deploying our framework in practice and the support of complex\u00a0\u2026", "num_citations": "72\n", "authors": ["1503"]}
{"title": "Revised version of the modelling language TROLL\n", "abstract": " CiteSeerX \u2014 Revised Version of the Modelling Language TROLL Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Revised Version of the Modelling Language TROLL (1994) Cached Download as a PDF Download Links [wwwiti.cs.uni-magdeburg.de] [wwwiti.cs.uni-magdeburg.de] [wwwiti.cs.uni-magdeburg.de] [wwwiti.cs.uni-magdeburg.de] [wwwiti.cs.uni-magdeburg.de] Save to List Add to Collection Correct Errors Monitor Changes by Thorsten Hartmann , Gunter Saake , Ralf Jungclaus , Peter Hartel , Jan Kusch Citations: 3 - 0 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases modelling language troll Powered by: Apache Solr About CiteSeerX Submit and \u2026", "num_citations": "70\n", "authors": ["1503"]}
{"title": "Descriptive specification of database object behaviour\n", "abstract": " Traditional database design methodologies are not appropriate for the specific requirements of object-oriented database systems and new database application areas. Apart from semantic complications arising from object-oriented database structures with complex objects, arbitrary data types as attribute domains, or generalization hierarchies, specification and semantics of dynamic database behaviour has to be of main interest for typical object-oriented applications, too. We propose the use of a temporal logic as a specification language for dynamic object behaviour and point out the formal semantics of such database dynamics specifications. A layered conceptual database design methodology is presented together with a discussion on design support techniques for behaviour specifications. Finally, implementation aspects are treated.", "num_citations": "70\n", "authors": ["1503"]}
{"title": "Improving XML schema matching performance using Pr\u00fcfer sequences\n", "abstract": " Schema matching is a critical step for discovering semantic correspondences among elements in many data-shared applications. Most of existing schema matching algorithms produce scores between schema elements resulting in discovering only simple matches. Such results partially solve the problem. Identifying and discovering complex matches is considered one of the biggest obstacle towards completely solving the schema matching problem. Another obstacle is the scalability of matching algorithms on large number and large-scale schemas. To tackle these challenges, in this paper, we propose a new XML schema matching framework based on the use of Pr\u00fcfer encoding. In particular, we develop and implement the XPr\u00fcM system, which consists mainly of two parts\u2014schema preparation and schema matching. First, we parse XML schemas and represent them internally as schema trees. Pr\u00fcfer sequences\u00a0\u2026", "num_citations": "67\n", "authors": ["1503"]}
{"title": "Algorithmen und Datenstrukturen: Eine Einf\u00fchrung mit Java\n", "abstract": " Algorithmen und Datenstrukturen von Grund auf verstehen Kenntnisse von Algorithmen und Datenstrukturen sind ein Grundbaustein des Studiums der Informatik und verwandter Fachrichtungen. Das Buch behandelt diese Thematik in Verbindung mit der Programmiersprache Java und schl\u00e4gt so eine Br\u00fccke zwischen den klassischen Lehrb\u00fcchern zur Theorie von Algorithmen und Datenstrukturen und den praktischen Einf\u00fchrungen in eine konkrete Programmiersprache. Die konkreten Algorithmen und deren Realisierung in Java werdenumfassend dargestellt. Daneben werden die theoretischen Grundlagen vermittelt, die in Programmiersprachen-Kursen oft zu kurz kommen: abstrakte Maschinenmodelle, Berechenbarkeit, Algorithmenparadigmen sowie parallele und verteilte Abl\u00e4ufe. Einen weiteren Schwerpunkt bilden Datenstrukturen wie Listen, B\u00e4ume, Graphen und Hashtabellen sowie deren objektorientierte Implementierung mit modernen Methoden der Softwareentwicklung. Die 6. Auflage f\u00fchrt einige neue Algorithmen ein und ber\u00fccksichtigt die Neuerungen der aktuellen Java-Versionen, ua zu Themen wie Parallelisierung.", "num_citations": "62\n", "authors": ["1503"]}
{"title": "Extensible and similarity-based grouping for data integration\n", "abstract": " Data integration as required in a variety of applications like data warehousing, information system integration etc. makes great demands regarding features to deal with overlapping and inconsistent data. Object-relational and other data management systems available today provide only limited concepts to deal with these requirements. The general concept of grouping and aggregation appears to be a fitting paradigm for various of the current issues in data integration, but in its common form of equality-based grouping a number of problems remain unsolved. Various extensions to this concept have been introduced over the last years regarding user-defined functions for aggregation and grouping. Especially, existing extensions to the grouping operation like simple derivations of group-by values do not meet the requirements of data integration applications. We propose generic interfaces for user-defined grouping and aggregation as part of a SQL extension, allowing for more complex functions, for instance integration of data mining algorithms. Furthermore, we discuss high-level language primitives for common applications and illustrate the approach by introducing new concepts for similarity-based duplicate detection and elimination. For both approaches implementation and optimization issues are considered.", "num_citations": "60\n", "authors": ["1503"]}
{"title": "Advanced grouping and aggregation for data integration\n", "abstract": " New applications from the areas of analytical data processing and data integration require powerful features to condense and reconcile available data. As outlined in [1], the general concept of grouping and aggregation appears to be a fitting paradigm for a number of these issues, but in its common form of equality based groups or with current extensions like simple user-defined functions to derive group-by values on a per tuple basis and restricted aggregate functions a number of problems remain unsolved. We describe two extensions to the grouping mechanism, a generic one to support holistic user-defined grouping functions and higher level construct that provides similarity based grouping suitable in a number of applications like duplicate detection and elimination.", "num_citations": "60\n", "authors": ["1503"]}
{"title": "Analyzing data quality issues in research information systems via data profiling\n", "abstract": " The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their\u00a0\u2026", "num_citations": "59\n", "authors": ["1503"]}
{"title": "Formal specification of object systems\n", "abstract": " The conceptual modeling of the Universe of Discourse (UoD) is an important phase for the development of information systems because the conceptual model is the basis for system development. Conceptual model specifications must be formal in order to be precise and unambiguous and to support consistency and completeness checks. The object-oriented paradigm is suitable for providing an integrated formal description of all relevant static and dynamic aspects of the UoD structured in objects. In this paper we introduce a formal concept of object suitable to represent the UoD by a collection of concurrent interacting objects. The Oblog               +-language for object-oriented UoD-specification based on this concept supports the integrated description of data about objects, the development of objects through time and of various relationships between objects taking into account static and dynamic aspects of\u00a0\u2026", "num_citations": "59\n", "authors": ["1503"]}
{"title": "JavAdaptor\u2014Flexible runtime updates of Java applications\n", "abstract": " Software is changed frequently during its life cycle. New requirements come, and bugs must be fixed. To update an application, it usually must be stopped, patched, and restarted. This causes time periods of unavailability, which is always a problem for highly available applications. Even for the development of complex applications, restarts to test new program parts can be time consuming and annoying. Thus, we aim at dynamic software updates to update programs at runtime. There is a large body of research on dynamic software updates, but so far, existing approaches have shortcomings either in terms of flexibility or performance. In addition, some of them depend on specific runtime environments and dictate the program's architecture. We present JavAdaptor, the first runtime update approach based on Java that (a) offers flexible dynamic software updates, (b) is platform independent, (c) introduces only minimal\u00a0\u2026", "num_citations": "56\n", "authors": ["1503"]}
{"title": "Measuring Non-Functional Properties in Software Product Line for Product Derivation\n", "abstract": " A software product line (SPL) enables stakeholders to derive different software products for a domain while providing a high degree of reuse of their code units. Software products are derived in a configuration process by composing different code units. The configuration process becomes complex if SPLs contain hundreds of features. In many cases, a stakeholder is not only interested in functional but also in non-functional properties of a desired product. Because SPLs can be used in different application scenarios alternative implementations of already existing functionality are developed to meet special non-functional requirements, like restricted binary size and performance guarantees. To enable these complex configurations we discuss and present techniques to measure non-functional properties of software modules and use these values to compute SPL configurations optimized to the users needs.", "num_citations": "52\n", "authors": ["1503"]}
{"title": "Integration of inheritance trees as part of view generation for database federations\n", "abstract": " Schema integration is the basis for successfully building a database federation. Current proposals directly integrate the different schemata using a semantical powerful data model, for example an extended ER model or an object-oriented model. We propose instead to use for integration a semantical poor model and build semantically rich representations as external views only. This approach enables a flexible integration and derivation especially for inheritance trees. We present an algorithm enabling the derivation of different inheritance trees as external views onto an integrated schema. The resulting inheritance tree can be influenced by giving priorities to input classes or attribute combinations and satisfies several formalized quality criteria for external views.", "num_citations": "50\n", "authors": ["1503"]}
{"title": "Data quality measures and data cleansing for research information systems\n", "abstract": " The collection, transfer and integration of research information into different research Information systems can result in different data errors that can have a variety of negative effects on data quality. In order to detect errors at an early stage and treat them efficiently, it is necessary to determine the clean-up measures and the new techniques of data cleansing for quality improvement in research institutions. Thereby an adequate and reliable basis for decision-making using an RIS is provided, and confidence in a given dataset increased. In this paper, possible measures and the new techniques of data cleansing for improving and increasing the data quality in research information systems will be presented and how these are to be applied to the Research information.", "num_citations": "48\n", "authors": ["1503"]}
{"title": "Merkle Hash Tree based Techniques for Data Integrity of Outsourced Data\n", "abstract": " One of the problems associated with outsourcing data to cloud service providers is the data integrity of outsourced data. In this paper we present data integrity techniques for the outsourced data. Data integrity encompasses the completeness, correctness and freshness of the data. This paper focuses on the Merkle Hash Tree based data integrity techniques. It also presents the techniques for storage and retrieval of Merkle Hash Tree based authentication data to and from cloud data service provider. Critical analysis of the Radix Path Identifiers, a technique for storage of Merkle Hash Trees in the databases, is presented in this paper.", "num_citations": "47\n", "authors": ["1503"]}
{"title": "Aggregation in a behavior oriented object model\n", "abstract": " TROLL is a language to specify information systems with dynamic behavior. Here, we elaborate on the specification of object aggregation in TROLL. We distinguish between two kinds of aggregation, static and dynamic aggregation. Static aggregation means that the composition of objects is described using predicates over constant properties. Dynamic aggregation means that we may alter the composition of objects by invoking special operations (events) that are implicitly defined for each dynamic complex object. Additionally, we describe the specification of disjoint complex as a means for structuring a specification. We introduce language features to describe object aggregation and give some hints towards their semantics.", "num_citations": "47\n", "authors": ["1503"]}
{"title": "Metaproteome analysis reveals that syntrophy, competition, and phage-host interaction shape microbial communities in biogas plants\n", "abstract": " In biogas plants, complex microbial communities produce methane and carbon dioxide by anaerobic digestion of biomass. For the characterization of the microbial functional networks, samples of 11 reactors were analyzed using a high-resolution metaproteomics pipeline. Examined methanogenesis archaeal communities were either mixotrophic or strictly hydrogenotrophic in syntrophy with bacterial acetate oxidizers. Mapping of identified metaproteins with process steps described by the Anaerobic Digestion Model 1 confirmed its main assumptions and also proposed some extensions such as syntrophic acetate oxidation or fermentation of alcohols. Results indicate that the microbial communities were shaped by syntrophy as well as competition and phage-host interactions causing cell lysis. For the families Bacillaceae, Enterobacteriaceae, and Clostridiaceae, the number of phages exceeded up to 20-fold the\u00a0\u2026", "num_citations": "46\n", "authors": ["1503"]}
{"title": "An Extension of BPMN Meta-model for Evaluation of Business Processes\n", "abstract": " Business process modeling is used for better understanding and communication of company\u2019s processes. Mostly, business process modeling is discussed from the information system development perspective. Execution of a business process involves various factors (costs and time) which are important and should be represented in business process models. Controlling of business units uses post execution analysis for detection of failures for improvement. The process models conceived for information system development are not sufficient for post execution analysis. This paper focuses on the challenges of business process modeling in the post execution context. We provide a meta model for evaluation of a business process and discuss BPMN in this context. We also extend existing BPMN meta model for performance analysis of business processes. The proposed extensions are presented with the help of an example.", "num_citations": "46\n", "authors": ["1503"]}
{"title": "Using finite-linear temporal logic for specifying database dynamics\n", "abstract": " The specification of a database system consists of the description of its static information structure as well as of its dynamic behaviour. Whereas in classic conceptual database design the main interest was on the static part, specification of database dynamics became an important topic in the last few years.The specification of dynamic database behaviour has to describe the correct evolutions of the stored information. This can be done in terms of allowed user actions, which update database states, or in a descriptive manner by specifying the correct sequences of database states. Here, we concentrate on the latter aspect and present a corresponding logical specification calculus.", "num_citations": "45\n", "authors": ["1503"]}
{"title": "Combining Feature-Oriented and Aspect-Oriented Programming to Support Software Evolution.\n", "abstract": " Starting from the advantages of using Feature-Oriented Programming (FOP) and program families to support software evolution, this paper discusses the drawbacks of current FOP techniques. In particular we address the insufficient crosscutting modularity that complicates software evolution. To overcome this tension we propose the integration of concepts of Aspect-Oriented Programming (AOP) into existing FOP solutions. As study object we utilize FeatureC++, a proprietary extension to C++ that supports FOP. After a short introduction to basic language features of FeatureC++, we summarize the problems regarding the crosscutting modularity. In doing so, we point to the strengths of AOP that can help. Thereupon, we introduce three approaches that combine FOP and AOP concepts: Multi Mixins, Aspectual Mixins, and Aspectual Mixin Layers. Furthermore, we discuss their benefits for software evolution.", "num_citations": "44\n", "authors": ["1503"]}
{"title": "Integrated Product Line Model for Semi-Automated Product Derivation Using Non-Functional Properties.\n", "abstract": " Software product lines (SPLs) allow to generate tailormade software products by selecting and composing reusable code units. However, SPLs with hundreds of features and millions of possible products require an appropriate support for semi-automated product derivation. We envision this derivation to be extended by non-functional properties that are associated to code units and domain features. Code units and domain features are commonly organized in different models and connected via complex mappings, what make automation difficult. We propose a model that integrates features and code units in order to allow semi-automated product derivation using non-functional properties.", "num_citations": "42\n", "authors": ["1503"]}
{"title": "Application modelling in heterogeneous environments using an object specification language\n", "abstract": " We propose an object-oriented logical formalism to conceptionally model applications in an interoperable environment. Such an environment consists of heterogeneous and autonomous local (database) systems. Applications in such an environment use several resources and services. Their conceptual modelling involves re-specification of existing systems in terms of homogeneous views, modelling of behavior and system dynamics, modelling of logically distributed components in an open environment and the modelling of communication relationships and dependencies between components. We introduce a formal object-oriented language capable of dealing with these requirements and illustrate its use to model applications in an interoperable environment.", "num_citations": "42\n", "authors": ["1503"]}
{"title": "A feature-based personalized recommender system for product-line configuration\n", "abstract": " Today\u2019s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we\u00a0\u2026", "num_citations": "41\n", "authors": ["1503"]}
{"title": "An efficient k-means with good initial starting points\n", "abstract": " The k-means algorithm is one of the most widely used methods to partition a dataset into groups of patterns. However, the k-means method converges to one of many local minima. And it is known that, the final result depends on the initial starting points (means). We introduce an efficient method to start the k-means with good starting points (means). The good initial starting points allow the k-means algorithm to converge to a \u201cbetter\u201d local minimum, also the number of iteration over the full dataset is decreased. Our experimental results show that, good initial starting points lead to improved solution.", "num_citations": "40\n", "authors": ["1503"]}
{"title": "Toward Hardware-Sensitive Database Operations\n", "abstract": " Satisfying the performance needs of tomorrow typically implies using modern processor capabilities (such as single instruction, multiple data) and co-processors (such as graphics processing units) to accelerate database operations. Algorithms are typically hand-tuned to the underlying (co-) processors. This solution is error-prone, introduces high implementation and maintenance cost and one implementations is not portable to other (co-) processors. To this end, we argue for a combination of database research with modern software-engineering approaches. We emphasize our vision of generating optimized database algorithms tailored to used (co-) processors from a common code base. With this, we maximize performance while minimizing implementation and maintenance effort of hardware-tailored database operations.", "num_citations": "38\n", "authors": ["1503"]}
{"title": "Datenbanken & Java\n", "abstract": " Datenbanken & Java Page 1 Datenbankstammtisch Dresden, 27. November 2002 Datenbanken & Java Gunter Saake Institut f\u00fcr Technische und Betriebliche Informationssysteme Otto-von-Guericke-Universit\u00e4t Magdeburg saake@iti.cs.uni-magdeburg.de Page 2 1. \u00dcberblick Datenbanken & Java 2 Gunter Saake Motivation Java und DB-Anwendungsentwicklung JDBC Embedded SQL: SQLJ Java im DB-Server Java und OO-Datenbanken Objektrelationale Abbildung Ausblick Page 3 2. Motivation Datenbanken & Java 3 Gunter Saake Datenbanken: Basis vieler Anwendungssysteme Internet-Anwendungen, E-Commerce Betriebliche Informationssysteme Java als Plattform f\u00fcr Entwicklung solcher Anwendungen Objektorientierung Plattformunabh\u00e4ngigkeit Vielzahl von verf\u00fcgbaren Bibliotheken Datenbankzugriff mit Java Page 4 DB-Anwendungsentwicklung Datenbanken & Java 4 Gunter Saake Kopplung von SQL \u2026", "num_citations": "38\n", "authors": ["1503"]}
{"title": "K-means for spherical clusters with large variance in sizes\n", "abstract": " Data clustering is an important data exploration technique with many applications in data mining. The k-means algorithm is well known for its efficiency in clustering large data sets. However, this algorithm is suitable for spherical shaped clusters of similar sizes and densities. The quality of the resulting clusters decreases when the data set contains spherical shaped with large variance in sizes. In this paper, we introduce a competent procedure to overcome this problem. The proposed method is based on shifting the center of the large cluster toward the small cluster, and recomputing the membership of small cluster points, the experimental results reveal that the proposed algorithm produces satisfactory results.", "num_citations": "37\n", "authors": ["1503"]}
{"title": "Theoretical foundations of handling large substitution sets in temporal integrity monitoring\n", "abstract": " Temporal integrity constraints describe long-term data dependencies in databases to be satisfied by each correct database evolution. They can be formulated in a temporal logic. For a runtime monitoring of temporal integrity the problem arises to handle the historical information necessary to monitor the long-term dependencies. This paper extends the already known techniques for minimizing the stored information for a single substitution of the free constraint variables using transition graph construction. Our extension allows to decrease also the amount of handled substitutions for constraint monitoring. For this purpose, the notion of substitution descriptions is formally introduced allowing to monitor simultaneously whole substitution sets. The notions of formula validity and of stepwise monitoring potential validity of temporal constraints are redefined for descriptions. Based on these notions an algorithm for\u00a0\u2026", "num_citations": "37\n", "authors": ["1503"]}
{"title": "QuEval: Beyond High-Dimensional Indexing a la Carte\n", "abstract": " In the recent past, the amount of high-dimensional data, such as feature vectors extracted from multimedia data, increased dramatically. A large variety of indexes have been proposed to store and access such data efficiently. However, due to specific requirements of a certain use case, choosing an adequate index structure is a complex and time-consuming task. This may be due to engineering challenges or open research questions. To overcome this limitation, we present QuEval, an open-source framework that can be flexibly extended w.r.t. index structures, distance metrics, and data sets. QuEval provides a unified environment for a sound evaluation of different indexes, for instance, to support tuning of indexes. In an empirical evaluation, we show how to apply our framework, motivate benefits, and demonstrate analysis possibilities.", "num_citations": "36\n", "authors": ["1503"]}
{"title": "Specification of database applications in the TROLL language\n", "abstract": " In the area of large database applications, i.e. complex interactive software systems with persistently stored information, suitable formalisms for specification are almost not present. Traditionally, the description of a database application consists of two parts, the database schema and a more or less formal description of the application functions. We present an object-oriented approach to integrate both aspects. A formal model is described that models objects as processes that can be observed through attributes. Based on this model, the language TROLL is introduced. It is a logical language for the abstract object-oriented description of information systems. Finally, we propose mechanisms for defining external views and for queries that fit in the framework.", "num_citations": "36\n", "authors": ["1503"]}
{"title": "Object-Oriented Specification and Stepwise Refinement.\n", "abstract": " A basic concept in object-oriented approaches is the notion of object as integrated unit of structure and behavior. Conceptually, objects are modeled as processes of which certain dynamic characteristics of their internal state can be observed using attributes. Objects are the basic units of design. Systems are composed from objects that interact to provide the desired services. In the semantics domain, concepts related to the object-oriented paradigm like interaction, inheritance and object aggregation can be uniformily modelled by object morphisms. In this paper we introduce the language TROLL to conceptually specify dynamic information systems. TROLL supports abstract description of temporal evolvement of objects, classi cation of objects, complex objects, active objects and specialization hierarchies. Additionally, the integration of concepts like modularization support, and component reusability into the TROLL framework is brie y discussed.", "num_citations": "35\n", "authors": ["1503"]}
{"title": "Combining schema and level-based matching for web service discovery\n", "abstract": " Due to the availability of huge number of Web services (WSs), finding an appropriate WS according to the requirement of a service consumer is still a challenge. In this paper, we present a new and flexible approach, called SeqDisc, that assesses the similarity between WSs. In particular, the approach exploits the Pr\u00fcfer encoding method to represent WSs as sequences capturing both semantic and structure information of service descriptions. Based on the sequence representation, we develop an efficient sequence-based schema matching approach to measure the similarity between WSs. A set of experiments is conducted on real data sets, and the results confirm the performance of the proposed solution.", "num_citations": "34\n", "authors": ["1503"]}
{"title": "Updates in a rule-based language for objects\n", "abstract": " The integration of object-oriented concepts into deductive databases has been investigated for a certain time now. Various approaches to incorporate updates into deduction have been proposed. The current paper presents an approach which is based on object versioning; di erent versions of one object may be created and referenced during an update-process. By means of such versions it becomes possible to exert explicit control on the update process during bottom-up evaluation in a rather intuitive way. The units for updates are the result sets of base methods, ie methods, whose results are stored in the object-base and are not de ned by rules. However, the update itself may be de ned by rules. Update-programs have xpoint semantics; the xpoint can be computed by a bottomup evaluation according to a certain strati cation.", "num_citations": "34\n", "authors": ["1503"]}
{"title": "A robust and universal metaproteomics workflow for research studies and routine diagnostics within 24 h using phenol extraction, FASP digest, and the MetaProteomeAnalyzer\n", "abstract": " The investigation of microbial proteins by mass spectrometry (metaproteomics) is a key technology for simultaneously assessing the taxonomic composition and the functionality of microbial communities in medical, environmental and biotechnological applications. We present an improved metaproteomics workflow using an updated sample preparation and a new version of the MetaProteomeAnalyzer software for data analysis. Multidimensional separation (GelLC, MudPIT) was sacrificed aiming at fast analysis of a broad range of different samples in less than 24 h. The improved workflow generates at least two times as many protein identifications than our previous workflow, and a drastic increase of taxonomic and functional annotations. Improvements of all aspects of the workflow, particularly the speed, make it potentially suitable for routine clinical diagnostics (i.e. fecal samples) and analysis of technical and environmental samples. The MetaProteomeAnalyzer is provided to the scientific community as a central remote server solution at www.mpa.ovgu.de.", "num_citations": "33\n", "authors": ["1503"]}
{"title": "Automatic selection of processing units for coprocessing in databases\n", "abstract": " Specialized processing units such as GPUs or FPGAs provide great opportunities to speed up database operations by exploiting parallelism and relieving the CPU. But utilizing coprocessors efficiently poses major challenges to developers. Besides finding fine-granular data parallel algorithms and tuning them for the available hardware, it has to be decided at runtime which (co)processor should be chosen to execute a specific task. Depending on input parameters, wrong decisions can lead to severe performance degradations since involving coprocessors introduces a significant overhead, e.g., for data transfers. In this paper, we present a framework that automatically learns and adapts execution models for arbitrary algorithms on any (co)processor to find break-even points and support scheduling decisions. We demonstrate its applicability for three common use cases in modern database systems and\u00a0\u2026", "num_citations": "33\n", "authors": ["1503"]}
{"title": "Schema integration with integrity constraints\n", "abstract": " In this paper we discuss the use and treatment of integrity constraints in the federated database design process. We consider different situations occurring frequently in the schema transformation and schema integration process. Based on that, general rules are given which describe the correct treatment of integrity constraints. Other proposed integration approaches do not consider integrity constraints at all or are restricted to special kinds of constraints. Therefore, our approach can be used to extend or complete existing integration methodologies.", "num_citations": "33\n", "authors": ["1503"]}
{"title": "Data measurement in research information systems: metrics for the evaluation of data quality\n", "abstract": " In recent years, research information systems (RIS) have become an integral part of the university\u2019s IT landscape. At the same time, many universities and research institutions are still working on the implementation of such information systems. Research information systems support institutions in the measurement, documentation, evaluation and communication of research activities. Implementing such integrative systems requires that institutions assure the quality of the information on research activities entered into them. Since many information and data sources are interwoven, these different data sources can have a negative impact on data quality in different research information systems. Because the topic is currently of interest to many institutions, the aim of the present paper is firstly to consider how data quality can be investigated in the context of RIS, and then to explain how various dimensions of\u00a0\u2026", "num_citations": "32\n", "authors": ["1503"]}
{"title": "Ocelot/hype: Optimized data processing on heterogeneous hardware\n", "abstract": " The past years saw the emergence of highly heterogeneous server architectures that feature multiple accelerators in addition to the main processor. Efficiently exploiting these systems for data processing is a challenging research problem that comprises many facets, including how to find an optimal operator placement strategy, how to estimate runtime costs across different hardware architectures, and how to manage the code and maintenance blowup caused by having to support multiple architectures. In prior work, we already discussed solutions to some of these problems: First, we showed that specifying operators in a hardware-oblivious way can prevent code blowup while still maintaining competitive performance when supporting multiple architectures. Second, we presented learning cost functions and several heuristics to efficiently place operators across all available devices. In this demonstration, we\u00a0\u2026", "num_citations": "32\n", "authors": ["1503"]}
{"title": "Objektrelationale Datenbanken\n", "abstract": " Objektrelationale Datenbanken - Research Collection Header Upper Right Menu Log in de jump to https://www.ethz.ch Research Collection Toggle navigation Upper Right Menu Login Help Help Language Deutsch Toggle navigation Search View Item Home Books Book Chapter View Item Home Books Book Chapter View Item Research Collection Navigational link Search Objektrelationale Datenbanken Mendeley CSV RIS BibTeX Thumbnail Metadata only Author T\u00fcrker, Can Date 2007 Type Book Chapter ETH Bibliography yes Altmetrics Publication status published External links Search via SFX Editor Kudrass, Thomas Book title Taschenbuch Datenbanken Pages / Article No. 338 - 360 Publisher Fachbuchverlag Leipzig im Hanser Verlag Organisational unit 02207 - Functional Genomics Center Zurich / Functional Genomics Center Zurich More Show all metadata ETH Bibliography yes Altmetrics \ufeff Browse \u2026", "num_citations": "32\n", "authors": ["1503"]}
{"title": "Towards unanticipated runtime adaptation of Java applications\n", "abstract": " Modifying an application usually means to stop the application, apply the changes, and start the application again. That means, the application is not available for at least a short time period. This is not acceptable for highly available applications. One reasonable approach which faces the problem of unavailability is to change highly available applications at runtime. To allow extensive runtime adaptation the application must be enabled for unanticipated changes even of already executed program parts. This is due to the fact that it is not predictable what changes become necessary and when they have to be applied. Since Java is commonly used for developing highly available applications, we discuss its shortcomings and opportunities regarding unanticipated runtime adaptation. We present an approach based on Java HotSwap and object wrapping which overcomes the identified shortcomings and evaluate it in\u00a0\u2026", "num_citations": "31\n", "authors": ["1503"]}
{"title": "Interactive example-driven integration and reconciliation for accessing database federations\n", "abstract": " The integration of heterogeneous databases affects two main problems: schema integration and instance integration. At both levels a mapping from local elements to global elements is specified and various conflicts caused by the heterogeneity of the sources have to be resolved. For the detection and resolution of instance-level conflicts we propose an interactive, example-driven approach. The basic idea is to combine an interactive query tool similar to query-by-example with facilities for defining and applying integration operations. This integration approach is supported by a multidatabase query language, which provides special mechanisms for conflict resolution. The foundations of these mechanisms are introduced and their usage in instance integration and reconciliation is presented. In addition, we discuss basic techniques for supporting the detection of instance-level conflicts.", "num_citations": "31\n", "authors": ["1503"]}
{"title": "User authentication in multidatabase systems\n", "abstract": " The aspect of security needs more consideration in the area of architectures for multidatabase systems. Particularly, the authentication of users which is a main prerequisite for a successful authorization is not considered sufficiently in current architectures. Due to the autonomy and heterogeneity of the component database systems, the problem of authentication in multidatabase systems is more complex than in traditional database systems. In this paper we discuss the foundations and prerequisites for architectures of authentication in multidatabase systems. We present several approaches with respect to different degrees of autonomy and heterogeneity. Especially, we work out the authentication process and show the advantages compared with related approaches.", "num_citations": "31\n", "authors": ["1503"]}
{"title": "Schema integration and view derivation by resolving intensional and extensional overlappings\n", "abstract": " The integration of schemata is a very essential but also a very complex task in a federated database environment. Especially the integration of di erent inheritance hierarchies into one hierarchy is not satisfactorily solved up to now. We show, why some existing approaches fail and propose a new algorithm, which further meets the demand for complete, correct and minimal integrated schemata. Furthermore, with respect to the well-known concept of logical data independence, our algorithm supports the derivation of di erent view-dependent external schemata. External object-oriented schemata can be automatically generated by our algorithm.", "num_citations": "31\n", "authors": ["1503"]}
{"title": "Three-Level-Specification of databases using an extended entity-relationship model\n", "abstract": " A great deal of (so-called) semantic data models has been proposed for the specification of conceptual schemas. Each of these offers many different, but semantically related constructs for modelling.               We, therefore, attempt to integrate all constructs into a few semantic constructs. The resulting data model emphasizes a clear conceptual separation of data, object, and transactions resulting in corresponding levels.               The object level enhances the Entity-Relationship (ER) model with the concepts of type construction, complex object types, derived information, and proposes an extended key concept. The type constructor allows to formulate specialization, generalization, and partition abstractions. Aggregation and grouping can be modelled by means of complex object types. Derived information incorporates concepts of deductive databases. Additionally, all concepts can be enriched by specifying\u00a0\u2026", "num_citations": "31\n", "authors": ["1503"]}
{"title": "A comprehensive database schema integration method based on the theory of formal concepts\n", "abstract": " Integrating heterogeneous database schemata is a major task in federated database design where preexisting and heterogeneous database systems need to be integrated virtually by providing a homogenization database interface. Most proposed schema integration methods suffer from very complex result schemata and insufficient handling of extensional relations, i.e. in the way how redundant data of the input systems are dealt with. Redundancy among the input systems may thus remain undetected and, hence, remains uncontrolled.               Our GIM (Generic Integration Model) method is based on the elegant and mathematically founded theory of formal concept analysis (FCA). The main idea is to integrate schemata into one formal context which is a binary relation between a set of attributes and a set of base extensions (set of potential objects). From that context we apply an FCA-algorithm to semi\u00a0\u2026", "num_citations": "30\n", "authors": ["1503"]}
{"title": "Managing object identity in federated database systems\n", "abstract": " The federation of heterogeneous data management systems is still a challenge for the database system research. A main topic in federated system environments is the global identification of objects. On the federation level, a framework for object identification is needed that meets the demand for global uniqueness and allows to access the objects in the local systems. Furthermore, we suggest an identification module in this paper which satisfies these requirements and supports the handling of proxy-objects representing the same real-world entity as well as restructuring objects during the federation process. After describing this module we shall examine some aspects of implementation.", "num_citations": "30\n", "authors": ["1503"]}
{"title": "Language Features for Object-Oriented Conceptual Modelling.\n", "abstract": " In this paper, an approach to formally specify a conceptual model of a system and its environment to be developed in a behavior-oriented way is presented. We identify those concepts of object-oriented approaches that have to be formalized. We then propose characteristics of behavior-oriented mathematical models representing a system as a collection of interacting objects. To specify such models, we present the language TROLL which is a formal object-oriented language for conceptual modeling. TROLL avoids separate description of static and dynamic aspects of the system to be developed. Features of the language are explained using an example of a library information system.", "num_citations": "30\n", "authors": ["1503"]}
{"title": "Migration from annotation-based to composition-based product lines: towards a tool-driven process\n", "abstract": " Software product lines allow a developer to produce similar programs based on a common code base. Two main techniques exist: composition-based and annotation-based approaches. Although composition-based approaches offer potential advantages such as maintainability, in practice mostly annotationbased approaches are used. The main reason hindering the migration of existing projects is the difficulty of the transformation process that can take a lot of time in which maintenance and evolution of the system are put on hold. Thus, for a company, it is hard to estimate the transformation costs and the success is uncertain. As already stated in previous work, a hybrid solution using both approaches may be an adequate solution to overcome this problem. Therefore, we propose a migration concept focusing on technical requirements, such as tool-and language support to reduce the risk during the error-prone migration process. We exemplify the concept by considering the partial migration of a real-world system from preprocessorbased variability to an implementation based on feature-oriented programming. We identify conceptual and tool-based challenges that must be addressed for the practical application. We present technical considerations that must be taken into account for stepwise migration and specific challenges related to our case study.", "num_citations": "29\n", "authors": ["1503"]}
{"title": "Towards optimization of hybrid CPU/GPU query plans in database systems\n", "abstract": " Current database research identified the computational power of GPUs as a way to increase the performance of database systems. Since GPU algorithms are not necessarily faster than their CPU counterparts, it is important to use the GPU only if it is beneficial for query processing. In a general database context, only few research projects address hybrid query processing, i.e., using a mix of CPU- and GPU-based processing to achieve optimal performance. In this paper, we extend our CPU/GPU scheduling framework to support hybrid query processing in database systems. We point out fundamental problems and provide an algorithm to create a hybrid query plan for a query using our scheduling framework.", "num_citations": "29\n", "authors": ["1503"]}
{"title": "Research directions in database architectures for the internet of things: a communication of the first international workshop on database architectures for the internet of\u00a0\u2026\n", "abstract": " This paper is a record of the First International Workshop on Database Architectures for the Internet of Things. The Internet of Things refers to the future internet which will contain trillions of nodes representing various objects from small ubiquitous sensor devices and handhelds to large web servers and supercomputer clusters. The workshop investigated a number of areas appertaining to data management in the Internet of Things from storage structures, through database management methods, to service-oriented architectures and new approaches to information search. Running orthogonal to these layers the matter of security was also considered. Taking a philosophical viewpoint our whole current framework for understanding data management may be ill-equipped to meet the challenges of the Internet of Things. The workshop gave participants the chance to discuss these matters, exchange ideas and\u00a0\u2026", "num_citations": "29\n", "authors": ["1503"]}
{"title": "Concepts of Object-Orientation\n", "abstract": " An object is a unit of structure and behavior; it has an identity which persists through change; objects communicate with each other; they are classified by object types, collected into object classes, related by inheritance, and composed to form complex objects. In the first part of the paper, this rich world of concepts and constructions is explained in an informal but systematic way, independent of any language or system. In the second part, features of an object specification language are outline which incorporate most of these concepts and constructions.", "num_citations": "29\n", "authors": ["1503"]}
{"title": "A survey on scalability and performance concerns in extended product lines configuration\n", "abstract": " Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii\u00a0\u2026", "num_citations": "27\n", "authors": ["1503"]}
{"title": "Business process improvement framework and representational support\n", "abstract": " Business process management and improvement are vital for enterprises in competitive environments. Understanding of a process is a pre-requisite and important step for improvement. Interaction between humans, computers, and business objects provide excellent opportunities for knowledge extraction. However, the specification of a framework is required for business process improvement, which extends from data collection, analytical methods, storage, and representation of knowledge. The process models conceived for information system development are not sufficient for post execution analysis and improvement. In this paper, we specify such a framework briefly and focus on providing representational support for business process improvement. The main objective is to improve the overall improvement process by providing enriched graphical process models. Furthermore, we use a case study to\u00a0\u2026", "num_citations": "27\n", "authors": ["1503"]}
{"title": "A schema matching-based approach to XML schema clustering\n", "abstract": " The relationship between XML data clustering and schema matching is bidirectional. On one side, clustering techniques have been adopted to improve matching performance, and on the other side schema matching is the backbone of the clustering technique. This paper presents a new approach for clustering XML schema based on schema matching. In particular, we develop and implement an XML schema matching system, which determines semantic similarities between XML schemas based on the Pr\u00fcfer sequence representation of schema trees. The proposed computation similarity algorithm makes use of the semantic meaning of XML elements as well as the hierarchical features of XML schemas. The computed similarities are then exploited by an agglomerative clustering algorithm to group similar schemas. Our experimental results show that the proposed approach is fast and accurate in clustering\u00a0\u2026", "num_citations": "27\n", "authors": ["1503"]}
{"title": "Database scan variants on modern CPUs: A performance study\n", "abstract": " Main-memory databases rely on highly tuned database operations to achieve peak performance. Recently, it has been shown that different code optimizations for database operations favor different processors. However, it is still not clear how the combination of code optimizations (e.g., loop unrolling and vectorization) will affect the performance of database algorithms on different processors.                 In this paper, we extend prior studies by an in-depth performance analysis of different variants of the scan operator. We find that the performance of the scan operator for different processors gets even harder to predict when multiple code optimizations are combined. Since the scan is the most simple database operator, we expect the same effects for more complex operators such as joins. Based on these results, we identify practical problems for a query processor and discuss how we can counter these\u00a0\u2026", "num_citations": "26\n", "authors": ["1503"]}
{"title": "Categorization of concerns: a categorical program comprehension model\n", "abstract": " Program comprehension models lack associations with the paradigm of separation of concerns. We present a holistic program comprehension model based on categorization studies of psychology. A comparison of research about categorization and separation of concerns is used to develop the model. The cognition in this model is influenced by the context wherein a programmer investigates the code. The comprehension process starts with some ad-hoc concerns that are about to be refined by following an investigation strategy and a vertical process study. Through this study, the concerns refinement may imply an update on the knowledge and the adoption of a new behavior for the investigation strategy. Our model can serve as a starting point for further investigations.", "num_citations": "26\n", "authors": ["1503"]}
{"title": "Using Active Objects for Query Processing.\n", "abstract": " Most object-oriented databases can only be accessed by a programming language rather than a query language, although an ad hoc query facility is a desirable feature of such systems. In this paper, we present a way to process queries to extract data from a collection of interacting objects. We adopt the object-oriented paradigm to describe the operational semantics of queries given as expressions of a query algebra. An expression can be transformed into a collection of active objects performing the operations. We show that we are able to implement query processing using the existing framework for collections of objects, which is based on objects modeled as processes. Our approach makes parallel query processing possible.", "num_citations": "26\n", "authors": ["1503"]}
{"title": "Towards robust data storage in wireless sensor networks\n", "abstract": " Wireless sensor networks are used for sensing data if wired communication is not suitable. Connection losses and depletion of nodes, however, result in reduced data availability of such networks. This is problematic in upcoming scenarios like health care or home automation where data availability is highly important. In this paper, we present an approach to provide robust data storage for wireless sensor networks. We achieve this goal by providing FAME-DBMS, a customizable database management system which can be tailored according to the varying requirements of a sensor network. FAME-DBMS provides reliable data storage using security and integrity features, transaction management and recovery and a customizable query engine. Since data reliability in wireless sensor networks also suffers from node failures, we propose a new S - RAID storage layer inspired from the RAID approach of server systems\u00a0\u2026", "num_citations": "25\n", "authors": ["1503"]}
{"title": "A component-based Petri net model for specifying and validating cooperative information systems\n", "abstract": " This paper presents fundamentals of a novel framework adequately tailored for specifying and validating complex information systems as fully distributed, autonomous yet cooperating components. The proposed framework, referred to as C o-nets, is a multi-paradigm soundly integrating object-oriented (OO) structuring mechanisms, modularity concepts and some constructions from semantical data modeling into an appropriate variety of algebraic Petri nets. The model is semantically interpreted in a true concurrency way using an adequate instantiation of rewriting logic. C o-nets main features may be highlighted as follows: firstly, C o-nets allow to build autonomous components as a hierarchy of OO classes with explicit interfaces. Each component behaves with respect to an intra-component evolution pattern that fully exhibits intra-as well as inter-object concurrency. Secondly, while such components autonomously\u00a0\u2026", "num_citations": "25\n", "authors": ["1503"]}
{"title": "Personalized recommender systems for product-line configuration processes\n", "abstract": " Product lines are designed to support the reuse of features across multiple products. Features are product functional requirements that are important to stakeholders. In this context, feature models are used to establish a reuse platform and allow the configuration of multiple products through the interactive selection of a valid combination of features. Although there are many specialized configurator tools that aim to provide configuration support, they only assure that all dependencies from selected features are automatically satisfied. However, no support is provided to help decision makers focus on likely relevant configuration options. Consequently, since decision makers are often unsure about their needs, the configuration of large feature models becomes challenging. To improve the efficiency and quality of the product configuration process, we propose a new approach that provides users with a limited set of\u00a0\u2026", "num_citations": "24\n", "authors": ["1503"]}
{"title": "Accelerating multi-column selection predicates in main-memory-the Elf approach\n", "abstract": " Evaluating selection predicates is a data-intensive task that reduces intermediate results, which are the input for further operations. With analytical queries getting more and more complex, the number of evaluated selection predicates per query and table rises, too. This leads to numerous multicolumn selection predicates. Recent approaches to increase the performance of main-memory databases for selection-predicate evaluation aim at optimally exploiting the speed of the CPU by using accelerated scans. However, scanning each column one by one leaves tuning opportunities open that arise if all predicates are considered together. To this end, we introduce Elf, an index structure that is able to exploit the relation between several selection predicates. Elf features cache sensitivity, an optimized storage layout, fixed search paths, and slight data compression. In our evaluation, we compare its query performance to\u00a0\u2026", "num_citations": "24\n", "authors": ["1503"]}
{"title": "XML schema element similarity measures: a schema matching context\n", "abstract": " In this paper, we classify, review, and experimentally compare major methods that are exploited in the definition, adoption, and utilization of element similarity measures in the context of XML schema matching. We aim at presenting a unified view which is useful when developing a new element similarity measure, when implementing an XML schema matching component, when using an XML schema matching system, and when comparing XML schema matching systems.", "num_citations": "24\n", "authors": ["1503"]}
{"title": "Merging Inheritance Hierarchies for Schema Integration based on Concept Lattices\n", "abstract": " Merging inheritance hierarchies is an essential task of schema integration as part of the design of a federated database. Based on the formalization of inheritance hierarchies as concept lattices we present an e cient algorithm for deriving an integrated inheritance hierarchy as result of merging two inheritance hierarchies with overlapping extensions and types. Several methods are presented to modify the integrated hierarchy to optimize the resulting object-oriented schema with respect to certain quality criteria (disjoint specialization, number of classes, support of null values, etc). These methods are necessary to adjust the integrated schema to speci c requirements and for realizing applicationspeci c views.", "num_citations": "24\n", "authors": ["1503"]}
{"title": "Gaining a uniform view of different integration aspects in a prototyping environment\n", "abstract": " Prototyping based on formal specification should satisfy the requirements of engineers and users equally. The support of both groups requires a formalization of all activities as contract base and an increase of experience by experimental strategies. Contrary to these requirements several problems rest with existing software development environments, which result in complex systems becoming easily unmanageable. Our approach to develop a prototyping environment integrates former results of our project \u201cImplementation of Information Systems\u201d. We propose the object-oriented specification language Troll as core of the integrated software development environment TBench. Troll is used for conceptual modeling of structural and behavioral aspects of the universe of discourse by offering a formal framework. Moreover the TBench utilizes Troll to realize relationships between the various graphical\u00a0\u2026", "num_citations": "24\n", "authors": ["1503"]}
{"title": "Transitional Monitoring of Dynamic Integrity Constraints\n", "abstract": " Database specification should include not only the static structure of a database system, but also its dynamic behaviour. In order to determine admissible sequences of states dynamic integrity constraints may be utilized. They give conditions on state transitions as well as long-term relations between database states in a descriptive manner. Complementarily, predefined transactions, ie basic elements of application programs, or ad-hoc transactions completed by triggers induce executable state sequences in an operational manner.We have investigated how to monitor dynamic constraints without looking at entire state sequences, ie database histories, but considering only single state transitions, as it is usual in monitoring. Such monitoring can be achieved either by a universal (application-independent) monitor algorithm, or by (application-specific and thus more efficient) transactions or triggers into which the constraints are transformed and specialized during database design. In our approach, dynamic integrity constraints are expressed by formulae of temporal logic (sect. 2). We have developed procedures to construct from such formulae so-called transition graphs, whose paths correspond to admissible state sequences (section 3). On the one hand, these graphs can control execution of a monitor that reduces analysis of state sequences to tests on state transitions. On the other hand, these graphs can be transformed systematically into refined pre-/postconditions of transactions or into triggers, such that each executable sequence becomes admissible (section 4). Such transformations can partially be supported by an environment for integrity\u00a0\u2026", "num_citations": "24\n", "authors": ["1503"]}
{"title": "Evolving Object Specifications\n", "abstract": " The notion of object evolution covers several aspects being important for objectoriented information systems. An object keeps its identity (and some of its properties, of course) while changing its external interface and the implementation of its methods. We present a framework based on a temporal logic where the speci cation of an object template may be modi ed during system runtime. The presented extended temporal logic dyOSL explicitly manipulates state-dependent sets of current axioms. This framework can handle several problems arising with object evolution.", "num_citations": "23\n", "authors": ["1503"]}
{"title": "Beyond software product lines: Variability modeling in cyber-physical systems\n", "abstract": " Smart IT has an increasing influence on the control of daily life. For instance, smart grids manage power supply, autonomous automobiles take part in traffic, and assistive robotics support humans in production cells. We denote such systems as Cyber-physical Systems (CPSs), where cyber addresses the controlling software, while physical describes the controlled hardware. One key aspect of CPSs is their capability to adapt to new situations autonomously or with minimal human intervention. To achieve this, CPSs reuse, reorganize and reconfigure their components during runtime. Some components may even serve in different CPSs and different situations simultaneously. The hardware of a CPS usually consists of a heterogeneous set of variable components. While each component can be designed as a software product line (SPL), which is a well established approach to describe software and hardware\u00a0\u2026", "num_citations": "22\n", "authors": ["1503"]}
{"title": "Load-aware inter-co-processor parallelism in database query processing\n", "abstract": " For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a\u00a0\u2026", "num_citations": "22\n", "authors": ["1503"]}
{"title": "Hurdles in Multi-Language Refactoring of Hibernate Applications\n", "abstract": " Different programming languages can be involved in the implementation of a single software application. In these software applications, source code of one programming language interacts with code of a different language. By refactoring an artifact of one programming language, the interaction of this artifact with an artifact written in another programming language may break. We present a study on refactoring an software application that contains artifacts of different languages.", "num_citations": "22\n", "authors": ["1503"]}
{"title": "Sorting, grouping and duplicate elimination in the advanced information management prototype\n", "abstract": " Sorting, duplicate suppression and grouping are important operations in relational database man-agement systems. This paper is devoted to the re-lated language features and their implementation in the Advanced Information Management PrototypeAIM-P. The query language HDBL is an SQL-like database language supporting the extended NF? data model. The proposed language extensions follow the classical SQL approach for sorting and duplicate elimination by extending the SFW con-struct with appropriate clauses. For the grouping operation we chose a new syntactical construct be-cause the implicit structure transformation of grouping differs from the sorting and duplicate suppression operations. Finally, the integration into the query evaluation of the AIM prototype is de-scribed. can be packaged in an SQL-like fashion, how their semantics are to be generalized to cope with the structures supported in AIM-P, and how they are related to sorting, grouping, and duplicate elimi-nation.", "num_citations": "22\n", "authors": ["1503"]}
{"title": "A Framework for Cost based Optimization of Hybrid CPU/GPU Query Plans in Database Systems\n", "abstract": " Current database research identified the use of computational power of GPUs as a way to increase the performance of database systems. As GPU algorithms are not necessarily faster than their CPU counterparts, it is important to use the GPU only if it is beneficial for query processing. In a general database context, only few research projects address hybrid query processing, ie, using a mix of CPU-and GPU-based processing to achieve optimal performance. In this paper, we extend our CPU/GPU scheduling framework to support hybrid query processing in database systems. We point out fundamental problems and propose an algorithm to create a hybrid query plan for a query using our scheduling framework. Additionally, we provide cost metrics, accounting for the possible overlapping of data transfers and computation on the GPU. Furthermore, we present algorithms to create hybrid query plans for query sequences and query trees.", "num_citations": "21\n", "authors": ["1503"]}
{"title": "A sequence-based ontology matching approach\n", "abstract": " The recent growing of the Semantic Web requires the need to cope with highly semantic heterogeneities among available ontologies. Ontology matching techniques aim to tackle this problem by establishing correspondences between ontologies\u2019 elements. An intricate obstacle faces the ontology matching problem is its scalability against large number and large-scale ontologies. To tackle these challenges, in this paper, we propose a new matching framework based on Pr\u00fcfer sequences. The proposed approach is applicable for matching a database of XML trees. Our approach is based on the representation of XML ontologies as sequences of labels and numbers by the Pr\u00fcfer\u2019s method that constructs a one-to-one correspondence between schema ontologies and sequences. We capture ontology tree semantic information in Label Pr\u00fcfer Sequences (LPS) and ontology tree structural information in Number Pr\u00fcfer Sequences (NPS). Then, we develop a new structural matching algorithm exploiting both LPS and NPS. Our Experimental results demonstrate the performance benefits of the proposed approach.", "num_citations": "21\n", "authors": ["1503"]}
{"title": "Rule-based schema matching for ontology-based mediators\n", "abstract": " Mediating heterogeneous data sources heavily relies on explicit domain knowledge expressed, for example, as ontologies and mapping rules. We discuss the use of logic representations for mapping schema elements onto concepts expressed in a simplified ontology for cultural assets. Starting with a logic representation of the ontology, criteria for a rule-based schema matching are exemplified. Special requirements are the handling of uncertain information and the processing of hierarchical XML structures representing instances.", "num_citations": "21\n", "authors": ["1503"]}
{"title": "Analysis of breast cancer detection using different machine learning techniques\n", "abstract": " Data mining algorithms play an important role in the prediction of early-stage breast cancer. In this paper, we propose an approach that improves the accuracy and enhances the performance of three different classifiers: Decision Tree (J48), Na\u00efve Bayes (NB), and Sequential Minimal Optimization (SMO). We also validate and compare the classifiers on two benchmark datasets: Wisconsin Breast Cancer (WBC) and Breast Cancer dataset. Data with imbalanced classes are a big problem in the classification phase since the probability of instances belonging to the majority class is significantly high, the algorithms are much more likely to classify new observations to the majority class. We address such problem in this work. We use the data level approach which consists of resampling the data in order to mitigate the effect caused by class imbalance. For evaluation, 10 fold cross-validation is performed. The\u00a0\u2026", "num_citations": "20\n", "authors": ["1503"]}
{"title": "Verteiltes und paralleles Datenmanagement\n", "abstract": " Verteilte und parallele Verarbeitung in Datenbanken erlebte eine erste Bl\u00fcte in den 80er-Jahren, als parallele Verarbeitung und geografisch verteilte Speicherung von relationalen Daten erstmals intensiv untersucht wurden. In dieser \u00c4ra erschienen ein ganze Reihe von fundamentalen Ver\u00f6ffentlichungen und Lehrb\u00fcchern, die die Grundlagen f\u00fcr derartige Verfahren legten. In den darauffolgenden Jahren wurden diese Verfahren genutzt, ohne dass dabei ein erneuter Hype zu beobachten war. Dies \u00e4nderte sich in den letzten Jahren mit Konzepten wie der Cloud-Speicherung und der Analyse in Big-Data-Szenarien, die nun moderne Anforderungen an die Skalierbarkeit mit 20 Jahre altem Lehrbuchwissen konfrontierten. Auch wenn viele der damaligen Entwicklungen auch nach dieser Zeit unver\u00e4ndert g\u00fcltig sind, f\u00fcgen diese neuen Szenarien neue Aspekte hinzu, die damals nicht vorhersehbar waren. Es ist\u00a0\u2026", "num_citations": "20\n", "authors": ["1503"]}
{"title": "Toward GPU Accelerated Data Stream Processing\n", "abstract": " In recent years, the need for continuous processing and analysis of data streams has increased rapidly. To achieve high throughput-rates, stream-applications make use of operatorparallelization, batching-strategies and distribution. Another possibility is to utilize co-processors capabilities per operator. Further, the database community noticed, that a columnoriented architecture is essential for efficient co-processing, since the data transfer overhead is smaller compared to transferring whole tables.However, current systems still rely on a row-wise architecture for stream processing, because it requires data structures for high velocity. In contrast, stream portions are in rest while being bound to a window. With this, we are able to alter the per-window event representation from row to column orientation, which will enable us to exploit GPU acceleration.", "num_citations": "20\n", "authors": ["1503"]}
{"title": "Business Process Modeling Language for Performance Evaluation\n", "abstract": " Evaluation of business processes is important for analysis and improvement of an organization. Different methods are used to evaluate the performance like statistics or visualization. However, these methods meet demands mainly on the top organizational level. There is insufficient support to evaluate processes at the process managerial level leading to a limited visibility of deficiencies in business processes at process level. In this paper, we address this challenge and focus on the relation between evaluation of business processes and their representation at the process managerial level. In our research, we follow the design science methodology in order to provide business process models for performance analysis. We also provide constructs and patterns of our proposed modeling language for analysis and improvement of business processes. The analytical business process modeling language is further\u00a0\u2026", "num_citations": "20\n", "authors": ["1503"]}
{"title": "Supporting Information Fusion with Federated Database Technologies (Position Paper).\n", "abstract": " A common problem facing many users today is to extract and combine information from multiple, heterogeneous sources and to derive information of a new quality or abstraction level. Though essential parts of this information fusion process can be supported by techniques developed in the field of federated databases, new approaches for managing consistency, uncertainty or quality of data and enabling efficient analysis of distributed, heterogenous sources are still required. This paper presents a brief survey of requirements and applications of information fusion from the database view, discusses the usage of federated database technologies in fusion systems and points out possible research directions.", "num_citations": "20\n", "authors": ["1503"]}
{"title": "Abstract Specification of Object Interaction\n", "abstract": " As a consequence of the growing interest in active databases the need for abstract activity modelling concepts arises. The object-oriented approach provides a natural view of evolving systems as collections of objects having both structure as well as behaviour. Based on this view we dicuss event calling structures as an abstract model for synchronous communication of objects. This basic model defines a powerf...", "num_citations": "20\n", "authors": ["1503"]}
{"title": "Spezifikation, Semantik und \u00dcberwachung von Objektlebensl\u00e4ufen in Datenbanken\n", "abstract": " Der Entwurf und die Implementierung von gro en Softwaresystemen ist eine kostenund arbeitsintensive Aufgabe, die sich in der Regel uber einen langen Zeitraum erstreckt und von mehreren Personen durchgef uhrt werden mu. Ausgehend von einer Anforderungsanalyse wird das Gesamtsystem in mehreren aufeinander folgenden Phasen entwickelt, wobei jede dieser Phasen durch eine Beschreibung des Systemverhaltens, der Systemarchitektur und weiterer Eigenschaften dokumentiert wird.Fehler in den fr uhen Entwurfsphasen, die erst in einer sp ateren Entwurfsphase oder gar erst nach Implementierung des Systems entdeckt werden, erfordern einen aufwendigen Neuentwurf des Gesamtsystems oder von einzelnen Systemkomponenten. Nur eine formale Beschreibung des Softwaresystems mit fester Semantik erm oglicht das Erkennen von derartigen Inkonsistenzen und unvollst andigen Beschreibungen in fr uhen Entwurfsphasen.", "num_citations": "20\n", "authors": ["1503"]}
{"title": "Gridformation: Towards self-driven online data partitioning using reinforcement learning\n", "abstract": " In this paper we define a research agenda to develop a general framework supporting online autonomous tuning of data partitioning and layouts with a reinforcement learning formulation. We establish the core elements of our approach: agent, environment, action space and supporting components. Externally predicted workloads and the current physical design serve as input to our agent. The environment guides the search process by generating immediate rewards based on fresh cost estimates, for either the entirety or a sample of queries from the workload, and by deciding the possible actions given a state. This set of actions is configurable, enabling the representation of different partitioning problems. For use in an online setting the agent learns a fixed-length sequence of n actions that maximize the temporal reward for the predicted workload. Through an initial implementation we assert the feasibility of our\u00a0\u2026", "num_citations": "19\n", "authors": ["1503"]}
{"title": "Generating highly customizable SQL parsers\n", "abstract": " Database technology and the Structured Query Language (SQL) have grown enormously in recent years. Applications from different domains have different requirements for using database technology and SQL. The major problem of current standards of SQL is complexity and unmanageability. In this paper we present an approach based on software product line engineering which can be used to create customizable SQL parsers and consequently different SQL dialects. We present an overview of how SQL can be decomposed in terms of features and compose different features to create different parsers for SQL.", "num_citations": "19\n", "authors": ["1503"]}
{"title": "Towards an Agent-Oriented Framework for Specification of Information Systems\n", "abstract": " Objects in information systems usually have a very long lifespan. Therefore, it often happens that during the life of an object external requirements are changing, e.g. changes of laws. Such changes often require the object to adopt another behavior. In consequence, it is necessary to get a grasp of dynamically changing object behavior. Unfortunately, not all possible changes can in general be taken into account in advance at specification time. Hence, current object specification approaches cannot deal with this problem. Flexible extensions of object specification are needed to capture such situations.             The approach we present and discuss in this paper is an important step towards a specification framework based on the concept of agents by introducing a certain form of knowledge as part of the internal state of objects. Especially, we concentrate on the specification of evolving temporal behavior. For\u00a0\u2026", "num_citations": "19\n", "authors": ["1503"]}
{"title": "Formal analysis of the Shlaer-Mellor method: towards a toolkit of formal and informal requirements specification techniques\n", "abstract": " In this paper, we define a number of tools that we think belong to the core of any toolkit for requirements engineers. The tools are conceptual and hence, they need precise definitions that lay down as exactly as possible what their meaning and possible use is. We argue that this definition can best be achieved by a formal specification of the tool. This means that for each semi-formal requirements engineering tool we should provide a formal specification that precisely specifies its meaning. We argue that this mutually enhances the formal and semi-formal technique: it makes formal techniques more usable and, as we will argue, at the same time simplifies the diagram-based notations.               At the same time, we believe that the tools of the requirements engineer should, where possible, resemble the familiar semi-formal specification techniques used in practice today. In order to achieve this, we should\u00a0\u2026", "num_citations": "19\n", "authors": ["1503"]}
{"title": "Monitoring Temporal Preconditions in a Behaviour Oriented Object Model\n", "abstract": " Modern database applications require advanced means for modelling system structure and dynamics. Temporal logic has been proven to be a suitable vehicle for specifying the possible evolution of objects to be stored in databases. Past-directed temporal logic, as a means to describe the influence of the historical evolution of a database on applicable state changes, is one facet for the specification of object behaviour. The conceptual modelling language TROLL emphasizes the behaviour of objects over the course of time. Especially the restriction of events with preconditions in past-directed temporal logic has to be monitored, when a system specified in TROLL is implemented or prototyped. In this report we introduce a technique for monitoring (past-directed) temporal preconditions during database r runtime. This technique avoids storing the whole database history for evaluating temporal preconditions. Instead\u00a0\u2026", "num_citations": "19\n", "authors": ["1503"]}
{"title": "Views and formal implementation in a three-level schema architecture for dynamic objects\n", "abstract": " The three-level schema architecture proposed as part of a framework for database standardization supports data independence resulting in a database architecture being flexible and adaptable to changes. Dynamic object bases differ from classical database models in their integrated description of structure and behaviour of objects. The arguments for introducing the different schema levels for database applications hold for object bases, too, and may help to structure large object-oriented applications. This paper discusses the transfer of the three-level approach to an object-oriented approach for describing dynamic objects. Requirements for description formalisms for the external, conceptual and internal level lead to desired language features for object-oriented system specification languages.", "num_citations": "19\n", "authors": ["1503"]}
{"title": "Duplicate detection and deletion in the extended NF2 data model\n", "abstract": " A current research topic in the area of relational databases is the design of systems based on the Non First Normal Form (NF2) data model. One particular development, the so-called extended NF2 data model, even permits structured values like lists and tuples to be included as attributes in relations. It is thus well suited to represent complex objects for non-standard database applications. A DBMS which uses this model, called the Advanced Information Management Prototype, is currently being implemented at the IBM Heidelberg Scientific Center. In this paper we examine the problem of detecting and deleting duplicates within this data model. Several alternative approaches are evaluated and a new method, based on sorting complex objects, is proposed, which is both time- and space-efficient.", "num_citations": "19\n", "authors": ["1503"]}
{"title": "Building Information System Variants with Tailored Database Schemas Using Features\n", "abstract": " Database schemas are an integral part of many information systems (IS). New software-engineering methods, such as software product lines, allow engineers to create a high number of different programs tailored to the customer needs from a common code base. Unfortunately, these engineering methods usually do not take the database schema into account. Particularly, a tailored client program requires a tailored database schema as well to form a consistent IS. In this paper, we show the challenges of tailoring relational database schemas in software product lines. Furthermore, we present an approach to treat the client and database part of an IS in the same way using a variable database schema. Additionally, we show the benefits and discuss disadvantages of the approach during the evolution of an industrial case study, covering a time span of more than a year.", "num_citations": "18\n", "authors": ["1503"]}
{"title": "Using software product lines for runtime interoperability\n", "abstract": " Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we\u00a0\u2026", "num_citations": "18\n", "authors": ["1503"]}
{"title": "The Active Vertice method: a performant filtering approach to high-dimensional indexing\n", "abstract": " The problem of finding nearest neighbors has emerged as an important foundation of feature-based similarity search in multimedia databases. Most spatial index structures based on the R-tree have failed to efficiently support nearest neighbor search in arbitrarily distributed high-dimensional data sets. In contrast, the so-called filtering principle as represented by the popular VA-file has turned out to be a more promising approach. Query processing is based on a flat file of compact vector approximations. In a first stage, those approximations are sequentially scanned and filtered so that in a second stage the nearest neighbors can be determined from a relatively small fraction of the data set.In this paper, we propose the Active Vertice method as a novel filtering approach. As opposed to the VA-file, approximation regions are arranged in a quad-tree like structure. High-dimensional feature vectors are assigned to\u00a0\u2026", "num_citations": "17\n", "authors": ["1503"]}
{"title": "Datenbanken kompakt\n", "abstract": " Datenbanken kompakt - Dbis Repository Dbis Repository Home About Browse Login Datenbanken kompakt Heuer, Andreas and Saake, Gunter and Sattler, Kai-Uwe and other, (2001) Datenbanken kompakt. MITP Verlag, Bonn. Full text not available from this repository. Official URL: http://wwwdb.informatik.uni-rostock.de/biber-kompa... Item Type: Book Uncontrolled Keywords: Lehrbuch Subjects: Autorenart > DBIS-Publikationen Depositing User: Dbis Admin Date Deposited: 24 Mar 2016 11:06 Last Modified: 24 Mar 2016 11:06 URI: http://eprints.dbis.informatik.uni-rostock.de/id/eprint/165 Actions (login required) View Item View Item EPrints Logo Dbis Repository is powered by EPrints 3 which is developed by the School of Electronics and Computer Science at the University of Southampton. More information and software credits. \u2026", "num_citations": "17\n", "authors": ["1503"]}
{"title": "Integrity constraints in federated database design\n", "abstract": " In this paper we demonstrate the application of a methodology for federated database design. Federated database design includes the transformation of heterogeneous local schemata from the native data models of the component database systems into a common data model, the integration of these homogenized schemata into the federated schema, and the derivation of external schemata for global applications. Our methodology seamlessly captures all phases from schema transformation and integration up to derivation of external schemata. Our approach differs from other recently proposed methodologies because we consider integrity constraints throughout all phases and use them for resolving conflicts. Therefore, the main focus of this paper is the treatment of integrity constraints and their evolution in the federated database design process. For resolving conflicts which arise during the transformation and integration process the consideration of integrity constraints can be useful, although the...", "num_citations": "17\n", "authors": ["1503"]}
{"title": "Relationships between dynamic objects\n", "abstract": " In this paper, we describe the speci cation of communication relationships between objects in a framework for object-oriented conceptual modeling of information systems. In our approach, objects have an observable state and can evolve by the occurrence of events. The possible behavior of objects over time is modeled by a set of admissible object life cycles. A high-level approach to specify systems of interacting objects should provide a means to specify communications between objects explicitly. We introduce language features for the speci cation of communications and present an approach to transform such relationships into communication channels in order to make implementation easier.", "num_citations": "17\n", "authors": ["1503"]}
{"title": "Conceptual modeling of database applications\n", "abstract": " The conceptual model is a high-level description of the functionality of a software system usually notated in a logic-based formalism. We discuss the special properties of descriptions of database applications using an appropriate conceptual formalism. A multi-layered approach for conceptual modeling of database applications is discussed together with first ideas on a design methodology using this approach. The specification of values, persistent database objects, temporal object evolution, update operations and arbitrary application processes is separated into different specification layers. Special attention is paid to problems of consistency checking for conceptual models and object-oriented modularization of design documents.", "num_citations": "17\n", "authors": ["1503"]}
{"title": "Algorithmen und Datenstrukturen\n", "abstract": " Nach fast 5 Jahren Bestand der 3. Auflage war es an der Zeit, wieder einmal eine \u00dcberarbeitung vorzunehmen. Auch wenn die relevanten \u00c4nderungen an der Programmiersprache Java eher marginal sind\u2013die Java-Plattform hat sich dagegen sehr viel weiter entwickelt, aber das ist f\u00fcr ein Buch dieser Art weniger von Bedeutung\u2013, haben wir einige Hinweise und eigene Lehrerfahrungen integriert. Unser Anliegen bleibt jedoch weiterhin ein Begleitbuch f\u00fcr Erst-und Zweitsemester in Informatik-lastigen Studieng\u00e4ngen: Weder wollten wir den Umfang durch Aufnahme einer Vielzahl weiterer Algorithmen und Datenstrukturen sprengen noch im Interesse von \u00dcberblicksvorlesungen oder Programmierkursen abspecken. Wir haben in dieser Auflage als neue Algorithmen den f\u00fcr die Routenplanung wichtigen A*-Algorithmus und die Levenshtein-Distanz zum \u00c4hnlichkeitsvergleich von Texten aufgenommen. Weiterhin ist\u00a0\u2026", "num_citations": "16\n", "authors": ["1503"]}
{"title": "Informationsfusion\u2014Herausforderungen an die Datenbanktechnologie\u2014Kurzbeitrag\u2014\n", "abstract": " In vielen Anwendungsbereichen besteht die Aufgabe, Daten oder Informationen aus verschiedenen, zum Teil heterogenen Quellen zu kombinieren, zu verdichten und daraus Informationen einer neuen Qualit\u00e4t abzuleiten. Wesentliche Kernfunktionen dieses als Informationsfusion bezeichneten Prozesses sind dabei durch Methoden der Datenintegration und der Datenanalyse / Data Mining bereitzustellen. Die gewachsenen Strukturen der heute genutzten Informationsquellen und die damit im Zusammenhang stehenden Probleme wie Heterogenit\u00e4t, Inkonsistenz oder Ungenauigkeit der Daten sind mit den aktuell verf\u00fcgbaren Techniken nur bedingt beherrschbar. Ausgehend vom aktuellen Stund der Forschung diskutiert der vorliegende Beitrag Anforderungen an Datenbanktechnologien aus Sicht der Informationsfusion, zeigt m\u00f6gliche Forschungsrichtungen auf und skizziert aktuelle und zuk\u00fcnftige\u00a0\u2026", "num_citations": "16\n", "authors": ["1503"]}
{"title": "Transitive dependencies in transaction closures\n", "abstract": " Complex applications consist of a large set of transactions which are interrelated. There are different kinds of dependencies among transactions of a complex application, e.g. termination or execution dependencies which are constraints on the occurrence of significant transaction events. The authors analyze a set of (orthogonal) transaction dependencies. They do not follow traditional approaches which consider advanced transaction structures as a certain kind of nested transactions. They introduce the notion of transaction closure as a generalization of nested transactions. A transaction closure comprises all transactions which are (transitively) initiated by one (root) transaction. By specifying dependencies among transactions of a transaction closure they are then able to define well-known transaction structures like nested transactions as well as advanced activity structures, e.g. workflows, in a common framework\u00a0\u2026", "num_citations": "16\n", "authors": ["1503"]}
{"title": "Object-oriented design of information systems: TROLL language features\n", "abstract": " We present features of the language Troll for the abstract specification of information systems. Information systems are regarded to be reactive systems with a large database. Before we present the constructs of Troll, we briefly explain the basic ideas on which the language relies. The Universe of Discourse is regarded to be a collection of interacting objects. An object is modeled as a process with an observable state. The language Troll allows for the integrated description of structure and behavior of objects. We explain the abstraction mechanisms provided by Troll, namely roles, specialization, and aggregation. To support the description of systems composed from objects, the concepts of relationships and interfaces may be used.", "num_citations": "16\n", "authors": ["1503"]}
{"title": "Composing annotations without regret? Practical experiences using FeatureC\n", "abstract": " Software product lines enable developers to derive similar products from a common code base. Existing implementation techniques can be categorized as composition\u2010based and annotation\u2010based approaches, with both approaches promising complementary benefits. However, annotation\u2010based approaches are commonly used in practice despite composition allowing physical separation of features and, thus, improving traceability and maintenance. A main hindrance to migrate annotated systems toward a composition\u2010based product line is the challenging and time\u2010consuming transformation task. For a company, it is difficult to predict the corresponding costs, and a successful outcome is uncertain. To overcome such problems, a solution proposed by the previous work is to use a hybrid approach, utilizing composition and annotation simultaneously. Based on this idea, we introduce a stepwise migration process\u00a0\u2026", "num_citations": "15\n", "authors": ["1503"]}
{"title": "Applying process mining in SOA environments\n", "abstract": " Process mining is an emerging analysis technique, which extracts process knowledge from data and provides various benefits to organizations. In Service Oriented Computing environment, different services collaborate with others to carry out the operations and therefore overall picture of operations and execution is not clear. Process mining extracts the information from log files of systems, as recorded during executions, and depicts the reality. In order to apply process mining, extraction of process trace data from log files is a pre-requisite step. A case study demonstrates the practical applicability of our proposed framework for extraction of the process trace data from application systems and integration portals.", "num_citations": "15\n", "authors": ["1503"]}
{"title": "An architecture for interoperability of embedded systems and virtual reality\n", "abstract": " Virtual Reality enhances the development process of complex and inter-operating products due to bringing existing systems together with virtual prototypes. The modeling of existing products within the virtual reality environment and furthermore the properties of products and product combination are important factors for success in a product life cycle. A reduction of effort for modeling of existing products and simulation of properties can be achieved, when systems and their properties are transported to the virtual reality environment. In this paper, we present a service-oriented architecture for embedded systems and virtual reality. The multiplicity of interfaces, protocols, and hardware and software aspects requires an architecture that overcomes the related difficulties to increase efficiency. Service-oriented architectures make different scenarios in the product life cycle possible, whereas the implementation effort for\u00a0\u2026", "num_citations": "15\n", "authors": ["1503"]}
{"title": "Consistent handling of integrity constraints and extensional assertions for schema integration\n", "abstract": " Schema integration is a main step in the database integration process. Since the semantics of a schema is also determined by its integrity constraints, a correct and complete schema integration has to deal with integrity constraints existing in the di.erent schemas to be integrated. In this paper, we present a framework for the correct handling of integrity constraints during schema integration. In particular, we work out the correspondence between local integrity constraints and global extensional assertions. The knowledge about the correspondences between the underlying integrity constraints and extensional assertions can then be utilized for an augmented schema integration process.", "num_citations": "15\n", "authors": ["1503"]}
{"title": "A systematic literature review on the semi-automatic configuration of extended product lines\n", "abstract": " Product line engineering has become essential in mass customisation given its ability to reduce production costs and time to market, and to improve product quality and customer satisfaction. In product line literature, mass customisation is known as product configuration. Currently, there are multiple heterogeneous contributions in the product line configuration domain. However, a secondary study that shows an overview of the progress, trends, and gaps faced by researchers in this domain is still missing. In this context, we provide a comprehensive systematic literature review to discover which approaches exist to support the configuration process of extended product lines and how these approaches perform in practice. Extend product lines consider non-functional properties in the product line modelling. We compare and classify a total of 66 primary studies from 2000 to 2016. Mainly, we give an in-depth view of\u00a0\u2026", "num_citations": "14\n", "authors": ["1503"]}
{"title": "Hardware-Sensitive Scan Operator Variants for Compiled Selection Pipelines\n", "abstract": " The ever-increasing demand for performance on huge data sets forces database systems to tweak the last bit of performance out of their operators. Especially query compiled plans allow for several tuning opportunities that can be applied depending on the query plan and the underlying data. Apart from classical query optimization opportunities, it includes to tune the code using code optimizations for processor specifics, e.g., using Single Instruction Multiple Data processing or predication. In this paper, we examine code optimizations that can be applied for compiled scan pipelines that include aggregations, evaluate impact factors that influence the performance of the scan pipelines, and derive guidelines that a query compiler should implement to choose the best variant for a given query plan and workload.", "num_citations": "14\n", "authors": ["1503"]}
{"title": "Analyzing And Formalizing Dependencies In Generalized Transaction Structures\n", "abstract": " Modern information systems require advanced transaction models providing means to describe complex activities such as transactional workflows. Complex activities consist of sets of transactions which are interrelated, ie, there are dependencies among several transactions. We analyze different kinds of dependencies and present an extended formal framework (based on ACTA) for describing advanced transaction models. We introduce the notion of transaction closures as a generalization of nested transactions which captures different orthogonal dependencies such as abort and object visibility dependencies, and explicitly show how these dependencies work together. The formal framework is demonstrated by formalizing detached transactions known from the area of active database systems. 1 Introduction Advanced transaction models are the basis for realizing complex activities in information systems. Traditionally, such models base on the idea of nesting transactions. Nested transactions (Mos85...", "num_citations": "14\n", "authors": ["1503"]}
{"title": "Representation of the historical information necessary for temporal integrity monitoring\n", "abstract": " Temporal integrity constraints describe long-term data dependencies to be respected by correct database evolutions. Such temporal constraints can be monitored by a runtime evaluation of corresponding transitions of an equivalent finite automaton for each substitution of the free constraint variables with database objects. The current states of the automaton are the historical information necessary for temporal integrity monitoring. This paper presents techniques for decreasing the amount of historical information by monitoring automata for whole sets of substitutions instead of single substitutions thus enabling a monitoring even for large sets of monitored substitutions.", "num_citations": "14\n", "authors": ["1503"]}
{"title": "Toward efficient variant calling inside main-memory database systems\n", "abstract": " Mutations in genomes indicate predisposition for diseases or effects on efficacy of drugs. A variant calling algorithm determines possible mutations in sample genomes. Afterwards, scientists have to decide about the impact of these mutations. Certainly, many different variant calling algorithms exist that generate different outputs due to different sequence alignments as input and parameterizations of variant calling algorithms. Thus, a combination of variant calling results is necessary to provide a more complete set of mutations than single algorithm runs can provide. Therefore, a system is required that facilitates the integration and parameterization of different variant calling algorithms and processing of different sequence alignments. Moreover, against the backdrop of ever increasing amounts of available genome sequencing data, such a system must provide matured database management capabilities to enable\u00a0\u2026", "num_citations": "13\n", "authors": ["1503"]}
{"title": "An operator-stream-based scheduling engine for effective GPU coprocessing\n", "abstract": " Since a decade, the database community researches opportunities to exploit graphics processing units to accelerate query processing. While the developed GPU algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while over utilizing others. Therefore, an approach is needed that effectively distributes a workload on available (co-)processors while providing accurate performance estimations for the query optimizer. In this paper, we extend our hybrid query-processing engine with heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach doubles the throughput compared to our previous solution and state-of-the-art approaches, because of nearly equal device utilization while preserving accurate performance estimations.", "num_citations": "13\n", "authors": ["1503"]}
{"title": "Scalable varied density clustering algorithm for large datasets\n", "abstract": " Finding clusters in data is a challenging problem especially when the clusters are being of widely varied shapes, sizes, and densities. Herein a new scalable clustering technique which addresses all these issues is proposed. In data mining, the purpose of data clustering is to identify useful patterns in the underlying dataset. Within the last several years, many clustering algorithms have been proposed in this area of research. Among all these proposed methods, density clustering methods are the most important due to their high ability to detect arbitrary shaped clusters. Moreover these methods often show good noise-handling capabilities, where clusters are defined as regions of typical densities separated by low or no density regions. In this paper, we aim at enhancing the well-known algorithm DBSCAN, to make it scalable and able to discover clusters from uneven datasets in which clusters are regions of homogenous densities. We achieved the scalability of the proposed algorithm by using the k-means algorithm to get initial partition of the dataset, applying the enhanced DBSCAN on each partition, and then using a merging process to get the actual natural number of clusters in the underlying dataset. This means the proposed algorithm consists of three stages. Experimental results using synthetic datasets show that the proposed clustering algorithm is faster and more scalable than the enhanced DBSCAN counterpart.", "num_citations": "13\n", "authors": ["1503"]}
{"title": "Downsizing Data Management for Embedded Systems.\n", "abstract": " Data management functionality is not only needed in large scale database management systems, but also in embedded systems that are the predominant form of computing systems today. However, resource restrictions and heterogeneity of hardware complicate the development of data management solutions. In current practice, this typically leads to redevelopment of data management solutions since existing applications cannot be customized sufficiently. In this paper we describe our vision of tailor-made data management, based on a software product line approach, where data management software can be tailored to satisfy special requirements. We illustrate how such a software product line for data management functionality can be derived by downsizing existing database systems.", "num_citations": "13\n", "authors": ["1503"]}
{"title": "An adaptive eca-centric architecture for agile service-based business processes with compliant aspectual. net environment\n", "abstract": " In today's competitive business environment, enterprises businesses are subjected to continuously adapt change to ensure achieving targeted goals. As enterprise businesses are managed by enterprise information systems, adaptation to changes at information system level is important. It raises the need for architectures and mechanisms that support such adaptation at finer granularity. Rule-based information systems using service-oriented computing [1] and aspect-oriented [2] approach promises high adaptability in this domain. In this research we propose an adaptable ECA (Event-Condition-Action) centric architecture and implementation mechanism based on service-oriented computing and aspect-oriented programming for rule-based enterprise information systems ensuring high adaptability.", "num_citations": "13\n", "authors": ["1503"]}
{"title": "Integrating and Rapid-Prototyping UML Structural and Behavioural Diagrams Using Rewriting Logic\n", "abstract": " Although the diversity of UML diagrams provides users with different views of any complex software under development, in most cases system designers face challenging problems to keeping such diagrams coherently related. In this paper we propose to contribute to the tremendous efforts being undertaken towards rigorous and coherent views of UML-based modelling techniques. In this sense, we propose to integrate most of UML diagrams in a very smooth yet sound way. Moreover, by equipping such integration with an intrinsically concurrent and operational semantics, namely rewriting logic, we also provide validation by rapid-prototyping using Maude implementations. More precisely, the diagrams we propose to smoothly integrate include: object-and class-diagrams with their related object constraints (using OCL), statecharts and life-cycle diagrams. The integration of such diagrams is based on very\u00a0\u2026", "num_citations": "13\n", "authors": ["1503"]}
{"title": "Towards an object Petri nets model for specifying and validating distributed information systems\n", "abstract": " We present first results towards a tailored conceptual model for advanced distributed information systems regarded as open reactive and distributed systems with large databases and application programs. The proposed model, referred to as CO-Nets, is based on a complete integration of object oriented concepts with some constructions from semantical data modeling into an appropriate variant of algebraic Petri Nets named ECATNets. The CO-Nets behaviour is interpreted into rewriting logic. Particularly, it is shown how CO-Nets promote incremental construction of complex components, regarded as a hierarchy of classes, through simple and multiple inheritance (with redefinition, associated polymorphism and dynamics binding). Each component behaves with respect to an appropriate intra-component evolution pattern that supports intra- as well as inter-object concurrency. On the other hand, we\u00a0\u2026", "num_citations": "13\n", "authors": ["1503"]}
{"title": "Extending transaction closures by n-ary termination dependencies\n", "abstract": " Transaction dependencies have been recognized as a valuable method in describing restrictions on the executions of sets of transactions. A transaction closure is a generalized transaction structure consisting of a set of related transactions which are connected by special dependencies. Traditionally, relationships between transactions are formulated by binary dependencies. However, there are applications scenarios where dependencies must be specified among more than two transactions. Since n-ary dependencies cannot be expressed by binary dependencies, appropriate extensions are required. In this paper, we extend the concept of transaction closure by ternary termination dependencies. We show how n-ary termination dependencies can be expressed by binary and ternary termination dependencies. As a result, we present rules for reasoning about the combination of these termination\u00a0\u2026", "num_citations": "13\n", "authors": ["1503"]}
{"title": "Object-Oriented Database Design: What is the Difference with Relational Database Design?\n", "abstract": " Object-oriented database design is not only a simple extension of relational database design. By modelling structure as well as behaviour of real world entities as coherent units, object-oriented database design succeeds in capturing more semantics of applications already in the design phase. The use of object-oriented concepts like inheritance allows an adequate modelling and a better application implementation based on an object-oriented database system. However, the results of object-oriented design can also be applied to classical database systems. Keywords: object-oriented database design, relational database design, semantics of applications. 1 Introduction In this paper we clearly point out the main differences between object-oriented database design and relational database design. There is an amazing confusion about the meaning of object-oriented database design which is often mixed up with object-oriented modelling (or specification), with object-oriented analysis, or object...", "num_citations": "13\n", "authors": ["1503"]}
{"title": "Monitoring temporal permissions using partially evaluated transition graphs\n", "abstract": " The conceptual design of a database application includes not only the design of its static structure but also the design of its dynamic behaviour. Temporal permissions are one of several concepts which can be used to specify dynamic database behaviour. They are used as enabling conditions for the occurrence of specific state-changing operations. Temporal permissions refer to the whole past life cycle of a database and are therefore specified in a past tense temporal logic. An efficient method for monitoring past tense temporal formulae, presented in this paper, avoids storing the whole past life cycle of the database. Instead, past tense temporal formulae are partially evaluated during database runtime using special evaluation schemes (so-called transition graphs). Necessary information about the database history is represented as a non-temporal predicate logic formula. The actual evaluation of a past\u00a0\u2026", "num_citations": "13\n", "authors": ["1503"]}
{"title": "ETL best practices for data quality checks in RIS databases\n", "abstract": " The topic of data integration from external data sources or independent IT-systems has received increasing attention recently in IT departments as well as at management level, in particular concerning data integration in federated database systems. An example of the latter are commercial research information systems (RIS), which regularly import, cleanse, transform and prepare the analysis research information of the institutions of a variety of databases. In addition, all these so-called steps must be provided in a secured quality. As several internal and external data sources are loaded for integration into the RIS, ensuring information quality is becoming increasingly challenging for the research institutions. Before the research information is transferred to a RIS, it must be checked and cleaned up. An important factor for successful or competent data integration is therefore always the data quality. The removal of data errors (such as duplicates and harmonization of the data structure, inconsistent data and outdated data, etc.) are essential tasks of data integration using extract, transform, and load (ETL) processes. Data is extracted from the source systems, transformed and loaded into the RIS. At this point conflicts between different data sources are controlled and solved, as well as data quality issues during data integration are eliminated. Against this background, our paper presents the process of data transformation in the context of RIS which gains an overview of the quality of research information in an institution\u2019s internal and external data sources during its integration into RIS. In addition, the question of how to control and improve the quality\u00a0\u2026", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Integration of FPGAs in database management systems: Challenges and opportunities\n", "abstract": " In the presence of exponential growth of the data produced every day in volume, velocity, and variety, online analytical processing (OLAP) is becoming increasingly challenging. FPGAs offer hardware reconfiguration to enable query-specific pipelined and parallel data processing with the potential of maximizing throughput, speedup as well as energy and resource efficiency. However, dynamically configuring hardware accelerators to match a\u00a0given OLAP query is a\u00a0complex task. Furthermore, resource limitations restrict the coverage of OLAP operators. As a\u00a0consequence, query optimization through partitioning the processing onto components of heterogeneous hardware/software systems seems a\u00a0promising direction. While there exists work on operator placement for heterogeneous systems, it mainly targets systems combining multi-core CPUs with GPUs. However, an inclusion of FPGAs, which uniquely\u00a0\u2026", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Exploring the Design Space of a GPU-Aware Database Architecture\n", "abstract": " The vast amount of processing power and memory bandwidth provided by modern graphics cards make them an interesting platform for data-intensive applications. Unsurprisingly, the database research community has identified GPUs as effective co-processors for data processing several years ago. In the past years, there were many approaches to make use of GPUs at different levels of a database system. In this paper, we summarize the major findings of the literature on GPU-accelerated data processing. Based on this survey, we present key properties, important trade-offs and typical challenges of GPU-aware database architectures, and identify major open research questions.", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Service variability patterns\n", "abstract": " Service-oriented computing (SOC) increases flexibility of IT systems and helps enterprises to meet their changing needs. Different methods address changing requirements in service-oriented environment. Many solutions exist to address variability, however, each solution is tailored to a specific problem, e.g. at one specific layer in SOC. We survey variability mechanisms from literature and summarize solutions, consequences, and possible combinations in a pattern catalogue. Based on the pattern catalogue, we compare different variability patterns and their combinations. Our catalogue helps to choose an appropriate technique for the variability problem at hand and illustrates its consequences in SOC.", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Logics for emerging applications of databases\n", "abstract": " In this era of heterogeneous and distributed data sources, ranging from semistructured documents to knowledge about coordination processes or workflows, logic provides a rich set of tools and techniques with which to address the questions of how to represent, query and reason about complex data. This book provides a state-of-the-art overview of research on the application of logic-based methods to information systems, covering highly topical and emerging fields: XML programming and querying, intelligent agents, workflow modeling and verification, data integration, temporal and dynamic information, data mining, authorization, and security. It provides both scientists and graduate students with a wealth of material and references for their own research and education.", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Dynamically evolving concurrent information systems specification and validation: a component-based Petri nets proposal\n", "abstract": " Besides the steady growing of size-complexity and distribution of present-day information systems, business volatility with rapid changes in users' wishes and technological upgrading are stressing an overwhelmingly need for more advanced conceptual modeling approaches. Such advanced conceptual models should coherently and soundly reflect these three crucial dimensions, namely the size, space and (evolution over) time dimensions. In contribution towards such advanced conceptual approaches, we presented in [Data Know. Eng. 42 (2)(2002) 143] a new form of integration of object-orientation with emphasize on componentization into a variety of algebraic Petri nets, we referred to as C o-nets. The purpose of the present paper is to soundly extend this proposal for coping with dynamic changing of structural and behavioral aspects of C o-nets components. To this aim, we are proposing an adequate Petri\u00a0\u2026", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Distributed temporal logic for concurrent object families\n", "abstract": " A global information infrastructure is rapidly developing. As a consequence, interest is shifting from local systems to global, widely distributed systems. This implies a growing need for advanced modeling and speci cation techniques for distributed systems, both for designing new systems and for reengineering old ones.We are concerned with information systems, ie, reactive systems with the capability of storing and retrieving large amounts of data. Distributed reactive systems are more complex and di cult to specify, analyse, implement and recon gure than other kinds of systems. They are not well understood, and methods for designing, implementing and reengineering them did not yet reach sound engineering practice.", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Animation support for a conceptual modelling language\n", "abstract": " The first phase in developing information systems is often called conceptual modelling phase. The model constructed is the first formal document describing the desired system. The conceptual model must deal with structural and behavioural aspects of the world. The complexity of such a model often prevents a detailed understanding by the later users. One way to estimate if (s)he really gets what (s)he wants are prototype systems constructed by conventional programming. We propose a way to construct prototypes from formal specifications by means of transformation into a kernel language that can be executed in a suitable distributed runtime environment. We introduce the modelling language, the basic execution mechanism, and the basic architecture of the runtime system.", "num_citations": "12\n", "authors": ["1503"]}
{"title": "Are databases fit for hybrid workloads on GPUs? A storage engine's perspective\n", "abstract": " Employing special-purpose processors (e.g., GPUs) in database systems has been studied throughout the last decade. Research on heterogeneous database systems that use both general-and special-purpose processors has addressed either transaction-or analytic processing, but not the combination of them. Support for hybrid transaction-and analytic processing (HTAP) has been studied exclusively for CPU-only systems. In this paper we ask the question whether current systems are ready for HTAP workload management with cooperating general- and special-purpose processors. For this, we take the perspective of the backbone of database systems: the storage engine. We propose a unified terminology and a comprehensive taxonomy to compare state-of-the-art engines from both domains. We show similarities and differences, and determine a necessary set of features for engines supporting HTAP workload\u00a0\u2026", "num_citations": "11\n", "authors": ["1503"]}
{"title": "Toward GPU-accelerated database optimization\n", "abstract": " For over three decades, research investigates optimization options in DBMSs. Nowadays, the hardware used in DBMSs become more and more heterogeneous, because processors are bound by a fixed energy budget leading to increased parallelism. Existing optimization approaches in DBMSs do not exploit parallelism for a single optimization task and, hence, can only benefit from the parallelism offered by current hardware by batch-processing multiple optimization tasks.               Since a large optimization space often allows us to process sub-spaces in parallel, we expect large gains in result quality for optimization approaches in DBMSs and, hence, performance for query processing on modern (co-)processors. However, parallel optimization on CPUs is likely to slow down query processing, because DBMSs can fully exploit the CPUs computing resources due to high data parallelism. In contrast, the\u00a0\u2026", "num_citations": "11\n", "authors": ["1503"]}
{"title": "A layered architecture for enterprise data warehouse systems\n", "abstract": " The architecture of Data Warehouse systems is described on basis of so-called reference architectures. Today\u2019s requirements to Enterprise Data Warehouses are often too complex to be satisfactorily achieved by the rather rough descriptions of this reference architecture. We describe an architecture of dedicated layers to face those complex requirements, and point out additional expenses and resulting advantages of our approach compared to the traditional one.", "num_citations": "11\n", "authors": ["1503"]}
{"title": "Data quality on the Web\n", "abstract": " Although techniques for managing, querying, and integrating data on the Web have significantly matured over the last few years, well-founded and applicable approaches to determine or even to guarantee a certain degree of quality of the data are still missing. Reasons for this include in particular the lack of common, agreed-upon models of quality measurements and the difficulty of handling quality information during data integration and query processing. The problem of data quality arises in many scenarios, eg, during the integration of business or scientific data, in Web mining, data dissemination, and in particular in querying the Web using search and meta-search engines. Furthermore, it affects various kinds of data, such as structured and semistructured data, text documents as well as streaming data. Information about data quality is becoming more and more important since it provides some kind of yardstick describing the value and reliability of (possibly heterogeneous) forms of distributed or integrated data. The aim of this seminar was to foster collaboration among researchers from different areas working on problems related to data quality. This included but was not limited to data integration, information retrieval (particularly search engines), scientific data warehousing and applications domains from the computational sciences and bioinformatics. In all these areas, data quality plays a crucial role and therefore different specific solutions have been developed. Sharing and exchanging this knowledge could result in significant synergy effects.", "num_citations": "11\n", "authors": ["1503"]}
{"title": "Consistency management in object\u2010oriented databases\n", "abstract": " The paper presents concepts and ideas underlying an approach for consistency management in object\u2010oriented (OO) databases. In this approach constraints are considered as first class citizens and stored in a meta\u2010database called constraints catalog. When an object is created constraints of this object are retrieved from the constraints catalog and relationships between these constraints and the object are established. The structure of constraints has several features that enhance consistency management in OO database management systems which do not exist in conventional approaches in a satisfactory way. This includes: monitoring object consistency at different levels of update granularity, integrity independence, and efficiency of constraints maintenance; controlling inconsistent objects; enabling and disabling constraints, globally to all objects or locally to individual objects; and declaring constraints on\u00a0\u2026", "num_citations": "11\n", "authors": ["1503"]}
{"title": "Design Support for Database Federations\n", "abstract": " Federated database systems provide a homogeneous interface to possibly heterogeneous local database systems. This homogeneous interface consists of a global schema which is the result of a logical integration of the schemata of the corresponding local database systems and file systems. In this paper, we sketch the integration process and a set of tools for supporting the design process. Besides the classical database schema integration, the design process for federated information systems requires the integration of other aspects like integrity rules, authorization policies and transactional processes. This paper reports on an integrated approach to tool support of several of these integration aspects. The different integration facets are linked via the database integration method GIM allowing a high degree of automatic integration steps.", "num_citations": "11\n", "authors": ["1503"]}
{"title": "From object specification towards agent design\n", "abstract": " Nowadays, object specification technology is successfully used for modelling information systems. However, object-oriented modelling cannot cover all aspects (e.g. dynamic behaviour evolution) of such systems. A promising approach to get a grasp of these additional requirements for modelling information systems is the specification of systems as communicating, cooperating and autonomous agents. In the present paper we sketch an evolutionary extension of object specification towards agent modelling. Here, we have to emphasize that this paper is not intended to offer perfect solutions. Our aim is to discuss and fix a research agenda for a formal agent approach which promises to provide a semantically richer framework for the specification of information systems.", "num_citations": "11\n", "authors": ["1503"]}
{"title": "Abstract data type semantics for many-sorted object query algebras\n", "abstract": " Traditional database query algebras usually have only one special sort to describe database and result structures, for example the sort \u2018relation\u2019 in relational algebra. For the new developments concerning object-oriented and extensible database systems, it seems to be more appropriate to define a many-sorted algebra based on several primitive domain algebras and type constructors like sets or lists. In this paper, we present the denotational semantics of QUAL expressions using an abstract data type framework. QUAL is a many-sorted query algebra defined as a query formalism for the structural part of the OBLOG object model. The abstract data type semantics of QUAL allows easy extension of the structural part of OBLOG models by new domain data types and new type constructors.", "num_citations": "11\n", "authors": ["1503"]}
{"title": "Konzeptioneller Entwurf von Objektgesellschaften\n", "abstract": " Die konzeptionelle Modellierung des Weltausschnitts, der durch eine Datenbank oder ein Informationssystem dargestellt werden soll, ist die entscheidende Phase beim Systementwurf, da das konzeptionelle Modell die Grundlage der Implementierung ist. In diesem Papier schlagen wir die Sprache Oblog                 + zur objektorientierten Spezifikation von Informationssystemen vor. Oblog                 + erm\u00f6glicht die vollst\u00e4ndige Darstellung des Weltausschnitts durch integrierte Beschreibung von Daten \u00fcber Objekte, Manipulationen dieser Daten und der zeitlichen Entwicklung von Objekten sowie der vielf\u00e4ltigen Beziehungen zwischen Objekten. Die Semantik der Sprache ist definiert \u00fcber einem mathematischen Objektmodell.", "num_citations": "11\n", "authors": ["1503"]}
{"title": "Interactive chord visualization for metaproteomics\n", "abstract": " Metaproteomics is an analytic approach to research microorganisms that live in complex microbial communities. A key aspect of understanding microbial communities is to link the functions of proteins identified by metaproteomics to their taxonomy. In this paper we demonstrate the interactive chord visualization as a powerful tool to explore such data. To evaluate the tools efficacy, we use the relation data between functions and taxonomies from a large metaproteomics experiment. We evaluated the work flow in comparison to previous methods of data analysis and showed that interactive exploration of data using the chord diagram is significantly faster in four of five tasks. Therefore, the chord visualization improves the user's ability to discover complex biological relationships.", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Efficient mutation testing in configurable systems\n", "abstract": " Mutation testing is a technique to evaluate the quality of test cases by assessing their ability to detect faults. Mutants are modified versions of the original program that are generated automatically and should contain faults similar to those caused by developers' mistakes. For configurable systems, existing approaches propose mutation operators to produce faults that may only exist in some configurations. However, due to the number of possible configurations, generating and testing all mutants for each program is not feasible. To tackle this problem, we discuss to use static analysis and adopt the idea of T-wise testing to limit the number of mutants. In particular, we i) discuss dependencies that exist in configurable systems, ii) how we can use them to identify code to mutate, and iii) assess the expected outcome. Our preliminary results show that variability analysis can help to reduce the number of mutants and, thus\u00a0\u2026", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Challenges in Finding an Appropriate Multi-Dimensional Index Structure with Respect to Specific Use Cases\n", "abstract": " In recent years, index structures for managing multi-dimensional data became increasingly important. Due to heterogeneous systems and specific use cases, it is a complex challenge to find an appropriate index structure for specific problems, such as finding similar fingerprints or micro traces in a database. One aspect that should be considered in general is the dimensionality and the related curse of dimensionality. However, dimensionality of data is just one component that have to be considered. To address the challenges of finding the appropriate index, we motivate the necessity of a framework to evaluate indexes for specific use cases. Furthermore, we discuss core components of a framework that supports users in finding the most appropriate index structure for their use case.", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Supporting Program Comprehension in Large Preprocessor-Based Software Product Lines\n", "abstract": " Software product line (SPL) engineering provides an effective mechanism to implement variable software. However, using preprocessors to realise variability, which is typical in industry, is heavily criticised, because it often leads to obfuscated code. Using background colours to highlight code annotated with preprocessor statements to support comprehensibility has proved to be effective, however, scalability to large SPLs is questionable. The authors\u2019 aim is to implement and evaluate scalable usage of background colours for industrial-sized SPLs. They designed and implemented scalable concepts in a tool called FeatureCommander. To evaluate its effectiveness, the authors conducted a controlled experiment with a large real-world SPL with over 99\u2005000 lines of code and 340 features. They used a within-subjects design with treatment colours and no colours. They compared correctness and response time of\u00a0\u2026", "num_citations": "10\n", "authors": ["1503"]}
{"title": "The Pervasive Nature of Variability in SOC\n", "abstract": " Service-oriented computing has gained attention from researchers and industry due to various benefits. Enterprises are supported by SOC-based solutions. Enterprise requirements change with the customer needs, market conditions, and variability is often needed in SOC. Research usually focuses at variability on the business process level and ignores variability at other layers. In this paper, we show what kind of variability can occur and how it spans across all layers of SOC. We present some example scenarios to discuss different kinds of variability and their reasons in SOC environments. Finally, we classify variability to show that variability is not an isolated problem in a single layer.", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Dcbor: a density clustering based on outlier removal\n", "abstract": " Data clustering is an important data exploration technique with many applications in data mining. We present an enhanced version of the well known single link clustering algorithm. We will refer to this algorithm as DCBOR. The proposed algorithm alleviates the chain effect by removing the outliers from the given dataset. So this algorithm provides outlier detection and data clustering simultaneously. This algorithm does not need to update the distance matrix, since the algorithm depends on merging the most k-nearest objects in one step and the cluster continues grow as long as possible under specified condition. So the algorithm consists of two phases; at the first phase, it removes the outliers from the input dataset. At the second phase, it performs the clustering process. This algorithm discovers clusters of different shapes, sizes, densities and requires only one input parameter; this parameter represents a threshold for outlier points. The value of the input parameter is ranging from 0 to 1. The algorithm supports the user in determining an appropriate value for it. We have tested this algorithm on different datasets contain outlier and connecting clusters by chain of density points, and the algorithm discovers the correct clusters. The results of our experiments demonstrate the effectiveness and the efficiency of DCBOR.", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Combining effectiveness and efficiency for schema matching evaluation\n", "abstract": " Schema matching plays a central role in many applications that require interoperability among heterogeneous data sources. A good evaluation for different capabilities of schema matching systems has become vital as the complexity of such systems arises. The capabilities of matching systems incorporate different (possibly conflicting) aspects among them match quality and match efficiency. The analysis of efficiency of a schema matching system, if it is done, tends to be done in a way separate from the analysis of effectiveness. In this paper, we present the trade-off between schema matching effectiveness and efficiency as a multi-objective optimization problem. This representation enables us to obtain a combined measure as a compromise between them. We combine both performance aspects in a weighted-average function to determine the cost-effectiveness of a schema matching system. We apply our\u00a0\u2026", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Federation services for heterogeneous digital libraries accessing cooperative and non-cooperative sources\n", "abstract": " Today, bibliographical information is kept in a variety of digital libraries available on the Internet. The integration of bibliographical data is considered as one of the most important tasks in the area of digital library community. The available sources of data vary widely in terms of data representation and access interfaces. To overcome this heterogeneity during the last years attempts were made to apply methods developed for information system integration. We describe our approach of a federation service for digital libraries using the loosely coupled federated system FRAQL that offers a variety of conflict resolution mechanisms. Furthermore, we present different kinds of adapters for accessing cooperative and noncooperative sources of bibliographical information. In order to integrate systems with limited query capabilities we introduce concepts for source descriptions.", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Schema derivation for WWW information sources and their integration with databases in bioinformatics\n", "abstract": " In this paper we discuss first experiences and results of current work on the BioBench, an integrated information system for Bioinformatics. Since the major part of Bioinformatic data is distributed in many heterogeneous systems all over the world one has to deal with problems of integration of heterogeneous systems. Especially semi-structured data, presented via WWW-interfaces has to be integrated. Therefore, we focus on the aspects of acquisition, integration and management of the data for the BioBench. First we give a short motivation of the project and an overview of the system. In the main follows a discussion of schema derivation for the WWW-interfaces. Thereby, we discuss the application of domain knowledge and automatic grammar generation. Finally we briefly describe an automatic wrapper generation approach, supporting high quality wrappers as well as wrapper modification according to local\u00a0\u2026", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Deriving relationships between integrity constraints for schema comparison\n", "abstract": " Schema comparison is essential for integrating different database schemata. Since the semantics of a schema is also represented by its integrity constraints, they must be considered by a correct schema comparison method. Especially the extensional relationships between classes are determined by the relationship between the corresponding integrity constraint sets. In this paper, we work out the relationships between different types of integrity constraints. As a result, we present rule for comparing integrity constraint sets. These rules can be used after a schema conforming step, where naming, type and structural conflicts are solved, to exactly fix the extensional relationship between object classes.", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Deriving liveness goals from temporal logic specifications\n", "abstract": " A propositional temporal logic is brie y introduced and its use for reactive systems speci cation is motivated and illustrated. G-automata are proposed as a new operational semantics domain designed to cope with fairness/liveness properties. G-automata are a class of labelled transition systems with an additional structure of goal achievement which forces the eventual ful lment of every pending goal. An algorithm is then presented, that takes a nite system speci cation as input and that, by a stepwise tableaux analysis method, builds up a canonical G-automaton matching the speci cation. Eventuality formulae correspond to goals of the automaton their satisfaction being thus assured. The direct execution of G-automata, and consequently of speci cations, is then discussed and suggested as an alternative approach to the execution of propositional temporal logic. A short overview of the advantages of applying the techniques to the speci c eld of database monitoring is presented.", "num_citations": "10\n", "authors": ["1503"]}
{"title": "Memory management strategies in CPU/GPU database systems: A survey\n", "abstract": " GPU-accelerated in-memory database systems have gained a lot of popularity over the last several years. However, GPUs have limited memory capacity, and the data to process might not fit into the GPU memory entirely and cause a memory overflow. Fortunately, this problem has many possible solutions, like splitting the data and processing each portion separately, or storing the data in the main memory and transferring it to the GPU on demand. This paper provides a survey of four main techniques for managing GPU memory and their applications for query processing in cross-device powered database systems.", "num_citations": "9\n", "authors": ["1503"]}
{"title": "Getting rid of clone-and-own: moving to a software product line for temperature monitoring\n", "abstract": " Due to its fast and simple applicability, clone-and-own is widely used in industry to develop software variants. In cooperation with different companies for thermoelectric products, we implemented multiple variants of a heat monitoring tool based on clone-and-own. After encountering redundancy-related problems during development and maintenance, we decided to migrate towards a software product line. Within this paper, we describe this case study of migrating cloned variants to a software product line based on the extractive approach. The resulting software product line encapsulates variability on several levels, including the underlying hardware systems, interfaces, and use cases. Currently, we support monitoring hardware from three different companies that use the same core system and provide a configurable front-end. We share our experiences and encountered problems with cloning and migration\u00a0\u2026", "num_citations": "9\n", "authors": ["1503"]}
{"title": "A decision model to select the optimal storage architecture for relational databases\n", "abstract": " Requirements for database systems differ from small-scale database programs for embedded devices with minimal footprint to large-scale on-line analytical processing applications. For relational database management systems, two storage architectures have been introduced: a) row-oriented architecture and b) column-oriented architecture. In this paper, we present a query decomposition approach to evaluate database operations with respect to their performance according to the storage architecture. We map decomposed queries to workload patterns which contain aggregated database statistics. Further, we develop our complementary decision models which advise the selection of the optimal storage architecture for a given application domain. The first decision model improves the performance of running systems (on-line). The second and third decision model advise an efficient database design or decide\u00a0\u2026", "num_citations": "9\n", "authors": ["1503"]}
{"title": "Representing and Composing First-class Features with FeatureJ\n", "abstract": " Software product lines (SPLs) enable creating product families, set of products that differ in terms of features. Traditionally, techniques for implementing features have sided with one of the two views of features: as distinguishable characteristics or as increments/changes in program functionality of the software under consideration. We argue that in order to realize the full potential of features as a separation of concerns mechanism or a modularity mechanism in its own right, both of these views must be supported by the representation of features. Furthermore, the composition of such uniformly represented features should be streamlined so that both can evolve together. Towards this end, we present FeatureJ, an implementation technique that integrates features, variants, and product lines as first-class entities, namely types, in the Java programming language. We review the trends in the representation and composition of features in the current implementation techniques and arrive at a set of requirements to represent features as first-class entities.", "num_citations": "9\n", "authors": ["1503"]}
{"title": "Fuzzy constraint-based schema matching formulation\n", "abstract": " \u041d\u041a \u0421\u0432\u0438\u0436\u0433 \u0439 \u0438 \u0433\u0432\u041a \u042c \u0434 \u042f \u0414 \u0430\u0437\u0433 \u0432\u0433\u043b\u0432 \u0437 \u0434\u0432 \u0438 \u0433\u0436 \u0438 \u0432 \u042f \u0415 \u0436 \u0436\u0437 \u0438\u0433 \u0438 \u042f\u0433\u0436\u0430 \u042f \u042f \u0433\u0432\u0438 \u0432\u0438 \u0438 \u0438 \u0437 \u0432\u0433\u0438 \u0434 \u0436\u0438 \u0433 \u0438 \u0437\u0439\u0436 \u042f \u041a \u0421\u0438 \u0437 \u0437\u0438 \u0431 \u0438 \u0438 \u0438 \u0438 \u0434 \u042f \u0437 \u0437 \u043a \u0436 \u0430 \u0433\u0436 \u0436\u0437 \u0433 \u0431 \u0432 \u0438\u0439 \u0430 \u0436 \u0436 \u0438 \u0432 \u0438 \u0437\u0439\u0436 \u042f\u2104 \u041a \u0437 \u0438 \u0432\u0439\u0431 \u0436 \u0433 \u0434 \u042f \u0437\u0433\u0439\u0436 \u0437 \u0437 \u0432 \u0432 \u0436 \u0437 \u0432 \u0437 \u0438 \u0433\u0436\u0438\u0437 \u0432 \u0438\u0433 \u0432 \u0430 \u0439\u0437 \u0436\u0437 \u0438\u0433 \u043c\u0434\u0430\u0433\u0436 \u0432 \u0432\u0438 \u0436 \u0438 \u0438 \u0437 \u0437\u0433\u0439\u0436 \u0437 \u0433\u0431 \u0437\u0437 \u0432\u0438 \u0430\u041a \u0437 \u0436 \u0437\u0439\u0430\u0438 \u0437\u0433 \u0438\u043b \u0436 \u0437\u043d\u0437\u0438 \u0431\u0437 \u043a \u0432 \u043a \u0430\u0433\u0434 \u0438\u0433 \u0433\u0434 \u0432 \u0438 \u0434 \u042f \u0438\u0433 \u0439\u0437 \u0436\u0437\u041a \u042b \u0431 \u0431 \u0438 \u0432 \u0437 \u0438 \u0433\u0436 \u0438 \u0437 \u0433 \u0438 \u0437 \u0437\u043d\u0437\u0438 \u0431\u0437\u041a \u042b \u0431 \u0431 \u0438 \u0432 \u0437 \u0438 \u0438 \u0437 \u0433 \u0432\u0438 \u043d \u0432 \u0437 \u0431 \u0432\u0438 \u0433\u0436\u0436 \u0437\u0434\u0433\u0432 \u0432 \u0437 \u0431\u0433\u0432 \u0430 \u0431 \u0432\u0438\u0437 \u0433 \u0438\u043b\u0433 \u0433\u0436 \u0431\u0433\u0436 \u0437 \u0431 \u0437\u041a \u0421\u0438 \u0434\u0430 \u043d\u0437 \u0432\u0438\u0436 \u0430 \u0436\u0433\u0430 \u0432 \u0431 \u0432\u043d \u0438 \u0434\u0434\u0430 \u0438 \u0433\u0432 \u0437 \u0432 \u0436 \u0433\u0437 \u041e\u041e\u0418 \u041d\u2104 \u0432 \u0438 \u0432\u0438 \u0436 \u0438 \u0433\u0432\u0418 \u0438\u0433 \u0432\u0438 \u043d \u0432 \u0436 \u0438 \u0436 \u043e \u0432\u0438 \u0436\u0419\u0437 \u0431 \u0436 \u0430 \u0438 \u0433\u0432\u0437 \u0434\u0437 \u0438\u043b \u0432 \u0431\u0439\u0430\u0438 \u0434\u0430 \u0414 \u0438 \u0436\u0433 \u0432 \u0433\u0439\u0437\u0415 \u0437 \u0431 \u0437 \u0432 \u0438 \u043b \u0436 \u0433\u0439\u0437 \u0432\u0418 \u0438\u0433 \u0431 \u0434 \u0438 \u0437\u0433\u0439\u0436 \u0437 \u0438\u0433 \u043b \u0436 \u0433\u0439\u0437 \u0437 \u0431 \u0432 \u0419 \u0439\u0437 \u0432 \u0437\u0437\u0418 \u0438\u0433 \u0430\u0434 \u0438\u0433 \u0431 \u0434 \u0431 \u0437\u0437 \u0437 \u0438\u043b \u0432 \u0436 \u0432\u0438 \u0425\u0424 \u0433\u0436\u0431 \u0438\u0437 \u0432 \u0438 \u042b \u0431 \u0432\u0438 \u042f \u0418 \u0438\u0433 \u0437\u0438 \u0430 \u0437 \u0437 \u0431 \u0432\u0438 \u0433\u0436\u0436 \u0437\u0434\u0433\u0432 \u0432 \u0437 \u0438\u043b \u0432 \u0433\u0432 \u0434\u0438\u0437 \u0433 \u0436 \u0432\u0438 \u043b \u0437 \u0438 \u0437 \u0433\u0432\u0438\u0433\u0430\u0433 \u0437 \u0432 \u0432 \u0438 \u0431 \u0436 \u0438 \u0433\u0432\u0418 \u0438\u0433 \u0431 \u0436 \u0438 \u0430 \u043d \u0438 \u0436\u0433\u0431 \u0431\u0439\u0430\u0438 \u0434\u0430 \u0437\u0433\u0439\u0436 \u0437 \u0432\u0438\u0433 \u0432 \u043b \u0433\u0432 \u041d\u041c\u2104 \u041a \u0439 \u0438\u0433 \u0438 \u0433\u0431\u0434\u0430 \u043c \u0438\u043d \u0433 \u0437 \u0431 \u0431 \u0438 \u0432 \u0418 \u0438 \u043b \u0437 \u0431\u0433\u0437\u0438\u0430\u043d \u0434 \u0436 \u0433\u0436\u0431 \u0431 \u0432\u0439 \u0430\u0430\u043d \u043d \u0439\u0431 \u0432 \u043c\u0434 \u0436\u0438\u041a \u0420\u0433\u043b \u043a \u0436\u0418 \u0431 \u0432\u0439 \u0430 \u0436 \u0433\u0432 \u0430 \u0438 \u0433\u0432 \u0438 \u0432 \u0437 \u0438\u0433 \u0437\u0430\u0433\u043b \u0432 \u0432 \u0432\u0438 \u0434\u0436\u0433 \u0437\u0437 \u0437\u0434 \u0430\u0430\u043d \u0432 \u0430 \u0436 \u0419\u0437 \u0430 \u0432 \u043d\u0432 \u0431 \u0432\u043a \u0436\u0433\u0432\u0419 \u0431 \u0432\u0438\u0437\u041a \u042c \u0436 \u0433\u0436 \u0418 \u0438 \u0432 \u0433\u0436 \u0439\u0438\u0433\u0431 \u0438 \u0437 \u0431 \u0431 \u0438 \u0432 \u0437 \u0433\u0431 \u0437\u0437 \u0432\u0438 \u0430\u041a \u0433\u0432\u0437 \u0435\u0439 \u0432\u0438\u0430\u043d\u0418 \u0431 \u0432\u043d \u0437 \u0431 \u0431 \u0438 \u0432 \u0437\u043d\u0437\u0438 \u0431\u0437 \u043a \u0432 \u043a \u0430\u0433\u0434 \u0433\u0436 \u0439\u0438\u0433\u0431 \u0438 \u0432 \u0438 \u0434\u0436\u0433 \u0437\u0437\u0418 \u0437\u0439 \u0437 \u0439\u0434 \u041d\u2104 \u0418 \u0427\u0425 \u041b \u0427\u0425 \u0417\u0417 \u0418 \u041d\u2104 \u0418 \u0424\u042b\u2104 \u0418 \u042b \u0431 \u0430 \u0436 \u0438\u043d \u0430\u0433\u0433 \u0432 \u041e\u041c\u2104 \u0418 \u0427\u0432\u0438\u0433 \u0439 \u0430 \u0436 \u041d\u041f\u2104 \u0418 \u0429\u0427\u0425 \u041d\u041e\u2104 \u0418 \u042c\u0436 \u0425 \u0438 \u041d\u041d\u2104 \u0418 \u042b\u0419\u0425 \u0438 \u041d\u2104 \u0418 \u0432 \u042b\u0434 \u043d \u041f\u2104 \u041a \u0425 \u0432\u0439 \u0430 \u0437 \u0431 \u0432\u0438 \u0431 \u0438 \u0432 \u0433\u043a \u0436 \u0433\u0431 \u0437 \u0431 \u0437\u0431 \u0438 \u0437 \u043b \u043c \u0437\u0438 \u0432 \u0430 \u0431 \u0432\u0438 \u0432 \u0431 \u0437 \u0432 \u0430\u0437\u0433 \u0436 \u0432\u0438 \u0438 \u0437 \u0438\u043b \u0432 \u0436 \u0432 \u0437 \u0433 \u0433\u0431 \u0432\u0437\u041a \u0420 \u0432 \u0418 \u043b \u0433\u0439\u0430 \u0437\u0437\u0439\u0431 \u0438 \u0438 \u0431 \u0432\u0439 \u0430 \u0431 \u0438 \u0432 \u0437 \u0434 \u0436 \u0438 \u0434\u0436\u0433 \u0437\u0437\u041a \u0427\u0432 \u0438 \u0433\u0438 \u0436 \u0432 \u0418 \u0439\u0438\u0433\u0431 \u0438 \u0431 \u0438 \u0432 \u0431 \u043d \u0436\u0436\u043d \u043b \u0438 \u0438 \u0436 \u0433 \u0439\u0432 \u0436\u0438 \u0432\u0438\u043d\u0418 \u0437 \u0438 \u0437 \u0437 \u0433\u0432 \u0437\u043d\u0432\u0438 \u0438 \u0418 \u0436 \u0438 \u0436 \u0438 \u0432 \u0437 \u0431 \u0432\u0438 \u0418 \u0431 \u0432\u0437\u041a \u0439\u0436\u0438 \u0436\u0431\u0433\u0436 \u0418 \u0436 \u0432\u0438\u0430\u043d\u0418 \u0438 \u0436 \u0437 \u0432 \u0436 \u0432 \u043b \u0432\u0438 \u0436 \u0437\u0438 \u0432 \u0439 \u0430 \u0432 \u0438 \u0437 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0438 \u0438 \u0432 \u0430 \u0439\u0432\u0419 \u0436\u0438 \u0432 \u0438 \u0432 \u0434\u0436 \u0432 \u0434\u0430 \u043b \u043d\u2104 \u041a \u0420 \u0432 \u0437 \u0433\u0436\u0438 \u0436 \u0432\u0438 \u0433\u0439\u0438 \u0438 \u0436 \u0430 \u0438 \u0433\u0432\u0437 \u0434 \u0438\u043b \u0432 \u0438 \u0437 \u0437 \u0438 \u0438 \u0431 \u0432 \u0439\u0432 \u0436\u0438 \u0432\u0438\u043d \u0432 \u0438 \u0432\u0438 \u0436 \u0438 \u0433\u0432 \u0437\u043d\u0437\u0438 \u0431\u0437 \u0434\u0434 \u0436\u0437\u041a \u042c \u0436 \u0433\u0436 \u0418 \u043b \u0437 \u0433\u0439\u0430 \u0437\u0439\u0436\u00a0\u2026", "num_citations": "9\n", "authors": ["1503"]}
{"title": "Piggyback Meta-Data Propagation in Distributed Hash Tables.\n", "abstract": " Distributed Hashtables (DHT) are intended to provide Internet-scale data management. By following the peer-to-peer paradigm, DHT consist of independent peers and operate without central coordinators. Consequentially, global knowledge is not available and any information have to be exchanged by local interactions between the peers. Beneath data management operations, a lot of meta-data have to be exchanged between the nodes, eg, status updates, feedback for reputation management or application-specific information. Because of the large scale of the DHT, it would be expensive to disseminate meta-data by peculiar messages. In this article we investigate in a lazy dissemination protocol that piggybacks attachments to messages the peers send out anyhow. We present a software engineering approach based on mixin layers and aspect-oriented programming to cope with the extremely differing application-specific requirements. The applicability of our protocol is confirmed by means of experiments with a CAN implementation.", "num_citations": "9\n", "authors": ["1503"]}
{"title": "Global extensional assertions and local integrity constraints in federated schemata\n", "abstract": " Integrated access to multiple data sources requires a homogeneous interface provided by a federated schema. Such a federated schema should correctly reflect the semantics of the component schemata of which it is composed. Since the semantics of a database schema is also determined by a set of semantic integrity constraints, a correct schema integration has to deal with integrity constraints existing in the different component schemata. Traditionally, most schema integration approaches solely concentrate on the structural integration of given database schemata. Local integrity constraints are often simply neglected. Their relationship to global extensional assertions, which form the basic integration constraints, are even ignored completely. In this paper, we discuss the impact of global extensional assertions and local integrity constraints on federated schemata. In particular, we point out the correspondence\u00a0\u2026", "num_citations": "9\n", "authors": ["1503"]}
{"title": "Entwurf von Systemverhalten durch Spezifikation und Transformation temporaler Anforderungen\n", "abstract": " Beim Entwurf von reaktiven Softwaresystemen (z.B. Informationssystemen) sind neben der Struktur von Zust\u00e4nden auch die zul\u00e4ssigen zeitlichen Entwicklungen festzulegen; diese lassen sich durch Folgen von Zust\u00e4nden charakterisieren und werden durch Anwendungsoperationen induziert. In dieser Arbeit wird eine Methode f\u00fcr die schrittweise formale Spezifikation von dynamischem Systemverhalten vorgestellt. Es wird eine zweischichtige Beschreibung verwendet, die einerseits deskriptiv Anforderungen an ganze Zustandsfolgen, formuliert in temporaler Logik, und andererseits Vor-/Nachbedingungen f\u00fcr die einzelnen Operationen vorsieht. Nachdem die beiden Spezifikationsteile erst komplement\u00e4r aufgestellt werden, lassen sich dann aus den temporalen Anforderungen korrekte Objektlebensl\u00e4ufe ableiten und graphisch darstellen. Schlie\u00dflich werden die so erhaltenen Bedingungen an\u00a0\u2026", "num_citations": "9\n", "authors": ["1503"]}
{"title": "Text data mining and data quality management for research information systems in the context of open data and open science\n", "abstract": " In the implementation and use of research information systems (RIS) in scientific institutions, text data mining and semantic technologies are a key technology for the meaningful use of large amounts of data. It is not the collection of data that is difficult, but the further processing and integration of the data in RIS. Data is usually not uniformly formatted and structured, such as texts and tables that cannot be linked. These include various source systems with their different data formats such as project and publication databases, CERIF and RCD data model, etc. Internal and external data sources continue to develop. On the one hand, they must be constantly synchronized and the results of the data links checked. On the other hand, the texts must be processed in natural language and certain information extracted. Using text data mining, the quality of the metadata is analyzed and this identifies the entities and general keywords. So that the user is supported in the search for interesting research information. The information age makes it easier to store huge amounts of data and increase the number of documents on the internet, in institutions' intranets, in newswires and blogs is overwhelming. Search engines should help to specifically open up these sources of information and make them usable for administrative and research purposes. Against this backdrop, the aim of this paper is to provide an overview of text data mining techniques and the management of successful data quality for RIS in the context of open data and open science in scientific institutions and libraries, as well as to provide ideas for their application. In particular, solutions for the RIS\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "(Automated) Literature Analysis-Threats and Experiences\n", "abstract": " The number of scientific publications is increasing each year, specifically in the field of computer science. In order to condense existing knowledge, evidence-based software engineering is concerned with systematic literature reviews, surveys, and other kinds of literature analysis. These methods are used to summarize the evidence on empirical studies - or approaches in general - and to identify gaps for new research opportunities. However, executing systematic review processes requires a considerable amount of time and effort. Consequently, researchers have proposed several semi-automated approaches to support and facilitate different steps of such methods. With our current research, we aim to assist researchers to efficiently and effectively execute different steps, namely the search for and selection of primary studies. In this paper, we report several issues we identified during our research that threaten any\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Visual guidance for product line configuration using recommendations and non-functional properties\n", "abstract": " Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (eg, performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Exploring Large Scholarly Networks with Hermes.\n", "abstract": " Every year, the number of scientific publications increases, adding complexity to the networks of collaborations, citations, and topics, in which papers are embedded. Analyzing these networks with efficient tools is important to help researchers identify relevant works and understand scientific impact. However, available tools face several limitations, indicating that there is still room for improvement. We present Hermes, a prototype for exploring large and heterogeneous scholarly networks. Hermes allows users to seamlessly navigate diverse types of networks within a single graph, spanning hundreds of millions of nodes and relationships. Our prototype achieves reasonable responsiveness on commodity hardware through: a) comprehensive indexing, b) a careful coupling of a graph database and a search engine, and c) incremental processing of temporal queries. In this demonstration, we explain the techniques we adopt and illustrate how to use Hermes for exploring the Microsoft Academic Graph.", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Column vs. Row Stores for Data Manipulation in Hardware Oblivious CPU/GPU Database Systems.\n", "abstract": " Column vs. Row Stores for Data Manipulation in Hardware Oblivious CPU/GPU Database Systems Page 1 Column vs. Row Stores for Data Manipulation in Hardware Oblivious CPU/GPU Database Systems Iya Arefyeva, David Broneske, Marcus Pinnecke, Mudit Bhatnagar, Gunter Saake Arbeitsgruppe Datenbanken und Software Engineering Otto-von-Guericke Universit\u00e4t Magdeburg GvDB2017, Blankenburg/Harz, 02.06.2017 Page 2 Iya Arefyeva, \u201cColumn vs. Row Stores for Data Manipulation in Hardware Oblivious CPU/GPU Database Systems\u201d Motivation Online analytical processing (OLAP): - few transactions performed on big chunks of data - easy to exploit data parallelism Online transaction processing (OLTP): - thousands of transactions within a short period of time - many small transactions with various operations - data should be processed as soon as possible due to user interaction 2 fits perfectly to the \u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Challenges for a GPU-Accelerated Dynamic Programming Approach for Join-Order Optimization\n", "abstract": " Relational database management systems apply query optimization in order to determine efficient execution plans for declarative queries. Since the execution time of equivalent query execution plans can differ by several orders of magnitude based on the used join order, join-order optimization is one of the most important problems within query processing. Since the time-budget of query optimization is limited, efficient join-order optimization approaches are needed to determine execution plans with low execution times. The state of the art in commercial systems for determining optimal join orders is dynamic programming. Unfortunately, existing algorithms are mainly sequential algorithms, which do not benefit from current parallel system architectures. In current system architectures, specialized co-processors, such as GPUs, provide a higher computational power compared to CPUs. If the full potential of GPUs is used, query optimizer can provide optimal solutions for more complex problems. Unfortunately, adapting existing dynamic programming approaches for join-order optimization to GPUs is not straightforward. In this paper, we discuss the challenges for a GPU-accelerated dynamic programming approach for join-order optimization, and propose different ways to handle these challenges.", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Improving clustering-based schema matching using latent semantic indexing\n", "abstract": " The increasing size and the widespread use of XML data and different types of ontologies result in the big challenge of how to integrate these data. A critical step towards building this integration is to identify and discover semantically corresponding elements across heterogeneous data sets. This identification process becomes more and more challenging when dealing with large schemas and ontologies. Clustering-based matching is a great step towards more significant reduction of the search space and thus improving the matching efficiency. However, current methods used to identify similar clusters depend on literally matching terms. To keep high matching quality along with high matching efficiency, hidden semantic relationships among clusters\u2019 elements should be discovered. To this end, in this paper, we propose a Latent Semantic Indexing-based approach that allows retrieving the conceptual\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Interoperability of Non-functional Requirements in Complex Systems\n", "abstract": " Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Cellular dbms: An attempt towards biologically-inspired data management\n", "abstract": " Existing database management systems (DBMS) are complex and less predictable (ie, the consistency of performance with the increase of functionality and the data growth is not certain). Database researchers acknowledge the need for revisiting DBMS architectures to fulfill the needs of new hardware and application trends. We propose a biologically inspired DBMS architecture called\u201d Cellular DBMS\u201d. The Cellular DBMS architecture promises development of highly customizable and autonomous DBMS. This report explains in detail the design principles for Cellular DBMS architecture. It also explains an aspect-oriented programming based model to equip Cellular DBMS architecture with autonomy. Finally, it presents an extension to decomposed storage model (DSM) for use in Cellular DBMS.", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Incremental specification validation and runtime adaptivity of distributed component information systems\n", "abstract": " Despite all colossal efforts and investments by information systems (IS) practitioners and researchers, today's IS still remain far from timely exhibiting the required levels of adaptivity dictated by highly volatile, competitive and inter-organizational ('socio-techno-business') distributed environment. Difficulties in approaching dynamic adaptivity in IS remain essentially on how promoting manageability (e.g. simplicity/understandability/incrementality) without compromising correctness and distribution. Recapitulating from the wide acceptance and understandability of UML diagrammatical artifacts and the rigorous and distributed capabilities of component-based formalisms, we propose a stepwise approach for developing evolving distributed IS. Staring from UML-OCL descriptions, we smoothly shift them towards a tailored component Petri nets variant we called Co-NETS, where validation is geared by true-concurrent\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Implementing Bounded Aspect Quantification in AspectJ.\n", "abstract": " The integration of aspects into the methodology of stepwise software development and evolution is still an open issue. This paper focuses on the global quantification mechanism of nowadays aspect-oriented languages that contradicts basic principles of this methodology. One potential solution to this problem is to bound the potentially global effects of aspects to a set of local development steps. We discuss several alternatives to implement such bounded aspect quantification in AspectJ. Afterwards, we describe a concrete approach that relies on meta-data and pointcut restructuring in order to control the quantification of aspects. Finally, we discuss open issues and further work.", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Software evolution: A trip through reflective, aspect, and meta-data oriented techniques\n", "abstract": " Previous workshops related to aspect oriented software development, reflection organized at previous ECOOP conferences (e.g., RMA\u201900. and AOMMeT\u201901. ) and conferences on the same topics (Reflection\u201901 and AOSD since 2002) have pointed out the growing interest on these topics and their relevance in the software evolution as techniques for code instrumentation. Very similar conclusions can be drawn by reading the contributions to the workshops on unanticipated software evolution (USE 2002 and USE 2003. ).             Following the example provided by these venues, the RAM-SE (Reflection, AOP and Meta-Data for Software Evolution) workshop has provided an opportunity for researchers with a broad range of interests in reflective techniques and aspect-oriented software development to discuss recent developments of such a techniques in application to the software evolution.             The\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Informationsfusion auf heterogenen Datenbest\u00e4nden\n", "abstract": " Die Informationsfusion als Prozess der Integration und Interpretation heterogener Daten mit dem Ziel der Gewinnung von Informationen einer neuen, h\u00f6heren Qualit\u00e4t er\u00f6ffnet eine Vielzahl von Anwendungsgebieten. Dabei erfordert dieser Prozess eine enge Verzahnung der bislang h\u00e4ufig noch isoliert vorliegenden Werkzeuge und Techniken zum Zugriff auf heterogene Datenquellen, deren Integration, Aufbereitung, eine Analyse der syntaktischen, semantischen sowie temporalen Strukturen und Visualisierung derselben. In diesem Beitrag werden Rahmenbedingungen der Informationsfusion ebenso dargestellt wie die sich aus ihnen ergebenden Aufgaben. Es werden L\u00f6sungsans\u00e4tze zur Erstellung einer Workbench vorgestellt, die eine durchg\u00e4ngige Unterst\u00fctzung von Fusionsschritten erm\u00f6glicht. Dabei wird das Ziel einer konsequenten Nutzung von Datenbanktechniken verfolgt.", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Database design\n", "abstract": " The sections in this article are", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Extensible Grouping and Aggregation for Data Reconciliation.\n", "abstract": " New applications from the areas of analytical data processing and data integration require powerful features to condense and reconcile available data. Object-relational and other data management systems available today provide only limited concepts to deal with these requirements. The general concept of grouping and aggregation appears to be a fitting paradigm for a number of the mentioned issues, but in its common form of equality based groups and restricted aggregate functions a number of problems remain unsolved. Various extensions to this concept have been introduced over the last years, especially regarding user-defined functions for aggregation and derivation of grouping properties. We propose generic interfaces for user-defined grouping and aggregation as part of a SQL extension, allowing for more complex functions, for instance integration of data mining algorithms. Furthermore, we discuss high-level language primitives for common applications and illustrate the approach by introducing new concepts for similarity-based duplicate detection and elimination.", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Specifying Distributed and Dynamically Evolving Information Systems Using an Extended Co-nets Approach\n", "abstract": " The Co-nets approach that we are developing is an object- oriented (OO) specification model based on a formal and complete integration of OO concepts and constructions into an appropriate variant of algebraic Petri nets. Interpreted in rewriting logic, the approach is particularly tailored for specifying and validating advanced information systems as distributed, autonomous yet cooperative components. However, in spirit of most existing conceptual models, the Co-nets approach requires that all system aspects have to be known during its specification and fixed at once; a fact going in contrast to reality where most systems, due to different changes in business and law factors, have to change their behaviour in unexpected way during their long life-span. With the objective to overcome this crucial limitation, we present in this paper first steps towards an appropriate extension of Co-nets approach for naturally\u00a0\u2026", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Specifying Information System Dynamics in TROLL\n", "abstract": " Information Systems are becoming more and more complex. They span over many di erent application domains, have to integrate several heterogeneous resources, and to cooperate with existing systems SJH93]. Therefore it is important to model such systems on an abstract level without having to refer to implementation aspects. The following list states requirements towards a conceptual modeling approach:", "num_citations": "8\n", "authors": ["1503"]}
{"title": "Foundations of collaborative, real-time feature modeling\n", "abstract": " Feature models are core artifacts in software-product-line engineering to manage, maintain, and configure variability. Feature modeling can be a cross-cutting concern that integrates technical and business aspects of a software system. Consequently, for large systems, a team of developers and other stakeholders may be involved in the modeling process. In such scenarios, it can be useful to utilize collaborative, real-time feature modeling, analogous to collaborative text editing in Google Docs or Overleaf. However, current techniques and tools only support a single developer to work on a model at a time. Collaborative and simultaneous editing of the same model is often achieved by using version control systems, which can cause merge conflicts and do not allow immediate verification of a model, hampering real-time collaboration outside of face-to-face meetings. In this paper, we describe the formal foundations of\u00a0\u2026", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Toward efficient and reliable genome analysis using main-memory database systems\n", "abstract": " Improvements in DNA sequencing technologies allow to sequence complete human genomes in a short time and at acceptable cost. Hence, the vision of genome analysis as standard procedure to support and improve medical treatment becomes reachable. In this vision paper, we describe important data-management challenges that have to be met to make this vision come true. Besides genome-analysis performance, data-management capabilities such as data provenance and data integrity become increasingly important to enable comprehensible and reliable genome analysis. We argue to meet these challenges by using main-memory database technologies, which combine fast processing capabilities with extensive data-management capabilities. Finally, we discuss possibilities of integrating genome-analysis tasks into DBMSs and derive new research questions.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Program comprehension in preprocessor-based software\n", "abstract": " To adapt to heterogeneous hardware, software of embedded systems provides customization capacities. Typically, this customization is achieved using conditional compilation with preprocessors. However, preprocessor usage can lead to obfuscated source code that can be difficult to comprehend, which in turn cause increased maintenance costs, bugs, and security vulnerabilities. To profit from the benefit of preprocessors usage, we need to improve their comprehensibility. In this paper, we describe how program comprehension can be improved and, to that end, measured. We show that reliably measuring program comprehension requires considerably effort. However, the benefit is that we can apply concepts that have proven to improve program comprehension, and thus can e.g. improve maintainability, reliability, and security of source code.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "ECOS: evolutionary column-oriented storage\n", "abstract": " As DBMS has grown more powerful over the last decades, they have also become more complex to manage. To achieve efficiency by DBMS tuning is nowadays a hard task carried out by experts. This development inspired the ongoing research on self-tuning to make DBMS more easily manageable. We present a customizable self-tuning storage manager, we termed as Evolutionary Column-Oriented Storage (ECOS). The capability of self-tuning data management with minimal human intervention, which is the main design goal for ECOS, is achieved by dynamically adjusting the storage structures of a column-oriented storage manager according to data size and access characteristics. ECOS is based on the Decomposed Storage Model (DSM). It supports customization at the table-level using five different variations of DSM. ECOS also proposes fine-grained customization of storage structures at the column\u00a0\u2026", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Flexible Runtime Program Adaptations in Java--A Comparison\n", "abstract": " Software development is an ongoing process which does not end when the first version of an application is released. Bugs must be fixed and requirements evolve. Maintaining an application usually means to stop the application, apply the required changes, and start the application again. This downtime is not acceptable for applications that must be available 24 hours a day, 7 days a week. On the other hand even for end-user desktop applications, restarts to apply patches can be an annoying user experience. For that reason we investigate how to maintain applications at runtime. However, due to the fact that it is not predictable what changes become necessary and when they have to be applied the application must be enabled for unanticipated changes even of already executed program parts. In previous work we proposed a solution for Java, since Java is commonly used for developing 24/7 applications. Unfortunately, this solution came with some limitations. Therefore, we present a novel runtime maintenance approach based on class replacements and mediators which overcomes these limitations and allows unanticipated changes of applications that run on top of a standard Java virtual machine.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Improved DBSCAN for spatial databases with noise and different densities\n", "abstract": " Spatial data clustering is one of the important data mining techniques for extracting knowledge from large amount of spatial data collected in various applications, such as remote sensing, GIS, computer cartography, environmental assessment and planning, etc. many useful spatial data clustering algorithms have been proposed in the past decade. DBSCAN is the most popular density clustering algorithm, which can discover clusters of any arbitrary shape and can handle the noise points effectively. However, DBSCAN can not discover clusters with large variance in density because it depends on globular value for its parameters Eps and MinPts which depend on each other. This paper presents an improved DBSCAN which can cluster spatial databases that contain clusters of different densities, shapes, and sizes effectively. Experimental results included to establish that the proposed improved DBSCAN is superior to DBSCAN in discovering clustering with large variance in density.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "INFUSE-Eine datenbankbasierte Plattform f\u00fcr die Informationsfusion\n", "abstract": " Informationsfusion als Prozess der Integration und Interpretation heterogener Daten mit dem Ziel der Gewinnung neuer Informationen einer h\u00f6heren Qualit\u00e4t er\u00f6ffnet eine Vielzahl von Anwendungsgebieten. Gleichzeitig erfordert dieser Prozess aber auch eine enge Verzahnung der bislang h\u00e4ufig noch isoliert vorliegenden Werkzeuge und Techniken zum Zugriff auf heterogene Datenquellen, deren Integration, Aufbereitung, Analyse und Visualisierung. In diesem Beitrag werden erste Ergebnisse der Entwicklung einer Workbench vorgestellt, die durch konsequente Nutzung von Datenbanktechniken eine durchg\u00e4ngige Unterst\u00fctzung dieser Schritte erm\u00f6glicht.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Execution dependencies in transaction closures\n", "abstract": " Activities of advanced applications can be modeled by interrelated transactions. These relations can be described by different kinds of transaction dependencies. The notion of transaction closure is a generalization of nested transactions providing means to describe complex activities such as transactional workflows. In this paper our main focus lies on execution dependencies for describing certain control flows among related transactions of transaction closures. In particular we consider the transitivity property for all kinds of transaction execution dependencies and discuss their relationship to other kinds of dependencies such as transaction termination dependencies. We point out that some of these dependency combinations are incompatible. As a result we present rules for reasoning about the transitivity of execution dependencies. Thus, we are able to conclude how arbitrary transactions of a transaction closure\u00a0\u2026", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Dynamically changing behavior: An agent-oriented view to modeling intelligent information systems\n", "abstract": " Although object specification technology is successfully used for modeling information systems, it is not able to get a grasp of dynamically changing behavior. Due to the fact that objects in information systems can have a very long life-span, it often happens that during the life of an object external requirements are changing (e.g. changes of laws or banking rules). Such changes often require the object to adopt another behavior. The main problem for current object specification approaches is that, in general, not all possible changes can be taken into account in advance at specification time. Therefore, a flexible extension is needed to capture this situation. The approach we present and discuss in this paper is an important step towards a specification framework based on the concept of agent by introducing a certain form of knowledge as part of the internal state.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Modelling information systems as object societies\n", "abstract": " Conceptual modelling of complex information systems requires the use of a formal design approach covering both static and dynamic aspects of the system and the modelled Universe of Discourse. Viewing an information system as a collection of communicating objects is close to the intuitive perception of such systems on a conceptual level. Objects have a local state, show a specific behaviour, communicate with other objects and may be itself composed from smaller objects. This article presents an abstract concept of such dynamic objects and discusses language features of a specification language for describing object systems. The presented language Troll2 supports structuring mechanisms of semantic data models together with process specification constructs to cover object dynamics. Extensions of the presented framework are discussed covering the step from communicating objects to cooperating\u00a0\u2026", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Objektorientierte Modellierung von Informationssystemen\n", "abstract": " Im Bereich des Entwurfs von Informationssystemen spielt die formale Beschreibung auf der konzeptionellen Ebene eine wichtige Rolle im Entwurfsproze. Etablierte formale Beschreibungsmethoden sind oft nicht geeignet, die inh arente Komplexit at interaktiver Informationssysteme mit persistenten Daten in der Modellbildung ad aquat zu beherrschen. Als Ergebnis m ussen in diesen Methoden Teilbereiche der Systemmodellierung in von einander unabh angigen Formalismen oder gar informell beschrieben werden, so da keine ubergreifende formale Basis der Gesamtbeschreibung bestehen kann.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Flexible Generation of Global Integrated Schemata using GIM\n", "abstract": " The integration of schemata is a very essential but also a very complex task in a federated database environment. Especially the integration of di erent inheritance hierarchies into one hierarchy is not satisfactorily solved up to now. We show, why some existing approaches fail and propose a new algorithm, which further meets the demand for complete, correct and minimal integrated schemata. Furthermore, with respect to the well-known concept of logical data independence, our algorithm supports the derivation of di erent view-dependent external schemata. External object-oriented schemata can be automatically generated by our algorithm. Our approach can be applied to scenarios di erent from the integration problem, eg for schema evolution, views in object-oriented database systems, and classication problems.", "num_citations": "7\n", "authors": ["1503"]}
{"title": "Search. Review. Repeat? An empirical study of threats to replicating SLR searches\n", "abstract": " A systematic literature review (SLR) is an empirical method used to provide an overview of existing knowledge and to aggregate evidence within a domain. For computer science, several threats to the completeness of such reviews have been identified, leading to recommendations and guidelines on how to improve their quality. However, few studies address to what extent researchers can replicate an SLR. To conduct a replication, researchers have to first understand how the set of primary studies has been identified in the original study, and can ideally retrieve the same set when following the reported protocol. In this article, we focus on this initial step of a replication and report a two-fold empirical study: Initially, we performed a tertiary study using a sample of SLRs in computer science and identified what information that is needed to replicate the searches is reported. Based on the results, we conducted a\u00a0\u2026", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Automated vertical partitioning with deep reinforcement learning\n", "abstract": " Finding the right vertical partitioning scheme to match a workload is one of the essential database optimization problems. With the proper partitioning, queries and management tasks can skip unnecessary data, improving their performance. Algorithmic approaches are common for determining a partitioning scheme, with solutions being shaped by their choice of cost models and pruning heuristics. In spite of their advantages, these can be inefficient since they don\u2019t improve with experience (e.g., learning from errors in cost estimates or heuristics employed). In this paper we consider the feasibility of a general machine learning solution to overcome such drawbacks. Specifically, we extend the work in GridFormation, mapping the partitioning task to a reinforcement learning (RL) task. We validate our proposal experimentally using a TPC-H database and workload, HDD cost models and the Google Dopamine\u00a0\u2026", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Quality of research information in RIS databases: A multidimensional approach\n", "abstract": " For the permanent establishment and use of a RIS in universities and academic institutions, it is absolutely necessary to ensure the quality of the research information, so that the stakeholders of the science system can make an adequate and reliable basis for decision-making. However, to assess and improve data quality in RIS, it must be possible to measure them and effectively distinguish between valid and invalid research information. Because research information is very diverse and occurs in a variety of formats and contexts, it is often difficult to define what data quality is. In the context of this present paper, the data quality of RIS or rather their influence on user acceptance will be examined as well as objective quality dimensions (correctness, completeness, consistency and timeliness) to identify possible data quality deficits in RIS. Based on a quantitative survey of RIS users, a reliable and valid framework for\u00a0\u2026", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Codd's World: Topics and their Evolution in the Database Community Publication Graph.\n", "abstract": " Scholarly network analysis is the study of a scienti c research network aiming to discover meaningful insights and making datadriven research decisions. Analyzing such networks has become increasingly challenging, due to the amount of scienti c research that is added every day. Furthermore, online resources often include information from other online sources (eg, academic social platforms), enabling to study networks on a larger and more complex scope. In this paper, we present a study on a speci c research network: The (relational) database community publication graph, that we call Codd\u2019s World; a transitive closure over citations from the foundational work of EF Codd. We speci cally analyze the topics of the published papers, the relevance of authors and papers, and how this relates to raw publication counts. Among our ndings, we show that topic modeling can be a useful entry point for scholarly network analysis.", "num_citations": "6\n", "authors": ["1503"]}
{"title": "An eight-dimensional systematic evaluation of optimized search algorithms on modern processors\n", "abstract": " Searching in sorted arrays of keys is a common task with a broad range of applications. Often searching is part of the performance critical sections of a database query or index access, raising the question what kind of search algorithm to choose and how to optimize it to obtain the best possible performance on real-world hardware. This paper strives to answer this question by evaluating a large set of optimized sequential, binary and k-ary search algorithms on a modern processor. In this context, we consider hardware-sensitive optimization strategies as well as algorithmic variations resulting in an eight-dimensional evaluation space. As a result, we give insights on expected interactions between search algorithms and optimizations on modern hardware. In fact, there is no single best optimized algorithm, leading to a set of advices on which variants should be considered first given a particular array size.", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Exploiting capabilities of modern processors in data intensive applications\n", "abstract": " In main-memory database systems, the time to process the data has become a limiting factor due to the missing access gap. With changing processing capabilities (e.g., branch prediction, pipelining) in every new CPU architecture, code that was optimal once will probably not stay the best code forever. In this article, we analyze processing capabilities of the classical CPU and describe code optimizations to exploit the capabilities. Furthermore, we present state-of-the-art compiler techniques that already implement code optimizations, while also showing gaps for further code optimization integration.", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Efficient Storage and Analysis of Genome Data in Databases\n", "abstract": " Genome-analysis enables researchers to detect mutations within genomes and deduce their consequences. Researchers need reliable analysis platforms to ensure reproducible and comprehensive analysis results. Database systems provide vital support to implement the required sustainable procedures. Nevertheless, they are not used throughout the complete genome-analysis process, because (1) database systems su er from high storage overhead for genome data and (2) they introduce overhead during domain-specific analysis. To overcome these limitations, we integrate genome-specific compression into database systems using a specialized database schema. Thus, we can reduce the storage overhead to 30%. Moreover, we can exploit genome-data characteristics during query processing allowing us to analyze real-world data sets up to five times faster than specialized analysis tools and eight times faster than a straightforward database approach.", "num_citations": "6\n", "authors": ["1503"]}
{"title": "A Latent Semantic Indexing-Based Approach to Determine Similar Clusters in Large-scale Schema Matching\n", "abstract": " Schema matching plays a central role in identifying the semantic correspondences across shared-data applications, such as data integration. Due to the increasing size and the widespread use of XML schemas and different kinds of ontologies, it becomes toughly challenging to cope with large-scale schema matching. Clustering-based matching is a great step towards more significant reduction of the search space and thus improved efficiency. However, methods used to identify similar clusters depend on literally matching terms. To improve this situation, in this paper, a new approach is proposed which uses Latent Semantic Indexing that allows retrieving the conceptual meaning between clusters. The experimental evaluations show encourage results towards building efficient large-scale matching approaches.", "num_citations": "6\n", "authors": ["1503"]}
{"title": "A Framework for Optimal Selection of a Storage Architecture in RDBMS\n", "abstract": " Self-tuning techniques are widely used in the database community especially for large analyses systems. Within this domain, a new architecture known as column store comes up to analyse and aggregate data. Column stores demonstrate good results in real world examples and benchmarks like TPC-H. In recent years, row stores dominate the data warehousing domain. To the best of our knowledge, there is no advisor for the optimal selection of storage architecture. We discuss this fact with respect to our example based on TPC-H. Afterwards, we present our research steps to develop a decision model for optimal selection of storage architecture. Finally, we motivate and discuss the development of a hybrid architecture.", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Multi-level weighting in multimedia retrieval systems\n", "abstract": " Ranking of relevant objects plays an important role in various applications especially in multimedia database systems and information retrieval systems. In contrast to traditional database systems, multimedia database systems deal with similarity queries returning a list of objects ranked by the objects\u2019 overall score. The overall score for objects in the database is calculated using a scoring rule which is commonly based on similarity functions and fuzzy logic. One aspect which enhances the user\u2019s flexibility to formulate preferences regarding the search criteria, is to assign weights to each argument in a query. In this paper a formal specification of the requirements for an adequate weighted scoring rule is given. Based on this specification different methods for incorporating weights into scoring rules are evaluated and their limitations are shown. Furthermore, we discuss that weighting on different levels in\u00a0\u2026", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Consistency Management in Object-Oriented Databases\n", "abstract": " . The paper presents concepts and ideas underlying an approach for consistency management in object oriented databases. In this approach constraints are structured as first class citizens and stored in a meta-database called constraints catalog. When an object is created constraints of this object are retrieved from the constraints catalog and relationships between these constraints and the object are established. The structure of constraints has several features that enhance consistency management in OODBMS which do not exist in conventional approaches in a satisfactory way. These features are monitoring objects consistency at different levels of update granularity, integrity independence, efficiency of constraints maintenance, controlling inconsistent objects, enabling and disabling of constraints globally to all objects of database as well as locally to individual objects, and the possibility of declaring constraints on individual objects. All these features are provided b...", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Spezifikation von Informationssystemen als Objektsysteme: Das TROLL-Projekt\n", "abstract": " Zusammenfassung Der Einsatzbereich von Software-Systemen ist gekennzeichnet durch immer komplexer werdende Anforderungen und eine Orientierung von funktionsorientierten Systemen zu daten-orientierten Systemen. Klassische Entwurfsmethoden f ur datenintensive Anwendungen beschreiben strukturelle resp. dynamische Eigenschaften des modellierten Weltausschnittes mit unterschiedlichen, wenig intergrierten Formalismen. Objektorientierte Ans atze integrieren dagegen strukturelle und dynamische Aspekte in Objekten, sind aber oft nicht formal beschrieben. In diesem Artikel stellen wir die Sprache Troll vor, die auf einem formalen Objektkonzept aufbaut. Troll dient der Spezi kation und Modellierung von Software-und Informationssytemen. Ein Troll Modell stellt dabei deklarativ die Eigenschaften dar, die das System erf ullen soll. Eine Ubersicht zu verwandten Ans atzen wird gegeben und auf gegenw artige und zuk unftige Arbeiten des Projektes verwiesen.", "num_citations": "6\n", "authors": ["1503"]}
{"title": "Concept Hierarchy Extraction from Legal Literature.\n", "abstract": " Due to the ever-increasing amount of legal regulations, it became an interest of scholars to find ways of capturing domain-relevant knowledge and facilitate the navigation in legal text corpora. Furthermore, the contextual nature of legislation requires enhanced semantic capabilities to identify relevant regulations for specific user needs. This work aims for collecting concept hierarchies from German literature in the legal domain which are then integrated into a knowledge base with multiple clusters, allowing for different perspectives and efficient lookups. Having references to regulations in the leaves of the concept tree and higher levels with an increasingly abstract context, the resulting hierarchies provide the basis for creating legal domain knowledge in German law. Starting with rule-based annotation, we cluster extracted references, given their context features derived from tables of contents and reasons for citing from various textbook formats. We study the expressiveness of the obtained reference context features. Since different authors have their own notion of hierarchy given by the table of contents, we propose a heterogeneous lightweight ontology allowing for the coexistence of similar, yet diverse concept hierarchies to dynamically determine the best fit for a user in a semisupervised setting. This approach is novel, since state-of-the-art ontologies are conventionally modeled under full integration and in a top-down manner, often not accounting for perspectives in knowledge representation.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Backlogs and Interval Timestamps: Building Blocks for Supporting Temporal Queries in Graph Databases\n", "abstract": " The analysis of networks, either at a single point in time or through their evolution, is an increasingly important task in modern data management. Graph databases are uniquely suited to improve static network analysis. However, there\u2019s still no consensus on how to best model data evolution with these databases. In our work we propose an elementary concept to support temporal analysis with property graph databases, using a single-graph model limited to structural changes. We manage the temporal aspects of items with interval timestamps and backlogs. To include backlogs in the model we examine two alternatives:(1) global indexes, and (2) using the graph as an index by resorting to timestamp denormalization. We evaluate density calculation and time slice retrieval over successive days from a SNAP dataset, on an Apache Titan prototype of our model, observing from 2x to 100x response time gains by comparing differential vs. snapshot methods; and no conclusive difference between the backlog alternatives.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Elf: A Main-Memory Structure for Efficient Multi-Dimensional Range and Partial Match Queries\n", "abstract": " Efficient evaluation of selection predicates (eg, range predicates) defined on multiple columns of the same table is a difficult, but nevertheless important task. Especially for subsequent join processing or aggregation, we need to reduce the amount of tuples to be processed. As we have seen an enormous increase of data with the last decade, this kind of selection predicate became more important. Especially in OLAP scenarios or scientific data management tasks, we often face multi-dimensional data sets that need to be filtered based on several dimensions. So far, the state-of-the-art solution strategy is to apply highly optimized sequential scans. However, the intermediate results are often large, while the final query result often only contains a small fraction of the data set. This is due to the combined selectivity of all predicates. In this report, we propose Elf-a new tree-based approach to efficiently support such queries. In contrast, to other tree-based approaches, we do not suffer from the curse of dimensionality. The main reason is that we do not apply space or data partitioning methods, like bounding boxes, but incrementally index sub-spaces. In addition, our Elf is cache sensitive, contains an optimized storage layout, fixed search paths, and even supports slight data compression rates. Our experimental results indicate a clear superiority of our approach compared to a highly optimized SIMD sequential scan as competitor. For TPC-H queries with multi-column selection predicates, we achieve a speed-up between factor five and two orders of magnitude, mainly depending on the selectivity of the predicates.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Flexible Analysis of Plant Genomes in a Database Management System\n", "abstract": " Analysis of genomes has a wide range of applications from disease susceptibility studies to plant breeding research. For example, different types of barley have differing characteristics regarding draught or salt tolerance. Thus, a typical use case is comparing two plant genomes and try to deduce which genes are responsible for a certain resistance. For this, we need to find differences in large volumes of aligned genome data, which is already available in large genome databases.The challenge is to efficiently retrieve the genotypes of a certain range of the genome, and then, to determine variants and their impact on the plant organism. State-of-the-art tools are fixed pipelines with a fixed parametrization. However, in practice, users want to interactively analyse genome data and need to customize the parametrization. In this demonstration, we show how we can support flexible ad-hoc analyses of arbitrary plant genomes using SQL with a small set of user-defined aggregation functions and dynamic parametrization. Furthermore, we demonstrate how genome analysis workflows for variant calling can be applied to our system and provide insights about the performance of our system.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Privacy-Aware Multidimensional Indexing\n", "abstract": " Deleting data from a database system in a forensic secure environment and in a high performant way is a complex challenge. Due to redundant copies and additional information stored about data items, it is not appropriate to delete only data items themselves. Additional challenges arise when using multidimensional index structures. This is because information of data items are used to index the space. As initial result, we present different deletion levels, to overcome this challenge. Based on this classification, we analyze how data can be reconstructed from the index and modify index structures to improve privacy of data items. Second, we benchmark our index structure modifications and quantify our modifications. Our results indicate that forensic secure deletion is possible with modification of multidimensional index structures having only a small impact on computational performance, in some cases.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Building AS-IS process models from task descriptions\n", "abstract": " Business processes have to adopt the changes driven by the market to remain competitive. This requires improvement of business processes, which can be carried out by understanding the existing business processes. For this AS-IS business process models are built that represent the actual situation of the enterprise. In this paper, we propose an approach to build an AS-IS process model from employee's descriptions instead of instance executions in information systems. We also present an algorithm that builds an AS-IS model from control flow perspective and we further elaborate it with the help of a case study.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Algebraic and Cost-based Optimization of Refactoring Sequences\u22c6\n", "abstract": " Software product lines comprise techniques to tailor a program by selecting features. One approach to implement product lines is to translate selected features into sequenced program transformations which extend a base program. However, a sequence translated from the user selection can be inefficient to execute. In this paper, we show how we optimize sequences of refactoring transformations to reduce the composition time for product line programs.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "A multi-dimensional architectural approach to behavior-intensive adaptive pervasive applications\n", "abstract": " Pervasive applications are featured by their transient interactions, explicit surrounding contexts and by the flow of activities. Towards transparently tackling these features yet rigorously modelling and validating such applications, this contribution capitalizes on SE advances: (1) explicit separation of concerns (e.g. interaction-centric functionalities and context-awareness); (2) flexible description of intrinsic rules governing any behavior-intensive task; and (3) concern-based transient architectural connectors to cope with adaptability. More precisely, we propose a multi-concern architectural approach that explicitly separate interaction concerns from context-aware ones. Each concern is first independently conceived through tailored ECA architectural connectors. These concerns are then accordingly integrated at the fine-grained activity level.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Towards an adequate framework for specifying and validating runtime evolving complex discrete-event systems\n", "abstract": " Although formal specification/validation is nowadays definitely accepted as a necessary step in developing reliable complex discrete-event systems, most of existing formalisms are facing challenging problems due to the multi-dimensional requirements to be met by such systems. As a result of this unsatisfactory state of affair, different system's facets are often separately approached by different formalisms which avoid any coherent description of the whole system. Besides that, most of formal proposals are lacking conceptual means for dynamically updating the (initial) system's specification as unavoidable consequences of technological change, user's requirement modification, etc.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Evolving objects: conceptual description of adaptive information systems\n", "abstract": " Today, information systems are essential parts of large organizations. Since such kinds of systems have a very long life-span, they have to be adapted to new changing requirements occurring during their lifetime. Evolution must be regarded not only at the object state level, but also at the object behavior level. Especially, the explicit handling of (behavior) evolution on the conceptual level is necessary. For that, we introduce the notion of evolving objects as basic building blocks of information systems. The behavior of such an object is divided into a rigid and an evolving part. The rigid behavior is ideally stable for the whole life-span of the object; the evolving behavior can be changed dynamically at runtime. In this paper, we present an extended specification framework for modeling evolving objects. Particularly, this framework provides the basis to explicitly specify behavior evolution.", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Spezifikation von Objektsystemen\n", "abstract": " Die konzeptionelle Modellierung des Weltausschnitts, der durch ein Informationssystem dargestellt werden soll, ist die entscheidende Phase beim Systementwurf, da das konzeptionelle Modell die Grundlage der Implementierung ist. Eine Sprache zur konzeptionellen Modellierung sollte daher auf einer soliden formalen Grundlage basieren, um einerseits die systematische Konstruktion einer Implementierung, andererseits eine fr\u00fche Animation zu erm\u00f6glichen.             In diesem Artikel wird die Sprache TROLL vorgestellt, die auf einem proze\u00dforientierten Objektmodell basiert. TROLL erm\u00f6glicht die deklarative Beschreibung der statischen sowie der dynamischen Aspekte eines zu beschreibenden Weltausschnittes mit Hilfe von Objekten.             Da deklarative Beschreibungsformalismen in der Regel nicht direkt ausf\u00fchrbar sind, werden Ideen zur Transformation von deklarativen in operationale\u00a0\u2026", "num_citations": "5\n", "authors": ["1503"]}
{"title": "Modern Applications and Challenges for Rare Itemset Mining\n", "abstract": " Data mining is the process of extracting useful unknown knowledge from large datasets. Frequent itemset mining is the fundamental task of data mining that aims at discovering interesting itemsets that frequently appear together in a dataset. However, mining infrequent (rare) itemsets may be more interesting in many real-life applications such as predicting telecommunication equipment failures, genetics, medical diagnosis, or anomaly detection. In this paper, we survey up-to-date methods of rare itemset mining. The main goal of this survey is to provide a comprehensive overview of the state-of-the-art algorithms of rare itemset mining and its applications. The main contributions of this survey can be summarized as follows. In the first part, we define the task of rare itemset mining by explaining key concepts and terminology, motivation examples, and comparisons with underlying concepts. Then, we highlight the state-of-art methods for rare itemsets mining. Furthermore, we present variations of the task of rare itemset mining to discuss limitations of traditional rare itemset mining algorithms. After that, we highlight the fundamental applications of rare itemset mining. In the last, we point out research opportunities and challenges for rare itemset mining for future research.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Implementation and user acceptance of research information systems: An empirical survey of German universities and research organisations\n", "abstract": " PurposeThe purpose of this paper is to present empirical evidence on the implementation, acceptance and quality-related aspects of research information systems (RIS) in academic institutions.Design/methodology/approachThe study is based on a 2018 survey with 160 German universities and research institutions.FindingsThe paper presents recent figures about the implementation of RIS in German academic institutions, including results on the satisfaction, perceived usefulness and ease of use. It contains also information about the perceived data quality and the preferred quality management. RIS acceptance can be achieved only if the highest possible quality of the data is to be ensured. For this reason, the impact of data quality on the technology acceptance model (TAM) is examined, and the relation between the level of data quality and user acceptance of the associated institutional RIS is addressed\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Solving problems of research information heterogeneity during integration\u2013using the European CERIF and German RCD standards as examples\n", "abstract": " Integrating data from a variety of heterogeneous internal and external data sources (eg CERIF and RCD data models with different modeling languages) in a federated database system such as \u201cResearch Information Management System (RIMS)\u201d is becoming more challenging for (inter-) national universities and research institutions. Data quality is an important factor for successful integration and interpretation of research information and interoperability of various independent information systems. Before the data is loaded into RIMS, they should be reviewed during data integration process to resolve conflicts between the different data sources and clean the data quality issues. Poor data quality leads to distortion in data presentation, and thus to erroneous basis for decisions. It is ultimately a cost for scientific institutions and it starts with integrating research information into the RIMS. Therefore, the investment in the\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Mutation operators for feature\u2010oriented software product lines\n", "abstract": " Mutation testing is an approach to assess the quality of test cases. Mutants are modified versions of a system that ideally compose faulty behaviour. Test cases for a system are effective if they kill these mutants. For software product lines, several works have addressed mutation testing to inject variability faults, which may only exist in some variants. These works focus on variability models or specific implementation techniques. In contrast, feature\u2010oriented programming has been rarely investigated, wherefore, we (1) derive corresponding mutation operators, (2) investigate the feasibility of our proposed and conventional operators on 4 software product lines, and (3) discuss open challenges in mutation testing of software product lines. The results show that our proposed operators are suitable to cause variability faults and extend the capabilities of conventional operators. Nonetheless, mutation testing of software\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Cooking DBMS Operations using Granular Primitives\n", "abstract": " The increasing heterogeneity of the underlying hardware forces modern database system engineers to implement multiple variants of a\u00a0single database operator (e.g., join, selection). With increasing heterogeneity, these variants become too complex to maintain and tune for different devices. To overcome these disadvantages, developers use an alternative, primitive-based operator design. This design paradigm splits the database operators into granular functions or primitives and executes a\u00a0given operator by combing the necessary primitives. Hence, we require only a\u00a0limited set of these primitives as we reuse them for multiple database operations. Thus, tuning a\u00a0single primitive improves efficiency of all the database operations using it.               In this survey, we provide an overview of a\u00a0primitive-based database engine. First, we list different primitives from literature and place them in a\u00a0hierarchy from the\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Pclocator: A tool suite to automatically identify configurations for code locations\n", "abstract": " The source code of highly-configurable software is challenging to comprehend, analyze, and test. In particular, it is hard to identify all configurations that comprise a certain code location. We contribute PCLocator, a tool suite that solves this problem by utilizing static analysis tools for compile-time variability. Using BusyBox and the Variability Bugs Database (VBDb), we evaluate the correctness and performance of PCLocator. The results show that we are able to analyze files in a matter of seconds and derive correct configurations in 95% of all cases.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "SIMD vectorized hashing for grouped aggregation\n", "abstract": " Grouped aggregation is a commonly used analytical function. The common implementation of the function using hashing techniques suffers lower throughput rate due to the collision of the insert keys in the hashing techniques. During collision, the underlying technique searches for an alternative location to insert keys. Searching an alternative location increases the processing time for an individual key thereby degrading the overall throughput. In this work, we use Single Instruction Multiple Data (SIMD) vectorization to search multiple slots at an instant followed by direct aggregation of results. We provide our experimental results of our vectorized grouped aggregation with various open-addressing hashing techniques using several dataset distributions and our inferences on them. Among our findings, we observe different impacts of vectorization on these techniques. Namely, linear probing and two-choice\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Efficient evaluation of multi-column selection predicates in main-memory\n", "abstract": " Efficient evaluation of selection predicates is a performance-critical task, for instance to reduce intermediate result sizes being the input for further operations. With analytical queries getting more and more complex, the number of evaluated selection predicates per query and table rises, too. This leads to numerous multi-column selection predicates. Recent approaches to increase the performance of main-memory databases for selection-predicate evaluation aim at optimally exploiting the speed of the CPU by using accelerated scans. However, scanning each column one by one leaves tuning opportunities open that arise if all predicates are considered together. To this end, we introduce Elf, an index structure that is able to exploit the relation between several selection predicates. Elf features cache sensitivity, an optimized storage layout, fixed search paths, and slight data compression. In a large-scale evaluation\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Efficiently storing and analyzing genome data in database systems\n", "abstract": " Genome-analysis enables researchers to detect mutations within genomes and deduce their consequences. Researchers need reliable analysis platforms to ensure reproducible and comprehensive analysis results. Database systems provide vital support to implement the required sustainable procedures. Nevertheless, they are not used throughout the complete genome-analysis process, because (1) database systems suffer from high storage overhead for genome data and (2) they introduce overhead during domain-specific analysis. To overcome these limitations, we integrate genome-specific compression into database systems using a specialized database schema. Thus, we can reduce the storage consumption of a database approach by up to 35%. Moreover, we exploit genome-data characteristics during query processing allowing us to analyze real-world data sets up to five times faster than\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Experience from Measuring Program Comprehension\u2014Toward a General Framework\n", "abstract": " Program comprehension plays a crucial role during the software-development life cycle: Maintenance programmers spend most of their time with comprehending source code, and maintenance is the main cost factor in software development. Thus, if we can improve program comprehension, we can save considerable amount of time and cost. To improve program comprehension, we have to measure it first. However, program comprehension is a complex, internal cognitive process that we cannot observe directly. Typically, we need to conduct controlled experiments to soundly measure program comprehension. However, empirical research is applied only reluctantly in software engineering. To close this gap, we set out to support researchers in planning and conducting experiments regarding program comprehension. We report our experience with experiments that we conducted and present the resulting framework to support researchers in planning and conducting experiments. Additionally, we discuss the role of teaching for the empirical researchers of tomorrow.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "A Variability Model for Query Optimizers\n", "abstract": " By adopting to more domains, database management systems (DBMSs) increase their functionality continously. This leads to DBMSs that often include unnecessary functionality, which decreases performance. A result of this trend is that new specialized systems arise that focus only on a certain application scenario but often reimplement already existing functionality. To avoid bloated DBMSs, we propose to introduce variability in DBMS implementations that allows users to select only needed functionality for a specific application scenario. In this paper, we focus on the query optimizer as it is a key component of DBMSs. We describe the potentials of tailoring query optimizers. Furthermore, we analyze common and differing functionality of three query optimizers of industrial DBMSs (SQLite, Oracle, and PostgreSQL) to create a variability model for query optimizers that can be used as a basis for future variability\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Workload-based Heuristics for Evaluation of Physical Database Architectures\n", "abstract": " Database systems are widely used in different application domains. Therefore, it is difficult to decide which database management system meets the requirements of a certain application at most. This observation is also true for scientific and statistical data management, due to new application and research fields. New requirements are often implied to data management while discovering unknown research and applications areas. That is, heuristics and tools do not exist to select an optimal database management system. In previous work, we proposed a decision framework based on application workload analyses. Our framework supports application performance analyses by mapping and merging workload information to patterns. In this paper, we present heuristics for performance estimation to select an optimal database management system for a given application. We show that these heuristics improve our decision framework by complexity reduction without loss of accuracy.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Generierung ma\u00dfgeschneiderter Relationenschemata in Softwareproduktlinien mittels Superimposition\n", "abstract": " Die Erstellung eines individuellen Programms aus einer Softwareproduktlinie (Programmfamilie) erfordert auf Anwendungsund Datenbankseite einen speziell angepassten und aufeinander abgestimmten Funktionsumfang. Die Modellierung ma\u00dfgeschneiderter Relationenschemata stellt z.B. aufgrund der gro\u00dfen Anzahl an Programmen, die aus einer Produktlinie erstellt werden k\u00f6nnen, eine Herausforderung dar. Wir pr\u00e4sentieren einen L\u00f6sungsvorschlag zur Modellierung und Generierung von ma\u00dfgeschneiderten Relationenschemata mittels Superimposition. Wir zeigen anhand einer realen, produktiv eingesetzten Fallstudie welche Vorteile unser Ansatz in den Bereichen Wartung und Weiterentwicklung erzeugt und welche Herausforderungen beispielsweise durch redundant definierte Schemaelemente existieren.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Evaluation of Techniques for the Instrumentation and Extension of Proprietary OpenGL Applications\n", "abstract": " Instrumentation of interfaces is a popular design pattern in engineering. Academic and industrial projects are already using instrumented OpenGL clients for various purposes. We perceive instrumentation of proprietary OpenGL applications as a basic technology to open up interactive three-dimensional graphics as a potent interoperability platform for heterogeneous simulation software in engineering. Hence, we describe and compare four instrumentation techniques on the MS Windows platform: relink library, dynamic library replacement, virtual display driver, and binary interception. We qualitatively evaluate them for four capabilities: to instrument proprietary simulation software; to instrument a subset only of the OpenGL interface; to instrument multiple interfaces simultaneously; and to chain intermediaries. The relink library technique is powerful, except that it cannot be used with proprietary simulation software. Dynamic library replacement and virtual display drivers potentially support all features, although some features are difficult to implement. The binary interception technique inherently supports all capabilities. We conclude with directions for future research.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Rulespect: Language-Independent Rule-Based AOP Model for Adaptable Context-Sensitive Web Services\n", "abstract": " Business domain has always been competitive. Adaptability to changes in business processes is mandatory to keep business organizations ahead in competition. Information system is a fundamental tool to manage business processes. Information system should be adaptable to accommodate changes in business processes and should be context-sensitive to give more personalized perspective to information system user. Web services and aspect-oriented programming possess good potential for adaptable context-sensitive information system development. However, main obstacle in the widespread adoption of aspect-oriented programming paradigm in information system domain is verbose and alien syntax of implementation languages that make it difficult to use. To solve this problem we propose Rulespect, a language-independent rule-based aspect-oriented programming model for adaptable context-sensitive web services.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Cellular DBMS\u2014Architecture for biologically-inspired customizable autonomous DBMS\n", "abstract": " Data management is one of the fundamental requirements of ubiquitous computing. Existing data management systems are complex and provide a multitude of functionalities. Due to complexity and their monolithic architecture, it is difficult to tune these data management systems for consistent performance. In this paper, we extend our existing work of Cellular DBMS with the concept of autonomy. We present an aspect-oriented programming based model that enables us to monitor and evolve cells during data management operations for consistent performance.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Validating and dynamically adapting and composing features in concurrent product-lines applications\n", "abstract": " With the pressing in-time-market towards customized services, software product lines (SPL) are increasingly characterizing most of software landscape. SPL are mainly structured through offered features, where consistent composition and dynamic variability are the driving forces. We contribute to these two challenging problems when distribution and correctness are at stake. First, we soundly specify and validate any feature-oriented requirements using a component-based Petri nets framework referred to as co-nets. For rapid-prototyping, we semantically interpret in true-concurrent rewriting logic. For consistently composing features, a flexible feature-algebra is proposed. Finally, for runtime adaptability and integration of features, we leverage co-nets with an explicit aspectual-level, where features can be dynamically (un)woven on running components. The approach is thoroughly explained using a feature\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Efficiently Locating Web Services using a Sequence-based Schema Matching Approach.\n", "abstract": " Locating desired Web services has become a challenging research problem due to the vast number of available Web services within an organization and on the Web. This necessitates the need for developing flexible, effective, and efficient Web service discovery frameworks. To this purpose, both the semantic description and the structure information of Web services should be exploited in an efficient manner. This paper presents a flexible and efficient service discovery approach, which is based on the use of the Pr\u00fcfer encoding method to construct a one-to-one correspondence between Web services and sequence representations. In this paper, we describe and experimentally evaluate our Web service discovery approach.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Understanding the schema matching problem\n", "abstract": " Schema matching plays the central role in many applications that require interoperability between heterogeneous data sources. The best way to attain comprehensive understanding of the schema matching problem is to construct a complete, if possible, problem formulation. Schema matching has been intensively researched and many matching systems have been developed. However, specifications of the schema matching problem being solved by these systems do not exist, or if it exists do not take uncertainty problems into account. In this paper, we propose the use of the fuzzy constraint problem (FCP) as a framework to model and understand the schema matching problem. In an effort to achieve more generic approach, we first transform the schema matching problem into a graph matching problem by transforming schemas to be matched into a common model namely rooted labeled graphs. Then, with the aid of this common model, we formulate the graph matching problem into a fuzzy constraint problem. By formalizing the schema matching problem as a FCP, we could express it as a combinatorial problem with soft constraints which enables us dealing with inherent uncertainty in schema matching.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Beitragsband zum Workshop Grundlagen und Anwendungen Mobiler Informationstechnologie des GI-Arbeitskreises Mobile Datenbanken und Informationssysteme: Heidelberg, 23.-24. M\u00e4rz 2004\n", "abstract": " Beitragsband zum Workshop 'Grundlagen und Anwendungen mobiler Informationstechnologie' des GI-Arbeitskreises Mobile Datenbanken und Informationssysteme, Heidelberg, 23.-24. Maerz 2004 - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/150986 Title : Beitragsband zum Workshop 'Grundlagen und Anwendungen mobiler Informationstechnologie' des GI-Arbeitskreises Mobile Datenbanken und Informationssysteme, Heidelberg, 23.-24. Maerz 2004 Beitragsband zum Workshop 'Grundlagen und Anwendungen mobiler Informationstechnologie' des GI-Arbeitskreises Mobile Datenbanken und Informationssysteme, Heidelberg, 23.-24. Maerz 2004 Authors : Hoepfner, H. ; Saake, G. (eds.) ; Corporate author : Magdeburg Univ. (Germany). Fakultaet fuer \u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "RAM-SE08-ECOOP08 Workshop on Reflection, AOP, and Meta-Data for Software Evolution\n", "abstract": " Software evolution and adaptation is a research area, as the name states, in continuous evolution, that offers stimulating challenges for both academic and industrial researchers. The evolution of software systems, to face unexpected situations or just for improving their features, relies on software engineering techniques and methodologies. Nowadays a similar approach is not applicable in all situations eg, for evolving nonstopping systems or systems whose code is not available.Reflection and aspect-oriented programming are young disciplines that are steadily attracting attention within the community of object-oriented researchers and practitioners. The properties of transparency, separation of concerns, and extensibility supported by reflection and aspect-oriented programming have largely been accepted as useful for software development and design. Reflective features have been included in successful software development technologies such as the Java language and the .NET framework. Reflection has proved to be useful in some of the most challenging areas of software engineering, including Component-Based Software Development (CBSD), as demonstrated by extensive use of the reflective concept of introspection in the Enterprise JavaBeans component technology.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "A two-level temporal logic for evolving specifications\n", "abstract": " Traditional information system specifications are fixed: the rules of the system are frozen at specification time. In practice, most systems have to change their rules in unexpected ways during their lifetime. We present here a simple variant of a temporal logic that deals with specification evolution. It is a linear time temporal logic with two levels of time: intervals, interrupted by mutations (changes of rules), which compose lives of the system. We present a complete axiom system and complexity results, which show a large compatibility with classical linear temporal logic.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Limiting result cardinalities for multidatabase queries using histograms\n", "abstract": " Integrating, cleaning and analyzing data from heterogeneous sources is often complicated by the large amounts of data and its physical distribution which can result in poor query response time. One approach to speed up the processing is to reduce the cardinality of results \u2013 either by querying only the first tuples or by obtaining a sample for further processing. In this paper we address the processing of such queries in a multidatabase environment. We discuss implementations of the query operators, strategies for their placement in a query plan and particularly the usage of histograms for estimating attribute value distributions and result cardinalities in order to parameterize the operators.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "An Appropriate Semantics for Distributed Active Object Oriented Databases on the Basis of Co-nets Approach\n", "abstract": " The purpose of this paper is to present first results towards an appropriate approach for formally specifying in a uniform and simple way all aspects characterizing distributed active object-oriented databases (AOODB). Referred to as CO-NETS, the approach is based on a complete and a formal integration of OO concepts and constructions into a an appropriate variety of algebraic Petri nets. The CO-NETS semantics is expressed in rewriting logic with a full exhibition of a true intra-as well as inter-object concurrency. The suitability of the approach for modeling AOODB may be highlighted as follows. First, it straightforwardly captures different forms of inheritance, object composition and aggregation leading to (a hierarchy of) complex classes we named components. Second, for constructing more complex OODB as interacting components via explicit interfaces, we propose an inter-component interaction pattern. Third, for querying and viewing the resulting CO-NETS database, first ideas towards an appropriate query-pattern are forwarded. Fourth, for composing business rules, we propose an appropriate event algebras which may be regarded as suitable extension of the commonly used ECA rules. Last but least, time constraints are captured by associating time-stamps with time-dependent events and transitions.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Integrating execution dependencies into the transaction closure framework\n", "abstract": " The transaction closure framework provides means to describe and reason about different kind of dependencies between interrelated transactions. In this paper, we investigate execution dependencies for describing certain control flows among related transactions of a transaction closure. In particular, we consider the transitivity property for all kinds of transaction execution dependencies and present a complete and minimal set of rules for reasoning about the transitivity of execution dependencies. Furthermore, we analyze the relationship between execution and termination dependencies and point out that some dependency combinations are incompatible. Using derived transitive dependencies, we are able to conclude how arbitrary transactions of a transaction closure are transitively interrelated and, thus, to detect contradictory dependency specifications as well as superfluous transactions.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Integrity Independence in Object-Oriented Database Systems\n", "abstract": " Applying the feature of integrity independence means that we can modify constraints without modifying and hence recompiling updating transactions and applications. Implicit constraints of OODBS are a superset for almost all constraints that are considered as explicit for other data models, in particular for the relational model JQ92, GJ91]. However, the principle of integrity independence is still missing in OODBS. Following one basic concept of the object-oriented paradigm, encapsulation, the explicit constraints are maintained through encoding them in methods.The aim of this short paper is to present an approach for implementing the feature of integrity independence formally and in terms of basic OO data model concepts only. That is, we will consider the problem of how state constraints can be compiled, stored and manipulated using OO data model notions and operations. In this paper we use a generic OO data model that has basic features of the OODBMS manifesto presented in ABD+89]. In addition, we assume that the inverse relationship is maintained by the model automatically. Examples of such a model are O2 BDK92] and GOM KM94]. Here we follow the notations of O2. Integrity constraints are range-restricted w in prenex normal form. Due to space limitation and for sake of exposition, we only consider local and inter-class constraints, that is constraints on relationships between di erent classes. We also assume that relationships between classes are one-to-one. Thus the quanti er structure of constraint will only contains universally quanti ed variables. As an example database we will use the following example.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Extending temporal logic for capturing evolving behaviour\n", "abstract": " The known approaches to object specification based on first-order temporal logic fail in capturing the often occurring need to change the dynamic behaviour of a system during lifetime of that system. Usually all possible behaviours have to be described in advance, i.e. at specification time. Therefore, we here present an extension going beyond first-order temporal logic. Now, it becomes possible to specify ways of dynamically changing the behaviour of a system during lifetime. This can be done by giving each object an additional (non-first-order) attribute. The value of this attribute contains a set of first-order formulas being the currently valid behaviour specification. In addition, this approach can easily be extended for introducing a way of default reasoning.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "F\u00f6derierung heterogener Datenbanksysteme und lokaler Datenhaltungskomponenten zur system\u00fcbergreifenden Integrit\u00e4tssicherung - Grundlagen und Ziele des Projektes SIGMA_FDB\n", "abstract": " In diesem Beitrag geben wir einen \u00dcberblick \u00fcber das Projekt SIGMA                 FDB               , welches sich mit der Entwicklung einer Basis-Informationsinfrastruktur als Grundlage einer integrierten und einheitlichen Datenhaltung f\u00fcr alle Phasen der Fabrikplanung befa\u00dft. Dazu soll ein f\u00f6deriertes heterogenes Informationssystem entstehen, das als Rahmensystem zur Integration aller an der Fabrikplanung beteiligten Software-Werkzeuge einschlie\u00dflich deren lokaler Datenbest\u00e4nde dient. Damit sollen bislang separate Werkzeuge, die f\u00fcr Produktentwurf, Produktionsplanung und Fabriksimulation eingesetzt werden oder daf\u00fcr noch entwickelt werden, synergetisch zusammengef\u00fcgt werden. Zentrale Aufgabe eines solchen f\u00f6derierten Informationssystems ist neben der Bereitstellung einer homogenen Datenbankschnittstelle f\u00fcr globale Anwendungen die system\u00fcbergreifende Gew\u00e4hrleistung der\u00a0\u2026", "num_citations": "4\n", "authors": ["1503"]}
{"title": "Expressing temporal behaviour with extended ECA rules\n", "abstract": " The ECA (EventConditionAction) model constitutes the underlying theoretical model for active databases. The action of an ECA rule is triggered, when the event is detected and the condition is satisfied. Currently, only predicate logic expressions are allowed as conditions in ECA rules. We want to consider the use of temporal logic expressions instead. This allows the specification of situations involving the temporal database development. Temporal conditions are necessary for, among other things, the maintenance of dynamic integrity constraints. We introduce initialized temporal ECA rules which allow the explicit specification of the evaluation-periods of temporal conditions. Moreover we discuss the construction and evaluation of an IT-ECA automaton for the evaluation of intialized temporal ECA rules.", "num_citations": "4\n", "authors": ["1503"]}
{"title": "SQLValidator\u2013An Online Student Playground to Learn SQL\n", "abstract": " The Structured Query Language (SQL) is the most widely-used language in database-related courses. As a consequence, writing SQL queries is a fundamental expectation from any university course in database systems. Practical exercises are an essential part of the SQL learning experience. These exercises enable participants to practice and acquire experience in the use of the different SQL concepts, such as clauses, predicates, and expressions. To this end, we developed the tool SQLValidator as a web-based interactive tool for learning and practicing SQL. Apart from using it for teaching, we also use it to administer questionnaires and practice tests to improve students\u2019 learning experience. In this paper, we present the architecture and functions of SQLValidator. In order to assess the usefulness of SQLValidator, we monitor the performance of our students based on the semester activities and examinations\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "RPP Algorithm: A Method for Discovering Interesting Rare Itemsets\n", "abstract": " The importance of rare itemset mining stems from its ability to discover unseen knowledge from datasets in real-life domains, such as identifying network failures, or suspicious behavior. There are significant efforts proposed to extract rare itemsets. The RP-growth algorithm outperforms previous methods proposed for generating rare itemsets. However, the performance of the RP-growth degrades on sparse datasets, and it is costly in terms of time and memory consumption. Hence, in this paper, we propose the RPP algorithm to extract rare itemsets. The advantage of the RPP algorithm is that it avoids time for generating useless candidate itemsets by omitting conditional trees as RP-growth does. Furthermore, our RPP algorithm uses a novel data structure, RN-list, for creating rare itemsets. To evaluate the performance of the proposed method, we conduct extensive experiments on sparse and dense datasets\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Data Quality as a Critical Success Factor for User Acceptance of Research Information Systems\n", "abstract": " In our present paper, the influence of data quality on the success of the user acceptance of research information systems (RIS) is investigated and determined. Until today, only a little research has been done on this topic and no studies have been carried out. So far, just the importance of data quality in RIS, the investigation of its dimensions and techniques for measuring, improving, and increasing data quality in RIS (such as data profiling, data cleansing, data wrangling, and text data mining) has been focused. With this work, we try to derive an answer to the question of the impact of data quality on the success of RIS user acceptance. An acceptance of RIS users is achieved when the research institutions decide to replace the RIS and replace it with a new one. The result is a statement about the extent to which data quality influences the success of users\u2019 acceptance of RIS. View Full-Text", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Automated Selection and Quality Assessment of Primary Studies: A Systematic Literature Review\n", "abstract": " Researchers use\u00a0systematic literature reviews (SLRs) to synthesize existing evidence regarding a research topic. While being an important means to condense knowledge, conducting an SLR requires a large amount of time and effort. Consequently, researchers have proposed semi-automatic techniques to support different stages of the review process. Two of the most time-consuming tasks are (1) to select primary studies and (2) to assess their quality. In this article, we report an SLR in which we identify, discuss, and synthesize existing techniques of the software-engineering domain that aim to semi-automate these two tasks. Instead of solely providing statistics, we discuss these techniques in detail and compare them, aiming to improve our understanding of supported and unsupported activities. To this end, we identified eight primary studies that report unique techniques that have been published between 2007\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Indicating studies\u2019 quality based on open data in digital libraries\n", "abstract": " Researchers publish papers to report their research results and, thus, contribute to a steadily growing corpus of knowledge. To not unintentionally repeat research and studies, researchers need to be aware of the existing corpus. For this purpose, they crawl digital libraries and conduct systematic literature reviews to summarize existing knowledge. However, there are several issues concerned with such approaches: Not all documents are available to every researcher, results may not be found due to ranking algorithms, and it requires time and effort to manually assess the quality of a document. In this paper, we provide an overview of the publicly available information of different digital libraries in computer science. Based on these results, we derive a taxonomy to describe the connections between this information and discuss their suitability for quality assessments. Overall, we observe that bibliographic\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Investigations of concept development to improve data quality in research information systems\n", "abstract": " The implementation of research information systems at German universities and research institutions is currently a topical subject. With their help, the documentation and reporting of the research activities of the respective institution can be supported and a significant part of the data incurring there can be managed. As there are usually many data sources available and the collection, transmission, and integration of research information in different research information systems can lead to different data errors which can have various negative effects on data quality. It is necessary to recognize these errors early and to handle them efficiently, so that users can get better results. For this reason, this paper examines data quality in research information systems and introduces measurement and enhancement methods that enable organizations to secure their quality of data.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Low-latency transaction execution on graphics processors: Dream or reality?\n", "abstract": " In this paper we take a close look into the role of GPUs for executing OLTP workloads, with a focus on CRUD operatorbased processing, as opposed to more complex OLTP transactions. To this end we develop a prototype system supporting GPU and CPU variants of DSM and NSM processing, with a delegation-based approach that uses a singlethread scheduler to manage concurrency control, enabling reads with guaranteed bounded staleness. We evaluate our prototype using workloads from the Yahoo! cloud serving benchmark. We report the impact of layout choices, batching configuration and concurrency control designs. Through our study we are able to pinpoint that the contradicting needs in GPU processing for small batches to reduce waiting time, but large batches to reduce execution time, is the essential challenge for OLTP on these processors, affecting all design choices we study. Hence, we propose two preconditions for supporting OLTP with GPUs, aiming to guide researchers in finding scenarios for extending the applicability of GPUs in supporting data management tasks.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Variability management in infrastructure as a service: Scenarios in cloud deployment models\n", "abstract": " Flexible IT landscapes and efficient management of resources are key issues for enterprises. Variability is an important aspect for flexible IT landscape. The main part of the paper discusses variability characteristics in Infrastructure as a Service (IaaS). We show what kind of variability is possible at IaaS. In addition, a case study is provided that shows practical examples of infrastructure variability and how these variability issues can be managed using variability solutions and the software configuration tool Puppet. Software configuration tools enable us to treat infrastructure as code and to meet the varying requirements of customers. In the end, we summarize our paper and provide an outlook for future work.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Cloudcraft: Cloud-based data management for mmorpgs\n", "abstract": " Massively Multiplayer Online Role-Playing Games (MMORPGs) are very sophisticated applications, which have significantly grown in popularity since their early days in the mid-90s. Along with growing numbers of users the requirements on these systems have reached a point where technical problems become a severe risk for the commercial success. Within the CloudCraft project we investigate how Cloud-based architectures and data management can help to solve some of the most critical problems regarding scalability and consistency. In this article, we describe an implemented working environment based on the Cassandra DBMS and some of the key findings outlining its advantages and shortcomings for the given application scenario.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Cost-Aware Query Optimization during Cloud-Based Complex Event Processing\n", "abstract": " Complex Event Processing describes the problem of timely and continuous processing of event streams. The load of Complex Event Processing systems can vary (eg, event rates). Static resource provision leads to higher monetary costs because enough resources have to be provided to efficiently handle peak loads. Therefore, most of the time the resources will not be fully utilized. One way to achieve scalable processing and elastical resource allocation fitting varying requirements is to use Cloud Computing. Properties of Cloud Computing are the pay-as-you-go-payment model and high availability. These properties can be used in Complex Event Processing systems to minimize the monetary costs of systems while satisfying Service Level Agreements. Complex Event Processing systems must continuously optimize the event processing to adapt to varying loads without violation of Service Level Agreements. To guarantee efficiency, the optimization cost must be considered, leading to cost savings without violating the Service Level Agreements. In this work, we discuss factors, which should be considered during the optimization of cloud-based Complex Event Processing systems that use the pause-train-resume strategy to migrate operators. Furthermore, we propose heuristics to estimate the cost of these factors. In our experiments, the cost could be decreased by 15% by using a cost-aware optimizer. This proofs that the costs of cloud-based Complex Event Processing systems can be further decreased if optimization is cost-aware.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Flexibility in SOA Operations: The Need for a Central Service Component\n", "abstract": " Efficient, controlled, and managed IT operations are strongly influenced by the architecture of a solution. A service-oriented architecture has a high flexibility and can respond to environmental changes and requirements by adapting the service orchestration. To take advantage of this flexibility in SOA, the traditional operations system oriented approach has to be changed into a process oriented approach. Furthermore, this results in fundamental new requirements for operation tools and services. In addition to common central elements of SOA, another fundamental element is required for process-oriented operations: a central operations cockpit. We present which requirements are raised for the basic components of SOA and how the central operations cockpit is designed in SOA. We use as an example a subset of ITIL processes for operation.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Heuristics-based workload analysis for relational dbmss\n", "abstract": " Database systems are widely used in heterogeneous applications. However, it is difficult to decide which database management system meets requirements of a certain application best. This observation is even more true for scientific and statistical data management, because new application and research fields are often first observed in this domain. New requirements are often implied to data management while discovering unknown research and applications areas. That is, heuristics and tools do not exist to select an optimal database management system. We develop a decision support framework to support application-performance analyses on database management systems. We use mappings and merge workload information to patterns. We present heuristics for performance estimation to select the optimal database management system for a given workload. We show, these heuristics improve our\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Intra-service adaptability for eca-centric web services using contract and aspect\n", "abstract": " In today's competitive environment, enterprise business has to adapt the changes driven by the market in order to achieve targeted goals. Information systems are used to manage the enterprise business, and thus adaptation at information system level is very important. It raises the need for architectures and mechanisms that support such adaptation at finer granularity (process activity level). Service-oriented computing and aspect-oriented programming approaches promises high adaptability in rule-based information systems. In this paper, authors propose an adaptable ECA (Event-Condition-Action) centric architecture with implementation mechanism, which is based on service-oriented computing and aspectoriented programming concepts to adapt changes at intra-activity level for rule-based enterprise information systems.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Natural Language Processing and Information Systems: 5th International Conference on Applications of Natural Language to Information Systems, NLDB 2000, Versailles, France\u00a0\u2026\n", "abstract": " This book includes the papers presented at the fifth International Conference on Application of Natural Language to Information Systems (NLDB 2000) which was held in Versailles (France) on June 28-30. Following NLDB95 in Versailles, NLDB96 in Amsterdam, NLDB97 in Vancouver, and NLDB99 in Klagenfurt, NLDB 2000 was a forum for exchanging new research results and trends on the benefits of integrating Natural Language resources in Information System Engineering. Since the first NLDB workshop in 1995 it has become apparent that each aspect of an information system life cycle may be improved by natural language techniques: database design (specification, validation, conflict resolution), database query languages, and application programming that use new software engineering research (natural language program specifications). As information systems are now evolving into the communication area, the term databases should be considered in the broader sense of information and communication systems. The main new trend in NLDB 2000 is related to the WEB wave: WEB querying, WEB answering, and information retrieval. Among 47 papers submitted from 18 countries, the program committee selected 29 papers to be presented during the conference. Besides these regular papers, two invited talks (given by Pr. Reind P. van de Riet and Pr. Maurice Gross), and a set of posters and demonstrations are also included in these proceedings.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Specifying and validating train control systems using an appropriate component-based Petri nets model\n", "abstract": " The purpose of this paper is to emphasize the adequacy of the Co-NETS approach in specifying and validating a substantial part of the European train control system, namely the generalized railroad crossing system (GRC). This adequacy is particularly expressed by handling in a satisfactory way the following GRC aspects. First, due to the Co-NETS capabilities for modeling components as a hierarchy of object oriented classes using different forms of inheritance, it is possible to cope with the size complexity in each component, namely train, gate and control system. Second, as the Co-NETS approach provides explicit interfaces for communicating different components, interaction of components in this application becomes very manageable by hiding the complex internal behaviour of each component. Third, by providing a full exhibition of intra- as well inter-object concurrency through Co-NETS semantics\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Datenbank-und Visualisierungstechnologie in der Informationsfusion\n", "abstract": " In vielen Anwendungsbereichen besteht die Aufgabe, Daten oder Informationen aus verschiedenen, zum Teil heterogenen Quellen zu kombinieren, zu verdichten und daraus Informationen einer neuen Qualit\u00e4t abzuleiten. Wesentliche Kernfunktionen dieses als Informationsfusion bezeichneten Prozesses sind dabei durch Methoden der Datenintegration und der Datenanalyse/Data Mining bereitzustellen. Die gewachsenen Strukturen der heute genutzten Informationsquellen und die damit im Zusammenhang stehenden Probleme wie Heterogenit\u00e4t, Inkonsistenz oder Ungenauigkeit der Daten sind mit den aktuell verf\u00fcgbaren Techniken nur bedingt beherrschbar. Ausgehend vom aktuellen Stand der Forschung diskutiert der vorliegende Beitrag Anforderungen an Datenbank-und Visualisierungstechnologien aus Sicht der Informationsfusion und zeigt aktuelle Forschungsrichtungen auf.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "On the Benefits of Rewrite Logic as a Semantics for Algebraic Petri Nets in Computing Siphons and Traps\n", "abstract": " A lot of research is being done to directly apply to high level nets structural techniques similar to those existing for place transition nets. In particular, the question of computing siphons for high level nets is of intrest since the well known results about relations between liveness of a net and control of its siphons. In this paper, we show how one can derive an efficient computing of siphons containing a given place from an appropriate interpretation of algebraic Petri nets in rewrite logic as a true concurrent semantics. Within this method, the verification procedure of symbolic siphons is reduced to two easy-understandable inference rewrite rules instead to a completeness problem as in the recent approach of K. Schmidt (97). Moreover, the proposed method can be implemented using rewriting techniques.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Cooperative information systems modelling and validation using the CO-NETS approach: the chessmen making shop case study\n", "abstract": " The CO-NETS approach that we are developing is an object oriented specification model based on a sound and complete integration of object oriented (00) concepts and constructions into a variant of algebraic Petri nets. The CO-NETS approach is interpreted in rewriting logic and is particularly suited for specifying and validating advanced information systems as autonomous, distributed, but yet cooperative components. This suitability is especially reflected by the following main CO-NETS features. First, the approach is based on a percep-tion of classes rather as modules with hidden (structural and behavioural) features enhancing autonomy and external features which may be exchanged with the environment and other classes. Second, the approach allows an incremental constructions of complex components, as a hierarchy of modules, through different forms of inheritance, object composition and ag-gregations; where each component behaves with respect to an appropriate intra-component evolution pattern supporting intra-as well as inter-object concurrency. Third, for interacting components, an appropriate inter-component interaction pattern is proposed promoting concurrency and keeping encapsulated all local features of communicating components. Fourth, due to their interpretation in rewriting logic, CO-NETS modules can be rapid-prototyped using concurrent rewriting techniques.Besides a didactic presentation of this approach, the objective of this paper is to en-hance its practicability by handling a complex case study dealing with a Chessmen Making Shop (CMS) as a specific variant of a complex production system. In this system\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Towards a New Semantics for Mondel Specifications Based on the CO-Net Approach\n", "abstract": " We present first results towards a tailored object-oriented (OO) specification model for distributed systems. Referred to as CO-Nets, the model is a variant of object Petri nets. CO-Nets support an integration of object-oriented concepts and constructions into an appropriate form of algebraic Petri nets, two communication patterns for intra- and inter-object interaction enhancing modularity and concurrency without violating the encapsulated aspects, and last but not least the interpretation of the behaviour of the constructed model in rewriting logic allowing validation by rapid-prototyping using rewrite techniques. We assess the suitability of the CO-Nets approach by providing the Mondel specification language with a formal semantics based on it. Mondel has been designed for developing distributed applications and supports a state-oriented style of description, synchronous communication based on the rendez\u00a0\u2026", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Derived Transaction Termination Dependencies: An Algorithm for Computing Transitivity Rules\n", "abstract": " Complex applications consist of a large set of transcations which are interrelated. There are different kinds of dependencies among transactions of a complex application, eg termination dependencies which are constraints on the occurrence of significant transaction events. Often, it is very difficult for the application/transaction designer to get a grasp of the transitive relationships among the transactions of a complex application. In this paper, we introduce the notion of transaction closure as a generalization of nested transactions. A transaction closure comprises all transactions which are transitively initiated by one (root) transaction. The relationships among transactions of a transaction closure are specified by different kinds of dependencies. We identify transcation termination dependencies which determine reasonable termination event combinations of two related transactions. These basic termination dependencies are further refined considering the aspect of compensatable transactions. In particular, we consider the transitivity property for all kinds of transaction termination dependencies discussed in this paper and present a set of rules for reasoning about the transitivity of these dependencies. Thus, we are able to conclude how two arbitrary transcations are transitively interrelated. This issue is essential for understanding the entire semantics of a complex application.(orig.);", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Towards agent-oriented specification of information systems\n", "abstract": " Objects in information systems usually have a very long life-span. Therefore, it often happens that during the life of an object external requirements are changing, eg changes of laws. Such changes often require the object to adopt another behavior. In consequence, it is necessary to get a grasp of dynamically changing object behavior. Unfortunately, not all possible changes can in general be taken into account in advance at specification time. Hence, current object specification approaches cannot deal with this problem. Flexible extensions of object specification are needed to capture such situations. The concept of agent which can be seen as a further development of the concept of object seems to provide a more adequate basis for modeling such information system dynamics. In comparison to traditional objects, agents are flexible in that sense they may change their behavior dynamically during system run-time, ie the behavior of an agent is not (or can not be) completely determine...", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Specifying Evolving Temporal Behaviour\n", "abstract": " The usual approaches to object specification based on first-order temporal logic fail in capturing the often occurring need to change the dynamic behaviour of a system during lifetime of that system. Usually all possible behaviours have to be described in advance, ie at specification time. Therefore, we here present an extension going beyond first-order temporal logic. Now, it becomes possible to specify ways of dynamically changing the behaviour of a system during lifetime. This can be done by giving each object an additional (non-first-order) attribute. The value of this attribute contains a set of first-order formulas being the currently valid behaviour specification. In addition, this approach can easily be extended for introducing a way of default reasoning. 1 Motivation In the area of information systems, temporal logic is a widely accepted means to specify dynamic behaviour of objects. This is due to the fact that information systems (as a generalization of database...", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Integration einer Prototyping-Umgebung durch Objektorientierte Spezifikation\n", "abstract": " Eine Prototyping-Umgebung auf Basis formaler Spezi kation vereint die Anforderungen von Entwicklern, die eine vertragsbildende Formalisierung ihrer Aktivit aten ben otigen und Anwendern, die Erfahrungsgewinne durch experimentelles Vorgehen erlangen k onnen, gleicherma en. Diesen Anforderungen entgegen lasten verschiedene Probleme auf existierenden Software-Entwicklungsumgebungen. Hauptschwierigkeiten sind der Bruch zwischen den verschiedenen zumeist nicht konsistent integrierten Sichten auf das konzeptionelle Modell und zwischen den verschiedenen Entwurfsphasen, die fehlende Eindeutigkeit bei der eher informalen Modellierung und die fehlende transparente Integrationsbasis. Das Ergebnis sind komplexe und schwierig erweiterbare Systeme.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "A possible world semantics for updates by versioning\n", "abstract": " Recently a rule-language for updating objects based on versioning has been proposed [1]. The units for update are base properties of the objects Updates are defined by rules; several rules may be used to a specific update. Rules are evaluated in a bottom-up way according to a certain stratification. Up to now semantics is defined by the fixpoint resulting from the bottom-up evaluation procedure. In this paper a possible-world semantics is introduced to give a declarative semantics to the rule-language.", "num_citations": "3\n", "authors": ["1503"]}
{"title": "Implementation and user acceptance of research information systems\n", "abstract": " PurposeThe purpose of this paper is to present empirical evidence on the implementation, acceptance and quality-related aspects of research information systems (RIS) in academic institutions.Design/methodology/approachThe study is based on a 2018 survey with 160 German universities and research institutions.FindingsThe paper presents recent figures about the implementation of RIS in German academic institutions, including results on the satisfaction, perceived usefulness and ease of use. It contains also information about the perceived data quality and the preferred quality management. RIS acceptance can be achieved only if the highest possible quality of the data is to be ensured. For this reason, the impact of data quality on the technology acceptance model (TAM) is examined, and the relation between the level of data quality and user acceptance of the associated institutional RIS is addressed.Research limitations/implicationsThe data provide empirical elements for a better understanding of the role of the data quality for the acceptance of RIS, in the framework of a TAM. The study puts the focus on commercial and open-source solutions while in-house developments have been excluded. Also, mainly because of the small sample size, the data analysis was limited to descriptive statistics.Practical implicationsThe results are helpful for the management of RIS projects, to increase acceptance and satisfaction with the system, and for the further development of RIS functionalities.Originality/valueThe number of empirical studies on the implementation and acceptance of RIS is low, and very few address in this context the question of data\u00a0\u2026", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Integrated Cycles for Urban Biomass as a Strategy to Promote a CO2-Neutral Society\u2014A Feasibility Study\n", "abstract": " The integration of closed biomass cycles into residential buildings enables efficient resource utilization and avoids the transport of biowaste. In our scenario called Integrated Cycles for Urban Biomass (ICU), biowaste is degraded on-site into biogas that is converted into heat and electricity. Nitrification processes upgrade the liquid fermentation residues to refined fertilizer, which can be used subsequently in house-internal gardens to produce fresh food for residents. Our research aims to assess the ICU scenario regarding produced amounts of biogas and food, saved CO2 emissions and costs, and social\u2013cultural aspects. Therefore, a model-based feasibility study was performed assuming a building with 100 residents. The calculations show that the ICU concept produces 21% of the annual power (electrical and heat) consumption from the accumulated biowaste and up to 7.6 t of the fresh mass of lettuce per year in a 70 m2 professional hydroponic production area. Furthermore, it saves 6468 kg CO2-equivalent (CO2-eq) per year. While the ICU concept is technically feasible, it becomes economically feasible for large-scale implementations and higher food prices. Overall, this study demonstrates that the ICU implementation can be a worthwhile contribution towards a sustainable CO2-neutral society and decrease the demand for agricultural land.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Optimising Operator Sets for Analytical Database Processing on FPGAs\n", "abstract": " The high throughput and partial reconfiguration capabilities of modern FPGAs make them an attractive hardware platform for query processing in analytical database systems using overlay architectures. The design of existing systems is often solely based on hardware characteristics and thus does not account for all requirements of the application. In this paper, we identify two design issues impeding system integration of low-level database operators for runtime-reconfigurable overlay architectures on FPGAs: First, the granularity of operator sets within each processing pipeline; Second, the mapping of query (sub-)graphs to complex hardware operators. We solve these issues by modeling them as variants of the subgraph isomorphism problem. Via optimised operator fusion guided by a heuristic we reduce the number of required reconfigurable regions between 30% and 85% for relevant TPC-H database benchmark\u00a0\u2026", "num_citations": "2\n", "authors": ["1503"]}
{"title": "ERST: Leveraging Topic Features for Context-Aware Legal Reference Linking.\n", "abstract": " As legal regulations evolve, companies and organizations are tasked with quickly understanding and adapting to regulation changes. Tools like legal knowledge bases can facilitate this process, by either helping users navigate legal information or become aware of potentially relevant updates. At their core, these tools require legal references from many sources to be unified, eg, by legal entity linking. This is challenging since legal references are often implicitly expressed, or combined via a context. In this paper, we prototype a machine learning approach to link legal references and retrieve combinations for a given context, based on standard features and classifiers, as used in entity resolution. As an extension, we evaluate an enhancement of those features with topic vectors, aiming to capture the relevant context of the passage containing a reference. We experiment with a repository of authoritative sources on German law for building topic models and extracting legal references and report that topic models do indeed contribute in improving supervised entity linking and reference retrieval.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Protobase: It's About Time for Backend/Database Co-Design\n", "abstract": " In this interactive demonstration, we show the current state of Protobase, our main-memory analytic document store that is designed from scratch to enable rapid prototyping of efficient microservices that perform analytics and explorations on (third-party) JSON-like documents stored in a novel columnar binary-encoded format, called the Cabin file format. In contrast to other solutions, our database system exposes neither a particular query language, nor a fixed REST API to its clients. Instead, the entire user-defined backend logic, whose user code is written in Python, is placed inside a sandbox that runs in the systems process. Protobase in turn exposes a user-defined REST API that the (frontend) application interacts with. Thus, our system acts as a backend server while at the same time avoids full exposure of its database to the clients. Consequently, a Protobase instance (database + user code + REST API) serves as (the entire) microservice -potentially minimizing the number of systems running in a typical analytic software stack. In terms of execution performance, Protobase therefore takes the inter-process communication overhead between backend and database system out of the picture and heavily utilizes columnar binary document storage to scale-up for analytic queries. Both features lead to a notable performance gain for non-trivial services, potentially minimizing the number of required nodes in a cloud setting, too. In our demo, we overview Protobases internals, spot major design decisions, and show how to prototype a scholarly search engine managing the Microsoft Academic Graph, a real-world scientific paper graph of roughly 154\u00a0\u2026", "num_citations": "2\n", "authors": ["1503"]}
{"title": "SIMD Acceleration for Main-Memory Index Structures\u2013A Survey\n", "abstract": " Index structures designed for disk-based database systems do not fulfill the requirements for modern database systems. To improve the performance of these index structures, different approaches are presented by several authors, including horizontal vectorization with SIMD and efficient cache-line usage.               In this work, we compare the adapted index structures Seg-Tree/Trie, FAST, VAST, and ART and evaluate the usage of SIMD within these. We extract important criteria of these adaptations and weight them according to their impact on the performance. As a result, we infer adaptations that are promising for our own index structure Elf.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Adaptive Data Processing in Heterogeneous Hardware Systems.\n", "abstract": " In recent years, Database Management System have seen advancements in two major sectors, namely functional and hardware support. Before, a traditional DBMS was sufficient for performing a given operation, whereas a current DBMS is required to perform complex analytical tasks like graph analysis or OLAP. These operations require additional functions to be added to the system for processing. Also, a similar evolution is seen in the underlying hardware. This advancement in both functional and hardware domain of DBMS requires modification of its overall architecture. Hence, it is evident that an adaptable DBMS is necessary for supporting this highly volatile environment. In this work, we list the challenges present for an adaptable DBMS and propose a conceptual model for such a system that provides interfaces to easily adapt to the software and hardware changes.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Forward secure searchable symmetric encryption\n", "abstract": " Data outsourcing to third party clouds poses numerous data security threats. Access by unauthorized users is one of the security threat to the outsourced data. Unauthorized access can be avoided by encrypting the data before outsourcing. However, encrypting data before outsourcing renders it unsearchable to the data owner. Searchable encryption schemes are developed to specifically target this problem. A dynamic searchable encryption is the one that allows the data owner to add or delete a file after data outsourcing. Dynamic searchable encryption schemes are vulnerable to two specific security threats that are not applicable to the static searchable encryption schemes namely forward privacy and backward privacy. Forward privacy requires that the addition of a file should not reveal the presence of a previously searched keyword. Backward privacy requires that a search should not return the file identifier of a\u00a0\u2026", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Developing an elevator with feature-oriented programming\n", "abstract": " In this chapter, we illustrate how to implement software product lines with feature-oriented programming in FeatureIDE. The goal of this chapter is to illustrate and practice how feature-oriented programming works in detail and how the workflow of FeatureIDE implements the product line. We use our running example of the elevator product line and extend it with features for control logics Sabbath and FIFO and with an optional Service feature.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Quality assurance for feature models and configurations\n", "abstract": " Feature modeling and product configuration are central parts of software product line development. They define the domain and which features are contained in the final products. Feature modeling and product configuration are manual and thus error-prone tasks. Thus, the design of a feature model comes with several pitfalls for validity. It is necessary to support the user during the creation as much as possible. A qualitative feature model is even important as it affects all other parts, such as product configuration and generation. Therefore, this chapter gives an overview on FeatureIDE\u2019s support to assure the quality of feature models and configurations.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "A Self-Tuning Framework for Cloud Storage Clusters\n", "abstract": " The well-known problems of tuning and self-tuning of data management systems are amplified in the context of Cloud environments that promise self management along with properties like elasticity and scalability. The intricate criteria of Cloud storage systems such as their modular, distributed, and multi-layered architecture add to the complexity of the tuning and self-tuning process. In this paper, we provide an architecture for a self-tuning framework for Cloud data storage clusters. The framework consists of components to observe and model certain performance criteria and a decision model to adjust tuning parameters according to specified requirements. As part of its implementation, we provide an overview on benchmarking and performance modeling components along with experimental results.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Performance Impacts in Database Privacy-Preserving Biometric Authentication\n", "abstract": " Nowadays, biometric data are more and more used within authentication processes. These data are often stored in databases. However, these data underlie inherent privacy concerns. Therefore, special attention should be paid for handling of these data. We propose an extension of a similarity verification system with the help of the Paillier cryptosystem. In this paper, we use this system for signal processing in the encrypted domain for privacy-preserving biometric authentication. We adapt a biometric authentication system for enhancing privacy. We focus on performance issues with respect to database response time for our authentication process. Although encryption implicates computational effort, we show that only small computational overhead is required. Furthermore, we evaluate our implementation with respect to performance. However, the concept of verification of encrypted biometric data comes at the cost of increased computational effort in contrast to already available biometric systems. Nevertheless, currently available systems lack privacy enhancing technologies. Our findings emphasize that a focus on privacy in the context of user authentication is available. This solution leads to user-centric applications regarding authentication. As an additional benefit, results using data mining are more difficult to be obtained in the domain of user tracking.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Classic, tool-driven variability mechanisms\n", "abstract": " Besides language-based techniques, which encode variability with available concepts within programming languages (discussed in the previous chapter), external tools can also be used to implement and manage variability.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "A Layered Architecture Approach for Large-Scale Data Warehouse Systems\n", "abstract": " A proper architecture is significant to cope with complex requirements of today\u2019s large-scale Data Warehouses. We compare the assignment of so-called reference architectures with an architecture of dedicated layers to satisfactorily face those requirements. Moreover, we point out additional expenses and resulting advantages of this layered approach.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Variables Nanodatenmanagement f\u00fcr eingebettete Systeme\n", "abstract": " Weltweit sind ca. 98 Prozent aller hergestellten Rechnersysteme als eingebettete Systeme im Einsatz. In diesem Kontext sind die Kosten f\u00fcr die Hardware ein sehr wichtiger Faktor. Deshalb kommt es bei der Softwareentwicklung darauf an, den Ressourcenbedarf zu minimieren, um so preisg\u00fcnstige Hardware einsetzen zu k\u00f6nnen. Beispielanwendungen aus dem Bereich der Sensornetzwerke zeigen, dass derartig eingebettete Rechnersysteme stark variierende Elemente klassischer Infrastruktursoftware zur Datenhaltung ben\u00f6tigen. Eine Adaption vorhandener Mehrzwecksysteml\u00f6sungen f\u00fcr diesen Bereich des Datenmanagements aus dem Gro\u00dfrechner-oder dem PC-Bereich ist auf Grund der Heterogenit\u00e4t der Hard-und Software sowie der extremen Ressourcenbeschr\u00e4nkungen nicht m\u00f6glich. Das Resultat in der Praxis ist die wiederkehrende Entwicklung \u00e4hnlicher Datenhaltungskomponenten als Teil der Anwendungssoftware.Eine im Rahmen dieser Dissertation angefertigte Analyse von vorhandenen Forschungsprototypen und kommerziellen DBMS beziehungsweise Datenhaltungskomponenten f\u00fcr eingebettete Systeme zeigt, dass der Gro\u00dfteil der komplexen Basis dieser DBMS auf Forschungsergebnissen der sp\u00e4ten achtziger beziehungsweise fr\u00fchen neunziger Jahre basiert. Eine Anpassung auf die heutigen Entwicklungen in der Softwaretechnik oder der Programmiersprachenentwicklung hat nur in Ans\u00e4tzen stattgefunden. Die Adaption der verwendeten Methoden und Techniken ist aber auf Grund der zunehmenden Heterogenit\u00e4t des Anwendungsspektrums in diesem Bereich unumg\u00e4nglich.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Improving Usability of UML Modeling Tools for Feature-Based Product Line Development\n", "abstract": " Software product line engineering (SPLE) and model-driven engineering (MDE) are powerful approaches that increase efficiency of the software engineering process. They can be combined to model-driven product line engineering (MDPLE). To further establish this combined approach, it is necessary to provide good tool support. To this end, we aim to improve the usability of an already existing tool for MDPLE: pure:: variants, which is one of the leading tools for SPLE, with its connectors to the powerful UML tools Rational Rhapsody (Rhapsody) and Enterprise Architect (Architect). Inspired by user requests, we identify usability problems of both Rhapsody and Architect when used with the connection to pure:: variants. To solve the problems, we propose concepts for a UML tool extension (eg, visualizations that improve UML model comprehension, and an editor that provides autocompletion and syntax highlighting for editing SPLErelated information of the UML model). To test whether the concepts can be realized, we implemented them for Rhapsody and Architect. During implementation, we encountered tool restrictions, such as the inability to apply visualizations to some parts of the UML tools, or to integrate our extension into Rhapsody\u2019s user interface. Nevertheless,", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Workload Representation across Different Storage Architectures for Relational DBMS\n", "abstract": " Database systems differ from small-scale stripped database programs for embedded devices with minimal footprint to large-scale OLAP applications for server devices. For relational database management systems, two storage architectures have been introduced: the row-oriented and the column-oriented architecture. To select the optimal architecture for a certain application, we need workload information and statistics. In this paper, we present a workload representation approach that enables us to represent workloads across different DBMSs and architectures. Our approach also supports fine granular workload analyses based on database operations.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Investigation of graph mining for business processes\n", "abstract": " Business process management and business intelligence are fields which gain a lot of attention in recent years. These techniques try to improve not only efficiency of processes but also save considerable cost. Graph based representation of concepts (objects, data) are also used in business domain to support aforementioned techniques. Graph mining methods are successful in many fields for discovery of new relations, knowledge, and visualization. In this paper, we briefly discuss the fields in which graph mining is successfully applied. We also discuss challenges of applying graph mining in business processes and what are the benefits.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Towards a Disciplined Engineering of Adaptive Service-oriented Business Processes\n", "abstract": " This paper propose a progressive and disciplined engineering of adaptive service-oriented business processes (SO-BPs). Each business activity is first informally governed through interaction-centric event-conditions-actions (ECA) based business rules. They are then smoothly conceptualized as transient ECA-driven architectural connectors, with roles playing service interfaces and glues capturing the service composition logic. This ECA-driven architectural conceptualization is formally validated using (still rule-centric) rewriting logic and its MAUDE language. From these founded, certified and adaptive SO-BPs, a compliant .NET based Web-services are systematically derived.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "A classification scheme for XML data clustering techniques\n", "abstract": " The proliferation of XML-based applications radically increases the need to high-performance techniques, which effectively and efficiently discover the desired knowledge. An elegant solution is to group the similar XML data based on their content and structures. In this paper, we present an overview of the most popular methodologies and implementations of clustering methods of XML data. We present a survey about current status and future trends in clustering employed for the XML data. We also present a criterion to classify the existing methods based on the exploited proximity computation approaches. We aim at introducing an integrated view which is useful when comparing XML data clustering approaches, when developing a new clustering algorithm, and when implementing an XML clustering component.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Towards enhanced compression techniques for efficient high-dimensional similarity search in multimedia databases\n", "abstract": " In this paper, we introduce a new efficient compression technique for high-dimensional similarity search in MMDBS. We propose the Active Vertice Tree which is based on concave cluster geometries. Furthermore, we briefly sketch a model for high-dimensional point alignments and specify basic requirements for high-dimensional cluster shapes. Finally, we compare the Active Vertice Tree with other methods for high-dimensional similarity search in terms of their retrieval performance.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Global transaction termination rules in composite database systems\n", "abstract": " In composite database systems, global transactions are decomposed by the global transaction manager into several global subtransactions that are executed at the corresponding component database systems. This paper shows that the execution and termination of a global transaction depend on the specified extensional assertions on the local classes as well as on the given kinds of global requests. In some cases, a global transaction is supposed to be successful if exactly one global subtransaction is successfully executed. In other cases, all global subtransactions have to be successful in order to commit the global transaction. We discuss various termination rules for global transactions and thus provide new insights into a complex problem.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Example-driven Integration of Heterogeneous Data Sources\n", "abstract": " The integration of heterogeneous databases affects two main problems: schema integration and instance integration. At both levels a mapping from local elements to global elements is specified and various conflicts caused by the heterogeneity of the sources have to be resolved. For the detection and resolution of instance-level conflicts we propose an example-driven approach. The basic idea is to combine an interactive query tool similar to query-by-example with facilities for defining and applying integration operations. This integration approach is supported by a multidatabase query language, which provides special mechanisms for conflict resolution. The foundations of these mechanisms are introduced and their usage in instance integration is presented. In addition, we discuss basic techniques for supporting the detection of instance-level conflicts.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "CO-NETS: A Formal OO Framework for Specifying and Validating Distributed Information Systems\n", "abstract": " This paper formally defines a new object oriented (OO) Petri net-based approach for specifying and rapid-prototyping distributed (information) systems-regarded as a community of interacting, through explicit interfaces, and concurrently existing objects. The proposed model, referred to as CO-Nets, is based on a complete integration of OO concepts into an appropriate variant of algebraic Petri nets named ECATNets. The CO-Nets behaviour is interpreted in rewriting logic allowing validation by rapid-prototyping using rewrite techniques. Particular to the CO-Nets approach are mainly: the modeling of a class rather as a module with a hidden part (ie structure and behaviour) and an observed imported/exported part; the capability of incrementally constructing complex components, as a hierarchy of modules, through simple and multiple inheritance with possibility of overriding, dynamic binding and associated polymorphism. Each component behaves with respect to an appropriate intra-component evolution pattern that supports intra-as well as inter-object concurrency. For interacting such components, an appropriate inter-component interaction pattern is proposed that enhances concurrency and preserves the encapsulated features of each component.(orig.);", "num_citations": "2\n", "authors": ["1503"]}
{"title": "SIGMAFDB: Overview of the Magdeburg-Approach to Database Federations.\n", "abstract": " The SIGMA FDB project attempts to offer an approach to schema integration and integegrity constraint maintenance in the field of federated database systems. In this extended abstract, we present our view on federated database systems and sketch the main results of our group's work. Especially, we briefly discuss different research aspects and implementation activities.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Prototyping object specifications using the CO-NETS approach\n", "abstract": " The CO-Nets approach, that we are developing, is an object oriented Petri net-based framework for specifying as well as prototyping--through graphical animation accompanied by a concurrent computation based on its semantics expressed in rewriting logic--distributed information systems. Taking benets of these (validation) capabilities, we presents how prototyping and implementation of systems specied using widely accepted information systems languages, namely the Troll language, can be directly drawn up. This is mainly achieved through an intuitive translation of such specications into the CO-Nets approach where graphical animation and formal computation are carried out. Moreover, because of the capabilities of the CO-Nets approach for conceiving such systems as autonomous but yet cooperative components, it becomes semantically sound to enrich these languages with syntactical constructions for more modularity leading to more efficient rapid-prototyping.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Foundations for integrity independence in relational databases\n", "abstract": " Integrity independence is the ability to change integrity constraints without changing update transactions and application programs (Codd, 1990). Except for entity and referential integrity rules, notions of the relational data model are not rich enough to represent other types of constraints into database schemata. Thus in relational DBMSs integrity independence is either missing or restricted. In this paper we provide this capability to relational databases on an abstract level and not on the implementation level as discussed in (Codd, 1990). This means that integrity independence is provided using notions and operations of the relational data model and not depending on capabilities of DBMSs which may differ from one system to another. We do that by storing templates for simplified forms of constraints together with additional information about the constraints into meta relations. Meta relations are\u00a0\u2026", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Introduction to logics for databases and information systems\n", "abstract": " The designers and users of present-day information systems deal with more and more complex applications that have to meet stringent quality requirements. It is no longer enough to capture the static aspect of the world \u2014 modeling the dynamics, i.e., time, change, and concurrency becomes equally important. Also, there is a need for multiple modalities to distinguish between what is true, known, believed, permitted, obligatory, past, present, and future.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Requirements for Agent-based Modeling of Federated Database Systems\n", "abstract": " ) Can Turker Gunter Saake Stefan Conrad Institut fur Technische Informationssysteme Otto-von-Guericke-Universitat Magdeburg Postfach 4120, D--39016 Magdeburg E-mail: ftuerker---saake---conradg@ iti. cs. uni-magdeburg. de Phone:++ 49-391-67-f12994---18800---18066g Fax:++ 49-391-67-12020 1 Motivation Considering the needs of present and future information systems, interoperability of independent databases systems becomes more and more important for an enterprise in order to be able to act competitively in an increasing world-wide market [Bro92]. Today, nearly every enterprise has to face the situation that different database systems are used in its departments. Problems caused by this situation are redundant data, inter-database dependencies and data-exchange between different databases [HM93]. For each database system there often exists a large number of applications which have to be preserved. On the other hand, new multidatabase applications must be supported, for instance, for re...", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Goal-Driven Operational Semantics of Temporal Specification of Reactive Systems Behaviour\n", "abstract": " A propositional linear temporal logic is brie y introduced and its use for reactive systems speci cation is motivated and illustrated. G-automata are proposed as a new operational semantics domain designed to cope with fairness/liveness properties. G-automata are a class of labelled transition systems with an additional structure of goal achievement which forces the eventual ful lment of every pending goal. An algorithm is then presented, that takes a nite system speci cation as input and that, by a stepwise tableaux analysis method, builds up a canonical G-automaton matching the speci cation. Eventuality formulae correspond to goals of the automaton their satisfaction being thus assured. Applications to monitoring are discussed.", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Objektspezifikation von Benutzerschnittstellen in Troll\n", "abstract": " Die konzeptionelle Modellierung des Weltausschnittes, der durch ein Informationssystem dargestellt werden soll, ist die erste Phase des Systementwurfes. Das konzeptionelle Modell ist damit die Grundlage der Implementierung. In diesem Modell sind das sp\u00e4ter zu implementierende System und seine Umgebung integriert. Zwischen diesen beiden Komponenten ist die Benutzerschnittstelle einzuordnen. Die Definition dieser Schnittstelle ist damit der konzeptionellen Modellierung zuzuordnen, w\u00e4hrend ihre konkrete Darstellung im Verlaufe des Designs gestaltet wird.               In unserem Ansatz erfolgt die Modellierung der Benutzerschnittstelle in dem Formalismus, der auch zur Modellierung des Gesamtsystems verwendet wird. Das Systemdesign trennt dabei die Teile des im Modell dargestellten Weltauschnittes, die in eine interne Repr\u00e4sentierung des Systems abgebildet werden, von den Teilen, die eine\u00a0\u2026", "num_citations": "2\n", "authors": ["1503"]}
{"title": "Fecal Metaproteomics Reveals Reduced Gut Inflammation and Changed Microbial Metabolism Following Lifestyle-Induced Weight Loss\n", "abstract": " Gut microbiota-mediated inflammation promotes obesity-associated low-grade inflammation, which represents a hallmark of metabolic syndrome. To investigate if lifestyle-induced weight loss (WL) may modulate the gut microbiome composition and its interaction with the host on a functional level, we analyzed the fecal metaproteome of 33 individuals with metabolic syndrome in a longitudinal study before and after lifestyle-induced WL in a well-defined cohort. The 6-month WL intervention resulted in reduced BMI (\u2212 13.7%), improved insulin sensitivity (HOMA-IR,\u2212 46.1%), and reduced levels of circulating hsCRP (\u2212 39.9%), indicating metabolic syndrome reversal. The metaprotein spectra revealed a decrease of human proteins associated with gut inflammation. Taxonomic analysis revealed only minor changes in the bacterial composition with an increase of the families Desulfovibrionaceae, Leptospiraceae, Syntrophomonadaceae, Thermotogaceae and Verrucomicrobiaceae. Yet we detected an increased abundance of microbial metaprotein spectra that suggest an enhanced hydrolysis of complex carbohydrates. Hence, lifestyle-induced WL was associated with reduced gut inflammation and functional changes of human and microbial enzymes for carbohydrate hydrolysis while the taxonomic composition of the gut microbiome remained almost stable. The metaproteomics workflow has proven to be a suitable method for monitoring inflammatory changes in the fecal metaproteome. View Full-Text", "num_citations": "1\n", "authors": ["1503"]}
{"title": "variED: an editor for collaborative, real-time feature modeling\n", "abstract": " Feature models are a helpful means to document, manage, maintain, and configure the variability of a software system, and thus are a core artifact in software product-line engineering. Due to the various purposes of feature models, they can be a cross-cutting concern in an organization, integrating technical and business aspects. For this reason, various stakeholders (eg, developers and consultants) may get involved into modeling the features of a software product line. Currently, collaboration in such a scenario can only be done with face-to-face meetings or by combining single-user feature-model editors with additional communication and version-control systems. While face-to-face meetings are often costly and impractical, using version-control systems can cause merge conflicts and inconsistency within a model, due to the different intentions of the involved stakeholders. Advanced tools that solve these\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Combining Two Worlds: MonetDB with Multi-Dimensional Index Structure Support to Efficiently Query Scientific Data\n", "abstract": " Reproducibility and generalizability are important criteria for today\u2019s data management society. Hence, stand-alone solutions that work well in isolation, but cannot convince at system level lead to a frustrating user experience. As a consequence, in our demo, we take the step of accelerating queries on scientific data by integrating the multi-dimensional index structure Elf into the main-memory-optimized database management system MonetDB. The overall intention is to show that the stand-alone speed ups of using Elf can also be observed when integrated into a holistic system storing scientific data sets. In our prototypical implementation, we demonstrate the performance of an Elf-backed MonetDB on the standard OLAP-benchmark, TPC-H, and the genomic multi-dimensional range query benchmark from the scientific data community. Queries can be run live on both benchmarks by the audience, while they are\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "He.. ro DB: a concept for parallel data processing on heterogeneous hardware\n", "abstract": " Due to the growing demand on processing power and energy efficiency by today\u2019s data-intensive applications developers have to deal with heterogeneous hardware platforms composed of specialized computing resources. These are highly efficient for certain workloads but difficult to handle from the software engineering perspective. Even state-of-the-art database management systems do not exploit all heterogeneous hardware components, as their characteristics differ significantly. They are thus hard to integrate within a coherent database architecture.                 To address this problem, we propose a design concept that is based on a layered system software architecture: He..ro DB transforms a data-flow graph that describes the data-processing application to a task-based execution plan. Task implementations for the different computing resources and a reasonable degree of parallelism are chosen\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "GridTables: A One-Size-Fits-Most H 2 TAP Data Store\n", "abstract": " Abstract Heterogeneous Hybrid Transactional Analytical Processing (H^ 2 H 2 TAP) database systems have been developed to match the requirements for low latency analysis of real-time operational data. Due to technical challenges, these systems are hard to architect, non-trivial to engineer, and complex to administrate. Current research has proposed excellent solutions to many of those challenges in isolation\u2013a unified engine enabling to optimize performance by combining these solutions is still missing. In this concept paper, we suggest a highly flexible and adaptive data structure (called gridtable) to physically organize sparse but structured records in the context of H^ 2 H 2 TAP. For this, we focus on the design of an efficient highly-flexible storage layout that is built from scratch for mixed query workloads. The key challenges we address are:(1) partial storage in different memory locations, and (2) the ability to\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "EXtracting product lines from vAriaNTs (EXPLANT)\n", "abstract": " The project EXtracting Product Lines from vAriaNTs (EXPLANT) funded by the German Research Foundation (DFG) is currently in its second phase. In this project, we are concerned with the stepwise migration of cloned variants into a software product line (ie, the extractive approach of adopting systematic software reuse). While the extractive approach is the most common one in practice, many of its characteristics (eg, processes, costs, best practices) are still unclear and tool support is limited (eg, for feature location, refactoring, quality assurance). Within this extended abstract, we report on a selection of results we achieved in EXPLANT so far, and highlight our goals as well as opportunities for research collaborations in the second phase.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "MStream: Proof of Concept of an Analytic Cloud Platform for Near-real-time Diagnostics Using Mass Spectrometry Data\n", "abstract": " A mass spectrometer is a device to extract biomarkers of biological environments. Using these biomarkers, it is possible to diagnose thousands of diseases with only one mass spectrometer. Unfortunately, the mass spectrometry pipeline is sequential, including hours of waiting time between the workflow steps. Additionally, the data analysis is complex and needs qualified employees and a stable infrastructure, which involves very high costs and effort. Hence, only few hospitals use a mass spectrometer for diagnostics with success.In our work, we present a proof of concept of an analytical platform for real-time analysis of mass spectrometry experiments. In collaboration with Bruker Daltonik GmbH, we implemented MStream, a cloud-based platform on the SMACK stack (Spark, Mesos, Akka, Cassandra, Kafka) for scalable, streamlined protein identification. Our evaluation shows superior performance in comparison to the state-of-the-art X! Tandem software package. Additionally, we minimize the effort of the hospital by allowing the full analysis pipeline to be outsourced to our cloud platform.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Efficient Inter-Kernel Communication for OpenCL Database Operators on FPGAs\n", "abstract": " Many modern database engines use OpenCL to target heterogeneous hardware. Queries are evaluated by execution of chains of low-level operators. The common paradigm for OpenCL workloads facilitates communication between kernels using buffers in off-chip memory. This poses a severe performance limitation due to weak memory systems of FPGAs in contrast to the memory hierarchy available in CPUs and GPUs. To overcome this bottleneck, we propose the use of structural optimizations of kernel code. On-chip pipelining and code fusion are analyzed as alternatives to buffer-based inter-kernel communication. We assess the impact on resource utilization and system throughput and thereby demonstrate that properly structured code achieves a speedup of more than 4x over the default paradigm. This shows that it is essential for chains of kernels to consider not only optimization techniques for individual\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Errata for\" Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products\"\n", "abstract": " In the published version of EnumerateCmp in the Section 3.3 on Page 936 [1], see also Algorithm 1, a small error is included in Line 5. In the first call of EnumerateCsgRec, too many nodes (X \u222a N) will be excluded for the emission of complements, leading to the fact that, in general, not all complements will be emitted correctly.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Industrialization of IT-an information system architecture for application system landscape providers\n", "abstract": " Information technology (IT) service providers struggle with efficient and integrated production processes when compared to modern manufacturers. Manufacturers produce products build-to-order in a mass-customization approach or engineer-to-order in a highly customized but streamlined production process while using computer-integrated manufacturing approaches. Software\u2019s inherent complexity and its heterogeneous implementations make such consistent management of application service production difficult. This thesis examines if the IT service provider type of application system landscape providers can implement a production process similarly efficient and integrated as that of manufacturers. The thesis makes the argument that software for operations automation, such as infrastructure as a service and configuration management software, can wrap application software\u2019s complexity and its\u2019 heterogeneous implementations. Operations automation approaches facilitate an automated, modularized, and standardized build-and-engineer-to-order production process.This thesis creates its artifact based on an analysis of current technology, a case study of various IT service providers, including application system landscape providers, as well as literature. It follows the design science paradigm of information systems research. The main contribution is an information system architecture for application system landscape providers (ISAA). The ISAA explores the limits of standardization, automation, and modularization for application system landscape production. A domain model explicates the relationships between relevant entities of application\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Piecing Together Large Puzzles, Efficiently: Towards Scalable Loading Into Graph Database Systems.\n", "abstract": " Many applications rely on network analysis to extract business intelligence from large datasets, requiring specialized graph tools such as processing frameworks (eg Apache Giraph, Gradoop), database systems (eg Neo4j, JanusGraph) or applications/libraries (eg NetworkX, nvGraph). A recent survey reports scalability, particularly for loading, as the foremost practical challenge faced by users. In this paper we consider the design space of tools for efficient and scalable graph bulk loading. For this we implement a prototypical loader for a property graph DBMS, using a distributed message bus. With our implementation we evaluate the impact and limits of basic optimizations. Our results confirm the expectation that bulk loading can be best supported as a server-side process. We also find, for our specific case, gains from batching writes (up to 64x speedups in our evaluation), uniform behavior across partitioning strategies, and the need for careful tuning to find the optimal configuration of batching and partitioning. In future work we aim to study loading into alternative physical storages with GeckoDB, an HTAP database system developed in our group.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Cost-function complexity matters: When does parallel dynamic programming pay off for join-order optimization\n", "abstract": " The execution time of queries can vary by several orders of magnitude depending on the join order. Hence, an efficient query execution can be ensured by determining optimal join orders. Dynamic programming determines optimal join orders efficiently. Unfortunately, the runtime of dynamic programming depends on the characteristics of the query, limiting the applicability to simple optimization problems. To extend the applicability, different parallelization strategies were proposed. Although existing parallelization strategies showed benefits for complex cost functions, the effects of the cost-function complexity was not evaluated.                 Therefore, in this paper, we compare different sequential and parallel dynamic programming variants with respect to different query characteristics and cost-function complexities. We show that the parallelization of a parallel dynamic programming variant is most often only\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Efficient storage and analysis of genome data in relational database systems\n", "abstract": " Genome analysis allows researchers to reveal insights about the genetic makeup of living organisms. In the near future, genome analysis will become a key means in the detection and treatment of diseases that are based on variations of the genetic makeup. To this end, powerful variant detection tools were developed or are still under development.However, genome analysis faces a large data deluge. The amounts of data that are produced in a typical genome analysis experiment easily exceed several 100 gigabytes. At the same time, the number of genome analysis experiments increases as the costs drop. Thus, the reliable and efficient management and analysis of large amounts of genome data will likely become a bottleneck, if we do not improve current genome data management and analysis solutions.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Conditional Compilation with FeatureIDE\n", "abstract": " Conditional compilation is one of the most important and popular techniques to implement variable systems. Using preprocessors, code can be annotated with directives to include or exclude statements depending on feature selections. That way, products can be customized to the needs of a customer. Due to this mark and exclude principle, preprocessors are simple to use and understandable and enable a fine-grained way to implement variability. However, implementing configurable software is a difficult task, especially if the implementation is not coupled with a representation of the domain. To solve these difficulties, we provide an integrated process in FeatureIDE. The process incorporates feature modeling, configuration, implementation, generation, and execution.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Featureide in a nutshell\n", "abstract": " FeatureIDE implements a general support to implement feature-oriented software product lines. In this chapter, we give a general overview on the functionalities of FeatureIDE. To get a first impression of FeatureIDE, we use a small \u201cHello World\u201d application. As FeatureIDE supports all phases of the feature-oriented software development process, we introduce how all these phases are realized.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Konsistenz in Cloud-Datenbanken\n", "abstract": " Die bisher diskutierten Verfahren zur Konsistenzsicherung in Verteilten Datenbanksystemen zielen auf strikte Konsistenz, also kurz gefasst auf die Gew\u00e4hrleistung der ACID-Eigenschaften unter allen Umst\u00e4nden. Derartige Verfahren skalieren jedoch oft nur eingeschr\u00e4nkt und begrenzen die Leistungsf\u00e4higkeit der betroffenen Systeme. Daher wurden und werden gerade in Cloud-Anwendungen abgeschw\u00e4chte Konsistenzforderungen und zugeh\u00f6rige Verfahren diskutiert, die besser skalieren, aber Einschr\u00e4nkungen in der Konsistenz bedeuten. Der erste Abschnitt dieses Kapitels motiviert und detailliert den Einsatz abgeschw\u00e4chter Konsistenzforderungen in Cloud-Szenarien. Replikationstechniken, die durch derartige abgeschw\u00e4chte Konsistenzforderungen erm\u00f6glicht werden, werden danach behandelt. Abschlie\u00dfend wird die Realisierung derartiger Verfahren in kommerziellen Cloud-Datenbanken\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Ein Modell zum zentralen Betrieb von hoch flexiblen SOA-L\u00f6sungen auf Basis definierter Standards\n", "abstract": " Design und Implementierung einer neuen Applikation gem\u00e4\u00df den Anforderungen der potentiellen Anwender wird viel Zeit und Aufwand einger\u00e4umt. Es geht darum, schneller und aufwandsarm, aber flexibel eine Applikation zu realisieren. Aber wird in diesen Phasen des Lebenszyklus bereits der sp\u00e4tere, permanente und unterbrechungsfreie, langfristige Betrieb dieser Applikation mit m\u00f6glichst geringem Aufwand betrachtet und vorbereitet? Applikationen auf Basis einer service-orientierteren Architektur unterst\u00fctzen die Flexibilit\u00e4t, Services bei Bedarf auszutauschen, um Fehler zu beheben oder sich neuen Anforderungen anzupassen. Daraus ergeben sich jedoch neue Anforderungen an den t\u00e4glichen Betrieb im Rahmen der IT Service und Support Prozesse. In dieser Arbeit werden diese Anforderungen und die sich daraus ergebenen notwendigen Anpassungen untersucht. Als Grundlage der Untersuchung dient die IT Infrastructure Library, die in genereller Form die IT Service und Supportprozesse und den Betrieb von IT Applikationen beschreibt. Es wird gezeigt, welche Anforderungen sich im Betrieb durch die Nutzung der Flexibilit\u00e4t SOA-basierter Applikationen ergeben, soll die Flexibilit\u00e4t tats\u00e4chlich genutzt werden, was in der Praxis heute kaum der Fall ist. Durch den h\u00f6heren Grad der Verteilung und der Granularit\u00e4t auf Services besteht auf der anderen Seite der Bedarf der Zentralisierung des Betriebs und der Vereinheitlichung unabh\u00e4ngig von der genutzten Technologie und konkreten Implementierung eines Services. Es wird gezeigt, wie die in SOA entstehenden bzw. entstandenen neuen Herausforderungen an den Betrieb mittels\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Relational On Demand Data Management for IT-Services\n", "abstract": " Database systems are widely used in technical applications. However, it is difficult to decide which database management system fits best for a certain application. For many applications, different workload types often blend to mixed workloads that cause mixed requirements. The selection of an appropriate database management system is more critical for mixed workloads because classical domains with complementary requirements are combined, e.g., OLTP and OLAP. A definite decision for a database management system is not possible. Hybrid database system are developed to accept this challenge, i.e., these systems combine different storage approaches. However, a mutual optimization in hybrid systems is not available for mixed workloads. We develop a decision-support framework to provide application-performance estimation on a certain database management system on the one hand and to provide\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Performance Prediction in the Presence of Feature Interactions\n", "abstract": " 1 Introduction. Customizable programs and program families provide user-selectable features allowing users to tailor the programs to the application scenario. Beside functional requirements, users are often interested in non-functional requirements, such as a binary-size limit, a minimized energy consumption, and a maximum response time. To tailor a program to non-functional requirements, we have to know in advance which feature selection, that is, configuration, affects which non-functional properties. Due to the combinatorial explosion of possible feature selections, a direct measurement of all of them is infeasible.In our work, we aim at predicting a configuration\u2019s non-functional properties for a specific workload based on the user-selected features [SRK+11, SRK+13]. To this end, we quantify the influence of each selected feature on a non-functional property to compute the properties of a specific configuration. Here, we concentrate on performance only. Unfortunately, the accuracy of performance predictions may be low when considering features only in isolation, because many factors influence performance. Usually, a property is program-wide: it emerges from the presence and interplay of multiple features. For example, database performance depends on whether a search index or encryption is used and how both features interplay. If we knew how the combined presence of two features influences performance, we could predict a configuration\u2019s performance more accurately. Two features interact (ie, cause a performance interaction) if their simultaneous presence in a configuration leads to an unexpected performance, whereas their\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "\" Vergleich ausgew\u00e4hlter Datenaustauschstrategien im Ingenieurwesen\n", "abstract": " Das Problem bei mehreren Systemen ist die Weiterverwendung einmal erzeugter Daten. Eine neue Eingabe ist aufwendig und fehleranf\u00e4llig. Daher wird ein Austausch von Daten zwischen den Systemen angestrebt. Dies stellt bereits seit Jahren eine Herausforderung dar, die auch heute noch nicht vollkommen gel\u00f6st ist [MPKS11]. 1999 sch\u00e4tze eine Studie [BM99], dass etwa eine Milliarde Dollar in den USA j\u00e4hrlich allein bei den Automobilzulieferern wegen fehlerhafter Interoperabilit\u00e4t ausgegeben werden. Eine Verbesserung dieser Situation ist nicht zu beobachten.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Advantages of a Layered Architecture for Enterprise Data Warehouse Systems\n", "abstract": " Advantages of a Layered Architecture for Enterprise Data Warehouse Systems 1 Enterprise Data Warehouse Systems Page 1 Advantages of a Layered Architecture for Enterprise Data Warehouse Systems 1 Complex Systems Design & Management 2011 1 Enterprise Data Warehouse Systems Thorsten Winsemann, Veit K\u00f6ppen, Gunter Saake Otto-von-Guericke-Universit\u00e4t, Magdeburg/Germany Complex Systems Design & Management 2011 \u2013 CSD&M, Paris/France, December 7-9 Page 2 Table of contents 1. Characteristics of Enterprise Data Warehouses 2. Traditional Data Warehouse Architecture 1. Reference Architecture 2. Dataflow-Example 3. Architectures for Enterprise Data Warehouses 1. SAP\u2019s Layered, Scalable Architecture 2 Complex Systems Design & Management 2011 2 1. SAP\u2019s Layered, Scalable Architecture 2. Layers in Detail 3. Dataflow-Example 4. Simple, but Detailed Example 4. Architectural \u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Challenges in an Assistance World\n", "abstract": " The development of small and embedded devices, smart interaction techniques, and integration in an ubiquitous data access environment enable the support of humans in daily life. A systematic engineering of situation-aware assistance systems needs methods, models, and techniques that use knowledge on human actions. In this paper, we address challenges of engineering assistance systems: Models of structures of actions, reactions on situations, systems\u2019 reliability due to uncertainty of reality and human actions, and architecture of assistance systems network. A holistic engineering concept that maps reality to a world model is not in scope of these challenges. However, best practices and patterns for the development of models and systems can enhance engineering assistance systems.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Rapid-prototyping of Adaptive Component-based Systems using Runtime Aspectual Interactions\n", "abstract": " Aspect-oriented mechanisms have been widely exploited at the programming-level. Nevertheless, little has been achieved at the architectural level, and this despite the potential benefits to gain. These benefits include the promotion of transparency, rapid-prototyping,correctness and adaptability. This contribution aims at filling this gap. We first put forward an intuitive event-driven architectural conceptualization that promotes behavioral rule-centric transient interactions.Towards non-intrusive and runtime adaptability of such exogenous interactions, we then abstractly endow them with aspect-oriented mechanisms. Finally, a compliant foundation with rapid-prototyping capabilities is proposed. It is based on leveraging the distributed rewriting-logic based Maude language.In particular, capitalizing on Maude reflection, we perform inherent behavioral interactions as suitable advices and dynamically weave them on\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Type Checking Software Product Lines-A Formal Approach for Annotation-Based Implementations\n", "abstract": " A software product line (SPL) is an efficient means to generate a family of program variants for a domain from a single code base. However, because of the potentially high number of possible program variants, it is difficult to test them all and ensure properties like type safety for the entire SPL. While first steps to type check an entire SPL have been taken, they are informal and incomplete. In this paper, we extend the Featherweight Java calculus with feature annotations useful for SPL development and discuss different solutions for the problem of typing mutually exclusive features. By extending Featherweight Java\u2019s type system, we guarantee that\u2013given a welltyped SPL\u2013all possible program variants are well-typed as well. We show how results from this formalization helped implementing our own language-independent SPL tool CIDE and describe how we used CIDE to detect type errors in existing SPLs.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Towards Maude-Tla based Foundation for Complex Concurrent Systems Specification and Certification\n", "abstract": " This paper contributes towards a multi-paradigm approach for the specification validation verification and refinement of concurrent agile systems. It brings together two complementary rigorous and largely accepted frameworks: Meseguer's true- concurrent Rewriting Logic (RL) with its MAUDE prototype and Lamport's Temporal Logic of Actions (TLA) with its current prototype TLA + . At the specification / validation phase, we adopt a variant of MAUDE that we endow with strategy for controlling rules and state splitting / recombining for exhibiting full intra-concurrency. For the verification / refinement phase, we automatically derive TLA's formulas from validated MAUDE specification, on which crucial properties are checked using TLA's deductions and invariants. A production system is considered as proof-of-concept.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "UML-driven Information Systems and their Formal Integration Validation and Distribution.\n", "abstract": " Being the de-facto standard (object-oriented-OO) method (-logy) for software-intensive systems development, UML with its different diagrams and supporting tools represent nowadays the mostly adopted software-engineering means for information systems (IS). Nevertheless, due to this wide-acceptance by all organization stakeholders several enhancements at the modelling level are required before adventuring into further implementation phases. The coherence and complementarity between different diagrams have to tackled; On the basic of such endeavored coherent global view, the consistency and validation of the whole IS conceptual models are to undertaken; and last but not least as current information systems are mostly networked and concurrent, UML-driven have to cater for intrinsic distribution and concurrency. To leverage UML-driven IS conceptual modelling towards these crucial enhancements, we propose a semi-automatic intermediate abstract phase before any implementation, we govern by a rigorous component-based operational and visual conceptual model. Referred to as CO-NETS, this specification/validation formalism is based on a tailored formal integration of most OO concepts and mechanisms enhanced by modularity principles into a variant of algebraic Petri Nets. For rapid-prototyping purposes, CO-NETS is semantically interpreted into rewriting logic. This UML-CO-NETS proposal for distributed IS rigorous development is illustrated through a non-trivial case-study for production systems.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "DYNAMIC INTERACTION OF INFORMATION SYSTEMS Weaving Architectural Connectors on Component Petri Nets\n", "abstract": " Keywords: High-Level Nets, rewriting logic, specification/validation, architectural techniques, dynamic evo-lution. Abstract: Advances in networking over heterogenous infrastructures are boosting market globalization and thereby forcing most software-intensive information systems to be fully distributed, cooperating and evolving to stay competitive. The emerging composed behaviour in such interacting components evolve dynamically/rapidly and unpredictably as market laws and users/application requirements change on-the-fly both at the coarse-type and fine-grained instance levels. Despite significant proposals for promoting interactions and adaptivity using mainly architectural techniques (eg components and connectors), rigorously specifying/validating/verifying and dynamically adapting complex communicating information systems both at type and instance levels still remains challenging. In this contribution, we present a component-based Petri nets governed by a true-concurrent rewriting-logic based semantics for specifying and validating interacting distributed information systems. For runtime adaptivity, we enhance this proposal with (ECA-business) rules Petri nets-driven behavioral connectors, and demonstrate how to dynamically weaving them on running components to reflect any emerging behavior. 1", "num_citations": "1\n", "authors": ["1503"]}
{"title": "A unified schema matching framework\n", "abstract": " The proliferation of applications dealing with shared data radically increases the need to identify and discover the semantically corresponding elements. To cope with the difficulties of the necessary schema matching, we propose a unified framework. The framework tries to collect the most well-known work concerning schema matching in a generalized approach. We observe that nearly all of this work share in their phases but each phase may has its own steps depending on the nature of the matching approach. Therefore, we suggest a unified approach to give a complete and integrated view about schema matching. Throughout this paper, we take mainstream matching systems, namely COMA, Cupid, SemInt, and LSD as main examples to demonstrate the applicability of our approach. This paper aims mainly at presenting a unified approach for the schema matching problem.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Stepwise and rigorous development of evolving concurrent information systems: From semi-formal objects to sound evolving components\n", "abstract": " Most of existing software are nowadays characterized as complex information systems. For their crucial phase of specification/validation, the present paper proposes to perceive information systems as fully distributed, autonomous yet cooperating evolving concurrent components. The formal specification/validation framework for this advanced perception is an adequate integration of object concepts with modularity features into an appropriately tailored variant of algebraic Petri nets. For a true (intra-and inter-object) concurrent exhibition and symbolic computation, this integration referred to as Co-nets is semantically interpreted using an adaptation of rewriting logic. More precisely, we first propose a clear incremental methodology for constructing complex information systems starting from their informal UML-based diagrammatic description and leading to interacting Co-nets components. As a second-level of\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Dynamic Visualisation for Feedback-driven Online Aggregation\n", "abstract": " Aggregation is generally a time consuming process only marginally suited for interactive result exploration. We present a framework of techniques to support an extended aggregation, which works online and can be controlled via user feedback. Supportive visualisation methods are presented with an overview of applicable interaction techniques and a generic architecture that combines relevant individual components.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Multimedia Database Support for Digital Libraries\n", "abstract": " Digital libraries are a key technology of the coming years allowing the effective use of the Internet for research and personal information. National initiatives for digital libraries have been started in several countries, including the DLI initiative in USA http://www-sal. cs. uiuc. edu/sharad/cs491/dli. html, Global Info http://www. globalinfo. org/index. html. en in Germany, and comparable activities in Japan and other European countries.A digital library allows the access to huge amounts of documents, where documents themselves have a considerably large size. This requires the use of advanced database technology for building a digital library. Besides text documents, a digital library contains multimedia documents of several kinds, for example audio documents, video sequences, digital maps and animations. All these document classes may have specific retrieval methods, storage requirements and Quality of Service parameters for using them in a digital library.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Database Design: Object-Oriented versus Relational\n", "abstract": " Object-oriented database design is not only a simple extension of relational database design. By modeling structure as well as behavior of real-world entities as coherent units, object-oriented database design succeeds in capturing more semantics of applications already in the design phase. The use of objectoriented concepts like inheritance promises a more adequate modeling and a better application implementation based on an object-oriented database system. However, the results of object-oriented design can also be applied to classical database systems.               In this paper we briefly compare object-oriented database design with traditional design of relational databases. It is not our intention to end up with stating that one of the two approaches is superior to the other one. Instead we want to point out in which way particularly the object-oriented approach can still learn from the more established\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Compiling State Constraints\n", "abstract": " The evaluation of constraints needs in many cases that a large portion of the database to be accessed. This makes the process of integrity checking very di cult to implement e ciently. Thus nding methods to improve the evaluation of integrity constraints is an important area of research in the eld of database integrity. Most of these methods are based on simpli cation principles. One of these methods is presented by Nicolas in Nic82]. In this method the simpli ed form of a constraint depends on updating operations performed on database states. For that reason, the simpli ed form is obtained at update time. In this report we show that, for a given constraint W and an update that is to be performed to a relation R, it is not necessary to do all the steps of the method at run time, but we can do most of these steps at compile time. We do that by developing a representation that stores simpli ed instances of W together with other information about occurrences of R in W into meta relations. The simpli ed instances stored in the meta relations are obtained form W by applying the same simpli cation steps of the method, but here we use generic constants instead of speci c update values. When an update is performed to the relation R, the generic constants in the meta relations are replaced with the update values and a relational algebra expression is performed on the obtained relation, resulting in a set of formulas. We will show that by only applying the third step of the method to the conjunctions of these formulas we can get the simpli ed form obatined by the simpli cation method at run time.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "A Comparison of the Notations Used in the Shlaer-Mellor Method and in TCM\n", "abstract": " This report compares two notation systems for requirements modeling, the notations used in the Shlaer-Mellor method for object-oriented analysis and the notations uses in TCM (Toolkit for Conceptual Modeling). The notations used in the Shlaer-Mellor method are semi-formal, ie they consist of diagrams annotated by natural language text. The notations used in TCM are semi-formal, as in the Shlaer-Mellor notation, but there is also a formal part. The formal part of a TCM speci cation is written down in LCM (Language for Conceptual Modeling), a language based on order-sorted dynamic logic. The formal and semi-formal parts of a TCM speci cation supplement each other and each can be used without using the other. Because the semi-formal and formal notations in TCM are precisely related, the semi-formal notations have unambiguous de nitions, and the formal notations have simple and clear diagram representations. The report analyzes the notations used for information model, state model, process model and communication model used in the Shlaer-Mellor method and shows how these relate to the notations used for the class model, life cycle model and communication model in TCM. It is shown that the notations of the Shlaer-Mellor method contain ambiguities and redundancies, that are resolved when we transform these notations into TCM notations. However, some of these redundancies provide useful information and TCM is extended with a simpli ed form of some of the Shlaer-Mellor notations. The report repeatedly refers to gures from 39]. For copyright reasons, these gures are not reproduced here. The reader is expected to have a\u00a0\u2026", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Supporting Autonomy for Information Systems in a Changing Environment\n", "abstract": " Availability and scalability are important features of information systems. To gain this kind of requirements, we propose a distributed schema catalog in conjunction with appropriate development phases. The distributed schema catalog has to support distribution transparency including partitioning and replication to be exible for changes within organization structures. Additionally, phases of autonomy design have to appoint the adequate usage of distribution transparency aspects to enable execution autonomy by a minimum of replication.", "num_citations": "1\n", "authors": ["1503"]}
{"title": "Feasible Object Certification\n", "abstract": " In this paper we deal with the problem of proving properties of object behaviour based on temporal logic speci cations. In general proving temporal properties for arbitrary speci cations is not feasible. We show that using a restricted speci cation language and translating it to temporal logic enables an e cient certi cation of relevant classes of properties namely invariants and liveness properties. The proposed certi cation method uses tableau systems and techniques for arithmetic and equational reasoning leading to small and structured proofs that are easy to manage.", "num_citations": "1\n", "authors": ["1503"]}