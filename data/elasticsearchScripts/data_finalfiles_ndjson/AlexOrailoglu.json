{"title": "Test volume and application time reduction through scan chain concealment\n", "abstract": " A test pattern compression scheme is proposed in order to reduce test data volume and application time. The number of scan chains that can be supported by an ATE is significantly increased by utilizing an on-chip decompressor. The functionality of the ATE is kept intact by moving the decompression task to the circuit under test. While the number of virtual scan chains visible to the ATE is kept small, the number of internal scan chains driven by the decompressed pattern sequence can be sinificantly increased.", "num_citations": "263\n", "authors": ["530"]}
{"title": "Flow graph representation\n", "abstract": " All silicon compilers can be distinguished broadly by the detail in the input description and the model of the target architecture. The input description detail is characterized by bindings of variables to storage elements (registers and memories), operations to functional units (ALUs, multipliers, shifters, etc.) and register transfers to control states. In structural silicon compilers all of the three bindings must be specified explicitly by the designer. Functional silicon compilers do not require all three bindings to be specified explicitly ([Park84]). SILC ([BlFR85]), and MacPitts ([Sout83]) specify states and registers but allow the compiler to allocate functional units. Some compilers in this class allow functional units of different type and within the same type they allow different cost/performance options ((BlFRES]). DAA does not require any binding to be specified ([KoTh85]). When no binding is specified, the compiler may use a fixed algorithm to translate the input description to the target architecture. Intelligent silicon compilers are capable of making tradeoffs and tuning the design to user constraints in area, time, and power. This is achieved by using a flexible, intermediate representation that will allow tradeoff analysis. The data flow graph is such a representation that exposes maximum parallelism in the input description (] Camp85]). However, it is not sufficient, since it does not show control precedences. In functional silicon compilation, a dataflow representation can be mapped to one of these three target architecture models: datapath, control unit and datapath, control unit and datapath with memory. The first model is very popular with commercial silicon\u00a0\u2026", "num_citations": "134\n", "authors": ["530"]}
{"title": "Architectures for silicon nanoelectronics and beyond\n", "abstract": " Although nanoelectronics won't replace CMOS for some time, research is needed now to develop the architectures, methods, and tools to maximally leverage nanoscale devices and terascale capacity. Addressing the complementary architectural and system issues involved requires greater collaboration at all levels. The effective use of nanotechnology calls for total system solutions", "num_citations": "106\n", "authors": ["530"]}
{"title": "Reducing test application time through test data mutation encoding\n", "abstract": " In this paper we propose a new compression algorithm geared to reduce the time needed to test scan-based designs. Our scheme compresses the test vector set by encoding the bits that need to be flipped in the current test data slice in order to obtain the mutated subsequent test data slice. Exploitation of the overlap in the encoded data by effective traversal search algorithms results in drastic overall compression. The technique we propose can be utilized as not only a stand-alone technique but also can be utilized on test data already compressed, extracting even further compression. The performance of the algorithm is mathematically analyzed and its merits experimentally confirmed on the larger examples of the ISCAS '89 benchmark circuits.", "num_citations": "92\n", "authors": ["530"]}
{"title": "Test power reduction through minimization of scan chain transitions\n", "abstract": " Parallel test application helps reduce the otherwise considerable test times in SOCs; yet its applicability is limited by average and peak power considerations. The typical test vector loading techniques result infrequent transitions in the scan chain, which in turn reflect into significant levels of circuit switching unnecessarily. Judicious utilization of logic in the scan chain can help reduce transitions while loading the test vector needed. No performance degradation ensues as scan chain modifications have no impact on functional execution. A computationally efficient scheme is proposed to identify, the location and type of the logic to be inserted. The experimental results confirm the significant reductions in test power possible under the proposed scheme.", "num_citations": "85\n", "authors": ["530"]}
{"title": "Decompression hardware determination for test volume and time reduction through unified test pattern compaction and compression\n", "abstract": " A methodology for the determination of decompression hardware that guarantees complete fault coverage for a unified compaction/compression scheme is proposed. Test cube information is utilized for the determination of a near optimal decompression hardware. The proposed scheme attains simultaneously high compression levels and reduced pattern counts through a linear decompression hardware. Significant test volume and test application time reductions are delivered through the scheme we propose while a highly cost effective hardware implementation is retained.", "num_citations": "78\n", "authors": ["530"]}
{"title": "Automatic synthesis of self-recovering VLSI systems\n", "abstract": " We describe an integrated system for synthesizing self-recovering microarchitectures called /spl Sscr//spl Yscr//spl Nscr//spl Cscr//spl Escr//spl Rscr//spl Escr/ in the /spl Sscr//spl Yscr//spl Nscr//spl Cscr//spl Escr//spl Rscr//spl Escr/ model for self-recovery, transient faults are detected using duplication and comparison, while recovery from transient faults is accomplished via checkpointing and rollback. /spl Sscr//spl Yscr//spl Nscr//spl Cscr//spl Escr//spl Rscr//spl Escr/ initially inserts checkpoints subject to designer specified recovery time constraints. Subsequently, /spl Sscr//spl Yscr//spl Nscr//spl Cscr//spl Escr//spl Rscr//spl Escr/ incorporates detection constraints by ensuring that two copies of the computation are executed on disjoint hardware. Towards ameliorating the dedicated hardware required for the original and duplicate computations, /spl Sscr//spl Yscr//spl Nscr//spl Cscr//spl Escr//spl Rscr//spl Escr/ imposes\u00a0\u2026", "num_citations": "76\n", "authors": ["530"]}
{"title": "Test application time and volume compression through seed overlapping\n", "abstract": " We propose in this paper an extension on the Scan Chain Concealment technique to further reduce test time and volume requirement. The proposed methodology stems from the architecture of the existing SCC scheme, while it attempts to overlap consecutive test vector seeds, thus providing increased flexibility in exploiting effectively the large volume of don't-care bits in test vectors. We also introduce modified ATPG algorithms upon the previous SCC scheme and explore various implementation strategies. Experimental data exhibit significant reductions on test time and volume over all current test compression techniques.", "num_citations": "69\n", "authors": ["530"]}
{"title": "Microarchitectural synthesis of performance-constrained, low-power VLSI designs\n", "abstract": " New portable signal-processing applications such as mobile telephony, wireless computing, and personal digital assistants place stringent power consumption limits on their constituent components. Substantial power servings can be realized if 5 V designs are translated to use the new lower supply voltage standards. This conversion, however is not achieved easily: a design originally targeted for implementation in a 5 V technology will typically require significant re-work to meet timing and throughput requirements at the lower operating voltage. We describe a high-level synthesis system which assists the designer in performing this task, minimizing the need for manual re-design. Techniques employed in this work include pipelining and a new approach to module selection which minimizes power consumption subject to timing constraints. Using these and other high-level synthesis techniques to target designs to 3\u00a0\u2026", "num_citations": "67\n", "authors": ["530"]}
{"title": "CircularScan: a scan architecture for test cost reduction\n", "abstract": " Scan-based designs are widely used to decrease the complexity of the test generation process; nonetheless, they increase test time and volume. A new scan architecture is proposed to reduce test time and volume while retaining the original scan input count. The proposed architecture allows the use of the captured response as a template for the next pattern with only the necessary bits of the captured response being updated while observing the full captured response. The theoretical and experimental analysis promises a substantial reduction in test cost for large circuits.", "num_citations": "66\n", "authors": ["530"]}
{"title": "Concurrent application of compaction and compression for test time and data volume reduction in scan designs\n", "abstract": " A test pattern compression scheme for test data volume and application time reduction is proposed. While compression reduces test data volume, the increased number of internal scan chains due to an on-chip, fixed-rate decompressor reduces test application time proportionately. Through on-chip decompression, both the number of virtual scan chains visible to the ATE and the functionality of the ATE are retained intact. Complete fault coverage is guaranteed by constructing the decompression hardware deterministically through analysis of the test pattern set.", "num_citations": "66\n", "authors": ["530"]}
{"title": "A unified transformational approach for reductions in fault vulnerability, power, and crosstalk noise & delay on processor buses\n", "abstract": " In this paper we propose a coding scheme for general-purpose applications that can reduce power dissipation, crosstalk noise and crosstalk delay on the bus lines while simultaneously detecting errors at run time. The reduction in power dissipation can be achieved through reducing the bus switching activity. Not only is the switching activity in individual lines reduced but so is the coupling activity across the adjacent lines, the major contributor to the overall power dissipation in deep submicron technology. Detailed analysis of crosstalk noise and delay shows that eliminating certain patterns of transitions and reducing the infeasible ones in terms of crosstalk noise and power dissipation is a feasible strategy for alleviating these problems. We propose an encoding technique consisting of the use of predefined patterns of transitions, one for each possible combination of input data, to generate the codewords. The\u00a0\u2026", "num_citations": "63\n", "authors": ["530"]}
{"title": "Low-power instruction bus encoding for embedded processors\n", "abstract": " This paper presents a low-power encoding framework for embedded processor instruction buses. The encoder is capable of adjusting its encoding not only to suit applications but furthermore to suit different aspects of particular program execution. It achieves this by exploiting application-specific knowledge regarding program hot-spots, and thus identifies efficient instruction transformations so as to minimize the bit transitions on the instruction bus lines. Not only is the switching activity on the individual bus lines considered but so is the coupling activity across adjacent bus lines, a foremost contributor to the total power dissipation in the case of nanometer technologies. Low-power codes are utilized in a reprogrammable application specific manner. The restriction to two well-selected classes of simply computable, functional transformations delivers significant storage benefits and ease of reprogrammability, in the\u00a0\u2026", "num_citations": "63\n", "authors": ["530"]}
{"title": "Microarchitectural synthesis of VLSI designs with high test concurrency\n", "abstract": " The testability of a VLSI design is strongly a ected by its register-transfer level (RTL) structure. Since the high-level synthesis process determines the RTL structure, it is necessary to consider testability during high-level synthesis. A synthesis system composed of scheduling and binding components minimizes the number of hardware sharing con icts between tests in the test schedule. Novel test con ict estimates are used to direct the synthesis process. The test con ict estimation is based on examination of the in terconnect structure of the partial design state during synthesis. Test con ict estimates enable our synthesis system to select design options which increase test concurrency, thereby decreasing test time. Experimental results show that designs generated by this approach are testable in a highly concurrent manner.", "num_citations": "61\n", "authors": ["530"]}
{"title": "A novel scan architecture for power-efficient, rapid test\n", "abstract": " Scan-based testing methodologies remedy the testability problem of sequential circuits; yet they suffer from prolonged test time and excessive test power due to numerous shift operations. The high density of the unspecified bits in test data enables the utilization of the test response data captured in the scan chain for the generation of the subsequent test stimulus, thus reducing both test time and test data volume. The proposed scan-based test scheme accesses only a subset of scan cells for loading the subsequent test stimulus while freezing the remaining scan cells with the response data captured, thus decreasing the scan chain transitions during shift operations. The experimental results confirm the significant reductions in test application time, test data volume and test power achieved by the proposed scan-based testing methodology.", "num_citations": "54\n", "authors": ["530"]}
{"title": "Modeling scan chain modifications for scan-in test power minimization\n", "abstract": " Rapid and reliable test of SOCs necessitates upfront consideration of the test power issues. Special attention should be paid to scanbased cores as the test power problem is more severe due to excessive switching activity stemming from scan chain transitions during shift operations. We propose a scan chain modification methodology that transforms the stimuli to be inserted to the scan chain through logic gate insertion between scan cells, reducing scan chain transitions. We provide a mathematical analysis that helps model the impact of scan chain modifications on test stimuli transformations. Based on this analysis, we develop algorithms for transforming a set of test vectors into power-optimal test stimuli through cost-effective scan chain modifications. Even in the highly challenging case of fully specified test vectors, more than an order of magnitude reduction in scan-in power is attained by the proposed methodology, exceeding previous power reduction levels significantly.", "num_citations": "51\n", "authors": ["530"]}
{"title": "Coactive scheduling and checkpoint determination during high level synthesis of self-recovering microarchitectures\n", "abstract": " The growing trend towards VLSI implementation of crucial tasks in critical applications has increased both the demand for and the scope of fault-tolerant VLSI systems. In this paper, we present a self-recovering microarchitecture synthesis system. In a self-recovering microarchitecture, intermediate results are compared at regular intervals, and if correct saved in registers (checkpointing). On the other hand, on detecting a fault, the self-recovering microarchitecture rolls back to a previous checkpoint and retries. The proposed synthesis system comprises of a heuristic and an optimal subsystem. The heuristic synthesis subsystem has two components. Whereas the checkpoint insertion algorithm identifies good checkpoints by successively eliminating clock cycle boundaries that either have a high checkpoint overhead or violate the retry period constraint, the novel edge-based schedule, assigns edges to clock cycle\u00a0\u2026", "num_citations": "50\n", "authors": ["530"]}
{"title": "Test cost minimization through adaptive test development\n", "abstract": " The ever-increasing complexity of mixed-signal circuits imposes an increasingly complicated and comprehensive parametric test requirement, resulting in a highly lengthened manufacturing test phase. Attaining parametric test cost reduction with no test quality degradation constitutes a critical challenge during test development. The capability of parametric test data to capture systematic process variations engenders a highly accurate prediction of the efficiency of each test for a particular lot of chips even on the basis of a small quantity of characterized data. The predicted test efficiency further enables the adjustment of the test set and test order, leading to an early detection of faults. We explore such an adaptive strategy, by introducing a technique that prunes the test set based on a test correlation analysis. A test selection algorithm is proposed to identify the minimum set of tests that delivers a satisfactory defect\u00a0\u2026", "num_citations": "49\n", "authors": ["530"]}
{"title": "Scan power reduction through test data transition frequency analysis\n", "abstract": " Significant reductions in test application times can be achieved through parallelizing core tests; however, simultaneous test of various cores may result in exceeding power thresholds, endangering the SoC being tested. Test power dissipation is exceedingly high in scan-based environments wherein scan chain transitions during the shift of test data further reflect into significant levels of circuit switching unnecessarily. Scan chain modification helps mitigate this problem as it enables the reduction of transitions in the test stimuli to be inserted to the modified scan chain and in the response to be collected through the scan-out pin. The proposed modifications in the scan chain consist of inverter insertion and scan cell reordering, leading to significant power reductions with neither area nor performance penalty whatsoever A computationally efficient algorithm is presented to identify the optimal scan chain modification\u00a0\u2026", "num_citations": "48\n", "authors": ["530"]}
{"title": "Transformation-based high-level synthesis of fault-tolerant ASICs\n", "abstract": " The authors present a transformation-based approach to the high-level synthesis of fault-tolerant application-specific ICs (ASICs) satisfying a given performance constraint but requiring less than proportional increase in hardware over their nonredundant counterparts. They propose a synthesis methodology to exploit hardware minimizing transformations. A simple set of transformations are identified that minimize the fault-tolerance overhead. The selected transformations make the final design resilient to common mode failures. These transformations can be composed to form a rich set of complex transformations. An algorithm is presented to automatically identify structures in a flow graph where transformations can improve hardware utilization, and transformations that suit the structure best are applied. The system has been used to schedule several flow graphs.", "num_citations": "46\n", "authors": ["530"]}
{"title": "Low-power branch target buffer for application-specific embedded processors\n", "abstract": " A methodology for a low-power branch identification mechanism which enables the design of extremely power-efficient branch predictors for embedded processors is presented. The proposed technique utilises application-specific information regarding the control-flow structure of the program's major loops. Such information is used to completely eliminate the power hungry branch target buffer (BTB) lookups which normally occur at every execution cycle. Exact application knowledge regarding the control-flow structure of the program obviates the power expensive BTB operations, thus enabling the utilisation of contemporary branch predictors in high-end, yet power-sensitive embedded processors. The utilisation of exact application knowledge results not only in the complete elimination of the power hungry BTB structure but also in a perfect branch and target address identification. A cost-efficient and programmable\u00a0\u2026", "num_citations": "43\n", "authors": ["530"]}
{"title": "Fault tolerant quantum cellular array (QCA) design using triple modular redundancy with shifted operands\n", "abstract": " Due to their extremely small feature sizes and ultra low power consumption, Quantum-dot Cellular Automata (QCA) technology is projected to be a promising nanotechnology. However, in nanotechnologies, manufacture time defect levels and operational time fault rates are expected to be quite high. Straightforward Triple Modular Redundancy (TMR) based fault tolerance is inappropriate for QCA nanotechnology since wire delays dominate the logic delays and faults in wires dominate the faults in a QCA based design. Furthermore, long wires are necessary in TMR based designs. In this paper we show that fault-tolerance can be obtained by using TMR with Shifted Operands (TMRSO). TMRSO uses shorter wires of QCA cells and exploits the self-latching property of clocked QCA arrays to provide the same level of fault tolerance capability as straightforward TMR while being significantly faster and smaller. This\u00a0\u2026", "num_citations": "42\n", "authors": ["530"]}
{"title": "On the relation between SAT and BDDs for equivalence checking\n", "abstract": " State-of-the-art verification tools are based on efficient operations on Boolean formulas. Traditional manipulation techniques are based on binary decision diagrams (BDDs) and SAT (Boolean satisfiability) solvers. In this paper, we study the relation between the two procedures and show how the number of backtracks obtained in the Davis-Putnam (DP) procedure is linked to the number of paths in the BDD. We utilize this relation to devise a method that uses BDD variable ordering techniques to run the DP procedure. Experimental results confirm that the proposed method results in a dramatic decrease in the number of backtracks and in the time needed to prove the Boolean satisfiability problem as well.", "num_citations": "41\n", "authors": ["530"]}
{"title": "Testability implications in low-cost integrated radio transceivers: a Bluetooth case study\n", "abstract": " As the use of wireless communications in daily life increases, attaining low-cost solutions becomes increasingly important due to shrinking profit margins. Cost optimization that solely targets at minimization of the cost of system architecture may result in suboptimal, highly untestable, solutions. Test design and design for testability need to be incorporated into the system design flow to achieve viable solutions. This paper presents an analysis of test requirements, implications and test cost for low-cost Bluetooth systems. Testability problems are identified and possible solutions along with avenues to reduce the test cost by utilizing lower-cost testers are discussed.", "num_citations": "40\n", "authors": ["530"]}
{"title": "Towards effective embedded processors in codesigns: customizable partitioned caches\n", "abstract": " This paper explores an application-specific customization technique for the data cache, one of the foremost area/power consuming and performance determining microarchitectural features of modern embedded processors. The automated methodology for customizing the processor microarchitecture that we propose results in increased performance, reduced power consumption and improved determinism of critical system parts while the fixed design ensures processor standardization. The resulting improvements help to enlarge the significant role of embedded processors in modern hardware/software codesign techniques by leading to increased processor utilization and reduced hardware cost. A novel methodology for static analysis and a field-reprogrammable implementation of a customizable cache controller that implements a partitioned cache structure is proposed. The simulation results show significant\u00a0\u2026", "num_citations": "40\n", "authors": ["530"]}
{"title": "Logic mapping in crossbar-based nanoarchitectures\n", "abstract": " This article presents a mathematical model and algorithm that address the problem of logic function mapping in a nanoelectronic environment. Enhancement techniques improve the algorithm's runtime by significantly cutting down on unnecessary backtracking processes.", "num_citations": "39\n", "authors": ["530"]}
{"title": "Topology aware mapping of logic functions onto nanowire-based crossbar architectures\n", "abstract": " Highly regular, nanodevice based architectures have been proposed to replace pure CMOS based architectures in the emerging post CMOS era. Since bottom-up self-assembly is used to build these architectures, regular nanowire crossbars are emerging as a promising candidate. While these regular structures resemble CMOS programmable logic arrays (PLAs), PLA logic synthesis methodologies fail to solve the associated problems since the length and connectivity constraints imposed by individual nanowires in these crossbars translate into challenges hitherto not considered. These strict topological constraints should be considered while mapping Boolean functions onto nanowire crossbars during logic synthesis. We develop a mathematical model for this problem, an algorithm to solve it and three heuristics to improve the algorithm runtime", "num_citations": "39\n", "authors": ["530"]}
{"title": "Microarchitectural synthesis for rapid BIST testing\n", "abstract": " The impact of testability on design cost necessitates its consideration during the earliest stages of synthesis. Built-in self test (BIST) is an accepted testing approach, but its application to many designs is limited by the long test application time required to achieve high fault coverage. This work addresses the problem of BIST test time for high fault coverage by targeting test concurrency during high-level and structural synthesis. High-level synthesis generates RTL circuits which guarantee concurrent controllability and observability of all hardware components from test registers. Structural synthesis for testability completes the microarchitectural definition by specifying the test registers in the circuit, and defining a BIST test plan for the circuit. Remaining test conflicts are avoided without reducing test throughput by using partial-intrusion BIST to enable test data to be pipelined through nontest registers. The use of\u00a0\u2026", "num_citations": "39\n", "authors": ["530"]}
{"title": "SYNCBIST: synthesis for concurrent built-in self-testability\n", "abstract": " We present a system which synthesizes, from a behavioral description, an RTL circuit which is testable with a high degree of test concurrency. The system produces a datapath containing test registers, and a BIST test plan for the testing of the chip. All design decisions are made using an estimate of test conflicts, which is based on an analysis of the reachability of each component port from I/O pins and test registers. Chip testing according to the partial-intrusion BIST methodology is assumed. Empirical results show the effect of test conflicts on the test application time, and highlight the benefit of using the proposed synthesis approach for test conflict reduction.< >", "num_citations": "39\n", "authors": ["530"]}
{"title": "Predictable execution adaptivity through embedding dynamic reconfigurability into static MPSoC schedules\n", "abstract": " Advances in semiconductor technologies have placed MPSoCs center stage as a standard architecture for embedded applications of ever increasing complexity. Because of real-time constraints, applications are usually statically parallelized and scheduled onto the target MPSoC so as to obtain predictable worst-case performance. However, both technology scaling trends and resource competition among applications have led to variations in the availability of resources during execution, thus questioning the dynamic viability of the initial static schedules. To eliminate this problem, in this paper we propose to statically generate a compact schedule with predictable response to various resource availability constraints. Such schedules are generated by adhering to a novel band structure, capable of spawning dynamically a regular reassignment upon resource variations. Through incorporating several soft constraints\u00a0\u2026", "num_citations": "38\n", "authors": ["530"]}
{"title": "Test planning and design space exploration in a core-based environment\n", "abstract": " This paper proposes a comprehensive model for test planning in a core-based environment. The main contribution of this work is the use of several types of TAMs and the consideration of different optimization factors (area, ping and test time) during the global TAM and test schedule definition. This expansion of concerns makes possible an efficient yet fine-grained search in the huge design space of a reuse-based environment. Experimental results clearly show the variety of trade-offs that can be explored using the proposed model, and its effectiveness on optimizing the system test design.", "num_citations": "36\n", "authors": ["530"]}
{"title": "Deterministic partitioning techniques for fault diagnosis in scan-based BIST\n", "abstract": " A deterministic partitioning technique for fault diagnosis in scan-based BIST is proposed. Properties of high quality partitions for improved fault diagnosis times are identified and low cost hardware implementations of high quality deterministic partitions are outlined. The superiority of the partitions generated by the proposed approach is confirmed through mathematical analysis. Theoretical analyses, worst case bounds, and experimental simulation data all confirm the superiority of the proposed deterministic approaches.", "num_citations": "36\n", "authors": ["530"]}
{"title": "High-level synthesis of gracefully degradable ASICs\n", "abstract": " We propose a novel graceful degradation scheme, L/U reconfiguration, which can tolerate a single permanent fault in each hardware class of ASIC data paths. In the proposed scheme, dynamic hardware rebinding and operation rescheduling are performed by a systematic perturbation of the original configuration. A high-level synthesis procedure, which automatically generates such fault-tolerant systems, is also presented. Experiments show that our reconfigurable AISC designs, as compared to optimal nonfault-tolerant designs, achieve optimal pre-reconfiguration and near-optimal post-reconfiguration speed performance.", "num_citations": "36\n", "authors": ["530"]}
{"title": "Scheduling with Rollback Constraints in High-Level Synthesis of Self-Recovering ASICs.\n", "abstract": " Whereas advances in VLSI technology are making it feasible to integrate millions of transistors on a single chip, such large scale integration is increasing the number of transient errors. Design techniques for recovering from transient errors have been well understood, while software mechanisms for the automatic synthesis of self-recovering application specific ICs (ASICs) remain relatively unexplored. In this paper, we develop software mechanisms for incorporating on-chip self-recovery-using checkpointing and rollback-during high-level synthesis. We propose an algorithm for rollback point insertion to minimize rollback overhead. It identifies good rollback points by successively eliminating clock cycle boundaries that are either expensive or violate the recovery time constraint. Moreover, only the minimum number of rollback points are inserted. Subsequently, we present a flexible synthesis methodology in which\u00a0\u2026", "num_citations": "36\n", "authors": ["530"]}
{"title": "Improved fault diagnosis in scan-based BIST via superposition\n", "abstract": " An improved approach for diagnosis of scan-based BIST designs is proposed. The enhancement in diagnosis is achieved by utilizing the superposition principle. Scan cells are partitioned pseudorandomly for observation and the ones provably fault free are removed from the potentially faulty list. Diagnostic resolution is improved by a novel application of the superposition principle, resulting in significant reductions in diagnosis time.", "num_citations": "35\n", "authors": ["530"]}
{"title": "Time-constrained scheduling during high-level synthesis of fault-secure VLSI digital signal processors\n", "abstract": " Advances in VLSI technology are making it feasible to pack millions of transistors on a single chip. A consequent increase in the number of on-chip faults as well as the growing importance of quality-metrics such as reliability and fault-tolerance are making on-chip fault-tolerance mandatory. On-chip realization of a computation is fault-secure if an observable error in the computation is detected. Components used in life-critical systems should be secured against all faults. While fault-security can be realized by duplicating the computation on disjoint hardware and voting on the result(s), such straightforward strategies entail appreciable hardware overhead. This paper presents computer-aided behavioral synthesis of fault-secure microarchitectures which require less than proportional increase in hardware. The strategy selects intermediate computations for additional voting. The resulting class of fault-secure\u00a0\u2026", "num_citations": "34\n", "authors": ["530"]}
{"title": "High-level synthesis of fault-secure microarchitectures\n", "abstract": " Advances in VLSI technology are making it feasible to pack millions of transistors on a single chip. A consequent increase in the number of on-chip faults as well as the growing import of quality metrics such as reliability and fault-tolerance are necessitating on-chip fault-tolerance. On-chip realization of a computation is fault-secure if no fault in the computation goes undetected. In this paper, we present high-level synthesis of fault-secure microarchitectures which require less than proportional increase in hardware. The proposed strategy selects intermediate computations for additional voting. The resulting class of fault-secure microarchitectures supplants the enormous hardware requirements of naive fault-secure strategies with enhanced hardware utilization afforded by securing the intermediate computations.", "num_citations": "34\n", "authors": ["530"]}
{"title": "Toward future systems with nanoscale devices: Overcoming the reliability challenge\n", "abstract": " Nanoelectronic devices are envisioned to deliver major improvements in device density, power, and performance, but turning such promises into reality hinges on overcoming the reliability challenge. At the same time, new characteristics exhibited by nanoscale devices demand rethinking reliability strategies.", "num_citations": "33\n", "authors": ["530"]}
{"title": "Tag compression for low power in dynamically customizable embedded processors\n", "abstract": " We present a methodology for power reduction by instruction/data cache-tag compression for low-power embedded processors. By statically analyzing the code/data memory layouts for the application hot spots, a variety of proposed schemes for effective tag-size reduction can be employed for power minimization in instruction and data caches. The schemes rely on significantly reducing the number of tag bits stored in the tag arrays for cache-conflict identification, thus considerably decreasing the number of active bitlines, sense amps, and comparator cells. We present a set of tag compression techniques and evaluate each of them separately in terms of efficiency and required hardware support. A detailed very large scale integrated implementation has been performed and a number of experimental results on a set of embedded applications is reported for each technique. Energy dissipation decreases of up to 95\u00a0\u2026", "num_citations": "33\n", "authors": ["530"]}
{"title": "Fault dictionary size reduction through test response superposition\n", "abstract": " The exceedingly large size of fault dictionaries constitutes a fundamental obstacle to their usage. We outline a new method to reduce significantly, the size of fault dictionaries. The proposed method partitions the test set and a combined signature is stored for each partition. The new approach aims to provide high diagnostic resolution with a small number of combined signatures. The experimental results show a considerable decrease in the storage requirement of fault dictionaries.", "num_citations": "32\n", "authors": ["530"]}
{"title": "Application specific non-volatile primary memory for embedded systems\n", "abstract": " Memory subsystems have been considered as one of the most critical components in embedded systems and furthermore, displaying increasing complexity as application requirements diversify. Modern embedded systems are generally equipped with multiple heterogeneous memory devices to satisfy diverse requirements and constraints. NAND flash memory has been widely adopted for data storage because of its outstanding benefits on cost, power, capacity and non-volatility. However, in NAND flash memory, the intrinsic costs for the read and write accesses are highly disproportionate in performance and access granularity. The consequent data management complexity and performance deterioration have precluded the adoption of NAND flash memory. In this paper, we introduce a highly effective non-volatile primary memory architecture which incorporates application specific information to develop a NAND\u00a0\u2026", "num_citations": "31\n", "authors": ["530"]}
{"title": "Logic level fault tolerance approaches targeting nanoelectronics plas\n", "abstract": " A regular structure and capability to implement arbitrary logic functions in a two-level logic form have placed crossbar-based programmable logic arrays (PLAs) as promising implementation architectures in the emerging nanoelectronics environment. Yet reliability constitutes an important concern in the nanoelectronics environment, necessitating a thorough investigation and its effective augmentation for crossbar-based PLAs. We investigate in this paper fault masking for crossbar-based nanoelectronics PLAs. Missing nanoelectronics devices at the crosspoints have been observed as a major source of faults in nanoelectronics crossbars. Based on this observation, we present a class of fault masking approaches exploiting logic tautology in two-level PLAs. The proposed approaches enhance the reliability of nanoelectronics PLAs significantly at low hardware cost", "num_citations": "30\n", "authors": ["530"]}
{"title": "The construction of optimal deterministic partitionings in scan-based BIST fault diagnosis: Mathematical foundations and cost-effective implementations\n", "abstract": " Partitioning techniques enable identification of fault-embedding scan cells in scan-based BIST. We introduce deterministic partitioning techniques capable of resolving the location of the fault-embedding scan cells. We outline a complete mathematical analysis that identifies the class of deterministic partitioning structures and complement this rigorous mathematical analysis with an exposition of the appropriate cost-effective implementation techniques. We validate the superiority of the deterministic techniques both in an average-case sense by conducting simulation experiments and in a worst-case sense through a thorough mathematical analysis.", "num_citations": "30\n", "authors": ["530"]}
{"title": "Multilevel testability analysis and solutions for integrated Bluetooth transceivers\n", "abstract": " As use of wireless communications rises and profit margins shrink, low-cost solutions are becoming increasingly important. Incorporating test design and DFT into the system design flow is essential to achieving such solutions. This case study analyzes test requirements, implications, and test cost for low-cost Bluetooth systems, which enable communication among several electronic components.", "num_citations": "30\n", "authors": ["530"]}
{"title": "Piercing logic locking keys through redundancy identification\n", "abstract": " The globalization of the IC supply chain witnesses the emergence of hardware attacks such as reverse engineering, hardware Trojans, IP piracy and counterfeiting. The consequent losses sum to billions of dollars for the IC industry. One way to defend against these threats is to lock the circuit by inserting additional key-controlled logic such that correct outputs are produced only when the correct key is applied. The viability of logic locking techniques in precluding IP piracy has been tested by researchers who have identified extensive weaknesses when access to a functional IC is guaranteed.In this paper, we uncover weaknesses of logic locking techniques when the attacker has no access to an activated IC, thus exposing vulnerabilities at the earliest stage even for applications that seek refuge from attacks through functional opaqueness. We develop an attack algorithm that prunes out the incorrect value of each\u00a0\u2026", "num_citations": "29\n", "authors": ["530"]}
{"title": "Performance and power effectiveness in embedded processors-customizable partitioned caches\n", "abstract": " This paper explores an application-specific customization technique for the data cache, one of the foremost area/power consuming and performance determining microarchitectural features of modern embedded processors. The automated methodology for. customizing the processor microarchitecture that we propose results in increased performance, reduced power consumption and improved determinism of critical system parts while the fixed design ensures processor standardization. The resulting improvements help to enlarge the significant role of embedded processors in modern hardware-software codesign techniques by leading to increased processor utilization and reduced hardware cost. A novel methodology for static analysis and a microarchitecturally field-reprogrammable implementation of a customizable cache controller that implements a partitioned cache structure is proposed. Partitioning the load\u00a0\u2026", "num_citations": "28\n", "authors": ["530"]}
{"title": "Power efficient branch prediction through early identification of branch addresses\n", "abstract": " Ever increasing performance requirements have elevated deeply pipelined architectures to a standard even in the embedded processor domain, requiring the incorporation of dynamic branch prediction subsystems to hide the execution latency of control-altering instructions. In this paper a low power early branch identification technique which enables the design of extremely power-efficient branch predictors and BTBs is proposed. Through static extraction of program information regarding the distance to subsequent branches, this technique enables the calculation of the next branch address as soon as the direction of the current branch has been predicted. Early identification of branch addresses enables a complete elimination of the power hungry BTB lookups normally occurring at every execution cycle, as well as a just-in-time wake-up mechanism when accessing\" hibernating\" entries in complex predictors\u00a0\u2026", "num_citations": "27\n", "authors": ["530"]}
{"title": "Test cost reduction through a reconfigurable scan architecture\n", "abstract": " Scan-based designs are widely used to keep test generation complexity within practical limits; nevertheless, scan-based design substantially increases test application time and test data volume. A novel scan-based design is proposed to reduce the test cost. The new scan-design exploits the low specified bit density of the test sets. The circular structure of the proposed architecture enables the use of the captured response of the previously applied pattern as a template for the subsequent pattern while allowing the full observation of the captured response. The functionality provided by the new architecture is utilized to update the template quickly to obtain the next pattern. The experimental results show a substantial reduction in test cost, reaching 90% levels.", "num_citations": "27\n", "authors": ["530"]}
{"title": "Virtual compression through test vector stitching for scan based designs\n", "abstract": " We propose a technique for compressing test vectors. The technique reduces test application time and tester memory requirements by utilizing part of the predecessor response in constructing the subsequent test vector. An algorithm is provided for stitching test vectors that retains full fault coverage while appreciably reducing time and tester requirements. The analysis provided enables significant compression ratios, while necessitating no hardware outlay whatsoever, making the technique we propose particularly suitable for SOC testing. The test time benefits necessitate no MISR utilization, ensuring no consequent aliasing loss. We examine a number of implementation considerations for the new compression technique and we provide experimental data that can be used to guide an eventual commercial implementation. Experimental data confirms the significant test application time and tester memory reductions.", "num_citations": "27\n", "authors": ["530"]}
{"title": "Enhancing reliability of RTL controller-datapath circuits via invariant-based concurrent test\n", "abstract": " We present a low-cost concurrent test methodology for enhancing the reliability of RTL controller-datapath circuits, based on the notion of path invariance. The fundamental observation supporting the proposed methodology is that the inherent transparency behavior of RTL components, typically utilized for hierarchical off-line test, renders rich sources of invariance within a circuit. Furthermore, additional sources of invariance are obtained by examining the algorithmic interaction between the controller, and the datapath of the circuit. A judicious selection & combination of modular transparency functions, based on the algorithm implemented by the controller-datapath pair, yields a powerful set of invariant paths in a design. Compliance to the invariant behavior is checked whenever the latter is activated. Thus, such paths enable a simple, yet very efficient concurrent test capability, achieving fault security in excess of 90\u00a0\u2026", "num_citations": "26\n", "authors": ["530"]}
{"title": "Scan power minimization through stimulus and response transformations\n", "abstract": " Scan-based cores impose considerable test power challenges due to excessive switching activity during shift cycles. The consequent test power constraints force SOC designers to sacrifice parallelism among core tests, as exceeding power thresholds may damage the chip being tested. Reduction of test power for SOC cores can thus increase the number of cores that can be tested in parallel, improving significantly SOC test application time. In this paper, we propose a scan chain modification technique that inserts logic gates on the scan path. The consequent beneficial test data transformations are utilized to reduce the scan chain transitions during shift cycles and hence test power. We introduce a matrix band algebra that models the impact of logic gate insertion between scan cells on the test stimulus and response transformations realized. As we have successfully modeled the response transformations as well\u00a0\u2026", "num_citations": "26\n", "authors": ["530"]}
{"title": "Gate level fault diagnosis in scan-based BIST\n", "abstract": " A gate level, automated fault diagnosis scheme is proposed for scan-based BIST designs. The proposed scheme utilizes both fault capturing scan chain information and failing test vector information and enables location identification of single stuck-at faults to a neighborhood of a few gates through set operations on small pass/fail dictionaries. The proposed scheme is applicable to multiple stuck-at faults and bridging faults as well. The practical applicability of the suggested ideas is confirmed through numerous experimental runs on all three fault models.", "num_citations": "26\n", "authors": ["530"]}
{"title": "Dynamic, multi-core cache coherence architecture for power-sensitive mobile processors\n", "abstract": " Today, mobile smartphones are expected to be able to run the same complex, memory-intensive applications that were originally designed and coded for general-purpose processors. However, these mobile processors are also expected to be compact, ultra-portable, and provide an always-on, continuous data access paradigm necessitating a low-power design. As mobile processors increasingly begin to leverage multi-core functionality, the power consumption incurred from maintaining coherence between local caches due to bus snooping becomes more prevalent. This paper explores a novel approach to mitigating multi-core processor power consumption in mobile smartphones. By using dynamic application memory behavior, one can intelligently target adjustments in the cache coherency protocol to help reduce the overhead of maintaining consistency when the benefits of multi-core shared cache coherence\u00a0\u2026", "num_citations": "25\n", "authors": ["530"]}
{"title": "Low-power scan testing for test data compression using a routing-driven scan architecture\n", "abstract": " A new scan architecture is proposed to reduce peak test power and capture power. Only a subset of scan flip-flops is activated to shift test data or capture test responses in any clock cycle. This can effectively reduce the capture test power and peak test power. Two routing-driven schemes are proposed to reduce the routing overhead. Experimental results show that the proposed scan architecture can effectively reduce peak test power, capture power, test data volume, and test application cost.", "num_citations": "24\n", "authors": ["530"]}
{"title": "Fault tolerant approaches to nanoelectronic programmable logic arrays\n", "abstract": " Programmable logic arrays (PLA), which can implement arbitrary logic functions in a two-level logic form, are promising as platforms for nanoelectronic logic due to their highly regular structure compatible with the nano crossbar architectures. Reliability is an important challenge as far as nanoelectronic devices are concerned. Consequently, it is necessary to focus on the fault tolerance aspects of nanoelectronic PLAs to ensure their viability as a foundation for nanoelectronic systems. In this paper, we investigate two types of fault tolerance techniques for nanoelectronic device based PLAs, focusing at the online faults occurring at the cross-points of nano devices. We develop a scheme to precisely locate the faults online, as this is a crucial step for efficient online reconfiguration based fault tolerance schemes. We also propose a tautology based fault masking scheme. We demonstrate that these two types of fault\u00a0\u2026", "num_citations": "24\n", "authors": ["530"]}
{"title": "Efficient construction of aliasing-free compaction circuitry\n", "abstract": " Parallel testing of cores can reduce SOC test times, but the finite number of chip I/Os limits such parallelism. Space and time compaction can maximize the required test bandwidth at the core outputs. Our proposed space and time compaction methodology guarantees a single-bit bandwidth, enabling the test of cores through the allocation of fewer chip pin-outs. In this way, our scheme maximizes parallelism among core tests.", "num_citations": "24\n", "authors": ["530"]}
{"title": "System-level test synthesis for mixed-signal designs\n", "abstract": " Hierarchical test approaches are a must for large designs due to the computational complexity and tight time-to-market requirements. In hierarchical test synthesis, test design is conducted at a subsystem level where the design complexity is manageable. For analog systems, tests are generally designed at the basic block level. This paper outlines a tool for translating basic block-level tests into system-level tests for large analog systems. Computational effectiveness is achieved by the use of high level models and by a pre-analysis of the system to identify feasible translation paths. A method to compute the fault and yield coverages of the resultant system-level tests is also provided in order to evaluate the translation. Experimental results show that test translation reduces design for testability overhead significantly while satisfying coverage requirements.", "num_citations": "23\n", "authors": ["530"]}
{"title": "Processor reliability enhancement through compiler-directed register file peak temperature reduction\n", "abstract": " Each semiconductor technology generation brings us closer to the imminent processor architecture heat wall, with all its associated adverse effects on system performance and reliability. Temperature hotspots not only accelerate the physical failure mechanisms such as electromigration and dielectric breakdown, but furthermore make the system more vulnerable to timing-related intermittent failures. Traditional thermal management techniques suffer from considerable performance overhead as the entire processor needs to be stalled or slowed down to preclude heat accumulation. Given the significant temporal and spatial variations of the chip-wide temperature, we propose in this paper a technique that directly targets one of the resources that is most likely to overheat in current processors, namely, the register files. Instead of duplicating or physically distributing the register file, we suggest to attain power density\u00a0\u2026", "num_citations": "22\n", "authors": ["530"]}
{"title": "Test power reductions through computationally efficient, decoupled scan chain modifications\n", "abstract": " SOC test time minimization hinges on the attainment of core test parallelism; yet test power constraints hamper this parallelism as excessive power dissipation may damage the SOC being tested. We propose a test power reduction methodology for SOC cores through scan chain modification. By inserting logic gates between scan cells, a given set of test vectors & captured responses is transformed into a new set of inserted stimuli & observed responses that yield fewer scan chain transitions. In identifying the best possible scan chain modification, we pursue a decoupled strategy wherein test data are decomposed into blocks, which are optimized for power in a mutually independent manner. The decoupled handling of test data blocks not only ensures significantly high levels of overall power reduction but it furthermore delivers computational efficiency at the same time. The proposed methodology is applicable to\u00a0\u2026", "num_citations": "22\n", "authors": ["530"]}
{"title": "DFT guidance through RTL test justification and propagation analysis\n", "abstract": " We introduce a formal mechanism for capturing test justification and propagation related behavior of blocks. Based on the identified test translation behavior, an RTL testability analysis methodology for hierarchical designs is derived. An algorithm for pinpointing the local-to-global test translation controllability and observability bottlenecks is presented. The analysis results are validated through an ATPG-based experimental flow and the applicability of the scheme for addressing test challenges in large designs by guiding DFT decisions is discussed.", "num_citations": "22\n", "authors": ["530"]}
{"title": "Microarchitectural synthesis of gracefully degradable, dynamically reconfigurable ASICs\n", "abstract": " In this paper, we propose a novel fault-tolerance scheme, band reconfiguration, to handle multiple permanent faults in functional units of general ASIC designs. An associated high-level synthesis procedure that automatically generates such fault-tolerant systems is also presented. The proposed scheme permits multiple levels of graceful degradation. During each reconfiguration the system instantly reconfigures itself through operation rescheduling and hardware rebinding. The design objectives are optimization of resource utilization rate under each configuration, and reduction of hardware and performance overheads. The proposed high-level synthesis approach enables fast and area-effective implementations of gracefully degradable ASICs.", "num_citations": "22\n", "authors": ["530"]}
{"title": "Testability metrics for synthesis of self-testable designs and effective test plans\n", "abstract": " We propose a set of unified metrics for self-testability which are portable across different phases of synthesis. Furthermore, applicability of the proposed test metrics is verified through extensive experiments on benchmark designs.", "num_citations": "22\n", "authors": ["530"]}
{"title": "Towards no-cost adaptive MPSoC static schedules through exploitation of logical-to-physical core mapping latitude\n", "abstract": " The computing engines of many current applications are powered by MPSoC platforms, which promise significant speedup but induce increased reliability problems as a result of ever growing integration density and chip size. While static MPSoC execution schedules deliver predictable worst-case performance, the absence of dynamic variability unfortunately constrains their usefulness in such an unreliable execution environment. Adaptive static schedules with predictable responses to run-time resource variations have consequently been proposed, yet the extra constraints imposed by adaptivity on task assignment have resulted in schedule length increases. We propose to eradicate the associated performance degradation of such techniques while retaining all the concomitant benefits, by exploiting an inherent degree of freedom in task assignment regarding the logical to physical core mapping. The proposed\u00a0\u2026", "num_citations": "21\n", "authors": ["530"]}
{"title": "Miss reduction in embedded processors through dynamic, power-friendly cache design\n", "abstract": " Today, embedded processors are expected to be able to run complex, algorithm-heavy applications that were originally designed and coded for general-purpose processors. As a result, traditional methods for addressing performance and determinism become inadequate. This paper explores a new data cache design for use in modern high-performance embedded processors that will dynamically improve execution time, power efficiency, and determinism within the system. The simulation results show significant improvement in cache miss ratios and reduction in power consumption of approximately 30% and 15%, respectively.", "num_citations": "21\n", "authors": ["530"]}
{"title": "Improving circuit robustness with cost-effective soft-error-tolerant sequential elements\n", "abstract": " Soft errors induced by alpha particles and cosmic radiation have become a highly challenging problem in the design of UDSM or nanoscale circuits, making the incorporation of circuit hardening techniques essential. In this paper, a design technique for soft-error-tolerant sequential elements is presented to improve circuit robustness. The proposed technique exploits time and space redundancy using an elaborate flip-flop structure, and provides complete soft error immunity for both the transient faults generated in the combinatorial logic and the particle strikes inside the flip- flops. The proposed technique is developed to be compatible with current digital design technology, thus having minimal impact on design flow and hardware cost. Simulation results confirm the effectiveness of the proposed technique.", "num_citations": "21\n", "authors": ["530"]}
{"title": "Compacting test responses for deeply embedded SoC cores\n", "abstract": " Test bandwidth allocation issues greatly limit the parallel testing of SoC cores. Here, the authors propose a response compaction methodology for reducing the required output core bandwidth, enabling increased parallelism among core tests and hence reducing the overall SoC test time.", "num_citations": "21\n", "authors": ["530"]}
{"title": "Data cache energy minimizations through programmable tag size matching to the applications\n", "abstract": " An application-specific customization methodology for minimizing the energy dissipation in the data cache of embedded processors is presented in this paper. The data cache subsystem is one of the most power consuming microarchitectural parts of embedded processors. We target in this work particularly the data cache tag operations and show how an exceedingly small number of tag bits, if any, are needed to compute the miss/hit behavior for the vast majority of load/store instructions executed within application loops. The energy needed to perform the tag reads and comparisons can be thus dramatically reduced. We follow up this conceptual enhancement with a presentation of an efficient, reprogrammable implementation that utilizes application-specific information to apply the suggested energy minimization approach. The conducted experimental results confirm the expected significant decrease of energy\u00a0\u2026", "num_citations": "21\n", "authors": ["530"]}
{"title": "Low-cost, software-based self-test methodologies for performance faults in processor control subsystems\n", "abstract": " A software-based testing methodology for processor control subsystems, targeting hard-to-test performance faults in high-end embedded and general-purpose processors, is presented. An algorithm for directly controlling, using the instruction-set architecture only, the branch-prediction logic, a representative example of the class of processor control subsystems particularly prone to such performance faults, is outlined. Experimental results confirm the viability of the proposed methodology as a low-cost and effective answer to the problem of hard-to-test performance faults in processor architectures.", "num_citations": "21\n", "authors": ["530"]}
{"title": "A design methodology for the high-level synthesis of fault-tolerant ASICs\n", "abstract": " Increased levels of integration are leading to single-chip systems and on-chip fault-tolerance. Whereas methodologies for designing fault-tolerant systems have been well understood, software mechanisms for the automatic synthesis of fault-tolerant application specific ICs (ASICs) remain relatively unexplored. In this paper, we systematically explore the three-dimensional design space spanned by cost, performance, and fault-tolerance constraints. In particular, we propose synthesis methodologies to (i) minimize cost subject to performance and fault-tolerance constraints, and (ii) maximize fault-tolerance given cost and performance constraints.", "num_citations": "21\n", "authors": ["530"]}
{"title": "Sanity-check: Boosting the reliability of safety-critical deep neural network applications\n", "abstract": " The widespread usage of deep neural networks in autonomous driving necessitates a consideration of the safety arguments against hardware-level faults. This study confirms the possible catastrophic impact of hardware-level faults on DNN accuracy; the consequent need for low-cost fault tolerance methods can be met through a rigorous exploration of the mathematical properties of the associated computations. We propose Sanity-Check, which makes use of the linearity property and employs spatial and temporal checksums to protect fully-connected and convolutional layers in deep neural networks. Sanity-Check can be purely implemented on software and deployed on different execution platforms with no additional modification. We also propose Sanity-Check hardware which integrates seamlessly with modern DNN accelerators and neutralizes the small performance overhead in pure software implementations\u00a0\u2026", "num_citations": "20\n", "authors": ["530"]}
{"title": "Reducing average and peak test power through scan chain modification\n", "abstract": " Parallel test application helps reduce the otherwise considerable test times in SOCs; yet its applicability is limited by average and peak power considerations. The typical test vector loading techniques result in frequent transitions in the scan chain, which in turn reflect into significant levels of circuit switching unnecessarily. Judicious utilization of logic in the scan chain can help reduce transitions while loading the test vector needed. The transitions embedded in both test stimuli and the responses are handled through scan chain modifications consisting of logic gate insertion between scan cells as well as inversion of capture paths. No performance degradation ensues as these modifications have no impact on functional execution. To reduce average and peak power, we herein propose computationally efficient schemes that identify the location and the type of logic to be inserted. The experimental results\u00a0\u2026", "num_citations": "20\n", "authors": ["530"]}
{"title": "Parity-based Output Compaction for Core-based SOCs [logic testing]\n", "abstract": " SOC test application time is strongly determined by the parallelism attained among core tests. Yet, test bandwidth allocation issues typically impose significant limitations on parallel testing of the cores. In this paper, we propose a response compaction methodology for reducing the required output bandwidth of cores, enabling increased parallelism among core tests and hence reducing overall SOC test time. The proposed methodology is based on judiciously partitioning test responses, with each response fragment being compacted individually. The signature corresponding to the response fragments consists of the parity information which is computed through a cost-effective on-chip space and time compaction mechanism. We show that the proposed technique not only delivers negligible aliasing for any (modeled or unmodeled) fault but that it also provides diagnostic and unknown response value handling\u00a0\u2026", "num_citations": "20\n", "authors": ["530"]}
{"title": "Compiler-based register name adjustment for low-power embedded processors\n", "abstract": " We present an algorithm for compiler-driven register name adjustment with the main goal of power minimization on instruction fetch and register file access. In most instruction set architecture (ISA) designs, the register fields reside in fixed positions within the instruction encoding, hence forming streams of indices on the instruction bus and to the register file address decoder. The number of bit transitions in these streams greatly determines the power consumption on the address bus and the register file decoder. While general-purpose registers are semantically indistinguishable and hence interchangeable, the particular register indices do have a direct impact on power consumption. The algorithms presented in this paper address this power minimization problem by reassigning/encoding the registers so that the bit transitions within the register index streams are minimized.", "num_citations": "19\n", "authors": ["530"]}
{"title": "Cost-effective deterministic partitioning for rapid diagnosis in scan-based BIST\n", "abstract": " Identifying fault-embedding scan cells is a significant challenge for fault diagnosis in scan based BIST. Deterministic partitioning techniques provide cost-effective solutions to this problem. Both mathematical solutions and simulations on hardware implementations demonstrate the effectiveness of these techniques.", "num_citations": "19\n", "authors": ["530"]}
{"title": "Automated test development and test time reduction for RF subsystems\n", "abstract": " Increasing percentage of test cost within the overall manufacturing cost for RF sub-systems results in a need for new, low-cost, and efficient test development methods. A methodology for automating test development for RF systems is presented. Test time reduction is achieved by selecting test signal attributes that can target several parameters at once. Due to its high computational efficiency, the tool can be applied multiple times at early design stages; thus enabling parallel test and design flow.", "num_citations": "19\n", "authors": ["530"]}
{"title": "Faults in processor control subsystems: Testing correctness and performance faults in the data prefetching unit\n", "abstract": " The processor control subsystems have for a long time been recognized as a bottleneck in the process of achieving complete fault coverage through various functional test propagation approaches. The difficult-to-test corner cases are further accentuated in fault-resilient control subsystems as no functional effect is incurred as a result of the fault, even though performance suffers. We investigate the construction of software programs, capable of providing full fault coverage at minimal hardware cost, for one such fault resilient subsystem in processor architecture: the data prefetching unit. Experimental results confirm the efficacy of the proposed method.", "num_citations": "19\n", "authors": ["530"]}
{"title": "Space and time compaction schemes for embedded cores\n", "abstract": " Testing embedded cores in a system-on-a-chip necessitates the use of a test access mechanism, which provides for transportation of the test data between the chip and the core I/Os. We outline an aliasing-free space and time compaction scheme, for both combinational and sequential cores, which minimizes the required test bandwidth and reduces the bandwidth consumption of the test access mechanism at the core output side. The experimental results show that the test bandwidth gain is achieved with no appreciable increase in test application time.", "num_citations": "19\n", "authors": ["530"]}
{"title": "Towards 100% testable FIR digital filters\n", "abstract": " Testability problems that arise in the design of fixed-coefficient finite impulse response (FIR) filters are examined. A class of redundant faults that naturally derive from the structure and behavior of these filters are examined, and design-for-test (DFT) techniques based on scaling theory are used to eliminate the redundancies. Eliminating these redundancies makes it possible for built-in self-test (BIST) approaches to reach 100% coverage, and automatic test-pattern generation (ATPG) based approaches can benefit by more than an order of magnitude reduction in test generation time. A case study provides a demonstration of the approach.", "num_citations": "19\n", "authors": ["530"]}
{"title": "Dynamic tag reduction for low-power caches in embedded systems with virtual memory\n", "abstract": " This paper presents a low-power tag organization for physically tagged caches in embedded processors with virtual memory support. An exceedingly small subset of tag bits is identified for each application hot-spot so that only these tag bits are used for cache access with no performance sacrifice as they provide complete address resolution. The minimal subset of physical tag bits is dynamically updated following the changes in the physical address space of the application. Operating system support is introduced in order to maintain the reduced tags during program execution. Efficient algorithms are incorporated within the memory allocator and the dynamic linker in order to achieve dynamic update of the reduced tags. The only hardware support needed within the I/D-caches is the support for disabling bitlines of the tag arrays. An extensive set of experimental results demonstrates the efficacy of the proposed\u00a0\u2026", "num_citations": "17\n", "authors": ["530"]}
{"title": "Power-constrained SOC test schedules through utilization of functional buses\n", "abstract": " In this paper, we are proposing a core-based test methodology that utilizes the functional bus for test stimuli and response transportation. An efficient algorithm for the generation of a complete test schedule that efficiently utilizes the functional bus under a power constraint is described. The test schedule is composed of a set of test vector delivery sequences in small chunks, denoted as packets. The utilization of small packet sizes optimizes the functional bus utilization. The experimental results show that the methodology is highly effective compared to previous approaches that do not use the functional bus. The strong results of the proposed approach are particularly highlighted when small bus widths are considered, an important consideration in current SOC designs where increasingly larger bus widths pose routing and reliability challenges.", "num_citations": "17\n", "authors": ["530"]}
{"title": "Nanofabric topologies and reconfiguration algorithms to support dynamically adaptive fault tolerance\n", "abstract": " Emerging nanoelectronics are expected to have very high manufacture-time defect rates and operation-time fault rates. Traditional N-modular redundancy (NMR) exploits the large device densities offered by these nanoelectronics to tolerate these high fault rates by allocating redundant resources according to the worst case fault rates. However, this approach is inflexible when the fault rates are time varying. In this paper, we propose a dynamically adaptive NMR approach by developing: (i) a genre of nanofabric topologies that supports sharing of redundancies in the NMR approach so as to adapt to the time varying fault rates and (ii) reconfiguration algorithms for these topologies to deal with fault tolerance loss caused by manufacturing defects and operation-time online faults, respectively. Simulation results verify that the ability to construct reliable systems, possibly the paramount consideration in constructing\u00a0\u2026", "num_citations": "17\n", "authors": ["530"]}
{"title": "Fault tolerant arithmetic with applications in nanotechnology based systems\n", "abstract": " Several emerging nanotechnologies have been displaying the negative differential resistance (NDR) characteristic, which makes them naturally support multi-valued logic with a large number of logic states. Such multi-valued logic with a large number of logic states can support a native digit-level redundant number system and hence a native digit-level carry save arithmetic. We present a new approach to linear block code based fault-tolerant arithmetic in NDR nanotechnologies. Specifically, we show how linear block codes can be used for error checking and error correction in carry save arithmetic operations. The proposed approach significantly improves timing and fault-tolerance of arithmetic operations in the highly unreliable nanoelectronic environment. Since digit-level information redundancy via linear block codes is widely used for fault tolerant communications and storage systems, the proposed scheme\u00a0\u2026", "num_citations": "17\n", "authors": ["530"]}
{"title": "RT-level fault simulation based on symbolic propagation\n", "abstract": " The rapid rise in size and complexity of VLSI circuits has stimulated a need to handle fault simulation at higher levels of abstraction. We outline an RT-level fault simulation technique that utilizes symbolic data to group fault effects. Experimental results show that the proposed methodology provides superior speed-ups and accurate fault coverages.", "num_citations": "17\n", "authors": ["530"]}
{"title": "Diagnosis for scan-based BIST: Reaching deep into the signatures\n", "abstract": " For partitioning-based diagnosis in a scan-based BIST environment, an exact analysis scheme, capable of identifying all scan cells that receive incorrect data, is proposed. In contrast to previously suggested approaches, the scheme we propose identifies all failing scan cells with no ambiguity whatsoever. Not only do we resolve failing scan cells unambiguously, but we do so at the earliest possible instance through reexamination of already computed signatures. Intensive utilization of this highly precise diagnostic state information leads to prognostic information regarding the usefulness of running upcoming tests which in turn leads to reductions in diagnosis time in excess of 30% compared to previous approaches.", "num_citations": "17\n", "authors": ["530"]}
{"title": "Extracting Precise Diagnosis of Bridging Faults from Stuck-at Fault Information.\n", "abstract": " Although the stuck-at fault model is the standard fault model. the frequently occurring faults in some technologies arc unintentional shorts, denoted as bridging faults. We outline a method that utilizes the information from the stuck-at fault model to accurately diagnose the bridging faults that affect two lines. The proposed method exploits the observation that the bridging fault response matches the stuck-at fault responses on the shorted lines for the failing test vectors and generates a candidate list that accounts for all failures. A further reduction in the size of the candidate set is achieved by extracting information from the test vectors that do not fail. The proposed method uses no layout information whatsoever. Nonetheless, the experimental results indicate that the bridging faults can be accurately diagnosed delivering a reduction in the sizes of the ambiguity sets and full capture of the offending bridging fault.", "num_citations": "16\n", "authors": ["530"]}
{"title": "Virtual page tag reduction for low-power TLBs\n", "abstract": " We present a methodology for a power-optimized, software-controlled translation lookaside buffer (TLB) organization. A highly reduced number of virtual page number (VPN) bits sufficient to perform physical address translation is efficiently identified and used when performing TLB lookups, delivering significant power reductions. Information regarding the virtual address space of the program code and data provided by the compiler is augmented with information regarding the dynamically linked libraries and data allocated run-time by the loader, the dynamic linker, and the memory manager. The hardware support needed is constrained to disabling bitlines of the tag arrays associated to the 1-TLB and the D-TLB. Algorithms for identifying the reduced VPNs for power optimized TLB operations together with the required OS support are presented.", "num_citations": "16\n", "authors": ["530"]}
{"title": "Invariance-based on-line test for RTL controller-datapath circuits\n", "abstract": " We present a low-cost on-line test methodology for RTL controller-datapath pairs, based on the notion of path invariance. The fundamental observation supporting the proposed methodology is that the transparency behavior inherent in RTL components renders rich sources of invariance in a design. Furthermore, the algorithmic controller-datapath interaction provides additional sources of invariance. A judicious selection and combination of modular transparency, based on the algorithm implemented by the controller-datapath pair, yields a powerful set of invariant paths. Such paths enable a simple, yet very efficient on-line test capability, achieving fault security in excess of 90% while keeping the hardware overhead below 40% on complicated, difficult to test, benchmarks.", "num_citations": "16\n", "authors": ["530"]}
{"title": "TRANSPARENT: A system for RTL testability analysis, DFT guidance and hierarchical test generation\n", "abstract": " We discuss a methodology for analyzing the testability of large hierarchical RTL designs, based upon the existence of module reachability paths, suitable for automatically deriving globally applicable test from locally generated vectors. Such reachability paths utilize module transparency behavior, as captured by the introduced channel transparency definition. Lack of transparency and unreachable module UOs pinpoint testability bottlenecks apt for efficient DFT modifications. Application of this methodology on example designs results in significant fault coverage improvement and test generation speedup, as compared to complete design gate-level ATPG.", "num_citations": "16\n", "authors": ["530"]}
{"title": "Microarchitectural synthesis of ICs with embedded concurrent fault isolation\n", "abstract": " In an increasing number of applications, reliability is essential. On-line resiliency when confronted with permanent faults is a difficult and important aspect of providing reliability. Particularly vexing is the problem of fault identification. Current methods are either domain specific or expensive. We have developed an approach to permanent fault isolation. In high-level synthesis that enables isolation through algorithmic application without necessitating complete functional unit replication. Fault identification is achieved through a unique binding methodology based on an extension of parity-like error correction equations in the domain of functional units. The result is an automated chip level approach with extremely low area and cost overhead.", "num_citations": "16\n", "authors": ["530"]}
{"title": "Computer-aided design of fault-tolerant VLSI systems\n", "abstract": " The authors present a flexible methodology for compiling an algorithmic description into an equivalent fault-tolerant VLSI circuit and a CAD framework embodying this methodology. Experimental designs illustrate and validate algorithms for automated synthesis of ICs featuring either self-recovery capability or enhanced reliability.", "num_citations": "16\n", "authors": ["530"]}
{"title": "Hierarchical modeling of the VLSI design process\n", "abstract": " A description is given of the Kinden environment, which combines object-oriented modeling and model-based reasoning to capture, integrate, and manage VLSI design process attributes and hierarchies. Related work is briefly reviewed, and the modeling of the design process is discussed, focusing on the Kinden approach. The model-based reasoning on which Kinden's knowledge-processing architecture is based is described. The present implementation of Kinden is examined.< >", "num_citations": "15\n", "authors": ["530"]}
{"title": "Fault tolerant nanoelectronic processor architectures\n", "abstract": " In this paper we propose a fault-tolerant processor architecture and an associated fault-tolerant computation model capable of fault tolerance in the nanoelectronic environment that is characterized by high and time varying fault rates. The proposed fault tolerant processor architecture not only guarantees the correctness of computation but also is flexible in that it dynamically trades-off computation resources and performance. The core of the architecture is a decentralized instruction control unit called the voter that achieves both fault tolerance and the maximum parallel execution of instructions by exploiting the abundant computational resources provided by nanotechnologies. Although the result of each instruction needs to be confirmed by executing it on multiple computation units, multiple unconfirmed instructions can proceed as speculative branches. The voter implements a hardware-frugal computation unit\u00a0\u2026", "num_citations": "14\n", "authors": ["530"]}
{"title": "Application-specific instruction memory customizations for power-efficient embedded processors\n", "abstract": " An encoding technique exploits application information to reduce power consumption along the instruction memory communication path in embedded processors. Microarchitectural support enables reprogrammability of the encoding transformations to track specific code effectively, and the restriction to functional transformations delivers major power savings. Having reprogrammable hardware also allows flexible, inexpensive switches between transformations.", "num_citations": "14\n", "authors": ["530"]}
{"title": "High-level synthesis of fault-tolerant ASICs\n", "abstract": " Methodologies for the high-level synthesis of fault-tolerant application-specific ICs (ASICs) that maximize performance in the presence of fault-tolerance and cost constraints are developed. The fault-tolerance constraints supported include number of faults per module (fault-masking constraint) and chip reliability (reliability constraint). Experience with the system shows that (a) it is feasible to automate design for fault-tolerance and (b) controlled interplay between cost, performance, and fault-tolerance, during high-level synthesis, helps synthesize high-quality and cost-effective fault-tolerant ASICs.< >", "num_citations": "14\n", "authors": ["530"]}
{"title": "Register allocation for simultaneous reduction of energy and peak temperature on registers\n", "abstract": " In this paper, we focus on register allocation techniques to simultaneously reduce energy consumption and heat buildup of register accesses. The conflict between these two objectives is resolved through the introduction of a hardware rotator. A register allocation algorithm followed by a refinement method is proposed based on the access patterns and the effects of the rotator. Experimental results show that the proposed algorithms obtain notable improvements in energy consumption and temperature reduction for embedded applications.", "num_citations": "13\n", "authors": ["530"]}
{"title": "A light-weight cache-based fault detection and checkpointing scheme for MPSoCs enabling relaxed execution synchronization\n", "abstract": " While technology advances have made MPSoCs a standard architecture for embedded systems, their applicability is increasingly being challenged by dramatic increases in the amount of device failures that may occur during execution. Conventional fault tolerance techniques employ a duplication-and-comparison strategy to detect arbitrary execution faults, as well as a checkpointing-and-rollback strategy to recover from the faulty state. Comparison and checkpointing are performed either at task level, thus imposing a large amount of overhead in verifying and backing up memory pages, or at instruction level, thus necessitating a lock-step execution model which significantly limits the attainable performance. To overcome the shortcomings of both strategies, in this paper we propose a cache-based fault tolerance scheme wherein the comparison and checkpointing process is performed at the cache-memory interface\u00a0\u2026", "num_citations": "13\n", "authors": ["530"]}
{"title": "Towards fault tolerant parallel prefix adders in nanoelectronic systems\n", "abstract": " Future nanoelectronics based arithmetic components will enjoy abundant hardware, yet at the same time confront severe unreliability challenges. We focus on the fault tolerance of high performance parallel prefix adders (PPA), and exploit the inherent redundancy in PPAs to develop efficient fault tolerance approaches. We show that the internal invariant inherent in the parallel prefix adders provides support for online fault detection and fault masking. Furthermore, based on the particular regular structure of PPAs, an online diagnosis scheme can be developed, thus enabling the application of reconfigurability of nanoelectronics for the highly flexible online repair approaches. In contrast to traditional fault tolerance techniques that rely solely on significant external overhead, the proposed approach opens up a new genre of efficient fault tolerance techniques for arithmetic components in the nanoelectronic environment.", "num_citations": "13\n", "authors": ["530"]}
{"title": "Fault identification in reconfigurable carry lookahead adders targeting nanoelectronic fabrics\n", "abstract": " Online repair through reconfiguration is a particularly advantageous approach in the nanoelectronic environment since reconfigurability is naturally supported by the devices. However, precise identification of faulty locations is of critical importance for fine-grain repairs. A CLA is mainly composed of: (1) carry generation blocks; and (2) g,p signal generation blocks. In this paper we propose two schemes for fault identification in these two parts correspondingly. For carry generation blocks, an inherently redundant computation path is exploited to identify the faulty block with high precision. As a time redundancy approach, recomputation with rotated operands (RERO) has been utilized in online fault detection for CLA's (Li and Swartzlander, 1992). For g,p generation blocks, we exploit the RERO scheme to achieve precise fault identification. A comprehensive analysis is provided for the aliasing in the proposed fault\u00a0\u2026", "num_citations": "13\n", "authors": ["530"]}
{"title": "Frugal linear network-based test decompression for drastic test cost reductions\n", "abstract": " In This work we investigate an effective approach to construct a linear decompression network in the multiple scan chain architecture. A minimal pin architecture, complemented by negligible hardware overhead, is constructed by mathematically analysing test data relationships, delivering in turn drastic test reductions. The proposed network drives a large number of internal scan chains with a short input vector, thus allowing significant reductions in both test time and test volume. The proposed method constructs an inverter-interconnect based network by exploring the pairwise linear dependencies of the internal scan chain vectors, resulting in a very low cost network that is nonetheless capable of outperforming much costlier compression schemes. We propose an iterative algorithm to construct the network from an initial set of test cubes. The experimental data shows significant reductions in test time and test volume\u00a0\u2026", "num_citations": "13\n", "authors": ["530"]}
{"title": "Energy frugal tags in reprogrammable I-caches for application-specific embedded processors\n", "abstract": " In this paper we present a software-directed customization methodology for minimizing the energy dissipation in the instruction cache, one of the most power consuming microarchitectural components of high-end embedded processors. We target particularly the instruction cache tag operations and show how an exceedingly small number of tag bits, if any, are needed to compute the miss/hit behavior for the most frequently executed application loops, thus minimizing the energy needed to perform the tag reads and comparisons. The proposed methodology exploits the fact that the code layout structure of the program loops can be identified after compile and link, and that it typically resides in a very confined memory location, for which very few bits from the effective address can be utilized as a tag. Subsequently, we present an efficient, programmable implementation to apply the suggested energy minimization\u00a0\u2026", "num_citations": "13\n", "authors": ["530"]}
{"title": "Concurrent test for digital linear systems\n", "abstract": " Invariant-based concurrent test schemes can provide economical solutions to the problem of concurrent error detection. An invariant-based concurrent error-detection scheme for linear digital systems is proposed. The cost of concurrent error-detection hardware is appreciably reduced due to utilization of a time-extended invariant, which extends the error-checking computation over time and, thus, reduces hardware requirements. Error-detection capabilities of the scheme proposed in this work are analyzed and conditions on the implementation for achieving complete fault coverage are outlined. Implementations fulfilling such conditions have been shown through experiments to provide 100% concurrent fault detection.", "num_citations": "13\n", "authors": ["530"]}
{"title": "Test synthesis for mixed-signal SOC paths\n", "abstract": " Higher levels of integration, the need for test re-use, and the mixed-signal nature of today's SOC's necessitate hierarchical test generation and system level test composition to meet stringent market requirements. In this paper a novel methodology for testing analog and digital components in a signal path is discussed. Consequent testability analysis can be utilized to reduce DFT requirements, while test translation provides highly effective low cost test. The proposed approach seamlessly propagates test information across the analog/digital divide. Experimental results substantiate the effectiveness of the proposed mixed-signal test synthesis methodology.", "num_citations": "13\n", "authors": ["530"]}
{"title": "Low-cost on-line test for digital filters\n", "abstract": " A low-cost on-line test scheme for digital filters is proposed. The scheme uses an invariant of the digital filter, the frequency response at specific points, in order to detect possible malfunctioning of the circuit. The analysis performed indicates that 100% fault secureness is possible, if certain design constraints are followed.", "num_citations": "13\n", "authors": ["530"]}
{"title": "Detecting hardware Trojans without a Golden IC through clock-tree defined circuit partitions\n", "abstract": " The sensitive identification and detection of hardware Trojans in ICs without a golden reference constitutes a key challenge. Traditional circuit partitioning and side-channel analysis techniques fall short of perfect sensitivity and accuracy and rely on golden references. In this work, a novel layout-aware clock tree driven circuit partitioning is coupled with an algorithm that selects transition delay fault test patterns that will deliver equal power on partitions. The circuit partitioning through the clock tree results in minimal hardware additions that can be effected through ECO. The selection of pairs of power uniform small regions results in reduction of inter-die variation effects, thus delivering increased detection sensitivity. The comparison for equal power of numerous pairs thoroughly perturbs the circuit under various activation conditions, resulting in elevated sensitivity and accuracy while obviating the need for reliance on\u00a0\u2026", "num_citations": "12\n", "authors": ["530"]}
{"title": "Sleep-aware variable partitioning for energy-efficient hybrid PRAM and DRAM main memory\n", "abstract": " Energy consumption of memories is always a significant issue for computing systems. Recently, hybrid PRAM and DRAM memory architectures have been proposed. It combines the advantages of DRAM and PRAM, such as low leakage power in PRAM and short write latency in DRAM. However, the leakage power in DRAM is still considerable in hybrid memories. The leakage power can only be reduced by turning DRAM into sleep state. In this paper, a novel proximity concept is proposed to guide the variable partitioning to maximize the possibility of turning DRAM into sleep mode. A novel Sleep-Aware Variable Partition Algorithm (SAVPA) is then proposed with the objective of maximizing the sleep time of DRAM while satisfying the performance and endurance constraints. The experiment results show that SAVPA reduces the energy consumption by 11.25% in average (up to 15.84%) compared to the state-of-art\u00a0\u2026", "num_citations": "12\n", "authors": ["530"]}
{"title": "Power-efficient instruction delivery through trace reuse\n", "abstract": " As power dissipation inexorably becomes the major bottleneck in system integration and reliability, the front-end instruction delivery path in a traditional out-of-order superscalar processor needs to deliver high application performance in an energy-effective manner. This challenge can be addressed by efficiently reusing the work of fetch and decode performed during preceding loop iterations and resident mostly within the processor itself. As a large percentage of the instructions currently under fetch have previously dispatched copies resident in the Reorder Buffer (ROB), in this paper we develop a mechanism to utilize the ROB as a storage location for previously decoded instructions. Thus instructions can be fed directly from the ROB into the rename and issue stages, enabling the gating off of the fetch and decode logic for large periods of time so as to deliver significant power savings. Power and performance\u00a0\u2026", "num_citations": "12\n", "authors": ["530"]}
{"title": "Architectural-level fault tolerant computation in nanoelectronic processors\n", "abstract": " Nanoelectronic devices are expected to have extremely high and variable fault rates; thus future processor architectures based on these unreliable devices need to be built with fault tolerance embedded so as to satisfy the fundamental requirement of computational correctness. In this paper an architectural-level computation model is proposed for fault tolerant computations in nanoelectronic processors. The proposed scheme is capable of guaranteeing the correctness of each instruction through exploitation of both hardware and time redundancy, even under high and variable fault rates. Each instruction is confirmed by multiple computation instances. Through a speculative execution based on unconfirmed results, the proposed scheme eliminates the severe performance deterioration typically caused by time redundancy approaches on data dependent instructions. To avoid the exponential growth of resource\u00a0\u2026", "num_citations": "12\n", "authors": ["530"]}
{"title": "Guest editors' introduction: Application-specific microprocessors\n", "abstract": " The proliferation of electronic solutions in all aspects of life has dramatically increased, not only the number of embedded processors, but also the domains in which they reside. For example, nowadays, electronics consume a significant portion of an automobile's cost. The mobile telecommunications revolution would have been unthinkable without the heavy use of embedded processors. Countless consumer electronics, from refrigerators to video games, rely on embedded processors.This expansion in use, along with a dramatic increase in volume, calls for a reevaluation of existing embedded-processor models. The variety of dominant attributes\u2014power in mobile applications, cost in automotive applications, reliability in critical aerospace applications, and performance in many other applications\u2014impose significantly different requirements on the design and implementation of embedded-processor architectures\u00a0\u2026", "num_citations": "12\n", "authors": ["530"]}
{"title": "On-line test for fault-secure fault identification\n", "abstract": " In an increasing number of applications, reliability is essential. On-line resistance to permanent faults is a difficult and important aspect of providing reliability. Particularly vexing is the problem of fault identification. Current methods are either domain specific or expensive. We have developed a fault-secure methodology for permanent fault identification through algorithmic duplication without necessitating complete functional unit replication. Fault identification is achieved through a unique binding methodology during high-level synthesis based on an extension of parity-like error correction equations in the domain of functional units. The result is an automated chip-level approach with extremely low area and cost overhead.", "num_citations": "12\n", "authors": ["530"]}
{"title": "Unifying methodologies for high fault coverage concurrent and off-line test of digital filters\n", "abstract": " A low-cost on-line test scheme for digital filters that additionally provides an off-line BIST solution is proposed. The scheme utilizes an invariant of the digital filter in order to detect on-line possible circuit malfunctions. The on-line checking hardware is shared with off-line BIST. The analysis performed indicates that exact 100% fault secureness is attained when the digital filter is designed according to design criteria that we identify in the paper. Furthermore, fault simulations show near 100% fault coverage for off-line BIST.", "num_citations": "12\n", "authors": ["530"]}
{"title": "Testability improvement in high-level synthesis through reconvergence reduction\n", "abstract": " Justification of multiple circuit lines in automatic test pattern generation (ATPG) is exponential in complexity in the presence of reconvergent fanout. Reconvergent fanout consequently is a chief source of increased complexity in the ATPG process. Reconvergence also degrades the pseudo-random test by producing correlation between inputs of the same combinational logic block. Consideration of reconvergence during synthesis can result in its elimination at minimal area or performance cost. The high regularity of DSP architectures facilitates reconvergence reduction, when, it is addressed during synthesis. We present a design-for-testability approach to remove reconvergence during high-level synthesis. We have developed a method for estimating the degree of reconvergence, based on an estimate of the existence of paths between each pair of hardware units. We have designed and implemented scheduling\u00a0\u2026", "num_citations": "12\n", "authors": ["530"]}
{"title": "Test path generation and test scheduling for self-testable designs\n", "abstract": " The high cost of chip testing makes testability an important aspect of any chip design. Early inclusion of test considerations into the design process can reduce test time and area overhead. We have developed an algorithm which defines built-in self-testing (BIST) tests for an RTL datapath. Datapath registers are chosen to be upgraded to testable registers to execute the define tests. Test time of an individual test is reduced by considering pattern randomness and error masking transparency properties of modules involved in the test. Parallelism between different tests is increased by reducing the number of conflicts between tests. Intertwining the creation of tests with the selection of testable registers in the datapath guides the choice of testable registers to a minimal area solution. The use of these metrics provides our system with an accurate estimate of test time, and therefore facilitates the definition of tests which\u00a0\u2026", "num_citations": "12\n", "authors": ["530"]}
{"title": "Intertwined scheduling, module selection and allocation in time-and-area\n", "abstract": " Most high-level synthesis systems assume the existence of only one type of hardware module for each different type of operation. The system presented assumes the existence of multiple modules with identical functionality but different area and delay characteristics. The authors' results show that use of multiple modules allows area and time resources to be used more efficiently than would be the case for the use of a single module. Module selection must be performed for each operation. Scheduling and module selection decisions are made in a time-constrained and area-constrained fashion by pruning decisions which lead to designs that violate constraints. This system requires the user to specify only a total chip area constraint, allowing the system to fully explore tradeoffs between different allocations. Module selection, scheduling, and allocation are performed in an intertwined fashion.< >", "num_citations": "12\n", "authors": ["530"]}
{"title": "Branch prediction-directed dynamic instruction cache locking for embedded systems\n", "abstract": " Cache locking is a cache management technique to preclude the replacement of locked cache contents. Cache locking is often adopted to improve cache access predictability in Worst-Case Execution Time (WCET) analysis. Static cache locking methods have been proposed recently to improve Average-Case Execution Time (ACET) performance. This article presents an approach, Branch Prediction-directed Dynamic Cache Locking (BPDCL), to improve system performance through cache conflict miss reduction. In the proposed approach, the control flow graph of a program is first partitioned into disjoint execution regions, then memory blocks worth locking are determined by calculating the locking profit for each region. These two steps are conducted during compilation time. At runtime, directed by branch predictions, locking routines are prefetched into a small high-speed buffer. The predetermined cache locking\u00a0\u2026", "num_citations": "11\n", "authors": ["530"]}
{"title": "Cost-effective IR-drop failure identification and yield recovery through a failure-adaptive test scheme\n", "abstract": " Ever-increasing test mode IR-drop results in a significant amount of defect-free chips failing at-speed testing. The lack of a systematic IR-drop failure identification technique engenders a highly increased failure analysis time/cost and significant yield loss. In this paper, we propose a failure-adaptive test scheme that enables a fast differentiation of the IR-drop induced failure from the actual defects of the chip. The proposed technique debugs the failing chips using low IR-drop vectors that are custom-generated from the observed faulty response. Since these special vectors are designed in such a way that all the actual defects captured by the original vectors are still manifestable, their application can clearly pinpoint whether the root cause of failure is IR-drop or not, thus eliminating reliance on an intrusive debugging process that incurs quite a high cost. Such a test scheme further enables effective yield recovery from\u00a0\u2026", "num_citations": "11\n", "authors": ["530"]}
{"title": "Scan power reduction in linear test data compression scheme\n", "abstract": " XOR network-based on-chip test compression schemes have been widely employed in large industrial scan designs due to their high compression ratio and efficient decompression mechanism. Nevertheless, such a scheme necessitates high unspecified bit ratios in the original test cubes, resulting in quite significant difficulties in preprocessing test cubes for scan power reduction. The linear mapping from the original cubes to the compressed seeds typically provides extra degrees of flexibility as multiple seeds may reconstruct the test cube. Appreciable power reductions in the decompressed test data can be attained through the pinpointing of the power-optimal seeds during the compression phase. The proposed work explores the aforementioned flexibility in the seed space, and proposes the mathematical and algorithmic framework for a power-aware linear test compression scheme. The proposed technique\u00a0\u2026", "num_citations": "11\n", "authors": ["530"]}
{"title": "Towards nanoelectronics processor architectures\n", "abstract": " In this paper, we focus on reliability, one of the most fundamental and important challenges, in the nanoelectronics environment. For a processor architecture based on the unreliable nanoelectronic devices, fault tolerance schemes are required so as to ensure the basic correctness of any computation. Since any fault tolerance approach demands redundancy either in the form of time or hardware, reliability needs to be considered in conjunction with the performance and hardware tradeoffs. We propose a new computational model for the nanoelectronics based processor architectures, that provides flexible fault tolerance to deal with the high and time varying faults. The model guarantees the correctness of instruction executions, while dynamically balancing hardware and performance overheads. The correctness of every instruction is confirmed by multiple execution instances through a hybrid hardware\u00a0\u2026", "num_citations": "11\n", "authors": ["530"]}
{"title": "An integrated tool for analog test generation and fault simulation\n", "abstract": " High levels of design integration and increasing number of analog blocks within a system necessitate automated system-level analog test generation and fault simulation tools. We outline a methodology and toolset for specification-based automated test generation and fault simulation for analog circuits. Test generation is targeted at providing the highest coverage for each specified parameter. The flexibility of assigning analog test attributes is utilized for merging tests leading to test time reduction with no loss in test coverage. Further optimization in test time is obtained through fault simulations by selecting tests that provide adequate coverage in terms of several components and dropping the ones that do not provide additional coverage. The generated test set, fault and yield coverages in terms of each targeted parameter, and testability problems are reported by the tool.", "num_citations": "11\n", "authors": ["530"]}
{"title": "Fast hierarchical test path construction for DFT-free controller-datapath circuits\n", "abstract": " We discuss a hierarchical test generation method for DFT-free controller-datapath pairs. A transparency based scheme is devised for the datapath, wherein locally generated vectors are translated into global design test. The controller is examined through influence tables, used to generate valid control state sequences for testing each module through hierarchical test paths. Fault coverage levels and vector counts thus attained match closely, those of traditional test generation methodologies, while sharply reducing the corresponding computational cost.", "num_citations": "11\n", "authors": ["530"]}
{"title": "A high-level synthesis methodology for low-power VLSI design\n", "abstract": " A high-level synthesis methodology for low-power design is described. With the objective of supporting the design of low-power, performance-constrained systems such as signal processing applications, the methodology enables the designer to place throughput and latency constraints on the synthesized design. A library-based design style is used, where libraries may include multiple implementations of each component type. Library components are characterized by their relative power, area, and delay performance. The methodology has been implemented in the Sierra high-level synthesis system.", "num_citations": "11\n", "authors": ["530"]}
{"title": "Variation-aware hardware Trojan detection through power side-channel\n", "abstract": " A hardware Trojan (HT) denotes the malicious addition or modification of circuit elements. The purpose of this work is to improve the HT detection sensitivity in ICs using power side-channel analysis. This paper presents three detection techniques in power based side-channel analysis by increasing Trojan-to-circuit power consumption and reducing the variation effect in the detection threshold. Incorporating the three proposed methods has demonstrated that a realistic fine-grain circuit partitioning and an improved pattern set to increase HT activation chances can magnify Trojan detectability.", "num_citations": "10\n", "authors": ["530"]}
{"title": "On-device objective-C application optimization framework for high-performance mobile processors\n", "abstract": " Smartphones provide applications that are increasingly similar to those of interactive desktop programs, providing rich graphics and animations. To simplify the creation of these interactive applications, mobile operating systems employ highlevel object-oriented programming languages and shared libraries to manipulate the device's peripherals and provide common userinterface frameworks. The presence of dynamic dispatch and polymorphism allows for robust and extensible application coding. Unfortunately, the presence of dynamic dispatch also introduces significant overheads during method calls, which directly impact execution time. Furthermore, since these applications rely heavily on shared libraries and helper routines, the quantity of these method calls is higher than those found in typical desktop-based programs. Optimizing these method calls centrally before consumers download the application onto a\u00a0\u2026", "num_citations": "10\n", "authors": ["530"]}
{"title": "Dynamic transient fault detection and recovery for embedded processor datapaths\n", "abstract": " As microprocessors continue to evolve and grow in functionality, the use of smaller nanometer technology scaling coupled with high clock frequencies and exponentially increasing transistor counts dramatically increases the susceptibility of transient faults. However, the correct and reliable operation of these processors is often compulsory, both in terms of consumer experience and for high-risk embedded domains such as medical and transportation systems. Thus, economical fault detection and recovery becomes essential to meet all necessary market requirements. This paper explores the efficient leveraging of superscalar, out-of-order architectures to enable multi-cycle transient fault-tolerance throughout the datapath in a novel manner. By using dynamic instruction execution redundancy, soft errors within the datapath are both detected and recovered. The proposed microarchitecture selectively reevaluates\u00a0\u2026", "num_citations": "10\n", "authors": ["530"]}
{"title": "Energy-efficient physically tagged caches for embedded processors with virtual memory\n", "abstract": " In this paper we present a low-power tag organization for physically tagged caches in embedded processors with virtual memory support. An exceedingly small subset of tag bits is identified for each application hot-spot so that only these tag bits are used for cache access with no performance sacrifice as they provide complete address resolution. The minimal subset of physical tag bits, i.e. the compressed tag, is dynamically updated following the changes in the physical address space of the application. Special support from the operating system (OS) is introduced in order to maintain the compressed tag during program execution. The compressed tag is updated by the OS to match the current set of physical memory pages allocated to the application. We have proposed efficient algorithms that are incorporated within the memory allocator and the dynamic linker in order to achieve dynamic update of the compressed\u00a0\u2026", "num_citations": "10\n", "authors": ["530"]}
{"title": "Aggressive test power reduction through test stimuli transformation\n", "abstract": " Excessive switching activity during shift cycles in scan-based cores imposes considerable test power challenges. To ensure rapid and reliable test of SOCs, we propose a scan chain modification methodology that transforms the stimuli to be inserted to the scan chain through logic gate insertion between scan cells, reducing scan chain transitions. We introduce a novel matrix band algebra to formulate the impact of scan chain modifications on test stimuli transformations. Based on this analysis, we develop algorithms for transforming a set of test vectors into power-optimal test stimuli through cost-effective scan chain modifications. Experimental results show that scan-in power reductions exceeding 90% for test vectors and 99.5% for test cubes can be attained by the proposed methodology.", "num_citations": "10\n", "authors": ["530"]}
{"title": "Property-based testability analysis for hierarchical rtl designs\n", "abstract": " We present an analysis methodology that identifies testability bottlenecks in RTL designs, based on the concept of transparency properties. We discuss a hierarchical test generation methodology, wherein test is locally generated for each module and subsequently translated into global design applicable test. We introduce the notion of transparency properties for capturing test translation related behavior of modules, without reasoning on the complete functionality of the design. A recursive search algorithm that combines properties into test justification and propagation paths and reveals the reachability bottlenecks for each module in the design is subsequently devised. An ATPG-based experimental setup validates that the proposed methodology identifies accurately the test translation bottlenecks in the hierarchical design.", "num_citations": "10\n", "authors": ["530"]}
{"title": "Channel-based behavioral test synthesis for improved module reachability\n", "abstract": " We introduce a novel behavioral test synthesis methodology that attempts to increase module reachability, driven by powerful global design path analysis. Based on the notion of transparency channels, test justification and propagation bottlenecks are revealed for each module in the design. Subsequently the proposed behavioral test synthesis scheme eliminates during scheduling, allocation and binding, as many reachability bottlenecks, as possible. Furthermore, it identifies the control states and provides the templates required for translating each module's test into global design rest. We demonstrate our scheme on a representative example, unveiling the potential of path analysis based techniques to accurately identify and eliminate module reachability bottlenecks, thus guiding behavioral rest synthesis.", "num_citations": "10\n", "authors": ["530"]}
{"title": "Transient and intermittent fault recovery without rollback\n", "abstract": " Increasing chip density combined with heightened reliability expectations has spawned greater interest in fault tolerant design. In recent years, research into rollback and retry techniques has established them as an effective approach to recovery from transient and intermittent faults. For applications with strict timing requirements, however, the high error latency inherent in retry approaches is unacceptable. We have developed an alternative recovery method with strict error latency boundaries. In addition, the bulky state storage hardware required in rollback designs has been eliminated. The result is a more efficient, more broadly applicable approach to fault tolerant design.", "num_citations": "10\n", "authors": ["530"]}
{"title": "Benchmarking at the frontier of hardware security: Lessons from logic locking\n", "abstract": " Integrated circuits (ICs) are the foundation of all computing systems. They comprise high-value hardware intellectual property (IP) that are at risk of piracy, reverse-engineering, and modifications while making their way through the geographically-distributed IC supply chain. On the frontier of hardware security are various design-for-trust techniques that claim to protect designs from untrusted entities across the design flow. Logic locking is one technique that promises protection from the gamut of threats in IC manufacturing. In this work, we perform a critical review of logic locking techniques in the literature, and expose several shortcomings. Taking inspiration from other cybersecurity competitions, we devise a community-led benchmarking exercise to address the evaluation deficiencies. In reflecting on this process, we shed new light on deficiencies in evaluation of logic locking and reveal important future directions. The lessons learned can guide future endeavors in other areas of hardware security.", "num_citations": "9\n", "authors": ["530"]}
{"title": "Small-delay defects detection under process variation using inter-path correlation\n", "abstract": " Detection of Small Delay Defects (SDDs) is a major concern in modern circuits using nanometer technologies. They are difficult to test and an important source of test escapes, and even when SDDs do not produce functional failures, they represent a reliability risk. The detection of these defects aggravates in the presence of process variations. In this paper, a methodology to detect SDDs in the presence of process variations using delay correlation information between paths of a circuit is proposed. This methodology exploits the concept that for two highly correlated paths, an important part of the delay variance in one path can be described by the delay variance in the second path. The methodology has been further extended to consider multiple path correlation thus improving the detection of SDDs. This methodology is able to distinguish delay defects from process variations. A metric is also proposed to quantify\u00a0\u2026", "num_citations": "9\n", "authors": ["530"]}
{"title": "Application specific low latency instruction cache for NAND flash memory based embedded systems\n", "abstract": " In embedded systems, the demand for high capacity and updatable code storage increases as the amount of code grows. NAND Flash memory is one of the most popular storage solutions for this purpose. However, an extremely long access latency makes the NAND flash memory less attractive in the domain of code storage solutions. In this paper, we introduce a highly effective cache system which utilizes application specific HotSpot information and a HotSpot-aware prefetching technique to effectively hide the long latency of the NAND flash memory based code storage systems. To lessen the negative impact of the extremely long access latency, we propose a page prefetching algorithm utilizing HotSpot information which provides a temporal distance sufficient to load a page from the physical NAND flash memory. The experimental results show that our proposed architecture exhibits up to 96% reduction of the\u00a0\u2026", "num_citations": "9\n", "authors": ["530"]}
{"title": "Light-weight synchronization for inter-processor communication acceleration on embedded MPSoCs\n", "abstract": " The advances in semiconductor technologies have placed MPSoCscenter stage as a standard architecture for embedded applications of ever increasing complexity. Efficient utilization of the ample hardware resources requires applications to be decomposed into fine-grained threads, engendering in turn a large amount of interprocessor communications. While fine-grained on-chip interconnects can reduce the data transfer overhead, the traditional synchronization mechanisms, such as spin locks and barriers, still cause significant contention in polling shared variables. To overcome this issue, in this paper we propose a light-weight distributed synchronization mechanism which statically encodes the semantically correct order of accesses to each shared variable. A sharp reduction in the number of code bits is attained through a reference coloring algorithm, which furthermore enables an implementation within\u00a0\u2026", "num_citations": "9\n", "authors": ["530"]}
{"title": "Core-based testing of multiprocessor system-on-chips utilizing hierarchical functional buses\n", "abstract": " An integrated test scheduling methodology for multiprocessor system-on-chips (SOC) utilizing the functional buses for test data delivery is described. The proposed methodology handles both flat bus single processor SOC and hierarchical bus multiprocessor SOC. It is based on a resource graph manipulation and a packet-based packet set scheduling methodology. The resource graph is decomposed into a set of test configuration graphs, which are then used to determine the optimum test configurations and test delivery schedule under a given power constraint. In order to validate the effectiveness of the proposed methodology, a number of experiments are run on several modified benchmark circuits. The results clearly underscore the advantages of the proposed methodology.", "num_citations": "9\n", "authors": ["530"]}
{"title": "Seamless test of digital components in mixed-signal paths\n", "abstract": " For today's large, mixed-signal designs, test generation requires propagating signals through digital and analog modules. We offer an innovative seamless approach that defines a digital test methodology for digital modules wherein the test inputs and responses can be propagated through a path containing analog signals.", "num_citations": "9\n", "authors": ["530"]}
{"title": "Low-power data memory communication for application-specific embedded processors\n", "abstract": " We propose a novel customization methodology for power reduction on the communication link between an embedded processor and its data memory. We target the address bus and show how by utilizing application information about the memory references in the data intensive program loops, a power efficient address communication protocol can be established between the processor core and the data memory. The data memory controller thus generates the addresses for the various data streams with minimal run-time information from the processor engine, achieving significant power reductions on the address bus. An efficient reprogrammable hard-ware ware support is presented for enabling the proposed methodology. The experimental results demonstrate the efficacy of the approach for a set of data intensive applications.", "num_citations": "9\n", "authors": ["530"]}
{"title": "Efficient transparency extraction and utilization in hierarchical test\n", "abstract": " We introduce a methodology for identifying transparency behavior appropriate for hierarchical test, based on the theoretical principles of transparency composition. Unlike high level approaches that identify limited, coarse transparency behavior, the proposed methodology is capable of extracting a wide class of fine grained transparency functions for arbitrary sub-word bit clusters. The functions in this class can furthermore be rapidly extracted on the fly and efficiently utilized for hierarchical test translation, thus alleviating the exponential extraction time and storage space requirements of exhaustive approaches. The twin benefits of rapid, automated extraction coupled with the expansion of utilizable transparency scope deliver reduced DFT while enabling cost-effective hierarchical test of high quality.", "num_citations": "9\n", "authors": ["530"]}
{"title": "Test selection based on high level fault simulation for mixed-signal systems\n", "abstract": " Mixed-signal design and test tools are failing to keep pace with the increasing necessity for design exploration in the early stages. We outline a methodology and toolset to enable test selection at the early design stages by providing a high level fault simulator and associated block-level modeling and traversal capabilities. Experimental results show that the outlined methodology provides superior fault simulation speed-ups while helping to minimize the test time for a mixed-signal receiver system.", "num_citations": "9\n", "authors": ["530"]}
{"title": "Efficient self-recovering ASIC design\n", "abstract": " The authors present a framework for tailoring fault-tolerant approaches for both permanent and transient faults to the specific needs of an application. These methodologies provide an efficient alternative to traditional triplication and rollback schemes and allow tailoring of area-resiliency trade-offs for individual designs.", "num_citations": "9\n", "authors": ["530"]}
{"title": "Standard seven segmented display for Burmese numerals\n", "abstract": " Lau and Lwin (see ibid., vol.36, p.84-8, May 1990) proposed an eight-segment Burmese numeric display. In the present work, a small modification that uses only the commercially available seven-segment-based display is proposed. The advantage of this design is that the standard seven-segment display eliminates the need for the production of custom eight-segment displays.< >", "num_citations": "9\n", "authors": ["530"]}
{"title": "On diagnosis of timing failures in scan architecture\n", "abstract": " Excessive test mode power-ground noise in nanometer scale chips causes large delay uncertainties in scan chains, resulting in a highly elevated rate of timing failures. The hybrid timing violation types in scan chains, compounded by their possibly intermittent manifestations, invalidate the conventional assumptions in scan chain fault behavior, significantly increasing the ambiguity and difficulty in diagnosis. In this paper, we propose a novel methodology to identify the root cause of scan chain timing failures. The proposed work addresses the challenge of diagnosing multiple permanent or intermittent timing faults in scan chains and the associated clock trees, which closely approximate the realistic failure mechanisms observed in silicon. Instead of relying on fault simulation that is incapable of approximating the intermittent fault manifestation, the proposed technique characterizes the impact of timing faults by\u00a0\u2026", "num_citations": "8\n", "authors": ["530"]}
{"title": "Scan power reduction for linear test compression schemes through seed selection\n", "abstract": " XOR network-based on-chip test compression schemes have been widely employed in large industrial scan designs due to their high compression ratio and efficient decompression mechanism. Nevertheless, such a scheme necessitates high unspecified bit ratios in the original test cubes, resulting in quite significant difficulties in preprocessing test cubes for scan power reduction. The linear mapping from the original cubes to the compressed seeds typically provides extra degrees of flexibility as multiple seeds may reconstruct the test cube. Due to the highly divergent power impact of distinct seeds though, appreciable power reductions in the decompressed test data can be attained through the pinpointing of the power-optimal seeds during the compression phase. This work explores the aforementioned flexibility in the seed space, and outlines a mathematical and algorithmic framework for a power-aware linear\u00a0\u2026", "num_citations": "8\n", "authors": ["530"]}
{"title": "Performance and energy efficient cache migrationapproach for thermal management in embedded systems\n", "abstract": " In this paper we propose an approach for performance and power aware warm start for the data cache during core level migration events that originate from overheating. We utilize the concept of reuse in the references to eliminate unnecessary information from being migrated. Furthermore, we exploit the temperature predictability to trigger the cache migration slightly before the actual thermal limit to allow sufficient time for the extraction of the reuse information and transfer of it to the destination cache while the execution is resuming normally. The suggested hardware not only is cost efficient but is also programmable so as to maintain flexibility in targeting application particularities. The experimental results we provide confirm the applicability of this approach.", "num_citations": "8\n", "authors": ["530"]}
{"title": "Transforming binary code for low-power embedded processors\n", "abstract": " Two program code transformation methodologies reduce the power consumption of instruction communication buses in embedded processors. Aimed at deep-submicron process technologies, these techniques offer an efficient solution for applications in which low power consumption is the key quality factor. We have developed two techniques for power minimization on the instruction bus of embedded processors. The first is compiler-driven register name adjustment (RNA), with the main goal of power minimization on instruction fetch and register file access. The second technique, more general in nature, incorporates transformations into the binary program code and necessitates hardware support on the processor side to efficiently restore the power-optimized program code.", "num_citations": "8\n", "authors": ["530"]}
{"title": "Boosting the accuracy of analog test coverage computation through statistical tolerance analysis\n", "abstract": " Increasing numbers of analog components in today's systems necessitate system level test composition methods that utilize onchip capabilities rather than solely relying on costly DFT approaches. We outline a tolerance analysis methodology for test signal propagation to be utilized in hierarchical test generation for analog circuits. A detailed justification of this proposed novel tolerance analysis methodology is undertaken by comparing our results with detailed SPICE Monte-Carlo simulation data on several combinations of analog modules. The results of our experiments confirm the high accuracy and efficiency of the proposed tolerance analysis methodology.", "num_citations": "8\n", "authors": ["530"]}
{"title": "Power efficient embedded processor IPs through application-specific tag compression in data caches\n", "abstract": " In this paper, we present a methodology for power minimization by data cache tag compression. The set of tags being accessed by the major application loops is analyzed statically during compile time and an efficient and optimal compression scheme is proposed Only a very limited number of tag bits are stored in the tag array for cache conflict identification, thus achieving a significant reduction in the number of active bitlines, sense amps, and comparator cells. The underlying hardware support for dynamically compressing the tags consists of a highly cost and power efficient programmable encoder which lies outside the cache access path, thus not affecting the processor cycle time. A detailed VLSI implementation has been performed and a number of experimental results on a set of embedded applications and numerical kernels is reported Energy dissipation decreases of up to 95% can be observed for the tag\u00a0\u2026", "num_citations": "8\n", "authors": ["530"]}
{"title": "Fast hierarchical test path construction for circuits with DFT-free controller-datapath interface\n", "abstract": " Hierarchical approaches address the complexity of test generation through symbolic reachability paths that provide access to the I/Os of each module in a design. However, while transparency behavior suitable for symbolic design traversal can be utilized for constructing reachability paths for datapath modules, control modules do not exhibit transparency. Therefore, incorporating such modules in reachability path construction requires exhaustive search algorithms or expensive DFT hardware. In this paper, we discuss a fast hierarchical test path construction method for circuits with DFT-free controller-datapath interface. A transparency-based RT-Level hierarchical test generation scheme is devised for the datapath, wherein locally generated vectors are translated into global design test. Additionally, the controller is examined through the introduced concept of influence tables, which are used to generate\u00a0\u2026", "num_citations": "8\n", "authors": ["530"]}
{"title": "Redundancy and testability in digital filter datapaths\n", "abstract": " Test issues in application-specific digital filter datapaths are investigated. It is found that such designs can contain hundreds of redundant faults, making it difficult to accurately determine fault coverage. Since these redundant faults tend to appear in the same general location as test-resistant faults, the presence of many redundant faults can hide significant untested faults despite high overall test coverage. Classes of redundant faults that arise in digital filter datapaths are described, and we propose a suite of techniques for identifying and eliminating the most common redundancies based on arithmetic optimization. The approach is suitable as a front-end to more accurate fault simulation, or can be used in the design process to eliminate redundant logic. The approach is validated as a tool for developing very high-coverage built-in self-test circuits, showing that 100% fault coverage can be achieved in 24k-gate filters\u00a0\u2026", "num_citations": "8\n", "authors": ["530"]}
{"title": "Area-EfficientFault Detection During Self-Recovering Microarchitecture Synthesis\n", "abstract": " We will present the area-efficient fault-detection synthesis component of SYNCERE, an integrated system for synthesizing area-efficient self-recovering microarchitectures. In the SYNCERE model for self-recovery, transient fault detection is based on duplication and comparison, while recovery from transient faults is accomplished via checkpointing and rollback. SYNCERE minimizes the overhead of duplication using two complementary area-optimization techniques. Whereas imposing inter-copy hardware disjointness at a sub-computation level instead of at the overall computation level ameliorates the dedicated hardware required for the original and duplicate computations, restructuring the pliable input representation of the duplicate computation further moderates the overall hardware.", "num_citations": "8\n", "authors": ["530"]}
{"title": "Fine-grained concurrency in test scheduling for partial-intrusion BIST\n", "abstract": " Partial-intrusion BIST reduces area overhead and increases chip performance by reducing the number of test registers, but it requires a test schedule definition. The scheduling of the tests impacts directly the test application time. This paper presents a novel model of the test plan scheduling problem for partial-intrusion BIST circuits. Test application time is reduced by performing scheduling to allow the execution of test plans in a pipelined fashion. Test scheduling conflicts are avoided by exploiting the parallelism which is revealed by a flexible test representation; consequently, test concurrency is increased. Computational efficiency is gained by performing incremental, subtractive heuristic test scheduling decisions. The effects of each test decision are rigorously propagated, limiting the test scheduling possibilities to only those which lead to a feasible scheduling solution. Experimental results show that high levels of\u00a0\u2026", "num_citations": "8\n", "authors": ["530"]}
{"title": "Detecting gas vapor leaks using uncalibrated sensors\n", "abstract": " Chemical and infra-red sensors generate distinct responses under similar conditions because of sensor drift, noise or resolution errors. In this paper, we develop novel machine learning methods for detecting and identifying VOC and Ammonia vapor from time-series data obtained by uncalibrated chemical and infrared sensors. We process time-series sensor signals using deep neural networks (DNN). Three neural network algorithms are utilized for this purpose. Additive neural networks (termed AddNet) are based on a multiplication-devoid operator and consequently exhibit energy efficiency compared to regular neural networks. The second algorithm uses generative adversarial neural networks so as to expose the classifying neural network to more realistic data points in order to help the classifier network to deliver improved generalization. Finally, we use conventional convolutional neural networks as a\u00a0\u2026", "num_citations": "7\n", "authors": ["530"]}
{"title": "Aggressive test cost reductions through continuous test effectiveness assessment\n", "abstract": " The inclusion of various new test types in production test suites with the hope of keeping defect escape level in check continuously increases the test cost while the economics of the intensely competitive consumer marketplace dictates test strategies that are low cost yet still effective in defect detection. An adaptive test methodology is proposed in this paper to address the simultaneous demand for efficiency and effectiveness in testing through an adaptive prioritization of test vectors according to defect detection effectiveness, subsequently, leading to the identification of a compact yet effective test set. Experimental results confirm the effectiveness of the vector prioritization process and show substantial cost reduction levels.", "num_citations": "7\n", "authors": ["530"]}
{"title": "Register allocation for embedded systems to simultaneously reduce energy and temperature on registers\n", "abstract": " Energy and thermal issues are two important concerns for embedded system design. Diminished energy dissipation leads to a longer battery life, while reduced temperature hotspots decelerate the physical failure mechanisms. The instruction fetch logic associated with register access has a significant contribution towards the total energy consumption. Meanwhile, the register file has also been previously shown to exhibit the highest temperature compared to the rest of the components in an embedded processor. Therefore, the optimization of energy and the resolution of the thermal issue for register accesses are of great significance. In this article, register allocation techniques are studied to simultaneously reduce energy consumption and heat buildup on register accesses for embedded systems. Contrary to prevailing intuition, we observe that optimizing energy and optimizing temperature on register accesses\u00a0\u2026", "num_citations": "7\n", "authors": ["530"]}
{"title": "Profit maximization through process variation aware high level synthesis with speed binning\n", "abstract": " As integrated circuits continuously scale up, process variation plays an increasingly significant role in system design and semiconductor economic return. In this paper, we explore the potential of profit improvement under the inherent semiconductor variability based on the speed binning technique. We first accordingly propose a set of high level synthesis techniques, including allocation, scheduling and resource binding, thus essentially constructing designs that maximize the number of chips that can be sold at the most advantageous price, leading to the maximization of the overall profit. We explore subsequently the optimal bin placement strategy for further profit improvement. Experimental results confirm the superiority of the high level synthesis results and the associated improvement in profit margins.", "num_citations": "7\n", "authors": ["530"]}
{"title": "Design automation for hybrid CMOS-nonoelectronics crossbars\n", "abstract": " We developed the first automatic design system targeting a promising hybrid CMOS-nanoelectronics architecture called CMOL. |5|. The CMOL architecture uses NOR gates to implement combinational logic. In this hybrid CMOS-nanoelectronics architecture, logical functions and the interconnections share the nanoelectronics hardware resource. Towards automating the CMOL physical design process, we developed a model for the CMOL architecture, formulated the placement and routing problems for the CMOL architecture subject to the unique CMOL. specific constraints, and solved it by combining a placement algorithm with a gate assignment algorithm in a loop. We validated the proposed approach by implementing several industrial strength designs.", "num_citations": "7\n", "authors": ["530"]}
{"title": "Forward discrete probability propagation method for device performance characterization under process variations\n", "abstract": " Process variations are becoming influential at the device level in deep sub-micron and sub-wavelength design regimes, whereas they used to be a few generations away only influential at circuit level. Process variations cause device performance parameters, such as current or output resistance, to acquire a probability distribution. Estimation of these distributions has been accomplished using Monte Carlo techniques so far. The large number of samples needed by Monte Carlo methods adversely affects the possibility of integrating probabilistic device performance at the circuit level due to run-time inefficiency. In this paper, we introduce a novel technique called Forward Discrete Probability Propagation (FDPP). This method discretizes the probability distributions and effectively propagates these probabilities across a device formula hierarchy, such as the one present in the SPICE3v3 model. Consequently\u00a0\u2026", "num_citations": "7\n", "authors": ["530"]}
{"title": "Test data manipulation techniques for energy-frugal, rapid scan test\n", "abstract": " Scan-based testing methodologies remedy the testability problem of sequential circuits; yet they suffer from prolonged test time and excessive test power due to numerous shift operations. The significant correlation among test stimuli along with the high density of unspecified bits in test data enables the utilization of the existing test stimulus in the scan chain as the seed for the generation of the subsequent test stimulus, thus reducing both test time and test data volume. The proposed scan-based test scheme accesses only a subset of scan cells for loading the subsequent test stimulus while freezing the remaining scan cells with the preceding test stimulus, thus decreasing scan chain transitions during shift operations. The proposed scan architecture is coupled with test data manipulation techniques which include test stimuli ordering and partitioning algorithms, boosting test time reductions. The experimental results\u00a0\u2026", "num_citations": "7\n", "authors": ["530"]}
{"title": "Pseudorandom-pattern test resistance in high-performance DSP datapaths\n", "abstract": " The testability of basic DSP datapath structures using pseudorandom built-in self-test techniques is examined. The addition of variance mismatched signals is identified as a testing problem, and the associated fault detection probabilities are derived in terms of signal probability distributions. A method of calculating these distributions is described, and it is shown how these distributions can be used to predict testing problems that arise from the correlation properties of test sequences generated using linear-feedback shift registers. Finally, it is shown empirically that variance matching using associativity transformations can reduce the number of untested faults by a factor of eight over variance mismatched designs.", "num_citations": "7\n", "authors": ["530"]}
{"title": "Concurrent monitoring of operational health in neural networks through balanced output partitions\n", "abstract": " The abundant usage of deep neural networks in safety-critical domains such as autonomous driving raises concerns regarding the impact of hardware-level faults on deep neural network computations. As a failure can prove to be disastrous, low-cost safety mechanisms are needed to check the integrity of the deep neural network computations. We embed safety checksums into deep neural networks by introducing a custom regularization term in the network training. We partition the outputs of each network layer into two groups and guide the network to balance the summation of these groups through an additional penalty term in the cost function. The proposed approach delivers twin benefits. While the embedded checksums deliver low-cost detection of computation errors upon violations of the trained equilibrium during network inference, the regularization term enables the network to generalize better during\u00a0\u2026", "num_citations": "6\n", "authors": ["530"]}
{"title": "Diagnosing scan chain timing faults through statistical feature analysis of scan images\n", "abstract": " Excessive test mode power-ground noise in nanometer scale chips causes large delay uncertainties in scan chains, resulting in a highly elevated rate of timing failures. The hybrid timing violation types in scan chains, plus their possibly intermittent manifestations, invalidate the traditional assumptions in scan chain fault behavior, significantly increasing the ambiguity and difficulty in diagnosis. In this paper, we propose a novel methodology to resolve the challenge of diagnosing multiple permanent or intermittent timing faults in scan chains. Instead of relying on fault simulation that is incapable of approximating the intermittent fault manifestation, the proposed technique characterizes the impact of timing faults by analyzing the phase movement of scan patterns. Extracting fault-sensitive statistical features of phase movement information provides strong signals for the precise identification of fault locations and types\u00a0\u2026", "num_citations": "6\n", "authors": ["530"]}
{"title": "Adaptive test optimization through real time learning of test effectiveness\n", "abstract": " Production test suites include a large number of redundant test patterns due to the inclusion of multiple test types with overlapping defect detection and the use of simple fault models for test generation. Identification and elimination of ineffective test patterns promises a significant reduction in test cost. This paper proposes a test framework that learns, without extensive data collection and at no additional test time, the effectiveness of individual test patterns during production testing by getting defect detection feedback from a dynamic test flow. The proposed technique is further capable of adapting to changes in the underlying defect mechanisms by tracking the defect detection trend of test patterns.", "num_citations": "6\n", "authors": ["530"]}
{"title": "Dynamic, non-linear cache architecture for power-sensitive mobile processors\n", "abstract": " Today, mobile smartphones are expected to be able to run the same complex, algorithm-heavy, memory-intensive applications that were originally designed and coded for general-purpose processors. All the while, it is also expected that these mobile processors be power-conscientious as well as of minimal area impact. These devices pose unique usage demands of ultra-portability, but also demand an always-on, continuous data access paradigm. As a result, this dichotomy of continuous execution versus long battery life poses a difficult challenge. This paper explores a novel approach to mitigating mobile processor power consumption, with a non-linear degradation in execution speed. The concept relies on using dynamic application memory behavior to intelligently target adjustments in the cache to significantly reduce overall processor power, taking into account both the dynamic and leakage power footprint\u00a0\u2026", "num_citations": "6\n", "authors": ["530"]}
{"title": "Delay test quality maximization through process-aware selection of test set size\n", "abstract": " The quality of a delay test set hinges not only on test patterns and the distribution of the delay defects but on the variations in process parameters as well. Process variations result in the same delay test set displaying differences from die to die in the detection of particular delay defects at the identical circuit node. The application of an identical test set to all devices independent of process variations consequently results in delivering inefficiencies in test time utilization. This paper proposes a delay test technique that adaptively changes the size of the test set based on the position of the device in the process variation space in order to maximize test quality within a given test time.", "num_citations": "6\n", "authors": ["530"]}
{"title": "A reprogrammable customization framework for efficient branch resolution in embedded processors\n", "abstract": " We present a customization framework for embedded processors which employs the utilization of application-specific information, thus specializing the processor's microarchitecture to the application needs. The increased processor utilization leads to a low-cost system implementation with no sacrifice in performance requirements and to reduced custom hardware in a typical SOC. We illustrate these ideas through the branch resolution problem, known to impose severe performance degradation on control-dominated embedded applications. A customization approach for early branch resolution and subsequent folding is presented. The application-specific information is captured by the microarchitecture through a low-cost reprogrammable hardware, thus attaining the twin benefits of processor standardization and application-specific customization. Experimental results show that for a representative set of control\u00a0\u2026", "num_citations": "6\n", "authors": ["530"]}
{"title": "Design of concurrent test hardware for linear analog circuits with constrained hardware overhead\n", "abstract": " Concurrent detection of failures in analog circuits is becoming increasingly more important as safety-critical systems become more widespread. A methodology for automatic design of concurrent failure detection circuitry for linear analog systems is discussed in this paper. The desired hardware bound is specified as a constraint; the methodology aims at providing coverage in terms of all the circuit components while minimizing the loading overhead by reducing the number of internal circuit nodes that need to be tapped. Parameter tolerances are incorporated through either statistical or mathematical analysis to determine the threshold for failure alarm.", "num_citations": "6\n", "authors": ["530"]}
{"title": "Frugal linear network-based test decompression for drastic test cost reduction\n", "abstract": " CiNii \u8ad6\u6587 - Frugal linear network-based test decompression for drastic test cost reduction CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e \u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 [\u6a5f\u95a2\u8a8d\u8a3c] \u5229\u7528\u7d99\u7d9a\u624b\u7d9a\u304d\u306e\u3054\u6848\u5185 Frugal linear network-based test decompression for drastic test cost reduction ORAILOGLU A. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 ORAILOGLU A. \u53ce\u9332\u520a\u884c\u7269 Proc. International Conference on Computer-Aided Design, Nov. 2004 Proc. International Conference on Computer-Aided Design, Nov. 2004, 721-725, 2004 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 On Reducing Test Power, Volume and Routing Cost by Chain Reordering and Test Compression Techniques LIN Chia-Yi , HSU Li-Chung , CHEN Hung-Ming IEICE \u2026", "num_citations": "6\n", "authors": ["530"]}
{"title": "Automated system-level test development for mixed-signal circuits\n", "abstract": " While in the digital domain, test development is primarily conducted with the use of automated tools, knowledge-based, ad hoc test methods have been in use in the analog domain. High levels of design integration and increasing complexity of analog blocks within a system necessitate automated system-level analog test development tools. We outline a methodology for specification-based automated test generation and fault simulation for analog circuits. Test generation is targeted at providing the highest coverage for each specified parameter. The flexibility of assigning analog test attributes is utilized for merging tests leading to test time reduction with no loss in test coverage. Further optimization in test time is obtained through fault simulations by selecting tests that provide adequate coverage in terms of several components and dropping the ones that do not provide additional coverage. A system-level\u00a0\u2026", "num_citations": "6\n", "authors": ["530"]}
{"title": "Graceful degradation in synthesis of VLSI ICs\n", "abstract": " Increasing levels of societal reliance on computerized solutions demand fault-resilient solutions. At the same time, system-on-a-chip levels of integration, demand a reexamination and migration of traditional system level fault resilience techniques to the integrated circuit level. Automated synthesis methodologies need to provide embedded, low-cost fault resilience properties, capable of ensuring fault resilience for all on-chip components and interconnects. The outlined approaches in this paper pioneer the insertion of unabridged fault resilience properties at the IC level through highly automated approaches. The experimental results show cost-effective solutions, with no performance degradation, in the synthesized ASICs.", "num_citations": "6\n", "authors": ["530"]}
{"title": "Integrating binding constraints in the synthesis of area-efficient self-recovering microarchitectures\n", "abstract": " Fault-tolerance increases hardware reliability of a VLSI design but it also has the disadvantage of increasing chip area. This area overhead can however be minimized by consideration of fault-tolerance during the high-level synthesis stage of design. We introduce an intertwined scheduling and binding algorithm for self-recovering fault-tolerant designs. Two copies in a self-recovering design have to be performed by disjoint sets of hardware to maintain fault-tolerance under a single fault assumption. The proposed algorithm is the first that satisfies the hardware disjointness condition which is necessary for fault-tolerant design. Area efficient self-recovering microarchitectures are achieved through novel binding techniques which influence scheduling decisions. Various synthesis benchmarks are used to illustrate the effectiveness of this approach.< >", "num_citations": "6\n", "authors": ["530"]}
{"title": "Rapid prototyping of fault tolerant VLSI systems\n", "abstract": " We relate fault-tolerance constraints to chip area and present a methodology for rapidly compiling an algorithmic description into area-efficient fault-tolerant VLSI ICs. Whereas detection and recovery from environment induced transient faults is accomplished by checkpointing and rollback, uninterrupted operation for the lifetime of a mission is ensured by injecting redundancy. Towards validating this methodology, we synthesized fault tolerant implementations of a 16-point FIR filter starting from an algorithmic description. These fault-tolerant designs were then critically appraised.< >", "num_citations": "6\n", "authors": ["530"]}
{"title": "ALPS: an algorithm for pipeline data path synthesis\n", "abstract": " While techniques for design of high performance computing systems have been well understood, software mechanisms for the automatic design of high performance application specific integrated circuits(ASICS) remain relatively u nexplored. Advances in levels of integration will make it feasi-ble to support performance-enhancing structures on a single chip. With the increasing demand for high performance in real-time signal processing applications, the design of high speed ASICS merits immediate attention. In this paper, we develop software mechanisms for the high-level synthesis of high-performance VLSI systems. We have extended our interactive behavioral synthesis framework that provides scheduling with multiple constraints including performance and cost, to support scheduling for high-performance. The system is powerful enough to allow trade-offs along mnltiple dimensions. The software mechanisms\u00a0\u2026", "num_citations": "6\n", "authors": ["530"]}
{"title": "Just say zero: containing critical bit-error propagation in deep neural networks with anomalous feature suppression\n", "abstract": " DNNs are abundantly employed in a variety of applications, including real-time systems with strict safety constraints. The consequences of errors prove disastrous in safety-critical systems, such as autonomous driving, healthcare, and industrial applications. DNNs are resilient to limited numerical perturbations yet fragile under large deviations in weights and activations. The traditional error tolerance measures fail to meet the tight design constraints of DNN processing systems due to extensive overheads or limited advantages in abundant error conditions. The algorithmic particularities of DNNs though create novel opportunities to deal with errors more effectively and economically. We revisit the two fundamental tasks in fault-tolerant system design, namely, error detection and correction, and demonstrate that the precise versions of these operations could be replaced by approximated counterparts in DNNs to deliver\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "Detecting gas vapor leaks through uncalibrated sensor based CPS\n", "abstract": " While Volatile Organic Compounds (VOC) and ammonia have a place in our daily lives, their leakage into the environment is harmful to human health. In order to prevent and detect gaseous leaks of harmful VOCs, a cyber-physical system (CPS) comprised of ordinary people or first responders is proposed. This CPS uses small, low-cost sensors coupled to smart phones or mobile devices with the necessary computation and communication capabilities. The efficacy of such a CPS hinges on its ability to address technical challenges stemming from the fact that identically produced sensors may produce different results under the same conditions due to sensor drift, noise, or resolution errors. The proposed system makes use of time-varying signals produced by sensors to detect gas leaks. Sensors sample the gas vapor level in a continuous manner and time-varying sensor data is processed using deep neural\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "Application-aware adaptive cache architecture for power-sensitive mobile processors\n", "abstract": " Today, mobile smartphones are expected to be able to run the same complex, algorithm-heavy, memory-intensive applications that were originally designed and coded for general-purpose processors. All the while, it is also expected that these mobile processors be power-conscientious as well as of minimal area impact. These devices pose unique usage demands of ultra-portability but also demand an always-on, continuous data access paradigm. As a result, this dichotomy of continuous execution versus long battery life poses a difficult challenge. This article explores a novel approach to mitigating mobile processor power consumption while abating any significant degradation in execution speed. The concept relies on efficiently leveraging both compile-time and runtime application memory behavior to intelligently target adjustments in the cache to significantly reduce overall processor power, taking into account\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "Tracing the best test mix through multi-variate quality tracking\n", "abstract": " The increasing multiplicity of defect types forces the inclusion of tests from a variety of fault models. The quest for test quality is checkmated though by the considerable and frequently unnecessary cost of the large number of tests, driven by the lack of a clear correspondence between defects and fault models. While the static derivation of the appropriate test mixes from a variety of fault models to deliver high test quality at low cost is a desirable goal, it is challenged by the frequent changes in defect characteristics. The consequent necessity for adaptivity is addressed in this paper through a test framework that utilizes the continuous stream of failing test data during production testing to track the varying test quality based on evolving defect characteristics and thus dynamically adjust the production test set to deliver a target defect escape level at minimal test cost.", "num_citations": "5\n", "authors": ["530"]}
{"title": "DiSC: A new diagnosis method for multiple scan chain failures\n", "abstract": " In scan-based testing environments, identifying the scan chain failures can be of significant help in guiding the failure analysis process for yield improvement. In this paper, we propose an efficient scan chain diagnosis method using a symbolic fault simulation to achieve high diagnostic resolution and small candidate list for single and multiple defects in scan chains. The main ideas of the proposed scan chain diagnosis method are twofold: 1) the reduction of the candidate scan cells through the analysis of the symbolic simulation responses, and 2) the identification of final candidate scan cells using the backward tracing method with the symbolic simulation responses. Experimental results show the effectiveness.", "num_citations": "5\n", "authors": ["530"]}
{"title": "Fully adaptive multicore architectures through statically-directed dynamic execution reconfigurations\n", "abstract": " As a result of ever growing integration density and application complexity, future multicore architectures will suffer from increased levels of core availability variations. Full resource utilization in the face of various levels of resource availability necessitates techniques that compactly engender numerous schedules in readiness at compile time. Such schedules, each of which can make maximum utilization of the available resources, can be adaptively applied at runtime, thus enabling a toleration of up to an arbitrarily large amount of resource variations. Core binding permutations furthermore minimize the performance impact imposed by adaptivity on the pre-reconfiguration schedules while retaining all the concomitant benefits. The efficacy of the proposed technique is confirmed by incorporating it into a conventional, widely adopted scheduling heuristic and experimentally verifying it in the context of multiple core\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "Reducing impact of cache miss stalls in embedded systems by extracting guaranteed independent instructions\n", "abstract": " Today, embedded processors are expected to be able to run algorithmically complex, memory-intensive applications that were originally designed and coded for general-purpose processors. As such, the impact of memory latencies on the execution time increasingly becomes evident. All the while, it is also expected that embedded processors be power-conscientious as well as of minimal area impact, as they are often used in mobile devices such as wireless smartphones and portable MP3 players. As a result, traditional methods for addressing performance and memory latencies, such as multiple issue, out-of-order execution and large, associative caches, are not aptly suited for the mobile embedded domain due to the significant area and power overhead. This paper explores a novel approach to mitigating execution delays caused by memory latencies that would otherwise not be possible in a regular in\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "A DFT approach for diagnosis and process variation-aware structural test of thermometer coded current steering DACs\n", "abstract": " A design for test (DFT) hardware is proposed to increase the controllability of a thermometer coded current steering digital to analog converter. A procedure is introduced to reduce the diagnosis and structural test time from quadratic to linear using the proposed DFT hardware. To evaluate the applicability of the proposed technique, principal component analysis is used to create virtual process variations to simulate in lieu of semiconductor fabrication data. An architecture specific soft fault model is suggested for the diagnosis problem. Random errors according to the fault model are introduced in the virtual test environment on top of the process variations and it is shown that diagnosis of a fault is possible with high accuracy with the proposed method. The same technique employing principal component analysis is furthermore used to provide process variation-aware reference test comparison values for a structural\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "Extending the applicability of parallel-serial scan designs\n", "abstract": " Although scan-based designs are widely used in order to reduce the complexity of test generation, test application time and test data volume are substantially increased. We propose two different methodologies for test cost reduction in scan-based designs. The first methodology improves on the Illinois scan architecture, aiming at reducing the high test cost of the test vectors that necessitate the serial test application mode. The second methodology employs on-chip serial transformations to generate an input stimulus that can be applied efficiently. The transformation-based methodology utilizes the proposed scan design to obtain the minimal cost input stimulus. The experimental results indicate that a substantial test cost reduction, reaching 90% levels, can be obtained.", "num_citations": "5\n", "authors": ["530"]}
{"title": "On mismatch in the deep sub-micron era-from physics to circuits\n", "abstract": " The rapid decrease in feature sizes has increasingly accentuated the importance of matching between transistors. Deep submicron designs will further emphasize the need to focus on the effects of mismatch. Furthermore, increased efforts on high level analog device modeling will necessitate accompanying mismatch simulation and measurement methods. The deep submicron era forces circuit designers to learn more about the physics and the technology of transistors. We introduce a method and assist circuit designers in including this method in their traditional design flow of circuits. By proposing a solution to the problem of building a modeling bridge between transistor mismatch and circuit response to it, we hope to enable designers to incorporate low level mismatch information in their higher level design.", "num_citations": "5\n", "authors": ["530"]}
{"title": "Test requirement analysis for low cost hierarchical test path construction\n", "abstract": " We propose a methodology that examines design modules and identifies appropriate vector justification and response propagation requirements for reducing the cost of hierarchical test path construction. Test requirements are defined as a set of fine-grained input and output bit clusters and pertinent symbolic values. They are independent of actual test sets and are adjusted to the inherent module connectivity and regularity. As a result, they combine the generality required for fast hierarchical test path construction with the precision necessary for minimizing the incurred cost, thus fostering cost-effective hierarchical test.", "num_citations": "5\n", "authors": ["530"]}
{"title": "Transparency-based hierarchical test generation for modular RTL designs\n", "abstract": " We discuss a novel hierarchical test generation methodology for RTL designs, based on the concept of modular transparency. We introduce the channel notion, a powerful mechanism that captures modular transparency in terms of bijection functions defined on variable bitwidth signal entities. Through a recursive search algorithm, transparency channels are further combined into reachability paths suitable for translating local test vectors for each module into global design test. A divide and conquer hierarchical test generation methodology is described, resulting in significant test generation time speed-up and comparable fault coverage and vector count to complete circuit gate-level ATPG.", "num_citations": "5\n", "authors": ["530"]}
{"title": "Block-based test integration for analog integrated circuits\n", "abstract": " We outline a methodology for system level test composition out of basic block level tests in the context of large analog IC\u2019s. The method can be utilized as soon as high level specifications are available providing avenues for testability insertion. Computational effectiveness is achieved by an early analysis to identify feasible test paths and test translation method. Parameter tolerances are incorporated into test translation through a probabilistic approach. Experimental results show that high fault and yield coverages for most tests can be attained with no hardware alterations.", "num_citations": "5\n", "authors": ["530"]}
{"title": "Low-cost test for large analog IC's\n", "abstract": " This paper outlines a basic block level test translation tool for analog systems. Test translation aims at minimizing DFT overhead in a hierarchical test translation scheme to meet the ever increasing integration, performance and test re-use requirements. The concept of analog signal propagation and necessary signal attributes are introduced to achieve effective and accurate test translation. A pre-analysis of the system to identify feasible paths and utilization of behavioral basic block models provide computational effectiveness. Experimental results show that test translation reduces DFT overhead significantly while satisfying coverage requirements.", "num_citations": "5\n", "authors": ["530"]}
{"title": "Variance mismatch: Identifying random-test resistance in DSP datapaths\n", "abstract": " Pseudorandom built-in self-test (BIST) is an attractive means of testing DSP datapath structures as long as the delay and area overhead can be kept low. One obstacle to low-overhead BIST is random-pattern test-resistant datapath structures. We examine the causes of this resistance in some typical DSP datapaths, and introduce variance mismatch as an analytical tool for identifying these test problems. By understanding the mechanisms behind random-pattern test resistance, it is possible to make design decisions that are compatible with pseudorandom BIST, resulting in reduced test length and higher fault coverage. Variance matching theory is applied to the design of two large FIR filters, resulting in an 88% reduction in the number of missed faults for a fixed test length, and two orders-of-magnitude reduction in test length for a specific fault coverage target.", "num_citations": "5\n", "authors": ["530"]}
{"title": "\ud835\udc9f\u2130\u2131ect \ud835\udcafolerant layout synthesis\n", "abstract": " A mismatch between the decrease in feet-densities and the increase in senstivity to defects of integrated circuits (lCs) is contributing towards deteriorating chip yields, in even the most advanced of IC fabrication lines. This unprofitable trend in IC yields can be arrested and even reversed by using two main techniques. The defect densities of an IC fabrication line can be minimized by tuning the process parameters. However, a zero-defect manufacturing line is a myth. Along an orthogonal dimension, the sensitivity of circuit structures to defects can be minimized by making them defect-tolerant. In this paper we present \ud835\udc9f\u2130\u2131\ud835\udcaf, a system for synthesizing such defect-tolerant layouts. \ud835\udc9f\u2130\u2131\ud835\udcaf ingrains tolerance to fabrication induced defects by reducing the defect-sensitive areas in a layout and this is accomplished by dispersing nets with large overlaps into non-adjacent tracks. Furthermore, \ud835\udc9f\u2130\u2131\ud835\udcaf affords trade-offs between\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "Synthesis of fault-tolerant and real-time microarchitectures\n", "abstract": " Microarchitectural implementations of real-time signal-processing algorithms for mission-critical applications (such as sensors on spacecrafts) are characterized by two unique requirements: real-time processing of a steady stream of input signals, which requires a high-performance implementation such as pipelining, and reliable operation over the mission lifetime, which mandates support for fault tolerance. We relate high-performance and fault-tolerance constraints to chip area and present a methodology for synthesizing area-efficient microarchitectures satisfying these requirements. High performance is achieved via pipelining, whereas desired fault tolerance is realized using hardware redundancy. The framework has been used to synthesize high-performance and fault-tolerant microarchitectures for a variety of signal-processing algorithms. Additionally, the framework has been used to explore design trade-offs\u00a0\u2026", "num_citations": "5\n", "authors": ["530"]}
{"title": "Examining Timing Path Robustness Under Wide-Bandwidth Power Supply Noise Through Multi-Functional-Cycle Delay Test\n", "abstract": " Circuits designed and fabricated with nanometer-scale technology are increasingly sensitive to power ground noise across a wide frequency range, thus necessitating a strict examination of circuit robustness against noise during manufacturing tests. Conventional at-speed testing techniques possibly result in the escape of marginal timing failures, as they are unable to account for the impact of middle- and low-frequency noise on circuit timing. To address this challenge, we propose, in this paper, a novel multi-functional-cycle test scheme that targets the noise-induced failures on critical paths of the circuit. The proposed technique explores the noise profile of at-speed functional cycles and approximates it in delay testing through the application of multiple capture operations, thus maximally detecting the timing failures that potentially take place under the worst case functional mode noise. The noise impact of\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Full exploitation of process variation space for continuous delivery of optimal delay test quality\n", "abstract": " The increasing magnitude of process variations individualizes effectively each chip, necessitating distinct quantities of test resources for each in order to optimize overall delay test quality without exceeding set test budgets. This paper proposes an analytical framework that delivers the optimal test time assignment per chip in order to minimize the delay defect escape rate. Adjustment of the chip-specific test time in the continuous process variation space is attained through an adaptive test flow that utilizes process data measurements from the device under test. The results evince that a substantial improvement in the delay test quality can be obtained at no increase whatsoever to test time consumed by conventional test flows.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Tackling resource variations through adaptive multicore execution frameworks\n", "abstract": " Multicore architectures have been widely adopted to accommodate the rising performance demand in various application domains, ranging from high-end supercomputing to low-end consumer electronics. Yet due to the ever growing integration density and application complexity, such architectures suffer from increased level of core availability variations. At runtime, issues such as device failures, heat buildup, as well as resource competitions and preemptions can make computational resources unavailable, necessitating execution schedules capable of delivering diverse performance levels to match the varying resource allocations. The adaptive execution framework introduced in this paper delivers high-quality schedules capable of predictably reconfiguring execution and gracefully degrading performance in the face of resource unavailability. By adhering to a novel band structure, a set of possible execution\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Full fault resilience and relaxed synchronization requirements at the cache-memory interface\n", "abstract": " While multicore platforms promise significant speedup for many current applications, they also suffer from increased reliability problems as a result of ever scaling device size. The projected elevation in fault rate, together with the diverse behavior of fault manifestation, argues for highly efficient solutions of full fault resilience. Traditional duplication and checkpointing strategies typically impose sizable overhead in checkpointing execution results, or in constantly synchronizing two threads for value checking. To reduce such overhead while at the same time delivering full fault resilience, we propose an integrated fault detection and checkpointing framework, wherein the comparison and checkpointing process is performed at the cache-memory interface. By sharing a single cache between two duplicated threads, execution results can be directly verified in the cache before being written back, thus strictly protecting the\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Flip-flop hardening and selection for soft error and delay fault resilience\n", "abstract": " The traditional test model of go/no-go testing being questioned by increasing delay fault manifestations has become even further challenged as a result of unpredictable soft errors. Consequent probabilistic fault manifestations shift the focus to fault resilience mechanisms and tradeoffs of false alarms vs. escapes. Fault manifestation at flip-flops necessitates solutions that rely on their hardening, possibly imposing inordinate cost as flip-flops constitute a significant fraction of current designs. A two-pronged approach for resolving this challenge is necessitated, consisting of frugal flip-flop designs, capable of withstanding such faults, and an economic rationalization model to enable a prioritized flip-flop selection within an overall design budget. In this paper, we propose a hardened flip-flop that increases circuit tolerance to soft errors and delay faults simultaneously and the associated selective hardening scheme guided\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Locality aware redundancy allocation in nanoelectronic systems\n", "abstract": " A high level of redundancy is required to deal with the challenge of high defect and fault rates in nano environments. The reconfigurability of nano devices and the regular structure of nano fabrics make reconfiguration based repair an essential approach for both defect and fault tolerance. Ideally, repair based approaches have the best hardware efficiency when full sharing of redundancy is achievable. However, nanoelectronic systems are subject to strict constraints on localized interconnections, which limit the sharing of redundant resources to within a small neighborhood. To more fully understand this challenge, we provide a model for the issue of redundant resource sharing under locality constraint. Our model captures the redundancy sharing essence of a system, and is applicable to any specific topology or layout of a nanoelectronic system. Based on this model, defect tolerance can be well defined, and can be\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Online test and fault-tolerance for nanoelectronic programmable logic arrays\n", "abstract": " Fault-tolerance is one of the major challenges facing nanoelectronic systems. Previous defect-tolerance techniques have focused on offline testing and are ill-suited for handling device death. We propose a fault-tolerance scheme for nanoelectronic PLAs that is based on checkpointing. With low overhead, our scheme is able to test and diagnose crosspoint faults that may appear in the PLA during its operational lifetime.We also present a new test vector compaction algorithm that significantly reduces the number of test vectors. This new algorithm takes advantage of the density and reconfigurability of nanoelectronic circuits by raising the granularity of defect diagnosis to the row/column level. We are therefore able to reduce the number of test vectors to O(n+p) for a PLA with n input and p product lines without sacrificing diagnosability. Experimental results show that our checkpointing scheme, together with the\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Nanoelectronic architectures: Reliable computation on defective devices\n", "abstract": " Nanoelectronics is increasingly poised to occupy an important role in providing solutions to surmount obstacles imposed by Moore\u2019s Law in semiconductor electronics [1]. While the fundamental issues may remain largely intact, nanoelectronics imposes significant quantitative changes resulting in qualitative modifications to the way we design; paramount among them are an increase in fabric size, density and fault rates which result in (i) increased emphasis on parallelism,(ii) strict constraints on localized interconnection,(iii) regularity in nanofabrics, and (iv) repair of faulty circuits, offline and possibly online. Perhaps, the most fundamental challenge to be overcome in constructing any nanoelectronics based systems is the high unreliability of nanoelectronic devices which manifests mainly in two forms. First, manufacturing defects increase significantly, due to the defect prone fabrication process in nano environments needed to pro-duce the small scale devices with a bottom-up self-assembly process. The defect rates of nanoelectronic systems are projected to be many orders higher than the current CMOS systems. Second, not only is high occurrence of transient faults expected during run-time, but so is a high variance in the fault rate and a clustered behavior of faults. These are essentially caused by the nano-scale size of the de-vices as well as the low voltage utilized, which result in extremely high sensitivity to environmental influences, such as temperature, cosmic ray particles and background noise. The emerging new challenges of nanoelectronics result in a reevaluation of the components typically utilized, and the approaches usually\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Statistical tolerance analysis for assured analog test coverage\n", "abstract": " Increasing numbers of analog components in today's systems necessitate system level test composition methods that utilize on-chip capabilities rather than solely relying on costly DFT approaches. We outline a tolerance analysis methodology for test signal propagation to be utilized in hierarchical test generation for analog circuits. A detailed justification of this proposed novel tolerance analysis methodology is undertaken by comparing our results with detailed SPICE Monte-Carlo simulation data on several combinations of analog modules. The results of our experiments confirm the high accuracy and efficiency of the proposed tolerance analysis methodology.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Fast and energy-frugal deterministic test through test vector correlation exploitation\n", "abstract": " Conversion of the flip-flops of the circuit into scan cells helps ease the test challenge; yet test application time is increased as serial shift operations are employed. Furthermore, the transitions that occur in the scan chains during these shifts reflect into significant levels of circuit switching unnecessarily, increasing the power dissipated. Judicious encoding of the correlation among the test vectors and construction of a test vector through predecessor updates helps reduce not only test application time but also scan chain transitions as well. Such an encoding scheme, which additionally reduces test data volume, can be further enhanced through appropriately ordering and padding of the test cubes given. The experimental results confirm the significant reductions in test application time, test data volume and test power achieved by the proposed compression methodology.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Microarchitectural synthesis of performance-constrained, low-power VLSI designs\n", "abstract": " New portable signal-processing applications such as mobile telephony, wireless computing, and personal digital assistants place stringent power consumption limits on their constituent components. Substantial power savings can be realized if 5 V designs are translated to use the new lower supply voltage standards. This conversion, however, is not achieved easily: a design originally targeted for implementation in a 5 V technology will typically require significant rework to meet timing and throughput requirements at the lower operating voltage. In this paper we describe a high-level synthesis system which assists the designer in performing this task, minimizing the need for manual redesign. Techniques employed in this work include pipelining and a new approach to module selection that minimizes power consumption subject to timing constraints. Using these and other high-level synthesis techniques to target\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Selecting a PRPG: randomness, primitiveness, or sheer luck?\n", "abstract": " The ability of randomness to constitute a quality measure in PRPG selection for logic BIST is investigated. Extensive correlation analyses performed on a rich set of pattern generators and benchmark circuits indicate that higher randomness is no guarantee of improved fault coverage in LFSM-based pseudo-random pattern generators. Further evaluation of fault coverage data indicates that the performance of PRPGs is dependent on both circuit particularities and the number of test patterns employed.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Accumulation-based concurrent fault detection for linear digital state variable systems\n", "abstract": " An algorithmic fault detection scheme for linear digital state variable systems is proposed. The proposed scheme eliminates the necessity of observing the internal states of the system for concurrent fault detection by utilizing an accumulation-based approach. Observation merely of the inputs and the outputs results in significantly reduced area overhead and no performance penalty. Experimental results verify that 100% concurrent fault detection is attainable for linear digital state variable systems.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Path-based test composition for mixed-signal SOC's\n", "abstract": " We outline a methodology for system level test composition out of module level tests in the context of system-on-a-chip (SOC). The method can be utilized as soon as high level specifications are available providing avenues for testability insertion. The digital/analog interface is handled by a conversion from digital bits to analog signals. Experimental results show that high fault and yield coverages for most tests can be attained with no hardware alterations.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Self recovering controller and datapath codesign\n", "abstract": " As society has become more reliant on electronics, the need for fault tolerant ICs has increased. This has resulted in significant research into both fault tolerant controller design, and mechanisms for datapath fault tolerance insertion. By treating these two issues separately, previous work has failed to address compatibility issues, as well as efficient codesign methodologies. In this paper, we present a unified approach to detecting control and datapath faults through the datapath, along with a method for fault identification and reconfiguration. By detecting control faults in the datapath, we avoid the area and performance overhead of detecting control faults through duplication or error checking codes. The result is a complete design methodology for self recovering architectures capable of far more efficient solutions than previous approaches.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Frequency-domain compatibility in digital filter BIST\n", "abstract": " We examine frequency-domain issues in the design and selectionof on-chip test generators for built-in self-test (BIST) of high-performancedigital filters. Test-generator/circuit compatibility isidentified as a significant factor in testing large filters. A fault-injectionexperiment is used to show that when an incompatible testgenerator is used, high fault coverage (over 99%) does not guaranteethat all serious faults will be detected. The frequency-domaincharacteristics of some basic test generation schemes are examined, and guidelines for test generator selection are proposed. Analyticaltechniques for identifying frequency-related testability problemsare discussed, and several test generation schemes are evaluated byfault simulating them against lowpass, bandpass, and highpass filters. A mixed test generation scheme is shown to reduce the numberof untested faults by a factor of two to three over a standard linear\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Synthesizing self-testable filters via scaling and redundant operator elimination\n", "abstract": " A synthesis-based approach to improving the testability of digital filters is presented, with the aim of producing designs that achieve very high fault coverage under low-overhead built-in self-test methodologies. The synthesis-based approach permits high coverages to be achieved without the addition of special test hardware or other manipulation of the gate-level netlist. The testability of a design is enhanced at the register-transfer level (RTL), prior to synthesis. Using scaling as a redundancy elimination technique, it is possible to reduce the area required by a design, as well as identify further redundancies that can be eliminated through the automatic selection of optimized RTL structures drawn from a parameterized VHDL library.", "num_citations": "4\n", "authors": ["530"]}
{"title": "Optimal self-recovering microarchitecture synthesis\n", "abstract": " The authors propose a novel ILP model for the scheduling problem in self-recovering microarchitecture synthesis. A self-recovering microarchitecture, on detecting a (transient), fault, roll back to a previously known correct state - the checkpoint - and retries the computation. The maximum distance between adjacent checkpoints - the retry period - is determined by the transient fault rate as well as the average lifetime of a transient fault. At a checkpoint, the results of intermediate computations are compared (using voters), and if correct saved in registers. Consequently, associated with each checkpoint, there is a time overhead due to comparison and an area overhead due to the fault-tolerant nature of the voters. The authors formulate time-constrained scheduling as minimizing either the number of voters or the overall hardware, subject to constraints on the number of clock cycles, the retry period, and the number of\u00a0\u2026", "num_citations": "4\n", "authors": ["530"]}
{"title": "Transformation-based register optimization in high-level synthesis\n", "abstract": " A novel approach to temporary register minimization that exploits the power of behavioral transformations is proposed. Behavioral transformations allow one to reduce the register requirements below the inherent lower bound imposed by the structure of the input flow graph. Specifically, the transformations minimize the lifetimes of registers in a scheduled flow graph. The transformations are applied across clock cycle boundaries with peak register use. Preliminary results for the benchmark examples show a significant reduction in the number of registers.< >", "num_citations": "4\n", "authors": ["530"]}
{"title": "Low-Cost Error Detection in Deep Neural Network Accelerators with Linear Algorithmic Checksums\n", "abstract": " The widespread adoption of deep neural networks in safety-critical systems necessitates the examination of the safety issues raised by hardware errors. The appropriateness of the concern is herein confirmed by evidencing the possible catastrophic impact of hardware bit errors on DNN accuracy. The consequent interest in fault tolerance methods that are comprehensive yet low-cost to match the margin requirements of consumer deep learning applications can be met through a rigorous exploration of the mathematical properties of the deep neural network computations. Our novel technique, Sanity-Check, allows error detection in fully-connected and convolutional layers through the use of linear algorithmic checksums. The purely software-based implementation of Sanity-Check facilitates the widespread adoption of our technique on a variety of off-the-shelf execution platforms while requiring no hardware\u00a0\u2026", "num_citations": "3\n", "authors": ["530"]}
{"title": "Boosting bit-error resilience of DNN accelerators through median feature selection\n", "abstract": " Deep learning techniques have enjoyed wide adoption in real life, including in various safety-critical embedded applications. While neural network computations require protection against hardware errors, the substantial overheads of conventional error-tolerance techniques limit their use on embedded platforms, which carry out demanding deep neural network computations with limited resources. The utilization of conventional techniques is further constrained in high error rate scenarios, increasingly prevalent under aggressive energy and performance optimizations. To resolve this conundrum, we introduce a novel median feature selection technique to filter the impact of bit errors prior to the execution of each layer. While our technique can be deemed as a fine-grained modular redundancy scheme, its construction purely out of the inherent redundancy of the network necessitates neither additional parameters\u00a0\u2026", "num_citations": "3\n", "authors": ["530"]}
{"title": "Towards a cost-effective hardware trojan detection methodology\n", "abstract": " Due to the increasing globalization of integrated circuit fabrication, hardware security has emerged as a major issue, necessitating hardware trojan detection mechanisms. Numerous techniques exist, a subset of which we applied to 6 sets of combinational circuits and 2 sets of sequential circuits in order to effectively determine the presence of a hardware trojan. Utilizing only a single type of functional test and a single type of side-channel test, we were able to make determinations about all 6 sets of combinational circuits and 1 of the 2 sets of sequential circuits.", "num_citations": "3\n", "authors": ["530"]}
{"title": "Adaptive test framework for achieving target test quality at minimal cost\n", "abstract": " Defect characteristics and consequently test quality vary throughout the production life cycle. An optimal test methodology needs to adjust the test set based on the ever changing defect characteristics to deliver consistent test quality levels. In this paper, we propose an adaptive test framework that alters the test set continuously by utilizing the history of recent failures to track the instantaneous defect escape level, thus reaching the target test quality level at minimal test cost.", "num_citations": "3\n", "authors": ["530"]}
{"title": "Power efficient register file update approach for embedded processors\n", "abstract": " In this paper we present an approach for a low power register file in the domain of embedded processors. The suggested approach obtains power savings through tackling the unnecessary writes to register files for short live registers. Writes to register files are essentially redundant when an instruction manages to forward its results to all of its dependents through forwarding hardware. As the percentage of registers that exhibit short liveness is shown to be significant, tackling unnecessary writes contributes to delivering appreciable power savings. In this work we show that tackling the unnecessary writes could be attained efficiently through a register based encoding scheme. The suggested encoding scheme exploits application-specific information and renames all or most of the short live registers to a small subset of the registers that are prespecified during the hardware design. The renaming process is performed\u00a0\u2026", "num_citations": "3\n", "authors": ["530"]}
{"title": "Circuit-level mismatch modelling and yield optimization for CMOS analog circuits\n", "abstract": " A methodology for constructing circuit-level mismatch models and performing yield optimization is presented for CMOS analog circuits. The methodology combines statistical techniques with direct investigation of circuit behavior, and achieves model simplification and computational efficiency while ensuring sufficient accuracy. The circuit-level mismatch model can be used in performance characterization and yield estimation, both important in providing information for circuit reliability analysis. The proposed yield optimization technique consists of constructing and refining a yield model over the designable parameters, and ensures fast convergence to the global optimal design. The experimental results on two representative circuits confirm the efficiency and effectiveness of the proposed method.", "num_citations": "3\n", "authors": ["530"]}
{"title": "On the identification of modular test requirements for low cost hierarchical test path construction\n", "abstract": " We discuss a novel method for identifying test requirements of modules in a hierarchical design in order to facilitate the construction of cost-effective hierarchical test paths. Unlike current practices, which construct very general paths capable of justifying all vectors and propagating all responses to and from each module in the design, test requirements in our method are defined as a set of fine-grained input and output bit clusters and pertinent symbolic values. These test requirements reflect the inherent connectivity and regularity of each module and, when supported by corresponding hierarchical test paths, they guarantee complete testability of the module. Their key advantage is that they are not fully specified test vectors and, therefore, they do not require a computationally expensive search algorithm to satisfy from the primary inputs and outputs of the circuit. At the same time, they are also not arbitrarily general\u00a0\u2026", "num_citations": "3\n", "authors": ["530"]}
{"title": "Application specific instruction memory transformations for power efficient, fault resilient embedded processors\n", "abstract": " In this paper we present a coding framework for a low energy instruction bus for embedded processors. The encoder exploits application-specific knowledge regarding program hot-spots to generate codewords that deliver savings in power and to furthermore provide concurrent detection of errors. Power savings can be obtained through the use of codewords that reduce the switching activity on the bus. The analysis shows that generating codewords that prohibit the occurrence of three consecutive transitions in three adjacent lines is fundamental to capturing the worst-case crosstalk faults in the bus lines at run time, thus improving the overall reliability of the bus. The desired codewords can be generated through a set of simple prespecified transformations. The detailed analysis we outline shows that the presented transformations are optimal. The proposed encoding scheme is dynamically reprogrammable, thus\u00a0\u2026", "num_citations": "3\n", "authors": ["530"]}
{"title": "Customizable embedded processor architectures\n", "abstract": " In this paper, we present a framework for dynamic application customization for high-performance and low-power embedded processors. The proposed architecture is capable of utilizing application information to boost the performance and lower the power consumption of the most important microarchitectural components such as instruction/data caches and the memory subsystem. We present a design framework, including CAD support infrastructure and reprogrammable hardware support, for a dynamically customizable microarchitecture. We outline the underlying algorithms for compile-time extraction of the utilized application properties and we present the architectural principles of the hardware support. Extensive experimental results confirm the efficacy of this novel embedded processor architecture.", "num_citations": "3\n", "authors": ["530"]}
{"title": "Cost effective digital filter design for concurrent test\n", "abstract": " Invariant-based concurrent test schemes can provide economical solutions to the problem of concurrent testing of digital filters. Design methodologies for digital filters ensuring concurrent testability are outlined. Experimental results confirm through fault simulation 100% fault coverage within area cost comparable to that of DfT for off-line test and with error detection latency well below human response times.", "num_citations": "3\n", "authors": ["530"]}
{"title": "How to avoid random walks in hierarchical test path identification\n", "abstract": " Hierarchical test approaches address the complexity of test generation through symbolic reachability paths that provide access to the I/Os of each module in a hierarchical design. While transparency behavior suitable for symbolic design traversal can be utilized for datapath modules, control modules do not exhibit transparency, and therefore require exhaustive search algorithms or expensive DFT hardware. In this paper we introduce a fast hierarchical test path identification methodology for circuits with no DFT at the controller-datapath interface. We introduce the concept of influence tables, modeling the impact of control states on the datapath, based on which appropriate state sequences for accessing each module are identified. Imposition of such sequences on a hierarchical test path identification algorithm, in the form of constraints, results in significant speedup over alternative non-DFT based approaches.", "num_citations": "3\n", "authors": ["530"]}
{"title": "A module diagnosis and design-for-debug methodology based on hierarchical test paths\n", "abstract": " Fault identification capabilities are becoming increasingly important in modern designs, not only in support of design debugging methodologies, but also for the purpose of process characterization and yield enhancement. At the same time, hierarchical test approaches are becoming the prevalent means for addressing the size and complexity of large designs and for accommodating the varying individual test needs of each design module. In this paper, we discuss a module diagnosis and design-for-debug methodology through hierarchical test paths. Based on debug information inherently attainable from hierarchical test paths, we outline a diagnosis algorithm that identifies the minimal set of faulty module candidates, under the single faulty module model. We further provide a disambiguation rule to ensure unfailing identification of the single faulty module. Low-cost, design-for-debug techniques are subsequently\u00a0\u2026", "num_citations": "3\n", "authors": ["530"]}
{"title": "An examination of PRPG selection approaches for large, industrial designs\n", "abstract": " A study for selecting effective pseudo-random pattern generators (PRPGs) for large industrial designs is undertaken. For the PRPG selection process, guidance from a set of ISCAS benchmark circuits is initially sought. The set of benchmark circuits used is shown to be ineffective in providing material guidance. The alternative of PRPG selection through actual design experimentation is examined and its weaknesses identified and outlined. A brief DFT analysis is outlined to indicate the importance of early PRPG selection.", "num_citations": "3\n", "authors": ["530"]}
{"title": "Concurrent error recovery with near-zero latency in synthesized ASICs\n", "abstract": " The importance of fault tolerant design has been steadily increasing as reliance on error free electronics continues to rise in critical military, medical, and automated transportation applications. While rollback and checkpointing techniques facilitate area efficient fault tolerant designs, they are inapplicable to a large class of time-critical applications. We have developed a novel synthesis methodology that avoids rollback, and provides both zero reduction in throughput and near-zero error latency. In addition, our design techniques reduce power requirements associated with traditional approaches to fault tolerance.", "num_citations": "3\n", "authors": ["530"]}
{"title": "Module selection in microarchitectural synthesis for multiple critical constraint satisfaction\n", "abstract": " Accurate design descriptions during synthesis allow efficient use of resources. The appropriate use of distinct implementations of RTL operators helps generate optimal VLSI designs. The system presented here utilizes libraries composed of multiple modules with identical functionality, but distinct performance and area characteristics. Such libraries allow the generation of an accurate estimate of the area and delay of the final design during synthesis. Full use of the module selection capability is possible by allowing the user to specify a total area limit rather than a detailed allocation. Consequently, tradeoffs between different allocations can be fully explored. Scheduling, module selection, and allocation are performed simultaneously to achieve optimal use of area and delay, and to facilitate the incorporation of lower level design considerations into behavioral synthesis. Synthesis decisions are made in a time-constrained and area-constrained fashion, by using both constraints to identify and avoid infeasible design possibilities. Module selection, scheduling, and allocation for pipelined designs is also implemented. Experimental results show that the use of module selection and time-and-area-constrained synthesis results in an area/delay design curve which is superior to the results of traditional systems.", "num_citations": "3\n", "authors": ["530"]}
{"title": "AdaTrust: Combinational Hardware Trojan Detection Through Adaptive Test Pattern Construction\n", "abstract": " As society becomes increasingly reliant on products and systems that make use of integrated circuits, the defense against potential hardware Trojan attacks by an untrusted foundry becomes an important part of any certification flow for critical components. The slew of recent proposals notwithstanding, a satisfactory solution is still wanting as the solutions offered heretofore either require impractical design/test pattern cost or deliver insufficient detection capabilities, primarily challenged by the noise induced by process variation. The methodology put forth by this proposal aims to remedy this, leveraging an adaptive approach that applies superposition to perform a fine-grained circuit analysis and expose any extant Trojan circuitry. Iterative test pattern modifications, circuit response analysis, and adaptive decision-making are deployed, all embedded within the design-for-test and test pattern cost paradigms of a\u00a0\u2026", "num_citations": "2\n", "authors": ["530"]}
{"title": "Shielding logic locking from redundancy attacks\n", "abstract": " The security of logic locking has been extensively examined under the threat model that assumes the availability of an activated IC. Recently, structural attacks such as ones based on redundancy analysis have challenged the viability of logic locking even when stringent measures are taken to preclude access to an activated IC. In this paper, we propose a gate selection based logic locking technique to identify key gate insertion sites such that the redundancy level deviates minimally under all key assignments. The proposed logic locking technique is evaluated on a set of benchmark circuits to confirm its resistance against redundancy analysis based attacks.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Ensuring system security through proximity based authentication\n", "abstract": " As Internet of Things applications using embedded systems enter wider markets, securing systems against attacks becomes necessary. In many applications, securely determining transmitter location helps maintaining system security. A relatively new RF-based localization technique called Received Signal Strength Ratio (RSSR), has potential utility in securing ad-hoc networks and body-area networks. We describe a novel attack on the security of such systems, and discuss a set of mitigation strategies that restore the effectiveness of RSSR.", "num_citations": "2\n", "authors": ["530"]}
{"title": "VLSI-SoC: At the Crossroads of Emerging Trends\n", "abstract": " This book contains extended and revised versions of the highest-quality papers that were presented during the 21st edition of the IFIP/IEEE WG10. 5 International Conference on Very Large Scale Integration (VLSI-SoC), a global system-on-chip design and CAD conference. This edition of the conference was held at Novotel Hotel in Istanbul, Turkey (October 6\u20139, 2013). Previous conferences have taken place in Edinburgh, Trondheim, Vancouver, Munich, Grenoble, Tokyo, Gramado, Lisbon, Montpellier, Darmstadt, Perth, Nice, Atlanta, Rhodes, Florianopolis, Madrid, Hong Kong, and Santa Cruz. The purpose of this conference, which was sponsored by IFIP TC 10 Working Group 10.5, the IEEE Council on Electronic Design Automation (CEDA), and the IEEE Circuits and Systems Society, and In-Cooperation of ACM SIGDA, was to provide a forum for the exchange of ideas and presentation of industrial and academic\u00a0\u2026", "num_citations": "2\n", "authors": ["530"]}
{"title": "Delay test resource allocation and scheduling for multiple frequency domains\n", "abstract": " As the number of frequency domains aggressively grows in today's SOCs, the delivery of high delay test quality across numerous frequency domains while meeting test budgets is crucial. This goal necessitates not only the consideration of fault coverage but also the distinct characteristics of each domain such as frequency and the distribution of path lengths and, additionally, the delay test quality tradeoffs across these domains. This paper proposes a method to identify the optimal test time allocation per domain based on the distinct characteristics of each in order to minimize overall delay defect escape level. The proposed method not only considers test time allocation but also concurrent scheduling of domains to optimize the delay test quality for SOCs that support the testing of multiple frequency domains in parallel.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Diagnosing scan clock delay faults through statistical timing pruning\n", "abstract": " A novel methodology for diagnosing the delay faults in the scan clock tree is proposed. The proposed scheme characterizes the timing impact of the defective clock buffers by extracting the change in the delay distribution of the clock paths, enabling the effective pruning of unrealistic fault hypotheses that would result in highly deviant timing behavior. The proposed scheme models the statistical delay variation due to test mode power-ground noise, thus maximally approximating the actual failure behavior. Simulation results have confirmed that the proposed methodology can yield highly accurate diagnosis results for complex fault manifestations.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Squashing microcode stores to size in embedded systems while delivering rapid microcode accesses\n", "abstract": " Microcoded customized IPs offer superior performance and direct programmability of micro-architectural structures compared to instruction-based processors, yet at the cost of drastically enlarged code sizes. Code compression can deliver size reductions but necessitates attention to performance issues, so that the performance benefits of microcoded IPs are not squandered in the process. To attain this goal, we propose in this paper a fast code compression technique through exploiting the fact that the microcodes contain a sizable amount of unspecified bits. Although the values and the positions of the specified bits are highly irregular, the proposed technique can still flexibly and precisely fill in these fully specified bits through utilizing a linear network. The linear property inherent in the compression strategy in turn enables the development of an extremely low-overhead decompression engine. At runtime, the\u00a0\u2026", "num_citations": "2\n", "authors": ["530"]}
{"title": "Making DNA self-assembly error-proof: attaining small growth error rates through embedded information redundancy\n", "abstract": " DNA self-assembly is emerging as the most promising technique for nanoscale self-assembly as it uses the simple, yet precise rules of DNA binding to create macroscale assemblies from nanoscale components. However, DNA self-assembly is also highly error-prone and requires the use of error-resilience techniques in order to unlock its potential. In this paper we propose a technique for error-resilience that is based on information redundancy but, in contrast to previous information redundancy schemes, can achieve much higher resilience to growth errors. By expanding the neighborhood from which redundant information is taken, we can extend the distance that errors are propagated and therefore increase the likelihood of the error being reversed. Given a growth error rate of isin, we show that with a neighborhood of only 2 we can reduce the error rate to isin 3.64  for arbitrary functions (as compared to isin 2.33\u00a0\u2026", "num_citations": "2\n", "authors": ["530"]}
{"title": "Partial core encryption for performance-efficient test of SOCs\n", "abstract": " The isolation of a core through full I/O scan helps ease SOC test challenges; yet the performance of high-speed SOCs is significantly hampered. We propose a partial core encryption methodology wherein the core vendor unveils only a small part of the core logic, successfully satisfying core IP protection requirements. Once the partially encrypted cores are merged into an SOC, the system integrator performs test generation on the visible SOC logic only, greatly reducing the test generation effort expended. By utilizing the test data provided by the core vendor as well, the SOC integrator can test the SOC with no performance degradation. We present an efficient fault analysis based core encryption algorithm which is guided by judiciously computed testability measures. The experimental results confirm the significantly high encryption levels attained by the proposed encryption algorithm.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Cost-effective concurrent test hardware design for linear analog circuits\n", "abstract": " Concurrent detection of failures in analog circuits is becoming increasingly more important as safety-critical systems become more widespread. A methodology for the automatic design of concurrent failure detection circuitry for linear analog systems is discussed in this paper In contrast to previous approaches, the methodology aims at providing coverage in terms of all the circuit components while minimizing the loading overhead by reducing the number of internal circuit nodes that need to be tapped Parameter tolerances are incorporated through either statistical or mathematical analysis to determine the threshold for failure alarm. Experimental results confirm that full coverage can be attained while keeping the hardware overhead within a pre-specified budget.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Dynamic test data transformations for average and peak power reductions\n", "abstract": " Parallel test application helps reduce the otherwise considerable test times in SoCs; yet its applicability is limited by average and peak power considerations. The typical test vector loading techniques result in frequent transitions in the scan chain, which in turn reflect into significant levels of circuit switching unnecessarily. Judicious utilization of logic in the scan chain can help reduce transitions while loading the test vector needed. The transitions embedded in both test stimuli and the responses are handled through scan chain modifications consisting of logic gate insertion between scan cells as well as inversion of capture paths with no performance degradation. To reduce average and peak power, we herein propose computationally efficient schemes that identify the location and the type of logic to be inserted. The experimental results confirm the significant reductions in test power possible under the proposed\u00a0\u2026", "num_citations": "2\n", "authors": ["530"]}
{"title": "Modular test generation and concurrent transparency-based test translation using gate-level ATPG\n", "abstract": " We introduce a hierarchical test generation methodology for modular designs, employing exclusively gate-level ATPG. Based on the notion of modular transparency, the search space of the design is reduced to alleviate the complexity of gate-level test generation. Although ATPG is applied at the full circuit, faults in each module are targeted individually, while the surrounding modules are replaced by their much simpler, transparency-equivalent logic. As analyzed theoretically and as demonstrated through a set of experimental data, the proposed methodology results in significant test generation speed-up, while preserving comparable fault coverage and vector count to full-circuit gate-level ATPG.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Low cost concurrent test implementation for linear digital systems\n", "abstract": " An implementation of a low-cost, time-extended invariant-based concurrent test scheme for linear digital systems is presented. Both feedback and non-feedback systems are analyzed to identify gate and RT level implementation requirements for high on-line fault coverage. Simulation results on implementations satisfying the outlined requirements indicate that low latency, 100% on-line fault coverage is attained within hardware costs comparable to those of scan insertion.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Design rule driven behavioral synthesis for test\n", "abstract": " We have developed a new approach to behavioral synthesis for testability. Utilizing a VHDL transformation environment, we have distilled design rules for testable VHDL generation. Design rules can be linked to specific design for test techniques, allowing simplified exploration of BIST, partial scan, and other test approaches. The result is a design methodology which is simple to use, improves testability, and decreases time to market.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Metric-based transformations for self testable VLSI designs with high test concurrency\n", "abstract": " We propose an approach for improving the testability of a design under BIST methodology through behavioral restructuring. Our results show that the proposed transformations help reduce the number of required test sessions.", "num_citations": "2\n", "authors": ["530"]}
{"title": "Simulated annealing based yield enhancement of layouts\n", "abstract": " This paper presents DEFT, a system for synthesizing defect-tolerant layouts, that in-grains tolerance to fabrication induced defects. This is accomplished by dispersing nets with large overlaps into nonadjacent tracks. DEFT also affords trade-offs between area (measured as the number of tracks) and yield of the resulting layout. The defect-tolerant layouts synthesized by DEFT have been consistently superior to those generated by other layout synthesis systems.< >", "num_citations": "2\n", "authors": ["530"]}
{"title": "Software design issues in the implementation of hierarchical, display editors\n", "abstract": " Language-based editors offer to the user a number of advantages which accrue from their cognizance of the edited language. Foremost among these are the advantages of manipulating and traversing the edited document in terms of the syntactic structures of the document. Also automatic formatting and error-detection capabilities are afforded to the user through syntax-directed editors.", "num_citations": "2\n", "authors": ["530"]}
{"title": "A crowd-based explosive detection system with two-level feedback sensor calibration\n", "abstract": " Large, open, public events, such as marathons and festivals, have always presented a unique safety challenge. These sprawling events, which can take up entire city blocks or stretch for many miles, can draw tens to hundreds of thousands of spectators and in some cases have open admission. As it is impracticable to guarantee the subjection of every event-goer to a security screening, we propose a crowd-based explosive detection system that uses a multitude of low-cost ChemFET sensors which are distributed to attendees. As the sensors offer limited accuracy, we further propose a server-based decision-making framework that utilizes a two-level feedback loop between the sensors and the server and explores spatial and temporal locality of the collected data to overcome the inherent low-accuracy of individual sensors. We thoroughly explore two distinct detection schemes, stressing their performance under a\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Hunting Sybils in Participatory Mobile Consensus-Based Networks\n", "abstract": " We focus on detecting adversarial non-existent nodes, Sybils, in anonymized participatory mobile networks where nodes support both a node-to-server and peer-to-peer connection capabilities. As data-driven decisions within such networks typically rely on local consensuses, they are susceptible to adversarial injection attacks which impersonate honest nodes and overpower local data through forgery.", "num_citations": "1\n", "authors": ["530"]}
{"title": "Squeezing correlated neurons for resource-efficient deep neural networks\n", "abstract": " DNNs are abundantly represented in real-life applications because of their accuracy in challenging problems, yet their demanding memory and computational costs challenge their applicability to resource-constrained environments. Taming computational costs has hitherto focused on first-order techniques, such as eliminating numerically insignificant neurons/filters through numerical contribution metric prioritizations, yielding passable improvements. Yet redundancy in DNNs extends well beyond the limits of numerical insignificance. Modern DNN layers exhibit a significant correlation among output activations; hence, the number of extracted orthogonal features at each layer rarely exceeds a small fraction of the layer size. The exploitation of this observation necessitates the quantification of information content at layer outputs. To this end, we employ practical data analysis techniques coupled with a novel feature\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Taming combinational Trojan detection challenges with self-referencing adaptive test patterns\n", "abstract": " While many side-channel methods have been proposed for detecting hardware Trojans inserted by an untrusted foundry, they are challenged in the face of process variation noise. The impacts of process variation have forced researchers to propose costly design enhancements to improve detection as a counter to the deficiency of current easy-to-implement test pattern-based methods. To overcome process variation noise with no design cost, we propose a novel self-referencing adaptive approach based on test pattern construction, which learns from and conforms to device characteristics to maximally magnify the Trojan signal. Through iterative test pattern modifications, response analyses, and decision-making, we can pursue suspicious behaviors and increase the likelihood of Trojan detection. Experiments on Trust-Hub Trojan circuit benchmarks show the efficacy of this technique, magnifying an equivocal\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Test pattern superposition to detect hardware Trojans\n", "abstract": " Current methods for the detection of hardware Trojans inserted by an untrusted foundry are either accompanied by unreasonable costs in design/test pattern overhead, or return results that fail to provide confident trustability. The challenges faced by these side-channel techniques are primarily a result of process variation, which renders pre-silicon expectations nearly meaningless in predicting the behavior of a manufactured IC. To overcome this hindrance in a cost-effective manner, we propose an easy-to-implement test pattern-based approach that is self-referential in nature, capable of dissecting and understanding the characteristics of a given manufactured IC to hone in on aberrant measurements that are demonstrative of malicious Trojan hardware. By leveraging the superposition principle to cancel out non-Trojan noise, we can isolate and magnify Trojan circuit effects, all within a regime considerate of\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "The Return of Power Gating: Smart Leakage Energy Reductions in Modern Out-of-Order Processor Architectures\n", "abstract": " Leakage power has been a significant concern in power constrained processor design as manufacturing technology has scaled down dramatically in the last decades. While power gating has been known to deliver leakage power reductions, its success has heavily relied on judicious power gating decisions. Yet delivering such prudent decisions has been particularly challenging for out-of-order processors due to the unpredictability of execution order. This paper introduces an intelligent power gating method for out-of-order embedded and mobile processor execution units by monitoring and utilizing readily available hints on the pipeline. First, we track the counts of different instruction types in the instruction queue to identify the execution units slated to remain idle in the near future. As the presence of an instruction is not a definite indicator of its execution start due to stalls, our second guidance improves\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Power-Aware Delay Test Quality Optimization for Multiple Frequency Domains\n", "abstract": " As the number of frequency domains aggressively grows in today's systems-on-chip (SoCs), the delivery of high-delay test quality across numerous frequency domains while meeting test budgets assumes crucial importance. This paper proposes a method to explore the delay test quality tradeoffs across these domains, determining an optimal distribution of the test time budget across all domains while minimizing the overall SoC delay defect escape level. Satisfaction of this goal necessitates not only consideration of fault coverage but also of the distinct characteristics of each domain, such as frequency, path length distribution, scan length, and shift speed as well as full utilization of concurrent test support while remaining within the constraints of power thresholds to provide a reliable test environment. An optimization formulation as well as efficient test time allocation methods based on convexity and fast concurrent\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Mobile ecosystem driven dynamic pipeline adaptation for low power\n", "abstract": " State-of-the-art mobile smartphone and tablet processors are beginning to employ fully speculative, out-of-order architectures with deep instruction pipelines. These processors often have pipeline lengths of 24 or more stages. Furthermore, to improve high-performance ILP, these processors provide multiple parallel pipeline paths for various instruction types. These architectures provide multiple execution clusters defined by instruction type, each with its own issue queue. Instructions are dispatched to one of the appropriate issue queues, and all issue queues are then scanned in parallel to identify instructions ready for execution. The goal of such a resource-intensive architectural design is to sustain peak processor performance. Unfortunately, applications oftentimes only leverage a small subset of these robust computation resources, and the excess hardware resources still consume power while idle\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Joint profit and process variation aware high level synthesis with speed binning\n", "abstract": " As integrated circuits continuously scale up, process variation plays an increasingly significant role in system design and semiconductor economic return. In this paper, we explore the potential of profit improvement under the inherent semiconductor variability based on the speed binning technique. We aim to develop a set of high level synthesis (HLS) solutions, for which purpose heuristic techniques, including allocation, scheduling, and resource binding, are proposed. The goal is to construct designs that maximize the number of chips that can be sold at the most advantageous price, leading to the maximization of the overall profit. In addition, a genetic algorithm-based formulation is constructed for HLS solutions. Then, we complement the HLS techniques with near-optimal bin placement strategies for further profit improvement. Experimental results confirm the superiority of the HLS results and the associated\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Frugal but flexible multicore topologies in support of resource variation-driven adaptivity\n", "abstract": " Given the projected higher variations in the availability of computational resources, adaptive static schedules have been developed to attain high-speed execution reconfiguration with no reliance on any runtime rescheduling decisions. These schedules are able to deliver predictable execution despite the increased levels of device unreliability in future multicore systems. Yet the associated runtime reconfiguration overhead is largely determined by the underlying system topology. Fully connected architectures, although they can effectively hide the overhead in execution migration, become infeasible as the core count grows to hundreds in the near future. We exploit in this paper the high locality associated with adaptive static schedules, and outline a scalable and locally shareable system organization for multicore platforms. With the incorporation of a limited set of neighborhood-centered communication links\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "High durability in NAND flash memory through effective page reuse mechanisms\n", "abstract": " In this paper, we introduce a highly effective page reuse mechanism to reduce the amount of block erasures and page programming in NAND based primary memory architectures. The proposed techniques provide a very high rate of page reuse by effectively incorporating bit differences in page updates along with a reduction in bit unprogrammability by minimizing programming interference among adjacent pages. We also propose an effective block reclamation scheme to alleviate overall programming stress in a block so as to reduce the probability of run-time cell defects. The page reordering scheme can further increase page reusability by reducing run-time programming disturbance. The experimental results show that our proposed techniques significantly diminish the amount of block reclamation and consequently enhance the durability of the NAND flash based storage systems. Furthermore, by alleviating\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Fine-grained adaptive CMP cache sharing through access history exploitation\n", "abstract": " Advances in semiconductor technologies have enabled the integration of multiple processor cores as well as varying sizes of L1 and L2 caches on a single chip. The ever growing complexity and diversity of the associated workloads impose a crucial challenge on the organization and management of the on-chip cache resources. As each core generates a varying amount of accesses to each cache line during execution, sharing a single L2 cache among all the cores can minimize off-chip misses. However, each access to a shared L2 cache imposes significant performance and power overhead, as the tags of all the blocks on a cache line need to be compared in parallel. To efficiently utilize cache resources while saving power, we present in this paper a fine-grained L2 cache management technique with minimum hardware overhead. Each core is allowed to set an ownership bit in an L2 cache block to directly\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "VDDmin test optimization for overscreening minimization through adaptive scan chain masking\n", "abstract": " Very-low-voltage (VDDmin) test assumes significant importance in detecting flaws in marginal chips as it can magnify the electrical impact of flaws. Yet such a test scheme is increasingly sensitive to test mode IR-drop. Exceedingly high test mode current and voltage surge may result in good chips failing VDDmin test, thus resulting in yield loss. A technique to counteract such yield loss, scan chain masking, has been widely incorporated in commercial DFT tools to address this issue. However, the masking of scan cells might lead to a reduced flaw coverage, necessitating a thorough investigation of its impact on test quality during the development of the optimal test plan. In this paper, we propose an analytical model to evaluate the cost of scan chain masking in terms of test escapes and overscreening effect. An adaptive scan chain masking flow guided by this model is then proposed to enhance the VDDmin test\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Filtering global history: power and performance efficient branch predictor\n", "abstract": " In this paper we present an Application Customizable Branch Predictor, ACBP, that delivers efficiency in energy savings and performance without compromising prediction accuracy. The idea of our technique is to filter unnecessary global history information within the global history register to minimize the predictor size while maintaining prediction accuracy. We suggest in this work an efficient algorithm to capture the beneficial correlations. A cost-efficient and programmable hardware architecture is presented. Extensive experimental analysis confirms significant improvements in power savings and latency, ranging up to 84% and 30%,respectively.", "num_citations": "1\n", "authors": ["530"]}
{"title": "Decision tree based mismatch diagnosis in analog circuits\n", "abstract": " Mismatch is a critical consideration in analog circuit design. Knowledge of mismatch locations and an understanding of their impact on circuit performance are crucial for design optimization and process improvement. We present a circuit level mismatch diagnosis methodology in this paper. The functional parameters with abnormal values are measured as manifestations of mismatch, from which reverse tracing is employed to determine the mismatch source. The methodology is implemented on a representative benchmark and its efficiency confirmed by simulation results", "num_citations": "1\n", "authors": ["530"]}
{"title": "Design space exploration for aggressive test cost reduction in CircularScan architectures\n", "abstract": " Scan-based designs effectively reduce test generation complexity and thus deliver improved fault coverage. Nevertheless, the traditional scan architectures suffer from increased test time and test data volume. The CircularScan architecture (Arslan and Orailoglu) provides a flexible environment for test cost reduction. The new scan design enables the use of the captured response of the previously applied test pattern as a template. The subsequent pattern is loaded by efficiently performing the necessary changes on the template through the functionality provided by the new architecture, conceptually exploiting the inherent low specified bit density of the test patterns. We explore the space of possible design alternatives built on the CircularScan architecture; the design alternatives are presented with accompanying test application methods. The experimental results indicate a substantial test cost reduction, reaching\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Autonomous yet deterministic test of SOC cores\n", "abstract": " Increased core test parallelism translates into reduced SOC test application time; yet the availability of a limited number of tester channels hampers this parallelism. Furthermore, the test vectors to be delivered into core scan chains need to be stored in the tester memory, imposing considerable costs on SOC tests. We propose an SOC test methodology delivering all the benefits of core self-test, while ensuring fault coverage levels identical to those attained in deterministic test. In the proposed methodology, a single LFSR broadcasts pseudo-random patterns to each core; the LFSR patterns are transformed into the actual test vectors of a core while they are being shifted into the core scan chain. The transformation is realized through the logic gates inserted between the core scan cells. The efficacy and the cost-effectiveness of the proposed methodology reflects into significantly reduced test costs.", "num_citations": "1\n", "authors": ["530"]}
{"title": "Searching for global test costs optimization in core-based systems\n", "abstract": " This paper proposes a comprehensive model for test planning and design space exploration in a core-based environment. The proposed approach relies on the reuse of available system resources for the definition of the test access mechanism, and for the optimization of several cost factors (area overhead, pin count, power constraints and test time). The use of an expanded test access model and its concurrent definition with the system test schedule makes it possible the search for a cost effective global solution. Experimental results over the ITC'02 SOC Test Benchmarks show the variety of trade-offs that can be explored using the proposed model, and its effectiveness on optimizing the system test planning.", "num_citations": "1\n", "authors": ["530"]}
{"title": "Power efficiency through application-specific instruction memory transformations\n", "abstract": " The instruction memory communication path constitutes a significant amount of power consumption in embedded processors. We propose an encoding technique that exploits application information to reduce the associated power consumption. The microarchitectural support enables reprogrammability of the encoding transformations so as to track code particularities effectively. The restriction to functional transformations enables effective coding while delivering major power savings, in the process obviating furthermore the necessity to rely on dictionary lookup, one of the major shortcomings of prior approaches. The frugal functional transformation, reliant on a single bit logic gate, introduces no impact to the critical fetch stage of the processor pipeline while delivering fully all the theoretically achievable power savings. The reprogrammable hardware implementation enables flexible and inexpensive switches\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Compaction schemes with minimum test application time\n", "abstract": " Testing embedded cores in a System-On-a-Chip (SoC) necessitates the use of a test access mechanism, which provides for transportation of the test data between the chip and the core I/Os. To relax the requirements on the test access mechanism at the core output side, we outline a space and time compaction scheme which minimizes test application time and required test bandwidth at the same time. We formulate the constraints on a mathematical basis for no aliasing compaction circuitry. The proposed compaction scheme is applicable to both combinational and sequential circuits. The experimental results illustrate that not only test application time is minimized but furthermore the associated area overhead is low as well.", "num_citations": "1\n", "authors": ["530"]}
{"title": "Test quality and fault risk in digital filter datagraph BIST\n", "abstract": " An objective of DSP testing should be to ensure that any errors due to missed faults are infrequent compared to a circuit\u2019s intrinsic errors, such as overflow. A method is proposed for quantifying test quality for digital filters by measuring the risk associated with any untested faults. Techniques for finding upper bounds on fault activation rates under worst-case operating conditions are described. These techniques enable test designers to objectively discriminate significant missed faults from near-redundant faults, which are unlikely to be activated in normal operation of the device. This complements fault coverage as a measure of test quality, providing a means of locating high-risk missed faults even in very high coverage test regimes.", "num_citations": "1\n", "authors": ["530"]}
{"title": "Combined On-Line/Off-Line Test Solutions for Digital Filters \u0403\n", "abstract": " A low-cost on-line test scheme for digital filters, capable of providing an off-line BIST solution, is proposed. The scheme utilizes an invariant of the digital filter in order to detect possible circuit malfunctioning on-line and shares most of this on-line checking hardware with off-line BIST. The analysis performed indicates that 100% fault secureness & 100% fault coverage are possible, if certain design constraints are followed.", "num_citations": "1\n", "authors": ["530"]}
{"title": "Property-based RTL Test Justification and Propagation Analysis\n", "abstract": " As the size and complexity of modern digital circuits increases, tools and methodologies for designing and testing them face extremely challenging barriers. Consequently, a number of approaches have been employed in order to alleviate the burdens associated with such ambitious tasks. Among them, functional abstraction from the transistor and gate level to the register transfer and behavioral level and modular decomposition of the complete design into its constituent blocks have provided an edge. The ultimate goal of these methodologies is to address larger designs by operating at a coarser granularity and to improve the tools\u2019 sophistication by applying a \u201cdivide & conquer\u201d type of approach. In the test domain, the insufficiency of even the state-of-the-art test generation tools to handle large and complex designs has necessitated the utilization of the above methodologies. Functional abstraction has allowed test generation tools to exploit behavioral descriptions of the design in order to generate high-level test, resulting in encouraging but nonetheless, still inadequate results. Modular decomposition, on the other hand, has enabled highly accurate tests to be derived at the structural level for design modules. However, the justification and propagation through the rest of the design of locally derived tests, has imposed a serious obstacle to combining them into a complete and applicable scheme. Attempts to benefit from the advantages of both approaches require employing modular decomposition for the test generation process and functional abstraction to a higher description level for test justification and propagation. The above observations\u00a0\u2026", "num_citations": "1\n", "authors": ["530"]}
{"title": "Propagation Analysis\u201d\n", "abstract": " We introduce a formal mechanism for capturing test justification and propagation related behavior of blocks. Based on the identified test translation behavior, an RTL testability analysis methodology for hierarchical designs is derived. An algorithm for pinpointing the local-to-global test translation controllability and observability bottlenecks is presented. The analysis results are validated through an AT PG-based experimental \ufb02ow and the applicability of the scheme for addressing test challenges in large designs by guiding DFT decisions is discussed.", "num_citations": "1\n", "authors": ["530"]}
{"title": "On-line fault resilience through gracefully degradable ASICs\n", "abstract": " We present two novel reconfiguration schemes, L/U reconfiguration and its generalization, band reconfiguration, to achieve graceful degradation for general microarchitecture datapaths. Upon detection of a datapath fault, hardware and algorithmic reconfigurations are performed dynamically through operation rescheduling and hardware rebinding. Instead of a complete shuffling, the proposed scheme perturbs the original schedule and binding in a systematic fashion. This regularity of the scheme allows well-structured design planning for the controller and the datapath. The underlying microarchitecture supporting such reconfiguration schemes is briefly outlined. Experimental evidence indicates negligible performance and small hardware overheads.", "num_citations": "1\n", "authors": ["530"]}
{"title": "High-level synthesis of self-recovering microarchitectures\n", "abstract": " A methodology for the computer aided synthesis of microarchitectures that can recover from transient faults is presented. The synthesis is formulated as a two-step procedure of checkpoint insertion followed by duplication. The checkpoint insertion technique minimizes the voting overhead subject to input constraints on maximum allowable recovery time and the maximum number of retries. Additionally checkpoint insertion is interspersed with the scheduling decisions of a novel edge-based scheduler. Self-recovering microarchitectures which perform optimally but require less than proportional increase in hardware are generated by exploiting cost minimizing transformations.<>", "num_citations": "1\n", "authors": ["530"]}