{"title": "Klee: unassisted and automatic generation of high-coverage tests for complex systems programs.\n", "abstract": " We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage\u2014on average over 90% per tool (median: over 94%)\u2014and significantly beat the coverage of the developers\u2019 own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100% coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.", "num_citations": "3452\n", "authors": ["1864"]}
{"title": "Exokernel: An operating system architecture for application-level resource management\n", "abstract": " Traditional operating systems limit the performance, flexibility, and functionality of applications by fixing the interface and implementation of operating system abstractions such as interprocess communication and virtual memory. The exokernel operating system architecture addresses this problem by providing application-level management of physical resources. In the exokernel architecture, a small kernel securely exports all hardware resources through alowlevel interface to untrusted library operating systems. Library operating systems use this interface to implement system objects and policies. This separation of resource protection from management allows application-specific customization of traditional operating system abstractions by extending, specializing, or even replacing libraries.We have implemented a prototype exokernel operating system. Measurements show that most primitive kernel operations\u00a0\u2026", "num_citations": "1794\n", "authors": ["1864"]}
{"title": "Bugs as deviant behavior: A general approach to inferring errors in systems code\n", "abstract": " A major obstacle to finding program errors in a real system is knowing what correctness rules the system must obey. These rules are often undocumented or specified in an ad hoc manner. This paper demonstrates techniques that automatically extract such checking information from the source code itself, rather than the programmer, thereby avoiding the need for a priori knowledge of system rules.The cornerstone of our approach is inferring programmer \"beliefs\" that we then cross-check for contradictions. Beliefs are facts implied by code: a dereference of a pointer, p, implies a belief that p is non-null, a call to \"unlock(1)\" implies that 1 was locked, etc. For beliefs we know the programmer must hold, such as the pointer dereference above, we immediately flag contradictions as errors. For beliefs that the programmer may hold, we can assume these beliefs hold and use a statistical analysis to rank the resulting errors\u00a0\u2026", "num_citations": "1013\n", "authors": ["1864"]}
{"title": "RacerX: Effective, static detection of race conditions and deadlocks\n", "abstract": " This paper describes RacerX, a static tool that uses flow-sensitive, interprocedural analysis to detect both race conditions and deadlocks. It is explicitly designed to find errors in large, complex multithreaded systems. It aggressively infers checking information such as which locks protect which operations, which code contexts are multithreaded, and which shared accesses are dangerous. It tracks a set of code features which it uses to sort errors both from most to least severe. It uses novel techniques to counter the impact of analysis mistakes. The tool is fast, requiring between 2-14 minutes to analyze a 1.8 million line system. We have applied it to Linux, FreeBSD, and a large commercial code base, finding serious errors in all of them. RacerX is a static tool that uses flow-sensitive, interprocedural analysis to detect both race conditions and deadlocks. It uses novel strategies to infer checking information such as which\u00a0\u2026", "num_citations": "995\n", "authors": ["1864"]}
{"title": "Checking system rules using system-specific, programmer-written compiler extensions\n", "abstract": " Systems software such as OS kernels, embedded systems, and libraries must obey many rules for both correctness and performance. Common examples include accesses to variable A must be guarded by lock B, system calls must check user pointers for validity before using them, and message handlers should free their buffers as quickly as possible to allow greater parallelism. Unfortunately, adherence to these rules is largely unchecked. This paper attacks this problem by showing how system implementors can use meta-level compilation MC to write simple, system-specific compiler extensions that automatically check their code for rule violations. By melding domain-specific knowledge with the automatic machinery of compilers, MC brings the benefits of language-level checking and optimizing to the higher, meta level of the systems implemented in these languages. This paper demonstrates the effectiveness of the MC approach by applying it to four complex, real systems Linux, OpenBSD, the Xok exokernel, and the FLASH machines embedded software. MC extensions found roughly 500 errors in these systems and led to numerous kernel patches. Most extensions were less than a hundred lines of code and written by implementors who had a limited understanding of the systems checked.Descriptors:", "num_citations": "802\n", "authors": ["1864"]}
{"title": "A few billion lines of code later: using static analysis to find bugs in the real world\n", "abstract": " How Coverity built a bug-finding tool, and a business, around the unlimited supply of bugs in software systems.", "num_citations": "730\n", "authors": ["1864"]}
{"title": "A system and language for building system-specific, static analyses\n", "abstract": " This paper presents a novel approach to bug-finding analysis and an implementation of that approach. Our goal is to find as many serious bugs as possible. To do so, we designed a flexible, easy-to-use extension language for specifying analyses and an efficent algorithm for executing these extensions. The language, metal, allows the users of our system to specify a broad class of analyses in terms that resemble the intuitive description of the rules that they check. The system, xgcc, executes these analyses efficiently using a context-sensitive, interprocedural analysis. Our prior work has shown that the approach described in this paper is effective: it has successfully found thousands of bugs in real systems code. This paper describes the underlying system used to achieve these results. We believe that our system is an effective framework for deploying new bug-finding analyses quickly and easily.", "num_citations": "447\n", "authors": ["1864"]}
{"title": "Application performance and flexibility on exokernel systems\n", "abstract": " The exokemel operating system architecture safely gives untrusted software efficient control over hardware and software resources by separating management from protection. This paper describes an exokemel system that allows specialized applications to achieve high performance without sacrificing the performance of unmodified UNIX programs. It evaluates the exokemel architecture by measuring end-to-end application performance on Xok, an exokernel for Intel x86-based computers, and by comparing Xok\u2019s performance to the performance of two widely-used 4.4 BSD UNIX systems (FreeBSD and OpenBSD). The results show that common unmodified UNIX applications can enjoy the benefits of exokernels: applications either perform comparably on Xok/ExOS and the BSD UNIXes, or perform significantly better. In addition, the results show that customized applications can benefit substantially from control\u00a0\u2026", "num_citations": "441\n", "authors": ["1864"]}
{"title": "Using programmer-written compiler extensions to catch security holes\n", "abstract": " This paper shows how system-specific static analysis can find security errors that violate rules such as \"integers from untrusted sources must be sanitized before use\" and \"do not dereference user-supplied pointers.\" In our approach, programmers write system-specific extensions that are linked into the compiler and check their code for errors. We demonstrate the approach's effectiveness by using it to find over 100 security errors in Linux and OpenBSD, over 50 of which have led to kernel patches. An unusual feature of our approach is the use of methods to automatically detect when we miss code actions that should be checked.", "num_citations": "338\n", "authors": ["1864"]}
{"title": "Execution generated test cases: How to make systems code crash itself\n", "abstract": " This paper presents a technique that uses code to automatically generate its own test cases at run time by using a combination of symbolic and concrete (i.e regular) execution The input values to a program (or software component) provide the standard interface of any testing framework with the program it is testing and generating input values that will explore all the \u201cinteresting\u201d behavior in the tested program remains an important open problem in software testing research. Our approach works by turning the problem on its head: we lazily generate from within the program itself the input values to the program (and values derived from input values) as needed. We applied the technique to real code and found numerous corner case errors ranging from simple memory overflows and infinite loops to subtle issues in the interpretation of language standards.", "num_citations": "331\n", "authors": ["1864"]}
{"title": "Archer: using symbolic, path-sensitive analysis to detect memory access errors\n", "abstract": " Memory corruption errors lead to non-deterministic, elusive crashes. This paper describes ARCHER (ARray CHeckER) a static, effective memory access checker. ARCHER uses path-sensitive, interprocedural symbolic analysis to bound the values of both variables and memory sizes. It evaluates known values using a constraint solver at every array access, pointer dereference, or call to a function that expects a size parameter. Accesses that violate constraints are flagged as errors. Those that are exploitable by malicious attackers are marked as security holes. Memory corruption errors lead to non-deterministic, elusive crashes. This paper describes ARCHER (ARray CHeckER) a static, effective memory access checker. ARCHER uses path-sensitive, interprocedural symbolic analysis to bound the values of both variables and memory sizes. It evaluates known values using a constraint solver at every array access\u00a0\u2026", "num_citations": "327\n", "authors": ["1864"]}
{"title": "RWset: Attacking path explosion in constraint-based test generation\n", "abstract": " Recent work has used variations of symbolic execution to automatically generate high-coverage test inputs [3, 4, 7, 8, 14]. Such tools have demonstrated their ability to find very subtle errors. However, one challenge they all face is how to effectively handle the exponential number of paths in checked code. This paper presents a new technique for reducing the number of traversed code paths by discarding those that must have side-effects identical to some previously explored path. Our results on a mix of open source applications and device drivers show that this (sound) optimization reduces the numbers of paths traversed by several orders of magnitude, often achieving program coverage far out of reach for a standard constraint-based execution system.", "num_citations": "313\n", "authors": ["1864"]}
{"title": "DPF: Fast, flexible message demultiplexing using dynamic code generation\n", "abstract": " Fast and flexible message demultiplexing are well-established goals in the networking community [1, 18, 22]. Currently, however, network architects have had to sacrifice one for the other. We present a new packet-filter system, DPF (Dynamic Packet Filters), that provides both the traditional flexibility of packet filters [18] and the speed of hand-crafted demultiplexing routines [3]. DPF filters run 10-50 times faster than the fastest packet filters reported in the literature [1, 17, 18, 27]. DPF's performance is either equivalent to or, when it can exploit runtime information, superior to hand-coded demultiplexors. DPF achieves high performance by using a carefully-designed declarative packet-filter language that is aggressively optimized using dynamic code generation. The contributions of this work are: (1) a detailed description of the DPF design, (2) discussion of the use of dynamic code generation and quantitative results on\u00a0\u2026", "num_citations": "285\n", "authors": ["1864"]}
{"title": "Z-ranking: Using statistical analysis to counter the impact of static analysis approximations\n", "abstract": " This paper explores z-ranking, a technique to rank error reports emitted by static program checking analysis tools. Such tools often use approximate analysis schemes, leading to false error reports. These reports can easily render the error checker useless by hiding real errors amidst the false, and by potentially causing the tool to be discarded as irrelevant. Empirically, all tools that effectively find errors have false positive rates that can easily reach 30\u2013100%. Z-ranking employs a simple statistical model to rank those error messages most likely to be true errors over those that are least likely. This paper demonstrates that z-ranking applies to a range of program checking problems and that it performs up to an order of magnitude better than randomized ranking. Further, it has transformed previously unusable analysis tools into effective program error finders.", "num_citations": "235\n", "authors": ["1864"]}
{"title": "VCODE: a retargetable, extensible, very fast dynamic code generation system\n", "abstract": " Dynamic code generation is the creation of executable code at runtime. Such \"on-the-fly\" code generation is a powerful technique, enabling applications to use runtime information to improve performance by up to an order of magnitude [4, 8,20, 22, 23].Unfortunately, previous general-purpose dynamic code generation systems have been either inefficient or non-portable. We present VCODE, a retargetable, extensible, very fast dynamic code generation system. An important feature of VCODE is that it generates machine code \"in-place\" without the use of intermediate data structures. Eliminating the need to construct and consume an intermediate representation at runtime makes VCODE both efficient and extensible. VCODE dynamically generates code at an approximate cost of six to ten instructions per generated instruction, making it over an order of magnitude faster than the most efficient general-purpose code\u00a0\u2026", "num_citations": "233\n", "authors": ["1864"]}
{"title": "Under-constrained symbolic execution: Correctness checking for real code\n", "abstract": " Software bugs are a well-known source of security vulnerabilities. One technique for finding bugs, symbolic execution, considers all possible inputs to a program but suffers from scalability limitations. This paper uses a variant, under-constrained symbolic execution, that improves scalability by directly checking individual functions, rather than whole programs. We present UC-KLEE, a novel, scalable framework for checking C/C++ systems code, along with two use cases. First, we use UC-KLEE to check whether patches introduce crashes. We check over 800 patches from BIND and OpenSSL and find 12 bugs, including two OpenSSL denial-of-service vulnerabilities. We also verify (with caveats) that 115 patches do not introduce crashes. Second, we use UC-KLEE as a generalized checking framework and implement checkers to find memory leaks, uninitialized data, and unsafe user input. We evaluate the checkers on over 20,000 functions from BIND, OpenSSL, and the Linux kernel, find 67 bugs, and verify that hundreds of functions are leak free and that thousands of functions do not access uninitialized data.", "num_citations": "189\n", "authors": ["1864"]}
{"title": "Exterminate all operating system abstractions\n", "abstract": " The defining tragedy of the operating systems community has been the definition of an operating system as software that both multiplexes and abstracts physical resources. The view that the OS should abstract the hardware is based on the assumption that it is possible bath to define abstractions that are appropriate for all areas and to implement them to perform efficiently in all situations. We believe that the fallacy of this quixotic goal is self-evident, and that the operating system problems of the last two decades (poor performance, poor reliability, poor adaptability, and inflexibility) can be traced back to it. The solution we propose is simple: complete elimination of operating system abstractions by lowering the operating system interface to the hardware level.", "num_citations": "187\n", "authors": ["1864"]}
{"title": "From uncertainty to belief: Inferring the specification within\n", "abstract": " Automatic tools for finding software errors require a set of specifications before they can check code: if they do not know what to check, they cannot find bugs. This paper presents a novel framework based on factor graphs for automatically inferring specifications directly from programs. The key strength of the approach is that it can incorporate many disparate sources of evidence, allowing us to squeeze significantly more information from our observations than previously published techniques.We illustrate the strengths of our approach by applying it to the problem of inferring what functions in C programs allocate and release resources. We evaluated its effectiveness on five codebases: SDL, OpenSSH, GIMP, and the OS kernels for Linux and Mac OS X (XNU). For each codebase, starting with zero initially provided annotations, we observed an inferred annotation accuracy of 80-90%, with often near perfect accuracy for functions called as little as five times. Many of the inferred allocator and deallocator functions are functions for which we both lack the implementation and are rarely called\u2014in some cases functions with at most one or two callsites. Finally, with the inferred annotations we quickly found both missing and incorrect properties in a specification used by a commercial static bug-finding tool.", "num_citations": "184\n", "authors": ["1864"]}
{"title": "C: A language for high-level, efficient, and machine-independent dynamic code generation\n", "abstract": " Dynamic code generation allows specialized code sequences to be created using runtime information. Since this information is by definition not available statically, the use of dynamic code generation can achieve performance inherently beyond that of static code generation. Previous attempts to support dynamic code generation have been low-level, expensive, or machine-dependent. Despite the growing use of dynamic code generation, no mainstream language provides flexible, portable, and efficient support for it. We describe'C (Tick C), a superset of ANSI C that allows flexible, high-level. efficient, and machine-independent specification of dynamically generated code.'C provides many of the performance benefits of pure partial evaluation, but in the context of a complex, statically typed, but widely used language.'C examples illustrate the ease of specifying dynamically generated code and how it can be put to\u00a0\u2026", "num_citations": "174\n", "authors": ["1864"]}
{"title": "C and tcc: a language and compiler for dynamic code generation\n", "abstract": " Dynamic code generation allows programmers to use run-time information in order to achieve performance and expressiveness superior to those of static code. The 'C(Tick C) language is a superset of ANSI C that supports efficient and high-level use of dynamic code generation. 'C provides dynamic code generation at the level of C expressions and statements and supports the composition of dynamic code at run time. These features enable programmers to add dynamic code generation to existing C code incrementally and to write important applications (such as \u201cjust-in-time\u201d compilers) easily. The article presents many examples of how 'C can be used to solve practical problems. The tcc compiler is an efficient, portable, and freely available implementation of 'C.  tcc allows programmers to trade dynamic compilation speed for dynamic code quality: in some aplications, it is most important to generate code quickly\u00a0\u2026", "num_citations": "167\n", "authors": ["1864"]}
{"title": "Practical, low-effort equivalence verification of real code\n", "abstract": " Verifying code equivalence is useful in many situations, such as checking: yesterday\u2019s code against today\u2019s, different implementations of the same (standardized) interface, or an optimized routine against a reference implementation. We present a tool designed to easily check the equivalence of two arbitrary C functions. The tool provides guarantees far beyond those possible with testing, yet it often requires less work than writing even a single test case. It automatically synthesizes inputs to the routines and uses bit-accurate, sound symbolic execution to verify that they produce equivalent outputs on a finite number of paths, even for rich, nested data structures. We show that the approach works well, even on heavily-tested code, where it finds interesting errors and gets high statement coverage, often exhausting all feasible paths for a given input size. We also show how the simple trick of checking equivalence\u00a0\u2026", "num_citations": "148\n", "authors": ["1864"]}
{"title": "DCG: An efficient, retargetable dynamic code generation system\n", "abstract": " Dynamic code generation allows aggressive optimization through the use of runtime information. Previous systems typically relied on ad hoc code generators that were not designed for retargetability, and did not shield the client from machine-specific details. We present a system, dcg, that allows clients to specify dynamically generated code in a machine-independent manner. Our one-pass code generator is easily retargeted and extremely efficient (code generation costs approximately 350 instructions per generated instruction). Experiments show that dynamic code generation increases some application speeds by over an order of magnitude.", "num_citations": "126\n", "authors": ["1864"]}
{"title": "The operating system kernel as a secure programmable machine\n", "abstract": " To provide modularity and performance, operating system kernels should have only minimal embedded functionality. Today's operating systems are large, inefficient and, most importantly, inflexible. In our view, most operating system performance and flexibility problems can be eliminated simply by pushing the operating system interface lower. Our goal is to put abstractions traditionally implemented by the kernel out into user-space, where user-level libraries and servers abstract the exposed hardware resources. To achieve this goal, we have defined a new operating system structure, exokernel, that safely exports the resources defined by the underlying hardware. To enable applications to benefit from full hardware functionality and performance, they are allowed to download additions to the supervisor-mode execution environment. To guarantee that these extensions are safe, techniques such as code inspection\u00a0\u2026", "num_citations": "121\n", "authors": ["1864"]}
{"title": "tcc: A system for fast, flexible, and high-level dynamic code generation\n", "abstract": " tcc is a compiler that provides efficient and high-level access to dynamic code generation. It implements the 'C (\"Tick-C\") programming language, an extension of ANSI C that supports dynamic code generation [15]. 'C gives power and flexibility in specifying dynamically generated code: whereas most other systems use annotations to denote run-time invariants. 'C allows the programmer to specify and compose arbitrary expressions and statements at run time. This degree of control is needed to efficiently implement some of the most important applications of dynamic code generation, such as \"just in time\" compilers [17] and efficient simulators [10, 48, 46].The paper focuses on the techniques that allow tcc to provide 'C's flexibility and expressiveness without sacrificing run-time code generation efficiency. These techniques include fast register allocation, efficient creation and composition of dynamic code\u00a0\u2026", "num_citations": "109\n", "authors": ["1864"]}
{"title": "The Exokernel operating system architecture\n", "abstract": " On traditional operating systems only trusted software such as privileged servers or the kernel can manage resources. This thesis proposes a new approach, the exokernel architecture, which makes resource management unprivileged but safe by separating management from protection: an exokernel protects resources, while untrusted application-level software manages them. As a result, in an exokernel system, untrusted software (e.g., library operating systems) can implement abstractions such as virtual memory, file systems, and networking. Themain thrusts of this thesis are: (1) how to build an exokernel system; (2) whether it is possible to build a real one; and (3) whether doing so is a good idea. Our results, drawn from two exokernel systems [25, 48], show that the approach yields dramatic benefits. For example, Xok, an exokernel, runs a web server an order of magnitude faster than the closest equivalent on the same hardware, common unaltered Unix applications up to three times faster, and improves global system performance up to a factor of five. The thesis also discusses some of the new techniques we have used to remove the overhead of protection. Themost unusual technique, untrusted deterministic functions, enables an exokernel to verify that applications correctly track the resources they own, eliminating the need for it to do so. Additionally, the thesis reflects on the subtle issues in using downloaded code for extensibility and the sometimes painful lessons learned in building three exokernel-based systems.", "num_citations": "108\n", "authors": ["1864"]}
{"title": "ASHs: Application-specific handlers for high-performance messaging\n", "abstract": " Application-specific safe message handlers (ASHs) are designed to provide applications with hardware-level network performance. ASHs are user-written code fragments that safely and efficiently execute in the kernel in response to message arrival. ASHs can direct message transfers (thereby eliminating copies) and send messages (thereby reducing send-response latency). In addition, the ASH system provides support for dynamic integrated layer processing (thereby eliminating duplicate message traversals) and dynamic protocol composition (thereby supporting modularity). ASHs provide this high degree of flexibility while still providing network performance as good as, or (if they exploit application-specific knowledge) even better than, hard-wired in-kernel implementations. A combination of user-level microbenchmarks and end-to-end system measurements using TCP demonstrate the benefits of the ASH system.", "num_citations": "99\n", "authors": ["1864"]}
{"title": "Server operating systems\n", "abstract": " We introduce server operating systems, which are sets of abstractions and runtime support for specialized, high-performance server applications. We have designed and are implementing a prototype server OS with support for aggressive specialization, direct device-to-device access, an event-driven organization, and dynamic compiler-assisted ILP. Using this server OS, we have constructed an HTTP server that outperforms servers running on a conventional OS by more than an order of magnitude and that can safely timeshare the hardware platform with other applications.", "num_citations": "97\n", "authors": ["1864"]}
{"title": "CDE: Using System Call Interposition to Automatically Create Portable Software Packages.\n", "abstract": " It can be painfully hard to take software that runs on one person\u2019s machine and get it to run on another machine. Online forums and mailing lists are filled with discussions of users\u2019 troubles with compiling, installing, and configuring software and their myriad of dependencies. To eliminate this dependency problem, we created a system called CDE that uses system call interposition to monitor the execution of x86-Linux programs and package up the Code, Data, and Environment required to run them on other x86-Linux machines. Creating a CDE package is completely automatic, and running programs within a package requires no installation, configuration, or root permissions. Hundreds of people in both academia and industry have used CDE to distribute software, demo prototypes, make their scientific experiments reproducible, run software natively on older Linux distributions, and deploy experiments to compute clusters.", "num_citations": "92\n", "authors": ["1864"]}
{"title": "Fast and flexible application-level networking on exokernel systems\n", "abstract": " Application-level networking is a promising software organization for improving performance and functionality for important network services. The Xok/ExOS exokernel system includes application-level support for standard network services, while at the same time allowing application writers to specialize networking services. This paper describes how Xok/ExOS's kernel mechanisms and library operating system organization achieve this flexibility, and retrospectively shares our experiences and lessons learned (both positive and negative). It also describes how we used this flexibility to build and specialize three network data services: the Cheetah HTTP server, the webswamp Web benchmarking tool, and an application-level TCP forwarder. Overall measurements show large performance improvements relative to similar services built on conventional interfaces, in each case reaching the maximum possible end-to\u00a0\u2026", "num_citations": "92\n", "authors": ["1864"]}
{"title": "AVM: Application-level virtual memory\n", "abstract": " Virtual memory (VM) is a notoriously complicated abstraction to implement, and is hard to change, specialize, or replace. Although a certain degree of flexibility is achieved by user-level pagers, the control they provide is limited: they leave much of the VM system fixed in the kernel, unreachable by the application. As applications become more diverse and the opportunity cost of bad memory policies grows, it is essential for applications to have more control over the VM abstraction. We motivate and describe a VM system that is implemented completely at the application level. To the best of our knowledge this system is the first complete example of application-level virtual memory (AVM). AVM allows applications to easily specialize, modify, or even replace the VM abstractions offered. For example, on architectures with software TLB management, applications can even select their own page-table structures. In\u00a0\u2026", "num_citations": "83\n", "authors": ["1864"]}
{"title": "A randomized web-cache replacement scheme\n", "abstract": " The problem of document replacement in Web caches has received much attention in the literature research, and it has been shown that the eviction rule \"replace the least recently used document\" performs poorly in Web caches. Instead, it has been shown that using a combination of several criteria, such as the recentness and frequency of use, the size, and the cost of fetching a document, leads to a sizeable improvement in hit rate and latency reduction. However, in order to implement these novel schemes, one needs to maintain complicated data structures. We propose randomized algorithms for approximating any existing Web-cache replacement scheme and thereby avoid the need for any data structures. At document-replacement times, the randomized algorithm samples N documents from the cache and replaces the least useful document from the sample, where usefulness is determined according to the\u00a0\u2026", "num_citations": "82\n", "authors": ["1864"]}
{"title": "Under-constrained execution: making automatic code destruction easy and scalable\n", "abstract": " Software testing is well-recognized as a crucial part of the modern software development process. However, manual testing is labor intensive and often fails to produce impressive coverage results. Random testing is easily applied but gets poor coverage on complex code. Recent work has attacked these problems using symbolic execution to automatically generate high-coverage test inputs [3, 6, 4, 8, 5, 2]. At a high-level these tools use variations on the following idea. Instead of running code on manually or randomly constructed input, they run it on symbolic input initially allowed to be \u201canything.\u201d They substitute program variables with symbolic values and replaces concrete program operations with ones that manipulate symbolic values. When program execution branches based on a symbolic value the system (conceptually) follows both branches at once, maintaining a set of constraints called the path condition\u00a0\u2026", "num_citations": "70\n", "authors": ["1864"]}
{"title": "Using redundancies to find errors\n", "abstract": " This paper explores the idea that redundant operations, like type errors, commonly flag correctness errors. We experimentally test this idea by writing and applying four redundancy checkers to the Linux operating system, finding many errors. We then use these errors to demonstrate that redundancies, even when harmless, strongly correlate with the presence of traditional hard errors (eg, null pointer dereferences, unreleased locks). Finally we show that how flagging redundant operations gives a way to make specifications\" fail stop\" bydetecting dangerous omissions.", "num_citations": "61\n", "authors": ["1864"]}
{"title": "A simple method for extracting models from protocol code\n", "abstract": " The use of model checking for validation requires that models of the underlying system be created. Creating such models is both difficult and error prone and as a result, verification is rarely used despite its advantages. In this paper we present a method for automatically extracting models from low level software implementations. Our method is based on the use of an extensible compiler system, xg++, to perform the extraction. The extracted model is combined with a model of the hardware, a description of correctness, and an initial state. The whole model is then checked with the Mur/spl phi/ model checker. As a case study, we apply our method to the cache coherence protocols of the Stanford FLASH multiprocessor. Our system has a number of advantages. First, it reduces the cost of creating models, which allows model checking to be used more frequently. Second, it increases the effectiveness of model checking\u00a0\u2026", "num_citations": "61\n", "authors": ["1864"]}
{"title": "Incorporating Application Semantics and Control into Compilation.\n", "abstract": " Programmers have traditionally been passive users of compilers, rather than active exploiters of their transformational abilities. This paper presents Magik, a system that allows programmers to easily and modularly incorporate application-specific extensions into the compilation process. The Magik system gives programmers two significant capabilities. First, it provides mechanisms that implementors can use to incorporate application semantics into compilation, thereby enabling both optimizations and semantic checking impossible by other means. Second, since extensions are invoked during the translation from source to machine code, code transformations (such as software fault isolation [14]) can be performed with full access to the symbol and data flow information available to the compiler proper, allowing them both to exploit source semantics and to have their transformations (automatically) optimized as any other code.", "num_citations": "58\n", "authors": ["1864"]}
{"title": "Using automatic persistent memoization to facilitate data analysis scripting\n", "abstract": " Programmers across a wide range of disciplines (eg, bioinformatics, neuroscience, econometrics, finance, data mining, information retrieval, machine learning) write scripts to parse, transform, process, and extract insights from data. To speed up iteration times, they split their analyses into stages and write extra code to save the intermediate results of each stage to files so that those results do not have to be re-computed in every subsequent run. As they explore and refine hypotheses, their scripts often create and process lots of intermediate data files. They need to properly manage the myriad of dependencies between their code and data files, or else their analyses will produce incorrect results.", "num_citations": "49\n", "authors": ["1864"]}
{"title": "Redundant state detection for dynamic symbolic execution\n", "abstract": " Many recent tools use dynamic symbolic execution to perform tasks ranging from automatic test generation, finding security flaws, equivalence verification, and exploit generation. However, while symbolic execution is promising, it perennially struggles with the fact that the number of paths in a program increases roughly exponentially with both code and input size. This paper presents a technique that attacks this problem by eliminating paths that cannot reach new code before they are executed and evaluates it on 66 system intensive, complicated, and widely-used programs. Our experiments demonstrate that the analysis speeds up dynamic symbolic execution by an average of 50.5 X, with a median of 10 X, and increases coverage by an average of 3.8%.", "num_citations": "48\n", "authors": ["1864"]}
{"title": "How to write system-specific, static checkers in metal\n", "abstract": " This paper gives an overview of the metal language, which we have designed to make it easy to construct systemspecific, static analyses. We call these analyses extensions because they act as the input to a generic analysis engine that runs the static analysis over a given source base. We also interchangeably refer to them as checkers because they check that a user-specified property holds in the source base and report any violations of that property. Note that checkers may not detect all violations of a specified property. Their goal is to find as many violations as possible with a minimum of false positives. We describe seven checkers in total; most are less than 50 lines of code, but, in aggregate, find hundreds of errors in a typical large system. In addition to language details, we give a feel for how to build good checkers: exploiting highlevel knowledge, ranking errors, suppressing false positives. Further, we show\u00a0\u2026", "num_citations": "48\n", "authors": ["1864"]}
{"title": "Using redundancies to find errors\n", "abstract": " Programmers generally attempt to perform useful work. If they performed an action, it was because they believed it served some purpose. Redundant operations violate this belief. However, in the past, redundant operations have been typically regarded as minor cosmetic problems rather than serious errors. This paper demonstrates that, in fact, many redundancies are as serious as traditional hard errors (such as race conditions or null pointer dereferences). We experimentally test this idea by writing and applying five redundancy checkers to a number of large open source projects, finding many errors. We then show that, even when redundancies are harmless, they strongly correlate with the presence of traditional hard errors. Finally, we show how flagging redundant operations gives a way to detect mistakes and omissions in specifications. For example, a locking specification that binds shared variables to their\u00a0\u2026", "num_citations": "46\n", "authors": ["1864"]}
{"title": "Linux Kernel Developer Responses to Static Analysis Bug Reports.\n", "abstract": " We present a study of how Linux kernel developers respond to bug reports issued by a static analysis tool. We found that developers prefer to triage reports in younger, smaller, and more actively-maintained files (\u00a7 2), first address easy-to-fix bugs and defer difficult (but possibly critical) bugs (\u00a7 3), and triage bugs in batches rather than individually (\u00a7 4). Also, although automated tools cannot find many types of bugs, they can be effective at directing developers' attentions towards parts of the codebase that contain up to 3X more user-reported bugs (\u00a7 5).", "num_citations": "45\n", "authors": ["1864"]}
{"title": "Reverse-Engineering Instruction Encodings.\n", "abstract": " Binary tools such as disassemblers, just-in-time compilers, and executable code rewriters need to have an explicit representation of how machine instructions are encoded. Unfortunately, writing encodings for an entire instruction set by hand is both tedious and error-prone. We describe DERIVE, a tool that extracts bit-level instruction encoding information from assemblers. The user provides DERIVE with assembly-level information about various instructions. DERIVE automatically reverse-engineers the encodings for those instructions from an assembler by feeding it permutations of instructions and analyzing the resulting machine code. DERIVE solves the entire MIPS, SPARC, Alpha, and PowerPC instruction sets, and almost all of the ARM and x86 instruction sets. Its output consists of C declarations that can be used by binary tools. To demonstrate the utility of DERIVE, we have built a code emitter generator that takes DERIVE\u2019s output and produces C macros for code emission, which we have then used to rewrite a Java JIT backend.", "num_citations": "43\n", "authors": ["1864"]}
{"title": "How to build static checking systems using orders of magnitude less code\n", "abstract": " Modern static bug finding tools are complex. They typically consist of hundreds of thousands of lines of code, and most of them are wedded to one language (or even one compiler). This complexity makes the systems hard to understand, hard to debug, and hard to retarget to new languages, thereby dramatically limiting their scope. This paper reduces checking system complexity by addressing a fundamental assumption, the assumption that checkers must depend on a full-blown language specification and compiler front end. Instead, our program checkers are based on drastically incomplete language grammars (\" micro-grammars\") that describe only portions of a language relevant to a checker. As a result, our implementation is tiny-roughly 2500 lines of code, about two orders of magnitude smaller than a typical system. We hope that this dramatic increase in simplicity will allow people to use more checkers on\u00a0\u2026", "num_citations": "38\n", "authors": ["1864"]}
{"title": "Using meta-level compilation to check FLASH protocol code\n", "abstract": " Building systems such as OS kernels and embedded software is difficult. An important source of this difficulty is the numerous rules they must obey: interrupts cannot be disabled for ~too long,\" global variables must be protected by locks, user pointers passed to OS code must be checked for safety before use, etc. A single violation can crash the system, yet typically these invariants are unchecked, existing only on paper or in the implementor's mind.This paper is a case study in how system implementors can use a new programming methodology, meta-level compilation (MC), to easily check such invariants. It focuses on using MC to check for errors in the code used to manage cache coherence on the FLASH shared memory multiprocessor. The only real practical method known for verifying such code is testing and simulation. We show that simple, system-specific checkers can dramatically improve this situation by\u00a0\u2026", "num_citations": "36\n", "authors": ["1864"]}
{"title": "Interface compilation: Steps toward compiling program interfaces as languages\n", "abstract": " Interfaces-the collection of procedures and data structures that define a library, a subsystem, a module-are syntactically poor programming languages. They have state (defined both by the interface's data structures and internally), operations on this state (defined by the interface's procedures), and semantics associated with these operations. Given a way to incorporate interface semantics into compilation, interfaces can be compiled in the same manner as traditional languages such as ANSI C or FORTRAN. The article makes two contributions. First, it proposes and explores the metaphor of interface compilation, and provides the beginnings of a programming methodology for exploiting it. Second, it presents MAGIK, a system built to support interface compilation. Using MAGIK, software developers can build optimizers and checkers for their interface languages, and have these extensions incorporated into\u00a0\u2026", "num_citations": "34\n", "authors": ["1864"]}
{"title": "Filaments: Efficient support for fine-grain parallelism\n", "abstract": " It has long been thought that coarse-grain parallelism is much more efficient than fine-grain parallelism due to the overhead of process (thread) creation, context switching, and synchronization. On the other hand, there are several advantages to fine-grain parallelism: architecture independence, ease of programming, ease of use as a target for code generation, and load-balancing potential. This paper describes a portable threads package, Filaments, that supports efficient execution of fine-grain parallel programs on shared-memory multiprocessors. Filaments supports three kinds of threads\u2014run-to-completion, barrier (iterative), and fork/join\u2014which appear to be sufficient for scientific computations. Filaments employs a unique combination of techniques to achieve efficiency: stateless threads, very small thread descriptors, optimized barrier synchronization, scheduling that enhances data locality, and automatic pruning of fork/join threads. The gains in performance are such that on an application such as Jacobi iteration, the execution time for a fine-grain program with a worst-case granularity of a thread per point can be within 10% of that for a coarse-grain program with only one task per processor. Execution times for problems with more work per thread are usually indistinguishable from coarse-grain programs, and they can be faster when the amount of work per thread varies.", "num_citations": "34\n", "authors": ["1864"]}
{"title": "A Factor Graph Model for Software Bug Finding.\n", "abstract": " Automatic tools for finding software errors require knowledge of the rules a program must obey, or \u201cspecifications,\u201d before they can identify bugs. We present a method that combines factor graphs and static program analysis to automatically infer specifications directly from programs. We illustrate the approach on inferring functions in C programs that allocate and release resources, and evaluate the approach on three codebases: SDL, OpenSSH, and the OS kernel for Mac OS X (XNU). The inferred specifications are highly accurate and with them we have discovered numerous bugs.", "num_citations": "31\n", "authors": ["1864"]}
{"title": "Uprooting Software Defects at the Source: Source code analysis is an emerging technology in the software industry that allows critical source code defects to be detected before\u00a0\u2026\n", "abstract": " Although the concept of detecting programming errors at compile time is not new, the technology to build effective tools that can process millions of lines of code and report substantive defects with only a small amount of noise has long eluded the market. At the same time, a different type of solution is needed to combat current trends in the software industry that are steadily diminishing the effectiveness of conventional software testing and quality assurance.", "num_citations": "31\n", "authors": ["1864"]}
{"title": "Towards practical incremental recomputation for scientists\n", "abstract": " 1. Write simple first version of script 2. Execute and wait for 1 hour to get results 3. Interpret results and notice a bug 4. Edit script slightly to fix that bug 5. Re\u2010execute and wait for a few seconds 6. Enhance script with new functions 7. Re\u2010execute and wait for a few minutes", "num_citations": "29\n", "authors": ["1864"]}
{"title": "tcc: A templatebased compiler for \u2018c\n", "abstract": " Dynamic code generation is an important technique for improving the performance of software by exploiting information known only at run time.\u2018C (Tick C) is a superset of ANSI C that, unlike most prior systems, allows high-level, efficient, and machineindependent specification of dynamically generated code.\u2018C provides facilities for dynamic code generation within the context of a statically typed, imperative language closely related to the language most widely used in systems development. This paper describes tcc, a compiler currently being written for \u2018C. tcc has two objectives:(1) to deliver a complete, solid implementation of \u2018C, and (2) to minimize the run-time costs of dynamic code generation. tcc implements dynamic code generation by emitting templates, segments of binary code which at run time can be combined and completed with the values of registers, stack offsets, and constants. tcc also allows some decisions about storage allocation and instruction selection to occur at run time. This provides flexibility in combining arbitrary pieces of dynamic code, while allowing run-time code generation to occur very efficiently.", "num_citations": "29\n", "authors": ["1864"]}
{"title": "Expression reduction from programs in a symbolic binary executor\n", "abstract": " Symbolic binary execution is a dynamic analysis method which explores program paths to generate test cases for compiled code. Throughout execution, a program is evaluated with a bit-vector theorem prover and a runtime interpreter as a mix of symbolic expressions and concrete values. Left untended, these symbolic expressions grow to negatively impact interpretation performance.             We describe an expression reduction system which recovers sound, context-insensitive expression reduction rules at run time from programs during symbolic evaluation. These rules are further refined offline into general rules which match larger classes of expressions. We demonstrate that our optimizer significantly reduces the number of theorem solver queries and solver time on hundreds of commodity programs compared to a default ad-hoc optimizer from a popular symbolic interpreter.", "num_citations": "20\n", "authors": ["1864"]}
{"title": "MJ-a system for constructing bug-finding analyses for Java\n", "abstract": " Many software defects result from the violation of programming rules: rules that describe how to use a programming language and its libraries and rules that describe the dos and don\u2019ts within a given application, library or system. MJ is a language and an engine that can succinctly express many of these rules for programs written in Java. MJ programs are checkers that are compiled into compiler extensions. A static analysis engine applies the extensions to user code and flags rule violations. We have implemented and tested several extensions in MJ for both general and application-specific rules. Our checkers have found dozens of bugs in some widely-deployed and mature software systems.", "num_citations": "14\n", "authors": ["1864"]}
{"title": "Efficient, safe, application-specific message processing\n", "abstract": " The complexity and ambition of applications increase commensurately with improvements in processing power and network performance. Unfortunately, although raw CPU and networking hardware speeds have increased, end-to-end network performance has lagged. This disparity is due both to the imbalance between memory subsystems and modern CPUs and to the inefficiency of current networking software. These limits on application performance will continue to increase [3, 4, 6]. This paper addresses the important problem of delivering network performance to applications by separating message demultiplexing (determining \u201cwho\u201d a message is for), message vectoring (determining where to put the message), and message handling (running applicationspecific operations on the message). We introduce dynamic packet filters (DPF), which allow packet filters to be executed safely and efficiently in the kernel, thereby reducing the cost of demultiplexing messages (see Fig 1). In addition, DPF allows application-specific safe message handlers (ASHs) to be associated with filters. These handlers are made safe by controlling their operations and bounding their runtime.", "num_citations": "12\n", "authors": ["1864"]}
{"title": "Static analysis versus model checking for bug finding\n", "abstract": " This talk tries to distill several years of experience using both model checking and static analysis to find errors in large software systems. We initially thought that the tradeoffs between the two was clear: static analysis was easy but would mainly find shallow bugs, while model checking would require more work but would be strictly better \u2014 it would find more errors, the errors would be deeper and the approach would be more powerful. These expectations were often wrong. This talk will describe some of the sharper tradeoffs between the two, as well as a detailed discussion of one domain \u2014 finding errors in file systems code \u2014 where model checking seems to work very well.", "num_citations": "11\n", "authors": ["1864"]}
{"title": "Design and implementation of a modular, flexible, and fast system for dynamic protocol composition\n", "abstract": " Distributed systems must communicate. To communicate at all requires high-level protocols be built with manageable complexity. To communicate well requires protocols efficient both in design and implementation. The ASH system provides mechanisms to address both of these needs. To manage complexity, it provides a simple interface that allows protocols to be dynamically composed in a modular manner. As a result, the complexity of building a high-level messaging service is reduced since it can be built from multiple, independent pieces at runtime. To provide efficiency, the ASH interface is designed so that the message processing steps of each protocol (such as checksumming, byteswapping, encryption, etc.) can be dynamically integrated into a single specialized loop that touches each byte of the message at most once. The ASH system is the first to dynamically integrate the data processing elements of each protocol. This ability is crucial since without it dynamic protocol composition cannot be efficient. This paper presents the design and implementation of the ASH system. It is interesting because it uses modern compilation techniques to eliminate the runtime overhead of modularity. To make the system efficient we have had to solve problems ranging from theoretical graph algorithms, to language design, to determining the most efficient way to access unaligned memory, and designing efficient and portable methods of generating executable code at runtime. As a result of careful design and the the use of dynamic code generation to specialize data processing paths, the ASH system provides performance that is comparable to (and in\u00a0\u2026", "num_citations": "11\n", "authors": ["1864"]}
{"title": "A randomized cache replacement scheme approximating LRU\n", "abstract": " A randomized algorithm is proposed for approximating the Least Recently Used (LRU) scheme for page replacement in caches. In its basic version the proposed algorithm performs as follows: When a new page is to be evicted from the cache, the algorithm randomly samples N pages from the cache and replaces the least recently used page from the sample. We then study the following enhancement of the basic version: After replacing the least recently used page from the sample, the next M< N least recently used pages are retained for the next iteration. And when the next replacement is to be performed, the algorithm obtains NM new samples from the cache, and replaces the least recently used page from the NM new samples and the M previously retained. Both the basic and enhanced versions perform very well compared to existing random replacement schemes. Rather surprisingly, we nd that the enhanced\u00a0\u2026", "num_citations": "10\n", "authors": ["1864"]}
{"title": "Sys: a static/symbolic tool for finding good bugs in good (browser) code\n", "abstract": " We describe and evaluate an extensible bug-finding tool, Sys, designed to automatically find security bugs in huge codebases, even when easy-to-find bugs have been already picked clean by years of aggressive automatic checking. Sys uses a two-step approach to find such tricky errors. First, it breaks down large\u2014tens of millions of lines\u2014systems into small pieces using user-extensible static checkers to quickly find and mark potential errorsites. Second, it uses user-extensible symbolic execution to deeply examine these potential errorsites for actual bugs. Both the checkers and the system itself are small (6KLOC total). Sys is flexible, because users must be able to exploit domain-or system-specific knowledge in order to detect errors and suppress false positives in real codebases. Sys finds many security bugs (51 bugs, 43 confirmed) in well-checked code\u2014the Chrome and Firefox web browsers\u2014and code that some symbolic tools struggle with\u2014the FreeBSD operating system. Sys\u2019s most interesting results include: an exploitable, cash bountied CVE in Chrome that was fixed in seven hours (and whose patch was backported in two days); a trio of bountied bugs with a CVE in Firefox; and a bountied bug in Chrome\u2019s audio support.", "num_citations": "9\n", "authors": ["1864"]}
{"title": "DERIVE: A tool that automatically reverse-engineers instruction encodings\n", "abstract": " Many binary tools, such as disassemblers, dynamic code generation systems, and executable code rewriters, need to understand how machine instructions are encoded. Unfortunately, specifying such encodings is tedious and error-prone. Users must typically specify thousands of details of instruction layout, such as opcode and field locations values, legal operands, and jump offset encodings. We have built a tool called DERIVE that extracts these details from existing software: the system assembler. Users need only provide the assembly syntax for the instructions for which they want encodings. DERIVE automatically reverse-engineers instruction encoding knowledge from the assembler by feeding it permutations of instructions and doing equation solving on the output. DERIVE is robust and general. It derives instruction encodings for SPARC, MIPS, Alpha, PowerPC, ARM, and x86. In the last case, it handles\u00a0\u2026", "num_citations": "9\n", "authors": ["1864"]}
{"title": "ASHs: application-specific handlers for high-performance messaging\n", "abstract": " Application-specific safe message handlers (ASHs) are designed to provide applications with hardware-level network performance. ASHs are user-written code fragments that safely and efficiently execute in the kernel in response to message arrival. ASHs can direct message transfers (thereby eliminating copies) and send messages (thereby reducing send-response latency). In addition, the ASH system provides support for dynamic integrated layer processing (thereby eliminating duplicate message traversals) and dynamic protocol composition (thereby supporting modularity). ASHs offer this high degree of flexibility while still providing network performance as good as, or (if they exploit application-specific knowledge) even better than, hard-wired in-kernel implementations. A combination of user-level microbenchmarks and end-to-end system measurements using TCP demonstrates the benefits of the ASH system.", "num_citations": "9\n", "authors": ["1864"]}
{"title": "Vcode: a portable, very fast dynamic code generation system\n", "abstract": " We present vcode, a portable, extensible, very fast dynamic code generation system. An important feature of vcode is that it generates machine code``in-place''without the use of intermediate data structures. Eliminating the need to construct and consume an intermediate representation at runtime makes vcode both efficient and extensible. vcode dynamically generates code at an approximate cost of six to ten instructions per generated instruction, making it over an order of magnitude faster than the most efficient general-purpose code generation system in the literature.", "num_citations": "9\n", "authors": ["1864"]}
{"title": "The design and implementation of a prototype exokernel operating system\n", "abstract": " Traditional operating systems abstract hardware resources. The cost of providing these abstractions is high: operating systems are typically unreliable, complex, and inflexible. More importantly, applications that are built on these systems are both inefficient and limited in scope: most resource management decisions cannot be made by application designers, leaving them with little recourse when the management of these resources is inappropriate. We believe that the quest to abstract physical resources has sharply limited the flexibility and ambition of applications.In this thesis, we propose a new operating system structure, exokernel, which is built on the premise that an operating system should not abstraction physical resources. Rather, operating systems should concentrate solely on multiplexing the raw hardware: from these hardware primitives, application-level libraries and servers can directly implement traditional operating system abstractions, specialized for appropriateness and speed. This thesis motivates the need for a new operating system structure, provides a set of precepts to guide its design, discusses general issues that exokernels must deal with in multiplexing physical hardware, and describes and measures a prototype exokernel system. In many cases this prototype system is an order of magnitude more efficient than a traditional operating system. Furthermore, it supports a degree of flexibility not attained in any operating system. For example, virtual memory is efficiently implemented entirely at the application level.", "num_citations": "9\n", "authors": ["1864"]}
{"title": "Performance experiments for the Filaments package\n", "abstract": " Ten representative benchmarks were run on two shared-memory multiprocessors using an e cient, ne-grain threads package called Filaments. This paper describes the implementation and performance of the applications and compares them to both coarse-grain and sequential counterparts. It also analyzes the results and explains why the ne-grain programs were faster or slower than the coarse-grain ones.", "num_citations": "9\n", "authors": ["1864"]}
{"title": "Interprocedural dataflow analysis: Alias analysis\n", "abstract": " This thesis presents some results in the area of interprocedural dataflow analysis and on the topic of interprocedural alias analysis in particular. The first is an efficient algorithm for computing interprocedural dominator and control dependence relations. Previous work computes the full (transitively closed) interprocedural post-dominator relation and then does an expensive transitive reduction step. Our approach directly computes the transitively reduced relation using the information in two relations between procedures in the program call graph.", "num_citations": "8\n", "authors": ["1864"]}
{"title": "symMMU: Symbolically executed runtime libraries for symbolic memory access\n", "abstract": " Symbolic execution calls for specialized address translation. Unlike a pointer on a traditional machine model, which corresponds to a single address, a symbolic pointer may represent multiple feasible addresses. A symbolic pointer dereference manipulates symbolic state, potentially submitting many theorem prover requests in the process. Hence, design and management of symbolic accesses critically affects symbolic executor performance, complexity, and completeness.", "num_citations": "7\n", "authors": ["1864"]}
{"title": "Statistical inference of static analysis rules\n", "abstract": " Various apparatus and methods are disclosed for identifying errors in program code. Respective numbers of observances of at least one correctness rule by different code instances that relate to the at least one correctness rule are counted in the program code. Each code instance has an associated counted number of observances of the correctness rule by the code instance. Also counted are respective numbers of violations of the correctness rule by different code instances that relate to the correctness rule. Each code instance has an associated counted number of violations of the correctness rule by the code instance. A respective likelihood of the validity is determined for each code instance as a function of the counted number of observances and counted number of violations. The likelihood of validity indicates a relative likelihood that a related code instance is required to observe the correctness rule. The\u00a0\u2026", "num_citations": "6\n", "authors": ["1864"]}
{"title": "Rwset: Attacking path explosion in constraint-based test generation\n", "abstract": " Recent work has used variations of symbolic execution to automatically generate high-coverage test inputs [2, 8, 3, 12, 6]. Such tools have demonstrated their ability to find very subtle errors. However, one challenge they all face is how to effectively handle the exponential number of paths in the code. This paper presents a new technique for reducing the number of traversed code paths by discarding those that must have side-effects identical to some previously explored path. Our results on a mix of open source applications and device drivers show that this (sound) optimization reduces the numbers of paths traversed by several orders of magnitude, often getting coverage out of reach for a standard constraint-based execution system. 1", "num_citations": "6\n", "authors": ["1864"]}
{"title": "A\u2019C Tutorial\n", "abstract": " This paper describes \u2018C (Tick C), a superset of ANSI C that allows high-level, efficient, and machine-independent specification of dynamically generated code.", "num_citations": "5\n", "authors": ["1864"]}
{"title": "The Exokernel approach to operating system extensibility\n", "abstract": " To provide modularity and performance, operating system (OS) kernels should have only minimal embedded functionality. Today's operating systems are large, ine cient, and most importantly, in exible. In our view, most OS performance and exibility problems can be eliminated simply by pushing the OS interface lower. To achieve this goal, we have de ned a new OS structure, exokernel, that safely exports only those resources de ned by the underlying hardware. Our goal is to put abstractions traditionally implemented by the kernel out into user-space, where user-level libraries or servers abstract the exposed hardware resources. 1 By moving management of resources out into userspace, an exokernel allows aggressive customization. We see three reasons for customization. The rst is to tailor policies to given hardware con gurations, allowing applications to advance with the hardware. The second is to tailor\u00a0\u2026", "num_citations": "5\n", "authors": ["1864"]}
{"title": "A vcode tutorial\n", "abstract": " This paper is a short tutorial on VCODE, a fast, portable dynamic code generation system. It should be read after [1].", "num_citations": "4\n", "authors": ["1864"]}
{"title": "A system's hackers crash course: techniques that find lots of bugs in real (storage) system code\n", "abstract": " This talk describes several effective bug-finding tools we have developed, which exploit not-widely-understood techniques\u2014implementation-level model checking and symbolic execution\u2014focusing on the key intuitions and ideas behind them.", "num_citations": "3\n", "authors": ["1864"]}
{"title": "Aegis: A secure programmable exokernel\n", "abstract": " To provide high performance and a high degree of modularity, operating systems should provide only minimal embedded kernel functionality. The Exokernel is a new operating system structure that achieves this minimalist goal by letting applications manage the underlying hardware resources directly instead of hiding the hardware functionality in layers of operating system software. Applications can customize the operating system by extending exokernel interfaces. Exokernels can use code inspection, inlined cross-domain procedure calls, and secure languages to allow the safe execution of user code in the kernel. By having direct access to the physical hardware applications can implement policies that incorporate application-speci c knowledge. To test and evaluate exokernels and their customization techniques a prototype exokernel system, Aegis, is being developed.", "num_citations": "3\n", "authors": ["1864"]}
{"title": "DCG: An E cient, Retargetable Dynamic Code Generation System\n", "abstract": " Dynamic code generation allows aggressive optimization through the use of runtime information. Previous systems typically relied on ad hoc code generators that were not designed for retargetability, and did not shield the client from machine-speci c details. We present a system, dcg, that allows clients to specify dynamically generated code in a machineindependent manner. Our one-pass code generator is easily retargeted and extremely e cient (code generation costs approximately 350 instructions per generated instruction). Experiments show that dynamic code generation increases some application speeds by over an order of magnitude.", "num_citations": "3\n", "authors": ["1864"]}
{"title": "Using Influence to Understand Complex Systems\n", "abstract": " This thesis is concerned with understanding the behavior of complex systems, particularly in the common case where instrumentation data is noisy or incomplete. We begin with an empirical study of logs from production systems, which characterizes the content of those logs and the challenges associated with analyzing them automatically, and present an algorithm for identifying surprising messages in such logs.", "num_citations": "2\n", "authors": ["1864"]}
{"title": "The Design and Implementation of E cient Thread-Based Parallelism\n", "abstract": " E cient thread management allows operating systems and applications to e ectively exploit parallelism. To aid designers in implementing an e cient thread package we record the evolution of a traditional stack-based thread package to a highly optimized\\stateless\" threads package. The e ects of each optimization are discussed and quanti ed. Compared to the initial package, thread creation is made an order of magnitude more e cient, and thread execution 12-40 times faster, depending on the type of application. The nal result is a new thread package, called Filaments. The cost of running a lament taking three arguments is approximately 6 instructions; creation is approximately 12. In addition, by exploiting the semantics of stateless threads, software based multithreading can be implemented with an e ciency competitive to a hardware implementation. Finally, Filaments is very portable; unlike other thread packages, it is completely written in a high-level language.", "num_citations": "1\n", "authors": ["1864"]}
{"title": "EXOKERNELS\n", "abstract": " EXOKERNELS Page 1 EXOKERNELS Dawson Engler, Frans Kaashoek, James O\u2019Toole MIT Laboratory for Computer Science http://www.pdos.lcs.mit.edu/ Page 2 Ambitious applications must fight OS \u2022 OS abstractions preempt application design decisions since: No perfect implementation exists No perfect interface exists You cannot avoid them \u2022 Result: applications run slowly or can\u2019t be written Page 3 Exokernel: maximize application freedom \u2022 Insight: implement OS abstractions at application-level \u2022 How: securely multiplex hardware without abstracting it Export hardware to applications Protection by thin OS veneer, the exokernel System objects and policies in untrusted libraries \u2022 Result: Can do operations impossible on traditional systems Page 4 Exokernel architecture \u2022 An exokernel safely exports hardware to applications \u2022 Applications build abstractions with library OSs \u2022 Shared libraries to reduce space \u2026", "num_citations": "1\n", "authors": ["1864"]}