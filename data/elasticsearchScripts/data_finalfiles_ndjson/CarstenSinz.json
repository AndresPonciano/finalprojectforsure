{"title": "Towards an optimal CNF encoding of boolean cardinality constraints\n", "abstract": " We consider the problem of encoding Boolean cardinality constraints in conjunctive normal form (CNF). Boolean cardinality constraints are formulae expressing that at most (resp. at least) k out of n propositional variables are true. We give two novel encodings that improve upon existing results, one which requires only 7n clauses and 2n auxiliary variables, and another one demanding  clauses, but with the advantage that inconsistencies can be detected in linear time by unit propagation alone. Moreover, we prove a linear lower bound on the number of required clauses for any such encoding.", "num_citations": "499\n", "authors": ["681"]}
{"title": "Formal methods for the validation of automotive product configuration data\n", "abstract": " In the automotive industry, the compilation and maintenance of correct product configuration data is a complex task. Our work shows how formal methods can be applied to the validation of such business critical data. Our consistency support tool BIS works on an existing database of Boolean constraints expressing valid configurations and their transformation into manufacturable products. Using a specially modified satisfiability checker with an explanation component, BIS can detect inconsistencies in the constraints set and thus help increase the quality of the product data. BIS also supports manufacturing decisions by calculating the implications of product or production environment changes on the set of required parts. In this paper, we give a comprehensive account of BIS: the formalization of the business processes underlying its construction, the modifications of satisfiability-checking technology we found\u00a0\u2026", "num_citations": "172\n", "authors": ["681"]}
{"title": "Configuration lifting: Verification meets software configuration\n", "abstract": " Configurable software is ubiquitous, and the term software product line (SPL) has been coined for it lately. It remains a challenge, however, how such software can be verified over all variants. Enumerating all variants and analyzing them individually is inefficient, as knowledge cannot be shared between analysis runs. Instead of enumeration we present a new technique called lifting that converts all variants into a meta-program, and thus facilitates the configuration-aware application of verification techniques like static analysis, model checking and deduction-based approaches. As a side-effect, lifting provides a technique for checking software feature models, which describe software variants, for consistency. We demonstrate the feasibility of our approach by checking configuration dependent hazards for the highly configurable Linux kernel which possesses several thousand of configurable features. Using our\u00a0\u2026", "num_citations": "109\n", "authors": ["681"]}
{"title": "PaSAT\u2014Parallel SAT-checking with lemma exchange: Implementation and applications\n", "abstract": " We present PaSAT, a parallel implementation of a Davis-Putnam-style prepositional satisfiability checker incorporating dynamic search space partitioning, intelligent backjumping, as well as lemma generation and exchange; the main focus of our implementation is on speeding up SAT-checking of prepositional encodings of real-world combinatorial problems. We investigate and analyze the speed-ups obtained by parallelization in conjunction with lemma exchange and describe the effects we observed during our experiments. Finally, we present performance measurements from the application of our prover in the areas of formal consistency checking of industrial product documentation, cryptanalysis, and hardware verification.We would like to thank J\u00fcrgen Ellinger for help on carrying out the experiments.", "num_citations": "106\n", "authors": ["681"]}
{"title": "Parallel propositional satisfiability checking with distributed dynamic learning\n", "abstract": " We address the parallelization and distributed execution of an algorithm from the area of symbolic computation: propositional satisfiability (SAT) checking with dynamic learning. Our parallel programming models are strict multithreading for the core SAT checking procedure, complemented by mobile agents realizing a distributed dynamic learning process. Individual threads treat dynamically created subproblems, while mobile agents collect and distribute pertinent knowledge obtained during the learning process. The parallel algorithm runs on top of our parallel system platform Distributed Object-Oriented Threads System, which provides support for our parallel programming models in highly heterogeneous distributed systems. We present performance measurements evaluating the performance gains by our approach in different application domains with practical significance.", "num_citations": "89\n", "authors": ["681"]}
{"title": "Proving consistency assertions for automotive product data management\n", "abstract": " We present a formal specification and verification approach for industrial product data bases containing Boolean logic formulae to express constraints. Within this framework, global consistency assertions about the product data are converted into propositional satisfiability problems. Today\"s state-of-the-art provers turn out to be surprisingly efficient in solving the SAT-instances generated by this process. Moreover, we introduce a method for encoding special nonmonotonic constructs in traditional Boolean logic. We have successfully applied our method to industrial automotive product data management and could establish a set of commercially used interactive tools that facilitate the management of change and help raise quality standards.", "num_citations": "77\n", "authors": ["681"]}
{"title": "Hordesat: A massively parallel portfolio SAT solver\n", "abstract": " A simple yet successful approach to parallel satisfiability (SAT) solving is to run several different (a portfolio of) SAT solvers on the input problem at the same time until one solver finds a solution. The SAT solvers in the portfolio can be instances of a single solver with different configuration settings. Additionally the solvers can exchange information usually in the form of clauses. In this paper we investigate whether this approach is applicable in the case of massively parallel SAT solving. Our solver is intended to run on clusters with thousands of processors, hence the name HordeSat. HordeSat is a fully distributed portfolio-based SAT solver with a modular design that allows it to use any SAT solver that implements a given interface. HordeSat has a decentralized design and features hierarchical parallelism with interleaved communication and search. We experimentally evaluated it using all the benchmark\u00a0\u2026", "num_citations": "73\n", "authors": ["681"]}
{"title": "Linking functional requirements and software verification\n", "abstract": " Synchronization between component requirements and implementation centric tests remains a challenge that is usually addressed by requirements reviews with testers and traceability policies. The claim of this work is that linking requirements, their scenario-based formalizations, and software verification provides a promising extension to this approach. Formalized scenarios, for example in the form of low-level assume/assert statements in C, are easier to trace to requirements than traditional test sets. For a verification engineer, they offer an opportunity to better participate in requirements changes. Changes in requirements can be more easily propagated because adapting formalized scenarios is often easier than deriving and updating a large set of test cases. The proposed idea is evaluated in a case study encompassing over 50 functional requirements of an automotive software developed at Robert Bosch\u00a0\u2026", "num_citations": "53\n", "authors": ["681"]}
{"title": "Reducing false positives by combining abstract interpretation and bounded model checking\n", "abstract": " Fully automatic source code analysis tools based on abstract interpretation have become an integral part of the embedded software development process in many companies. And although these tools are of great help in identifying residual errors, they still possess a major drawback: analyzing industrial code comes at the cost of many spurious errors that must be investigated manually. The need for efficient development cycles prohibits extensive manual reviews, however. To overcome this problem, the combination of different software verification techniques has been suggested in the literature. Following this direction, we present a novel approach combining abstract interpretation and source code bounded model checking, where the model checker is used to reduce the number of false error reports. We apply our methodology to source code from the automotive industry written in C, and show that the number of\u00a0\u2026", "num_citations": "45\n", "authors": ["681"]}
{"title": "Overview and analysis of the SAT Challenge 2012 solver competition\n", "abstract": " Programs for the Boolean satisfiability problem (SAT), i.e., SAT solvers, are nowadays used as core decision procedures for a wide range of combinatorial problems. Advances in SAT solving during the last 10\u201315 years have been spurred by yearly solver competitions. In this article, we report on the main SAT solver competition held in 2012, SAT Challenge 2012. Besides providing an overview of how SAT Challenge 2012 was organized, we present an in-depth analysis of key aspects of the results obtained during the competition.", "num_citations": "40\n", "authors": ["681"]}
{"title": "Visualizing SAT instances and runs of the DPLL algorithm\n", "abstract": " SAT-solvers have turned into essential tools in many areas of applied logic like, for example, hardware verification or satisfiability checking modulo theories. However, although recent implementations are able to solve problems with hundreds of thousands of variables and millions of clauses, much smaller instances remain unsolved. What makes a particular instance hard or easy is at most partially understood \u2013 and is often attributed to the instance\u2019s internal structure. By converting SAT instances into graphs and applying established graph layout techniques, this internal structure can be visualized and thus serve as the basis of subsequent analysis. Moreover, by providing tools that animate the structure during the run of a SAT algorithm, dynamic changes of the problem instance become observable. Thus, we expect both to gain new insights into the hardness of the SAT problem and to help in teaching\u00a0\u2026", "num_citations": "39\n", "authors": ["681"]}
{"title": "DPvis \u2013 A Tool to Visualize the Structure of SAT Instances\n", "abstract": " We present DPvis, a Java tool to visualize the structure of SAT instances and runs of the DPLL (Davis-Putnam-Logemann-Loveland) procedure. DPvis uses advanced graph layout algorithms to display the problem\u2019s internal structure arising from its variable dependency (interaction) graph. DPvis is also able to generate animations showing the dynamic change of a problem\u2019s structure during a typical DPLL run. Besides implementing a simple variant of the DPLL algorithm on its own, DPvis also features an interface to MiniSAT, a state-of-the-art DPLL implementation. Using this interface, runs of MiniSAT can be visualized\u2014including the generated search tree and the effects of clause learning. DPvis is supposed to help in teaching the DPLL algorithm and in gaining new insights in the structure (and hardness) of SAT instances.", "num_citations": "28\n", "authors": ["681"]}
{"title": "Information flow analysis via path condition refinement\n", "abstract": " We present a new approach to information flow control (IFC), which exploits counterexample-guided abstraction refinement (CEGAR) technology. The CEGAR process is built on top of our existing IFC analysis in which illegal flows are characterized using program dependence graphs (PDG) and path conditions (as described in [12]). Although path conditions provide an already precise abstraction that can be used to generate witnesses to the illegal flow, they may still cause false alarms. Our CEGAR process recognizes false witnesses by executing them and monitoring their executions, and eliminates them by automatically refining path conditions in an iterative way as needed. The paper sketches the foundations of CEGAR and PDG-based IFC, and describes the approach in detail. An example shows how the approach finds illegal flow, and demonstrates how CEGAR eliminates false alarms.", "num_citations": "24\n", "authors": ["681"]}
{"title": "Knowledge compilation for product configuration\n", "abstract": " In this paper we address the application of knowledge compilation techniques to product configuration problems. We argument that both the process of generating valid configurations, as well as validation of product configuration knowledge bases, can potentially be accelerated by compiling the instance independent part of the knowledge base. Besides giving transformations of both tasks into logical entailment problems, we give a short summary on knowledge compilation techniques, and present a new algorithm for computing unit-resolution complete knowledge bases.", "num_citations": "23\n", "authors": ["681"]}
{"title": "A service-based agent framework for distributed symbolic computation\n", "abstract": " We present Okeanos, a distributed service-based agent framework implemented in Java, in which agents can act autonomously and make use of stationary services. Each agent\u2019s behaviour can be controlled individually by a rule-based knowledge component, and cooperation between agents is supported through the exchange of messages at common meeting points (agent lounges). We suggest this general scheme as a new parallelization paradigm for Symbolic Computation, and demonstrate its applicability by an agent-based parallel implementation of a satisfiability (SAT) checker.", "num_citations": "23\n", "authors": ["681"]}
{"title": "Loop detection in rule-based expert systems\n", "abstract": " This invention describes a method to verify non-looping properties of programs implemented as rule-based expert systems. Our method detects conditions under which the expert system enters erroneous infinite program loops, which lead to non-terminating or oscillating computations, or otherwise proves the absence of such conditions. Our automatic procedure also gives advice on how to correct these errors. The expert systems considered consist of condition-action rules (IF-THEN-statements), where the conditions are logical expressions (formulas of a propositional finite domain logic), and the actions modify the value of a single variable which in turn can be part of other logical expressions. There may be additional (external) variables not controlled by the expert system, and each rule may have an associated evaluation priority.", "num_citations": "22\n", "authors": ["681"]}
{"title": "Problem-sensitive restart heuristics for the DPLL procedure\n", "abstract": " Search restarts have shown great potential in speeding up SAT solvers based on the DPLL procedure. However, most restart policies presented so far do not take the problem structure into account. In this paper we present several new problem-sensitive restart heuristics. They all observe different search parameters like conflict level or backtrack level over time and, based on their development, decide whether to perform a restart or not. We also present a Java tool to visualize these search parameters on a given SAT instance over time in order to analyze existing heuristics and develop new one.", "num_citations": "21\n", "authors": ["681"]}
{"title": "Verifikation regelbasierter Konfigurationssysteme\n", "abstract": " Die vorliegende Arbeit entstand w\u00e4hrend meiner T\u00e4tigkeit als wissenschaftlicher Mitarbeiter am Arbeitsbereich Symbolisches Rechnen der Fakult\u00e4t f\u00fcr Informations-und Kognitionswissenschaften der Universit\u00e4t T\u00fcbingen. Die Implementation des Baubarkeits-Informations-Systems BIS fand gr\u00f6\u00dftenteils w\u00e4hrend meiner Besch\u00e4ftigung am Steinbeis-Transferzentrum Objektund Internet-Technologien (OIT) an der Universit\u00e4t T\u00fcbingen statt.", "num_citations": "20\n", "authors": ["681"]}
{"title": "A Universal Parallel SAT Checking Kernel.\n", "abstract": " We present a novel approach to parallel Boolean satisfiability (SAT) checking. A distinctive feature of our parallel SAT checker is that it incorporates all essential heuristics employed by the state-of-the-art sequential SAT checking algorithm. This property makes our parallel SAT checker applicable in a wide range of different application domains. For its distributed execution a combination of the strict multithreading and the mobile agent programming model is employed. We give results of run-time measurements for problem instances taken from different application domains, indicating the usefulness of the presented method.", "num_citations": "20\n", "authors": ["681"]}
{"title": "Parallel Satisfiability.\n", "abstract": " The propositional satisfiability problem (SAT) is one of the fundamental problems in theoretical computer science, but it also has many practical applications. Parallel algorithms for the SAT problem have been proposed and implemented since the 1990s. This chapter provides an overview of current approaches and their evolution over recent decades towards efficiently solving hard combinatorial problems on multi-core computers and clusters.", "num_citations": "18\n", "authors": ["681"]}
{"title": "Towards automatic software model checking of thousands of Linux modules\u2014A case study with Avinux\n", "abstract": " Modular software model checking of large real\u2010world systems is known to require extensive manual effort in environment modelling and preparing source code for model checking. Avinux is a tool chain that facilitates the automatic analysis of Linux and especially of Linux device drivers. The tool chain is implemented as a plugin for the Eclipse IDE, using the source code bounded model checker CBMC as its backend. Avinux supports a verification process for Linux that is built upon specification annotations with SLICx (an extension of the SLIC language), automatic data environment creation, source code transformation and simplification, and the invocation of the verification backend. In this paper technical details of the verification process are presented: Using Avinux on thousands of drivers from various Linux versions led to the discovery of six new errors. In these experiments, Avinux also reduced the immense\u00a0\u2026", "num_citations": "18\n", "authors": ["681"]}
{"title": "Computation of renameable horn backdoors\n", "abstract": " Satisfiability of real-world Sat instances can be often decided by focusing on a particular subset of variables - a so-called Backdoor Set. In this paper we suggest two algorithms to compute Renameable Horn deletion backdoors. Both methods are based on the idea to transform the computation into a graph problem. This approach could be used as a preprocessing to solve hard real-world Sat\u00a0instances. We also give some experimental results of the computations of Renameable Horn backdoors for several real-world instances.", "num_citations": "17\n", "authors": ["681"]}
{"title": "Compressing propositional proofs by common subproof extraction\n", "abstract": " Propositional logic decision procedures [1,2,3,4,5,6] lie at the heart of many applications in hard- and software verification, artificial intelligence and automatic theorem proving [7,8,9,10,11,12]. They have been used to successfully solve problems of considerable size. In many practical applications, however, it is not sufficient to obtain a yes/no answer from the decision procedure. Either a model, representing a sample solution, or a justification, why the formula possesses none is required. So, e.g. in declarative modeling or product configuration [9,10] an inconsistent specification given by a customer corresponds to an unsatisfiable problem instance. To guide the customer in correcting his specification, a justification why it is erroneous can be of great help. In the context of model checking proofs are used, e.g., for abstraction refinement [11], or approximative image computations through interpolants [13]. In\u00a0\u2026", "num_citations": "17\n", "authors": ["681"]}
{"title": "Verifying CIM models of Apache web-server configurations\n", "abstract": " We show how configuration properties of the Apache Web-server can be formally verified, so that an installation is safe with respect to both universal and site specific local constraints. Our approach starts from an existing semi-formal component model of the Web-server in the common information model (CIM) standard. Hence, our approach is applicable also to the verification of other systems for which a CIM model exists.", "num_citations": "16\n", "authors": ["681"]}
{"title": "Detection of Inconsistencies in Complex Product Configuration Data Using Extended Propositional SAT-Checking.\n", "abstract": " We present our consistency support tool BIS, an extension to the electronic product data management system (EPDMS) used at DaimlerChrysler AG to configure the Mercedes lines of passenger cars and commercial vehicles. BIS allows verification of certain integrity aspects of the product data as a whole. The underlying EPDMS maintains a data base of sales options and parts together with a set of logical constraints expressing valid configurations and their transformation into manufacturable products. Due to the complexity of the products and the induced complexity of the constraints, maintenance of the data base is a nontrivial task and error-prone. By formalizing DaimlerChrysler\u2019s order processing method and converting global consistency assertions about the product data base into formulae of an extended propositional logic, we are able to employ a satisfiability checker integrated into BIS to detect inconsistencies, and thus increase the quality of the product data.", "num_citations": "15\n", "authors": ["681"]}
{"title": "A new bound for an NP-hard subclass of 3-SAT using backdoors\n", "abstract": " Abstract Knowing a Backdoor Set B for a given Sat instance, satisfiability can be decided by only examining each of the 2| B| truth assignments of the variables in B. However, one problem is to efficiently find a small backdoor up to a particular size and, furthermore, if no backdoor of the desired size could be found, there is in general no chance to conclude anything about satisfiability. In this paper we introduce a complete deterministic algorithm for an NP-hard subclass of 3-Sat, that is also a subclass of Mixed Horn Formulas (MHF). For an instance of the described class the absence of two particular kinds of backdoor sets can be used to prove unsatisfiability. The upper bound of this algorithm is O (p (n)* 1.427 n) which is less than the currently best upper bound for deterministic algorithms for 3-Sat and MHF.", "num_citations": "14\n", "authors": ["681"]}
{"title": "Visualizing the internal structure of SAT instances (preliminary report)\n", "abstract": " Modern algorithms for the SAT problem reveal an almost tractable behavior on \u201creal-world\u201d instances. This is frequently contributed to the fact that these instances possess an internal \u201cstructure\u201d that hard problem instances do not exhibit. However, little is known about this internal structure. We therefore propose a visualization of the instance\u2019s variable interaction graph (and of its dynamic change during a run of a SAT-solver) as a first step of an empirical research program to analyze the problem structure. We present first results of such an analysis on instances of bounded model checking benchmark problems.", "num_citations": "14\n", "authors": ["681"]}
{"title": "Detection of dynamic execution errors in IBM system automation's rule-based expert system\n", "abstract": " We formally verify aspects of the rule-based expert system of IBM's system automation software for IBM's zSeries mainframes. Starting with a formalization of the expert system in propositional dynamic logic (PDL), we encode termination and determinism properties in PDL and its extension \u0394PDL. We then translate our decision problems to propositional logic and apply advanced SAT techniques for automated proofs. In order to locate real program bugs for each failed proof attempt, we apply extra formalization steps and represent propositional error formulae in concise normal form as binary decision diagrams. In our experiments, we revealed residual non-termination bugs in a tested program version close to shipment, and, after correcting them, we formally verified the absence of this class of bugs in the production code.", "num_citations": "14\n", "authors": ["681"]}
{"title": "Ordered binary decision diagrams, pigeonhole formulas and beyond\n", "abstract": " Groote and Zantema proved that a particular OBDD computation of the pigeonhole formula has exponential size, and that limited OBDD derivations cannot simulate resolution polynomially. Here we show that an arbitrary OBDD refutation of the pigeonhole formula has exponential size: we prove that for any order of computation at least one intermediate OBDD in the proof has size \u03a9 (1.14 n). We also present a family of CNFs that show an exponential blow-up for all OBDD refutations compared to unrestricted resolution refutations.", "num_citations": "13\n", "authors": ["681"]}
{"title": "System Description: aRa\u2013An Automatic Theorem Prover for Relation Algebras\n", "abstract": " aRa is an automatic theorem prover for various kinds of relation algebras. It is based on Gordeev\u2019s Reduction Predicate Calculi for n-variable logic (RPC                   n                 ) which allow first-order finite variable proofs. Employing results from Tarski/Givant and Maddux we can prove validity in the theories of simple semi-associative relation algebras, relation algebras and representable relation algebras using the calculi RPC3 , RPC4 and RPC                   \u03c9                  . aRa, our implementation in Haskell, offers different reduction strategies for RPC                   n                  , and a set of simplifications preserving n-variable provability.", "num_citations": "13\n", "authors": ["681"]}
{"title": "Sat challenge 2012 random sat track: Description of benchmark generation\n", "abstract": " The SAT Challenge 2012 random SAT track benchmark set contains 600 instances, generated according to the uniform random generation model. The instances were divided in five major classes: k-SAT for k= 4, 5, 6, 7. Each class contains ten subclasses with varying clauses-to-variables ratios and numbers of variables. Each subclass contains 12 instances. Within this description we provide insights about the generation algorithm, the model according to which the size and properties of instances were chosen, and also about the filtering process.", "num_citations": "12\n", "authors": ["681"]}
{"title": "Proving functional equivalence of two AES implementations using bounded model checking\n", "abstract": " Bounded model checking-as well as symbolic equivalence checking-are highly successful techniques in the hardware domain. Recently, bit-vector bounded model checkers like CBMC have been developed that are able to check properties of (mostly low-level) software written in C. However, using these tools to check equivalence of software implementations has rarely been pursued. In this case study we tackle the problem of proving the functional equivalence of two implementations of the AES crypto-algorithm using automatic bounded model checking techniques. Cryptographic algorithms heavily rely on bit-level operations, which makes them particularly suitable for bit-precise tools like CBMC. Other software verification tools based on abstraction refinement or static analysis seem to be less appropriate for such software. We could semi-automatically prove equivalence of the first three rounds of the AES\u00a0\u2026", "num_citations": "12\n", "authors": ["681"]}
{"title": "Bridging the gap between test cases and requirements by abstract testing\n", "abstract": " In this article we propose a technique, called abstract testing, which replaces traditional test cases by abstract test cases. By doing so, fewer test cases are needed, and they are linked more closely to the requirements. Abstract tests can be considered as verification scenarios on the source code level which are derived from the requirements. Checking verification scenarios against the source code is done automatically using a software model checker. We also suggest a migration path from traditional tests to abstract test cases, which provides a smooth transition towards this new technique. Finally, we demonstrate feasibility of abstract testing by a case study from the automotive systems domain.", "num_citations": "11\n", "authors": ["681"]}
{"title": "Recognition of nested gates in CNF formulas\n", "abstract": " We present a new algorithm to efficiently extract information about nested functional dependencies between variables of a formula in CNF. Our algorithm uses the relation between gate encodings and blocked sets in CNF formulas. Our notion of \u201cgate\u201d emphasizes this relation. The presented algorithm is central to our new tool, cnf2aig, that produces equisatisfiable and-inverter-graphs (AIGs) from CNF formulas. We compare the novel algorithm to earlier approaches and show that the produced AIG are generally more succinct and use less input variables. As the gate-detection is related to the structure of input formulas, we furthermore analyze the gate-detection before and after applying preprocessing techniques.", "num_citations": "11\n", "authors": ["681"]}
{"title": "Minimizing models for tseitin-encoded SAT instances\n", "abstract": " Many applications of SAT solving can profit from minimal models\u2014a partial variable assignment that is still a witness for satisfiability. Examples include software verification, model checking, and counterexample-guided abstraction refinement. In this paper, we examine how a given model can be minimized for SAT instances that have been obtained by Tseitin encoding of a full propositional logic formula. Our approach uses a SAT solver to efficiently minimize a given model, focusing on only the input variables. Experiments show that some models can be reduced by over 50 percent.", "num_citations": "10\n", "authors": ["681"]}
{"title": "A Theory of Arrays with set and copy Operations.\n", "abstract": " The theory of arrays is widely used in order to model main memory in program analysis, software verification, bounded model checking, symbolic execution, etc. Nonetheless, the basic theory as introduced by McCarthy is not expressive enough for important practical cases, since it only supports array updates at single locations. In programs, memory is often modified using functions such as memset or memcpy/memmove, which modify a userspecified range of locations whose size might not be known statically. In this paper we present an extension of the theory of arrays with set and copy operations which make it possible to reason about such functions. We also discuss further applications of the theory.", "num_citations": "10\n", "authors": ["681"]}
{"title": "Integration of bounded model checking and deductive verification\n", "abstract": " Modular deductive verification of software systems is a complex task: the user has to put a lot of effort in writing module specifications that fit together when verifying the system as a whole. In this paper, we propose a combination of deductive verification and software bounded model checking\u00a0(SBMC), where SBMC is used to support the user in the specification and verification process, while deductive verification provides the final correctness proof. SMBC provides early \u2013 as well as precise \u2013 feedback to the user. Unlike modular deductive verification, the SBMC approach is able to check annotations beyond the boundaries of a single module \u2013 even if other relevant modules are not annotated (yet). This allows to test whether the different module specifications in the system match the implementation at every step of the specification process.", "num_citations": "10\n", "authors": ["681"]}
{"title": "ReDuX 1.5: New facets of rewriting\n", "abstract": " The ReDuX system is a term rewriting laboratory that allows the user to experiment with completion procedures. It features Knuth-Bendix completion with critical pair criteria, Peterson-Stickel completion for commutative and/or associative-commutative theories, inductive completion procedures based on positional ground reducibility tests, tools to analyze the set of irreducible ground terms, an unfailing completion procedure based on ordered rewriting and a random term generator. ReDuX has been first presented in [Bfin93] where the state of version 1.2 is reported. This system description will be restricted to extensions and modification since then.", "num_citations": "10\n", "authors": ["681"]}
{"title": "SAT-based consistency checking of automotive electronic product data\n", "abstract": " Complex products such as motor vehicles or computers need to be configured as part of the sales process [3, 8]. If the sale is electronic, then the configuration and some validity checking of the order must be done electronically as part of an electronic product data management system (EPDMS). The EPDMS typically maintains a data base of sales options and parts together with a set of logical constraints expressing valid combinations of sales options and their transformation into manufacturable products. Due to the complexity of these constraints, creation and maintenance of the configuration data base is a nontrivial task and error-prone. We present our system BIS which is commercially used to check global consistency assertions about the product data base used by the EPDMS of a major car and truck manufacturer. The EPDMS uses Boolean logic to encode the constraints, and BIS translates the consistency assertions into problems which it solves using a propositional satisfiability check...", "num_citations": "9\n", "authors": ["681"]}
{"title": "Static analysis and code complexity metrics as early indicators of software defects\n", "abstract": " Software is an important part of automotive product development, and it is commonly known that software quality assurance consumes considerable effort in safety-critical embedded software development. Increasing the effectiveness and efficiency of this effort thus becomes more and more important. Identifying problematic code areas which are most likely to fail and therefore require most of the quality assurance attention is required. This article presents an exploratory study investigating whether the faults detected by static analysis tools combined with code complexity metrics can be used as software quality indicators and to build pre-release fault prediction models. The combination of code complexity metrics with static analysis fault density was used to predict the pre-release fault density with an accuracy of 78.3%. This combination was also used to separate high and low quality components with a classification accuracy of 79%.", "num_citations": "8\n", "authors": ["681"]}
{"title": "Abstract testing: Connecting source code verification with requirements\n", "abstract": " Traditionally, test cases are used to check whether a system conforms to its requirements. However, to achieve good quality and coverage, large amounts of test cases are needed, and thus huge efforts have to be put into test generation and maintenance. We propose a methodology, called Abstract Testing, in which test cases are replaced by verification scenarios. Such verification scenarios are more abstract than test cases, thus fewer of them are needed and they are easier to create and maintain. Checking verification scenarios against the source code is done automatically using a software model checker. In this paper we describe the general idea of Abstract Testing, and demonstrate its feasibility by a case study from the automotive systems domain.", "num_citations": "8\n", "authors": ["681"]}
{"title": "Towards a verification of the rule-based expert system of the IBM SA for OS/390 automation manager\n", "abstract": " We formally verify consistency aspects of the rule-based expert system of IBM's System Automation software for IBM's e-server zSeries. Starting with a formalization of the expert system in propositional dynamic logic (PDL), we are able to encode termination and determinism properties. To circumvent direct proofs in PDL or its extension /spl Delta/PDL, we further translate versions of the occurring decision problems to propositional logic, where we can apply advanced SAT and BDD techniques. In our experiments we revealed some inconsistencies, and after correcting them, we successfully verified a non-looping property for a part of the expert system.", "num_citations": "8\n", "authors": ["681"]}
{"title": "Deep Learning for Software Defect Prediction: A Survey\n", "abstract": " Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (eg complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we\u00a0\u2026", "num_citations": "7\n", "authors": ["681"]}
{"title": "An exponential lower bound on OBDD refutations for pigeonhole formulas\n", "abstract": " Haken proved that every resolution refutation of the pigeonhole formula has at least exponential size. Groote and Zantema proved that a particular OBDD computation of the pigeonhole formula has an exponential size. Here we show that any arbitrary OBDD refutation of the pigeonhole formula has an exponential size, too: we prove that the size of one of the intermediate OBDDs is at least .", "num_citations": "7\n", "authors": ["681"]}
{"title": "Verifying the on-line help system of SIEMENS magnetic resonance tomographs\n", "abstract": " Large-scale medical systems\u2014like magnetic resonance tomographs\u2014are manufactured with a steadily growing number of product options. Different model lines can be equipped with large numbers of supplementary equipment options like (gradient) coils, amplifiers, magnets or imaging devices. The diversity in service and maintenance procedures, which may be different for each of the many product instances, grows accordingly. Therefore, instead of having one common on-line service handbook for all medical devices, SIEMENS parcels out the on-line documentation into small (help) packages, out of which a suitable subset is selected for each individual product instance. Selection of packages is controlled by XML terms. To check whether the existing set of help packages is sufficient for all possible devices and service cases, we developed the HelpChecker tool. HelpChecker translates the XML input\u00a0\u2026", "num_citations": "7\n", "authors": ["681"]}
{"title": "Parallel consistency checking of automotive product data\n", "abstract": " This paper deals with a parallel approach to the verification of consistency aspects of an industrial product configuration data base. The data base we analyze is used by DaimlerChrysler to check the orders for cars and commercial vehicles of their Mercedes lines. By formalizing the ordering process and employing techniques from symbolic computation we could establish a set of tools that allow the automatic execution of huge series of consistency checks, thereby ultimately enhancing the quality of the product data. However, occasional occurrences of computation intensive checks are a limiting factor for the usability of the tools. Therefore, a prototypical parallel re-implementation using our Distributed Object-Oriented Threads System (DOTS) was carried out. Performance measurements on a heterogeneous cluster of shared-memory multiprocessor Unix workstations and standard Windows PCs revealed\u00a0\u2026", "num_citations": "7\n", "authors": ["681"]}
{"title": "SAT challenge 2012\n", "abstract": " The area of SAT solving has seen tremendous progress over the last years. Many problems (eg, in hardware and software verification) that seemed to be completely out of reach a decade ago can now be handled routinely. Besides new algorithms and better heuristics, refined implementation techniques turned out to be vital for this success. To keep up the driving force in improving SAT solvers, we want to motivate implementors to present their work to a broader audience and to compare it with that of others. SAT Challenge 2012 (SC 2012), a competitive event for solvers of the Boolean Satisfiability (SAT) problem, took place within 2012. It was organized as a satellite event to the 15th International Conference on Theory and Applications of Satisfiability Testing (SAT 2012) and stands in the tradition of the SAT Competitions held yearly from 2002 to 2005 and biannually starting from 2007, and the SAT-Races held in 2006, 2008 and 2010. SC 2012 consisted of 5 competition tracks, including three main tracks for sequential solvers (Application SAT+ UNSAT containing problem encodings (both SAT and UNSAT) from real-world applications, such as hardware and software verification, bio-informatics, planning, scheduling, etc; Hard Combinatorial SAT+ UNSAT containing combinatorial problems (both SAT and UNSAT) to challenge current SAT solving algorithms, similar to the SAT Competition\u2019s category \u201ccrafted\u201d; and Random SAT, containing randomly generated satisfiable instances); one track for parallel solvers (with eight computing cores, using Application SAT+ UNSAT instances); and one track for sequential portfolio solvers (1/3 Application\u00a0\u2026", "num_citations": "6\n", "authors": ["681"]}
{"title": "SANchk: SQL-based SAN configuration checking\n", "abstract": " Storage Area Networks (SANs) connect groups of storage devices to servers over fast interconnects. An important challenge lies in managing the complexity of the resulting massive SAN configurations. Policy-based validation using new logical frameworks has been proposed earlier as a solution to this configuration problem. SANchk offers a new solution that uses standard technologies such as SQL, XML, and Java, to implement a rule-based configuration checker. SANchk works as a light-weight extension to the relational databases of storage management systems; current support includes IBM's TPC and the open source Aperi storage manager. Some five dozen best practices rules for SAN configuration are implemented in SANchk, many of them with configurable parameters. Empirical results with several commercial SANs show that the approach is viable in practice.", "num_citations": "6\n", "authors": ["681"]}
{"title": "Towards SLA-based optimal workload distribution in SANs\n", "abstract": " Storage area networks (SANs) connect storage devices to servers over fast network interconnects. We consider the problem of optimal SAN configuration with the goal of meeting service level agreements (SLAs) for server processes while retaining flexibility for future changes. Our approach proceeds in two stages, by setting up pseudo-Boolean constraint problems and solving them with an off-the-shelf solver. First, we give an algorithm for assigning storage devices to applications running on the SANpsilas hosts. This algorithm tries to balance the workload as evenly as possible over all storage devices. Our second algorithm takes these assignments and computes the interconnections (data paths) that are necessary to achieve the desired configuration while respecting redundancy (safety)requirements in the SLAs. Again, this algorithm tries to balance the workload of all connections and devices in proportion to\u00a0\u2026", "num_citations": "6\n", "authors": ["681"]}
{"title": "Dealing with temporal change in product documentation for manufacturing\n", "abstract": " Product documentation for manufacturing is subject to temporal change: Exchange of parts by successor models, shift from in-house production to external procurement, or assembly line reconfiguration are just a few examples. In this paper we show how DaimlerChrysler AG manages configuration for manufacturing of their Mercedes lines. Furthermore, we identify typical situations of change, their representation on the product documentation level, and how to keep the documentation consistent over time. We then develop two verification methods for computing the induced change at the parts level. Finally, we show how our methods can be applied to handle model year change and production relocation. 1", "num_citations": "6\n", "authors": ["681"]}
{"title": "Baubarkeitspr\u00fcfung von Kraftfahrzeugen durch automatisches Beweisen\n", "abstract": " Die formale Beschreibung und Verifikation von Proze\u00dfabl\u00e4ufen hat noch vor wenigen Jahren in der Industrie so gut wie keine Rolle gespielt, und auch heute ist der Einsatz von Beweismethoden auf diesem Gebiet nicht sehr weit verbreitet. Lediglich in der Halbleiterindustrie konnten mathematische Beweisverfahren schon relativ fr\u00fch (Ende der 80er\u2013Anfang der 90er Jahre) den Weg aus den Forschungslabors ins industrielle Umfeld schaffen. Dort konnten zum Teil beachtliche Erfolge bei der formalen \u00dcberpr\u00fcfung zunehmend komplexerer Chips erreicht werden. In [Mar97] findet man eine \u00dcbersicht dieser Untersuchungen und der dabei auftretenden Probleme.Die untergeordnete Rolle, die formale Beweisverfahren in der computerunterst\u00fctzten industriellen Produktion spielen, hat vielf\u00e4ltige Ursachen. Zum einen gibt es kein einfaches, allgemeing\u00fcltiges Verfahren, einen bestehenden Proze\u00dfablauf zu formalisieren. Daneben stehen oft Verst\u00e4ndigungsprobleme und unterschiedliche Begriffswelten einer zufriedenstellenden Modellierung im Weg. Zur Umsetzung der vorhandenen Betriebsabl\u00e4ufe in ein mathematisches Modell ist jedoch genau dieses Verst\u00e4ndnis zwingend erforderlich. Der hohe zeitliche Aufwand eines solchen Abstimmungs-und Verst\u00e4ndigungsprozesses wirkt oft abschreckend.", "num_citations": "6\n", "authors": ["681"]}
{"title": "Using gate recognition and random simulation for under-approximation and optimized branching in SAT solvers\n", "abstract": " We extract structure from CNF problems using a gate recognition algorithm and perform random simulation on that structure to generate conjectures about literal equivalences and backbone variables. We use these conjectures in two approaches to optimize CDCL SAT solving. In the first approach, we perform under-approximation by adding the conjectures to the original problem, while performing subsequent corrections in a refinement loop. In the second approach, we modify the branching heuristic such that it exploits the conjectures in order to stimulate clause learning. Experimental results show improvements, especially on unsatisfiable circuit-equivalence checking problems.", "num_citations": "5\n", "authors": ["681"]}
{"title": "Optimizing MiniSAT variable orderings for the relational model finder Kodkod\n", "abstract": " It is well-known that the order in which variables are processed in a DPLL-style SAT algorithm can have a substantial effect on its run-time. Different heuristics, such as VSIDS [2], have been proposed in the past to obtain good variable orderings. However, most of these orderings are general-purpose and do not take into account the additional structural information that is available on a higher problem description level. Thus, structural, problem-dependent strategies have been proposed (see, e.g., the work of Marques-Silva and Lynce on special strategies for cardinality constraints [1]).", "num_citations": "5\n", "authors": ["681"]}
{"title": "Avinux: Towards automatic verification of Linux device drivers\n", "abstract": " Avinux is a tool that facilitates the automatic analysis of Linux and especially of Linux device drivers. The tool is implemented as a plugin for the Eclipse IDE, using the source code bounded model checker CBMC as its backend. Avinux supports a verification process for Linux that includes specification annotation in SLICx (an extension of the SLIC language), automatic data environment creation, source code transformation and simplification, and the invocation of the verification backend. We have successfully used Avinux for the automatic analysis of Linux device drivers reducing the immense overhead of manual code preprocessing that other projects incurred.", "num_citations": "5\n", "authors": ["681"]}
{"title": "Comparing different logic-based representations of automotive parts lists\n", "abstract": " Parts lists in the automotive industry can be of considerable size. For the Mercedes cars of DaimlerChrysler, for example, they consist of more than 30.000 entries for the larger model lines. Selection of the right parts for a particular product instance is complicated, and typically done via a logical formalism relating order codes with parts. To simplify part assignment, formalisms which use compact and concise formulae are required. We present and compare five different formalisms for such compact logical representations.", "num_citations": "5\n", "authors": ["681"]}
{"title": "A problem meta-data library for research in SAT\n", "abstract": " Experimental data and benchmarks play a crucial role in developing new algorithms and implementations of SAT solvers. Besides comparing and evaluating solvers, they provide the basis for all kinds of experiments, for setting up hypothesis and for testing them. Currently\u2013even though some initiatives for setting up benchmark databases have been undertaken, and the SAT Competitions provide a \u201cstandardized\u201d collection of instances\u2013it is hard to assemble benchmark sets with prescribed properties. Moreover, the origin of SAT instances is often not clear, and benchmark collections might contain duplicates. In this paper we suggest an approach to store meta-data information about SAT instances, and present an implementation that is capable of collecting, assessing and distributing benchmark meta-data.", "num_citations": "4\n", "authors": ["681"]}
{"title": "Theory and Applications of Satisfiability Testing-SAT 2014: 17th International Conference, Held as Part of the Vienna Summer of Logic, VSL 2014, Vienna, Austria, July 14-17\u00a0\u2026\n", "abstract": " This book constitutes the refereed proceedings of the 17th International Conference on Theory and Applications of Satisfiability Testing, SAT 2014, held as part of the Vienna Summer of Logic, VSL 2014, in Vienna, Austria, in July 2014. The 21 regular papers, 7 short papers and 4 tool papers presented together with 2 invited talks were carefully reviewed and selected from 78 submissions. The papers have been organized in the following topical sections: maximum satisfiability; minimal unsatisfiability; complexity and reductions; proof complexity; parallel and incremental (Q) SAT; applications; structure; simplification and solving; and analysis.", "num_citations": "4\n", "authors": ["681"]}
{"title": "Analyzing separation of duties constraints with a probabilistic model checker\n", "abstract": " Separation of Duties (SoD) is the concept that conflicting activities cannot be assigned to the same individual. A goal of SoD is to separate roles and responsibilities to reduce the risk of fraud or error. We consider the problem of verifying SoD constraints in the presence of uncertain information. We demonsrate the feasibility of implementing probabilistic model checking in a business process design with a case study. Modeling and verification is done with the probabilistic model checker PRISM.", "num_citations": "4\n", "authors": ["681"]}
{"title": "Teamwork-PaReDuX: Knowledge-based search with multiple parallel agents\n", "abstract": " We present the combination of a distribution approach and a parallelization concept for knowledge-based search. We formally characterize distribution and parallelization and present one instantiation of each, TEAMWORK and PaRe-DuX. TEAMWORK-PaReDuX, the combination of them, employs collaborating parallel search agents to prove equational theorems. Our experiments indicate that the speedups obtained by the single approaches are multiplied when using the combination, thus making good use of networks of multi-processor computers and allowing us to solve harder problems in acceptable time.", "num_citations": "4\n", "authors": ["681"]}
{"title": "Distributed parallel SAT checking with dynamic learning using DOTS\n", "abstract": " We present a novel method for distributed parallel au-tomatic theorem proving. Our approach uses a dynam-ically learning parallel SAT checker incorporating dis-tributed multi-threading and mobile agents. Individual threads process dynamically created subproblems, while agents collect and distribute new knowledge created by the learning process. As parallelization platform we use the Distributed Object-Oriented Threads System (DOTS) that provides support for both distributed threads and mo-bile agents. We present experiments indicating the useful-ness of the presented approach for different application do-mains.", "num_citations": "4\n", "authors": ["681"]}
{"title": "QPR Verify: A Static Analysis Tool for Embedded Software Based on Bounded Model Checking\n", "abstract": " We present the tool QPR Verify, which is an extension of the bounded model checking approach implemented in the tool LLBMC. QPR Verify is designed to verify industrial embedded software in C and C++ and focuses on runtime errors like arithmetic overflow or invalid memory access. The requirements of verifying industrial embedded software motivated a number of features tuned towards both functionality and usability, for instance providing code traces for each runtime error, better performance and scalability, and a flexible CLI and GUI. Besides new features, we discuss the architecture of QPR Verify and it\u2019s functionality, using a case study from the embedded system domain.", "num_citations": "3\n", "authors": ["681"]}
{"title": "Probabilistic model checking of constraints in a supply chain business process\n", "abstract": " Business process models represent corporate activities, their dependencies and relations, as far as they are needed to reach a specific company goal. In practice, they often exhibit stochastic behavior, e.g., to deal with uncertain information. In this paper, we consider the problem of verifying properties over business processes that deal with such uncertain information. We employ a probabilistic model checking algorithm for verification, and demonstrate the applicability of this approach by a case study. Modeling and verification is achieved using the model checking tool PRISM. Based on the results, general specifications for modeling business processes using a probabilistic model checker are identified. Also, the difference between declarative and procedural business process modeling approaches is discussed. We propose to combine declarative and procedural techniques, thereby gaining increased\u00a0\u2026", "num_citations": "3\n", "authors": ["681"]}
{"title": "DEET for component-based software\n", "abstract": " Testing) is to detect errors automatically in component-based software that is developed under the doctrine of design-bycontract. DEET is not intended to be an alternative to testing or verification. Instead, it is intended as a complementary and cost-effective prelude. Unlike testing and run-time monitoring after deployment, which require program execution and comparison of actual with expected results, DEET requires neither; in this sense, it is similar to formal verification. Unlike verification, where the goal is to prove implementation correctness, the objective of DEET is to show that an implementation is defective; in this sense, it is similar to testing. The thesis is that if there is an error in a component-based software system either because of a contract violation in the interactions between components, or within the internal details of a component (eg, a violated invariant), then it is likely\u2014but not guaranteed\u2014that DEET will find it quickly. DEET is substantially different from other static checking approaches that achieve apparently similar outcomes. Yet it builds on a key idea from one of them (Alloy): Jackson\u2019s small scope hypothesis. Among other things, the DEET approach weakens full verification of component implementation correctness to static checking for errors, in a systematic way that makes it clear exactly which defects could have been detected, and which could have been overlooked.", "num_citations": "3\n", "authors": ["681"]}
{"title": "The humane bugfinder: Modular static analysis using a sat solver\n", "abstract": " Assertion checking is a widely used technique to discover inconsistencies between specified behavior and actual implementation behavior. A modular, static analysis approach that is suitable for component-based systems is introduced. In the first stage of this approach, using only specifications of reused components and internal assertions in the implementation code (eg, loop invariants), assertions for verification of correctness are generated. In the second stage, error hypotheses are generated as Boolean formulae\u2014an idea inspired by results on scope restriction from the model checking community. The generated formulae are such that a satisfiable assignment not only indicates an error but provides a directly human-readable trace of a witness to the bug. An example checked using an existing SAT solver suggests that the approach is promising from the practical standpoint.", "num_citations": "3\n", "authors": ["681"]}
{"title": "Verifying equivalence properties of neural networks with relu activation functions\n", "abstract": " Neural networks have become popular methods for tackling various machine learning tasks and are increasingly applied in safety-critical systems. This necessitates verified statements about their behavior and properties. One of these properties is the equivalence of two neural networks, which is important, e.g., when neural networks shall be reduced to smaller ones that fit space and memory constraints of embedded or mobile systems. In this paper, we present the encoding of feed-forward neural networks with ReLU activation functions and define novel and relaxed equivalence properties that extend previously proposed notions of equivalence. We define - and top-k-equivalence and employ it in conjunction with restricting the input space by hierarchical clustering. Networks and properties are encoded as mixed integer linear programs (MILP). We evaluate our approach using two existing reduction methods on\u00a0\u2026", "num_citations": "2\n", "authors": ["681"]}
{"title": "An enhanced fault prediction model for embedded software based on code churn, complexity metrics, and static analysis results\n", "abstract": " Software systems evolve over time because of functionality extensions, changes in requirements, optimization of code, fixes for security and reliability bugs, etc., and it is commonly known that software quality assurance is thus a continuous issue and is often extremely time-consuming. Therefore, techniques to obtain early estimates of fault-proneness can help in increasing the efficiency and effectiveness of software quality assurance. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to the quality of the software. This paper extends our previous work, where we demonstrated that the combination of code complexity metrics together with static analysis results allows accurate prediction of fault density and to build classifiers discriminating faulty from non-faulty components. The extension presented in this paper augments our predictor and classifier with code churn metrics. We applied our methodology to C++ projects from Daimler\u2019s head unit development. In experiments to separate fault-prone from non-fault-prone components, our new approach achieved a classification accuracy of 89%, and the regressor predicted the fault density with an accuracy of 85.7%. This is an improvement of 7.5% with respect to the accuracy of fault density prediction, and an improvement of 10% to the accuracy of fault classification compared to our previous approach that did not take code churn metrics into account.", "num_citations": "2\n", "authors": ["681"]}
{"title": "Memory efficient parallel sat solving with inprocessing\n", "abstract": " Automatic heuristic configuration and algorithm selection can tremendously improve performance in industrial use-cases of SAT solving. In contrast to attempting to select the best heuristic for the problem, portfolio approaches in parallel SAT solving run different heuristics and even algorithms in parallel. This kind of diversification can be very successful because different heuristics and heuristic configurations have better runtimes on different problems. However, such approaches often suffer from high memory consumption. We present a parallel portfolio SAT solver that is based on several totally different branching heuristics and configurations. In contrast to similar approaches, our portfolio solver uses a shared clause database. We show how to asynchronously manage concurrent access to a shared clause database in a parallel portfolio of solvers that can also perform inprocessing.", "num_citations": "2\n", "authors": ["681"]}
{"title": "Integrating static code analysis toolchains\n", "abstract": " This paper proposes an approach for a tool-agnostic and heterogeneous static code analysis toolchain in combination with an exchange format. This approach enhances both traceability and comparability of analysis results. State of the art toolchains support features for either test execution and build automation or traceability between tests, requirements and design information. Our approach combines all those features and extends traceability to the source code level, incorporating static code analysis. As part of our approach we introduce the \"ASSUME Static Code Analysis tool exchange format\" that facilitates the comparability of different static code analysis results. We demonstrate how this approach enhances the usability and efficiency of static code analysis in a development process. On the one hand, our approach enables the exchange of results and evaluations between static code analysis tools. On the\u00a0\u2026", "num_citations": "2\n", "authors": ["681"]}
{"title": "Application and hard combinatorial benchmarks in SAT challenge 2012\n", "abstract": " We outline the selection process of application and hard combinatorial benchmarks for SAT Challenge 2012, and present a description of the benchmark sets. The 600 selected benchmarks in each category were used in Application SAT+ UNSAT, Hard Combinatorial SAT+ UNSAT, Parallel Application SAT+ UNSAT, and Sequential Portfolio tracks of the event.", "num_citations": "2\n", "authors": ["681"]}
{"title": "SLA-based SAN design\n", "abstract": " Storage Area Networks (SANs) connect storage devices to servers over fast network interconnects. We consider the problem of optimal SAN configuration with the goal of retaining more security in meeting service level agreements (SLAs) on unexpected peaks. First, we give an algorithm for assigning storage devices to applications running on the SAN's hosts. This algorithm tries to balance the workload as evenly as possible over all storage devices. Our second algorithm takes these assignments and computes the interconnections (data paths) that are necessary to achieve the desired configuration while respecting redundancy (safety) requirements in the SLAs. Again, this algorithm tries to balance the workload of all connections and devices. Thus, our network configurations respect all SLAs and provide flexibility for future changes by avoiding bottlenecks on storage devices or switches. We also discuss\u00a0\u2026", "num_citations": "2\n", "authors": ["681"]}
{"title": "Automatic Software Model Checking of Thousands of Linux Modules-A Case Study with Avinux\n", "abstract": " Modular software model checking of large real-world systems is known to require extensive manual effort in environment modelling and preparing source code for model checking. Avinux is a tool chain that facilitates the automatic analysis of Linux and especially of Linux device drivers. The tool chain is implemented as a plugin for the Eclipse IDE, using the source code bounded model checker CBMC as its backend. Avinux supports a verification process for Linux that is built upon specification annotations with SLICx (an extension of the SLIC language), automatic data environment creation, source code transformation and simplification, and the invocation of the verification backend. In this paper technical details of the verification process are presented: Using Avinux on thousands of drivers from various Linux versions lead to the discovery of six new errors. In these experiments, Avinux also reduced the immense overhead of manual code preprocessing that other projects incurred.", "num_citations": "2\n", "authors": ["681"]}
{"title": "Checking consistency and completeness of on-line product manuals\n", "abstract": " As products are growing more complex, so is their documentation. With an increasing number of product options, the diversity in service and maintenance procedures grows accordingly. This trend also holds for large-scale medical devices such as magnetic resonance (MR) tomographs. Siemens Medical Solutions has thus decided against one common on-line service handbook for all its MR tomographs. Instead, they fragment the on-line documentation into small packages, out of which a suitable subset is selected for each individual product instance. Selection of (so-called) help packages is controlled by XML terms encoding Boolean choice conditions. To assure that the set of available help packages is sufficient for all valid product instances, we developed a tool called HelpChecker that provides a transformation of XML terms to propositional logic formulas and then employs BDD-based methods to\u00a0\u2026", "num_citations": "2\n", "authors": ["681"]}
{"title": "Practical Applications of SAT\n", "abstract": " Practical Applications of SAT Page 1 Practical Applications of SAT Carsten Sinz Institute for Formal Models and Verification Johannes Kepler University Linz Linz, Austria Page 2 1 Motivation \u220e (x*y == x+y+674) && (x-6 == 4*(y-6)) \u220e Solution for x, y in Z? \u220e SW-Verification: Solution in Z mod 232? \u220e Demo: c32sat \u220e SAT-based solver / tautology checker for Cexpressions \u220e Just checked 23\u22c532 \" 7.9\u22c51028 variable assignments using a state-of-the-art SAT-solver! Page 3 2 Part 1: Industrial Applications Page 4 3 Application 1: Product Configuration \u220e Configurable products, model lines \u220e Products assembled out of standardized components \u220e Eg computers, cars, telecommunication equipment \u220e Dependencies between components \u220e Specified using logical formalism (\u201eproduct overview\u201c) \u220e Automatic (rule-based) order processing system \u220e Checks customer#s order, transforms it into a parts list \u220e Computational \u2026", "num_citations": "2\n", "authors": ["681"]}
{"title": "Collaborative Management of Benchmark Instances and their Attributes\n", "abstract": " Experimental evaluation is an integral part in the design process of algorithms. Publicly available benchmark instances are widely used to evaluate methods in SAT solving. For the interpretation of results and the design of algorithm portfolios their attributes are crucial. Capturing the interrelation of benchmark instances and their attributes is considerably simplified through our specification of a benchmark instance identifier. Thus, our tool increases the availability of both by providing means to manage and retrieve benchmark instances by their attributes and vice versa. Like this, it facilitates the design and analysis of SAT experiments and the exchange of results.", "num_citations": "1\n", "authors": ["681"]}
{"title": "SoftWipe-a tool and benchmark to assess scientific software quality\n", "abstract": " Scientific software from all areas of scientific research is pivotal to obtaining novel insights. Yet the quality of scientific software is rarely assessed, even though it might lead to incorrect scientific results in the worst case. Therefore, we have developed an open source tool and benchmark called SoftWipe, that provides a relative software quality ranking of 51 computational tools from diverse research areas. SoftWipe can be used in the review process of software papers and to inform the scientific software selection process.", "num_citations": "1\n", "authors": ["681"]}
{"title": "Automatic modularization of large programs for bounded model checking\n", "abstract": " The verification of real-world applications is a continuous challenge which yielded numerous different methods and approaches. However, scalability of precise analysis methods on large programs is still limited. We thus propose a formal definition of modules that allows a partitioning of the program into smaller code fragments suitable for verification by bounded model checking. We consider programs written in C/C++ and use LLVM as an intermediate representation. A formal trace semantics for LLVM program runs is defined that also takes modularization into account. Using different abstractions and a selection of fragments of a program for each module, we describe four different modularization approaches. We define desirable properties of modularizations, and show how a bounded model checking approach can be adapted for modularization. Two modularization approaches are implemented within\u00a0\u2026", "num_citations": "1\n", "authors": ["681"]}
{"title": "Unbounded Software Model Checking with Incremental SAT-Solving\n", "abstract": " This paper describes a novel unbounded software model checking approach to find errors in programs written in the C language based on incremental SAT-solving. Instead of using the traditional assumption based API to incremental SAT solvers we use the DimSpec format that is used in SAT based automated planning. A DimSpec formula consists of four CNF formulas representing the initial, goal and intermediate states and the relations between each pair of neighboring states of a transition system. We present a new tool called LLUMC which encodes the presence of certain errors in a C program into a DimSpec formula, which can be solved by either an incremental SAT-based DimSpec solver or the IC3 algorithm for invariant checking. We evaluate the approach in the context of SAT-based model checking for both the incremental SAT-solving and the IC3 algorithm. We show that our encoding expands the\u00a0\u2026", "num_citations": "1\n", "authors": ["681"]}
{"title": "Automatic heavy-weight static analysis tools for finding bugs in safety-critical embedded C/C++ code\n", "abstract": " This paper motivates the use of automatic heavy-weight static analysis tools to find bugs in C (and C++) code for safety-critical embedded systems. By heavy-weight we mean tools that employ powerful analysis to cover all cases. The paper introduces two automatic and relatively heavy-weight tools that are currently employed in the automotive industry, and depicts their underlying techniques, advantages, and disadvantages. Since their results are often imprecise (false positives or false negatives), we advocate the use of alternative techniques such as software bounded model checking (SBMC), which can achieve bit-precise results. Finally, the tool LLBMC is described as an example of a tool implementing SBMC, which makes use of satisfiability modulo theories (SMT) decision procedures as well as the LLVM compiler framework.", "num_citations": "1\n", "authors": ["681"]}
{"title": "Theory and Applications of Satisfiability Testing\u2014SAT 2014\n", "abstract": " This volume contains the papers presented at the 17th International Conference on Theory and Applications of Satisfiability Testing (SAT 2014) held during July 14\u201317, 2014, in Vienna, Austria. SAT 2014 was part of the Federated Logic Conference (FLoC) 2014 and the Vienna Summer of Logic (VSL) and was hosted by the Vienna University of Technology.The International Conference on Theory and Applications of Satisfiability Testing (SAT) is the primary annual meeting for researchers focusing on the theory and applications of the propositional satisfiability problem, broadly construed: Besides plain propositional satisfiability, it includes Boolean optimization (including MaxSAT and Pseudo-Boolean, PB, constraints), Quantified Boolean Formulas (QBF), Satisfiability Modulo Theories (SMT), and Constraint Programming (CP) for problems with clear connections to propositional reasoning. Many hard combinatorial\u00a0\u2026", "num_citations": "1\n", "authors": ["681"]}
{"title": "Proceedings of SAT Challenge 2012: Solver and Benchmark Descriptions\n", "abstract": " The area of SAT solving has seen tremendous progress over the last years. Many problems (eg, in hardware and software verification) that seemed to be completely out of reach a decade ago can now be handled routinely. Besides new algorithms and better heuristics, refined implementation techniques turned out to be vital for this success. To keep up the driving force in improving SAT solvers, we want to motivate implementors to present their work to a broader audience and to compare it with that of others. SAT Challenge 2012 (SC 2012), a competitive event for solvers of the Boolean Satisfiability (SAT) problem, took place within 2012. It was organized as a satellite event to the 15th International Conference on Theory and Applications of Satisfiability Testing (SAT 2012) and stands in the tradition of the SAT Competitions held yearly from 2002 to 2005 and biannually starting from 2007, and the SAT-Races held in 2006, 2008 and 2010. SC 2012 consisted of 5 competition tracks, including three main tracks for sequential solvers (Application SAT+ UNSAT containing problem encodings (both SAT and UNSAT) from real-world applications, such as hardware and software verification, bio-informatics, planning, scheduling, etc; Hard Combinatorial SAT+ UNSAT containing combinatorial problems (both SAT and UNSAT) to challenge current SAT solving algorithms, similar to the SAT Competition\u2019s category \u201ccrafted\u201d; and Random SAT, containing randomly generated satisfiable instances); one track for parallel solvers (with eight computing cores, using Application SAT+ UNSAT instances); and one track for sequential portfolio solvers (1/3 Application\u00a0\u2026", "num_citations": "1\n", "authors": ["681"]}
{"title": "Automatisches Beweisen\n", "abstract": " F\u00fcr eine Formel F\u2208 \u03a6 bezeichnen wir mit V ar (F) die in F auftretenden Variablen. Ein Literal ist ein Atom oder dessen Negation.(zB x,\u00ac y,...). \u03a60 ist die Menge der positiven Literale.\u00ac \u03a60 ist die Menge der negativen Literale. Dabei gilt:\u00ac \u03a60:={\u00ac x| x\u2208 \u03a60} Die Menge der Literale bezeichnen wir mit L (Es ist L= \u03a60\u222a\u00ac \u03a60)", "num_citations": "1\n", "authors": ["681"]}
{"title": "Combining parallel and distributed search in automated equational deduction\n", "abstract": " We present an automated deduction system for equational reasoning combining two different parallelization/distribution schemes: Strategy-compliant parallelization on the level of individual deduction steps (PaReDuX) and distributed cooperation of multiple agents with different search strategies (Teamwork). In our experiments we mainly observed a multiplication of the speed-ups of each approach in our combined system.", "num_citations": "1\n", "authors": ["681"]}