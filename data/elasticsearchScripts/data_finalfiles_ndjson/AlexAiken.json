{"title": "Winnowing: local algorithms for document fingerprinting\n", "abstract": " Digital content is for copying: quotation, revision, plagiarism, and file sharing all create copies. Document fingerprinting is concerned with accurately identifying copying, including small partial copies, within large sets of documents. We introduce the class of local document fingerprinting algorithms, which seems to capture an essential property of any finger-printing technique guaranteed to detect copies. We prove a novel lower bound on the performance of any local algorithm. We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33% of the lower bound. Finally, we also give experimental results on Web data, and report experience with MOSS, a widely-used plagiarism detection service.", "num_citations": "1530\n", "authors": ["1863"]}
{"title": "Scalable statistical bug isolation\n", "abstract": " We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.", "num_citations": "1044\n", "authors": ["1863"]}
{"title": "A first step towards automated detection of buffer overrun vulnerabilities.\n", "abstract": " We describe a new technique for finding potential buffer overrun vulnerabilities in security-critical C code. The key to success is to use static analysis: we formulate detection of buffer overruns as an integer range analysis problem. One major advantage of static analysis is that security bugs can be eliminated before code is deployed. We have implemented our design and used our prototype to find new remotely-exploitable vulnerabilities in a large, widely deployed software package. An earlier hand audit missed these bugs.", "num_citations": "1041\n", "authors": ["1863"]}
{"title": "Bug Isolation via Remote Program Sampling\n", "abstract": " We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.", "num_citations": "753\n", "authors": ["1863"]}
{"title": "Titanium: a high\u2010performance Java dialect\n", "abstract": " Titanium is a language and system for high-performance parallel scientific computing. Titanium uses Java as its base, thereby leveraging the advantages of that language and allowing us to focus attention on parallel computing issues. The main additions to Java are immutable classes, multidimensional arrays, an explicitly parallel SPMD model of computation with a global address space, and zone-based memory management. We discuss these features and our design approach, and report progress on the development of Titanium, including our current driving application: a three-dimensional adaptive mesh refinement parallel Poisson solver.\u00a9 1998 John Wiley & Sons, Ltd.", "num_citations": "725\n", "authors": ["1863"]}
{"title": "Sequoia: programming the memory hierarchy\n", "abstract": " We present Sequoia, a programming language designed to facilitate the development of memory hierarchy aware parallel programs that remain portable across modern machines featuring different memory hierarchy configurations. Sequoia abstractly exposes hierarchical memory in the programming model and provides language mechanisms to describe communication vertically through the machine and to localize computation to particular memory locations within it. We have implemented a complete programming system, including a compiler and runtime systems for Cell processor-based blade systems and distributed memory clusters, and demonstrate efficient performance running Sequoia programs on both of these platforms.", "num_citations": "661\n", "authors": ["1863"]}
{"title": "Effective static race detection for Java\n", "abstract": " We present a novel technique for static race detection in Java programs, comprised of a series of stages that employ a combination of static analyses to successively reduce the pairs of memory accesses potentially involved in a race. We have implemented our technique and applied it to a suite of multi-threaded Java programs. Our experiments show that it is precise, scalable, and useful, reporting tens to hundreds of serious and previously unknown concurrency bugs in large, widely-used programs with few false alarms.", "num_citations": "655\n", "authors": ["1863"]}
{"title": "Legion: Expressing locality and independence with logical regions\n", "abstract": " Modern parallel architectures have both heterogeneous processors and deep, complex memory hierarchies. We present Legion, a programming model and runtime system for achieving high performance on these machines. Legion is organized around logical regions, which express both locality and independence of program data, and tasks, functions that perform computations on regions. We describe a runtime system that dynamically extracts parallelism from Legion programs, using a distributed, parallel scheduling algorithm that identifies both independent tasks and nested parallelism. Legion also enables explicit, programmer controlled movement of data through the memory hierarchy and placement of tasks based on locality information via a novel mapping interface. We evaluate our Legion implementation on three applications: fluid-flow on a regular grid, a three-level AMR code solving a heat diffusion\u00a0\u2026", "num_citations": "619\n", "authors": ["1863"]}
{"title": "Static Detection of Security Vulnerabilities in Scripting Languages.\n", "abstract": " We present a static analysis algorithm for detecting security vulnerabilities in PHP, a popular server-side scripting language for building web applications. Our analysis employs a novel three-tier architecture to capture information at decreasing levels of granularity at the intrablock, intraprocedural, and interprocedural level. This architecture enables us to handle dynamic features of scripting languages that have not been adequately addressed by previous techniques.", "num_citations": "519\n", "authors": ["1863"]}
{"title": "Type inclusion constraints and type inference\n", "abstract": " We present a general algorithm for solving systems of inclusion constraints over type expressions. The constraint language includes function types, constructor types, and liberal intersection and union types. We illustrate the application of our constraint solving algorithm with a type inference system for the lambda calculus with constants. In this system, every pure lambda term has a (computable) type and every term typable in the Hindley/Milner system has all of its Hindley/Milner types. Thus, the inference system is an extension of the Hindley/Milner system that can type a very large set of lambda terms.", "num_citations": "428\n", "authors": ["1863"]}
{"title": "Optimal loop parallelization\n", "abstract": " Parallelizing compilers promise to exploit the parallelism available in a given program, particularly parallelism that is too low-level or irregular to be expressed by hand in an algorithm. However, existing parallelization techniques do not handle loops in a satisfactory manner. Fine-grain (instruction level) parallelization, or compaction, captures irregular parallelism inside a loop body but does not exploit parallelism acfoss loop iterations.\u2019 Coarser methods, such as doacross [9], sacrifice irregular forms of parallelism in favor of pipelining iterations (software pipelining). Both of these approaches often yield suboptimal speedups even under the best conditions-when resources are plentiful and processors are synchronous. In this paper we present a new technique bridging the gap between fine-and coarse-grain loop parallelization, allowing the exploitation of parallelism inside and across loop iterations. Furthermore, we\u00a0\u2026", "num_citations": "376\n", "authors": ["1863"]}
{"title": "Soft typing with conditional types\n", "abstract": " We present a simple and powerful type inference method for dynamically typed languages where no type information is supplied by the user. Type inference is reduced to the problem of solvability of a system of type inclusion constraints over a type language that includes function types, constructor types, union, intersection, and recursive types, and conditional types. Conditional types enable us to analyze control flow using type inference, thus facilitating computation of accurate types. We demonstrate the power and practicality of the method with examples and performance results from an implementation.", "num_citations": "313\n", "authors": ["1863"]}
{"title": "Liszt: a domain specific language for building portable mesh-based PDE solvers\n", "abstract": " Heterogeneous computers with processors and accelerators are becoming widespread in scientific computing. However, it is difficult to program hybrid architectures and there is no commonly accepted programming model. Ideally, applications should be written in a way that is portable to many platforms, but providing this portability for general programs is a hard problem.", "num_citations": "312\n", "authors": ["1863"]}
{"title": "Behavior of database production rules: Termination, confluence, and observable determinism\n", "abstract": " Static analysis methods are given for determining whether arbitrary sets of database production rules are (1) guaranteed to terminate;(2) guaranteed to produce a unique final database state;(3) guaranteed to produce a unique stream of observable actions. When the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System; they will form the basis of an interactive development environment for Starburst rule programmers.", "num_citations": "312\n", "authors": ["1863"]}
{"title": "Attack-Resistant Trust Metrics for Public Key Certification.\n", "abstract": " This paper investigates the role of trust metrics in attack-resistant public key certification. We present an analytical framework for understanding the effectiveness of trust metrics in resisting attacks, including a characterization of the space of possible attacks. Within this framework, we establish the theoretical best case for a trust metric. Finally, we present a practical trust metric based on network flow that meets this theoretical bound.", "num_citations": "294\n", "authors": ["1863"]}
{"title": "Stochastic superoptimization\n", "abstract": " We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly.", "num_citations": "292\n", "authors": ["1863"]}
{"title": "Scalable error detection using boolean satisfiability\n", "abstract": " We describe a software error-detection tool that exploits recent advances in boolean satisfiability (SAT) solvers. Our analysis is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the boolean formulas that model the control-and data-flow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing inter-procedural analysis without a dramatic increase in the size of the boolean constraints to be solved. We demonstrate the effectiveness of our approach by constructing a lock interface inference and checking tool. In an interprocedural analysis of more than 23,000 lock related functions in the latest Linux kernel, the checker generated 300 warnings, of which\u00a0\u2026", "num_citations": "263\n", "authors": ["1863"]}
{"title": "Memory management with explicit regions\n", "abstract": " Much research has been devoted to studies of and algorithms for memory management based on garbage collection or explicit allocation and deallocation. An alternative approach, region-based memory management, has been known for decades, but has not been well-studied. In a region-based system each allocation specifies a region, and memory is reclaimed by destroying a region, freeing all the storage allocated therein. We show that on a suite of allocation-intensive C programs, regions are competitive with malloc/free and sometimes substantially faster. We also show that regions support safe memory management with low overhead. Experience with our benchmarks suggests that modifying many existing programs to use regions is not difficult.", "num_citations": "263\n", "authors": ["1863"]}
{"title": "Perfect pipelining: A new loop parallelization technique\n", "abstract": " Parallelizing compilers do not handle loops in a satisfactory manner. Fine-grain transformations capture irregular parallelism inside a loop body not amenable to coarser approaches but have limited ability to exploit parallelism across iterations. Coarse methods sacrifice irregular forms of parallelism in favor of pipelining (overlapping) iterations. In this paper we present a new transformation, Perfect Pipelining, that bridges the gap between these fine- and coarse-grain transformations while retaining the desirable features of both. This is accomplished even in the presence of conditional branches and resource constraints. To make our claims rigorous, we develop a formalism for parallelization. The formalism can also be used to compare transformations across computational models. As an illustration, we show that Doacross, a transformation intended for synchronous and asynchronous multiprocessors, can be\u00a0\u2026", "num_citations": "262\n", "authors": ["1863"]}
{"title": "Static analysis techniques for predicting the behavior of active database rules\n", "abstract": " This article gives methods for statically analyzing sets of active database rules to determine if the rules are (1) guaranteed to terminate, (2) guaranteed to produce a unique final database state, and (3) guaranteed to produce a unique stream of observable actions. If the analysis determines that one of these properties is not guaranteed, it isolates the rules responsible for the problem and determines criteria that, if satisfied, guarantee the property. The analysis methods are presented in the context of the Starburst Rule System.", "num_citations": "237\n", "authors": ["1863"]}
{"title": "Statistical debugging: simultaneous identification of multiple bugs\n", "abstract": " We describe a statistical approach to software debugging in the presence of multiple bugs. Due to sparse sampling issues and complex interaction between program predicates, many generic off-the-shelf algorithms fail to select useful bug predictors. Taking inspiration from bi-clustering algorithms, we propose an iterative collective voting scheme for the program runs and predicates. We demonstrate successful debugging results on several real world programs and a large debugging benchmark suite.", "num_citations": "229\n", "authors": ["1863"]}
{"title": "Conditional must not aliasing for static race detection\n", "abstract": " Race detection algorithms for multi-threaded programs using the common lock-based synchronization idiom must correlate locks with the memory locations they guard. The heart of a proof of race freedom is showing that if two locks are distinct, then the memory locations they guard are also distinct. This is an example of a general property we call conditional must not aliasing: Under the assumption that two objects are not aliased, prove that two other objects are not aliased. This paper introduces and gives an algorithm for conditional must not alias analysis and discusses experimental results for sound race detection of Java programs.", "num_citations": "224\n", "authors": ["1863"]}
{"title": "Beyond Data and Model Parallelism for Deep Neural Networks.\n", "abstract": " Existing deep learning systems commonly parallelize deep neural network (DNN) training using data or model parallelism, but these strategies often result in suboptimal parallelization performance. We introduce SOAP, a more comprehensive search space of parallelization strategies for DNNs that includes strategies to parallelize a DNN in the Sample, Operator, Attribute, and Parameter dimensions. We present FlexFlow, a deep learning engine that uses guided randomized search of the SOAP space to find a fast parallelization strategy for a specific parallel machine. To accelerate this search, FlexFlow introduces a novel execution simulator that can accurately predict a parallelization strategy\u2019s performance and is three orders of magnitude faster than prior approaches that execute each strategy. We evaluate FlexFlow with six real-world DNN benchmarks on two GPU clusters and show that FlexFlow increases training throughput by up to 3.3\u00d7 over state-of-the-art approaches, even when including its search time, and also improves scalability.", "num_citations": "206\n", "authors": ["1863"]}
{"title": "Introduction to set constraint-based program analysis\n", "abstract": " This paper given an introduction to using set constraints to specify program analyses. Several standard analysis problems are formulated using set constraints, which serves both to illustrate the style of using constraints to specify program analysis problems and the range of application of set constraints.", "num_citations": "203\n", "authors": ["1863"]}
{"title": "Solving systems of set constraints\n", "abstract": " Systems of set constraints are a natural formalism for many problems in program analysis. Set constraints are also a generalization of tree automata. We present an algorithm for solving systems of set constraints built from free variables, constructors, and the set operations of intersection, union, and complement. Furthermore, we show that all solutions of such systems can be nitely represented. 1", "num_citations": "203\n", "authors": ["1863"]}
{"title": "Saturn: A scalable framework for error detection using boolean satisfiability\n", "abstract": " This article presents Saturn, a general framework for building precise and scalable static error detection systems. Saturn exploits recent advances in Boolean satisfiability (SAT) solvers and is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the Boolean formulas that model the control flow and data flow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing interprocedural analysis without a dramatic increase in the size of the Boolean constraints to be solved. We have experimentally validated our approach by conducting two case studies involving a Linux lock checker and a memory leak checker. Results from the experiments show that our system\u00a0\u2026", "num_citations": "194\n", "authors": ["1863"]}
{"title": "Relational queries over program traces\n", "abstract": " Instrumenting programs with code to monitor runtime behavior is a common technique for profiling and debugging. In practice, instrumentation is either inserted manually by programmers, or automatically by specialized tools that monitor particular properties. We propose Program Trace Query Language (PTQL), a language based on relational queries over program traces, in which programmers can write expressive, declarative queries about program behavior. We also describe our compiler, Partiqle. Given a PTQL query and a Java program, Partiqle instruments the program to execute the query on-line. We apply several PTQL queries to a set of benchmark programs, including the Apache Tomcat Web server. Our queries reveal significant performance bugs in the jack SpecJVM98 benchmark, in Tomcat, and in the IBM Java class library, as well as some correct though uncomfortably subtle code in the Xerces XML\u00a0\u2026", "num_citations": "193\n", "authors": ["1863"]}
{"title": "Language support for regions\n", "abstract": " Region-based memory management systems structure memory by grouping objects in regions under program control. Memory is reclaimed by deleting regions, freeing all objects stored therein. Our compiler for C with regions, RC, prevents unsafe region deletions by keeping a count of references to each region. Using type annotations that make the structure of a program's regions more explicit, we reduce the overhead of reference counting from a maximum of 27% to a maximum of 11% on a suite of realistic benchmarks. We generalise these annotations in a region type system whose main novelty is the use of existentially quantified abstract regions to represent pointers to objects whose region is partially or totally unknown. A distribution of RC is available at http://www. cs. berkeley. edu/~ dgay/rc. tar. gz.", "num_citations": "178\n", "authors": ["1863"]}
{"title": "Automatic generation of peephole superoptimizers\n", "abstract": " Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler.", "num_citations": "177\n", "authors": ["1863"]}
{"title": "Stochastic optimization of floating-point programs with tunable precision\n", "abstract": " The aggressive optimization of floating-point computations is an important problem in high-performance computing. Unfortunately, floating-point instruction sets have complicated semantics that often force compilers to preserve programs as written. We present a method that treats floating-point optimization as a stochastic search problem. We demonstrate the ability to generate reduced precision implementations of Intel's handwritten C numeric library which are up to 6 times faster than the original code, and achieve end-to-end speedups of over 30% on a direct numeric simulation and a ray tracer by optimizing kernels that can tolerate a loss of precision while still remaining correct. Because these optimizations are mostly not amenable to formal verification using the current state of the art, we present a stochastic search technique for characterizing maximum error. The technique comes with an asymptotic guarantee\u00a0\u2026", "num_citations": "168\n", "authors": ["1863"]}
{"title": "Context-and path-sensitive memory leak detection\n", "abstract": " We present a context-and path-sensitive algorithm for detecting memory leaks in programs with explicit memory management. Our leak detection algorithm is based on an underlying escape analysis: any allocated location in a procedure P that is not deallocated in P and does not escape from P is leaked. We achieve very precise context-and path-sensitivity by expressing our analysis using boolean constraints. In experiments with six large open source projects our analysis produced 510 warnings of which 455 were unique memory leaks, a false positive rate of only 10.8%. A parallel implementation improves performance by over an order of magnitude on large projects; over five million lines of code in the Linux kernel is analyzed in 50 minutes.", "num_citations": "167\n", "authors": ["1863"]}
{"title": "Measuring empirical computational complexity\n", "abstract": " The standard language for describing the asymptotic behavior of algorithms is theoretical computational complexity. We propose a method for describing the asymptotic behavior of programs in practice by measuring their empirical computational complexity. Our method involves running a program on workloads spanning several orders of magnitude in size, measuring their performance, and fitting these observations to a model that predicts performance as a function of workload size. Comparing these models to the programmer's expectations or to theoretical asymptotic bounds can reveal performance bugs or confirm that a program's performance scales as expected. Grouping and ranking program locations based on these models focuses attention on scalability-critical code. We describe our tool, the Trend Profiler (trend-prof), for constructing models of empirical computational complexity that predict how many\u00a0\u2026", "num_citations": "160\n", "authors": ["1863"]}
{"title": "Sound, complete and scalable path-sensitive analysis\n", "abstract": " We present a new, precise technique for fully path-and context-sensitive program analysis. Our technique exploits two observations: First, using quantified, recursive formulas, path-and context-sensitive conditions for many program properties can be expressed exactly. To compute a closed form solution to such recursive constraints, we differentiate between observable and unobservable variables, the latter of which are existentially quantified in our approach. Using the insight that unobservable variables can be eliminated outside a certain scope, our technique computes satisfiability-and validity-preserving closed-form solutions to the original recursive constraints. We prove the solution is as precise as the original system for answering may and must queries as well as being small in practice, allowing our technique to scale to the entire Linux kernel, a program with over 6 million lines of code.", "num_citations": "149\n", "authors": ["1863"]}
{"title": "Barrier inference\n", "abstract": " Many parallel programs are written in SPMD style ie by running the same sequential program on all processes. SPMD programs include synchronization, but it is easy to write incorrect synchronization patterns. We propose a system that verifies a program's synchronization pattern. We also propose language features to make the synchronization pattern more explicit and easily checked. We have implemented a prototype of our system for Split-C and successfully verified the synchronization structure of realistic programs.", "num_citations": "148\n", "authors": ["1863"]}
{"title": "A data driven approach for algebraic loop invariants\n", "abstract": " We describe a Guess-and-Check algorithm for computing algebraic equation invariants of the form \u2227\u2009                   i                                  f                                    i                 (x                 1,\u2026,x                                    n                 )\u2009=\u20090, where each f                                    i                  is a polynomial over the variables x                 1,\u2026,x                                    n                  of the program. The \u201cguess\u201d phase is data driven and derives a candidate invariant from data generated from concrete executions of the program. This candidate invariant is subsequently validated in a \u201ccheck\u201d phase by an off-the-shelf SMT solver. Iterating between the two phases leads to a sound algorithm. Moreover, we are able to prove a bound on the number of decision procedure queries which Guess-and-Check requires to obtain a sound invariant. We show how Guess-and-Check can be extended to generate arbitrary boolean combinations of linear equalities\u00a0\u2026", "num_citations": "143\n", "authors": ["1863"]}
{"title": "The complexity of set constraints\n", "abstract": " Set constraints are relations between sets of terms. They have been used extensively in various applications in program analysis and type inference. We present several results on the computational complexity of solving systems of set constraints. The systems we study form a natural complexity hierarchy depending on the form of the constraint language.", "num_citations": "142\n", "authors": ["1863"]}
{"title": "An overview of the Saturn project\n", "abstract": " We present an overview of the Saturn program analysis system, including a rationale for three major design decisions: the use of function-at-a-time, or summary-based, analysis, the use of constraints, and the use of a logic programming language to express program analysis algorithms. We argue that the combination of summaries and constraints allows Saturn to achieve both great scalability and great precision, while the use of a logic programming language with constraints allows for succinct, high-level expression of program analyses.", "num_citations": "140\n", "authors": ["1863"]}
{"title": "Tioga-2: A direct manipulation database visualization environment\n", "abstract": " The paper reports on user experience with Tioga, a DBMS centric visualization tool developed at Berkeley. Based on this experience, we have designed Tioga-2 as a direct manipulation system that is more powerful and much easier to program. A detailed design of the revised system is presented, together with an extensive example of its application.", "num_citations": "140\n", "authors": ["1863"]}
{"title": "Synthesizing program input grammars\n", "abstract": " We present an algorithm for synthesizing a context-free grammar encoding the language of valid program inputs from a set of input examples and blackbox access to the program. Our algorithm addresses shortcomings of existing grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation, GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers.", "num_citations": "136\n", "authors": ["1863"]}
{"title": "Terra: a multi-stage language for high-performance computing\n", "abstract": " High-performance computing applications, such as auto-tuners and domain-specific languages, rely on generative programming techniques to achieve high performance and portability. However, these systems are often implemented in multiple disparate languages and perform code generation in a separate process from program execution, making certain optimizations difficult to engineer. We leverage a popular scripting language, Lua, to stage the execution of a novel low-level language, Terra. Users can implement optimizations in the high-level language, and use built-in constructs to generate and execute high-performance Terra code. To simplify meta-programming, Lua and Terra share the same lexical environment, but, to ensure performance, Terra code can execute independently of Lua's runtime. We evaluate our design by reimplementing existing multi-language systems entirely in Terra. Our Terra\u00a0\u2026", "num_citations": "136\n", "authors": ["1863"]}
{"title": "A development environment for horizontal microcode\n", "abstract": " A development environment for horizontal microcode is described that uses percolation scheduling-a transformational system for parallelism extraction-and an interactive profiling system to give the user control over the microcode compaction process while reducing the burdensome details of architecture, correctness preservation, and synchronization. Through a graphical interface, the user suggests what can be executed in parallel, while the system performs the actual changes using semantics-preserving transformations. If a request cannot be satisfied, the system reports the problem causing the failure. The user can then help eliminate the problem by supplying guidance or information not explicit in the code.< >", "num_citations": "132\n", "authors": ["1863"]}
{"title": "Alert detection in system logs\n", "abstract": " We present Nodeinfo, an unsupervised algorithm for anomaly detection in system logs. We demonstrate Nodeinfo's effectiveness on data from four of the world's most powerful supercomputers: using logs representing over 746 million processor-hours, in which anomalous events called alerts were manually tagged for scoring, we aim to automatically identify the regions of the log containing those alerts. We formalize the alert detection task in these terms, describe how Nodeinfo uses the information entropy of message terms to identify alerts, and present an online version of this algorithm, which is now in production use. This is the first work to investigate alert detection on (several) publicly-available supercomputer system logs, thereby providing a reproducible performance baseline.", "num_citations": "125\n", "authors": ["1863"]}
{"title": "Compilation for explicitly managed memory hierarchies\n", "abstract": " We present a compiler for machines with an explicitly managed memory hierarchy and suggest that a primary role of any compiler for such architectures is to manipulate and schedule a hierarchy of bulk operations at varying scales of the application and of the machine. We evaluate the performance of our compiler using several benchmarks running on a Cell processor.", "num_citations": "119\n", "authors": ["1863"]}
{"title": "Implementing regular tree expressions\n", "abstract": " Regular tree expressions are a natural formalism for describing the sets of tree-structured values that commonly arise in programs; thus, they are well-suited to applications in program analysis. We describe an implementation of regular tree expressions and our experience with that implementation in the context of the FL type system. A combination of algorithms, optimizations, and fast heuristics for computationally difficult problems yields an implementation efficient enough for practical use.", "num_citations": "119\n", "authors": ["1863"]}
{"title": "Static type inference in a dynamically typed language\n", "abstract": " We present a type inference system for FL based on an operational, rather than a denotational, formulation of types. The essential elements of the system are a type language based on regular trees and a type inference logic that implements an abstract interpretation of the operational semantics of FL. We use a non-standard approach to type inference because our requirements\u2014using type information in the optimiza-tion of functional programs\u2014differ substantially from those of other type systems.", "num_citations": "117\n", "authors": ["1863"]}
{"title": "Automated error diagnosis using abductive inference\n", "abstract": " When program verification tools fail to verify a program, either the program is buggy or the report is a false alarm. In this situation, the burden is on the user to manually classify the report, but this task is time-consuming, error-prone, and does not utilize facts already proven by the analysis. We present a new technique for assisting users in classifying error reports. Our technique computes small, relevant queries presented to a user that capture exactly the information the analysis is missing to either discharge or validate the error. Our insight is that identifying these missing facts is an instance of the abductive inference problem in logic, and we present a new algorithm for computing the smallest and most general abductions in this setting. We perform the first user study to rigorously evaluate the accuracy and effort involved in manual classification of error reports. Our study demonstrates that our new technique is very\u00a0\u2026", "num_citations": "115\n", "authors": ["1863"]}
{"title": "Set constraints: Results, applications and future directions\n", "abstract": " Set constraints are a natural formalism for many problems that arise in program analysis. This paper provides a brief introduction to set constraints: what set constraints are, why they are interesting, the current state of the art, open problems, applications and implementations.", "num_citations": "114\n", "authors": ["1863"]}
{"title": "From invariant checking to invariant inference using randomized search\n", "abstract": " We describe a general framework c2i\u00a0for generating an invariant inference procedure from an invariant checking procedure. Given a checker and a language of possible invariants, c2i\u00a0generates an inference procedure that iteratively invokes two phases. The search phase uses randomized search to discover candidate invariants and the validate phase uses the checker to either prove or refute that the candidate is an actual invariant. To demonstrate the applicability of c2i\u00a0, we use it to generate inference procedures that prove safety properties of numerical programs, prove non-termination of numerical programs, prove functional specifications of array manipulating programs, prove safety properties of string manipulating programs, and prove functional specifications of heap manipulating programs that use linked list data structures.", "num_citations": "111\n", "authors": ["1863"]}
{"title": "Fluid updates: Beyond strong vs. weak updates\n", "abstract": " We describe a symbolic heap abstraction that unifies reasoning about arrays, pointers, and scalars, and we define a fluid update operation on this symbolic heap that relaxes the dichotomy between strong and weak updates. Our technique is fully automatic, does not suffer from the kind of state-space explosion problem partition-based approaches are prone to, and can naturally express properties that hold for non-contiguous array elements. We demonstrate the effectiveness of this technique by evaluating it on challenging array benchmarks and by automatically verifying buffer accesses and dereferences in five Unix Coreutils applications with no annotations or false alarms.", "num_citations": "108\n", "authors": ["1863"]}
{"title": "Binary Translation Using Peephole Superoptimizers.\n", "abstract": " We present a new scheme for performing binary translation that produces code comparable to or better than existing binary translators with much less engineering effort. Instead of hand-coding the translation from one instruction set to another, our approach automatically learns translation rules using superoptimization techniques. We have implemented a PowerPC-x86 binary translator and report results on small and large computeintensive benchmarks. When compared to the native compiler, our translated code achieves median performance of 67% on large benchmarks and in some small stress tests actually outperforms the native compiler. We also report comparisons with the open source binary translator Qemu and a commercial tool, Apple\u2019s Rosetta. We consistently outperform the former and are comparable to or faster than the latter on all but one benchmark.", "num_citations": "107\n", "authors": ["1863"]}
{"title": "Resource-constrained software pipelining\n", "abstract": " This paper presents a software pipelining algorithm for the automatic extraction of fine-grain parallelism in general loops. The algorithm accounts for machine resource constraints in a way that smoothly integrates the management of resource constraints with software pipelining. Furthermore, generality in the software pipelining algorithm is not sacrificed to handle resource constraints, and scheduling choices are made with truly global information. Proofs of correctness and the results of experiments with an implementation are also presented.", "num_citations": "106\n", "authors": ["1863"]}
{"title": "Regent: A high-productivity programming language for HPC with logical regions\n", "abstract": " We present Regent, a high-productivity programming language for high performance computing with logical regions. Regent users compose programs with tasks (functions eligible for parallel execution) and logical regions (hierarchical collections of structured objects). Regent programs appear to execute sequentially, require no explicit synchronization, and are trivially deadlock-free. Regent's type system catches many common classes of mistakes and guarantees that a program with correct serial execution produces identical results on parallel and distributed machines.", "num_citations": "104\n", "authors": ["1863"]}
{"title": "Decidability of systems of set constraints with negative constraints\n", "abstract": " Set constraints are relations between sets of terms. They have been used extensively in various applications in program analysis and type inference. Recently, several algorithms for solving general systems of positive set constraints have appeared. In this paper we consider systems of mixed positive and negative constraints, which are considerably more expressive than positive constraints alone. We show that it is decidable whether a given such system has a solution. The proof involves a reduction to a number-theoretic decision problem that may be of independent interest.", "num_citations": "103\n", "authors": ["1863"]}
{"title": "GFS: evolution on fast-forward\n", "abstract": " Kirk McKusick and Sean Quinlan discuss the origin and evolution of the Google File System.", "num_citations": "101\n", "authors": ["1863"]}
{"title": "Static error detection using semantic inconsistency inference\n", "abstract": " Inconsistency checking is a method for detecting software errors that relies only on examining multiple uses of a value. We propose that inconsistency inference is best understood as a variant of the older and better understood problem of type inference. Using this insight, we describe a precise and formal framework for discovering inconsistency errors. Unlike previous approaches to the problem, our technique for finding inconsistency errors is purely semantic and can deal with complex aliasing and path-sensitive conditions. We have built a nullde reference analysis of C programs based on semantic inconsistency inference and have used it to find hundreds of previously unknown null dereference errors in widely used C programs.", "num_citations": "100\n", "authors": ["1863"]}
{"title": "How is aliasing used in systems software?\n", "abstract": " We present a study of all sources of aliasing in over one million lines of C code, identifying in the process the common patterns of aliasing that arise in practice. We find that aliasing has a great deal of structure in real programs and that just nine programming idioms account for nearly all aliasing in our study. Our study requires an automatic alias analysis that both scales to large systems and has a low false positive rate. To this end, we also present a new context-, flow-, and partially path-sensitive alias analysis that, together with a new technique for object naming, achieves a false aliasing rate of 26.2% on our benchmarks.", "num_citations": "98\n", "authors": ["1863"]}
{"title": "Interpolants as classifiers\n", "abstract": " We show how interpolants can be viewed as classifiers in supervised machine learning. This view has several advantages: First, we are able to use off-the-shelf classification techniques, in particular support vector machines (SVMs), for interpolation. Second, we show that SVMs can find relevant predicates for a number of benchmarks. Since classification algorithms are predictive, the interpolants computed via classification are likely to be invariants. Finally, the machine learning view also enables us to handle superficial non-linearities. Even if the underlying problem structure is linear, the symbolic constraints can give an impression that we are solving a non-linear problem. Since learning algorithms try to mine the underlying structure directly, we can discover the linear structure for such problems. We demonstrate the feasibility of our approach via experiments over benchmarks from various papers on\u00a0\u2026", "num_citations": "95\n", "authors": ["1863"]}
{"title": "Precise reasoning for programs using containers\n", "abstract": " Containers are general-purpose data structures that provide functionality for inserting, reading, removing, and iterating over elements. Since many applications written in modern programming languages, such as C++ and Java, use containers as standard building blocks, precise analysis of many programs requires a fairly sophisticated understanding of container contents. In this paper, we present a sound, precise, and fully automatic technique for static reasoning about contents of containers. We show that the proposed technique adds useful precision for verifying real C++ applications and that it scales to applications with over 100,000 lines of code.", "num_citations": "90\n", "authors": ["1863"]}
{"title": "Realm: An event-based low-level runtime for distributed memory architectures\n", "abstract": " We present Realm, an event-based runtime system for heterogeneous, distributed memory machines. Realm is fully asynchronous: all runtime actions are non-blocking. Realm supports spawning computations, moving data, and reservations, a novel synchronization primitive. Asynchrony is exposed via a light-weight event system capable of operating without central management.", "num_citations": "89\n", "authors": ["1863"]}
{"title": "Statistical Debugging of Sampled Programs.\n", "abstract": " We present a novel strategy for automatically debugging programs given sampled data from thousands of actual user runs. Our goal is to pinpoint those features that are most correlated with crashes. This is accomplished by maximizing an appropriately defined utility function. It has analogies with intuitive debugging heuristics, and, as we demonstrate, is able to deal with various types of bugs that occur in real programs.", "num_citations": "89\n", "authors": ["1863"]}
{"title": "The set constraint/CFL reachability connection in practice\n", "abstract": " Many program analyses can be reduced to graph reachability problems involving a limited form of context-free language reachability called Dyck-CFL reachability. We show a new reduction from Dyck-CFL reachability to set constraints that can be used in practice to solve these problems. Our reduction is much simpler than the general reduction from context-free language reachability to set constraints. We have implemented our reduction on top of a set constraints toolkit and tested its performance on a substantial polymorphic flow analysis application.", "num_citations": "88\n", "authors": ["1863"]}
{"title": "Verification as learning geometric concepts\n", "abstract": " We formalize the problem of program verification as a learning problem, showing that invariants in program verification can be regarded as geometric concepts in machine learning. Safety properties define bad states: states a program should not reach. Program verification explains why a program\u2019s set of reachable states is disjoint from the set of bad states. In Hoare Logic, these explanations are predicates that form inductive assertions. Using samples for reachable and bad states and by applying well known machine learning algorithms for classification, we are able to generate inductive assertions. By relaxing the search for an exact proof to classifiers, we obtain complexity theoretic improvements. Further, we extend the learning algorithm to obtain a sound procedure that can generate proofs containing invariants that are arbitrary boolean combinations of polynomial inequalities. We have evaluated our\u00a0\u2026", "num_citations": "85\n", "authors": ["1863"]}
{"title": "Simplifying loop invariant generation using splitter predicates\n", "abstract": " We present a novel static analysis technique that substantially improves the quality of invariants inferred by standard loop invariant generation techniques. Our technique decomposes multi-phase loops, which require disjunctive invariants, into a semantically equivalent sequence of single-phase loops, each of which requires simple, conjunctive invariants. We define splitter predicates which are used to identify phase transitions in loops, and we present an algorithm to find useful splitter predicates that enable the phase-reducing transformation. We show experimentally on a set of representative benchmarks from the literature and real code examples that our technique substantially increases the quality of invariants inferred by standard invariant generation techniques. Our technique is conceptually simple, easy to implement, and can be integrated into any automatic loop invariant generator.", "num_citations": "85\n", "authors": ["1863"]}
{"title": "Using correlated surprise to infer shared influence\n", "abstract": " We propose a method for identifying the sources of problems in complex production systems where, due to the prohibitive costs of instrumentation, the data available for analysis may be noisy or incomplete. In particular, we may not have complete knowledge of all components and their interactions. We define influences as a class of component interactions that includes direct communication and resource contention. Our method infers the influences among components in a system by looking for pairs of components with time-correlated anomalous behavior. We summarize the strength and directionality of shared influences using a Structure-of-Influence Graph (SIG). This paper explains how to construct a SIG and use it to isolate system misbehavior, and presents both simulations and in-depth case studies with two autonomous vehicles and a 9024-node production supercomputer.", "num_citations": "85\n", "authors": ["1863"]}
{"title": "Data-driven equivalence checking\n", "abstract": " We present a data driven algorithm for equivalence checking of two loops. The algorithm infers simulation relations using data from test runs. Once a candidate simulation relation has been obtained, off-the-shelf SMT solvers are used to check whether the simulation relation actually holds. The algorithm is sound: insufficient data will cause the proof to fail. We demonstrate a prototype implementation, called DDEC, of our algorithm, which is the first sound equivalence checker for loops written in x86 assembly.", "num_citations": "83\n", "authors": ["1863"]}
{"title": "Flow-insensitive type qualifiers\n", "abstract": " We describe flow-insensitive type qualifiers, a lightweight, practical mechanism for specifying and checking properties not captured by traditional type systems. We present a framework for adding new, user-specified type qualifiers to programming languages with static type systems, such as C and Java. In our system, programmers add a few type qualifier annotations to their program, and automatic type qualifier inference determines the remaining qualifiers and checks the annotations for consistency. We describe a tool CQual for adding type qualifiers to the C programming language. Our tool CQual includes a visualization component for displaying browsable inference results to the programmer. Finally, we present several experiments using our tool, including inferring const qualifiers, finding security vulnerabilities in several popular C programs, and checking initialization data usage in the Linux kernel. Our results\u00a0\u2026", "num_citations": "80\n", "authors": ["1863"]}
{"title": "Saturn: A SAT-based tool for bug detection\n", "abstract": " Saturn is a boolean satisfiability (SAT) based framework for static bug detection. Saturn targets software written in C and is designed to support a wide range of property checkers.", "num_citations": "79\n", "authors": ["1863"]}
{"title": "Type qualifiers: lightweight specifications to improve software quality\n", "abstract": " Software plays a pivotal role in our daily lives, yet software glitches and security vulnerabilities continue to plague us. Existing techniques for ensuring the quality of software are limited in scope, suggesting that we need to supply programmers with new tools to make it easier to write programs with fewer bugs. In this dissertation, we propose using type qualifiers, a lightweight, type-based mechanism, to improve the quality of software. In our framework, programmers add a few qualifier annotations to their source code, and type qualifier inference determines the remaining qualifiers and checks consistency of the qualifier annotations. In this dissertation we develop scalable inference algorithms for flow-insensitive qualifiers, which are invariant during execution, and for flow-sensitive qualifiers, which may vary from one program point to the next. The latter inference algorithm incorporates flow-insensitive alias analysis\u00a0\u2026", "num_citations": "79\n", "authors": ["1863"]}
{"title": "A realistic resource-constrained software pipelining algorithm\n", "abstract": " This paper presents a new approach to resource-constrained compiler extraction of fine-grain parallelism in loops. We present an algorithm which integrates resource limitations into software pipelining. Our ap-", "num_citations": "79\n", "authors": ["1863"]}
{"title": "TASO: optimizing deep learning computation with automatic generation of graph substitutions\n", "abstract": " Existing deep neural network (DNN) frameworks optimize the computation graph of a DNN by applying graph transformations manually designed by human experts. This approach misses possible graph optimizations and is difficult to scale, as new DNN operators are introduced on a regular basis.", "num_citations": "78\n", "authors": ["1863"]}
{"title": "Exploring hidden dimensions in parallelizing convolutional neural networks\n", "abstract": " The past few years have witnessed growth in the computational requirements for training deep convolutional neural networks. Current approaches parallelize training onto multiple devices by applying a single parallelization strategy (e.g., data or model parallelism) to all layers in a network. Although easy to reason about, these approaches result in suboptimal runtime performance in large-scale distributed training, since different layers in a network may prefer different parallelization strategies. In this paper, we propose layer-wise parallelism that allows each layer in a network to use an individual parallelization strategy. We jointly optimize how each layer is parallelized by solving a graph search problem. Our evaluation shows that layer-wise parallelism outperforms state-of-the-art approaches by increasing training throughput, reducing communication costs, achieving better scalability to multiple GPUs, while maintaining original network accuracy.", "num_citations": "75\n", "authors": ["1863"]}
{"title": "Compaction-based parallelization\n", "abstract": " We present a transformational system for extracting parallelism from programs. Our transformations generate code for synchronous parallel computers, such as  Very Long Instruction Word and pipelined machines. The transformational  system, which is based on percolation scheduling,is simple and uniform. There are four primitive transformations-three that perform code motion plus  loop unrolling-from which all parallelizing algorithms are constructed. Our transformations are studied as a formal system. We define a formal  measure of program improvement, and show that our transformations improve  programs with respect to the measure. This formal approach allows a number of  results on the expressive power of our transformations. Most importantly, we  show that it is possible to compute limits of infinite sequences of the  primitive transformations. This leads to a number of new algorithms for  software pipelining, including: an algorithm that generates optimal code for  loops without tests, an algorithm for software pipelining of multiple nested  loops, and a general solution to the problem of software pipelining in the  presence of tests.  Using the four primitives and the limit-taking transformation, it is possible  to express the classical parallelization techniques for vector,  multiprocessor, and VLIW machines, such as doacross, the wavefront method,  loop interchange, trace scheduling, and a simple form of vectorization. Thus,  our transformational system can be viewed as a formal foundation for the area  of parallelization.", "num_citations": "71\n", "authors": ["1863"]}
{"title": "Stratified synthesis: automatically learning the x86-64 instruction set\n", "abstract": " The x86-64 ISA sits at the bottom of the software stack of most desktop and server software. Because of its importance, many software analysis and verification tools depend, either explicitly or implicitly, on correct modeling of the semantics of x86-64 instructions. However, formal semantics for the x86-64 ISA are difficult to obtain and often written manually through great effort. We describe an automatically synthesized formal semantics of the input/output behavior for a large fraction of the x86-64 Haswell ISA\u2019s many thousands of instruction variants. The key to our results is stratified synthesis, where we use a set of instructions whose semantics are known to synthesize the semantics of additional instructions whose semantics are unknown. As the set of formally described instructions increases, the synthesis vocabulary expands, making it possible to synthesize the semantics of increasingly complex instructions. Using\u00a0\u2026", "num_citations": "70\n", "authors": ["1863"]}
{"title": "Directional type checking of logic programs\n", "abstract": " We present an algorithm for automatic type checking of logic programs with respect to directional types that describe both the structure of terms and the directionality of predicates. The type checking problem is reduced to a decision problem on systems of inclusion constraints over set expressions. The solution to this decision problem provides an effective type checking procedure. We discuss some properties of the reduction algorithm, and present a proof of correctness. We present lower bound complexity results for the general type checking problem where types are given as arbitrary set expressions, as well as for the restrictive class of discriminative directional types. For discriminative directional types the type checking procedure is shown to be sound and complete.", "num_citations": "66\n", "authors": ["1863"]}
{"title": "Banshee: A scalable constraint-based analysis toolkit\n", "abstract": " We introduce Banshee, a toolkit for constructing constraint-based analyses. Banshee\u2019s novel features include a code generator for creating customized constraint resolution engines, incremental analysis based on backtracking, and fast persistence. These features make Banshee useful as a foundation for production program analyses.", "num_citations": "65\n", "authors": ["1863"]}
{"title": "Language support for dynamic, hierarchical data partitioning\n", "abstract": " Applications written for distributed-memory parallel architectures must partition their data to enable parallel execution. As memory hierarchies become deeper, it is increasingly necessary that the data partitioning also be hierarchical to match. Current language proposals perform this hierarchical partitioning statically, which excludes many important applications where the appropriate partitioning is itself data dependent and so must be computed dynamically. We describe Legion, a region-based programming system, where each region may be partitioned into subregions. Partitions are computed dynamically and are fully programmable. The division of data need not be disjoint and subregions of a region may overlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that a computation uses a region read-only) and data coherence (e.g., expressing that the computation need\u00a0\u2026", "num_citations": "64\n", "authors": ["1863"]}
{"title": "Minimum satisfying assignments for SMT\n", "abstract": " A minimum satisfying assignment of a formula is a minimum-cost partial assignment of values to the variables in the formula that guarantees the formula is true. Minimum satisfying assignments have applications in software and hardware verification, electronic design automation, and diagnostic and abductive reasoning. While the problem of computing minimum satisfying assignments has been widely studied in propositional logic, there has been no work on computing minimum satisfying assignments for richer theories. We present the first algorithm for computing minimum satisfying assignments for satisfiability modulo theories. Our algorithm can be used to compute minimum satisfying assignments in theories that admit quantifier elimination, such as linear arithmetic over reals and integers, bitvectors, and difference logic. Since these richer theories are commonly used in software verification, we believe\u00a0\u2026", "num_citations": "64\n", "authors": ["1863"]}
{"title": "A tuning framework for software-managed memory hierarchies\n", "abstract": " Achieving good performance on a modern machine with a multi-level memory hierarchy, and in particular on a machine with software-managed memories, requires precise tuning of programs to the machine's particular characteristics. A large program on a multi-level machine can easily expose tens or hundreds of inter-dependent parameters which require tuning, and manually searching the resultant large, non-linear space of program parameters is a tedious process of trial-and-error. In this paper we present a general framework for automatically tuning general applications to machines with software-managed memory hierarchies. We evaluate our framework by measuring the performance of benchmarks that are tuned for a range of machines with different memory hierarchy configurations: a cluster of Intel P4 Xeon processors, a single Cell processor, and a cluster of Sony Playstation3's.", "num_citations": "64\n", "authors": ["1863"]}
{"title": "Cool: A portable project for teaching compiler construction\n", "abstract": " The compiler course is a fixture of undergraduate computer science education. Most CS programs offer a course on compilers that includes a substantial project where students write a compiler for a small programming language. The project often serves two distinct purposes: it teaches something about language design and compiler implementation, and it gives students the experience of building a substantial software system. A compiler project is the most complex software engineering task many students complete in an undergraduate program.", "num_citations": "62\n", "authors": ["1863"]}
{"title": "Cuts from proofs: A complete and practical technique for solving linear inequalities over integers\n", "abstract": " We propose a novel, sound, and complete Simplex-based algorithm for solving linear inequalities over integers. Our algorithm, which can be viewed as a semantic generalization of the branch-and-bound technique, systematically discovers and excludes entire subspaces of the solution space containing no integer points. Our main insight is that by focusing on the defining constraints of a vertex, we can compute a proof of unsatisfiability for the intersection of the defining constraints and use this proof to systematically exclude subspaces of the feasible region with no integer points. We show experimentally that our technique significantly outperforms the top four competitors in the QF-LIA category of the SMT-COMP \u201908 when solving linear inequalities over integers.", "num_citations": "60\n", "authors": ["1863"]}
{"title": "A distributed multi-gpu system for fast graph processing\n", "abstract": " We present Lux, a distributed multi-GPU system that achieves fast graph processing by exploiting the aggregate memory bandwidth of multiple GPUs and taking advantage of locality in the memory hierarchy of multi-GPU clusters. Lux provides two execution models that optimize algorithmic efficiency and enable important GPU optimizations, respectively. Lux also uses a novel dynamic load balancing strategy that is cheap and achieves good load balance across GPUs. In addition, we present a performance model that quantitatively predicts the execution times and automatically selects the runtime configurations for Lux applications. Experiments show that Lux achieves up to 20X speedup over state-of-the-art shared memory systems and up to two orders of magnitude speedup over distributed systems.", "num_citations": "59\n", "authors": ["1863"]}