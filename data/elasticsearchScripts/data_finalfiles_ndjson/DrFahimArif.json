{"title": "Smart urban planning using Big Data analytics to contend with the interoperability in Internet of Things\n", "abstract": " The recent growth and expansion in the field of Internet of Things (IoT) is providing a great business prospective in the direction of the new era of smart urban. The insight of the smart urban is extensively preferred, as it improves the excellence of life of citizens, connecting several regulations, that is, smart transportation, smart parking, smart environment, smart healthcare, and so forth. Continuous intensification of the multifaceted urban set-up is extensively challenged by real-time processing of data and smart decision capabilities. Consequently, in this paper, we propose a smart city architecture which is based on Big Data analytics. The proposed scheme is comprised of three modules: (1) data acquisition and aggregation module collects varied and diverse data interrelated to city services, (2) data computation and processing module performs normalization, filtration, processing and data analysis, and (3\u00a0\u2026", "num_citations": "97\n", "authors": ["1251"]}
{"title": "Real-time data processing scheme using big data analytics in internet of things based smart transportation environment\n", "abstract": " In recent times, a massive amount of smart devices or objects are connected that enhances the scale of the digital world. These smart objects are referred as \u201cthings\u201d or physical devices that have the potential to sense the real-world physical objects, collect the data, and network with others. The objects are connected through the internet, which crafts the terminology of Internet of Things (IoT). IoT has been developed and become the center of consideration due to the novelty of embedded device and a rapid enhancement in its number. This increase is resulting in the creative applications of smart environments. Smart transportation is a central stake for the quality of life of citizens in smart environment. Smart transportation involves the use of devices and sensors in the control system of vehicle; for example navigation system of cars, traffic signal management system, number recognition system and speed\u00a0\u2026", "num_citations": "52\n", "authors": ["1251"]}
{"title": "Fairness improvement in long chain multihop wireless ad hoc networks\n", "abstract": " In asymmetric multihop wireless ad hoc networks the original medium access control protocol does not perform well, especially when the offered load is high. Many papers have studied this issues, and premeditated to obtain better fairness in multihop wireless ad hoc networks. These techniques provide some degree of fairness but either provide low throughput or assume MAC layer fairness and hence need improvements. In this paper we have proposed a new method based probabilistic control on round robin queue, and have obtained better results in terms of fairness. Our proposed method perform better than FIFO scheduling, RR scheduling, and PCRQ scheduling. For evaluating the performance of the proposed method, we have used Network Simulator version-2 (NS-2). Simulation results have shown that our proposed method produces higher degree of fairness.", "num_citations": "45\n", "authors": ["1251"]}
{"title": "Urban data management system: Towards Big Data analytics for Internet of Things based smart urban environment using customized Hadoop\n", "abstract": " The unbroken amplification of a versatile urban setup is challenged by huge Big Data processing. Understanding the voluminous data generated in a smart urban environment for decision making is a challenging task. Big Data analytics is performed to obtain useful insights about the massive data. The existing conventional techniques are not suitable to get a useful insight due to the huge volume of data. Big Data analytics has attracted significant attention in the context of large-scale data computation and processing. This paper presents a Hadoop-based architecture to deal with Big Data loading and processing. The proposed architecture is composed of two different modules, i.e., Big Data loading and Big Data processing. The performance and efficiency of data loading is tested to propose a customized methodology for loading Big Data to a distributed and processing platform, i.e., Hadoop. To examine data\u00a0\u2026", "num_citations": "44\n", "authors": ["1251"]}
{"title": "Energy-harvesting based on internet of things and big data analytics for smart health monitoring\n", "abstract": " Current advancements and growth in the arena of the Internet of Things (IoT) is providing great potential in the novel epoch of healthcare. The future of healthcare is expansively promising, as it advances the excellence of life and health of humans, involving several health regulations. Continual increases of multifaceted IoT devices in healthcare is beset by challenges, such as powering IoT terminal nodes used for health monitoring, data processing, smart decisions, and event management. In this paper, we propose a healthcare architecture which is based on an analysis of energy harvesting for health monitoring sensors and the realization of Big Data analytics in healthcare. The rationale of the proposed architecture is two-fold: (1) comprehensive conceptual framework for energy harvesting for health monitoring sensors; and (2) data processing and decision management for healthcare. The proposed architecture\u00a0\u2026", "num_citations": "42\n", "authors": ["1251"]}
{"title": "Framework for better reusability in component based software engineering\n", "abstract": " To develop software from existing component is done to reduce time and cost of the software. Reusable modules and classes reduce implementation time, increase the likelihood that prior testing and use has eliminated bugs and localizes code modifications when a change in implementation is required. Different traditional approaches to achieve reusability in Component Based Software Engineering (CBSE) have been discussed. This paper provides framework to develop component based software. This framework helps to achieve better reusability of the component by providing ease of component selection using domain knowledge and off-the-shelf-components availability. It also helps less skilled developer to develop successful component based software system.", "num_citations": "41\n", "authors": ["1251"]}
{"title": "Edge-based features for localization of artificial Urdu text in video images\n", "abstract": " Content-based video indexing and retrieval has become an interesting research area with the tremendous growth in the amount of digital media. In addition to the audio-visual content, text appearing in videos can serve as a powerful tool for semantic indexing and retrieval of videos. This paper proposes a method based on edge-features for horizontally aligned artificial Urdu text detection from video images. The system exploits edge based segmentation to extract textual content from videos. We first find the vertical gradients in the input video image and average the gradient magnitude in a fixed neighborhood of each pixel. The resulting image is binarized and the horizontal run length smoothing algorithm (RLSA) is applied to merge possible text regions. An edge density filter is then applied to eliminate noisy non-text regions. Finally, the candidate regions satisfying certain geometrical constraints are accepted as\u00a0\u2026", "num_citations": "38\n", "authors": ["1251"]}
{"title": "An unconstrained benchmark Urdu handwritten sentence database with automatic line segmentation\n", "abstract": " In this paper we present and announce a novel off-line sentence database of Urdu handwritten documents along with a few preprocessing and text line segmentation procedures. Despite an increased research interest in Urdu handwritten document analysis over the recent years, a standard benchmark dataset, which could be used in Urdu handwriting recognition tasks, has been missing. Based on our own developed and updated corpus named CENIP-UCCP (Center for Image Processing-Urdu Corpus Construction Project), we have developed an Urdu handwritten database. The corpus is a collection of a variety of Urdu texts that were used to generate forms. These forms were subsequently filled by native writers in their natural handwritings. Six categories of text were used to generate these forms with each category using approximately 66 forms. Up till now, the database comprises 400 digitized forms produced\u00a0\u2026", "num_citations": "30\n", "authors": ["1251"]}
{"title": "A measurement model based on usability metrics for mobile learning user interface for children\n", "abstract": " Mobile technology is becoming an integral learning tool for children. The Interface of mobile educational applications (apps) should be usable and compatible with the cognitive skills of children in order to provide an effective learning experience. Usability is a key quality attributes to measure the usefulness of application; therefore evaluating usability is a vital task. With the rapid advancement of mobile technology, usability of educational apps for children gains attention of modern researchers. This paper focuses on providing a measurement model for evaluating the interface of mobile educational apps designed for children. The paper attempts to review the existing interface design guidelines and consequently develop a measurement model. The model serves as basis for comprehensive usability evaluation consisting of guidelines, usability characteristics, goals (interface design criteria), questions, usability metrics (objective and subjective) and two evaluation instruments (task list and satisfaction questionnaire). To ensure the effectiveness and reliability of the model, it was validated by applying the proposed metrics and evaluation instruments in a usability study conducted on two android educational apps for children. Results gathered from usability testing proved that the Model is applicable for evaluation of mobile educational apps for children.", "num_citations": "28\n", "authors": ["1251"]}
{"title": "Analysis of object oriented complexity and testability using object oriented design metrics\n", "abstract": " Wide applicability of object oriented technology in software development industry led to the development of high quality software products at the cost of increased complexity. The complexity of software systems directly contributes to increased testability efforts. This paper does review of testability and complexity of the software systems at the design level. Object oriented design metrics proposed in earlier research models is modified to analyze in detail the relationship between complexity, testability and different attributes of object oriented software design by predicting class level testability. Estimated results depict that different attributes of object oriented systems may add directly to the complexity of design requiring more testing efforts. The metrics proposed in this paper is further validated on four different software projects. Quantifiable results obtained justify the predicted relationship between object oriented\u00a0\u2026", "num_citations": "27\n", "authors": ["1251"]}
{"title": "Resampling air borne sensed data using bilinear interpolation algorithm\n", "abstract": " Air borne sensed data is gained by satellite/aerial images and other remote sensing or scanning platforms. Each grid-cell, or pixel, has a certain value depending on how the image was captured and what it represents. Reprojection and resampling of raster image data should be handled with caution. Issues of geometric distortions and errors due to resampling need to be considered carefully. Interpretation of such data requires correction of distortions and resampling of recorded data. There are different methods for this purpose but they have their inbuilt problems as well. This paper presents a bilinear interpolation method for resampling of air borne sensed data. The method is developed in Matlab and results are obtained using different interpolation rates. A review of the used method is also given at the end.", "num_citations": "25\n", "authors": ["1251"]}
{"title": "Collaboration Methodology for Integrating Non-Functional Requirements in Architecture\n", "abstract": " Architecture addresses the Non-functional requirements (NFRs) that are the qualities of the system. Functional requirements (FRs) are being taken under consideration at the early stage of software process development while NFRs are not taken into account. Usually, the NFRs are being focused at the end of the project, which does not fulfill the desired qualities. Early design decision is very important to achieve a strong connection between design and requirements, quality of a system and a consistent software product. The position which we put forward, the NFRs should be focused at architectural. Both runtime NFRs (such as performance, security and fault tolerance) and some of those which are not runtime (such as maintainability) should be considered at the architectural level. In this paper, we present a collaboration methodology to integrate the NFRs and Architecture. The proposed methodology focuses on the close collaboration of architect and analyst of the system. Specially, we elaborate this collaboration in form of the involvements of architect into the requirements.", "num_citations": "21\n", "authors": ["1251"]}
{"title": "The implementation of an IoT-based flood alert system\n", "abstract": " Floods are the most damaging natural disaster in this world. On the occasion of heavy flood, it can destroy the community and killed many lives. The government would spend billions of dollars to recover the affected area. It is crucial to develop a flood control system as a mechanism to reduce the flood risk. Providing a quick feedback on the occurrence of the flood is necessary for alerting resident to take early action such as evacuate quickly to a safer and higher place. As a solution, this paper propose a system that is not only able to detect the water level but also able to measure the rise speed of water level and alerted the resident. Waterfall model is adopted as the methodology in this project. Raspberry Pi is used to collect data from the water sensor and transmit the data to GSM module for sending an alert via SMS. The analysis will be done to show how the Raspberry Pi will be integrated with the smartphone to give an alert. The system is tested in an experiments consist of two different environment in order to ensure that the system is able to provide accurate and reliable data. The project is an IoT-based which significantly in line with the Industrial Revolution 4.0, supporting the infrastructure of Cyber-Physical System.", "num_citations": "19\n", "authors": ["1251"]}
{"title": "A secured data management scheme for smart societies in industrial internet of things environment\n", "abstract": " Smart societies have an increasing demand for quality-oriented services and infrastructure in an industrial Internet of Things (IIoT) paradigm. Smart urbanization faces numerous challenges. Among them, secured energy demand-side management (DSM) is of particular concern. The IIoT renders the industrial systems to malware, cyberattacks, and other security risks. The IIoT with the amalgamation of big data analytics can provide efficient solutions to such challenges. This paper proposes a secured and trusted multilayered DSM engine for a smart social society using IIoT-based big data analytics. The major objective is to provide a generic secured solution for smart societies in IIoT environment. The proposed engine uses a centralized approach to achieve optimum DSM over a home area network. To enhance the security of this engine, a payload-based authentication scheme is utilized that relies on a lightweight\u00a0\u2026", "num_citations": "19\n", "authors": ["1251"]}
{"title": "Modeling of real-time embedded systems using SysML and its verification using UPPAAL and DiVinE\n", "abstract": " SysML is a graphical modeling language that is more suitable for modeling of real-time and embedded systems. The application modeled in SysML must be verified in earlier phases of software development life cycle to increase the reliability and reduce the modeling and verification cost. The available tools for verification are sequential and parallel types. The sequential verification tools either fail or unable to show the significant performance to verify a large scale embedded real-time system. The limitations of sequential verification tools have increased the importance of parallel verification tools. While, DiVinE is parallel verification tool that doesn't support the timed verification of model. By keeping in view the limitations of both types of model checkers and their compatibility, we have proposed a methodology to use both types of model checkers for verification of real-time system that are graphically modeled using\u00a0\u2026", "num_citations": "19\n", "authors": ["1251"]}
{"title": "Systematic literature review of agile scalability for large scale projects\n", "abstract": " In new methods,\u201cagile\u201d has come out as the top approach in software industry for the development of the soft wares. With different shapes agile is applied for handling the issues such as low cost, tight time to market schedule continuously changing requirements, Communication & Coordination, team size and distributed environment. Agile has proved to be successful in the small and medium size project, however, it have several limitations when applied on large size projects. The purpose of this study is to know agile techniques in detail, finding and highlighting its restrictions for large size projects with the help of systematic literature review. The systematic literature review is going to find answers for the Research questions: 1) How to make agile approaches scalable and adoptable for large projects? 2) What are the existing methods, approaches, frameworks and practices support agile process in large scale projects? 3) What are limitations of existing agile approaches, methods, frameworks and practices with reference to large scale projects? This study will identify the current research problems of the agile scalability for large size projects by giving a detail literature review of the identified problems, existed work for providing solution to these problems and will find out limitations of the existing work for covering the identified problems in the agile scalability. All the results gathered will be summarized statistically based on these findings. Remedial work will be planned in future for handling the identified limitations of agile approaches for large scale projects.", "num_citations": "18\n", "authors": ["1251"]}
{"title": "Potential lung nodules identification for characterization by variable multistep threshold and shape indices from CT images\n", "abstract": " Computed tomography (CT) is an important imaging modality. Physicians, surgeons, and oncologists prefer CT scan for diagnosis of lung cancer. However, some nodules are missed in CT scan. Computer aided diagnosis methods are useful for radiologists for detection of these nodules and early diagnosis of lung cancer. Early detection of malignant nodule is helpful for treatment. Computer aided diagnosis of lung cancer involves lung segmentation, potential nodules identification, features extraction from the potential nodules, and classification of the nodules. In this paper, we are presenting an automatic method for detection and segmentation of lung nodules from CT scan for subsequent features extraction and classification. Contribution of the work is the detection and segmentation of small sized nodules, low and high contrast nodules, nodules attached with vasculature, nodules attached to pleura membrane, and nodules in close vicinity of the diaphragm and lung wall in one-go. The particular techniques of the method are multistep threshold for the nodule detection and shape index threshold for false positive reduction. We used 60 CT scans of \u201cLung Image Database Consortium-Image Database Resource Initiative\u201d taken by GE medical systems LightSpeed16 scanner as dataset and correctly detected 92% nodules. The results are reproducible.", "num_citations": "17\n", "authors": ["1251"]}
{"title": "Smart urban planning using big data analytics based internet of things\n", "abstract": " The extensive growth of the Internet of Things (IoT) is providing direction towards the smart urban. The smart urban is favored because it improves the standard of living of the citizens and provides excellence in the community services. The services may include but not limited to health, parking, transport, water, environment, power, and so forth. The diverse and heterogeneous environment of IoT and smart urban is challenged by real-time data processing and decision-making. In this research article, we propose IoT based smart urban architecture using Big Data analytics. The proposed architecture is divided into three different tiers:(1) data acquisition and aggregation,(2) data computation and processing, and (3) decision making and application. The proposed architecture is implemented and validated on Hadoop Ecosystem using reliable and authentic datasets. The research shows that the proposed system\u00a0\u2026", "num_citations": "15\n", "authors": ["1251"]}
{"title": "Spectrum decision framework to support cognitive radio based IoT in 5G\n", "abstract": " RF Spectrum Decision in Cognitive Radio enables unlicensed users of wireless communication systems to occupy the vacant spectrum slotsasasolution to scarce spectrum. Internet of Things (IoT) is a wide-reaching network of unified entities. IoT capable things will be interconnected through wireless communication technologies offering cost-effectiveness and accessibility to remote users making quality life style. IoT implementation suffers from challenges of vulnerabilities to dynamic environmental conditions, ease of access, bandwidth allocation and utilization, and cost to purchase RF spectrum. As RF spectrum is a precious commodity and there is a dearth of RF spectrum, hence IoT connections are drifting towards Cognitive Radio Networks (CRNs). Permeating things with cognitive abilities will be able to make RF spectrum decisions to achieve interference-free and wireless connectivity as per their QoS requirements. The wireless systems are rapidly advancing. The leap from packet switching along with circuit switching with 144 kbps data rate (2G and 2.5 G) to Long Term Evolution Advanced (LTE-A), ie, 4G occurred in one decade time frame. As the current wireless connectivity is aimed at higher capacity, higher data rate, low end-to-end latency, massive device connectivity, reduced cost and consistent Quality of Experience (QoE) provision, therefore, 4G is being replaced with 5G. Presently the Radio Frequency (RF) spectrum band is fully sold out and allocated to various wireless operators and applications. On the other hand, new wireless applications are emerging and there is a serious dearth of frequency spectrum to be allocated to\u00a0\u2026", "num_citations": "14\n", "authors": ["1251"]}
{"title": "Parallel verification of UML using DiVinE tool\n", "abstract": " Unified Modeling Language is being used for modeling of business processes, real-time systems, embedded systems and hybrid systems. In software engineering, there are different techniques for verification of such systems. Model checking is one of the techniques for the automatic verification of software models. Earlier, these model were verified by sequential model checking tools. For very large complex systems, parallel verification can save verification time. DiVinE is a model checking tool for both CPUs and GPUs clusters to perform parallel linear time logic (LTL) verification of systems. In this paper, we propose a parallel verification methodology for UML models using DiVinE tool. We explain this methodology by the case study example of fuel filling machine.", "num_citations": "14\n", "authors": ["1251"]}
{"title": "ARCA-IoT: An attack-resilient cloud-assisted IoT system\n", "abstract": " Putting trust in the world of the Internet of Things, where served and serving entities are often unknown, is very hard especially when personal and business information is often being exchanged for providing and consuming services. Moreover, the issues of interoperability and scalability of billions of heterogeneous things in the IoT systems require more attention. A user-centric model of complex interconnected things must be designed in a way that not only makes things trustworthy for common people but it also provides the solution for interoperability and scalability. ARCA-IoT is such a system which not only identifies the attributes (including quality of service) essential for trust but also presents a user-centric model that is robust enough to tackle the attacks made by dishonest entities to manipulate the trustworthiness. For scalability and interoperability, a cloud-assisted environment is introduced in the ARCA-IoT\u00a0\u2026", "num_citations": "12\n", "authors": ["1251"]}
{"title": "Performance improvement in wireless sensor and actor networks\n", "abstract": " Wireless sensor and actor network is a heterogeneous network in which actor nodes enjoy higher capabilities of sensing, transmitting, and processing. The collaboration of actor nodes with the sensor nodes has significant advantages compare to traditional sensing. Actor nodes take accurate decisions and appropriate actions based on the collected data by sensor nodes, and also reposition themselves to nearby event region. In Wireless Sensor and Actor Networks (WSAN), sensor nodes are larger in quantity with lesser capabilities and actor nodes are very few but have higher capabilities. Actor nodes are responsible for taking a localized decision which requires strong cooperation among neighboring actor nodes. Therefore, appropriate placement of actor nodes in WSAN is very important and it needs proper attention to cover larger region, reduce communication delay, and get better load balancing among actor nodes. However, in some applications this may not be possible as sensor networks are deployed on run time. Moreover, accurate deployment is difficult at the time of network establishment. After event detection sensor nodes inform the nearest actor node through multihop communication. To get better performances like low energy consumption by sensor nodes and better network life time, actor nodes must be repositioned near to the event region. In this paper we have introduced a novel mechanism for getting better network lifetime, low energy consumption, minimum delay, and high throughput through proper repositioning of actor nodes. In this paper an actor can find suitable coordinates for repositioning itself or some other actor\u00a0\u2026", "num_citations": "12\n", "authors": ["1251"]}
{"title": "Evaluation of quality assurance factors in agile methodologies\n", "abstract": " Abstract__ The agile software development claims to improve the quality of the software products. This claim has increases its usage in the new industry of the software and information technology. But, due to its immature nature, very few frameworks for the evaluation of quality exist. The proposed research has introduced a tool for the evaluation of quality in agile software development. Various factors addressing the quality have been evaluated among different phases in various practiced methodologies of agile software development. The proposed tool has provided a new platform for application of various types of testing in agile methodologies. It has opened a new horizon of research in software industry", "num_citations": "12\n", "authors": ["1251"]}
{"title": "Quantifying non-functional requirements in service oriented development\n", "abstract": " This research is aimed at improving quality in service oriented applications by improving requirement engineering of quality requirements. Idea is to propose quantification mechanism that covers service development from consumer perspective and able to move back for better quality requirement management in service oriented development. Quantification mechanism is a two way affective method, first is to align quality in service development (service identification, service design, service implementation, service usage) and secondly its link with SLA enables both producer and consumer to make a check on quality. In this way, quality requirement are better developed, can be regularly checked and enhanced if required.", "num_citations": "10\n", "authors": ["1251"]}
{"title": "Modeling of embedded system using SysML and its parallel verification using DiVinE tool\n", "abstract": " SysML is a modeling language that can be used for the modeling of embedded systems. It is rich enough to model critical and complex embedded systems. The available modeling tools have made the modeling of such large and complex systems much easier. They provide sufficient support for the specification of functional requirements in the elicitation phase as well as in the design phase by graphical modeling. These systems must be properly validated and verified before their manufacturing and deployment in order to increase their reliability and reduce their maintenance cost. In this paper, we have proposed a methodology for the modeling and verification of embedded systems in parallel and distributed environments. We demonstrate the suitability of the framework by applying it on the case study of embedded security system. The parallel model checking tool DiVinE has been used because the\u00a0\u2026", "num_citations": "9\n", "authors": ["1251"]}
{"title": "Flood detection/monitoring using adjustable histogram equalization technique\n", "abstract": " Flood monitoring technique using adjustable histogram equalization is proposed. The technique overcomes the limitations (overenhancement, artifacts, and unnatural look) of existing technique by adjusting the contrast of images. The proposed technique takes pre- and postimages and applies different processing steps for generating flood map without user interaction. The resultant flood maps can be used for flood monitoring and detection. Simulation results show that the proposed technique provides better output quality compared to the state of the art existing technique.", "num_citations": "9\n", "authors": ["1251"]}
{"title": "Internet of things\u2013based smart city environments using big data analytics: A survey\n", "abstract": " The intense growth and acceptance of the Internet of Things (IoT) is reflected in the trend of smart cities. Smart cities are being implemented to improve standards of living and provide higher-quality services to residents. These services may include (but are not limited to) parking, water, health, transportation, environment, and power. The varied implementations of smart cities and the IoT are challenged by the processing of gigantic data and real-time decision management. In this chapter, we explore the use of big data analytics in IoT-based smart city development and design. This chapter provides a conceptual framework for the use of big data analytics in IoT-based smart city environments.", "num_citations": "8\n", "authors": ["1251"]}
{"title": "Mobile technology in children education: Analyzing parents' attitude towards mobile technology for children\n", "abstract": " In this new era of technology, children are more exposed to the advance technology even at an early age with smartphones, tablets and e-readers being some dominant choices as observed by many educators and parents. The experiences with the latest technology can surely pave the way for extraordinary learning opportunities. Every time a new mode of technology is invented, say it from television to computers to mobile devices, it always presents some new potential as an educational tool for the young children. Nowadays there are many platforms for the educational media content than there were ever before. With children spending so many hours a day with these technologies, there are certain concerns which parents have regarding technology use for children. In this paper we have conducted a survey with the parents of elementary school age children (aged 6 to 10 years) in Pakistan. This paper is an\u00a0\u2026", "num_citations": "8\n", "authors": ["1251"]}
{"title": "Framework for evaluating the usability of mobile educational applications for children\n", "abstract": " Mobile technology is becoming an integral learning tool for children. The Interface of mobile educational applications (apps) should be usable and compatible with the cognitive skills of children in order to provide an effective learning experience. Usability is a key quality attributes to measure the usefulness of application; therefore evaluating usability is a vital task. With the rapid advancement of mobile technology, usability of educational apps for children gains attention of modern researchers. This paper focuses on providing a framework for evaluating the interface of mobile educational apps designed for children. The paper attempts to review the existing interface design guidelines and consequently develop a framework. The framework serves as basis for comprehensive usability evaluation consisting of guidelines, usability characteristics, goals (interface design criteria), questions, usability metrics (objective and subjective) and two evaluation instruments (task list and satisfaction questionnaire). To ensure the effectiveness and reliability of the framework, it was validated by applying the proposed metrics and evaluation instruments in a usability study conducted on two android educational apps for children. Results gathered from usability testing proved that the framework is applicable for evaluation of mobile educational apps for children.", "num_citations": "8\n", "authors": ["1251"]}
{"title": "CATSWoTS: Context aware trustworthy social Web of things system\n", "abstract": " The inevitable revolution of the Internet of Things (IoT) and its benefits can be witnessed everywhere. Two major issues related to IoT are the interoperability and the identification of trustworthy things. The proposed Context-Aware Trustworthy Social Web of Things System (CATSWoTS) addresses the interoperability issue by incorporating web technologies including Service Oriented Architecture where each thing plays the role of a service provider as well as a role of service consumer. The aspect of social web helps in getting recommendations from social relations. It was identified that the context dependency of trust along with Quality of Service (QoS) criteria, for identifying and recommending trustworthy Web of Things (WoT), require more attention. For this purpose, the parameters of context awareness and the constraints of QoS are considered. The research focuses on the idea of a user-centric system where the profiles of each thing (level of trustworthiness) are being maintained at a centralized level and at a distributed level as well. The CATSWoTS evaluates service providers based on the mentioned parameters and the constraints and then identifies a suitable service provider. For this, a rule-based collaborative filtering approach is used. The efficacy of CATSWoTS is evaluated with a specifically designed environment using a real QoS data set. The results showed that the proposed novel technique fills the gap present in the state of the art. It performed well by dynamically identifying and recommending trustworthy services as per the requirements of a service seeker. View Full-Text", "num_citations": "7\n", "authors": ["1251"]}
{"title": "Translating activity diagram from duration calculus for modeling of real-time systems and its formal verification using UPPAAL and DiVinE\n", "abstract": " The RTS (Real-Time Systems) are widely used in industry, home appliances, life saving systems, aircrafts, and automatic weapons. These systems need more accuracy, safety, and reliability. An accurate graphical modeling and verification of such systems is really challenging. The formal methods made it possible to model such systems with more accuracy. In this paper, we envision a strategy to overcome the inadequacy of SysML (System Modeling Language) for modeling and verification of RTS, and illustrate the framework by applying it on a case study of fuel filling machine. We have defined DC (Duration Calculus) implementaion based formal semantics to specify the functionality of RTS. The activity diagram in then generated from these semantics. Finally, the graphical model is verified using UPPAAL and DiVinE model checkers for validation of timed and untimed properties with accelerated verification speed\u00a0\u2026", "num_citations": "7\n", "authors": ["1251"]}
{"title": "Requirements based testing of software\n", "abstract": " Testing is the method that assures that the developed system conforms to the specification and no error arises during system usage. No system is deployed without testing as testing produces confidence in the system. This paper presents the Requirements Based Testing (RBT) technique for testing the Usability and Functional Requirements of a software. The main emphasis of this paper is to test the usability of a software, as being a non-functional requirement that is perceived subjectively it is difficult to measure it. A generic template has been proposed for testing usability. For the purpose of evaluation, this method is applied to test a software named \u201cProgramming Teaching Aid\u201d, a system that helps students in understanding the programming language C++.", "num_citations": "7\n", "authors": ["1251"]}
{"title": "Trust management for SOA based social WoT system\n", "abstract": " In recent past, the Internet of Things (IoT) has revolutionized the world. Web of Things (WoT) is a paradigm which comes under the umbrella of IoT. This paradigm is the result of communication solutions through web technologies resultantly ensuring interoperability among heterogeneous things. Such connectivity of things has many advantages, however, the quality of services provided by these things needs to be reputed and trustworthy. In this paper, a trust evaluation method using a genetic algorithm for a SOA-based Social WoT system has been proposed. The trustworthiness of each service is calculated by considering indirect and direct trust. Direct trust is based on the experience of service consumer whereas indirect trust considers the opinions of friends and community of the service seeker. By considering direct and indirect trust, the system minimizes the biasedness. The information related to\u00a0\u2026", "num_citations": "6\n", "authors": ["1251"]}
{"title": "Computational conversion via translation rules for transforming C++ code into UPPAAL\u2019s automata\n", "abstract": " Formal methods help in quantifying the functional and nonfunctional requirements that are later used in the verification process for safety assurance in real-time systems. System formalism is a crucial step in terms of exploring system's behavior and listing the non-functional requirements. In the context of real-time systems, the non-functional requirements refer to the verification properties of the system. Formalism in software development life cycle refines every process, starting from the formalization of system's requirements, analysis of system's behavior and exploring its properties, implementation of the problem's solution under consideration, and verification of safety critical properties. Rule-based expert system helps in inferring unknown on the basis of some known input, that is, knowledge and rule set. Knowledge is comprised of something known by an individual called an expert of that domain. It requires an\u00a0\u2026", "num_citations": "6\n", "authors": ["1251"]}
{"title": "Formal verification of time constrains SysML internal block diagram using prism\n", "abstract": " System Modeling Language (SysML) is a standardized profile of Object Management Group (OMG) and it is used for the purpose of graphical modeling a system engineering application. The embedded system is graphically modeled using an internal block diagram of SysML. For formal verification of graphical model, a methodology is proposed which maps the SysML's internal block diagram to input language of PRISM model checker using CTMC (Continuous Time Markov Chain) model for developing more reliable real-time application. The functionality of the system is graphically modeled using an internal block diagram of SysML that is further translated to input language of PRISM. The user requirements are specified using CSL (Continuous Stochastic Logic) which are further verified against the functionality of the system. The timed and untimed properties are presented and verified against the CTMC model\u00a0\u2026", "num_citations": "6\n", "authors": ["1251"]}
{"title": "Formal verification of internal block diagram of SysML for modeling real-time system\n", "abstract": " SysML is a graphical modeling language that is mostly used for the graphical representation of real-time systems, complex systems, safely critical systems, and embedded systems. In this paper, we present a methodology based on model checking tool for the correction and verification of SysML internal block diagram with discrete time constraint. We describe the mapping of SysML internal block diagram to PRISM input language and use Probabilistic Computational Tree Logic (PCTL) for the verification of properties. The methodology provides more reliable and quick results for the development of real time systems as PRISM supports parallel composition of components. Finally, we present the effectiveness of our approach with the help of a case study of real-time system. The discrete time factor is included in the case study to evaluate the performance characteristics of system functionality.", "num_citations": "6\n", "authors": ["1251"]}
{"title": "Brief Communication: Contrast-stretching-and histogram-smoothness-based synthetic aperture radar image enhancement for flood map generation\n", "abstract": " Synthetic-aperture-radar-image-based flood map generation is usually a challenging task (due to degraded contrast). A three-step approach (based on adaptive histogram clipping, histogram remapping and smoothing) is proposed for generation of a more visualized flood map image. The pre- and post-flood images are adaptively histogram equalized. The hidden details in difference image are enhanced using contrast-based enhancement and histogram smoothing. A fast-ready flood map is then generated using equalized pre-, post- and difference images. Results (evaluated using different data sets) show significance of the proposed technique.", "num_citations": "6\n", "authors": ["1251"]}
{"title": "Formal verification of sequence diagram using DiVinE\n", "abstract": " System modeling language is used to model the system engineering applications. This graphical modeling language is a semi-formal language. To develop a reliable application, the graphical models for large scale critical and complex applications must be validated and verified against user requirements in earlier phase of system development cycles. The sequence diagram is one of the popular diagram of SysML. The fragments of sequence diagram increase its functionality but the complexity as well. In this paper, a methodology is proposed to verify the sequence diagram including its fragments. The verification is performed using DiVinE parallel model checking tool that not only accelerates verification speed but also save modeling and verification cost. The rules have been specified to translate these individual fragments to DiVinE's supported language DVE. To develop a reliable product, our results suggest\u00a0\u2026", "num_citations": "6\n", "authors": ["1251"]}
{"title": "Fuzzy-based segmentation for variable font-sized text extraction from images/videos\n", "abstract": " Textual information embedded in multimedia can provide a vital tool for indexing and retrieval. A lot of work is done in the field of text localization and detection because of its very fundamental importance. One of the biggest challenges of text detection is to deal with variation in font sizes and image resolution. This problem gets elevated due to the undersegmentation or oversegmentation of the regions in an image. The paper addresses this problem by proposing a solution using novel fuzzy-based method. This paper advocates postprocessing segmentation method that can solve the problem of variation in text sizes and image resolution. The methodology is tested on ICDAR 2011 Robust Reading Challenge dataset which amply proves the strength of the recommended method.", "num_citations": "6\n", "authors": ["1251"]}
{"title": "The proposed blended-MDA for software modeling in architecture phase\n", "abstract": " The Model Driven Architecture is the modeling approach that ensures the re-usability, portability and inter-operability of the software. For the structuring of the system it provides basic guidelines to be modeled. Separating the architecture from the design is one of the main aim of MDA. Functional requirements of the system are being analyzed by the detailed design of the system (for instance, use cases) and for non-functional requirements, the infrastructure is provided by the architecture of the software ensuring performance, reliability, scalability etc. In this research paper, some case studies are extensively analyzed in the domain of web applications. Functional and non-functional aspects are analyzed in the comparative analysis table. Finally, the Blended-MDA Framework is proposed for the case study entitled as Online Event Manager.", "num_citations": "5\n", "authors": ["1251"]}
{"title": "A hybrid approach for artificial urdu text detection in video images\n", "abstract": " The rapid growth of multimedia data containing rich textual information demands for efficient indexing and retrieval techniques. In this paper, we propose a hybrid approach based on a combination of supervised and unsupervised techniques for the detection of horizontally aligned artificial Urdu text appearing in video images. First, we use an unsupervised approach to detect potential text regions which are later validated by a supervised method. In the first step, edge features followed by morphological operations are used to identify the candidate text regions. These regions are further refined by using edge density and geometrical filters. In the next step, these detected text regions are validated by an Artificial Neural Network which is trained on example text and non-text regions. We used a dataset of 500 images for the evaluation of the proposed system and achieved promising results.", "num_citations": "5\n", "authors": ["1251"]}
{"title": "Auditing Road Maintenance Work using Unmanned Aerial Vehicle\n", "abstract": " Road maintenance works performed by contractors require validation upon completion. Conventional methods are typically chosen to validate the works. Geographic Information System (GIS) is applied in this study to produce accurate data such that waste from road maintenance costs can be minimised. In this study, Unmanned Aerial Vehicle (UAV) was used to observe the images of two roads, namely Jalan Gelanggang and Jalan Temuan, both of which are under maintenance at the Universiti Kebangsaan Malaysia. The objectives of this study are; to determine the ability of UAV technology in supervision and verification of maintenance works, and the effectiveness of spatial works in road maintenance. This study then proceeds with analysing data by using ArcGIS and AutoCAD software to determine the width, perimeter, and area of the road that has been maintained. Comparisons were made between these two\u00a0\u2026", "num_citations": "4\n", "authors": ["1251"]}
{"title": "Fuzzification supported spectrum decision framework for cognitive radio networks\n", "abstract": " Efficient decision support framework and swift occupation of the available licensed spectrum slot by secondary users (SUs) are the challenging issues in cognitive radio networks. An SU has to sense multiple target frequency spectrum slots in the shortest possible time before deciding to select and occupy the best idle slot for its transmission. A spectrum decision framework for the SUs in cognitive radio networks based on fuzzy logic is proposed. The activity time, possession, and quality of service of the spectrum slot lead to the decision of occupying the available spectrum slot(s). The proposed decision framework ensures that the SU's transmission in the particular spectrum slot does not cause any interference with the licensed user of the slot. The proposed method yields enhanced throughput as compared with the existing channel assignment schemes.", "num_citations": "4\n", "authors": ["1251"]}
{"title": "Technology in primary schools: teachers\u2019 perspective towards the use of mobile technology in children education\n", "abstract": " Today technology is progressively being recognized as a significant learning tool for helping young children in developing their cognitive, social and learning skills. Now a day\u2019s even young children are exposed to the latest technology such smartphones, tablets and e-readers as observed by many teachers and parents. The new mode of technology is considered to present some potential as an educational tool. Many new platforms are available for the educational media content. Undoubtedly technology is an important element in the lives of most children now days. Although many schools have also incorporated the use of technology as a learning tool in their curriculum still some researchers and teachers have lots of concerns regarding the use of technology in schools and specifically the use of mobile technology by young children. In this paper we have conducted a survey with the teachers of 12\u00a0\u2026", "num_citations": "4\n", "authors": ["1251"]}
{"title": "\u201cSoftware requirement engineering\u201d, a new leave towards the silver bullet\n", "abstract": " Effective requirements engineering is the key to follow the daily momentum gaining technology and the software to support that technology. Software requirements engineering is the basis of any developing software. In order to meet with quality needs and user expectations requirements play the vital part. Non-functional Requirements are part of Requirement Engineering. With increasingly complex technology nonfunctional requirements are becoming more and more essential to overcome those complexities. Quality is measured from various dimensions which include development of projects within estimated time, cost and resource constraints. Quality cannot be achieved consistently due to release after release, so there is no silver bullet concept but if we evaluate the software deficiencies and study the approach and experiences of on-time, at-budget and less defective delivery of software development project\u00a0\u2026", "num_citations": "4\n", "authors": ["1251"]}
{"title": "Super-Resolution Using Edge Modification through Stationary Wavelet Transform\n", "abstract": " In this paper, a super-resolution technique is proposed that uses a combination of bicubic interpolation and wavelet transform. Bicubic interpolation produces a high resolution image but is prone to blurring artifact. So the blurring artifact is reduced in the wavelet domain. The input low-resolution is up-sampled using bicubic interpolation. The edges of the resultant high-resolution image are enhanced using stationary wavelet transform (SWT). SWT is applied to the image to produce sub-bands of the image and then these sub-bands are modified by multiplying with a boost value. Then these sub-bands are combined using inverse stationary wavelet transform (ISWT) to produce the final high-resolution image. The quantitative and qualitative analysis illustrate that the proposed technique is provides superior results as compared to other existing techniques.", "num_citations": "4\n", "authors": ["1251"]}
{"title": "An analysis of a comprehensive planning framework for customizing sqa\n", "abstract": " Software quality assurance (SQA) is an important part of software development that ensures the software quality. The effectiveness of the process is highly dependent on the technique (s) and tools (s) that are employed for the execution of the process. However, the choice of technique or tool to use is a hard and complicated decision that involves many factors. Research has been done to formalize the method of defining and characterizing these factors. In this paper we have analyzed the framework proposed by Frank Elberzhanger and Christian Denger. The shortcomings and limitations are highlighted and possible improvements are given. We further propose a new model/framework on the basis of above discussion.", "num_citations": "4\n", "authors": ["1251"]}
{"title": "Quality assurance of energy aware routing algorithm for wireless sensor networks\n", "abstract": " Wireless Sensor Networks are evolving rapidly and are becoming part of our every day life. Vast usage of these networks has resulted in development of many routing protocols for WSN. Sensor nodes have limited battery power, for which many protocols have been designed to use the available energy of the nodes in efficient manner. An energy aware routing algorithm has been developed . It works on location based routing strategy. The algorithm works on the values of average energy and minimum distance for efficient usage of energy and link reversing to cater for the problem of routing holes. In this research paper testing of routing algorithm has been performed. Black Box testing has been applied and found that the algorithm performs efficiently both in case of data delivery and energy consumption.", "num_citations": "4\n", "authors": ["1251"]}
{"title": "Projection method for geometric modeling of high resolution satellite images applying different approximations\n", "abstract": " Precise remote sensing and high resolution satellite images have made it necessary to revise the geometric correction techniques used for ortho-rectification. There have been improvements in algorithms from simple 2D polynomial models to rigorous mathematical models derived from digital photogrammetry. In such scenario, conventional methods of photogrametric modeling of remotely sensed images would be insufficient for mapping purposes and might need to be substituted with a more rigorous approach to get a true orthophoto. To correct geometric distortions in these, the process of geometric modeling becomes important.               Pixel projection method has been devised and used for geometric correction. Algorithm has been developed in C++ and used for FORMOSAT-2 high resolution satellite images. It geo-references a satellite image while geolocating vertices of the image with its geo\u00a0\u2026", "num_citations": "4\n", "authors": ["1251"]}
{"title": "Generation of digital elevation model through aerial technique\n", "abstract": " Unmanned Aerial Vehicle (UAV) system, nowadays, is highly utilized to solve problems in various applications across different fields due to its low-cost, safety, and its low flying altitude. With photogrammetric techniques and latest available aerial technology, it is possible to utilize UAV for generation of Digital Elevation Model (DEM) and subsequently, determination of elevation on various geomorphology. Previously, this is only possible through deployment of manned aircraft and metric camera which requires monumental cost. This paper reviews the applications and developments of DEM generation. Three main components are presented in this review: (a) a summary of conventional surveying methods to determine elevation, subsequently generate a DEM, (b) a summary of remote sensing methods to generate DEM; namely satellites, and airborne platforms, and (c) findings and future possibilities for further\u00a0\u2026", "num_citations": "3\n", "authors": ["1251"]}
{"title": "Reputation management system for fostering trust in collaborative and cohesive disaster management\n", "abstract": " The best management of a disaster requires knowledge, skills and other resources not only for relief and rehabilitation but also for recovery and mitigation of its effects. These multifaceted goals cannot be achieved by a single organization and require collaborative efforts in an agile manner. Blind trust cannot be applied while selecting collaborators/team members/partners therefore good reputation of a collaborator is mandatory. Currently, various Information and Communication Technology based artifacts, for collaborative disaster management, have been developed; however, they do not employ trust and reputation as their key factor. In this paper, a framework of reputation based trust management system is proposed for the support of disaster management. The key features of framework are Meta model, Reputation Indicator Matrix and Computational algorithm, deployed using Service Oriented Architecture. To evaluate the efficacy of the artifact, a prototype is implemented. Furthermore, an industrial survey is carried out to get the feedback on the proposed framework. The results support that the proposed reputation management system provides significant support in collaborative disaster management by assisting in agile and smart decision making in all phases of disaster management cycle.", "num_citations": "3\n", "authors": ["1251"]}
{"title": "From verification to implementation: UPPAAL to C++\n", "abstract": " Validation and Verification of safety critical systems is crucial and if done incorrectly can result in fatal loss. The research contribution is focused on providing the transformation mechanism from software verification to source code phase of software development life cycle. Modeling of the critical systems initializes with the formalism of requirements followed by early model verification. The verified model can be automated to get the high level language code via code generator. Basic steps of transformation starts with UPPAAL timed automaton as an input, then getting the XML structure of the automaton. On the basis of XML structure parse tree is generated to visualize the data structure to be used for the C++ source code generation. Finally the verification, kernel and elapsed time used by the safety, liveness, reachability, deadlock freeness properties and fairness property is presented. In real time systems, safety and deadlock freeness properties are among the most crucial verification properties because if the system is not safe then it leads to insecurities related to life, money, reputation and time. If the system is in deadlock state then the system is simply of no use. Thus verification of safety and deadlock freeness properties is mandatory as per the statistical report provided in the research.", "num_citations": "3\n", "authors": ["1251"]}
{"title": "Performance evaluation of stack-protocols, encapsulation methods and video codecs for live video streaming\n", "abstract": " There are diverse factors involved in the transmission of live video over internet. These factors have direct impact on the quality of the transmitted video. The underlying protocol, encapsulation method and the choice of video codec are the key factors involved in the transmission of video over streaming network. Up to our knowledge various studies have been performed to analyze the impact of these factors individually on the quality of video transmission however their joint impact on video transmission is not yet done. In this paper we characterize the performance of live video streaming setup, while keeping in view the effect of stack-protocol, encapsulation method and the underlying video codec on delay as the key investigating parameter. The results obtained while considering diverse video streaming setups are provided in the results section of this paper. More specifically, it was observed that stack (http, mjpeg\u00a0\u2026", "num_citations": "3\n", "authors": ["1251"]}
{"title": "Geometric correction of high resolution satellite imagery and its residual analysis\n", "abstract": " High resolution satellite images are prone to geometric distortions. To correct these, the process of geometric correction becomes vital. Only knowledge of satellite altitude, attitude, position and the information of the digital elevation model (DEM) will not be adequate for the geometric correction requirements. Therefore the authors designed an algorithm for removal of geometric distortions in satellite imagery. In that a new method of geo-referencing called pixel projection method was applied along with selection of precise ground control points (GCPs). In pixel projection method vertices of remotely sensed image is geo-located based on ancillary data. For precision of GCP least square method was used to cater for instrument bias. GCPs were selected from Google Earth's software. Though with that approach precise geo-referencing of satellite imagery was achieved and a level-1 image was successfully converted to\u00a0\u2026", "num_citations": "3\n", "authors": ["1251"]}
{"title": "Most favorable automatic georeferencing based on GCPs selection using least square method\n", "abstract": " All remotely sensed images as well as high resolution satellite images are prone to geometric distortions. To correct these, the process of geometric correction becomes vital while dealing with remotely sensed images. If one has the knowledge of satellite altitude, attitude, position and the information of the digital elevation model (DEM) will not be adequate for the geometric correction requirements.Therefore, before georeferencing remotely sensed images, it will be of utmost importance to estimate the magnitude of geometric errors present in them. These errors are caused from variations of the altitude, attitude and misalignment of the image due to camera or scanners motion. The proposed method performs correlation of the two coordinate systems. To increase the geometric accuracy of the image, a set of ground control points (GCPs) with maximum accuracy will be selected to determine the better knowledge of position, attitude and pixel alignment. Then transformation will be carried out from any pixel of the imager to the earth coordinate system. The nonlinear transformation can be linearalized by applying some polynomial function.", "num_citations": "3\n", "authors": ["1251"]}
{"title": "A new approach for anti-aliasing raster data in air borne imagery\n", "abstract": " Air borne sensed data is in the form of raster data. Aliasing is always present in a sampled image causing artifact error. To reduce possible aliasing effects, it is a good idea to blur an image slightly before applying a resampling method on it. This paper presents a technique for anti-aliasing remotely sensed images. The technique uses Gaussian low pass filter (GLPF) for generation of slight blur. Then resampling of raster data is performed with the help of bilinear interpolation. Algorithm is developed in MATLAB using some inbuilt functions.", "num_citations": "3\n", "authors": ["1251"]}
{"title": "An improved framework for modelling data warehouse systems using UML profile\n", "abstract": " Data Warehouse (DW) applications provide past detail for judgment process for the companies. It is acknowledged that these systems depend on Multidimensional (MD) modelling different from traditional database modelling. MD modelling keeps data in the form of facts and dimensions. Some proposals have been presented to achieve the modelling of these systems, but none of them covers the MD modelling completely. There is no any approach which considers all the major components of MD systems. Some proposals provide their proprietary visual notations, which force the architects to gain knowledge of new precise model. This paper describes a framework which is in the form of an extension to Unified Modelling Language (UML). UML is worldwide known to design a variety of perspectives of software systems. Therefore, any method using the UML reduces the endeavour of designers in understanding the novel notations. Another exceptional characteristic of the UML is that it can be extended to bring in novel elements for different domains. In addition, the proposed UML profile focuses on the accurate representations of the properties of the MD systems based on domain specific information. The proposed framework is validated using a specific case study. Moreover, an evaluation and comparative analysis of the proposed framework is also provided to show the efficiency of the proposed work.", "num_citations": "2\n", "authors": ["1251"]}
{"title": "Novel Approach Using Deep Learning for Intrusion Detection and Classification of the Network Traffic\n", "abstract": " A variety of challenges are being faced nowadays of network intrusion which is continually increasing. These are due to vulnerabilities in software, hardware, and network protocols. Therefore, stronger IDS is required; ML and DM have further strengthened the IDS technology. At the same time threat has also become more sophisticated. Now overfitting and structured optimization techniques are used in IDS. In this paper, we proposed a deep neural network-based IDS. The DL based system monitors the traffic coming from authentic and non-authentic sources. It classifies and segregates malicious traffic with accuracy up to 99.78used for experimentation and comparative analysis with previous techniques shows encouraging results.", "num_citations": "2\n", "authors": ["1251"]}
{"title": "Fusion based spectrum decision framework for cognitive radio users\n", "abstract": " Cognitive radio technology is the outcome of dynamic spectrum access which allows the unlicensed users to occupy the vacant spectrum of licensed users for their communication. Spectrum decision is a fundamental requirement in the successful implementation of cognitive radios which enables the unlicensed users to occupy the best channel out of available channel slots. This paper presents a novel spectrum decision scheme which remains an unexplored area in cognitive radio research. The proposed decision making scheme is based on the fusion of three key channel parameters i.e. channel idle time, channel occupancy status and channel performance, thereby facilitating the secondary user to occupy the targeted channel without impairing the licensed user's communication while maintaining its own QoS requirements. Decision structural function is used as an evaluation measure to check the robustness of\u00a0\u2026", "num_citations": "2\n", "authors": ["1251"]}
{"title": "Contrast enhancement based flood monitoring\n", "abstract": " A three step based contrast enhancement technique is proposed for flood monitoring. The proposed technique takes pre and post images and apply different processing steps for generating flood map quickly without user interaction which is used by disaster management and rehabilitation authorities. A specific pre processing phase \u201ccontrast enhancement\u201d, is also proposed. Simulation results show that the proposed technique provides better output quality (better visual results and remove unwanted artifacts) compared to state of art existing technique.", "num_citations": "2\n", "authors": ["1251"]}
{"title": "Analyzing effects of multi agent's technology towards software quality assurance and quality engineering\n", "abstract": " Technological boom has changed the working practices of software engineers and brought new ideas along with it. Thus from a standalone co-located development, the software now a days are developed in distributed, more collaborated and dynamic environment. This paper discusses quality engineering (QE) activities specifically dealing with the issues of quality assurance (QA) for software projects developed in a distributed environment. A multi agent framework is presented to help the quality manager to achieve quality in the software product.", "num_citations": "2\n", "authors": ["1251"]}
{"title": "Level-3 Geometric Correction of FORMOSAT-2 Satellite Imagery and Efficient Image Resampling\n", "abstract": " A significant problem in satellite imagery is geometric distortion. Accurate remote sensing and high resolution satellite images have made it necessary to revise the geometric correction techniques used for ortho-rectification. Conventional methods of photogrammetric modeling of remotely sensed images are insufficient for mapping purposes and need to be substituted with more rigorous approach to get a true orthophoto. FORMOSAT-2, a newly launched remote sensing Taiwanese satellite, has high spatial resolution sensor onboard for a daily revisit orbit. However, like any image acquisition system, it also produces geometric distortions in its raw images. Pixel Projection Model (PPM) was devised by National Space Program Office (NSPO) Taiwan, for processing of Level-1A (Raw) satellite images to Level-2 (radio metrically corrected) images. Being systematically corrected, Level-2 images still possess terrain\u00a0\u2026", "num_citations": "2\n", "authors": ["1251"]}
{"title": "A Secured Demand Side Management Engine for Smart Societies using Industrial IoT and Big Data Analytics\n", "abstract": " Smart societies have an increasing demand for quality-oriented services and infrastructure in an Industrial Internet of Things (IIoT) paradigm. Smart urbanization faces numerous challenges. Among them, secured energy Demand Side Management (DSM) is of particular concern. The IIoT renders the industrial systems to malware, cyber attacks, and other security risks. The IIoT with the amalgamation of Big Data analytics can provide efficient solutions to such challenges. This paper proposes a secured and trusted multi-layered DSM engine for a smart social society using IIoT-based Big Data analytics. The proposed engine uses a centralized approach to achieve optimum DSM over a Home Area Network (HAN). To enhance the security of this engine, a payload-based authentication scheme is utilized that relies on a lightweight handshake mechanism. Our proposed method utilizes the lightweight features of Constrained Application Protocol (CoAP) to facilitate the clients in monitoring various resources residing over the server in an energy-efficient manner. In addition, data streams are processed using Big Data analytics with MapReduce parallel processing. The proposed authentication approach is evaluated using NetDuino Plus 2 boards that yield a lower connection overhead, memory consumption, response time and a robust defense against various malicious attacks. On the other hand, our data processing approach is tested on reliable datasets using Apache Hadoop with Apache Spark to verify the proposed DMS engine. The test results reveal that the proposed architecture offers valuable insights into the smart social societies in the context\u00a0\u2026", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Frame Interpolation Using Phase Information and Guided Image Filtering\n", "abstract": " Videos with low frame rate lacks the visual quality element and unable to meet the standards of new multimedia systems. In this paper, a technique for frame interpolation utilizing phase information is proposed. Phase information gives the intuition that the motion of signals can be depicted as a phase shift. The two consecutive input frames of video are passed through a guided filter to preserve edges of objects in frames. These frames then decompose into multi scale pyramid and the difference in each pixel is calculated to compute the phase difference which then used to interpolate the in-between frame. The proposed technique can be used to increase the frame rate of videos. Subjective and objective comparison is performed with the state of art existing technique to prove the significance of proposed technique.", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Usability concerns of android casual game applications: analysis and improvements\n", "abstract": " For the game to be successful and for a good user experience, game usability and its interface usability is very important. Users now have a great variety of mobile game applications but their usability is not guaranteed. Among many other game applications, android casual game applications are very \u201cin\u201d these days. Bubble shooter concept based games are selected for this study, being the casual game applications. As there was no usability framework for these types of games, so usability questionnaire framework is developed to test their usability on android games audience. Secondly usability improvement framework is also suggested to improve these games and is also the outcome of the study along with the questionnaire framework.", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Evaluation of speak_ it application to identify usability heuristics loopholes with suggestions\n", "abstract": " Our social setup believes in dealing with every community equally. This research work is conducted to offer a social responsibility to improve usability of android applications (apps) for deaf and dumb so that they can enjoy ordinary experience of communication through their android devices. One of such applications is \u201cSpeak_it\u201d. \u201cSpeak_it\u201d is an android app that helps a deaf and dumb user to communicate with normal people around him. It converts the typed text into voice message. Usability is evaluated through Heuristics Evaluation and suggestions are provided to overcome the loopholes.", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Single image magnification with edge enhancement\n", "abstract": " Image magnification takes an input image and provides an output image of larger size as compared to the original image. Many techniques for the purpose like nearest-neighbor, bilinear and bicubic interpolation but these results in artifacts like blurring, aliasing and ringing effect. In this paper, we have proposed an algorithm that takes a single image as an input and after detecting the edges; these edges are preserved in the magnified image by enhancing the original pixels that are the part of the edges and then the left undefined pixels are filled in correspondence with the neighbor pixels. In this way the edges of the original image are enhanced and artifacts free image is obtained. A comparison of this algorithm with other technique is also done to provide the quantitative and qualitative result to prove the effectiveness of the methods.", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Analyzing impact of video codec, encapsulation methods and streaming protocols on the quality of video streaming\n", "abstract": " There are various factors involved in the transmission of video over internet. These factors have direct impact on the quality of the transmitted video. The underlying video codec, encapsulation method and protocol are the key players which affect the quality of video transmission on a network. Up to our knowledge various research studies have been carried out to analyze the impact of these factors independently on the quality of video transmission however their combined effect on video transmission is not yet done. In this paper we characterize the impact of video codec, encapsulation methods and transmission protocols of video streaming setup, while keeping in view the percent frame loss as the main investigating parameter. The results obtained from the experiments carried out for the key investigating factor are presented in the results section of this paper. To be more precise, it was observed that the wmv1\u00a0\u2026", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Report Generation on ECGs Survey Data Analysis Using Threshold Based Inference Engine\n", "abstract": " Heart diseases and strokes are considered as number one killer as they account for around 35 to 40 per cent of the total disease burden in Pakistan. The ratio of heart patients is increasing day by day, which is an alarming condition for the country. This situation needs a detailed analysis which can show the geographical distribution of heart patients and also the city wise attributes (age, weight, income etc) that are aggregating more in the heart disease. A Threshold Based Inference Engine is designed which infers the knowledge base by generating the association rules on each city. These rules infer the clustered data to extract the city wise more risk increasing attributes, and the common disease in that city. Automated Minnesota code is used for the verification of the collected ECGs. The results show that Threshold based Inference Engine successfully and efficiently generates a detailed report of each city including more diseased people and highlights the attributes increasing the risk factor.", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Service Oriented Architectural Framework for Pakistan's Disaster Emergency Response (in relation to floods)\n", "abstract": " Pakistan is vulnerable to disaster risks from a range of hazards. Over the years floods have proven to be the most damaging to life, property and overall economy. The National Disaster Risk Management Framework has been formulated by the National Disaster Management Authority. Nine priority areas have been identified within this framework to establish and strengthen policies, institutions and capacities. Emergency Response is one of the priority areas. The use of IT for disaster management is yet to be explored in Pakistan. No such software framework exists. Service oriented Architecture (SOA) is a solution based architecture which utilizes functions as loosely coupled services thus promoting flexibility and reusability. It can be very helpful in providing a framework for disaster management in Pakistan. This paper proposes a software framework for Emergency Response for Disasters through the use of Service\u00a0\u2026", "num_citations": "1\n", "authors": ["1251"]}
{"title": "An Agent-Based Autonomous Controller for Traffic Management\n", "abstract": " Emerging trends in software development has been changed due to the huge amount of data, growth of internet, mobile, dynamic and smart applications. These applications consist of small, intelligent, flexible and distributed components known as agents. This research proposes agent-based autonomous controller (ABAC) architecture for managing road traffic. It uses time series of historical traffic intensity to estimate the appropriate time allocation for each signal at a given intersection. Our approach takes care of the exceptional appearance of rescue vehicles (e.g., ambulance) in order to ensure a smooth flow of the traffic. The ABAC architecture counts on several AI techniques germane to assessing the intensity of the traffic using image recognition algorithms. It also counts on environment sensors (sound sensors) in order to detect the advent of emergency vehicles. The ABAC traffic management\u00a0\u2026", "num_citations": "1\n", "authors": ["1251"]}
{"title": "Colour Removal from Textile Dyeing Wastewater Using Different Adsorbents\n", "abstract": " The ability of different adsorbents/coagulants, such as liquid and solid polymers, ferric chloride, calcium carbonate and coal ash, was investigated for uptake of (reactive dyes, Red-120, Yellow-14 and Blue-4 from textile dyeing waste. Coal ash was used for the colour removal from the textile dyeing wastewater of reactive dyes. Different adsorbents removed the colour from the effluent in different degrees; in some cases the colour was removed 100%. White polymer was ineffective. Calcium carbonate gave excellent results. Liquid polymers were better effective than the solid ones. Coal ash yielded good results without any further treatment.", "num_citations": "1\n", "authors": ["1251"]}