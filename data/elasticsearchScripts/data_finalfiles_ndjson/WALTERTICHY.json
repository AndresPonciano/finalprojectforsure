{"title": "RCS\u2014A system for version control\n", "abstract": " An important problem in program development and maintenance is version control, i.e. the task of keeping a software system consisting of many versions and configurations well organized. The Revision Control System (RCS) is a software tool that assists with that task. RCS manages revisions of text documents, in particular source programs, documentation, and test data. It automates the storing, retrieval, logging and identification of revisions, and it provides selection mechanisms for composing configurations. This paper introduces basic version control concepts and discusses the practice of version control using RCS. For conserving space, RCS stores deltas, i.e. differences between successive revisions. Several delta storage methods are discussed. Usage statistics show that RCS's delta method is space and time efficient. The paper concludes with a detailed survey of version control tools.", "num_citations": "1456\n", "authors": ["164"]}
{"title": "Should computer scientists experiment more?\n", "abstract": " Computer scientists and practitioners defend their lack of experimentation with a wide range of arguments. Some arguments suggest that experimentation is inappropriate, too difficult, useless, and even harmful. This article discusses several such arguments to illustrate the importance of experimentation for computer science. It considers how the software industry is beginning to value experiments, because results may give a company a three- to five-year lead over the competition.", "num_citations": "723\n", "authors": ["164"]}
{"title": "Design, implementation and evaluation of a revision control system\n", "abstract": " ABSTRACl'The Revision Control System (ReS) is a software tool that helps in managing multiple versions of text: Res automates the saving. restoring. logging, identification, and merging of revisions, and provides access control as well as access synchronization. It is useful for text that is revised frequently, for example programs, documentation, and papers.This paper presents the design and implementation of Res. Both design and implementation are evaluated by contrasting ReS with sees, a similar system. sees is implemented with forward, merged deltas, while ReS uses reverse. separate deltas.(Deltas are the differences between successive revisions.) It is sho'Wn. that the latter technique improves runtime efficiency. while requiring no extra space.", "num_citations": "460\n", "authors": ["164"]}
{"title": "The string-to-string correction problem with block moves\n", "abstract": " Two improvements of the basic algorithm are developed. The first improvement performs well on strings with few replicated symbols. The second improvement runs in time and space linear to the size of the input. Efficient algorithms for regenerating a string from an edit sequence are also presented.", "num_citations": "309\n", "authors": ["164"]}
{"title": "Impact of software engineering research on the practice of software configuration management\n", "abstract": " Software Configuration Management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger, more long lasting, and more mission and life critical. This article discusses the evolution of SCM technology from the early days of software development to the present, with a particular emphasis on the impact that university and industrial research has had along the way. Based on an analysis of the publication history and evolution in functionality of the available SCM systems, we trace the critical ideas in the field from their early inception to their eventual maturation in commercially and freely available SCM systems. In doing so, this article creates a detailed record of the critical value of SCM research and illustrates how research results have shaped the functionality of today's SCM systems.", "num_citations": "246\n", "authors": ["164"]}
{"title": "Case study: extreme programming in a university environment\n", "abstract": " Extreme programming (XP) is a new and controversial software process for small teams. A practical training course at the University of Karlsruhe led to the following observations about the key practices of XP. First, it is unclear how to reap the potential benefits of pair programming, although pair programming produces high-quality code. Second, designing in small increments appears to be problematic but ensures rapid feedback about the code. Third, while automated testing is helpful, writing test cases before coding is a challenge. Last, it is difficult to implement XP without coaching. This paper also provides some guidelines for those starting out with XP.", "num_citations": "246\n", "authors": ["164"]}
{"title": "Tools for software configuration management\n", "abstract": " CiNii \u8ad6\u6587 - Tools for Software Configuration Management CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831 \u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c \u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b \u306b\u3064\u3044\u3066 Tools for Software Configuration Management TICHY WF \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 TICHY WF \u53ce\u9332\u520a\u884c\u7269 Proceeding of the International Workshop on Software Version and Configuration Control Proceeding of the International Workshop on Software Version and Configuration Control, 1-20, 1988 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30de\u30cd\u30b8\u30e1\u30f3\u30c8 \uff1a\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u69cb\u6210\u7ba1\u7406\u3068\u4fdd\u5b88\u7ba1\u7406 \u677e\u5c3e\u8c37 \u5fb9 \u60c5\u5831\u51e6\u7406 33(8), 945-953, 1992-08-15 \u53c2\u8003\u6587\u732e31\u4ef6 \u88ab \u5f15\u7528\u6587\u732e1\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10006727089 \u8cc7\u6599\u7a2e\u5225 \u4f1a\u8b70\u8cc7\u6599 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 /\u2026", "num_citations": "237\n", "authors": ["164"]}
{"title": "Hints for reviewing empirical work in software engineering\n", "abstract": " Papers about empirical work in software engineering are still somewhat of a novelty and reviewers, especially those inexperienced with empirical work themselves, are often unsure whether a paper is good enough for publication. Conservative reviewers tend to err on the side of rejection, ie, may sometimes reject a paper that, though not perfect, would advance our understanding or the empirical methods used. These hints are meant to help with judging empirical work and reduce some of the angst associated with accepting empirical papers.", "num_citations": "218\n", "authors": ["164"]}
{"title": "Delta algorithms: An empirical analysis\n", "abstract": " Delta algorithms compress data by encoding one file in terms of another. This type of compression is useful in a number of situations: strong multiple versions of data, displaying differences, merging changes, distributing updates, storing backups, transmitting video sequences, and others. This article studies the performance parameters of several delta algorithms, using a benchmark of over 1,300 pairs of files taken from two successive releases of GNU software. Results indicate that modern delta compression algorithms based on Ziv-Lempel techniques significantly outperform diff, a popular but older delta compressor, in terms of compression ratio. The modern compressors also correlate better with the actual difference between files without sacrificing performance.", "num_citations": "217\n", "authors": ["164"]}
{"title": "Smart recompilation\n", "abstract": " With current compiler technology, changing a single line in a large software system may trigger massive recompilations. If the change occurs in a file with shared declarations, all compilation units depending upon that file must be recompiled to assure consistency. However, many of those recompilations may be redundant, because the change may affect only a small fraction of the overall system. Smart recompilation is a method for reducing the set of modules that must be recompiled after a change. The method determines whether recompilation is necessary by isolating the differences among program modules and analyzing the effect of changes. The method is applicable to languages with and without overloading. A prototype demonstrates that the method is efficient and can be added with modest effort to existing compilers.", "num_citations": "190\n", "authors": ["164"]}
{"title": "EDGE: An extendible graph editor\n", "abstract": " EDGE is an editor kernel for the direct and visual manipulation of graphs. The kernel can be adapted quickly to diverse applications based on graphs, such as PERT chart editing, directory browsing, call graph display, logic circuit simulation or configuration visualization. EDGE provides potential solutions to the following general problems faced by any graph editor. (1) Automatic graph layout: how can application\u2010specific layout requirements, individual preferences, and layout stability be integrated with automatic layout algorithms? EDGE solves this problem with a novel algorithm that is based on layout constraints. (2) Graph abstraction: how can users deal with large graphs containing hundreds of nodes and edges, and thousands of edge crossings? EDGE can reduce the apparent complexity with subgraph abstractions and a novel clustering technique called edge concentration. (3) Adaptability: how should the\u00a0\u2026", "num_citations": "149\n", "authors": ["164"]}
{"title": "Software development control based on module interconnection\n", "abstract": " Constrtlcting large software systems is not merely a matter at r programming, but also a matter of communication and coordination. Problems arise because many people work on a joint project and use each other's programs. This paper presents an integrated development and maintenance system that provides a controlling environment to insure the consistency of a software system at the module interconnection level. It assists the programmers with two facilities: Interface control and version control. Interface control estal) lishes consistent interfaces between software modules and maintains this consistency when changes are made. Version control coordinates the g~. neration and integration of the various versions and configurations. Both facilities derive their information from an overall system(lescription formulated in the module interconnection language INTERCOL. A demonstration system is sketched that runs under the UNIX time-sharing system.", "num_citations": "108\n", "authors": ["164"]}
{"title": "Status of empirical research in software engineering\n", "abstract": " We provide an assessment of the status of empirical software research by analyzing all refereed articles that appeared in the Journal of Empirical Software Engineering from its first issue in January 1996 through June 2006. The journal publishes empirical software research exclusively and it is the only journal to do so. The main findings are: 1. The dominant empirical methods are experiments and case studies. Other methods (correlational studies, meta analysis, surveys, descriptive approaches, ex post facto studies) occur infrequently; long-term studies are missing. About a quarter of the experiments are replications. 2. Professionals are used somewhat more frequently than students as subjects. 3. The dominant topics studied are measurement/metrics and tools/methods/frameworks. Metrics research is dominated by correlational and case studies without any experiments. 4. Important topics are\u00a0\u2026", "num_citations": "97\n", "authors": ["164"]}
{"title": "Software engineering for multicore systems: an experience report\n", "abstract": " The emergence of inexpensive parallel computers powered by multicore chips combined with stagnating clock rates raises new challenges for software engineering. As future performance improvements will not come\" for free\" from increased clock rates, performance critical applications will need to be parallelized. However, little is known about the engineering principles for parallel general-purpose applications.", "num_citations": "96\n", "authors": ["164"]}
{"title": "Configuration management\n", "abstract": " Configuration management (CM) is the discipline of organizing and controlling evolving systems. For hardware systems, CM has been an established field since the 1950s. It was initially developed in the aerospace industry as an approach for guaranteeing reproducibility of spacecraft. The problem was that spacecraft underwent numerous, inadequately documented engineering changes during development. Moreover, the prototypes embodying the changes were usually expended during flight test. Consequently, neither accurate plans nor prototypes were available for replicating successful designs. Configuration management was thus born out of the need to track what designers and engineers developed during the course of a project. Software Configuration Management (SCM) is the specialization of CM for software systems. Software systems, too, undergo numerous, usually inadequately documented\u00a0\u2026", "num_citations": "83\n", "authors": ["164"]}
{"title": "Helgrind+: An efficient dynamic race detector\n", "abstract": " Finding synchronization defects is difficult due to non-deterministic orderings of parallel threads. Current tools for detecting synchronization defects tend to miss many data races or produce an overwhelming number of false alarms. In this paper, we describe Helgrind + , a dynamic race detection tool that incorporates correct handling of condition variables and a combination of the lockset algorithm and happens-before relation. We compare our techniques with Intel Thread Checker and the original Helgrind tool on two substantial benchmark suites. Helgrind +  reduces the number of both false negatives (missed races) and false positives. The additional accuracy incurs almost no performance overhead.", "num_citations": "82\n", "authors": ["164"]}
{"title": "The cost of selective recompilation and environment processing\n", "abstract": " When a single software module in a large system is modified, a potentially large number of other modules may have to be recompiled. By reducing both the number of compilations and the amount of input processed by each compilation run, the turnaround time after changes can be reduced significantly. Potential time savings are measured in a medium-sized, industrial software project over a three-year period. The results indicate that a large number of compilations caused by traditional compilation unit dependencies may be redundant. On the available data, a mechanism that compares compiler output saves about 25 percent, smart recompilation saves 50 percent, and smartest recompilation may save up to 80 percent of compilation work. Furthermore, all compilation methods other than smartest recompilation process large amounts of unused environment data. In the project analyzed, the average environment\u00a0\u2026", "num_citations": "81\n", "authors": ["164"]}
{"title": "An empirical study of delta algorithms\n", "abstract": " Delta algorithms compress data by encoding one file in terms of another. This type of compression is useful in a number of situations: storing multiple versions of data, distributing updates, storing backups, transmitting video sequences, and others. This paper studies the performance parameters of several delta algorithms, using a benchmark of over 1300 pairs of files taken from two successive releases of GNU software. Results indicate that modern delta compression algorithms based on Ziv-Lempel techniques significantly outperform diff, a popular but older delta compressor, in terms of compression ratio. The modern compressors also correlate better with the actual difference between files; one of them is even faster than diff in both compression and decompression speed.", "num_citations": "77\n", "authors": ["164"]}
{"title": "Software development control based on system structure description\n", "abstract": " Constructing large software systems is not merely a matter of programming but also a matter of human interaction. Problems arise because a number of people work on a joint project and need to cooperate. This dissertation discusses an integrated software development and maintenance environment that supports communication and cooperation among programmers. The environment has three aspects: (1) Representation. A notation to describe the overall structure of a programmed system is the basis of the environment. This notation is a module interconnection language called INTERCOL. It can represent multiple versions and configurations, written in multiple programming languages. The notation contains enough information for automating interface control and version control. (2) Interface Control. The environment establishes consistent interfaces among separately developed software components and\u00a0\u2026", "num_citations": "75\n", "authors": ["164"]}
{"title": "Atune-IL: An instrumentation language for auto-tuning parallel applications\n", "abstract": " Auto-tuners automate the performance tuning of parallel applications. Three major drawbacks of current approaches are 1) they mainly focus on numerical software; 2) they typically do not attempt to reduce the large search space before search algorithms are applied; 3) the means to provide an auto-tuner with additional information to improve tuning are limited.               Our paper tackles these problems in a novel way by focusing on the interaction between an auto-tuner and a parallel application. In particular, we introduce Atune-IL, an instrumentation language that uses new types of code annotations to mark tuning parameters, blocks, permutation regions, and measuring points. Atune-IL allows a more accurate extraction of meta-information to help an auto-tuner prune the search space before employing search algorithms. In addition, Atune-IL\u2019s concepts target parallel applications in general, not just\u00a0\u2026", "num_citations": "72\n", "authors": ["164"]}
{"title": "A catalogue of general-purpose software design patterns\n", "abstract": " Software design patterns describe proven solutions to recurring software design problems. Knowledge of these patterns increases designers' abilities, leads to cleaner and more easily maintained software, speeds up implementation and test, and helps programmers document and communicate their designs. This paper catalogues over 100 general-purpose design patterns. The organizing principle of the catalogue is the use of patterns, i.e., the problems they solve. Other considerations, such as whether a pattern is behavioral or structural, how it is implemented, or whether it is high or low level, are secondary, because these aspects are less important for a designer looking for a solution to a design problem. The catalogue collects general-purpose patterns from a variety of sources. It includes older patterns such as Module and Layers as well as modern, object-oriented patterns such as Observer and Visitor.", "num_citations": "71\n", "authors": ["164"]}
{"title": "A data model for programming support environments and its application\n", "abstract": " A critical issue in programming support environments is the data base that stores all project information. This paper presents a model that can be used for analyzing and designing such data bases. The model represents systems as families consisting of multiple versions and configurations. It is based on AND/OR graphs and has the hierachical model, the relational model, and the sequential release model as subclasses.", "num_citations": "66\n", "authors": ["164"]}
{"title": "Clusterfile: A flexible physical layout parallel file system\n", "abstract": " This paper presents Clusterfile, a parallel file system that provides parallel file access on a cluster of computers. We introduce a file partitioning model that has been used in the design of Clusterfile. The model uses a data representation that is optimized for multidimensional array partitioning while allowing arbitrary partitions. The paper shows how the file model can be employed for file partitioning into both physical subfiles and logical views. We also present how the conversion between two partitions of the same file is implemented using a general memory redistribution algorithm. We show how we use the algorithm to optimize non\u2010contiguous read and write operations. The experimental results include performance comparisons with the Parallel Virtual File System (PVFS) and an MPI\u2010IO implementation for PVFS. Copyright \u00a9 2003 John Wiley & Sons, Ltd.", "num_citations": "64\n", "authors": ["164"]}
{"title": "Highly parallel computation\n", "abstract": " Highly parallel computing architectures are the only means to achieve the computational rates demanded by advanced scientific problems. A decade of research has demonstrated the feasibility of such machines, and current research focuses on which architectures are best suited for particular dasses of problems. The architectures designated as MIMD and SIMD have produced the best results to date; neither shows a decisive advantage for most near-homogeneous scientific problems. For scientific problems with many dissimilar parts, more speculative architectures such as neural networks or data flow may be needed.", "num_citations": "64\n", "authors": ["164"]}
{"title": "Integrating collective I/O and cooperative caching into the\" clusterfile\" parallel file system\n", "abstract": " This paper presents the integration of two collective I/O techniques into the Clusterfile parallel file system: disk-directed I/O and two-phase I/O. We show that global cooperative cache management improves the collective I/O performance. The solution focuses on integrating disk parallelism with other types of parallelism: memory (by buffering and caching on several nodes), network (by parallel I/O scheduling strategies) and processors (by redistributing the I/O related computation over several nodes). The performance results show considerable throughput increases over ROMIO's extended two-phase I/O.", "num_citations": "62\n", "authors": ["164"]}
{"title": "Measuring high performance computing productivity\n", "abstract": " One key to improving high performance computing (HPC) productivity is to find better                 ways to measure it. We define productivity in terms of mission goals, i.e. greater                 productivity means that more science is accomplished with less cost and effort.                 Traditional software productivity metrics and computing benchmarks have proven                 inadequate for assessing or predicting such end-to-end productivity. In this paper                 we introduce a new approach to measuring productivity in HPC applications that                 addresses both development time and execution time. Our goal is to develop a public                 repository of effective productivity benchmarks that anyone in the HPC community can                 apply to assess or predict productivity.", "num_citations": "60\n", "authors": ["164"]}
{"title": "Parallelizing bzip2: A case study in multicore software engineering\n", "abstract": " We conducted a case study of parallelizing a real program for multicore computers using currently available libraries and tools. We selected the sequential Bzip2 compression program for the study because it's a computing-intensive, widely used, and relevant application in everyday life. Its source code is available, and its algorithm is well documented. In addition, the algorithm is non-trivial, but, with 8,000 LOC, the application is small enough to manage in a course.", "num_citations": "59\n", "authors": ["164"]}
{"title": "Impact of the research community on the field of software configuration management: summary of an impact project report\n", "abstract": " Software Configuration Management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger and more complex and mission/life-critical. This paper discusses the evolution of SCM technology from the early days of software development to present and the impact university and industrial research has had along the way. It also includes a survey of the industrial state-of-the-practice and research directions.The paper published here is not intended to be a definitive assessment. Rather, our intention is to solicit comments and corrections from the community to help refine the work. If you would like to provide further information, please contact the first author. A longer version of this report can be found at http://wwwadele.imag.fr/SCMImpact.pdf.", "num_citations": "55\n", "authors": ["164"]}
{"title": "Extensible language-aware merging\n", "abstract": " Parallel development has become standard practice in software development and maintenance. Though almost every revision control and configuration management system provides some form of merging for combining changes made in parallel, these mechanisms often yield unsatisfactory results. The authors present a new merging algorithm, that uses a fast differencing algorithm and renaming analysis to provide better merge results. The system is language aware, but not language dependent and does not require a special editor so it can be easily integrated in current development environments.", "num_citations": "52\n", "authors": ["164"]}
{"title": "Towards a distributed file system\n", "abstract": " This paper describes IBIS, a distributed file system for a network. of UNIX machines. IBIS provides two levels of abstraction: file access transparency and file location transparency. File access transparency means that all files are accessed in the same way, regardless of whether they are remote or local. File location transparency hides the location of files in the network. IBIS provides a single, location transparent, hierarchical file system that spans several machines. IBIS exploits location transparency by replicating files and mi~ rating them where they are needed. Replication and migration improve file system efficiency and fault tolerance. This paper reports on the design, implementation, and performance of the access transparency level, and descri: Jes the design of the location transparency level.", "num_citations": "52\n", "authors": ["164"]}
{"title": "Renaming detection\n", "abstract": " Finding changed identifiers is important for understanding the difference between two versions of a program and for detecting and resolving conflicts while merging variants of a program together. Standard practice for differencing and merging relies on line based techniques that do not recognize renamed identifiers. The design and implementation of a tool to automatically detect renamed identifiers between two versions of a program is presented. The system uses an abstract representation of language constructs to enable language awareness without introducing language dependence. Modules for Java and Scheme have been written. The detector works with multiple file pairs, taking into account renamings that span several files. A case study is presented that demonstrates proof of concept. The detector is part of a suite of intelligent differencing and merging programs that exploit the static semantics of\u00a0\u2026", "num_citations": "51\n", "authors": ["164"]}
{"title": "Distributed file system and method of operating a distributed file system\n", "abstract": " A distributed file system including a plurality of compute nodes and a plurality of input/output (I/O) nodes connected by an interconnection network wherein the system is adapted to use a common data representation for both physical and logical partitions of a file stored in the system and wherein the partitions are linearly addressable. Also provided is a method of operating a distributed file system including a plurality of input/output (I/O) nodes and a plurality of compute nodes, the method including partitioning a file into a plurality of subfiles distributed across a plurality of I/O nodes; logically partitioning a file by setting a view on it, computing mappings between a linear space of a file and a linear space of a subfile, computing the intersection between a view and a subfile, and performing data operations.", "num_citations": "48\n", "authors": ["164"]}
{"title": "On-the-fly race detection in multi-threaded programs\n", "abstract": " Multi-core chips enable parallel processing for general purpose applications. Unfortunately, parallel programs may contain synchronization defects. Such defects are difficult to detect due to nondeterministic interleavings of parallel threads. Current tools for detecting these defects produce numerous false alarms, thereby concealing the true defects. This paper describes an extended race detection technique based on a combination of lockset analysis and the happens-before relation. The approach provides more accurate warnings and significantly reduces the number of false positives, while limiting the number of false negatives. The technique is implemented in Helgrind+, an extension of the open source dynamic race detector Helgrind. Experimental results with several applications and benchmarks demonstrate a significant reduction in false alarms at a moderate runtime increase.", "num_citations": "47\n", "authors": ["164"]}
{"title": "What can software engineers learn from artificial intelligence?\n", "abstract": " T gence programming tools by a non-Al scientist. It is based on my intensive personal involvement with the follow-ing programming tools: frame-based representation languages, Lisp programming environments, natural-language parsers, productionsystems, and Prolog. My most significant initial insight resulting from this experience is that frame-based representation facilities with inheritance and a sophisticated Lisp system synthesizealanguage level noticeably higher than that of languages in the Algol tradition, such as Ada, Modula, C, or Pascal. The higher language level makes pro-gramming easier and allows programmers to see further. It makes it possible to envisage and build sophisticated systems that might be rejected as too advanced or too expensive to construct in traditional environments.", "num_citations": "44\n", "authors": ["164"]}
{"title": "From requirements to UML models and back: how automatic processing of text can support requirements engineering\n", "abstract": " Software engineering is supposed to be a structured process, but manual tasks leave much leeway. Ideally, these tasks lie in the hands of skilled analysts and software engineers. This includes creating the textual specification of the envisioned system as well as creating models for the software engineers. Usually, there is quite a bit of erosion during the process due to requirement changes, implementation decisions, etc. To deliver the software as specified, textual requirements, models, and the actual software need to be synchronized. However, in practice, the cost of manually maintaining consistency is too high. Our requirements engineering feedback system automates the process of keeping textual specification and models consistent when the models change. To improve overall processing of natural language specifications, our approach finds flaws in natural language specifications. In addition to the\u00a0\u2026", "num_citations": "43\n", "authors": ["164"]}
{"title": "Modula-2* and its compilation\n", "abstract": " Modula-2*, an extension of Modula-2, is a programming language for writing highly parallel programs in a machine-independent, problem-oriented way. The novel attributes of Modula-2* are that programs are independent of the number of processors, independent of whether memory is shared or distributed, and independent of the control modes (SIMD or MIMD) of a parallel machine.             This article briefly describes Modula-2* and discusses its major advantages over the data-parallel programming model. We also present the principles of translating Modula-2* programs to MIMD and SIMD machines and discuss the lessons learned from our first compiler, targeting the Connection Machine. We conclude with important architectural principles required of parallel computers to allow for efficient, compiled programs.", "num_citations": "43\n", "authors": ["164"]}
{"title": "Project Triton: Towards improved programmability of parallel machines\n", "abstract": " The approach taken in the Triton project is to let a high-level machine-independent parallel programming language drive the design of parallel hardware. This approach permits machine-independent parallel programs to be compiled into efficient machine code. The main results are as follows: (1) The parallel programming language Modula-2* extends Modula-2 with constructs for expressing a wide range of parallel algorithms in a high-level, portable, and readable way. (2) Techniques are used for efficiently translating Modula-2* programs to several modern parallel architectures and deriving recommendations for future parallel machine architectures. (3) Triton/1 is a scalable, mixed-mode SIMD/MIMD parallel computer with a highly efficient communications network. It overcomes several deficiencies of current parallel hardware and adequately supports high-level parallel languages.< >", "num_citations": "40\n", "authors": ["164"]}
{"title": "Identifying ad-hoc synchronization for enhanced race detection\n", "abstract": " Parallel programs contain a surprising number of ad-hoc synchronization operations. Ad-hoc synchronization operations are loops that busy-wait on condition variables. Current race detectors produce unnecessary warnings (false positives) when ad-hoc synchronization is used. False positives are also generated when programmers use synchronization primitives that are unknown to race detectors, for instance when programmers switch libraries. These shortcomings may result in an overwhelming number of false positives, dissuading programmers from using race detectors. This paper shows that ad-hoc synchronization operations can be detected automatically. The method requires no user intervention such as annotations and has been implemented in the race detector Helgrind + . Evaluation results on various benchmarks confirm that Helgrind +  is aware of all synchronizations in programs, reliably reports true\u00a0\u2026", "num_citations": "39\n", "authors": ["164"]}
{"title": "Thematic role based generation of UML models from real world requirements\n", "abstract": " Model-driven development depends on good initial models. Creating these models by hand is a challenging task, because of complex specification documents and change requests. We propose a new internal representation based on thematic roles, especially designed for (but not limited to) requirements documents. The representation can be generated automatically out of annotated real-world specification text and can be used to generate UML models based on graph rewriting rules.", "num_citations": "38\n", "authors": ["164"]}
{"title": "Transparent distributed threads for Java\n", "abstract": " Remote method invocation in Java RMI allows the flow of control to pass across local Java threads and thereby span multiple virtual machines. However, the resulting distributed threads do not strictly follow the paradigm of their local Java counterparts for at least three reasons. Firstly, the absence of a global thread identity causes problems when reentering monitors. Secondly, blocks synchronized on remote objects do not work properly. Thirdly, the thread interruption mechanism for threads executing a remote call is broken. These problems make multi-threaded distributed programming complicated and error prone. We present a two-level solution: On the library level, we extend KaRMI (Philippsen et al. (2000)), a fast replacement for RMI, with global thread identities for eliminating problems with monitor reentry. Problem with synchronization on remote objects are solved with a facility for remote monitor acquisition\u00a0\u2026", "num_citations": "38\n", "authors": ["164"]}
{"title": "Distributed configuration management via Java and the World Wide Web\n", "abstract": " The introduction of Java has been heralded as a revolution in network computing. Certainly, machine and operating system independent applets flittering through the Internet promised to jazz up web surfing; but could they be used to advantage for distributed computing? The authors had encountered substantial problems in implementing a distributed revision control system, called WWRC, based on passive Web browsers. Java seemed to offer solutions to these problems. To this end, the authors have developed WWCM, a successor to WWRC written in Java. WWCM extends the concepts of WWRC to distributed configuration management by using CME\u2014a new configuration management API. WWCM demonstrates that most of the design difficulties encountered with WWRC can be solved with Java. Furthermore, WWCM offers a test bed for a configuration management paradigm called template regulated\u00a0\u2026", "num_citations": "37\n", "authors": ["164"]}
{"title": "Compiling for massively parallel machines\n", "abstract": " This article discusses techniques for compiling high-level, explicitly-parallel languages for massively parallel machines.               We present mechanisms for translating asynchronous as well as synchronous parallelism for both SIMD and MIMD machines. We show how the parallelism specified in a program is mapped onto the available processors and discuss an effective optimization that eliminates redundant synchronization points. Approaches for improving scheduling, load balancing, and co-location of data and processes are also presented. We conclude with important architectural principles required of parallel computers to support efficient, compiled programs.               Our discussion is based on the language Modula-2*, an extension of Modula-2 for writing highly parallel programs in a machine-independent, problem-oriented way. The novel attributes of Modula-2* are that programs are independent\u00a0\u2026", "num_citations": "36\n", "authors": ["164"]}
{"title": "A critique of the programming language C\n", "abstract": " C* is a data parallel programming language originally developed for the Connection Machine. E orts are now underway to standardize a revised version of C* 6]. We think that standardization of C* is premature at this time, since the language contains a number of unproven constructs and obvious aws. We are concerned that standardization of a parallel language now might force its programming model upon future generations of programmers, even though we already know it is de cient. The purpose of this note is to make the relevant issues accessible to a wider audience and to make speci c recommendations for improving C*.C* is an extension of ANSI standard C and intended as an\\e cient, fairly low-level systems programming language 6]\" for parallel computers with distributed memory. Parallelism is expressed directly in the data parallel paradigm. In this paradigm,\\parallelism comes from simultaneous operations across large sets of data, rather than from multiple threads of control 2]\". Data parallelism is a synchronous paradigm and therefore well suited to SIMD machines. It has also been implemented successfully on a MIMD machine 5].", "num_citations": "36\n", "authors": ["164"]}
{"title": "NLCI: a natural language command interpreter\n", "abstract": " Natural language interfaces are becoming more and more common, because they are powerful and easy to use. Examples of such interfaces are voice controlled navigation devices, Apple\u2019s personal assistant Siri, Google Voice Search, and translation services. However, such interfaces are extremely difficult to build, to maintain, and to port to new domains. We present an approach for building and porting such interfaces quickly. NLCI is a natural language command interpreter that accepts action commands in English and translates them into executable code. The core component is an ontology that models an API. Once the API is \u201contologized\u201d, NLCI translates input sentences into sequences of API calls that implement the intended actions. Two radically different APIs were ontologized: openHAB for home automation and Alice for building 3D animations. Construction of the ontology can be automated if\u00a0\u2026", "num_citations": "35\n", "authors": ["164"]}
{"title": "Comments on\" Formal methods application: an empirical tale of software development\"\n", "abstract": " We comment on the experimental design and the result of the paper mentioned in the title. Our purpose is to show interested readers examples of what can go wrong with experiments in software research and how to avoid the attending problems.", "num_citations": "34\n", "authors": ["164"]}
{"title": "The ParaStation Project: using workstations as building blocks for parallel computing\n", "abstract": " The Parastation communication fabric provides a high-speed communication network with user-level access to enable efficient parallel computing on workstation clusters. The architecture, implemented on off-the-shelf workstations coupled by the Parastation communication hardware, removes the kernel and common network protocols from the communication path while still providing full protection in a multiuser, multiprogramming environment. The programming interface presented by Parastation consists of a UNIX socket emulation and widely used parallel programming environments such as PVM, P4, and MPI. This allows porting a wide range of client/server and parallel applications to the Parastation architecture. Implementations of Parastation using various platforms, such as Digital's AlphaGeneration workstations and Linux PCs, achieve end-to-end (process-to-process) latencies as low as 2 \u03bcs and a\u00a0\u2026", "num_citations": "33\n", "authors": ["164"]}
{"title": "NLH/E: A natural language help system\n", "abstract": " A natural language help (NLH) system answers questions that are entered as typed, natural language sentences. Compared to traditional keyword lookup, natural language input allows greater flexibility and precision. This report describes such a help system, NLH/E, which answers questions formulated in English. Currently, it can process questions about a substantial domain: over 130 functions described in five chapters of the Common Lisp manual.NLH/E is built with a novel caseframe parser that operates with a thesaurus, case inheritance, and noun/verb phrase unifica tion. These features reduce the size of the grammer considerably, making it feasible to build realistic NLH systems. Statistical data about the grammar are included.", "num_citations": "32\n", "authors": ["164"]}
{"title": "Programming-in-the-large: Past, Present, and Future\n", "abstract": " The historical development of the concepts of programming-in-the-large is surveyed, covering layered systems, information hiding, languages for programming-in-the-large, object-oriented languages, and software configuration management. Large-scale reuse, stronger foundations and better tools, cataloging of software architectures, and improved preparation of software engineers are identified as future directions.", "num_citations": "29\n", "authors": ["164"]}
{"title": "Poster: ProNat: An agent-based system design for programming in spoken natural language\n", "abstract": " The emergence of natural language interfaces has led to first attempts of programming in natural language. We present ProNat, a tool for script-like programming in spoken natural language (SNL). Its agent-based architecture unifies deep natural language understanding (NLU) with modular software design. ProNat focuses on the extraction of processing flows and control structures from spoken utterances. For evaluation we have begun to build a speech corpus. First experiments are conducted in the domain of domestic robotics, but ProNat's architecture makes domain acquisition easy. Test results with spoken utterances in ProNat seem promising, but much work has to be done to achieve deep NLU.", "num_citations": "28\n", "authors": ["164"]}
{"title": "Engineering parallel applications with tunable architectures\n", "abstract": " Current multicore computers differ in many hardware characteristics. Software developers thus hand-tune their parallel programs for a specific platform to achieve the best performance; this is tedious and leads to non-portable code. Although the software architecture also requires adaptation to achieve best performance, it is rarely modified because of the additional implementation effort. The Tunable Architectures approach proposed in this paper automates the architecture adaptation of parallel programs and uses an auto-tuner to find the best-performing software architecture for a particular machine. We introduce a new architecture description language based on parallel patterns and a framework to express architecture variants in a generic way. Several case studies demonstrate significant performance improvements due to architecture tuning and show the applicability of our approach to industrial applications\u00a0\u2026", "num_citations": "28\n", "authors": ["164"]}
{"title": "ParaStation: Efficient parallel computing by clustering workstations: Design and evaluation\n", "abstract": " ParaStation is a communications fabric for connecting off-the-shelf workstations into a supercomputer. The fabric employs technology used in massively parallel machines and scales up to 4096 nodes. ParaStation's user-level message passing software preserves the low latency of the fabric by taking the operating system out of the communication path, while still providing full protection in a multiprogramming environment. The programming interface presented by ParaStation consists of a UNIX socket emulation and widely used parallel programming environments such as PVM, P4, and MPI. Implementations of ParaStation using various platforms, such as Digital's AlphaGeneration workstations and Linux PCs, achieve end-to-end (process-to-process) latencies as low as 2 \u03bcs and a sustained bandwidth of up to 15 Mbyte/s per channel, even with small packets. Benchmarks using PVM on ParaStation demonstrate\u00a0\u2026", "num_citations": "28\n", "authors": ["164"]}
{"title": "Triton/1: A massively-parallel mixed-mode computer designed to support high level languages\n", "abstract": " We present the architecture of Triton/l, a scalable, mixed-mode (SIMLI/MIMD) parallel computer. The novel features of l? riton/l are:", "num_citations": "28\n", "authors": ["164"]}
{"title": "Knowledge-based editors for directed graphs\n", "abstract": " Directed graphs are used in a significant number of applications for visualizing concepts and relationships. This paper describes research in knowledge-based editors for the direct, visual manipulation of such graphs. The novel aspects of this work are: (1) The editor produces an aesthetically pleasing layout of the graph automatically, freeing the user from cut-and-paste work after changes. (2) The editor can be adapted quickly to a particular application. (3) The editor can invoke application-specific functions for processing the graph while it is being manipulated.             We first present kb-edit, a prototype editor that demonstrates the knowledge-based approach. We then describe current work on EDGE, a new graph editor that provides a more extensive set of methods for controlling the display, for example graphical abstraction, 2 1/2-D display, semi-automatic layout, display of edge attributes, and graph\u00a0\u2026", "num_citations": "28\n", "authors": ["164"]}
{"title": "Text to software: developing tools to close the gaps in software engineering\n", "abstract": " Software development relies heavily on manual processes for transforming requirements into software artifacts such as models, source code, or test cases. Requirements are the starting point for these transformations, and they are typically written in natural language. However, hardly any automated tools exist that translate natural language texts into software artifacts. We propose to adapt recent advances in natural language processing and semantic technologies to generate UML models, test cases, and perhaps even source code, from natural language input. Though earlier efforts in automatic programming had limited success, we are now in a situation where extracting meaning from text has made substantial progress. For example, encouraging results for generating UML models from textual requirements have been achieved. It might even be possible to generate executable test cases. An intermediate step\u00a0\u2026", "num_citations": "25\n", "authors": ["164"]}
{"title": "Application-independent autotuning for GPUs\n", "abstract": " Autotuning is an established technique for adjusting performance-critical parameters of applications to their specific run-time environment. In this paper, we investigate the potential of online autotuning for general purpose computation on GPUs. Our application-independent autotuner AtuneRT optimizes GPU-specific parameters such as block size and loop-unrolling degree. We also discuss the peculiarities of autotuning on GPUs. We demonstrate tuning potential using CUDA and by instrumenting the parallel algorithms library Thrust. We evaluate our online autotuning approach with various GPUs and sample applications.", "num_citations": "24\n", "authors": ["164"]}
{"title": "Software architecture\n", "abstract": " Software Architecture Page 1 Software Architecture David Garlan Carnegie Mellon University NASA Fault Management Workshop New Orleans April 2012 Page 2 4/14/2012 Garlan 2 About me \u220e Professor of Computer Science \u2751 At Carnegie Mellon University since 1990 \u2751 Before then in industry (test and measurement) \u220e Research interests \u2751 Software architecture tools and techniques \u2751 Self-healing and self-adaptive systems \u220e Connection with NASA \u2751 Engagement since 2004 \u2751 Sabbatical at JPL summer of 2006 \u2751 On-going education offerings for several NASA Centers Page 3 4/14/2012 Garlan 3 This Talk \u220e What is Software Architecture? \u2751 Why is it important? \u2751 What are key principles and concepts of software architecture? \u2751 How can formal \u201carchitectural thinking\u201d yield systems that better satisfy their requirements? \u220e Prospects for improving Fault Management through architectural design \u2751 How do these to /\u2026", "num_citations": "24\n", "authors": ["164"]}
{"title": "Advanced operating systems\n", "abstract": " load control, access control and pro-tection, hierarchical file system, device independence, I/0 redirection, and a high-lesel language shell. Another important concept of third-generation systems was the virtual machine, a simulated copy of the hiost. Virtual machines were first tested around 1966 on the M44/44X project at the IBM T. J. Watson Research Center. In the early 1970's sirtual machines were used in IBM's CP-67 system, a timesharing system that assigned each user's process to its own virtual copy of the IBM 360/67 machine. This system, which has been moxed to the IBM 370 machine, is now called VM/370.4,'Because each sirtuLal machine can run a different copy of the operating system, VM/370 is effective for developing new operat-ing systeim. s within the current operating systemn. However, because virtial machines are well isolated, com-municatioin among them is expensive and awkward.", "num_citations": "24\n", "authors": ["164"]}
{"title": "Operating systems\n", "abstract": " A computer operating system spans multiple layers of complexity, from commands entered at a keyboard to the details of electronic switching. The system is organized as a hierarchy of abstractions by Peter J. Denning and Robert L. Brown j\\a terminal connected to a comput er system you type the command date and press the key marked Return. Almost instantly the message September 15, 1984, appears on the dis play screen. Asking for the current date would seem to be among the simpler demands one might make of a comput er, and yet it sets in motion a complex series of events calling into action many of the hardware and software resources of the system. Coordinating the events and managing the resources are among the responsibilities of the collection of programs called the computer operating system. The operating system provides facilities and services needed by almost all other software.Consider\u00a0\u2026", "num_citations": "24\n", "authors": ["164"]}
{"title": "Distributed revision control via the world wide web\n", "abstract": " Revision control has long been a standard part of software development. With the enormous expansion of the Internet and its increasing use as a means of communicating among geographically dispersed software developers, the need for distributed version control over the Internet has become acute. In order to address this need, the authors have developed a revision control server based on the World Wide Web (WWW) and RCE (an outgrowth of RCS). This proves to be possible and it also highlights the strengths and weaknesses of using the Hyper Text Mark up Language and standard WWW browsers such as NetScape\u2122and Mosaic to accomplish this goal.", "num_citations": "23\n", "authors": ["164"]}
{"title": "Automatic generation of parallel unit tests\n", "abstract": " Multithreaded software is subject to data races. Currently available data race detectors report such errors to the developer, but consume large amounts of time and memory; many approaches are not applicable for large software projects. Unit tests containing fractions of the program lead to better results. We propose AutoRT, an approach to automatically generate parallel unit tests as target for data race detectors from existing programs. AutoRT uses the Single Static Multiple Dynamic (SSMD) analysis pattern to reduce complexity and can therefore be used efficiently even in large software projects. We evaluate AutoRT using Microsoft CHESS and show that with SSMD all 110 data races contained in our sample programs can be located.", "num_citations": "21\n", "authors": ["164"]}
{"title": "Fundamentals of multicore software development\n", "abstract": " With multicore processors now in every computer, server, and embedded device, the need for cost-effective, reliable parallel software has never been greater. By explaining key aspects of multicore programming, Fundamentals of Multicore Software Development helps software engineers understand parallel programming and master the multicore challenge.", "num_citations": "21\n", "authors": ["164"]}
{"title": "Panel: Empirical validation-what, why, when, and how\n", "abstract": " Opinions as to the methodology to apply to SE research appear to be in disagreement, as a large variety of possibilities exist [7]. Tichy promotes quantitative, controlled, statistically-analyzable experimentation [6]. Kitchenham et al. recognize the value of \u201cobservational studies\u201d in addition to formal experimentation [3], but emphasize industrial-context, quantitative evaluation, and statistics. Seaman promotes the value of qualitative evaluation [5]. Murphy et al. suggest that a different treatment is needed for emerging technologies than for more mature ones [4]. Briand et al. state that \u201ceach discipline needs to develop its own body of experience and strategies to answer its most pressing research questions\u201d[1, p. 398]. Without some consensus, an SE researcher is faced with a difficult task of convincing their peers that their selected methodology is appropriate, let alone the details of their validation. This panel session strives to address these issues in order to determine where a consensus does and does not exist. Brief synopses of each panelist\u2019s thoughts follow.", "num_citations": "21\n", "authors": ["164"]}
{"title": "Library-independent data race detection\n", "abstract": " Data races are a common problem on shared-memory parallel computers, including multicores. Analysis programs called race detectors help find and eliminate them. However, current race detectors are geared for specific concurrency libraries. When programmers use libraries unknown to a given detector, the detector becomes useless or requires extensive reprogramming. We introduce a new synchronization detection mechanism that is independent of concurrency libraries. It dynamically detects synchronization constructs based on a characteristic code pattern. The approach is non-intrusive and applicable to various concurrency libraries. Experimental results confirm that the approach identifies synchronizations and detects data races regardless of the concurrency libraries involved. With this mechanism, race detectors can be written once and need not be adapted to particular libraries.", "num_citations": "20\n", "authors": ["164"]}
{"title": "Automated test-case generation by cloning\n", "abstract": " Test cases are often similar. A preliminary study of eight open-source projects found that on average at least 8% of all test cases are clones; the maximum found was 42 %. The clones are not identical with their originals - identifiers of classes, methods, attributes and sometimes even order of statements and assertions differ. But the test cases reuse testing logic and are needed for testing. They serve a purpose and cannot be eliminated. We present an approach that generates useful test clones automatically, thereby eliminating some of the \u201cgrunt\u201d work of testing. An important advantage over existing automated test case generators is that the clones include the test oracle. Hence, a human decision maker is often not needed to determine whether the output of a test is correct. The approach hinges on pairs of classes that provide analogous functionality, i.e., functions that are tested with the same logic. TestCloner\u00a0\u2026", "num_citations": "19\n", "authors": ["164"]}
{"title": "Xjava: Exploiting parallelism with object-oriented stream programming\n", "abstract": " This paper presents the XJava compiler for parallel programs. It exploits parallelism based on an object-oriented stream programming paradigm. XJava extends Java with new parallel constructs that do not expose programmers to low-level details of parallel programming on shared memory machines. Tasks define composable parallel activities, and new operators allow an easier expression of parallel patterns, such as pipelines, divide and conquer, or master/worker. We also present an automatic run-time mechanism that extends our previous work to automatically map tasks and parallel statements to threads.               We conducted several case studies with an open source desktop search application and a suite of benchmark programs. The results show that XJava reduces the opportunities to introduce synchronization errors. Compared to threaded Java, the amount of code could be reduced by up to 39\u00a0\u2026", "num_citations": "19\n", "authors": ["164"]}
{"title": "Impact of the research community for the field of software configuration management\n", "abstract": " Software configuration management (SCM) is an important discipline in professional software development and maintenance. The importance of SCM has increased as programs have become larger, more complex, and more mission/life-critical. This paper presents a brief summary of a full report that discusses the evolution of SCM technology from the early days of software development to present, and the specific impact university and industrial research has had along the way.", "num_citations": "18\n", "authors": ["164"]}
{"title": "E cient parallel computing on workstation clusters\n", "abstract": " We present novel hard-and software that e ciently implements communication primitives for parallel execution on workstation clusters. We provide low communication latencies, minimal protocol, zero operating system overhead, and high throughput. With this technology, it is possible to build e ective parallel systems using o-the-shelf workstations. Our goal is to develop a standard interfaceboard and the necessary software for interfacing any number of computers, from a workstation to a cabinet full of workstation-boards.", "num_citations": "18\n", "authors": ["164"]}
{"title": "Transferring research into the real world: How to improve RE with AI in the automotive industry\n", "abstract": " For specifications, people use natural language. We show that processing natural language and combining this with intelligent deduction and reasoning with ontologies can possibly replace some manual processes associated with requirements engineering (RE). Our prior research shows that the software tools we developed can indeed solve problems in the RE process. This paper shows this does not only work in the software engineering domain, but also for embedded software in the automotive industry. We use artificial intelligence in the sense of combining semantic knowledge from ontologies and natural language processing. This enables computer systems to \u201cunderstand\u201d requirement texts and process these with \u201ccommon sense\u201d. Our specification improver RESI detects flaws in texts such as ambiguous words, incomplete process words, and erroneous quantifiers and determiners.", "num_citations": "17\n", "authors": ["164"]}
{"title": "Empirical evaluation of semi-automated XML annotation of text documents with the GoldenGATE editor\n", "abstract": " Digitized scientific documents should be marked up according to domain-specific XML schemas, to make maximum use of their content. Such markup allows for advanced, semantics-based access to the document collection. Many NLP applications have been developed to support automated annotation. But NLP results often are not accurate enough; and manual corrections are indispensable. We therefore have developed the GoldenGATE editor, a tool that integrates NLP applications and assistance features for manual XML editing. Plain XML editors do not feature such a tight integration: Users have to create the markup manually or move the documents back and forth between the editor and (mostly command line) NLP tools. This paper features the first empirical evaluation of how users benefit from such a tight integration when creating semantically rich digital libraries. We have conducted experiments\u00a0\u2026", "num_citations": "17\n", "authors": ["164"]}
{"title": "nlrpBENCH: a benchmark for natural language requirements processing\n", "abstract": " We present nlrpBENCH: a new platform and framework to improve software engineering research as well as teaching with focus on requirements engineering during the software engineering process. It is available on http://nlrp.ipd. kit.edu. Recent advances in natural language processing have made it possible to process textual software requirements automatically, for example checking them for flaws or translating them into software artifacts. This development is particularly fortunate, as the majority of requirements is written in unrestricted natural language. However, many of the tools in in this young area of research have been evaluated only on limited sets of examples, because there is no accepted benchmark that could be used to assess and compare these tools. To improve comparability and thereby accelerate progress, we have begun to assemble nlrpBENCH, a collection of requirements specifications meant both as a challenge for tools and a yardstick for comparison. We have gathered over 50 requirement texts of varying length and difficulty and organized them in benchmark sets. At present, there are two task types: model extraction (e.g., generating UML models) and text correction (e.g., eliminating ambiguities). Each text is accompanied by the expected result and metrics for scoring results. This paper describes the composition of the benchmark and the sources. Due to the brevity of this paper, we omit example tools comparisons which are also available.", "num_citations": "16\n", "authors": ["164"]}
{"title": "Dynamic data race detection for correlated variables\n", "abstract": " In parallel programs concurrency bugs are often caused by unsynchronized accesses to shared memory locations, which are called data races. In order to support programmers in writing correct parallel programs, it is therefore highly desired to have tools on hand that automatically detect such data races. Today, most of these tools only consider unsynchronized read and write operations on a single memory location. Concurrency bugs that involve multiple accesses on a set of correlated variables may be completely missed. Tools may overwhelm programmers with data races on various memory locations, without noticing that the locations are correlated. In this paper, we propose a novel approach to data race detection that automatically infers sets of correlated variables and logical operations by analyzing data and control dependencies. For data race detection itself, we combine a modified version of the\u00a0\u2026", "num_citations": "16\n", "authors": ["164"]}
{"title": "Propagator: A family of patterns\n", "abstract": " Propagator is a family of patterns for consistently updating objects in a dependency network. The Propagator patterns are found in such diverse applications as MAKE, WWW, spreadsheets, GUIs, reactive control systems, simulation systems, distributed file systems, distributed databases, workflow systems and multilevel caches. There are four main patterns: strict propagator, strict propagator with failure, lazy propagator and adaptive propagator. In the strict propagation patterns, updates flow from the point of original change forward through the network. Dependent objects are immediately updated, unless failure occurs. In the lazy propagation pattern, an updated object merely sets its timestamp, without notifying any other object. Upon request, a dependent object makes itself current after requesting predecessor objects to bring themselves up-to-date. The adaptive propagator separates propagation of out-of-date\u00a0\u2026", "num_citations": "16\n", "authors": ["164"]}
{"title": "Parallel matrix multiplication on the connection machine\n", "abstract": " Matrix multiplication is a computation and communication intensive problem. Six parallel algorithms for matrix multiplication on the Connection Machine are presented and compared with respect to their performance and processor usage. For n by n matrices, the algorithms have theoretical running times of O(n2 log n), O(n log n), O(n), and O(log n), and require n, n2, n2, and n3 processors, respectively. With careful attention to communication patterns, the theoretically predicted runtimes can indeed be achieved in practice. The parallel algorithms illustrate the tradeoffs between performance, communication cost, and processor usage.", "num_citations": "16\n", "authors": ["164"]}
{"title": "Empirical methods in software engineering research\n", "abstract": " Over the past decade, empirical methods have gained acceptance for validating tools and methods in software research. This tutorial aims at acquainting participants with the main methods used in empirical work in software research, enabling them to evaluate empirical results for validity as well as providing the basis for carrying out empirical studies. A wide range of empirical approaches are covered, including case studies, experiments, field studies, and surveys. These approaches are introduced by prominent examples from the software engineering literature. Common pitfalls are pointed out. Participants also critique empirical papers in small discussion groups, based on worksheets with prepared questions.", "num_citations": "15\n", "authors": ["164"]}
{"title": "PULL: ParaStation user-level communication. Design and overview\n", "abstract": " PULL is a user-level communication library for workstation clusters. PULL provides a multi-user, multi-programming communication library for user level communication on top of high-speed communication hardware. In this paper, we describe the design of the communication subsystem, a first implementation on top of the ParaStation communication card, and benchmark results of this first implementation. PULL removes the operating system from the communication path and offers a multi-process environment with user-space communication. Additionally, we have moved some operating system functionality to the user level to provide higher efficiency and flexibility. Message demultiplexing, protocol processing, hardware interfacing, and mutual exclusion of critical sections are all implemented in user-level. PULL offers the programmer multiple interfaces including TCP user-level sockets, MPI [CGH94], PVM\u00a0\u2026", "num_citations": "15\n", "authors": ["164"]}
{"title": "Software Configuration Management Overview\n", "abstract": " Software configuration management (SCM) is the discipline of controlling the evolution of complex software systems. This chapter surveys tools that support or automate aspects of SCM. It proposes a standard terminology, describes the areas that are amenable to automation, discusses a representative set of existing SCM tools, and identifies directions for future research and development. A glossary of terms is included.", "num_citations": "15\n", "authors": ["164"]}
{"title": "Performance analysis of file replication schemes in distributed systems\n", "abstract": " In distributed systems the efficiency of the network file system is a key performance issue. Replication of files and directories can enhance file system efficiency, but the choice of replication techniques is crucial. This paper studies a number of replication techniques, including remote access, prereplication, weighted voting, and two demand replication schemes: polling and staling. It develops a Markov chain model, which is capable of characterizing properties of file access sequences, including access locality and access bias. The paper compares the replication techniques under three different network file system architectures. The results show that, under reasonable assumptions, demand replication requires fewer file transfers than remote access, especially for files that have a high degree of access locality. Among the demand replication schemes, staling requires fewer auxiliary messages than polling.", "num_citations": "15\n", "authors": ["164"]}
{"title": "Agile development: evaluation and experience\n", "abstract": " With the development of server virtualization, it is earning widespread respect by more and more professions, and it is a research hotspot which lead the technology into X86-structure server. This paper design and implement the deployment solution about DHCP, DNS, WINS Server virtualization based on active directory, and show the figures about how to realize it.", "num_citations": "14\n", "authors": ["164"]}
{"title": "Mapping Functions and Data Redistribution for Parallel Files.\n", "abstract": " A parallel file may be physically stored on several independent disks and logically partitioned by several processors. This paper presents general algorithms for mapping between two arbitrary distributions of a parallel file. Each of the two distributions may be physical or logical. The algorithms are optimized for multidimensional array partitions. We motivate our approach and present potential utilizations. We compare and contrast with related work. The paper also presents a study case, the employment of mapping functions and redistribution algorithms in a parallel file system.", "num_citations": "14\n", "authors": ["164"]}
{"title": "Do design patterns improve communication? An experiment with pair design\n", "abstract": " One of the main advantages claimed for software design patterns is improved team communication. This paper reports on an experiment that tests this hypothesis. Team communication among pairs of designers with and without design pattern knowledge is observed in a maintenance setting and analyzed by protocol analysis. The results indicate that shared pattern knowledge leads to a more condensed explanation phase and a balanced give-and-take among team members during design work. 1", "num_citations": "14\n", "authors": ["164"]}
{"title": "Universal programmability-how AI can help\n", "abstract": " Everyone should be able to program. Programming in informal, but precise natural language would enable anyone to program and help eliminate the world-wide software backlog. Highly trained software engineers would still be needed for complex and demanding applications, but not for routine programming tasks.Programming in natural language is a monumental challenge and will require AI and software researchers to join forces. Early results, however, appear promising. Combining natural language understanding and ontological reasoning helps remove defects from requirements statements, transforms requirements into UML models, and might even enable script-like programming in specific, narrow domains. An important precondition for rapid progress in this area are benchmarks that help compare different approaches and stimulate competition among researchers.", "num_citations": "13\n", "authors": ["164"]}
{"title": "High-level multicore programming with XJava\n", "abstract": " Multicore chips are becoming mainstream, but programming them is difficult because the prevalent thread-based programming model is error-prone and does not scale well. To address this problem, we designed XJava, an extension of Java that permits the direct expression of producer/consumer, pipeline, master/slave, and data parallelism. The central concept of the extension is the task, a parallel activity similar to a filter in Unix. Tasks can be combined with new operators to create arbitrary nestings of parallel activities. Preliminary experience with XJava and its compiler suggests that the extensions lead to code savings and reduce the potential for synchronization defects, while preserving the advantages of object-orientation and type-safety. The proposed extensions provide intuitive ldquowhat-you-see-is-what-you-getrdquo parallelism. They also enable other software tools, such as auto-tuning and accurate\u00a0\u2026", "num_citations": "13\n", "authors": ["164"]}
{"title": "Adding Autonomic Functionality to Object-Oriented Applications.\n", "abstract": " Integrating applications with autonomic, fiinctions such as checkpointing/restart, self-healing or self-updating is diflcult and time consuming [8]. We demonstrate tlmt autonomic, f~ tnctionality can he sepuruted, frotn upplications und supplied by default imple? nentations, thereby dramatically reducing the cost oj'scipplying autonomy. This article proposes a proxy/vvrapper technique with an additiorzul code Izook-lip infrustr~ lct~ ire to provide applic~ ztion adaptation with self-upduting, self-configuratior~ and self-optimizution, fiinctionalities.", "num_citations": "13\n", "authors": ["164"]}
{"title": "Dynamically adapting the degree of parallelism with reflexive programs\n", "abstract": " In this paper we present a new method for achieving a higher cost-efficiency on parallel computers. We insert routines into a program which detect the amount of computational work without using problem-specific parameters and adapt the number of used CPUs at runtime under given speedup/efficiency constraints. Several user-tunable strategies for selecting the number of processors are presented and compared. The modularity of this approach and its application-independence permit a general use on parallel computers with a scalable degree of parallelism.", "num_citations": "13\n", "authors": ["164"]}
{"title": "The ParaPC, ParaStation Project: Efficient Parallel Computing by Clustering Workstations\n", "abstract": " ParaStation is a communications fabric for connecting off-the-shelf workstations into a supercomputer. The fabric employs technology used in massively parallel machines and scales up to 4096 nodes. The message passing software preserves the low latency of the fabric by taking the operating system out of the communication path, while still providing full protection.", "num_citations": "13\n", "authors": ["164"]}
{"title": "Self-tuning parallelism\n", "abstract": " Assigning additional processors to a parallel application may slow it down or lead to poor computer utilization. This paper demonstrates that it is possible for an application to automatically choose its own, optimal degree of parallelism. The technique is based on a simple binary search procedure for finding the optimal number of processors, subject to one of the following criteria:                                             maximum speed,                                                                 maximum benefit-cost ratio, or                                                                 maintaining an efficiency threshold                                         The technique has been implemented and evaluated on a Cray T3E with 512 processors using both kernels and real applications from Mathematics, Electrical Engineering, and Geophysics. In all tests, the optimal parallelism is found quickly. The technique can be used to determine the optimal degree of parallelism without manual\u00a0\u2026", "num_citations": "12\n", "authors": ["164"]}
{"title": "PSPVM: Implementing PVM on a high-speed Interconnect for Workstation Clusters\n", "abstract": " PSPVM in an implementation of the PVM package on top of ParaStations high-speed interconnent for workstation clusters. The ParaStation system uses user level communication for message exchange and removes the operating system from the critical path of message transmission. ParaStations user interface consists of a user-level socket emulation. Thus, we need only minor changes to the standard PVM package to get it running on the ParaStation system.             Throughput of the PSPVM is increased eight times and latency is reduced by a factor of four compared to regular PVM. The remaining latency is mainly (88%) caused by the PVM package itself. The underlying sockets are so fast (25\u03bcs) that the PVM package is the limiting factor. PSPVM offers nearly the raw performance of the network to the user and is object-code compatible to regular PVM. As a consequence, we achieve an application speed\u00a0\u2026", "num_citations": "12\n", "authors": ["164"]}
{"title": "Software change dynamics or half of all Ada compilations are redundant\n", "abstract": " This paper is an empirical study of the evolution of a medium-size, industrial software system written in Ada. Parameters surveyed include various size attributes, the number and distribution of changes, and compilation costs. The interesting aspect of this study is that a day-to-day, complete development history of the system was available, spanning three years. The history permitted a full trace of all day-to-day changes. Findings included:                                    \u2022 A large number of compilations performed by the Ada compiler are redundant. A simple mechanism that compares compiler output could save about one fourth of all compilations; a detailed dependency analysis could save one half.                                                     \u2022 Whenever package specifications change, almost a third of all compilation units must be recompiled, even though 80 percent of the declarations in the updated specifications are unmodified\u00a0\u2026", "num_citations": "12\n", "authors": ["164"]}
{"title": "STORK: an experimental migrating file system for computer networks\n", "abstract": " STORK is an experimental file system designed for local and long-haul networks. It ensures that each user has a single view of his flIes, independent of the network node where he works, and independent of the location of the files. STORK files have no fixed location; instead they migrate to the network node where they are needed. File consistency is ensured by permitting only one current copy of each file to exist in the net at any given time. A.. lock mechanism is provided for controlling concurrent access.", "num_citations": "12\n", "authors": ["164"]}
{"title": "Online-autotuning in the presence of algorithmic choice\n", "abstract": " In this paper we explore the problem of autotuning the choice of algorithm. For a given task, there may be multiple algorithms available, each of which may contain its own set of tunable parameters and may provide optimal performance under different sets of inputs. Algorithmic choice is a type of tuning parameter which has not been well studied in the history of autotuning. To close this gap, we examine established autotuning techniques with regard to their ability of handling these parameters. We discuss the inadequacy of the state-of-the-art autotuning toolbox in manipulating algorithmic choice parameters and introduce four strategies to tackle this task. We evaluate our strategies in two case studies of online-autotuning scenarios, both with and without additional, numeric tuning parameters. The strategies are able to determine the optimal algorithm, and can even interoperate with the autotuning of the additional\u00a0\u2026", "num_citations": "11\n", "authors": ["164"]}
{"title": "Patty: A pattern-based parallelization tool for the multicore age\n", "abstract": " The free lunch of ever increasing clock frequencies is over. Performance-critical sequential software must be parallelized, and this is tedious, hard, buggy, knowledge-intensive, and time-consuming. In order to assist software engineers appropriately, parallelization tools need to consider detection, transformation, correctness, and performance all together.", "num_citations": "11\n", "authors": ["164"]}
{"title": "A language-based tuning mechanism for task and pipeline parallelism\n", "abstract": " Current multicore computers differ in many hardware aspects. Tuning parallel applications is indispensable to achieve best performance on a particular hardware platform. Auto-tuners represent a promising approach to systematically optimize a program\u2019s tuning parameters, such as the number of threads, the size of data partitions, or the number of pipeline stages. However, auto-tuners require several tuning runs to find optimal values for all parameters. In addition, a program optimized for execution on one machine usually has to be re-tuned on other machines.               Our approach tackles this problem by introducing a language-based tuning mechanism. The key idea is the inference of essential tuning parameters from high-level parallel language constructs. Instead of identifying and adjusting tuning parameters manually, we exploit the compiler\u2019s context knowledge about the program\u2019s parallel structure\u00a0\u2026", "num_citations": "11\n", "authors": ["164"]}
{"title": "Schlanke Produktionsweisen in der modernen Softwareentwicklung\n", "abstract": " Kernpunkte               Der Beitrag zeigt am Beispiel von Extreme Programming (XP), dass agile Softwareentwicklungsmethoden auf den Ideen der Lean Production beruhen.                                        XP erreicht eine extreme Qualit\u00e4tssicherung durch Paarprogrammierung, testgetriebenes Entwickeln, h\u00e4ufiges Refaktorisieren und laufende Codeintegration.                                                           XP arbeitet mit extremer Kundenorientierung durch st\u00e4ndige Einbindung eines Kundenmitarbeiters, iteratives Planungsspiel, h\u00e4ufige Akzeptanztests und inkrementelle Auslieferung.                                                           XP erreicht eine extreme Verschlankung des Entwicklungsprozesses durch das konsequente Weglassen von Entwurf, Dokumentation und aufwendigen Koordinierungsmechanismen.                                                    Die Verankerung in der Lean Production liefert eine konzeptionelle Erkl\u00e4rung, warum agile Methoden\u00a0\u2026", "num_citations": "11\n", "authors": ["164"]}
{"title": "The TILDE project\n", "abstract": " The TILDE project investigates general-purpose computing systems that run on a multi\u00b7 machine computing engine. A multi-machine computing engine is a cluster of heterogeneous processors loosely coupled with a high-speed local area network. The goal of the project is to explore computing systems in which the user interlace hides details of the underlying architecture, making the distributed computing engine appear to be a single, large time-sharing system. This paper presents prqject plans and status. It describes the architecture of the TILDE computing engine as well as research in the areas of operating systems. user interfaces, and computational services. The novel computational services planned for TILDE include high-level electronic mail, a timed event service, satellite bulletin\u00b7 board broadcast, supercomputer access, and otbers.\" fbi. projce! II IUpPOrted io pur by 81'101. froID the NltiOllIl! Science\u00a0\u2026", "num_citations": "11\n", "authors": ["164"]}
{"title": "Unums 2.0: An interview with john l. gustafson\n", "abstract": " In an earlier interview (April 2016), Ubiquity spoke with John Gustafson about the unum, a new format for floating point numbers. The unique property of unums is that they always know how many digits of accuracy they have. Now Gustafson has come up with yet another format that, like the unum 1.0, always knows how accurate it is. But it also allows an almost arbitrary mapping of bit patterns to the reals. In doing so, it paves the way for custom number systems that squeeze the maximum accuracy out of a given number of bits. This new format could have prime applications in deep learning, big data, and exascale computing.", "num_citations": "10\n", "authors": ["164"]}
{"title": "Software-Konfigurationsmanagement: Wie, wann, was warum\n", "abstract": " Software Konfigurations- Management Page 1 1 Tichy/Software Configuration Management \u00a9 Tichy 1999 Software KonfigurationsManagement Walter F. Tichy Universit\u00e4t Karlsruhe Page 2 Tichy/Software Configuration Management 2 \u00a9 Tichy 1999 Definition Software Konfigurations-Management (SKM) ist die Disziplin zur Verfolgung und Steuerung der Evolution von Software. SKM ist unerl\u00e4\u00dflich f\u00fcr die Entwicklung und Pflege gro\u00dfer, langlebiger Softwaresysteme. Page 3 Tichy/Software Configuration Management 3 \u00a9 Tichy 1999 SKM: Software-Evolution im Griff \u2022 Software \u00e4ndert sich andauernd. \u2022 \u00c4nderungen verursachen Chaos. \u2022 SKM hilft, das \u00c4nderungschaos zu vermeiden: \u2212 Macht die Entwicklungsgeschichte r\u00fcckverfolgbar, \u2212 Verhindert oder gleicht konkurrierende \u00c4nderungen ab. \u2212 Verfolgt Konfigurationen und Freigaben, \u2212 Verfolgt Fehlermeldungen und Korrekturen, \u2212 Automatisiert Versionsselektion und , \u2026", "num_citations": "10\n", "authors": ["164"]}
{"title": "DeNom: a tool to find problematic nominalizations using NLP\n", "abstract": " Nominalizations in natural language requirements specifications can lead to imprecision. For example, in the phrase \"transportation of pallets\" it is unclear who transports the pallets from where to where and how. Guidelines for requirements specifications therefore recommend avoiding nominalizations. However, not all nominalizations are problematic. We present an industrial-strength text analysis tool called DeNom, which detects problematic nominalizations and reports them to the user for reformulation. DeNom uses Stanford's parser and the Cyc ontology. It classifies nominalizations as problematic or acceptable by first detecting all nominalizations in the specification and then subtracting those which are sufficiently specified within the sentence through word references, attributes, nominal phrase constructions, etc. All remaining nominalizations are incompletely specified, and are therefore prone to conceal\u00a0\u2026", "num_citations": "9\n", "authors": ["164"]}
{"title": "An interview with Prof. Andreas Zeller: Mining your way to software reliability\n", "abstract": " In 1976, Les Belady and Manny Lehman published the first empirical growth study of a large software system, IBMs OS 360. At the time, the operating system was twelve years old and the authors were able to study 21 successive releases of the software. By looking at variables such as number of modules, time for preparing releases, and modules handled between releases (a defect indicator), they were able to formulate three laws: The law of continuing change, the law of increasing entropy, and the law of statistically smooth growth. These laws are valid to this day. Belady and Lehman were ahead of their time. They understood that empirical studies such as theirs might lead to a deeper understanding of software development processes, which might in turn lead to better control of software cost and quality. However, studying large software systems proved difficult, because complete records were rare and\u00a0\u2026", "num_citations": "9\n", "authors": ["164"]}
{"title": "On the design and performance of kernel-level TCP connection endpoint migration in cluster-based servers\n", "abstract": " The TCP connection endpoint migration allows arbitrary server-side connection endpoint assignments to server nodes in cluster-based servers. The mechanism is client-transparent and supports back-end level request dispatching. It has been implemented in the Linux kernel and can be used as part of a policy-based software architecture for request distribution. We show that the TCP connection end-point migration can be successfully used for request distribution in cluster-based Web servers, both for persistent and non-persistent HTTP connections. We present locality-aware policies using TCP connection migration that outperform Round Robin by factors as high as 2.79 in terms of the average response time for certain classes of requests.", "num_citations": "9\n", "authors": ["164"]}
{"title": "What do programmers of parallel machines need? a survey\n", "abstract": " We performed semistructured, open-ended interviews with 11 professional developers of parallel, scientific applications to determine how their programming time is spent and where tools could improve productivity. The subjects were selected from a variety of research laboratories, both industrial and governmental. The major findings were that programmers would prefer a global over a per-processor view of data structures, struggle with load balancing and optimizations, and need interactive tools for observing the behavior of parallel programs. Furthermore, handling and processing massive amounts of data in parallel is emerging as a new challenge.", "num_citations": "9\n", "authors": ["164"]}
{"title": "On the Design and Semantics of User-Space Communication Subsystems.\n", "abstract": " The problem with Gbit/s networks is to get the hardware performance into the applications. The most promising technique is a zero-copy protocol combined with a user-space communication subsystem that (a) gives the application direct access to the network interface and (b) avoids all bu ering/copying.In this paper we examine the design space of user-space communication subsystems, especially how send and receive operations work and which communication semantics they imply. Furthermore, we propose a technique called page-exchange which avoids copy operations, is well de ned, and has communication semantics equivalent to those of standard programming interfaces such as UNIX sockets, PVM, or MPI.", "num_citations": "9\n", "authors": ["164"]}
{"title": "Is quantum computing for real? an interview with Catherine McGeoch of D-Wave Systems\n", "abstract": " In this interview, computer scientist Catherine McGeoch demystifies quantum computing and introduces us to a new world of computational thinking.", "num_citations": "8\n", "authors": ["164"]}
{"title": "Online-autotuning of parallel SAH kD-trees\n", "abstract": " We explore the benefits of using online-autotuning to find an optimal configuration for the parallel construction of Surface Area Heuristic (SAH) kD-trees. Using a quickly converging autotuning mechanism, we achieve a significant performance improvement of up to 1.96x. The SAH kD-tree is a spatial data structure and a fundamental tool in the domain of computer graphics and simulations. The parallel construction of these trees is influenced by several parameters, controlling various aspects of the algorithm. However, the parameter configurations advocated in the literature are hardly ever portable. To boost portability, we apply online-autotuning to four state-of-the-art variants of parallel kD-tree construction. We show that speedups over the variants' standard configurations are possible with low programmer effort. We further demonstrate the performance portability of our approach by evaluating performance on\u00a0\u2026", "num_citations": "8\n", "authors": ["164"]}
{"title": "The end of (numeric) error: An interview with John L. Gustafson\n", "abstract": " Crunching numbers was the prime task of early computers. The common element of these early computers is they all used integer arithmetic. John Gustafson, one of the foremost experts in scientific computing, has proposed a new number format that provides more accurate answers than standard floats, yet saves space and energy. The new format might well revolutionize the way we do numerical calculations.", "num_citations": "8\n", "authors": ["164"]}
{"title": "Empirical software research: an interview with Dag Sj\u00f8berg, University of Oslo, Norway\n", "abstract": " Punched cards were already obsolete when I began my studies at the Technical University of Munich in 1971. Instead, we had the luxury of an interactive, line-oriented editor for typing our programs. Doug Engelbart had already invented the mouse, but the device was not yet available. With line editors, users had to identify lines by numbers and type in awkward substitution commands just to add missing semicolons. Though cumbersome by today\u2019s standards, it was obvious that line-oriented editors were far better than punched cards. Not long after, screen oriented editors such as Vi and Emacs appeared. Again, these editors were obvious improvements and everybody quickly made the switch. No detailed usability studies were needed.\u201cTry it and you\u2019ll like it\u201d was enough.(Brian Reid at CMU likened screen editors to handing out free cocaine in the schoolyard.) Switching from Assembler to Fortran, Algol, or Pascal\u00a0\u2026", "num_citations": "8\n", "authors": ["164"]}
{"title": "Creating software models with semantic annotation\n", "abstract": " Requirements engineering is a big part of software engineering and consumes a lot of time. We propose a novel approach of automatically creating software domain models from textual requirements specifications using semantic annotation. Natural language processing (NLP) has progressed much in the last years and the usage of NLP tools for automatic annotation shows promising results. We use Fillmore's thematic roles to explicitly denote the semantic relations in a sentence.", "num_citations": "8\n", "authors": ["164"]}
{"title": "Request distribution-aware caching in cluster-based web servers\n", "abstract": " This work presents a performance analysis of request distribution-aware caching in cluster-based Web servers. We use the Zipf-like request distribution curve to guide static Web document caching. A combination of cooperative caching and exclusive caching provides for a cluster-wide caching system that avoids document replication accross the cluster. We explore the benefits of cooperative caching algorithms that use request distribution information to steer their behavior over general purpose cooperative caching algorithms. Exclusive caching exercises a fine-grained control over replication of data blocks across the cluster. The performance of the system has been assessed by using the WebStone benchmark. Our cluster-based server employs Linux kernel-level implementations of cooperative caching and exclusive caching. Current results show that request distribution-aware caching outperforms general\u00a0\u2026", "num_citations": "8\n", "authors": ["164"]}
{"title": "Semantic software engineering approaches for automatic service lookup and integration\n", "abstract": " Using Web services today has two major drawbacks: firstly, a programmer has to guess the appropriate service operations by interpreting syntactic operation names provided by WSDL descriptions, and secondly, the decision which services to use is fixed at design time. Using ontological descriptions, we automate the lookup of services. The lookup occurs at runtime, supporting location aware selection and the choice of more relevant or alternate services. We use Semantic Web techniques such as DAML-OIL to avoid the exact match between services invocations and make both service lookup and invocation independent of the strict syntactic Web services description.", "num_citations": "8\n", "authors": ["164"]}
{"title": "Prefetching on the Cray-T3E\n", "abstract": " In many parallel applications, network latency causes a dramatic loss in processor utilization. This paper examines software controlled access pipelining(SCAP) as a technique for hiding network latency. An analytic model of SCAP describes basic operation techniques and predicts performance. Results are validated with benchmarks on the Cray-T3E. They show vectorized version of SCAP (V-SCAP) to be at least as fast as the highly optimized shared memory system functions. SCAP on the Cray-T3E improves performance compared to a blocking execution between 35% and 900%, while V-SCAP performs better with a factor of 2.1 to 62. SCAP achieves a performance speed-up against HPF between 48% to a factor of 9.2 dependent on the data access pattern. It also performs well on irregular access patterns which are not, supported by the standard library.", "num_citations": "8\n", "authors": ["164"]}
{"title": "Future directions in software engineering, ACM SIGSOFT\n", "abstract": " CiNii \u8ad6\u6587 - Future directions in software engineering, ACM SIGSOFT CiNii \u56fd\u7acb\u60c5\u5831\u5b66 \u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Future directions in software engineering, ACM SIGSOFT TICHY WF \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 TICHY WF \u53ce\u9332\u520a\u884c\u7269 Software Engineering Notes Software Engineering Notes 18(1), 35-48, 1993 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 GQM\u30d1\u30e9\u30c0\u30a4\u30e0\u3092\u7528\u3044\u305f\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30e1\u30c8\u30ea\u30af\u30b9\u306e\u6d3b\u7528 \u6960\u672c \u771f\u4e8c , \u80a5\u5f8c \u82b3\u6a39 \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2 29(3), 29-38, 2012-07-25 \u53c2\u8003\u6587\u732e17\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10030497684 \u8cc7\u6599\u7a2e\u5225 \u96d1\u8a8c\u8ad6\u6587 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 Refer/BiblX| \u2026", "num_citations": "8\n", "authors": ["164"]}
{"title": "From Modula-2* to efficient parallel code\n", "abstract": " Initial evidence is presented that machine-independent, explicitly parallel programs can be translated automatically into machine-dependent, parallel code that is competitive in performance with handwritten code.The programming language used is Modula-2*, an extension of Modula-2, which incorporates both data and control parallelism in a portable fashion. An optimizing compiler targeting MIMD, SIMD, and SISD machines translates Modula-2* into machine-dependent C code. The performance of the resulting code is compared to the performance of equivalent, carefully hand-coded and optimized programs in MPL, a low-level parallel programming language for the MasPar MP-1 (a SIMD machine with 16K processors).", "num_citations": "8\n", "authors": ["164"]}
{"title": "Modula-2*: An extension of Modula-2 for highly parallel, portable programs\n", "abstract": " CiteSeerX \u2014 Modula-2*: an extension of Modula-2 for highly parallel, portable programs Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA Modula-2*: an extension of Modula-2 for highly parallel, portable programs (1990) Cached Download as a PDF Download Links [ntrs.nasa.gov] Save to List Add to Collection Correct Errors Monitor Changes by Walter F. Tichy , Christian G. Herter , Walter F. Tichy , Christian G. Tlerter , Walter F. Tichy , Christian G. Herter Citations: 14 - 4 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract c3/61 Keyphrases portable program Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact \u2026", "num_citations": "8\n", "authors": ["164"]}
{"title": "An Introduction to the Revision Control System\n", "abstract": " The Revision Control System (RCS) manages software libraries. It greatly increases programmer productivity by centralizing and cataloging changes to a software project. This document describes the benefits of using a source code control system. It then gives a tutorial introduction to the use of RCS.", "num_citations": "8\n", "authors": ["164"]}
{"title": "The Modula-2* environment for parallel programming\n", "abstract": " Presents a portable parallel programming environment for Modula-2*, an explicitly parallel machine-independent extension of Modula-2. Modula-2* offers synchronous and asynchronous parallelism, a global single address space, and automatic data and process distribution. The Modula-2* system consists of a compiler, a debugger, a cross-architecture make, graphical X Windows control panel, run-time systems for different machines, and sets of scalable parallel libraries. The existing implementation targets the MasPar MP series of massively parallel processors (SIMD), the KSR-1 parallel computer (MIMD), heterogeneous LANs of workstations (MIMD), and single workstations (SISD). We describe the important components of the Modula-2* environment, and discuss selected implementation issues. We focus on how we achieve a high degree of portability for our system, while at the same time ensuring efficiency.< >", "num_citations": "7\n", "authors": ["164"]}
{"title": "AT&T Laboratories\n", "abstract": " Delta algorithms compress data by encoding one file in terms of another. This type of compression is useful in a number of situations: storing multiple versions of data, displaying differences, merging changes, distributing updates, storing backups, transmitting video sequences, and others. This article studies the performance parameters of several delta algorithms, using a benchmark of over 1,300 pairs of files taken from two successive releases of GNU software. Results indicate that modern delta compression algorithms based on Ziv-Lempel techniques significantly outperform diff, a popular but older delta compressor, in terms of compression ratio. The modern compressors also correlate better with the actual difference between files without sacrificing performance. Categories and Subject Descriptors: D. 2.7 [Software Engineering]: Distribution and Maintenance\u2014version control; D. 2.8 [Software Engineering]: Metrics\u2014performance measures;", "num_citations": "7\n", "authors": ["164"]}
{"title": "Synchronizing domain models with natural language specifications\n", "abstract": " Textual specifications and domain models change during development and need to be kept consistent. However, in practice the cost of maintaining consistency is too high. Stakeholders need to be informed about model changes in natural language, software architects need to see the impact of specification changes on their models. Our Requirements Engineering Feedback System (REFS) automates the process of keeping specification and models consistent when the models change. Also, it can assess the impact of specification changes.", "num_citations": "6\n", "authors": ["164"]}
{"title": "Parallelizing an index generator for desktop search\n", "abstract": " Experience with the parallelization of an index generator for desktop search is presented. Several configurations of the index generator are compared on three different Intel platforms with 4, 8, and 32 cores. The optimal configurations for these platforms are not intuitive and are markedly different for the three platforms. For finding the optimal configuration, detailed measurements and experimentation were necessary. Several recommendations for parallel software design are derived from this study.", "num_citations": "6\n", "authors": ["164"]}
{"title": "Parallelism in curricula an international survey\n", "abstract": " The intention of this survey is to provide an overview of how parallelism and multicore/manycore topics are addressed in current curricula. Web sites of international universities were used as a source of information. The survey lists courses with titles, instructors,", "num_citations": "6\n", "authors": ["164"]}
{"title": "Empirische Methodik in der Softwaretechnik im Allgemeinen und bei der Software-Visualisierung im Besonderen\n", "abstract": " Empirische Untersuchungen sind ein fundamentaler Bestandteil der Forschung in der Softwaretechnik geworden. Software-Entwicklungsmethoden und Werkzeuge werden empirisch untersucht, um sie zu verstehen, zu evaluieren, einzusetzen und zu verbessern. Dieser Artikel erl\u00e4utert einige der wichtigsten empirischen Methoden, die in der Softwaretechnik und insbesondere bei der Visualisierung von Software einsetzbar sind (Korrelationsstudien, Fallstudien, Umfragen, Ethnografie, Metastudien, kontrollierte Experimente) und geht auf einige wichtige Aspekte wie Studenten versus professionelle Softwareentwickler sowie die Technik des Akzeptanztests ein.", "num_citations": "6\n", "authors": ["164"]}
{"title": "CARDS: cluster-aware remote disks\n", "abstract": " This paper presents Cluster-Aware Remote Disks (CARDs), a Single System I/O architecture for cluster computing. CARDs virtualize accesses to remote cluster disks over a System Area Network. Their operation is driven by cooperative caching policies that implement a joint management of the cluster caches. All the CARDS of a given disk employ a common policy, independently of other CARD sets. CARD drivers have been implemented as Linux kernel modules which can flexibly accommodate various cooperative caching algorithms. We designed and implemented a decentralized policy called Home-based Serverless Cooperative Caching (HSCC). HSCC showed cache hit ratios over 50% for workloads that go beyond the limit of the global cache. The best speedup of a CARD over a remote disk interface was 1.54.", "num_citations": "6\n", "authors": ["164"]}
{"title": "PSPVM2: PVM for ParaStation\n", "abstract": " This document describes the concept, implementation and performance of PSPVM2, the second port of the Parallel Virtual Machine (PVM, 2]) to the ParaStation system1. It is derived from the original PVM source code to retain compatibility with existing PVM applications and implementations, but uses a low level interface to the ParaStation system and direct communication within the ParaStation cluster. PSPVM is the fastest PVM implementation on a workstation cluster with a process-to-process latency as low as 11.5 s2 and a sustained transfer rate of up to 9.4 MB/s. It can still be used as part of a larger virtual machine and all PVM programs can be compiled for PSPVM without any changes.", "num_citations": "6\n", "authors": ["164"]}
{"title": "Hochgradiger Parallelismus\n", "abstract": " Highly parallel computers with thousands of processing elements have been commercially available for several years and are now being used in numerous applications. At the same time, Computer Science is shifting its attention towards studying the priniciples of parallel computation more intensively than before.This article surveys the quickly developing field of parallelism. A classification of the main architectural principles of parallel computers is given. The state-of-the-art is analyzed by comparing arithmetic performance, communications performance, and price/performance ratios of four commercially available, parallel computers. We note that arithmetic and communications performance continue to be unbalanced, preventing MIMD machines from being used effectively in most fine-grained parallel applications.", "num_citations": "6\n", "authors": ["164"]}
{"title": "Initial implementation of natural language turn-based dialog system\n", "abstract": " Our prototype implements a natural language dialog system for Excel spreadsheets. The work is motivated by a pilot study which shows that novice users have difficulties with the formula language of Excel and need interactive assistance. JustLingo1 allows spreadsheet calculations in ordinary, written English. Furthermore, users can extend the system functionality by explaining of the calculations to the prototype. The keyword-based system analyzes the input using natural language processing techniques and translates it into Excel formulas. It asks for missing information such as operands and target cells and provides alternatives if there are ambiguities. It also handles references to previous inputs, allowing step-by-step construction of calculations. An evaluation shows that it properly resolves 82% of references and correctly interprets 79,5% of overall user input. Although far from perfect, the prototype\u00a0\u2026", "num_citations": "5\n", "authors": ["164"]}
{"title": "Parallelizing a Real-Time Audio Application--A Case Study in Multithreaded Software Engineering\n", "abstract": " Multicore hardware is ubiquitous, but billions of lines of code in performance-critical commodity software are still sequential. Although parallel libraries, design patterns, and best practice guidelines are available, thinking parallel is still a big challenge for many software engineers. In this paper we present a case study on parallelizing commodity software using a commercial real-time audio application with over 700,000 lines of code. In contrast to best practice guidelines, our goal is to investigate what parallelization strategy can effectively be used in data stream-intensive applications. Performing an in-depth analysis of the software architecture and its run-time performance, we locate parallelization potential and propose three different parallelization strategies. We evaluate them with respect to their parallel performance impact. Regarding the application's intrinsic real-time requirement and a very short audio cycle\u00a0\u2026", "num_citations": "5\n", "authors": ["164"]}
{"title": "Auto-tuning parallel software: An interview with thomas fahringer: the multicore transformation (ubiquity symposium)\n", "abstract": " In this interview conducted by Ubiquity editor Walter Tichy, Prof. Thomas Fahringer of the Institute of Computer Science, University of Innsbruck (Austria) discusses the difficulty in predicting the performance of parallel programs, and the subsequent popularity of auto-tuning to automate program optimization.", "num_citations": "5\n", "authors": ["164"]}
{"title": "Detecting correlation violations and data races by inferring non-deterministic reads\n", "abstract": " With the introduction of multicore systems and parallel programs concurrency bugs have become more common. A notorious class of these bugs are data races that violate correlations between variables. This happens, for example, when the programmer does not update correlated variables atomically, which is needed to maintain their semantic relationship. The detection of such races is challenging because correlations among variables usually escape traditional race detectors which are oblivious of semantic relationships. In this paper, we present an effective method for dynamically identifying correlated variables together with a race detector based on the notion of non-deterministic reads that identifies malicious data races on correlated variables. In eight programs and 190 micro benchmarks, we found more than 100 races that were overlooked by other race detectors. Furthermore, we identified about 300\u00a0\u2026", "num_citations": "5\n", "authors": ["164"]}
{"title": "An evaluation of data race detectors using bug repositories\n", "abstract": " Multithreaded software is subject to data races. A large number of data race detectors exists, but they are mainly evaluated in academic examples. In this paper we present a study in which we applied data race detectors to real applications. In particular, we want to show, if these tools can be used to locate data races effectively at an early stage in software development. We therefore tracked 25 data races in bug repositories back to their roots, created parallel unit tests and executed 4 different data race detectors on these tests. We show, that with a combination of all detectors 92% of the contained data races can be found, whereas the best data race detector only finds about 50%.", "num_citations": "5\n", "authors": ["164"]}
{"title": "Parallelizing bzip2\n", "abstract": " This paper presents a case study on parallelizing the sequential version of the BZip2 compression program for usage on multicore computers. We describe the encountered software engineering problems, discuss the tradeoffs of different parallelization strategies, and present empirical performance results.The study was conducted during the last three weeks of a multicore software engineering course. Eight students, working in teams of two, were assigned the task to parallelize BZip2 in a team competition. Before starting with BZip2, all students had three months of extensive training in parallelization with POSIX threads and OpenMP, as well as knowledge of profiling strategies and tools. Our empirical findings show that considerable speedups can be gained by exploiting parallelism on higher abstraction levels through parallel patterns, which are more significant than speedups obtained from a fine-granular parallelization. Another key issue we identify is the systematic refactoring of existing sequential code to prepare it for parallelization; the time needed for such refactorings can be significantly longer than for the actual insertion of parallelization constructs. The team who mastered these tasks well won the contest with a speedup above 10 on an eight-core SUN Niagara T1, while the weakest team produced a parallel version that was even slower than the sequential one.", "num_citations": "5\n", "authors": ["164"]}
{"title": "Zhi#: Programming language inherent support for XML Schema Definition\n", "abstract": " Recently, XML Schema Definition (XSD) has become ever more important when it comes to define programming language independent content models and type systems. However, writing software that operates on XML instance documents still suffers from a lack of compile time support for XSD. Especially, obeying facet based constraints imposed on XSD simple data types is still error prone and laborious. This paper introduces the concept of XSD aware compilation. Zhi# provides static- and dynamic type checking for XML simple data types, which can be used along with off-the-shelf implementations of the W3C XML DOM.", "num_citations": "5\n", "authors": ["164"]}
{"title": "On the Design and Performance of Remote Disk Drivers for Clusters of PCs.\n", "abstract": " This paper presents the design and performance of remote disk drivers for clusters of Commodity-Off-The-Shelf PCs that fetch disk blocks over System Area Networks. The driver offers a flexible interface, being capable to logically act either as computer-or network-attached storage. It allows for fine-grain remote cache control through exclusive caching. An event-driven asynchronous block delivery mode of operation helps making the most out of the available parallelism by overlapping request processing with block delivery at both involved nodes and thus yielding better performance than local disks. The driver has been implemented as a Linux kernel module.", "num_citations": "5\n", "authors": ["164"]}
{"title": "A reliable transmission protocol for myrinet\n", "abstract": " This work presents a low-level communication protocol for Myrinet, which o ers reliable data transmission at network interface level. The protocol is used within the ParaStation2 system, a high-performance cluster for parallel computing. Although most projects using Myrinet assume the hardware to be reliable, there is strong evidence that this assumption does not hold and reliable data transmission has to be ensured using an appropriate protocol. ParaStation2 exploits Myrinet's programmable network interface (NI) to implement link level ow control based on an ACK/NACK mechanism with timeout and retransmission.", "num_citations": "5\n", "authors": ["164"]}
{"title": "Design and evaluation of parastation2\n", "abstract": " ParaStation is a communications fabric to connect off-the-shelf workstations into a supercomputer. This paper presents ParaStation2, an adaption of the ParaStation system (which was build on top of our own hardware) to the Myrinet hardware. The main focus lies on the design and implementation of ParaStation2\u2019s flow control protocol to ensure reliable data transmission at network interface level, which is different to most other projects using Myrinet.             One-way latency is 14.5\u00b5s to 18\u00b5s (depending on the hardware platform) and throughput is 50 MByte/s to 65 MByte/s, which compares well to other approaches. At application level, we were able to achieve a performance of 5.3 GFLOP running a matrix multiplication on 8 DEC Alpha machines (21164A, 500 MHz).", "num_citations": "5\n", "authors": ["164"]}
{"title": "Latency hiding in parallel systems: A quantitative approach\n", "abstract": " In many parallel applications, network latency causes a dramatic loss in processor utilization. This paper examines software pipelining as a technique for network latency hiding. It quanti es the potential improvements with detailed, instruction-level simulations. The benchmarks used are the Livermore Loop kernels and BLAS Level 1. These were parallelized and run on the instruction-level RISC simulator DLX, extended with both a blocking and a pipelined network. Our results show that prefetch in a pipelined network improves performance by a factor of 2 to 9, provided the network has su cient bandwidth to accept at least 10 requests per processor.", "num_citations": "5\n", "authors": ["164"]}
{"title": "Persistence for arbitrary C++ data structures\n", "abstract": " This paper presents an external representation that may be automatically generated from arbitrary C++ data structures. This representation can be used to achieve persistence in various applications. We describe the existing facilities for persistence in the EDGE graph editor. A similar technique can be used to achieve persistence in general. In addition to generating code for an external representation, a menu-controlled data structure editor for arbitrary C++ classes may be generated using the same technique.", "num_citations": "5\n", "authors": ["164"]}
{"title": "A knowledge-based graphical editor\n", "abstract": " A knowledge-based graphical editor - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/148042 Title : A knowledge-based graphical editor Authors : Tichy, WF (Carnegie Group, Inc., Pittsburgh, PA (United States) ; Karlsruhe Univ. (TH) (Germany). Inst. fuer Informatik 2) ; Ward, B. (Carnegie Group, Inc., Pittsburgh, PA (United States) ; Carnegie-Mellon Univ., Pittsburgh, PA (United States)) ; Corporate author : Karlsruhe Univ. (TH) (Germany). Fakultaet fuer Informatik ; Publication year : 1987 Language : English ; Pagination/Size : 25 p. ; SIGLE classification : 09H - Computer software, programming ; Document type : I - Miscellaneous ; Other identifier : DE_ 1988:3740 ; DE ; handle : http://hdl.handle.net/10068/148042 Provenance : SIGLE ; Get a copy : FIZ - Karlsruhe - : /\u2026", "num_citations": "5\n", "authors": ["164"]}
{"title": "Adabase: a data base for Ada programs\n", "abstract": " A central part of any programming support environment is the data base that stores all project information. This paper presents the design of Adabase, a data base that manages Ada program families. Program families consist of multiple versions and configurations, and family members share a significant number of common parts.", "num_citations": "5\n", "authors": ["164"]}
{"title": "Programming in natural language building algorithms from human descriptions\n", "abstract": " Our work is where the Software Engineering meets the Human Computer Interaction and the End User Programming to aim for a major breakthrough by making machines programmable in ordinary and unrestricted language. In this paper, we provide a solution on how new algorithms can be recognized and learned from human descriptions. Our focus is to improve the interaction between humans and machines and enable the end user to instruct programmable devices, without having to learn a programming language. In a test-driven development, we created a platform that allows users to manipulate spreadsheet data by using natural language. Therefore, the system (i) enables end users to give instructions step-by-step, to avoid the complexity in full descriptions and give directly feedback of success (ii) creates an abstract meta model for user input during the linguistic analysis and (iii) independently interprets the meta model to code sequences that contain loops, conditionals, and statements. The context then places the recognized program component in the history. In this way, an algorithm is generated in an interactive process. One of the result can be the code sequence for algorithm, like well-known selection sort. We present a series of ontology structures for matching instructions to declare variables, loop, make decisions, etc. Furthermore, our system asks clarification questions when the human user is ambiguous. During the evaluation, 11 undergraduate students were asked to solve tasks by using natural language, and describe algorithms in three classes of complexity. Overall, the system was able to transform 60% of the user\u00a0\u2026", "num_citations": "4\n", "authors": ["164"]}
{"title": "Combining unit tests for data race detection\n", "abstract": " Multithreaded programs are subject to data races. Data race detectors find such defects by static or dynamic inspection of the program. Current race detectors suffer from high numbers of false positives, slowdown, and false negatives. Because of these disadvantages, recent approaches reduce the false positive rate and the runtime overhead by applying race detection only on a subset of the whole program. To achieve this, they make use of parallel test cases, but this has other disadvantages: Parallel test cases have to be engineered manually, cover code regions that are affected by data races, and execute with input data that provoke the data races. This paper introduces an approach that does not need additional parallel use cases to be engineered. Instead, we take conventional unit tests as input and automatically generate parallel test cases, execution contexts and input data. As can be observed, most real\u00a0\u2026", "num_citations": "4\n", "authors": ["164"]}
{"title": "Where's the science in software engineering? Ubiquity Symposium: The science in computer science\n", "abstract": " This article is a personal account of the methodological evolution of software engineering research from the 1970s to the present.", "num_citations": "4\n", "authors": ["164"]}
{"title": "Ist XP etwas f\u00fcr mich? empirische Studien zur Einsch\u00e4tzung von XP\n", "abstract": " Agile Software-Entwicklungsmethoden, und speziell Extreme Programming (XP), haben gro\u00dfe Aufmerksamkeit unter Praktikern und Forschern auf sich gezogen. Dieser Artikel bietet eine Gesamtschau der bis jetzt bekannten, empirischen Erkenntnisse zu XP, mit dem Ziel, dem Praktiker fundierte Entscheidungshilfen zu bieten sowie die L\u00fccken in der wissenschaftlichen Untersuchung aufzuzeigen. Verh\u00e4ltnism\u00e4\u00dfig gut untersucht sind die Praktiken der Paarprogrammierung und der testgetriebenen Entwicklung; weitere Techniken von XP wurden bis jetzt nur in Erfahrungsberichten evaluiert. Langzeituntersuchungen \u00fcber die Nachhaltigkeit der XP-Praktiken stehen ebenfalls noch aus.", "num_citations": "4\n", "authors": ["164"]}
{"title": "Toward accurate HPC productivity measurement\n", "abstract": " One key to improving high-performance computing (HPC) productivity is finding better ways to measure it. We define productivity in terms of mission goals, ie, greater productivity means that more science is accomplished with less cost and effort. Traditional software productivity metrics and computing benchmarks have proven inadequate for assessing or predicting such end-to-end productivity. In this paper we describe a new approach to measuring productivity in HPC applications that addresses both development time and execution time. Our goal is to develop a public repository of effective productivity benchmarks that anyone in the HPC community can apply to assess or predict productivity", "num_citations": "4\n", "authors": ["164"]}
{"title": "Project Triton: Towards improved programmability of parallel computers\n", "abstract": " The main objective of Project Triton is adequate programmability of massively parallel computers. This goal can be achieved by tightly coupling the design of programming languages and parallel hardware.           The approach taken in the Project Triton is to let high-level, machine independent parallel programming languages drive the design of parallel hardware. This approach permits machine-independent parallel programs to be compiled into efficient machine code. The main results are as follows:                                                    Modula-2*. This language extends Modula-2 with constructs for expressing a wide range of parallel algorithms in a portable, problem-oriented, and readable way.                                                                  Compilation Techniques. We present techniques for the efficient translation of Modula-2* and similar imperative languages for several modern parallel machines and derive\u00a0\u2026", "num_citations": "4\n", "authors": ["164"]}
{"title": "On experimental computer science\n", "abstract": " The experimental branch of Computer Science appears to be underdeveloped. Few quantitative, experimentally verified results are published, depriving the field of answers to questions where theory cannot reach. A classification of papers in three systems-oriented journals yielded alarming statistics. Articles in ACM TOPLAS, IEEE Transactions on Software Engineering, and Comm. of the ACM are classified according to theory, design, quantitative evaluation, and hypothesis testing. Papers classified as theory typically contain mathematical models, theorems, proofs, or algorithms, but include no empirical evaluation. Design papers describe systems or languages. Although these papers may contain some qualitative evaluation, they contain neither theorems nor quantitative results. If a paper includes non-trivial, quantitative evaluation (at least two pages), it is counted in the third category. The fourth category\u00a0\u2026", "num_citations": "4\n", "authors": ["164"]}
{"title": "Approaching natural conversation chatbots by interactive dialogue modelling & microsoft LUIS\n", "abstract": " In this paper, we preset guidelines on how to model a dialogue-based conversation with a chatbot build on Microsoft conversational services. Therefore, we created a free framework which manages dialogues and user sessions. For this purpose, we use a systematic classification of the input and intents. Especially, we used Microsoft Language Understanding Intelligent Service (LUIS) for intent extraction. LUIS provides a platform to extract the user need by calculating the intent probability. Unfortunately, LUIS does not provide dialogue management. For that reason, developers need to create own dialogue behavior in Visual Studio project. Therefore, we extracted and use it as a free guideline for the community and other platform developers.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Natural language user interface for software engineering tasks\n", "abstract": " In this paper, we present the idea to use natural language as the user interface for programming tasks. Programming languages assist with repetitive tasks that involve the use of conditionals, loops and statements. This is what is often challenging users. However, users can easily describe tasks in their natural language. We aim to develop a Natural Language User Interface that enables users to describe algorithms, including statements, loops, and conditionals. For this, we extend our current spreadsheet system to support control flows. An evaluation shows that users solved more than 60% of tasks. Although far from perfect, this research might lead to fundamental changes in computer use. With natural language, programming would become available to everyone. We believe that it is a reasonable approach for end user software engineering and will therefore overcome the present bottleneck of IT proficients.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Changing the Game: Dr. Dave Schrader on sports analytics\n", "abstract": " Dave Schrader, known to his friends as Dr. Dave, worked for 24 years in advanced development and marketing at Teradata, a major data warehouse vendor. He actively gives talks on business analytics, and since retiring has spent time exploring the field of sports analytics. In this interview, Schrader discusses how analytics is playing a significant role in professional sports--from Major League Soccer to the NBA.", "num_citations": "3\n", "authors": ["164"]}
{"title": "A natural language dialog system based on active ontologies\n", "abstract": " Programming today requires years of training. With natural language, programming would become available to everyone and enable end users to program their devices or extend their functionality without any knowledge of programming languages. We present an assistant usable in technical domains that uses natural language understanding, programming stepby-step and an active dialog management system. It allows users to manipulate spreadsheet data by using natural language. We extend our previous system with active ontologies. By adding additional information to an ontology, such as a rule evaluation system and a fact store, it becomes an execution environment instead of just being a representation of knowledge. Sensor nodes register certain events and store them in the fact store. An evaluation mechanism tests the new facts against the existing rules and performs the associated action if one or more rules apply to the stored facts. The system also handles references to previous results and expressions, allowing the construction of complex expressions step-by-step. It also creates new formulas by using End-User Programming concepts and supports the use of repetitive tasks that involve use of conditions and negations. An evaluation shows that the active ontology-based approach resolves 90% of the input tasks which is an increase of 10% over the pattern matching approach.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Interleaving generation for data race and deadlock reproduction\n", "abstract": " Concurrency errors, like data races and deadlocks, are difficult to find due to the large number of possible interleavings in a parallel program. Dynamic tools analyze a single observed execution of a program, and even with multiple executions they can not reveal possible errors in other reorderings. This work takes a single program observation and produces a set of alternative orderings of the synchronization primitives that lead to a concurrency error. The new reorderings are enforced under a happens-before detector to discard reorderings that are infeasible or do not produce any error report. We evaluate our approach against multiple repetitions of a state of the art happens-before detector. The results show that through interleaving inference more errors are found and the counterexamples enable easier reproducibility by the developer.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Predicting and witnessing data races using CSP\n", "abstract": " Detecting and debugging data races is a complex task due to the large number of interleavings possible in a parallel program. Most tools can find the data races reliably in an observed execution, but they miss errors in alternative reorderings of events. In this paper we describe an automated approach to generate, from a single program trace, a model in CSP with alternative interleavings. We check for data races patterns and obtain a witness that allows the reproduction of errors. Reproduction reduces the developer effort to correct the error.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Automated bug fixing: an interview with Westley Weimer, Department of Computer Science, University of Virginia and Martin Monperrus, University of Lille and INRIA, Lille, France\n", "abstract": " Fixing bugs manually is expensive, time-consuming, and unpleasant. How about getting the computer to fix the bugs, automatically? Automatically repairing them might save us from misunderstandings, lack of time, carelessness, or plain old laziness. But this brings into question some fundamental limitations. Yet in the past 10 years, a number of young scientists have taken on automatic bug fixing. This interview discusses the approximations currently in use and how far they can take us.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Locating parallelization potential in object-oriented data structures\n", "abstract": " The free lunch of ever increasing single-processor performance is over. Software engineers have to parallelize software to gain performance improvements. But not every software engineer is a parallel expert and with millions of lines of code that have not been developed with multicore in mind, we have to find ways to assist in identifying parallelization potential. This paper makes three contributions: 1) An empirical study of more than 900,000 lines of code reveals five use cases in the runtime profile of object-oriented data structures that carry parallelization potential. 2) The study also points out frequently used data structures in realistic software in which these use cases can be found. 3) We developed DSspy, an automatic dynamic profiler that locates these use cases and makes recommendations on how to parallelize them. Our evaluation shows that DSspy reduces the search space for parallelization by up to 77\u00a0\u2026", "num_citations": "3\n", "authors": ["164"]}
{"title": "Mapping functions and data redistribution for parallel files\n", "abstract": " Data distribution in memory or on disks is an important factor influencing the performance of parallel applications. On the other hand, programs or systems, like a parallel file system, frequently redistribute data between memory and disks.               This paper presents a generalization of previous approaches of the redistribution problem. We introduce algorithms for mapping between two arbitrary distributions of a data set. The algorithms are optimized for multidimensional array partitions. We motivate our approach and present potential utilizations. The paper also presents a case study, the employment of mapping functions, and redistribution algorithms in a parallel file system.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Logging kernel events on clusters\n", "abstract": " We present tools for recording and analysing kernel events on Linux clusters. The tools provide cluster-wide event logging at system clock accuracy. We demonstrate the usefulness of our tools by verifying an implementation of a simple remote scheduling feature with interesting results regarding schedule responsiveness, by analysing local clock drift in a cluster, and by observing the effect of a kernel read-ahead policy upon sequential file read. With our GUI-based Java application, data recorded on multiple hosts is integrated for visualization. These tools can be used for analysis of cluster schedulers such as gang schedulers, SMP scheduling affinity, or the interaction between user level applications and kernel policies such as file read-ahead. Our work can be easily extended for integration with application level logging.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Addendum to \u201cDelta algorithms: an empirical analysis\u201d\n", "abstract": " The authors supply machine configurations for experiments reported in  \u201cDelta Algorithms: An Empirical Analysis,\u201d by Hunt et al. (ACM Trans. Softw. Eng. Methodol. 7, 2 (Apr. 1998), pp. 192-214).", "num_citations": "3\n", "authors": ["164"]}
{"title": "Prefetching on the Cray-T3E: A Model and its Evaluation\n", "abstract": " In many parallel applications, network latency causes a dramatic loss in processor utilization. This paper examines software controlled access pipelining (SCAP) as a technique for hiding network latency. An analytic model of SCAP briefly describes basic operation techniques and performance improvements. Results are quantified with benchmarks on the Cray-T3E. The benchmarks used are Jacobi-iteration, parts of the Livermore Loop kernels, and others representing six different parallel algorithm classes. These were parallelized and optimized by hand to show the performance tradeoff of severals pipelining techniques. Our results show that SCAP on the Cray-T3E improves performance compared to a blocking execution by a factor of 2.1 to 38. It also got a performance speed-up against HPF of at least 12% to a factor of 3.1 dependent on the algorithm class.", "num_citations": "3\n", "authors": ["164"]}
{"title": "Tichy's response to RW Schwanke and GE Kaiser's \u201cSmarter Recompilation\u201d\n", "abstract": " Schwanke and Kaiser's extension of smart recompilation is an intriguing idea. Their mechanism aims at delaying recompilation work by permitting \u201charmless\u201d compilation inconsistencies to remain after changes. Full consistency can be reestablished at a later time, after the change has been tested in a subpart of the system. If the change was inadequate, then no needless compilation work was performed. This strategy is used frequently in practice, by exploiting loopholes in system generation tools. Schwanke and Kaiser's mechanism is novel in that it makes this practice safe. The compiler is aware of the inconsistencies, and will not overlook dangerous ones. Furthermore, it can help reestablish full consistency once a change is deemed acceptable. Smarter recompilation defines a harmless inconsistency as follows. If a declaration is changed, this action is treated as introducing a new version of the declaration. The\u00a0\u2026", "num_citations": "3\n", "authors": ["164"]}
{"title": "Programming Languages and Learning\n", "abstract": " This site is designed to provide an overview of recent evidence on human factors evidence in programming language design. In some cases, our intent is to dispel myths. In others, it is to provide the result of research lines.The community of scholars advocating for and participating in evidence-based programming language design is growing. This sheet is not comprehensive, but covers several of the broad trends and highlights a number of empirical studies that have been performed.", "num_citations": "2\n", "authors": ["164"]}
{"title": "SEPS 2014: first international workshop on software engineering for parallel systems\n", "abstract": " The first international workshop on Software Engineering for Parallel Systems (SEPS) will be held in Portland, Oregon, USA on October 21, 2014 and co-located with the ACM SIGPLAN conference on Systems, Programming, Languages and Applications: Software for Humanity (SPLASH 2014). The purpose of this workshop is to provide a stable forum for researchers and practitioners dealing with compelling challenges of the software development life cycle on modern parallel platforms. The increased complexity of parallel applications on modern parallel platforms (eg multicore, manycore, distributed or hybrid) requires more insight into development processes, and necessitates the use of advanced methods and techniques supporting developers in creating parallel applications or parallelizing and reengineering sequential legacy applications. We aim to advance the state of the art in different phases of parallel\u00a0\u2026", "num_citations": "2\n", "authors": ["164"]}
{"title": "The multicore software challenge\n", "abstract": " {With multicore processors, servers, PCs, and laptops have become truly parallel machines. The mobile phone and embedded applications will follow. Hardware manufacturers predict a hundred processors and more per chip. General-purpose, parallel software, on the other hand, is scarce. What to do with all the processors? This briefing will overview the current hardware developments, provide examples of successfully parallelized, non-numeric applications, and discuss the lessons learnt for software engineering. The good news is that parallelization is not a black art\u2014it can be handled with reasonable effort. However, significant restructurings are typically required when parallelizing existing, serial applications. We also present recent advances in the areas of automatic performance tuning and programming languages that allow the succinct expression of frequent parallel patterns. A comparative case study will\u00a0\u2026", "num_citations": "2\n", "authors": ["164"]}
{"title": "A communication middleware for smart room environments\n", "abstract": " Processing sensor data to recognize and interpret human activity and human-human interaction in smart room environments is a computationally intensive task. Thus, processing components in smart rooms must be spread over multiple computers in a network. Dealing with the complexity of distributing and managing these components puts a considerable burden on these components\u2019 developers. In this paper we introduce ChilFlow, a distributed data-transfer middleware specifically designed to ease the work of smart room developers. We describe flows, the network-transparent, typed, one-to-many communication channels used for communication between processing components. We also present ChilFlow\u2019s programming interface for flows and demonstrate how it assures type-safety and network-transparency.", "num_citations": "2\n", "authors": ["164"]}
{"title": "Lean production methods in modern software development\n", "abstract": " This paper shows that Extreme Programming (XP) is rooted in the principle of Lean Production. XP drastically slims down the development process, but adds extreme customer orientation end extreme ways of quality assurance to the process. The fact thar XP and other agile methods in modern software development are based on lean principles explains why agilemethods can produce high-quality software in a cost-effective way. The paper also contains a discussion of problems that come up when using XP in practice, and raises important questions about lean methods in software development; for example , whether lean methods scale to large software projects and large software organizations. When trying to answer such questions, software engineering could draw from experience with lean production and lean development in other fields.", "num_citations": "2\n", "authors": ["164"]}
{"title": "Programming language inherent support for constrained xml schema definition data types and owl dl\n", "abstract": " Recently, the Web Ontology Language (OWL) and XML schema definition (XSD) have become ever more important when it comes to conceptualize knowledge and to define programming language independent type systems. However, writing software that operates on ontological data and on XML instance documents still suffers from a lack of compile time support for OWL and XSD. Especially, obeying lexical- and value space constraints that may be imposed on XSD simple data types and preserving the consistency of assertional ontological knowledge is still error prone and laborious. Validating XML instance documents and checking the consistency of ontological knowledge bases according to given XML schema definitions and ontological terminologies, respectively, requires significant amounts of code. This paper presents novel compile time- and code generation features, which were implemented as an\u00a0\u2026", "num_citations": "2\n", "authors": ["164"]}
{"title": "Die Bedeutung der Empirie f\u00fcr die Softwaretechnik\n", "abstract": " Empirische Untersuchungen sind aus der Softwareforschung nicht mehr wegzudenken. Explorative empirische Studien einschlie\u00dflich kontrollierter Experimente vergleichen verschiedene Techniken, um festzustellen, ob bei Verwendung der Techniken Unterschiede in der Qualit\u00e4t von Software, der Bearbeitungszeit oder anderen Eigenschaften tats\u00e4chlich zu beobachten sind. Diese Studien best\u00e4tigen oder verwerfen vermutete oder postulierte Eigenschaften von Werkzeugen und Methoden. Korrelationsstudien untersuchen Zusammenh\u00e4nge zwischen Variablen bei der Softwarebearbeitung. Vorhersageund Optimierungsmodelle schlie\u00dflich suchen die entdeckten Zusammenh\u00e4nge f\u00fcr die Praxis der Softwareentwicklung nutzbar zu machen. Letztlich sucht der Wissenschaftler nach Erkl\u00e4rungen f\u00fcr die beobachteten Zusammenh\u00e4nge, also nach Theorien \u00fcber die Software-Erstellung, aus denen praktisch nutzbare Modelle abgeleitet werden k\u00f6nnen. Anhand von Ergebnissen aus den Bereichen Kostensch\u00e4tzung, Inspektionen, Software-Entwurf, agile Methoden, u.a. illustriert dieser Vortrag den von der Empirie getriebenen Wissenschaftsprozess in der Softwaretechnik. Gezeigt wird, wo bereits beachtliche Fortschritte erzielt werden konnten und wo weiterer Bedarf an empirischen Untersuchungen besteht.", "num_citations": "2\n", "authors": ["164"]}
{"title": "Die rolle der empirie in der softwaretechnik\n", "abstract": " Die Rolle der Empirie in der Softwaretechnik Page 1 UNIVERSIT\u00c4T KARLSRUHE (TH) Fakult\u00e4t f\u00fcr Informatik Die Rolle der Empirie in der Softwaretechnik Walter F. Tichy Universit\u00e4t Karlsruhe Page 2 Walter F. Tichy Folie 2 Raffaels Schule von Athen Page 3 Walter F. Tichy Folie 3 Platon und Aristoteles Platon: Die Welt der Ideen und Formen kann nur durch die Vernunft erkannt werden. Sie existiert unabh\u00e4ngig von der Erscheinungswelt. Beobachtung ist wertlos f\u00fcr Erkenntnis. (\u201cReine Theorie\u201d) Aristoteles: Wissenschaft untersucht das Allgemeine, welches sich aber in der Existenz einzelner Individuen ausdr\u00fcckt. Demnach muss Wissenschaft einen Ausgleich zwischen dem Empirismus und der Deduktion schaffen, nicht blo\u00df dazwischen w\u00e4hlen. (\u201cAuch die Beobachtung z\u00e4hlt\u201d) Page 4 Walter F. Tichy Folie 4 Modernes Verst\u00e4ndnis von Wissenschaft \u220e Wir \u00fcberspringen Ptolem\u00e4us, Kopernikus, Tycho Brahe, Galileo, , , (\u2026", "num_citations": "2\n", "authors": ["164"]}
{"title": "Fast parallel i/o on cluster computers\n", "abstract": " Today's cluster computers suffer from slow I/O, which slows down I/O-intensive applications. We show that fast disk I/O can be achieved by operating a parallel file system over fast networks such as Myrinet or Gigabit Ethernet. In this paper, we demonstrate how the ParaStation3 communication system helps speed-up the performance of parallel I/O on clusters using the open source parallel virtual file system (PVFS) as testbed and production system. We will describe the set-up of PVFS on the Alpha-Linux-Cluster-Engine (ALiCE) located at Wuppertal University, Germany. Benchmarks on ALiCE achieve write-performances of up to 1 GB/s from a 32-processor compute-partition to a 32-processor PVFS I/O-partition, outperforming known benchmark results for PVFS on the same network by more than a factor of 2. Read-performance from buffer-cache reaches up to 2.2 GB/s. Our benchmarks are giant, I/O-intensive eigenmode problems from lattice quantum chromodynamics, demonstrating stability and performance of PVFS over Parastation in large-scale production runs.", "num_citations": "2\n", "authors": ["164"]}
{"title": "Selected patterns for software configuration management\n", "abstract": " Much of the detailed knowledge required to implement good Software Con guration Management systems has not been adequately cataloged and documented. The Design Pattern methodology provides a means to help rectify this de ciency. Based on the authors' experience in implementing Software Con guration Management tools, a selection of basic techniques for Software Con guration Management are presented as Design Patterns. Though the techniques present are not new, their formulation in terms of Design Patterns facilitates the learning and understanding of SCM system design. It may also facilitate the use of these techniques outside traditional SCM such as for CAD and databases.", "num_citations": "2\n", "authors": ["164"]}
{"title": "On the feasibility of a scalable opto-electronic CRCW shared memory\n", "abstract": " We discuss the results of a feasibility study of an opto-electronic shared memory with concurrent read, concurrent write capability. Unlike previous such work we consider a true hardware shared memory rather then a simulation on a tightly, optically connected distributed memory computer. We describe a design that could be implemented using compact integrated semiconductor modules and propose ways to solve two major problems faced by such a device: optical system complexity and parallel word level write consistency. It is shown that, in principle, a memory with GBytes capacity and a latency of less then 1 ns, accessed by up to 10/sup 5/ processors could be feasible. Using devices currently available as laboratory prototypes and taking into account energy and crosstalk considerations a capacity of more then 1 MB and a latency of about 50 ns might be attained for up to 1000 processors.< >", "num_citations": "2\n", "authors": ["164"]}
{"title": "From Modula-2* to Efficient Parallel Code\n", "abstract": " From Modula-2* to efficient parallel code - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/129078 Title : From Modula-2* to efficient parallel code Authors : Heinz, EA ; Lukowicz, P. ; Philippsen, M. ; Tichy, WF ; Corporate author : Karlsruhe Univ. (TH) (Germany). Fakultaet fuer Informatik ; Publication year : 1992 Language : English ; Pagination/Size : 15 p. ; SIGLE classification : : DE_ 1994:926 ; DE ; handle : http://hdl.handle.net/10068/129078 Provenance : SIGLE ; Get a copy : FIZ - Fachinformationszzentrum Karlsruhe TIB - Technische Informationsbibliothek Availability : Available from TIB Hannover: RA2045(21) Country : Germany ; CNRS INIST Creative Commons Help /\u2026", "num_citations": "2\n", "authors": ["164"]}
{"title": "Projekt Triton: Beitr\u00e4ge zur Verbesserung der Programmierbarkeit hochparalleler Rechensysteme\n", "abstract": " Im Projekt Triton steht das Ziel der ad aquaten Programmierbarkeit von Hochparallelrechnern mit mehreren tausend Prozessoren im Mittelpunkt. Dieses Ziel kann erreicht werden, wenn Programmiersprache und Hardware-Architektur gemeinsam entworfen und so aufeinander abgestimmt werden, da eine Ubersetzung der Sprache in effizienten Code m oglich wird. Diesen Ansatz verfolgen wir im Projekt Triton. 1. Parallele Programmiersprache. Mit Modula-2*, einer Erweiterung von Modula-2, stellen wir Sprachmittel vor, die erforderlich sind, um parallele Algorithmen hochsprachlich, lesbar und portabel zu formulieren. Wir beschreiben deren effiziente Ubersetzbarkeit f ur unterschiedliche moderne Parallelrechnerarchitekturen.", "num_citations": "2\n", "authors": ["164"]}
{"title": "The triton project\n", "abstract": " The Triton project - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/146773 Title : The Triton project Authors : Philippsen, M. ; Herter, CG ; Tichy, WF (Karlsruhe Univ. (TH) (Germany)) ; Corporate author : Karlsruhe Univ. (TH) (Germany). Fakultaet fuer Informatik ; Publication year : 1990 Language : English ; Pagination/Size : 5 p. ; SIGLE classification : 09G - Computer hardware ; 09H - Computer software, programming ; Document type : I - Miscellaneous ; Other identifier : DE_ 1991:1627 ; DE ; handle : http://hdl.handle.net/10068/146773 Provenance : SIGLE ; Get a copy : FIZ - Fachinformationszzentrum Karlsruhe TIB - Technische Informationsbibliothek Availability : TIB: RA 2045(1990,33) Country : Germany ; CNRS INIST Creative Commons Help Search Paper of /\u2026", "num_citations": "2\n", "authors": ["164"]}
{"title": "Students implement the European Student Card\n", "abstract": " Student mobility is a shared goal of the member states of the European Union. One ingredient that makes student mobility possible is a universal student ID card that is accepted everywhere and can be coded with services at the institutions visited. The European Student Card (ESC) is such a universal ID card, standardized in Europe. A team of students from the Karlsruhe Institute of Technology developed the software for it. It works as follows: A student with an ESC simply walks up to a self-service kiosk, presents the card to a reader, and then selects the desired services, such as cafeteria, library, lab access, etc. In this interview, the development team will explain how they made this work smoothly, including the security considerations. This project is another example of how undergraduate students can build impressive software if given a challenge, the right tools, and some supervision.", "num_citations": "1\n", "authors": ["164"]}
{"title": "At Your Command! An Empirical Study on How Laypersons Teach Robots New Functions\n", "abstract": " Even though intelligent systems such as Siri or Google Assistant are enjoyable (and useful) dialog partners, users can only access predefined functionality. Enabling end-users to extend the functionality of intelligent systems will be the next big thing. To promote research in this area we carried out an empirical study on how laypersons teach robots new functions by means of natural language instructions. The result is a labeled corpus consisting of 3168 submissions given by 870 subjects.The analysis of the dataset revealed that many participants used certain wordings to express their wish to teach new functionality; two corresponding trigrams are among the most frequent. On the contrary, more than one third (37%) did not verbalize the teaching intent at all. We labeled the semantic constituents in the utterances: declaration (including the name of the function) and intermediate steps. The full corpus is publicly\u00a0\u2026", "num_citations": "1\n", "authors": ["164"]}
{"title": "Natural Language Data Queries on Multiple Heterogenous Data Sources\n", "abstract": " Motivated by a real-world scenario, we enable end users to query data due natural language from different sources like spreadsheets and databases. We provide a natural language user interface (NLUI) solution on how real-world entities and relations between them can be interpreted as a model to allow end user questions on the data. Therefore, the system enables end users to give instructions step-by-step, to avoid the complexity in full descriptions and give directly feedback of success. An evaluation is conducted with human users who had to perform a series of tasks using natural language. Overall, 13 end user took part in our survey with ten questions. 94.9% of all answers in the first part could be resolved on spreadsheet data, and 62,5% on SQL database.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Hybrid Online Autotuning for Parallel Ray Tracing.\n", "abstract": " Acceleration structures are key to high performance parallel ray tracing. Maximizing performance requires configuring the degrees of freedom (eg, construction parameters) these data structures expose. Whether a parameter setting is optimal depends on the input (eg, the scene and view parameters) and hardware. Manual selection is tedious, error prone, and is not portable. To automate the parameter selection task we use a hybrid of model-based prediction and online autotuning. The combination benefits from the best of both worlds: one-shot configuration selection when inputs are known or similar, effective exploration of the configuration space otherwise. Online tuning additionally serves to train the model on real inputs without requiring a-priori training samples.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Context Detection in Spreadsheets Based on Automatically Inferred Table Schema\n", "abstract": " Programming requires years of training. With natural language and end user development methods, programming could become available to everyone. It enables end users to program their own devices and extend the functionality of the existing system without any knowledge of programming languages. In this paper, we describe an Interactive Spreadsheet Processing Module (ISPM), a natural language interface to spreadsheets that allows users to address ranges within the spreadsheet based on inferred table schema. Using the ISPM, end users are able to search for values in the schema of the table and to address the data in spreadsheets implicitly. Furthermore, it enables them to select and sort the spreadsheet data by using natural language. ISPM uses a machine learning technique to automatically infer areas within a spreadsheet, including different kinds of headers and data ranges. Since ranges can be\u00a0\u2026", "num_citations": "1\n", "authors": ["164"]}
{"title": "An empirical study on parallelism in modern open-source projects\n", "abstract": " Writing parallel programs is hard, especially for inexperienced programmers. Parallel language features are still being added on a regular basis to most modern object-oriented languages and this trend is likely to continue. Being able to support developers with tools for writing and optimizing parallel programs requires a deep understanding of how programmers approach and implement parallelism. We present an empirical study of 135 parallel open-source projects in Java, C# and C++ ranging from small (< 1000 lines of code) to very large (> 2M lines of code) codebases. We examine the projects to find out how language features, synchronization mechanisms, parallel data structures and libraries are used by developers to express parallelism. We also determine which common parallel patterns are used and how the implemented solutions compare to typical textbook advice. The results show that similar parallel\u00a0\u2026", "num_citations": "1\n", "authors": ["164"]}
{"title": "Prototyp einer nat\u00fcrlichsprachlichen Schnittstelle f\u00fcr Tabellenkalkulation\n", "abstract": " Nicht zuletzt zeigen uns Siri und Google Now die zunehmende Wichtigkeit sprachlicher Nutzerschnittstellen. Der vorliegende Artikel beschreibt die Konzeption und Umsetzung einer nat\u00fcrlichsprachlichen Schnittstelle f\u00fcr Tabellenkalkulationen. Im Zuge einer Pilotstudie wurden die zur Realisierung notwendigen Anforderungen erhoben. Basierend darauf wurde der Prototyp JustLingo1 entwickelt, der textuelle Rechenaufgaben, wie Please add 4 to A1. Then multiply it by 7., mithilfe einer musterbasierten Syntaxund Semantikanalyse verarbeitet. Eine Sprachsteuerung befindet sich bereits in der Entwicklung. JustLingo erkennt R\u00fcckbez\u00fcge sowie fehlende oder mehrdeutige Informationen, fragt entsprechend nach und wandelt die Eingabe zu 79\\% erfolgreich in die Excel-Formelsprache um.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Autotuning and Self-Adaptability in Concurrency Libraries\n", "abstract": " Autotuning is an established technique for optimizing the performance of parallel applications. However, programmers must prepare applications for autotuning, which is tedious and error prone coding work. We demonstrate how applications become ready for autotuning with few or no modifications by extending Threading Building Blocks (TBB), a library for parallel programming, with autotuning. The extended TBB library optimizes all application-independent tuning parameters fully automatically. We compare manual effort, autotuning overhead and performance gains on 17 examples. While some examples benefit only slightly, others speed up by 28% over standard TBB.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Truck Scheduling on Multicore\n", "abstract": " Transportation businesses reduce costs by optimizing the routes of their trucks. However, determining optimal truck schedules is computationally intensive, running for hours on sequential computers. This article describes experience with parallelizing SAP\u00b4s Vehicle Scheduling and Routing Optimizer on shared-memory multicore computers. Re-engineering this complex application for a 24-core machine reduced typical computation time on real data from 1.5 hours to 5 minutes. The article discusses successful and unsuccessful parallelization approaches and concludes with lessons learnt.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Parallelizing an Index Generator for Desktop Search\n", "abstract": " Experience with the parallelization of an index generator fordesktop search is presented. Several configurations of the index generatorwere compared on Intel platforms with 4, 8 and 32 cores. The optimalconfigurations were not intuitive and markedly different for the threeplatforms. For finding the optimum, detailed measurements and experimentationwere necessary. Several recommendations for parallel softwaredesign follow from this case study.", "num_citations": "1\n", "authors": ["164"]}
{"title": "International workshop on multicore software engineering (iwmse 2008)\n", "abstract": " We have reached a major turning point: microprocessor performance can no longer be improved by increasing clock frequencies; instead, higher performance will have to come from parallelism. As multi/manycore processors with multiple CPUs on a chip become standard and affordable for everyone, software engineers face the challenge of parallelizing applications of all sorts. However, compared to sequential applications, our repertoire of tools and methods for cost-effectively developing reliable, parallel applications is spotty. The mission of this workshop is to bring together researchers and practitioners with diverse backgrounds in order to advance the state of the art in software engineering for multi/manycore parallel applications. This is the first workshop specifically focusing on software engineering challenges of multi/manycore.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Streaming Extensions for Object-Oriented Languages\n", "abstract": " Stream languages provide constructs to express different types of parallelism, such as pipeline parallelism, in a simple way. In the past, these languages were mainly used for the development of signal processing or graphics applications. We argue that the integration of stream programming concepts into universal object-oriented languages has great potential to simplify general-purpose parallel programming on multicore architectures.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Speculative TCP Connection Admission Using Connection Migration in Cluster-Based Servers.\n", "abstract": " This paper presents speculative TCP connection admission, a mechanism for improving sub-optimal request distribution decisions in cluster-based servers. Overloaded server nodes in the cluster speculatively accept incoming requests only to offload them to less-loaded nodes. Speculative admission uses connection endpoint migration, a technique that allows server-side connection endpoints to be arbitrarily assigned to server nodes in the cluster. Speculative connection admission targets distributed load balancing policies at the back-end level that leverage request routing decisions taken outside the cluster. The mechanism has been implemented in the Linux kernel as part of a policy-based software architecture for request distribution. We have been able to show that speculative connection admission adds little overhead to the normal TCP processing, offsets load imbalances and accommodates higher request rates. That makes it suitable for an active role in request distribution in cluster-based servers.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Multitasking\n", "abstract": " Multitasking refers to an operating system's ability to support multiple processes simultaneously. A process is a program in execution. Support for multiple processes is necessary in applications where several computations must proceed in parallel. On a PC, a user may edit a file while another file is being printed and electronic mail (qv) is received. These three activities are best supported by three processes running simultaneously. Multitasking is also needed on servers and time-sharing systems where multiple users share a single computer system and all processes created by them should, at least in principle, execute simultaneously. Real-time systems that control multiple devices also need to support multiple processes. For instance, an avionics computer on board an aircraft runs processes for monitoring the engines, updating the flight instruments, processing radar signals, and keeping the airplane on course\u00a0\u2026", "num_citations": "1\n", "authors": ["164"]}
{"title": "The parastation project\n", "abstract": " ParaStation is a high speed communication subsystem which enables a cluster of workstations or PCs to offer performance comparable to a dedicated parallel machine. Each node is still usable as a regular workstation or PC. Speedups of 7.4 on a 8 node cluster have been achieved even on communication intensive programs. The ParaStation software supports a real multitasking environment and combines the advantages of workstation clusters and of parallel machines.", "num_citations": "1\n", "authors": ["164"]}
{"title": "Programming parallel supercomputers\n", "abstract": " This paper discusses future directions in tools and techniques for programming parallel supercomputers. We base the discussion on two important observations:", "num_citations": "1\n", "authors": ["164"]}
{"title": "Modula-2*: An extension of Modula-2 for highly parallel programs\n", "abstract": " Highly parallel computers with tens of thousands of processors will be of rapidly growing importance for highspeed computation. Parallel programs for these machines should be machine-independent, ie, independent of properties that are likely to differ from one parallel computer to the next. In particular, parallel programs should be independent of:1. memory organization and communication network, 2. number of physical processors available, 3. control mode of the parallel computer(SIMD, MIMD, or MSIMD).", "num_citations": "1\n", "authors": ["164"]}
{"title": "A replicated, distributed file system\n", "abstract": " IBIS-RP Is built on top of IBIS-RA, a package providing remote access for files and dlrectodas in a network of Unix machines [6]. IBIS-RP demonstrates that replication can speed up read operations significantly and that purely local operations do not suffer in performance if replication Is supported. The most novel aspect of IBIS-RP IS how it extends demand replication to directories. Directory trees can be partially replicated to any node In the network, and there IS no need to keep all pdmades of a directory tree at a fixed site. Thus, there IS no need for a central directory sewer.Demand replication is attractive, because It automatically adjusts access cost and availability according to demand. Objects like commar,: l files or public directories with a high read ratio experience a high level of replication, which provides efficient read access and high mad-availability for those objects. Private flies and directories with a high\u00a0\u2026", "num_citations": "1\n", "authors": ["164"]}
{"title": "Performance Analysis of Network File Systems\n", "abstract": " In a local area network, the efficiency of the network file system is a key performance issue. This paper compares several different classes of network file systems with respect to number of file transfers and auxiliary messages processed by each node.The file system classes compared are:(a) remote file access,(b) polling (each node may create a read-only replicate of a file, but must poll the primary copy to deter-mine whether it is up-to-date),(c) broadcast staling (each update broadcasts a mes-sage to mark replicates as stale),(d) multicast staling (each update sends an outdating message to the sites with replicates).", "num_citations": "1\n", "authors": ["164"]}
{"title": "Software development control based on system structure description[Ph. D. Thesis]\n", "abstract": " An integrated software development and maintenance environment that supports communication and cooperation among programmers is discussed. A notation to describe the overall structure of a programmed system is the basis of the environment. This notation is a module interconnection language called INTERCOL. It can represent multiple versions and configurations, written in multiple programming languages. The environment establishes consistent interfaces among separately developed software components and maintains this consistency when modifications are made. It automates the management of interface changes by determining the affected components, alerting the responsible programmers, and preventing the use of inconsistent components. The environment coordinates the creation and change of source program versions and implements system generation in a multiversion, multiconfiguration\u00a0\u2026", "num_citations": "1\n", "authors": ["164"]}