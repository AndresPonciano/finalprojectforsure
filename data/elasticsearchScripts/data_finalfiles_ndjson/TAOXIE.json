{"title": "Parseweb: a programmer assistant for reusing open source code on the web\n", "abstract": " Programmers commonly reuse existing frameworks or libraries to reduce software development efforts. One common problem in reusing the existing frameworks or libraries is that the programmers know what type of object that they need, but do not know how to get that object with a specific method sequence. To help programmers to address this issue, we have developed an approach that takes queries of the form\" Source object type\u2192 Destination object type\" as input, and suggests relevant method-invocation sequences that can serve as solutions that yield the destination object from the source object given in the query. Our approach interacts with a code search engine (CSE) to gather relevant code samples and performs static analysis over the gathered samples to extract required sequences. As code samples are collected on demand through CSE, our approach is not limited to queries of any specific set of\u00a0\u2026", "num_citations": "524\n", "authors": ["186"]}
{"title": "WHYPER: Towards Automating Risk Assessment of Mobile Applications.\n", "abstract": " Application markets such as Apple\u2019s App Store and Google\u2019s Play Store have played an important role in the popularity of smartphones and mobile devices. However, keeping malware out of application markets is an ongoing challenge. While recent work has developed various techniques to determine what applications do, no work has provided a technical approach to answer, what do users expect? In this paper, we present the first step in addressing this challenge. Specifically, we focus on permissions for a given application and examine whether the application description provides any indication for why the application needs a permission. We present WHYPER, a framework using Natural Language Processing (NLP) techniques to identify sentences that describe the need for a given permission in an application description. WHYPER achieves an average precision of 82.8%, and an average recall of 81.5% for three permissions (address book, calendar, and record audio) that protect frequently used security and privacy sensitive resources. These results demonstrate great promise in using NLP techniques to bridge the semantic gap between user expectations and application functionality, further aiding the risk assessment of mobile applications.", "num_citations": "423\n", "authors": ["186"]}
{"title": "WHYPER: towards automating risk assessment of mobile applications\n", "abstract": " Application markets such as Apple\u2019s App Store and Google\u2019s Play Store have played an important role in the popularity of smartphones and mobile devices. However, keeping malware out of application markets is an ongoing challenge. While recent work has developed various techniques to determine what applications do, no work has provided a technical approach to answer, what do users expect? In this paper, we present the first step in addressing this challenge. Specifically, we focus on permissions for a given application and examine whether the application description provides any indication for why the application needs a permission. We present WHYPER, a framework using Natural Language Processing (NLP) techniques to identify sentences that describe the need for a given permission in an application description. WHYPER achieves an average precision of 82.8%, and an average recall of 81.5% for three permissions (address book, calendar, and record audio) that protect frequently used security and privacy sensitive resources. These results demonstrate great promise in using NLP techniques to bridge the semantic gap between user expectations and application functionality, further aiding the risk assessment of mobile applications.", "num_citations": "423\n", "authors": ["186"]}
{"title": "Mining API patterns as partial orders from source code: from usage scenarios to specifications\n", "abstract": " A software system interacts with third-party libraries through various APIs. Using these library APIs often needs tofollow certain usage patterns. Furthermore, ordering rules (specifications) exist between APIs, and these rules govern the secure and robust operation of the system using these APIs. But these patterns and rules may not be well documented by the API developers. Previous approaches mine frequent association rules, itemsets, or subsequences that capture API call patterns shared by API client code. However, these frequent API patterns cannot completely capture some useful orderings shared by APIs, especially when multiple APIs are involved across different procedures. In this paper, we present a framework to automatically extract usage scenarios among user-specified APIs as partial orders, directly from the source code (API client code). We adapt a model checker to generate interprocedural\u00a0\u2026", "num_citations": "332\n", "authors": ["186"]}
{"title": "Appcontext: Differentiating malicious and benign mobile app behaviors using context\n", "abstract": " Mobile malware attempts to evade detection during app analysis by mimicking security-sensitive behaviors of benign apps that provide similar functionality (e.g., sending SMS messages), and suppressing their payload to reduce the chance of being observed (e.g., executing only its payload at night). Since current approaches focus their analyses on the types of security-sensitive resources being accessed (e.g., network), these evasive techniques in malware make differentiating between malicious and benign app behaviors a difficult task during app analysis. We propose that the malicious and benign behaviors within apps can be differentiated based on the contexts that trigger security-sensitive behaviors, i.e., the events and conditions that cause the security-sensitive behaviors to occur. In this work, we introduce AppContext, an approach of static program analysis that extracts the contexts of security-sensitive\u00a0\u2026", "num_citations": "275\n", "authors": ["186"]}
{"title": "DSD-Crasher: A hybrid analysis tool for bug finding\n", "abstract": " DSD-Crasher is a bug finding tool that follows a three-step approach to program analysis: D. Capture the program's intended execution behavior with dynamic invariant detection. The derived invariants exclude many unwanted values from the program's input domain. S. Statically analyze the program within the restricted input domain to explore many paths. D. Automatically generate test cases that focus on reproducing the predictions of the static analysis. Thereby confirmed results are feasible. This three-step approach yields benefits compared to past two-step combinations in the literature. In our evaluation with third-party applications, we demonstrate higher precision over tools that lack a dynamic step and higher efficiency over tools that lack a static step.", "num_citations": "274\n", "authors": ["186"]}
{"title": "Inferring resource specifications from natural language API documentation\n", "abstract": " Typically, software libraries provide API documentation, through which developers can learn how to use libraries correctly. However, developers may still write code inconsistent with API documentation and thus introduce bugs, as existing research shows that many developers are reluctant to carefully read API documentation. To find those bugs, researchers have proposed various detection approaches based on known specifications. To mine specifications, many approaches have been proposed, and most of them rely on existing client code. Consequently, these mining approaches would fail to mine specifications when client code is not available. In this paper, we propose an approach, called Doc2Spec, that infers resource specifications from API documentation. For our approach, we implemented a tool and conducted an evaluation on Javadocs of five libraries. The results show that our approach infers various\u00a0\u2026", "num_citations": "228\n", "authors": ["186"]}
{"title": "XEngine: A fast and scalable xacml policy evaluation engine\n", "abstract": " XACML has become the de facto standard for specifying access control policies for various applications, especially web services. With the explosive growth of web applications deployed on the Internet, XACML policies grow rapidly in size and complexity, which leads to longer request processing time. This paper concerns the performance of request processing, which is a critical issue and so far has been overlooked by the research community. In this paper, we propose XEngine, a scheme for efficient XACML policy evaluation. XEngine first converts a textual XACML policy to a numerical policy. Second, it converts a numerical policy with complex structures to a numerical policy with a normalized structure. Third, it converts the normalized numerical policy to tree data structures for efficient processing of requests. To evaluate the performance of XEngine, we conducted extensive experiments on both real-life and\u00a0\u2026", "num_citations": "221\n", "authors": ["186"]}
{"title": "Improving structural testing of object-oriented programs via integrating evolutionary testing and symbolic execution\n", "abstract": " Achieving high structural coverage such as branch coverage in object-oriented programs is an important and yet challenging goal due to two main challenges. First, some branches involve complex program logics and generating tests to cover them requires deep knowledge of the program structure and semantics. Second, covering some branches requires special method sequences to lead the receiver object or non-primitive arguments to specific desirable states. Previous work has developed the symbolic execution technique and the evolutionary testing technique to address these two challenges, respectively. However, neither technique was designed to address both challenges at the same time. To address the respective weaknesses of these two previous techniques, we propose a novel framework called Evacon that integrates evolutionary testing (used to search for desirable method sequences) and\u00a0\u2026", "num_citations": "215\n", "authors": ["186"]}
{"title": "A fault model and mutation testing of access control policies\n", "abstract": " To increase confidence in the correctness of specified policies, policy developers can conduct policy testing by supplying typical test inputs (requests) and subsequently checking test outputs (responses) against expected ones. Unfortunately, manual testing is tedious and few tools exist for automated testing of access control policies. We present a fault model for access control policies and a framework to explore it. The framework includes mutation operators used to implement the fault model, mutant generation, equivalent-mutant detection, and mutant-killing determination. This framework allows us to investigate our fault model, evaluate coverage criteria for test generation and selection, and determine a relationship between structural coverage and fault-detection effectiveness. We have implemented the framework and applied it to various policies written in XACML. Our experimental results offer valuable insights\u00a0\u2026", "num_citations": "205\n", "authors": ["186"]}
{"title": "Identifying security bug reports via text mining: An industrial case study\n", "abstract": " A bug-tracking system such as Bugzilla contains bug reports (BRs) collected from various sources such as development teams, testing teams, and end users. When bug reporters submit bug reports to a bug-tracking system, the bug reporters need to label the bug reports as security bug reports (SBRs) or not, to indicate whether the involved bugs are security problems. These SBRs generally deserve higher priority in bug fixing than not-security bug reports (NSBRs). However, in the bug-reporting process, bug reporters often mislabel SBRs as NSBRs partly due to lack of security domain knowledge. This mislabeling could cause serious damage to software-system stakeholders due to the induced delay of identifying and fixing the involved security bugs. To address this important issue, we developed a new approach that applies text mining on natural-language descriptions of BRs to train a statistical model on already\u00a0\u2026", "num_citations": "192\n", "authors": ["186"]}
{"title": "Where do developers log? an empirical study on logging practices in industry\n", "abstract": " System logs are widely used in various tasks of software system management. It is crucial to avoid logging too little or too much. To achieve so, developers need to make informed decisions on where to log and what to log in their logging practices during development. However, there exists no work on studying such logging practices in industry or helping developers make informed decisions. To fill this significant gap, in this paper, we systematically study the logging practices of developers in industry, with focus on where developers log. We obtain six valuable findings by conducting source code analysis on two large industrial systems (2.5 M and 10.4 M LOC, respectively) at Microsoft. We further validate these findings via a questionnaire survey with 54 experienced developers in Microsoft. In addition, our study demonstrates the high accuracy of up to 90% F-Score in predicting where to log.", "num_citations": "174\n", "authors": ["186"]}
{"title": "Mining exception-handling rules as sequence association rules\n", "abstract": " Programming languages such as Java and C++ provide exception-handling constructs to handle exception conditions. Applications are expected to handle these exception conditions and take necessary recovery actions such as releasing opened database connections. However, exception-handling rules that describe these necessary recovery actions are often not available in practice. To address this issue, we develop a novel approach that mines exception-handling rules as sequence association rules of the form ldquo(FC c   1 ...FC c   n ) nland FC a  rArr (FC e   1 ...FC e   m )rdquo. This rule describes that function call FCa should be followed by a sequence of function calls (FC e   1 ...FC e   m ) when FC a  is preceded by a sequence of function calls (FC e   1 ...FC c   n ). Such form of rules is required to characterize common exception-handling rules. We show the usefulness of these mined rules by applying\u00a0\u2026", "num_citations": "155\n", "authors": ["186"]}
{"title": "SpotWeb: Detecting framework hotspots and coldspots via mining open source code on the web\n", "abstract": " Software developers often face challenges in reusing open source frameworks due to several factors such as the framework complexity and lack of proper documentation. In this paper, we propose a code-search-engine-based approach that detects hotspots in a given framework by mining code examples gathered from open source repositories available on the Web; these hotspots are API classes and methods that are frequently reused. Hotspots can serve as starting points for developers in understanding and reusing the given framework. Our approach also detects coldspots, which are API classes and methods that are rarely used. Coldspots serve as caveats for developers as there can be difficulties in finding relevant code examples and are generally less exercised compared to hotspots. We developed a tool, called SpotWeb, for frameworks or libraries written in Java and used our tool to detect hotspots and\u00a0\u2026", "num_citations": "145\n", "authors": ["186"]}
{"title": "Alattin: Mining alternative patterns for detecting neglected conditions\n", "abstract": " To improve software quality, static or dynamic verification tools accept programming rules as input and detect their violations in software as defects. As these programming rules are often not well documented in practice, previous work developed various approaches that mine programming rules as frequent patterns from program source code. Then these approaches use static defect-detection techniques to detect pattern violations in source code under analysis. These existing approaches often produce many false positives due to various factors. To reduce false positives produced by these mining approaches, we develop a novel approach, called Alattin, that includes a new mining algorithm and a technique for detecting neglected conditions based on our mining algorithm. Our new mining algorithm mines alternative patterns in example form \"P 1  or P 2 \", where P 1  and P 2  are alternative rules such as condition\u00a0\u2026", "num_citations": "137\n", "authors": ["186"]}
{"title": "A framework and tool supports for generating test inputs of AspectJ programs\n", "abstract": " Aspect-oriented software development is gaining popularity with the wider adoption of languages such as AspectJ. To reduce the manual effort of testing aspects in AspectJ programs, we have developed a framework, called Aspectra, that automates generation of test inputs for testing aspectual behavior, ie, the behavior implemented in pieces of advice or intertype methods defined in aspects. To test aspects, developers construct base classes into which the aspects are woven to form woven classes. Our approach leverages existing test-generation tools to generate test inputs for the woven classes; these test inputs indirectly exercise the aspects. To enable aspects to be exercised during test generation, Aspectra automatically synthesizes appropriate wrapper classes for woven classes. To assess the quality of the generated tests, Aspectra defines and measures aspectual branch coverage (branch coverage within\u00a0\u2026", "num_citations": "135\n", "authors": ["186"]}
{"title": "Automated behavioral regression testing\n", "abstract": " When a program is modified during software evolution, developers typically run the new version of the program against its existing test suite to validate that the changes made on the program did not introduce unintended side effects (i.e., regression faults). This kind of regression testing can be effective in identifying some regression faults, but it is limited by the quality of the existing test suite. Due to the cost of testing, developers build test suites by finding acceptable tradeoffs between cost and thoroughness of the tests. As a result, these test suites tend to exercise only a small subset of the program's functionality and may be inadequate for testing the changes in a program. To address this issue, we propose a novel approach called Behavioral Regression Testing (BERT). Given two versions of a program, BERT identifies behavioral differences between the two versions through dynamical analysis, in three steps\u00a0\u2026", "num_citations": "133\n", "authors": ["186"]}
{"title": "Automated extraction of security policies from natural-language software documents\n", "abstract": " Access Control Policies (ACP) specify which principals such as users have access to which resources. Ensuring the correctness and consistency of ACPs is crucial to prevent security vulnerabilities. However, in practice, ACPs are commonly written in Natural Language (NL) and buried in large documents such as requirements documents, not amenable for automated techniques to check for correctness and consistency. It is tedious to manually extract ACPs from these NL documents and validate NL functional requirements such as use cases against ACPs for detecting inconsistencies. To address these issues, we propose an approach, called Text2Policy, to automatically extract ACPs from NL software documents and resource-access information from NL scenario-based functional requirements. We conducted three evaluations on the collected ACP sentences from publicly available sources along with use cases\u00a0\u2026", "num_citations": "131\n", "authors": ["186"]}
{"title": "Characterizing Smartphone Usage Patterns from Millions of Android Users\n", "abstract": " he prevalence of smart devices has promoted the popular-ity of mobile applications (aka apps) in recent years. A number of interesting and important questions remain unan-swered, such as why a user likes/dislikes an app, how an app becomes popular or eventually perishes, how a user selects apps to install and interacts with them, how frequently an app is used and how much traffic it generates, etc. This paper presents an empirical analysis of app usage behaviors collected from millions of users of Wandoujia, a leading An-droid app marketplace in China. The dataset covers two types of user behaviors of using over 0.2 million Android apps, including (1) app management activities (ie, installa-tion, updating, and uninstallation) of over 0.8 million unique users and (2) app network traffic from over 2 million unique users. We explore multiple aspects of such behavior data and present interesting patterns of app\u00a0\u2026", "num_citations": "120\n", "authors": ["186"]}
{"title": "Checking inside the black box: Regression testing by comparing value spectra\n", "abstract": " Comparing behaviors of program versions has become an important task in software maintenance and regression testing. Black-box program outputs have been used to characterize program behaviors and they are compared over program versions in traditional regression testing. Program spectra have recently been proposed to characterize a program's behavior inside the black box. Comparing program spectra of program versions offers insights into the internal behavioral differences between versions. In this paper, we present a new class of program spectra, value spectra, that enriches the existing program spectra family. We compare the value spectra of a program's old version and new version to detect internal behavioral deviations in the new version. We use a deviation-propagation call tree to present the deviation details. Based on the deviation-propagation call tree, we propose two heuristics to locate\u00a0\u2026", "num_citations": "116\n", "authors": ["186"]}
{"title": "Augmenting automatically generated unit-test suites with regression oracle checking\n", "abstract": " A test case consists of two parts: a test input to exercise the program under test and a test oracle to check the correctness of the test execution. A test oracle is often in the form of executable assertions such as in the JUnit testing framework. Manually generated test cases are valuable in exposing program faults in the current program version or regression faults in future program versions. However, manually generated test cases are often insufficient for assuring high software quality. We can then use an existing test-generation tool to generate new test inputs to augment the existing test suite. However, without specifications these automatically generated test inputs often do not have test oracles for exposing faults. In this paper, we have developed an automatic approach and its supporting tool, called Orstra, for augmenting an automatically generated unit-test suite with regression oracle checking. The\u00a0\u2026", "num_citations": "114\n", "authors": ["186"]}
{"title": "Automated test generation for access control policies via change-impact analysis\n", "abstract": " Access control policies are increasingly written in specification languages such as XACML. To increase confidence in the correctness of specified policies, policy developers can conduct policy testing with some typical test inputs (in the form of requests) and check test outputs (in the form of responses) against expected ones. Unfortunately, manual test generation is tedious and manually generated tests are often not sufficient to exercise various policy behaviors. In this paper we present a novel framework and its supporting tool called Cirg that generates tests based on change- impact analysis. Our experimental results show that Cirg can effectively generate tests to achieve high structural coverage of policies and outperforms random test generation in terms of structural coverage and fault-detection capability.", "num_citations": "111\n", "authors": ["186"]}
{"title": "Joint voting prediction for questions and answers in CQA\n", "abstract": " Community Question Answering (CQA) sites have become valuable repositories that host a massive volume of human knowledge. How can we detect a high-value answer which clears the doubts of many users? Can we tell the user if the question s/he is posting would attract a good answer? In this paper, we aim to answer these questions from the perspective of the voting outcome by the site users. Our key observation is that the voting score of an answer is strongly positively correlated with that of its question, and such correlation could be in turn used to boost the prediction performance. Armed with this observation, we propose a family of algorithms to jointly predict the voting scores of questions and answers soon after they are posted in the CQA sites. Experimental evaluations demonstrate the effectiveness of our approaches.", "num_citations": "107\n", "authors": ["186"]}
{"title": "Internetware: A software paradigm for internet computing\n", "abstract": " To meet the needs of computing in the Internet environment, the Internetware software paradigm provides a set of technologies that support the development of applications with characteristics that are autonomous, cooperative, situational, evolvable, emergent, and trustworthy.", "num_citations": "105\n", "authors": ["186"]}
{"title": "Designing fast and scalable XACML policy evaluation engines\n", "abstract": " Most prior research on policies has focused on correctness. While correctness is an important issue, the adoption of policy-based computing may be limited if the resulting systems are not implemented efficiently and thus perform poorly. To increase the effectiveness and adoption of policy-based computing, in this paper, we propose fast policy evaluation algorithms that can be adapted to support various policy languages. In this paper, we focus on XACML policy evaluation because XACML has become the de facto standard for specifying access control policies, has been widely used on web servers, and is most complex among existing policy languages. We implemented our algorithms in a policy evaluation system called XEngine and conducted side-by-side comparison with Sun Policy Decision Point (PDP), the industrial standard for XACML policy evaluation. The results show that XEngine is orders of magnitude\u00a0\u2026", "num_citations": "88\n", "authors": ["186"]}
{"title": "Automated generation of pointcut mutants for testing pointcuts in AspectJ programs\n", "abstract": " Aspect-oriented programming (AOP) provides new modularization of software systems by encapsulating cross-cutting concerns. AspectJ, an AOP language, uses abstractions such as pointcuts, advice, and aspects to achieve AOPpsilas primary functionality. Faults in pointcuts can cause aspects to fail to satisfy their requirements. Hence, testing pointcuts is necessary in order to ensure correctness of aspects. In mutation testing of pointcuts (a type of fault-based pointcut testing), the number of mutants (i.e., variations) for pointcuts is usually large due to the usage of wildcards. It is tedious to manually identify effective mutants that are of appropriate strength and resemble closely the original pointcut expression, reflecting the kind of mistakes that developers may make. To reduce developerspsila effort in this process, we have developed a new framework that automatically identifies the strength of each pointcut and\u00a0\u2026", "num_citations": "88\n", "authors": ["186"]}
{"title": "Malware detection in adversarial settings: Exploiting feature evolutions and confusions in android apps\n", "abstract": " Existing techniques on adversarial malware generation employ feature mutations based on feature vectors extracted from malware. However, most (if not all) of these techniques suffer from a common limitation: feasibility of these attacks is unknown. The synthesized mutations may break the inherent constraints posed by code structures of the malware, causing either crashes or malfunctioning of malicious payloads. To address the limitation, we present Malware Recomposition Variation (MRV), an approach that conducts semantic analysis of existing malware to systematically construct new malware variants for malware detectors to test and strengthen their detection signatures/models. In particular, we use two variation strategies (ie, malware evolution attack and malware confusion attack) following structures of existing malware to enhance feasibility of the attacks. Upon the given malware, we conduct semantic\u00a0\u2026", "num_citations": "86\n", "authors": ["186"]}
{"title": "Conformance checking of access control policies specified in XACML\n", "abstract": " Access control is one of the most fundamental and widely used security mechanisms. Access control mechanisms control which principals such as users or processes have access to which resources in a system. To facilitate managing and maintaining access control, access control policies are increasingly written in specification languages such as XACML. The specification of access control policies itself is often a challenging problem. Furthermore, XACML is intentionally designed to be generic: it provides the freedom in describing access control policies, which are well-known or invented ones. But the flexibility and expressiveness provided by XACML come at the cost of complexity, verbosity, and lack of desirable-property enforcement. Often common properties for specific access control policies may not be satisfied when these policies are specified in XACML, causing the discrepancy between what the policy\u00a0\u2026", "num_citations": "84\n", "authors": ["186"]}
{"title": "Automated testing and response analysis ofweb services\n", "abstract": " Web services are a popular way of implementing a Service-Oriented Architecture (SOA), which has gained rapid adoption and support from leading companies in industry. Testing can be used to help assure both the corectness and robustness of a web service. Because manual testing is tedious, tools are needed to automate test generation and execution for web services. This paper presents a framework and its supporting tool for automaically generating and executing web-service requests and analyzing the subsequent request-response pairs. Given a service provider's Web Service Description Language (WSDL) specification, we first automatically generate neessary Java code to implement a client (service requestor). We then leverage automated unit test generation tools for Java to generate unit tests (including extreme, special, and random input values), and execute the generated unit tests, which in turn\u00a0\u2026", "num_citations": "82\n", "authors": ["186"]}
{"title": "SafeDrive: Online Driving Anomaly Detection From Large-Scale Vehicle Data\n", "abstract": " Identifying driving anomalies is of great significance for improving driving safety. The development of the Internet-of-Vehicle (IoV) technology has made it feasible to acquire big data from multiple vehicle sensors, and such big data play a fundamental role in identifying driving anomalies. Existing approaches are mainly based on either rules or supervised learning. However, such approaches often require labeled data, which are typically not available in big data scenarios. In addition, because driving behaviors differ under vehicle statuses (e.g., speed and gear position), to precisely model driving behaviors needs to fuse multiple sources of sensor data. To address these issues, in this paper, we propose SafeDrive, an online and status-aware approach, which does not require labeled data. From a historical dataset, SafeDrive statistically offline derives a state graph (SG) as a behavior model. Then, SafeDrive splits\u00a0\u2026", "num_citations": "76\n", "authors": ["186"]}
{"title": "Automated robustness testing of web services\n", "abstract": " Web services are a popular way of implementing a Service-Oriented Architecture (SOA), which has gained rapid adoption and support from leading industrial players such as IBM, Oracle, and Microsoft. Testing can be used to help assure both the correctness and robustness of a web service. Because manual testing is tedious, tools are needed to automate test generation and execution for web services. This paper presents a new framework for automatically generating and executing web-service requests. Given a service provider\u2019s WSDL, we first generate the necessary code to implement a client (service requestor). We then leverage existing automated unit test generation tools to generate unit tests and finally execute the generated unit tests, which in turn invoke the service under test. Our preliminary results show that we can quickly generate and execute webservice requests that may reveal robustness problems with no knowledge of the underlying web service implementation.", "num_citations": "72\n", "authors": ["186"]}
{"title": "Mutually enhancing test generation and specification inference\n", "abstract": " Generating effective tests and inferring likely program specifications are both difficult and costly problems. We propose an approach in which we can mutually enhance the tests and specifications that are generated by iteratively applying each in a feedback loop. In particular, we infer likely specifications from the executions of existing tests and use these specifications to guide automatic test generation. Then the existing tests, as well as the new tests, are used to infer new specifications in the subsequent iteration. The iterative process continues until there is no new test that violates specifications inferred in the previous iteration. Inferred specifications can guide test generation to focus on particular program behavior, reducing the scope of analysis; and newly generated tests can improve the inferred specifications. During each iteration, the generated tests that violate inferred specifications are collected to be\u00a0\u2026", "num_citations": "72\n", "authors": ["186"]}
{"title": "Automated test input generation for Android: are we really there yet in an industrial case?\n", "abstract": " Given the ever increasing number of research tools to automatically generate inputs to test Android applications (or simply apps), researchers recently asked the question\" Are we there yet?\"(in terms of the practicality of the tools). By conducting an empirical study of the various tools, the researchers found that Monkey (the most widely used tool of this category in industrial practices) outperformed all of the research tools that they studied. In this paper, we present two significant extensions of that study. First, we conduct the first industrial case study of applying Monkey against WeChat, a popular messenger app with over 762 million monthly active users, and report the empirical findings on Monkey's limitations in an industrial setting. Second, we develop a new approach to address major limitations of Monkey and accomplish substantial code-coverage improvements over Monkey, along with empirical insights for\u00a0\u2026", "num_citations": "70\n", "authors": ["186"]}
{"title": "Learning for test prioritization: an industrial case study\n", "abstract": " Modern cloud-software providers, such as Salesforce. com, increasingly adopt large-scale continuous integration environments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimization mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in industrial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce. com shows that our approach significantly outperforms existing individual techniques.", "num_citations": "67\n", "authors": ["186"]}
{"title": "An empirical study on evolution of API documentation\n", "abstract": " With the evolution of an API library, its documentation also evolves. The evolution of API documentation is common knowledge for programmers and library developers, but not in a quantitative form. Without such quantitative knowledge, programmers may neglect important revisions of API documentation, and library developers may not effectively improve API documentation based on its revision histories. There is a strong need to conduct a quantitative study on API documentation evolution. However, as API documentation is large in size and revisions can be complicated, it is quite challenging to conduct such a study. In this paper, we present an analysis methodology to analyze the evolution of API documentation. Based on the methodology, we conduct a quantitative study on API documentation evolution of five widely used real-world libraries. The results reveal various valuable findings, and these\u00a0\u2026", "num_citations": "67\n", "authors": ["186"]}
{"title": "Defining and measuring policy coverage in testing access control policies\n", "abstract": " To facilitate managing access control in a system, security officers increasingly write access control policies in specification languages such as XACML, and use a dedicated software component called a Policy Decision Point (PDP). To increase confidence on written policies, certain types of policy testing (often in an ad hoc way) are usually conducted, which probe the PDP with some typical requests and check PDP\u2019s responses against expected ones. This paper develops a first step toward systematic policy testing by defining and measuring policy coverage when testing policies. We have developed a coverage-measurement tool to measure policy coverage given a set of XACML policies and a set of requests. We have developed a tool for request generation, which randomly generates requests for a given set of policies, and a tool for request reduction, which greedily selects a nearly minimal set of\u00a0\u2026", "num_citations": "64\n", "authors": ["186"]}
{"title": "Generating program inputs for database application testing\n", "abstract": " Testing is essential for quality assurance of database applications. Achieving high code coverage of the database application is important in testing. In practice, there may exist a copy of live databases that can be used for database application testing. Using an existing database state is desirable since it tends to be representative of real-world objects' characteristics, helping detect faults that could cause failures in real-world settings. However, to cover a specific program code portion (e.g., block), appropriate program inputs also need to be generated for the given existing database state. To address this issue, in this paper, we propose a novel approach that generates program inputs for achieving high code coverage of a database application, given an existing database state. Our approach uses symbolic execution to track how program inputs are transformed before appearing in the executed SQL queries and how\u00a0\u2026", "num_citations": "60\n", "authors": ["186"]}
{"title": "Mining API error-handling specifications from source code\n", "abstract": " API error-handling specifications are often not documented, necessitating automated specification mining. Automated mining of error-handling specifications is challenging for procedural languages such as C, which lack explicit exception-handling mechanisms. Due to the lack of explicit exception handling, error-handling code is often scattered across different procedures and files making it difficult to mine error-handling specifications through manual inspection of source code. In this paper, we present a novel framework for mining API error-handling specifications automatically from API client code, without any user input. In our framework, we adapt a trace generation technique to distinguish and generate static traces representing different API run-time behaviors. We apply data mining techniques on the static traces to mine specifications that define correct handling of API errors. We then use the mined\u00a0\u2026", "num_citations": "57\n", "authors": ["186"]}
{"title": "ACPT: A tool for modeling and verifying access control policies\n", "abstract": " Access control mechanisms are a widely adopted technology for information security. Since access decisions (i.e., permit or deny) on requests are dependent on access control policies, ensuring the correct modeling and implementation of access control policies is crucial for adopting access control mechanisms. To address this issue, we develop a tool, called ACPT (Access Control Policy Testing), that helps to model and implement policies correctly during policy modeling, implementation, and verification.", "num_citations": "55\n", "authors": ["186"]}
{"title": "Software analytics for incident management of online services: An experience report\n", "abstract": " As online services become more and more popular, incident management has become a critical task that aims to minimize the service downtime and to ensure high quality of the provided services. In practice, incident management is conducted through analyzing a huge amount of monitoring data collected at runtime of a service. Such data-driven incident management faces several significant challenges such as the large data scale, complex problem space, and incomplete knowledge. To address these challenges, we carried out two-year software-analytics research where we designed a set of novel data-driven techniques and developed an industrial system called the Service Analysis Studio (SAS) targeting real scenarios in a large-scale online service of Microsoft. SAS has been deployed to worldwide product datacenters and widely used by on-call engineers for incident management. This paper shares our\u00a0\u2026", "num_citations": "54\n", "authors": ["186"]}
{"title": "An Empirical Study of Android Test Generation Tools in Industrial Cases\n", "abstract": " User Interface (UI) testing is a popular approach to ensure the quality of mobile apps. Numerous test generation tools have been developed to support UI testing on mobile apps, especially for Android apps. Previous work evaluates and compares different test generation tools using only relatively simple open-source apps, while real-world industrial apps tend to have more complex functionalities and implementations. There is no direct comparison among test generation tools with regard to effectiveness and ease-of-use on these industrial apps. To address such limitation, we study existing state-of-the-art or state-of-the-practice test generation tools on 68 widely-used industrial apps. We directly compare the tools with regard to code coverage and fault-detection ability. According to our results, Monkey, a state-of-the-practice tool from Google, achieves the highest method coverage on 22 of 41 apps whose method\u00a0\u2026", "num_citations": "48\n", "authors": ["186"]}
{"title": "Automatic extraction of object-oriented observer abstractions from unit-test executions\n", "abstract": " Unit testing has become a common step in software development. Although manually created unit tests are valuable, they are often insufficient; therefore, programmers can use an automatic unit-test-generation tool to produce a large number of additional tests for a class. However, without a priori specifications, programmers cannot practically inspect the execution of each automatically generated test. In this paper, we develop the observer abstraction approach for automatically extracting object-state-transition information of a class from unit-test executions, without requiring a priori specifications. Given a class and a set of its initial tests generated by a third-party tool, we generate new tests to augment the initial tests and produce the abstract state of an object based on the return values of a set of observers (public methods with non-void returns) invoked on the object. From the executions of both the new\u00a0\u2026", "num_citations": "47\n", "authors": ["186"]}
{"title": "PRADA: prioritizing android devices for apps by mining large-scale usage data\n", "abstract": " Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model\u00a0\u2026", "num_citations": "46\n", "authors": ["186"]}
{"title": "Perturbation-based user-input-validation testing of web applications\n", "abstract": " User-input-validation (UIV) is the first barricade that protects web applications from application-level attacks. Most UIV test tools cannot detect semantics-related vulnerabilities in validators, such as filling a five-digit number to a field that accepts a year. To address this issue, we propose a new approach to generate test inputs for UIV based on the analysis of client-side information. In particular, we use input-field information to generate valid inputs, and then perturb valid inputs to generate invalid test inputs. We conducted an empirical study to evaluate our approach. The empirical result shows that, in comparison to existing vulnerability scanners, our approach is more effective than existing vulnerability scanners in finding semantics-related vulnerabilities of UIV for web applications.", "num_citations": "46\n", "authors": ["186"]}
{"title": "Neural detection of semantic code clones via tree-based convolution\n", "abstract": " Code clones are similar code fragments that share the same semantics but may differ syntactically to various degrees. Detecting code clones helps reduce the cost of software maintenance and prevent faults. Various approaches of detecting code clones have been proposed over the last two decades, but few of them can detect semantic clones, i.e., code clones with dissimilar syntax. Recent research has attempted to adopt deep learning for detecting code clones, such as using tree-based LSTM over Abstract Syntax Tree (AST). However, it does not fully leverage the structural information of code fragments, thereby limiting its clone-detection capability. To fully unleash the power of deep learning for detecting code clones, we propose a new approach that uses tree-based convolution to detect semantic clones, by capturing both the structural information of a code fragment from its AST and lexical information from\u00a0\u2026", "num_citations": "44\n", "authors": ["186"]}
{"title": "Understanding diverse usage patterns from large-scale appstore-service profiles\n", "abstract": " The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app-store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be\u00a0\u2026", "num_citations": "44\n", "authors": ["186"]}
{"title": "Guided test generation for database applications via synthesized database interactions\n", "abstract": " Testing database applications typically requires the generation of tests consisting of both program inputs and database states. Recently, a testing technique called Dynamic Symbolic Execution (DSE) has been proposed to reduce manual effort in test generation for software applications. However, applying DSE to generate tests for database applications faces various technical challenges. For example, the database application under test needs to physically connect to the associated database, which may not be available for various reasons. The program inputs whose values are used to form the executed queries are not treated symbolically, posing difficulties for generating valid database states or appropriate database states for achieving high coverage of query-result-manipulation code. To address these challenges, in this article, we propose an approach called SynDB that synthesizes new database interactions\u00a0\u2026", "num_citations": "44\n", "authors": ["186"]}
{"title": "First step towards automatic correction of firewall policy faults\n", "abstract": " Firewalls are critical components of network security and have been widely deployed for protecting private networks. A firewall determines whether to accept or discard a packet that passes through it based on its policy. However, most real-life firewalls have been plagued with policy faults, which either allow malicious traffic or block legitimate traffic. Due to the complexity of firewall policies, manually locating the faults of a firewall policy and further correcting them are difficult. Automatically correcting the faults of a firewall policy is an important and challenging problem. In this article, we first propose a fault model for firewall policies including five types of faults. For each type of fault, we present an automatic correction technique. Second, we propose the first systematic approach that employs these five techniques to automatically correct all or part of the misclassified packets of a faulty firewall policy. Third, we\u00a0\u2026", "num_citations": "42\n", "authors": ["186"]}
{"title": "Multiple-implementation testing for XACML implementations\n", "abstract": " Many Web applications enhance their security via access-control systems. XACML is a standardized policy language, which has been widely used in access-control systems. In an XACML-based access-control system, policies, requests, and responses are written in XACML. An XACML implementation implements XACML functionalities to validate XACML requests against XACML policies. To ensure the quality of an XACML-based access-control system, we need an effective means to test whether the XACML implementation correctly implements XACML functionalities. The test inputs of an XACML implementation are XACML policies and requests. The test outputs are XACML responses. This paper proposes an approach to detect defects in XACML implementations via observing the behaviors of different XACML implementations for the same test inputs. As XACML has been widely used, we can collect different\u00a0\u2026", "num_citations": "42\n", "authors": ["186"]}
{"title": "Mining interface specifications for generating checkable robustness properties\n", "abstract": " A software system interacts with its environment through interfaces. Improper handling of exceptional returns from system interfaces can cause robustness problems. Robustness of software systems are governed by various temporal properties related to interfaces. Static verification has been shown to be effective in checking these temporal properties. But manually specifying these properties is cumbersome and requires the knowledge of interface specifications, which are often either unavailable or undocumented. In this paper, we propose a novel framework to automatically infer system-specific interface specifications from program source code. We use a model checker to generate traces related to the interfaces. From these model checking traces, we infer interface specification details such as return value on success or failure. Based on these inferred specifications, we translate generically specified interface\u00a0\u2026", "num_citations": "42\n", "authors": ["186"]}
{"title": "Helping users avoid bugs in GUI applications\n", "abstract": " In this paper, we propose a method to help users avoid bugs in GUI applications. In particular, users would use the application normally and report bugs that they encounter to prevent anyone - including themselves - from encountering those bugs again. When a user attempts an action that has led to problems in the past, he/she will receive a warning and will be given the opportunity to abort the action - thus avoiding the bug altogether and keeping the application stable. Of course, bugs should be fixed eventually by the application developers, but our approach allows application users to collaboratively help each other avoid bugs - thus making the application more usable in the meantime. We demonstrate this approach using our \"Stabilizer\" prototype. We also include a preliminary evaluation of the Stabilizer's bug prediction.", "num_citations": "42\n", "authors": ["186"]}
{"title": "Automated test input generation for Android: Towards getting there in an industrial case\n", "abstract": " Monkey, a random testing tool from Google, has been popularly used in industrial practices for automatic test input generation for Android due to its applicability to a variety of application settings, e.g., ease of use and compatibility with different Android platforms. Recently, Monkey has been under the spotlight of the research community: recent studies found out that none of the studied tools from the academia were actually better than Monkey when applied on a set of open source Android apps. Our recent efforts performed the first case study of applying Monkey on WeChat, a popular messenger app with over 800 million monthly active users, and revealed many limitations of Monkey along with developing our improved approach to alleviate some of these limitations. In this paper, we explore two optimization techniques to improve the effectiveness and efficiency of our previous approach. We also conduct manual\u00a0\u2026", "num_citations": "41\n", "authors": ["186"]}
{"title": "Systematic structural testing of firewall policies\n", "abstract": " Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. As the quality of protection provided by a firewall directly depends on the quality of its policy (i.e., configuration), ensuring the correctness of security policies is important and yet difficult.To help ensure the correctness of a firewall policy, we propose a systematic structural testing approach for firewall policies. We define structural coverage (based on coverage criteria of rules, predicates, and clauses) on the policy under test. Considering achieving higher structural coverage effectively, we develop three automated packet generation techniques: the random packet generation, the one based on local constraint solving (considering individual rules locally in a policy), and the most sophisticated one based on global constraint solving (considering multiple rules globally in a policy).We have conducted\u00a0\u2026", "num_citations": "41\n", "authors": ["186"]}
{"title": "Measurement and Analysis of Mobile Web Cache Performance\n", "abstract": " The Web browser is a killer app on mobile devices such as smartphones. However, the user experience of mobile Web browsing is undesirable because of the slow resource loading. To improve the performance of Web resource loading, caching has been adopted as a key mechanism. However, the existing passive measurement studies cannot comprehensively characterize the performance of mobile Web caching. For example, most of these studies mainly focus on client-side implementations but not server-side configurations, suffer from biased user behaviors, and fail to study\" miscached\" resources. To address these issues, in this paper, we present a proactive approach for a comprehensive measurement study on mobile Web cache performance. The key idea of our approach is to proactively crawl resources from hundreds of websites periodically with a fine-grained time interval. Thus, we are able to uncover\u00a0\u2026", "num_citations": "40\n", "authors": ["186"]}
{"title": "Towards regression test selection for AspectJ programs\n", "abstract": " Regression testing aims at showing that code has not been adversely affected by modification activities during maintenance. Regression test selection techniques reuse tests from an existing test suite to test a modified program. By reusing such a test suite to retest modified programs, maintainers or testers can reduce the required testing effort. This paper presents a regression test selection technique for AspectJ programs. The technique is based on various types of control flow graphs that can be used to select from the original test suite test cases that execute changed code for the new version of the AspectJ program. The code-base technique operates on the control flow graphs of AspectJ programs. The technique can be applied to modified individual aspects or classes as well as the whole program that uses modified aspects or classes.", "num_citations": "40\n", "authors": ["186"]}
{"title": "A characteristic study on failures of production distributed data-parallel programs\n", "abstract": " SCOPE is adopted by thousands of developers from tens of different product teams in Microsoft Bing for daily web-scale data processing, including index building, search ranking, and advertisement display. A SCOPE job is composed of declarative SQL-like queries and imperative C# user-defined functions (UDFs), which are executed in pipeline by thousands of machines. There are tens of thousands of SCOPE jobs executed on Microsoft clusters per day, while some of them fail after a long execution time and thus waste tremendous resources. Reducing SCOPE failures would save significant resources. This paper presents a comprehensive characteristic study on 200 SCOPE failures/fixes and 50 SCOPE failures with debugging statistics from Microsoft Bing, investigating not only major failure types, failure sources, and fixes, but also current debugging practice. Our major findings include (1) most of the failures (84\u00a0\u2026", "num_citations": "39\n", "authors": ["186"]}
{"title": "\u9752\u9e1f\u6784\u4ef6\u5e93\u7684\u6784\u4ef6\u5ea6\u91cf\n", "abstract": " \u5bf9\u8f6f\u4ef6\u4ea7\u4e1a\u53d1\u5c55\u6240\u9700\u8981\u7684\u8f6f\u4ef6\u751f\u4ea7\u7387\u548c\u8f6f\u4ef6\u8d28\u91cf\u7684\u91cd\u89c6\u8d77\u5230\u4e86\u5bf9\u8f6f\u4ef6\u590d\u7528\u6280\u672f\u7684\u7814\u7a76,\u540c\u65f6,\u6709\u5173\u590d\u7528\u5ea6\u91cf\u7684\u7814\u7a76\u548c\u5e94\u7528\u4e5f\u5f15\u8d77\u4e86\u5e7f\u6cdb\u7684\u91cd\u89c6.\u9752\u5c9b\u6784\u4ef6\u5e93\u7cfb\u7edf\u53ef\u4ee5\u5bf9\u590d\u7528\u8f6f\u4ef6\u8fdb\u884c\u63cf\u8ff0,\u7ba1\u7406,\u5b58\u50a8\u548c\u68c0\u7d22,\u7528\u4ee5\u6ee1\u8db3\u57fa\u4e8e\u201c\u6784\u4ef6-\u6784\u67b6\u201d\u590d\u7528\u7684\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u7684\u9700\u8981.", "num_citations": "39\n", "authors": ["186"]}
{"title": "Locating need-to-translate constant strings for software internationalization\n", "abstract": " Modern software applications require internationalization to be distributed to different regions of the world. In various situations, many software applications are not internationalized at early stages of development. To internationalize such an existing application, developers need to externalize some hard-coded constant strings to resource files, so that translators can easily translate the application into a local language without modifying its source code. Since not all the constant strings require externalization, locating those need-to-translate constant strings is a necessary task that developers must complete for internationalization. In this paper, we present an approach to automatically locating need-to-translate constant strings. Our approach first collects a list of API methods related to the graphical user interface (GUI), and then searches for need-to-translate strings from the invocations of these API methods based on\u00a0\u2026", "num_citations": "37\n", "authors": ["186"]}
{"title": "Automatic extraction of abstract-object-state machines from unit-test executions\n", "abstract": " An automatic test-generation tool can produce a large number of test inputs to exercise the class under test. However, without specifications, developers cannot inspect the execution of each automatically generated test input practically. To address the problem, we have developed an automatic test abstraction tool, called Abstra, to extract high level object-state-transition information from unit-test executions, without requiring a priori specifications. Given a class and a set of its generated test inputs, our tool extracts object state machines (OSM): a state in an OSM represents an object state of the class and a transition in an OSM represents method calls of the class. When an object state in an OSM is concrete (being represented by the values of all fields reachable from the object), the size of the OSM could be too large to be useful for inspection. To address this issue, we have developed techniques in the tool to\u00a0\u2026", "num_citations": "37\n", "authors": ["186"]}
{"title": "PolicyLint: Investigating Internal Privacy Policy Contradictions on Google Play\n", "abstract": " Privacy policies are the primary mechanism by which companies inform users about data collection and sharing practices. To help users better understand these long and complex legal documents, recent research has proposed tools that summarize collection and sharing. However, these tools have a significant oversight: they do not account for contradictions that may occur within an individual policy. In this paper, we present PolicyLint, a privacy policy analysis tool that identifies such contradictions by simultaneously considering negation and varying semantic levels of data objects and entities. To do so, PolicyLint automatically generates ontologies from a large corpus of privacy policies and uses sentence-level natural language processing to capture both positive and negative statements of data collection and sharing. We use PolicyLint to analyze the policies of 11,430 apps and find that 14.2% of these policies contain contradictions that may be indicative of misleading statements. We manually verify 510 contradictions, identifying concerning trends that include the use of misleading presentation, attempted redefinition of common understandings of terms, conflicts in regulatory definitions (eg, US and EU), and\" laundering\" of tracking information facilitated by sharing or collecting data that can be used to derive sensitive information. In doing so, PolicyLint significantly advances automated analysis of privacy policies.", "num_citations": "36\n", "authors": ["186"]}
{"title": "Multiple-Implementation Testing of Supervised Learning Software\n", "abstract": " Machine Learning (ML) algorithms are now used in a wide range of application domains in society. Naturally, software implementations of these algorithms have become ubiquitous. Faults in ML software can cause substantial losses in these application domains. Thus, it is very critical to conduct effective testing of ML software to detect and eliminate its faults. However, testing ML software is difficult, partly because producing test oracles used for checking behavior correctness (such as using expected properties or expected test outputs) is challenging. In this paper, we propose an approach of multiple-implementation testing to test supervised learning software, a major type of ML software. In particular, our approach derives a test input's proxy oracle from the majority-voted output running the test input of multiple implementations of the same algorithm (based on a pre-defined percentage threshold). Our approach reports likely those test inputs whose outputs (produced by an implementation under test) are different from the majority-voted outputs as failing tests. We evaluate our approach on two highly popular supervised learning algorithms: k-Nearest Neighbor (kNN) and Naive Bayes (NB). Our results show that our approach is highly effective in detecting faults in real-world supervised learning software. In particular, our approach detects 13 real faults and 1 potential fault from 19 kNN implementations and 16 real faults from 7 NB implementations. Our approach can even detect 7 real faults and 1 potential fault among the three popularly used open-source ML projects (Weka, RapidMiner, and KNIME).", "num_citations": "36\n", "authors": ["186"]}
{"title": "Relationship-aware code search for JavaScript frameworks\n", "abstract": " JavaScript frameworks, such as jQuery, are widely used for developing web applications. To facilitate using these JavaScript frameworks to implement a feature (eg, functionality), a large number of programmers often search for code snippets that implement the same or similar feature. However, existing code search approaches tend to be ineffective, without taking into account the fact that JavaScript code snippets often implement a feature based on various relationships (eg, sequencing, condition, and callback relationships) among the invoked framework API methods. To address this issue, we present a novel Relationship-Aware Code Search (RACS) approach for finding code snippets that use JavaScript frameworks to implement a specific feature. In advance, RACS collects a large number of code snippets that use some JavaScript frameworks, mines API usage patterns from the collected code snippets, and\u00a0\u2026", "num_citations": "35\n", "authors": ["186"]}
{"title": "Automatic construction of an effective training set for prioritizing static analysis warnings\n", "abstract": " In order to improve ineffective warning prioritization of static analysis tools, various approaches have been proposed to compute a ranking score for each warning. In these approaches, an effective training set is vital in exploring which factors impact the ranking score and how. While manual approaches to build a training set can achieve high effectiveness but suffer from low efficiency (ie, high cost), existing automatic approaches suffer from low effectiveness. In this paper, we propose an automatic approach for constructing an effective training set. In our approach, we select three categories of impact factors as input attributes of the training set, and propose a new heuristic for identifying actionable warnings to automatically label the training set. Our empirical evaluations show that the precision of the top 22 warnings for Lucene, 20 for ANT, and 6 for Spring can achieve 100% with the help of our constructed training set.", "num_citations": "35\n", "authors": ["186"]}
{"title": "Inferring access-control policy properties via machine learning\n", "abstract": " To ease the burden of implementing and maintaining access-control aspects in a system, a growing trend among developers is to write access-control policies in a specification language such as XACML and integrate the policies with applications through the use of a policy decision point (PDP). To assure that the specified polices reflect the expected ones, recent research has developed policy verification tools; however, their applications in practice are still limited, being constrained by the limited set of supported policy language features and the unavailability of policy properties. This paper presents a data-mining approach to the problem of verifying that expressed access-control policies reflect the true desires of the policy author. We developed a tool to investigate this approach by automatically generating requests, evaluating those requests to get responses, and applying machine learning on the request\u00a0\u2026", "num_citations": "35\n", "authors": ["186"]}
{"title": "A study of grayware on google play\n", "abstract": " While there have been various studies identifying and classifying Android malware, there is limited discussion of the broader class of apps that fall in a gray area. Mobile grayware is distinct from PC grayware due to differences in operating system properties. Due to mobile grayware's subjective nature, it is difficult to identify mobile grayware via program analysis alone. Instead, we hypothesize enhancing analysis with text analytics can effectively reduce human effort when triaging grayware. In this paper, we design and implement heuristics for seven main categories of grayware. We then use these heuristics to simulate grayware triage on a large set of apps from Google Play. We then present the results of our empirical study, demonstrating a clear problem of grayware. In doing so, we show how even relatively simple heuristics can quickly triage apps that take advantage of users in an undesirable way.", "num_citations": "34\n", "authors": ["186"]}
{"title": "Inferring dependency constraints on parameters for web services\n", "abstract": " Recently many popular websites such as Twitter and Flickr expose their data through web service APIs, enabling third-party organizations to develop client applications that provide function-alities beyond what the original websites offer. These client appli-cations should follow certain constraints in order to correctly in-teract with the web services. One common type of such constraints is Dependency Constraints on Parameters. Given a web service operation O and its parameters Pi, Pj, these constraints describe the requirement on one parameter Pi that is dependent on the conditions of some other parameter (s) Pj. For example, when requesting the Twitter operation\" GET statuses/user_timeline\", a user_id parameter must be provided if a screen_name parameter is not provided. Violations of such constraints can cause fatal errors or incorrect results in the client applications. However, these con-straints are often not\u00a0\u2026", "num_citations": "34\n", "authors": ["186"]}
{"title": "Automated inference of pointcuts in aspect-oriented refactoring\n", "abstract": " Software refactoring is the process of reorganizing the internal structure of code while preserving the external behavior. Aspect-Oriented Programming (AOP) provides new modularization of software systems by encapsulating crosscutting concerns. Based on these two techniques, aspect-oriented (AO) refactoring restructures crosscutting elements in code. AO refactoring includes two steps: aspect mining (identification of aspect candidates in code) and aspect refactoring (semantic-preserving transformation to migrate the aspect-candidate code to AO code). Aspect refactoring clusters similar join points together for the aspect candidates and encapsulates each cluster with an effective pointcut definition. With the increase in size of the code and crosscutting concerns, it is tedious to manually identify aspects and their corresponding join points, cluster the join points, and infer pointcut expressions. Therefore, there is a\u00a0\u2026", "num_citations": "33\n", "authors": ["186"]}
{"title": "Contextual analysis of program logs for understanding system behaviors\n", "abstract": " Understanding the behaviors of a software system is very important for performing daily system maintenance tasks. In practice, one way to gain knowledge about the runtime behavior of a system is to manually analyze system logs collected during the system executions. With the increasing scale and complexity of software systems, it has become challenging for system operators to manually analyze system logs. To address these challenges, in this paper, we propose a new approach for contextual analysis of system logs for understanding a system's behaviors. In particular, we first use execution patterns to represent execution structures reflected by a sequence of system logs, and propose an algorithm to mine execution patterns from the program logs. The mined execution patterns correspond to different execution paths of the system. Based on these execution patterns, our approach further learns essential\u00a0\u2026", "num_citations": "32\n", "authors": ["186"]}
{"title": "Healing online service systems via mining historical issue repositories\n", "abstract": " Online service systems have been increasingly popular and important nowadays, with an increasing demand on the availability of services provided by these systems, while significant efforts have been made to strive for keeping services up continuously. Therefore, reducing the MTTR (Mean Time to Restore) of a service remains the most important step to assure the user-perceived availability of the service. To reduce the MTTR, a common practice is to restore the service by identifying and applying an appropriate healing action (ie, a temporary workaround action such as rebooting a SQL machine). However, manually identifying an appropriate healing action for a given new issue (such as service down) is typically time consuming and error prone. To address this challenge, in this paper, we present an automated mining-based approach for suggesting an appropriate healing action for a given new issue. Our\u00a0\u2026", "num_citations": "32\n", "authors": ["186"]}
{"title": "Random unit-test generation with MUT-aware sequence recommendation\n", "abstract": " A key component of automated object-oriented unit-test generation is to find method-call sequences that generate desired inputs of a method under test (MUT). Previous work cannot find desired sequences effectively due to the large search space of possible sequences. To address this issue, we present a MUT-aware sequence recommendation approach called RecGen to improve the effectiveness of random object-oriented unit-test generation. Unlike existing random testing approaches that select sequences without considering how a MUT may use inputs generated from sequences, RecGen analyzes object fields accessed by a MUT and recommends a short sequence that mutates these fields. In addition, for MUTs whose test generation keeps failing, RecGen recommends a set of sequences to cover all the methods that mutate object fields accessed by the MUT. This technique further improves the chance of\u00a0\u2026", "num_citations": "32\n", "authors": ["186"]}
{"title": "Demystifying the imperfect client-side cache performance of mobile web browsing\n", "abstract": " The web browser is one of the most significant applications on mobile devices such as smartphones. However, the user experience of mobile web browsing is undesirable because of the slow resource loading. To improve the performance of web resource loading, client-side cache has been adopted as a key mechanism. However, the existing passive measurement studies cannot comprehensively characterize the \u201cclient-side\u201d cache performance of mobile web browsing. For example, most of these studies mainly focus on client-side implementations but not server-side configurations, suffer from biased user behaviors, and fail to study \u201cmiscached\u201d resources. To address these issues, in this article, we present a proactive approach to making a comprehensive measurement study on client-side cache performance. The key idea of our approach is to proactively crawl resources from hundreds of websites periodically\u00a0\u2026", "num_citations": "31\n", "authors": ["186"]}
{"title": "Automatic test generation for mutation testing on database applications\n", "abstract": " To assure high quality of database applications, testing database applications remains the most popularly used approach. In testing database applications, tests consist of both program inputs and database states. Assessing the adequacy of tests allows targeted generation of new tests for improving their adequacy (e.g., fault-detection capabilities). Comparing to code coverage criteria, mutation testing has been a stronger criterion for assessing the adequacy of tests. Mutation testing would produce a set of mutants (each being the software under test systematically seeded with a small fault) and then measure how high percentage of these mutants are killed (i.e., detected) by the tests under assessment. However, existing test-generation approaches for database applications do not provide sufficient support for killing mutants in database applications (in either program code or its embedded or resulted SQL queries\u00a0\u2026", "num_citations": "31\n", "authors": ["186"]}
{"title": "Database state generation via dynamic symbolic execution for coverage criteria\n", "abstract": " Automatically generating sufficient database states is imperative to reduce human efforts in testing database applications. Complementing the traditional block or branch coverage, we develop an approach that generates database states to achieve advanced code coverage including boundary value coverage (BVC) and logical coverage (LC) for source code under test. In our approach, we leverage dynamic symbolic execution to examine close relationships among host variables, embedded SQL query statements, and branch conditions in source code. We then derive constraints such that data satisfying those constraints can achieve the target coverage criteria. We implement our approach upon Pex, which is a state-of-the-art DSE-based test-generation tool for .NET. Empirical evaluations on two real database applications show that our approach assists Pex to generate test database states that can effectively\u00a0\u2026", "num_citations": "31\n", "authors": ["186"]}
{"title": "Assessing quality of policy properties in verification of access control policies\n", "abstract": " Access control policies are often specified in declarative languages. In this paper, we propose a novel approach, called mutation verification, to assess the quality of properties specified for a policy and, in doing so, the quality of the verification itself. In our approach, given a policy and a set of properties, we first mutate the policy to generate various mutant policies, each with a single seeded fault. We then verify whether the properties hold for each mutant policy. If the properties still hold for a given mutant policy, then the quality of these properties is determined to be insufficient in guarding against the seeded fault, indicating that more properties are needed to augment the existing set of properties to provide higher confidence of the policy correctness. We have implemented Mutaver, a mutation verification tool for XACML, and applied it to policies and properties from a real-world software system.", "num_citations": "31\n", "authors": ["186"]}
{"title": "PerfRanker: prioritization of performance regression tests for collection-intensive software\n", "abstract": " Regression performance testing is an important but time/resource-consuming phase during software development. Developers need to detect performance regressions as early as possible to reduce their negative impact and fixing cost. However, conducting regression performance testing frequently (eg, after each commit) is prohibitively expensive. To address this issue, in this paper, we propose PerfRanker, the first approach to prioritizing test cases in performance regression testing for collection-intensive software, a common type of modern software heavily using collections. Our test prioritization is based on performance impact analysis that estimates the performance impact of a given code revision on a given test execution. Evaluation shows that our approach can cover top 3 test cases whose performance is most affected within top 30% to 37% prioritized test cases, in contrast to top 65% to 79% by 3 baseline\u00a0\u2026", "num_citations": "29\n", "authors": ["186"]}
{"title": "Performance issue diagnosis for online service systems\n", "abstract": " Monitoring and diagnosing performance issues of an online service system are critical to assure satisfactory performance of the system. Given a detected performance issue and collected system metrics for an online service system, engineers usually need to make great efforts to conduct diagnosis by first identifying performance issue beacons, which are metrics that pinpoint to the root causes. In order to reduce the manual efforts, in this paper, we propose a new approach to effectively detecting performance issue beacons to help with performance issue diagnosis. Our approach includes techniques for mining system metric data to address limitations when applying previous classification-based approaches. Our evaluations on both a controlled environment and a real production environment show that our approach can more effectively identify performance issue beacons from system metric data than previous\u00a0\u2026", "num_citations": "29\n", "authors": ["186"]}
{"title": "CarStream: An Industrial System of Big Data Processing for Internet-of-Vehicles\n", "abstract": " As the Internet-of-Vehicles (IoV) technology becomes an increasingly important trend for future transportation, designing large-scale IoV systems has become a critical task that aims to process big data uploaded by fleet vehicles and to provide data-driven services. The IoV data, especially high-frequency vehicle statuses (e.g., location, engine parameters), are characterized as large volume with a low density of value and low data quality. Such characteristics pose challenges for developing real-time applications based on such data. In this paper, we address the challenges in designing a scalable IoV system by describing CarStream, an industrial system of big data processing for chauffeured car services. Connected with over 30,000 vehicles, CarStream collects and processes multiple types of driving data including vehicle status, driver activity, and passenger-trip information. Multiple services are provided based\u00a0\u2026", "num_citations": "28\n", "authors": ["186"]}
{"title": "Iterative mining of resource-releasing specifications\n", "abstract": " Software systems commonly use resources such as network connections or external file handles. Once finish using the resources, the software systems must release these resources by explicitly calling specific resource-releasing API methods. Failing to release resources properly could result in resource leaks or even outright system failures. Existing verification techniques could analyze software systems to detect defects related to failing to release resources. However, these techniques require resource-releasing specifications for specifying which API method acquires/releases certain resources, and such specifications are not well documented in practice, due to the large amount of manual effort required to document them. To address this issue, we propose an iterative mining approach, called RRFinder, to automatically mining resource-releasing specifications for API libraries in the form of (resource-acquiring\u00a0\u2026", "num_citations": "27\n", "authors": ["186"]}
{"title": "UiRef: analysis of sensitive user inputs in Android applications\n", "abstract": " Mobile applications frequently request sensitive data. While prior work has focused on analyzing sensitive-data uses originating from well-defined API calls in the system, the security and privacy implications of inputs requested via application user interfaces have been widely unexplored. In this paper, our goal is to understand the broad implications of such requests in terms of the type of sensitive data being requested by applications.", "num_citations": "25\n", "authors": ["186"]}
{"title": "Transferring code-clone detection and analysis to practice\n", "abstract": " During software development, code clones are commonly produced, in the form of a number of the same or similar code fragments spreading within one or many large code bases. Numerous research projects have been carried out on empirical studies or tool support for detecting or analyzing code clones. However, in practice, few such research projects have resulted in substantial industry adoption. In this paper, we report our experiences of transferring XIAO, a code-clone detection and analysis approach and its supporting tool, to road industrial practices: (1) shipped in Visual Studio 2012, a widely used industrial IDE, (2) deployed and intensively used at the Microsoft Security Response Center. According to our experiences, technology transfer is a rather complicated journey that needs significant efforts from both the technical aspect and social aspect. From the technical aspect, significant efforts are needed to\u00a0\u2026", "num_citations": "25\n", "authors": ["186"]}
{"title": "Mining historical issue repositories to heal large-scale online service systems\n", "abstract": " Online service systems have been increasingly popular and important nowadays. Reducing the MTTR (Mean Time to Restore) of a service remains one of the most important steps to assure the user-perceived availability of the service. To reduce the MTTR, a common practice is to restore the service by identifying and applying an appropriate healing action. In this paper, we present an automated mining-based approach for suggesting an appropriate healing action for a given new issue. Our approach suggests an appropriate healing action by adapting healing actions from the retrieved similar historical issues. We have applied our approach to a real-world and large-scale product online service. The studies on 243 real issues of the service show that our approach can effectively suggest appropriate healing actions (with 87% accuracy) to reduce the MTTR of the service. In addition, according to issue characteristics\u00a0\u2026", "num_citations": "25\n", "authors": ["186"]}
{"title": "Intelligent software engineering: synergy between AI and software engineering\n", "abstract": " As an example of exploiting the synergy between AI and software engineering, the field of intelligent software engineering has emerged with various advances in recent years. Such field broadly addresses issues on intelligent [software engineering] and [intelligence software] engineering. The former, intelligent [software engineering], focuses on instilling intelligence in approaches developed to address various software engineering tasks to accomplish high effectiveness and efficiency. The latter, [intelligence software] engineering, focuses on addressing various software engineering tasks for intelligence software, e.g., AI software. In this paper, we discuss recent research and future directions in the field of intelligent software engineering.", "num_citations": "24\n", "authors": ["186"]}
{"title": "Automatically identifying special and common unit tests for object-oriented programs\n", "abstract": " Developers often create common tests and special tests, which exercise common behaviors and special behaviors of the class under test, respectively. Although manually created tests are valuable, developers often overlook some special or even common tests. We have developed a new approach for automatically identifying special and common unit tests for a class without requiring any specification. Given a class, we automatically generate test inputs and identify common and special tests among the generated tests. Developers can inspect these identified tests and use them to augment existing tests. Our approach is based on statistical algebraic abstractions, program properties (in the form of algebraic specifications) dynamically inferred based on a set of predefined abstraction templates. We use statistical algebraic abstractions to characterize program behaviors and identify special and common tests. Our\u00a0\u2026", "num_citations": "24\n", "authors": ["186"]}
{"title": "Generating Regular Expressions from Natural Language Specifications: Are We There Yet\n", "abstract": " Recent state-of-the-art approaches automatically generate regular expressions from natural language specifications. Given that these approaches use only synthetic data in both training datasets and validation/test datasets, a natural question arises: are these approaches effective to address various real-world situations? To explore this question, in this paper, we conduct a characteristic study on comparing two synthetic datasets used by the recent research and a real-world dataset collected from the Internet, and conduct an experimental study on applying a state-of-the-art approach on the real-world dataset. Our study results suggest the existence of distinct characteristics between the synthetic datasets and the real-world dataset, and the state-of-the-art approach (based on a model trained from a synthetic dataset) achieves extremely low effectiveness when evaluated on real-world data, much lower than the effectiveness when evaluated on the synthetic dataset. We also provide initial analysis on some of those challenging cases and discuss future directions.", "num_citations": "23\n", "authors": ["186"]}
{"title": "Substra: A framework for automatic generation of integration tests\n", "abstract": " A component-based software system consists of well-encapsulated components that interact with each other via their interfaces. Software integration tests are generated to test the interactions among different components. These tests are usually in the form of sequences of interface method calls. Although many components are equipped with documents that provide informal specifications of individual interface methods, few documents specify component interaction constraints on the usage of these interface methods, including the order in which these methods should be called and the constraints on the method arguments and returns across multiple methods. In this paper, we propose Substra, a framework for automatic generation of software integration tests based on call-sequence constraints inferred from initial-test executions or normal runs of the subsystem under test. Two types of sequencing constraints are\u00a0\u2026", "num_citations": "23\n", "authors": ["186"]}
{"title": "Record and replay for Android: are we there yet in industrial cases?\n", "abstract": " Mobile applications, or apps for short, are gaining popularity. The input sources (eg, touchscreen, sensors, transmitters) of the smart devices that host these apps enable the apps to offer a rich experience to the users, but these input sources pose testing complications to the developers (eg, writing tests to accurately utilize multiple input sources together and be able to replay such tests at a later time). To alleviate these complications, researchers and practitioners in recent years have developed a variety of record-and-replay tools to support the testing expressiveness of smart devices. These tools allow developers to easily record and automate the replay of complicated usage scenarios of their app. Due to Android's large share of the smart-device market, numerous record-and-replay tools have been developed using a variety of techniques to test Android apps. To better understand the strengths and weaknesses of\u00a0\u2026", "num_citations": "22\n", "authors": ["186"]}
{"title": "APTE: Automated pointcut testing for AspectJ programs\n", "abstract": " Aspect-Oriented Programming (AOP) has been proposed as a methodology that provides new modularization of software systems by allowing encapsulation of cross-cutting concerns. AspectJ, an aspect-oriented programming language, provides two major constructs: advice and pointcuts. The scope of pointcuts spans across various objects instantiated from the classes. With the increase in the number of objects, classes, and integration of source code, it is likely that a developer writes a pointcut that does not serve its intended purpose. Therefore, there is a need to test pointcuts for validating the correctness of their expressions. In this paper, we propose APTE, an automated framework that tests pointcuts in AspectJ programs with the help of AJTE, an existing unit-testing framework without weaving. Our new APTE framework identifies joinpoints that satisfy a pointcut expression and a set of boundary joinpoints\u00a0\u2026", "num_citations": "22\n", "authors": ["186"]}
{"title": "Text mining in supporting software systems risk assurance\n", "abstract": " Insufficient risk analysis often leads to software system design defects and system failures. Assurance of software risk documents aims to increase the confidence that identified risks are complete, specific, and correct. Yet assurance methods rely heavily on manual analysis that requires significant knowledge of historical projects and subjective, perhaps biased judgment from domain experts. To address the issue, we have developed RARGen, a text mining-based approach based on well-established methods aiming to automatically create and maintain risk repositories to identify usable risk association rules (RARs) from a corpus of risk analysis documents. RARs are risks that have frequently occurred in historical projects. We evaluate RARGen on 20 publicly available e-service projects. Our evaluation results show that RARGen can effectively reason about RARs, increase confidence and cost-effectiveness of risk\u00a0\u2026", "num_citations": "21\n", "authors": ["186"]}
{"title": "Mining Android app descriptions for permission requirements recommendation\n", "abstract": " During the development or maintenance of an Android app, the app developer needs to determine the app's security and privacy requirements such as permission requirements. Permission requirements include two folds. First, what permissions (i.e., access to sensitive resources, e.g., location or contact list) the app needs to request. Second, how to explain the reason of permission usages to users. In this paper, we focus on the multiple challenges that developers face when creating permission-usage explanations. We propose a novel framework, CLAP, that mines potential explanations from the descriptions of similar apps. CLAP leverages information retrieval and text summarization techniques to find frequent permission usages. We evaluate CLAP on a large dataset containing 1.4 million Android apps. The evaluation results outperform existing state-of-the-art approaches, showing great promise of CLAP as a\u00a0\u2026", "num_citations": "20\n", "authors": ["186"]}
{"title": "MetaSymploit: Day-one defense against script-based attacks with security-enhanced symbolic analysis\n", "abstract": " A script-based attack framework is a new type of cyberattack tool written in scripting languages. It carries various attack scripts targeting vulnerabilities across different systems. It also supports fast development of new attack scripts that can even exploit zero-day vulnerabilities. Such mechanisms pose a big challenge to the defense side since traditional malware analysis cannot catch up with the emerging speed of new attack scripts. In this paper, we propose MetaSymploit, the first system of fast attack script analysis and automatic signature generation for a network Intrusion Detection System (IDS). As soon as a new attack script is developed and distributed, MetaSymploit uses security-enhanced symbolic execution to quickly analyze the script and automatically generate specific IDS signatures to defend against all possible attacks launched by this new script from Day One. We implement a prototype of MetaSymploit targeting Metasploit, the most popular penetration framework. In the experiments on 45 real attack scripts, MetaSymploit automatically generates Snort IDS rules as signatures that effectively detect the attacks launched by the 45 scripts. Furthermore, the results show that MetaSymploit substantially complements and improves existing Snort rules that are manually written by the official Snort team.", "num_citations": "20\n", "authors": ["186"]}
{"title": "UnitPlus: assisting developer testing in Eclipse\n", "abstract": " In the software development life cycle, unit testing is an important phase that helps in early detection of bugs. A unit test case consists of two parts: a test input, which is often a sequence of method calls, and a test oracle, which is often in the form of assertions. The effectiveness of a unit test case depends on its test input as well as its test oracle because the test oracle helps in exposing bugs during the execution of the test input. The task of writing effective test oracles is not trivial as this task requires domain or application knowledge and also needs knowledge of the intricate details of the class under test. In addition, when developers write new unit test cases, much test code (including code in test inputs or oracles) such as method argument values is the same as some previously written test code. To assist developers in writing test code in unit test cases more efficiently, we have developed an Eclipse plugin for JUnit\u00a0\u2026", "num_citations": "20\n", "authors": ["186"]}
{"title": "Automatic extraction of abstract-object-state machines based on branch coverage\n", "abstract": " Some requirement modelling languages such as UML\u2019s statechart diagrams allow developers to specify requirements of state-transition behavior in a visual way. These requirement specifications are useful in many ways, including helping program understanding and specification-based testing. However, there are a large number of legacy systems that are not equipped with these requirement specifications. This paper proposes a new approach, called Brastra, for extracting object state machines (OSM) from unit-test executions. An OSM describes how a method call transits an object from one state to another. When the state of an object is represented with concrete-state information (the values of fields transitively reachable from the object), the extracted OSMs are simply too complex to be useful. Our Brastra approach abstracts an object\u2019s concrete state to an abstract state based on the branch coverage information exercised by methods invoked on the object. We have prototyped our Brastra approach and shown the utility of the approach with an illustrating example. Our initial experience shows that Brastra can extract compact OSMs that provide useful information for understanding state-transition behavior.", "num_citations": "20\n", "authors": ["186"]}
{"title": "An empirical study of Java dynamic call graph extractors\n", "abstract": " A dynamic call graph is the invocation relation that represents a specific set of runtime executions of a program. Dynamic call graph extraction is a typical application of dynamic analysis to aid compiler optimization, performance analysis, program understanding, etc. In this paper, we empirically compare the results of nine Java dynamic call graph extractors quantitatively and qualitatively. We investigate those differences among the dynamic call graph extracted by different tools mainly caused by different underlying Java program instrumentation techniques and other design decisions. A comparison between static call graph and dynamic call graph shows software engineering tools for program understanding place a different requirement on dynamic call graph from compilers or profilers whose main purpose is optimization or performance tuning. Dynamic call graphs require some complementary static information and an effective representation to aid program understanding. Choosing an appropriate instrumentation technique, integrating static and dynamic information, and providing flexible user manipulation for dynamic call graphs can better facilitate program understanding task. In this paper, we discuss the study and sketch the design considerations for an effective dynamic call graph tool to support program understanding.", "num_citations": "20\n", "authors": ["186"]}
{"title": "Exposing behavioral differences in cross-language API mapping relations\n", "abstract": " Due to various considerations, software vendors often translate their applications from one programming language to another, either manually or with the support of translation tools. Both these scenarios require translation of many call sites of API elements (i.e., classes, methods, and fields of API libraries). API mapping relations, either acquired by experienced programmers or already incorporated in translation tools, are much valuable in the translation process, since they describe mapping relations between source API elements and their equivalent target API elements. However, in an API mapping relation, a source API element and its target API elements may have behavioral differences, and such differences could lead to defects in the translated code. So far, to the best of our knowledge, there exists no previous study for exposing or understanding such differences. In this paper, we make the first\u00a0\u2026", "num_citations": "19\n", "authors": ["186"]}
{"title": "Cooperative testing and analysis: Human-tool, tool-tool and human-human cooperations to get work done\n", "abstract": " Tool automation to reduce manual effort has been an active research area in various sub fields of software engineering such as software testing and analysis. To maximize the value of software testing and analysis, effective support for cooperation between engineers and tools is greatly needed and yet lacking in state-of-the-art research and practice. In particular, testing and analysis are in a great need of (1) effective ways for engineers to communicate their testing or analysis goals and guidance to tools and (2) tools with strong enough capabilities to accomplish the given testing or analysis goals and with effective ways to communicate challenges faced by them to engineers -- enabling a feedback loop between engineers and tools to refine and accomplish the testing or analysis goals. In addition, different tools have their respective strengths and weaknesses, and there is also a great need of allowing these tools to\u00a0\u2026", "num_citations": "19\n", "authors": ["186"]}
{"title": "Improving software quality via code searching and mining\n", "abstract": " Enormous amount of open source code is available on the Internet and various code search engines (CSE) are available to serve as a means for searching in open source code. However, usage of CSEs is often limited to simple tasks such as searching for relevant code examples. In this paper, we present a generic life-cycle model that can be used to improve software quality by exploiting CSEs. We present three example software development tasks that can be assisted by our life-cycle model and show how these three tasks can contribute to improve the software quality. We also show the application of our life-cycle model with a preliminary evaluation.", "num_citations": "19\n", "authors": ["186"]}
{"title": "\u9006\u5411\u5de5\u7a0b\u7814\u7a76\u4e0e\u53d1\u5c55\n", "abstract": " A b s tr ae t I n or d or ro o ff eetlve l y use exlstin g assots I n l e g ae y s y ste m s, \u4e00 t \u4e00 s \u4e3b m p or ta n tt o de ve l o p a s y ste m at \u4e00 e strate g y fo r t h o eont \u4e00 nue d evo lu t \u4e00. n o fl o g ae y s y ste m s\u00b7 R eon g \u5143 neer \u4e00 n g o ff ers an a p-p roae h to m z g rate a l e g ae y s y s te m to w ar d s an evo l va bl o s y ste m, o f w h xe h p ro g ra m un d erstan d ln g 15 a ke yp art\u00b7 R everse en g ln eer ln g \u4e00 5 e ff eetlve a p p roae h to su pp ort p ro g ra m un d erstan d zn g, t he re-f ore. p la y s an x m p ortant ro l e in sueeess f u l ly reen g lneerln g l e g ae y s y ste m", "num_citations": "19\n", "authors": ["186"]}
{"title": "SemRegex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications\n", "abstract": " Recent research proposes syntax-based approaches to address the problem of generating programs from natural language specifications. These approaches typically train a sequence-to-sequence learning model using a syntax-based objective: maximum likelihood estimation (MLE). Such syntax-based approaches do not effectively address the goal of generating semantically correct programs, because these approaches fail to handle Program Aliasing, ie, semantically equivalent programs may have many syntactically different forms. To address this issue, in this paper, we propose a semantics-based approach named SemRegex. SemRegex provides solutions for a subtask of the program-synthesis problem: generating regular expressions from natural language. Different from the existing syntax-based approaches, SemRegex trains the model by maximizing the expected semantic correctness of the generated regular expressions. The semantic correctness is measured using the DFA-equivalence oracle, random test cases, and distinguishing test cases. The experiments on three public datasets demonstrate the superiority of SemRegex over the existing state-of-the-art approaches.", "num_citations": "18\n", "authors": ["186"]}
{"title": "Automatic extraction of sliced object state machines for component interfaces\n", "abstract": " Component-based software development has increasingly gained popularity in industry. Although correct component-interface usage is critical for successful understanding, testing, and reuse of components, interface usage is rarely specified formally in practice. To tackle this problem, we automatically extract sliced object state machines (OSM) for component interfaces from the execution of generated tests. Given a component such as a Java class, we generate a set of tests to exercise the component and collect the concrete object states exercised by the tests. Because the number of exercised concrete object states and transitions among these states could be too large to be useful for inspection, we slice concrete object states by each member field of the component and use sliced states to construct a set of sliced OSM\u2019s. These sliced OSM\u2019s provide useful state-transition information for helping understand behavior of component interfaces and also have potential for being used in component verification and testing.", "num_citations": "18\n", "authors": ["186"]}
{"title": "A comprehensive study on challenges in deploying deep learning based software\n", "abstract": " Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL\u00a0\u2026", "num_citations": "17\n", "authors": ["186"]}
{"title": "A large-scale empirical study on android runtime-permission rationale messages\n", "abstract": " After Android 6.0 introduces the runtime-permission system, many apps provide runtime-permission-group rationales for the users to better understand the permissions requested by the apps. To understand the patterns of rationales and to what extent the rationales can improve the users' understanding of the purposes of requesting permission groups, we conduct a large-scale measurement study on five aspects of runtime rationales. We have five main findings: (1) less than 25% apps under study provide rationales; (2) for permission-group purposes that are difficult to understand, the proportions of apps that provide rationales are even lower; (3) the purposes stated in a significant proportion of rationales are incorrect; (4) a large proportion of customized rationales do not provide more information than the default permission-requesting message of Android; (5) apps that provide rationales are more likely to explain\u00a0\u2026", "num_citations": "17\n", "authors": ["186"]}
{"title": "SWAROVsky: Optimizing Resource Loading for Mobile Web Browsing\n", "abstract": " Imperfect Web resource loading prevents mobile Web browsing from providing satisfactory user experience. In this article, we design and implement the SWAROVsky system to address three main issues of current inefficient Web resource loading: (1) on-demand and thus slow loading of sub-resources of webpages; (2) duplicated loading of resources with different URLs but the same content; and (3) redundant loading of the same resource due to improper cache configurations. SWAROVsky employs a dual-proxy architecture that comprises a remote cloud-side proxy and a local proxy on mobile devices. The remote proxy proactively loads webpages from their original Web servers and maintains a resource loading graph for every single webpage. Based on the graph, the remote proxy is capable of deciding which resources are \u201creally\u201d needed for the webpage and their loading orders, and thus can synchronize\u00a0\u2026", "num_citations": "17\n", "authors": ["186"]}
{"title": "Program-input generation for testing database applications using existing database states\n", "abstract": " Testing is essential for quality assurance of database applications. Achieving high code coverage of the database applications is important in testing. In practice, there may exist a copy of live databases that can be used for database application testing. Using an existing database state is desirable since it tends to be representative of real-world objects\u2019 characteristics, helping detect faults that could cause failures in real-world settings. However, to cover a specific program-code portion (e.g., block), appropriate program inputs also need to be generated for the given existing database state. To address this issue, in this paper, we propose a novel approach that generates program inputs for achieving high code coverage of a database application, given an existing database state. Our approach uses symbolic execution to track how program inputs are transformed before appearing in the executed SQL queries\u00a0\u2026", "num_citations": "17\n", "authors": ["186"]}
{"title": "Prado: Predicting app adoption by learning the correlation between developer-controllable properties and user behaviors\n", "abstract": " To survive and stand out from the fierce market competition nowadays, it is critical for app developers to know (desirably ahead of time) whether, how well, and why their apps would be adopted by users. Ideally, the adoption of an app could be predicted by factors that can be controlled by app developers in the development process, and factors that app developers are able to take actions on and improve according to the predictions. To this end, this paper proposes PRADO, an approach to measuring various aspects of user adoption, including app download and installation, uninstallation, and user ratings. PRADO employs advanced machine learning algorithms to predict user adoption based on how these metrics correlate to a comprehensive taxonomy of 108 developer-controllable features of the app. To evaluate PRADO, we use 9,824 free apps along with their behavioral data from 12.57 million Android users\u00a0\u2026", "num_citations": "16\n", "authors": ["186"]}
{"title": "The synergy of human and artificial intelligence in software engineering\n", "abstract": " To reduce human efforts and burden on human intelligence in software-engineering activities, Artificial Intelligence (AI) techniques have been employed to assist or automate these activities. On the other hand, human's domain knowledge can serve as starting points for designing AI techniques. Furthermore, the results of AI techniques are often interpreted or verified by human users. Such user feedback could be incorporated to further improve the AI techniques, forming a continuous feedback loop. We recently proposed cooperative testing and analysis including human-tool cooperation (consisting of human-assisted computing and human-centric computing) and human-human cooperation. In this paper, we present example software-engineering problems with solutions that leverage the synergy of human and artificial intelligence, and illustrate how cooperative testing and analysis can help realize such synergy.", "num_citations": "16\n", "authors": ["186"]}
{"title": "Experience report on applying software analytics in incident management of online service\n", "abstract": " As online services become more and more popular, incident management has become a critical task that aims to minimize the service downtime and to ensure high quality of the provided services. In practice, incident management is conducted through analyzing a huge amount of monitoring data collected at runtime of a service. Such data-driven incident management faces several significant challenges such as the large data scale, complex problem space, and incomplete knowledge. To address these challenges, we carried out 2-year software-analytics research where we designed a set of novel data-driven techniques and developed an industrial system called the Service Analysis Studio (SAS) targeting real scenarios in a large-scale online service of Microsoft. SAS has been deployed to worldwide product datacenters and widely used by on-call engineers for incident management. This paper shares\u00a0\u2026", "num_citations": "15\n", "authors": ["186"]}
{"title": "Inferring project-specific bug patterns for detecting sibling bugs\n", "abstract": " Lightweight static bug-detection tools such as FindBugs, PMD, Jlint, and Lint4j detect bugs with the knowledge of generic bug patterns (eg, objects of java. io. InputStream are not closed in time after used). Besides generic bug patterns, different projects under analysis may have some project-specific bug patterns. For example, in a revision of the Xerces project, the class field\" fDTDHandler\" is dereferenced without proper null-checks, while it could actually be null at runtime. We name such bug patterns directly related to objects instantiated in specific projects as Project-Specific Bug Patterns (PSBPs). Due to lack of such PSBP knowledge, existing tools usually fail in effectively detecting most of this kind of bugs. We name bugs belonging to the same project and sharing the same PSBP as sibling bugs. If some sibling bugs are fixed in a fix revision but some others remain, we treat such fix as an incomplete fix. To\u00a0\u2026", "num_citations": "15\n", "authors": ["186"]}
{"title": "Exploiting synergy between testing and inferred partial specifications\n", "abstract": " The specifications of a program can be dynamically inferred from its executions, or equivalently, from the program plus a test suite. A deficient test suite or a subset of a sufficient test suite may not help to infer generalizable program properties. But the partial specifications inferred from the test suite constitute a summary proxy for the test execution history. When a new test is executed on the program, a violation of a previously inferred specification indicates the need for a potential test augmentation. Developers can inspect the test and the violated specification to make a decision whether to add the new test to the existing test suite after equipping the test with an oracle. By selectively augmenting the existing test suite, the quality of the inferred specifications in the next cycle can be improved while avoiding noisy data such as illegal inputs. To experiment with this approach, we integrated the use of Daikon (a dynamic invariant detection tool) and Jtest (a commercial Java unit testing tool). This paper presents several techniques to exploit the synergy between testing and inferred partial specifications in unit test data selection.", "num_citations": "15\n", "authors": ["186"]}
{"title": "Jbooret: an automated tool to recover oo design and source models\n", "abstract": " This paper introduces a reverse engineering tool, JBOORET (Jade Bird Object-Oriented Reverse Engineering Tool). This tool is developed by adopting a parser-based approach to assist the activity of extracting the higher-level design and source models from system artifacts. A conceptual model is formulated as the knowledge representation. Multi-perspective design and source models are recovered by JBOORET based on the comprehensive program information extracted from source code. Its flexible user interface can assist users to browse the detailed information of design and source models by using the selection and compaction mechanism. This paper discusses the design principles and decisions of JBOORET and describes its implementation.", "num_citations": "15\n", "authors": ["186"]}
{"title": "Component metrics in jade bird component library system\n", "abstract": " Focusing on software productivity and software quality demanded by the development of software industry has spurred the research on software reuse technology. Therefore, the metrics about software reuse technology are also attracting more attention. The goal of Jade Bird Component Library (JBCL) system is to describe, manage, store and retrieve components in order to fulfil the requirement of component-reuse-based software development process. This article introduces the component metrics model in JBCL and discusses its implementation based on the Object-Oriented Metrics Tool of Jade Bird Program Analysis system (JBPAS) and JBCL Posteriori Metrics system, which is made up of Feedback Collection, Analysis and Processing Tools.", "num_citations": "15\n", "authors": ["186"]}
{"title": "Dependent-test-aware regression testing techniques\n", "abstract": " Developers typically rely on regression testing techniques to ensure that their changes do not break existing functionality. Unfortunately, these techniques suffer from flaky tests, which can both pass and fail when run multiple times on the same version of code and tests. One prominent type of flaky tests is order-dependent (OD) tests, which are tests that pass when run in one order but fail when run in another order. Although OD tests may cause flaky-test failures, OD tests can help developers run their tests faster by allowing them to share resources. We propose to make regression testing techniques dependent-test-aware to reduce flaky-test failures.", "num_citations": "14\n", "authors": ["186"]}
{"title": "Software analytics in practice: Mini tutorial\n", "abstract": " Summary form only given. A huge wealth of various data exists in the software development process, and hidden in the data is information about the quality of software and services as well as the dynamics of software development. With various analytic and computing technologies, software analytics is to enable software practitioners to performance data exploration and analysis in order to obtain insightful and actionable information for data-driven tasks around software and services [1].", "num_citations": "14\n", "authors": ["186"]}
{"title": "Test selection for result inspection via mining predicate rules\n", "abstract": " It is labor-intensive to manually verify the outputs of a large set of tests that are not equipped with test oracles. Test selection helps to reduce this cost by selecting a small subset of tests that are likely to reveal faults. A promising approach is to dynamically mine operational models as potential test oracles and then select tests that violate them. Existing work mines operational models from verified passing tests based on dynamic invariant detection. In this paper, we propose to mine common operational models, which are not always true in all observed traces, from a set of unverified tests based on mining predicate rules. Specifically, we collect values of simple predicates at runtime and then generate and evaluate predicate rules as potential operational models after running all the tests. We then select tests that violate the mined predicate rules for result inspection. Preliminary results on the Siemens suite and the grep\u00a0\u2026", "num_citations": "14\n", "authors": ["186"]}
{"title": "Advances on improving automation in developer testing\n", "abstract": " Developer testing, a common step in software development, involves generating desirable test inputs and checking the behavior of the program unit under test during the execution of the test inputs. Existing developer testing tools include various techniques to address challenges of generating desirable test inputs and checking the behavior of the program unit under test (referred to as test oracles). In this chapter, we present an overview of techniques implemented in these testing tools to address challenges in improving automation in developer testing. In particular, we focus on a recent state-of-the-art technique, called symbolic execution for test inputs. We briefly describe symbolic execution and discuss various challenges (along with the techniques developed to address those challenges) in generating test inputs automatically. For test inputs, the techniques presented in our chapter are summarized from two\u00a0\u2026", "num_citations": "13\n", "authors": ["186"]}
{"title": "Security policy testing via automated program code generation\n", "abstract": " Access control is one of the fundamental security mechanisms for information systems. It determines the availability of resources to principals, operations that can be performed, and under what circumstances. Traditionally the enforcement of access control is often hardcoded in applications or systems; such hardcoding makes it hard to verify the correctness of access control and to accommodate changes of security requirements. Recently, access control policies have been increasingly separated from enforcement mechanisms. An access control policy is explicitly specified using certain policy languages with well-defined syntax and semantics. An application then consults the policy during runtime to determine whether an access request from a principal should be allowed or denied. There are two main advantages of this approach. First, security officers can now perform systematic and formal security analysis on\u00a0\u2026", "num_citations": "13\n", "authors": ["186"]}
{"title": "Improving effectiveness of automated software testing in the absence of specifications\n", "abstract": " Program specifications can be valuable in improving the effectiveness of automated software testing in generating test inputs and checking test executions for correctness. Unfortunately, specifications are often absent from programs in practice. We present a framework for improving effectiveness of automated testing in the absence of specifications. The framework supports a set of related techniques, including redundant-test detection, non-redundant-test generation, test selection, test abstraction, and program-spectra comparison. The framework has been implemented and empirical results have shown that the developed techniques within the framework improve the effectiveness of automated testing by detecting high percentage of redundant tests among test inputs generated by existing tools, generating non-redundant test inputs to achieve high structural coverage, reducing inspection efforts for detecting\u00a0\u2026", "num_citations": "13\n", "authors": ["186"]}
{"title": "SoK: Certified Robustness for Deep Neural Networks\n", "abstract": " Great advancement in deep neural networks (DNNs) has led to state-of-the-art performance on a wide range of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial attacks, which have brought great concerns when deploying these models to safety-critical applications such as autonomous driving. Different defense approaches have been proposed against adversarial attacks, including: 1) empirical defenses, which can be adaptively attacked again without providing robustness certification; and 2) certifiably robust approaches, which consist of robustness verification providing the lower bound of robust accuracy against any attacks under certain conditions and corresponding robust training approaches. In this paper, we focus on these certifiably robust approaches and provide the first work to perform large-scale systematic analysis of different robustness verification and training approaches. In particular, we 1) provide a taxonomy for the robustness verification and training approaches, as well as discuss the detailed methodologies for representative algorithms, 2) reveal the fundamental connections among these approaches, 3) discuss current research progresses, theoretical barriers, main challenges, and several promising future directions for certified defenses for DNNs, and 4) provide an open-sourced unified platform to evaluate 20+ representative verification and corresponding robust training approaches on a wide range of DNNs.", "num_citations": "11\n", "authors": ["186"]}
{"title": "Mining likely properties of access control policies via association rule mining\n", "abstract": " Access control mechanisms are used to control which principals (such as users or processes) have access to which resources based on access control policies. To ensure the correctness of access control policies, policy authors conduct policy verification to check whether certain properties are satisfied by a policy. However, these properties are often not written in practice. To facilitate property verification, we present an approach that automatically mines likely properties from a policy via the technique of association rule mining. In our approach, mined likely properties may not be true for all the policy behaviors but are true for most of the policy behaviors. The policy behaviors that do not satisfy likely properties could be faulty. Therefore, our approach then conducts likely-property verification to produce counterexamples, which are used to help policy authors identify faulty rules in the policy. To show the\u00a0\u2026", "num_citations": "11\n", "authors": ["186"]}
{"title": "Bibliography on mining software engineering data\n", "abstract": " Software engineering data (such as code bases, execution traces, historical code changes, mailing lists, and bug databases) contains a wealth of information about a project's status, progress, and evolution. Using well-established data mining techniques, practitioners and researchers can explore the potential of this valuable data in order to better manage their projects and to produce higher quality software systems that are delivered on time and on budget.This tutorial presents the latest research in mining Software Engineering (SE) data, discusses challenges associated with mining SE data, highlights SE data mining success stories, and outlines future research directions. Attendees will acquire the knowledge and skills needed to perform research or conduct practice in the field and to integrate data mining techniques in their own research or practice. More information of the tutorial can be found at https://sites\u00a0\u2026", "num_citations": "11\n", "authors": ["186"]}
{"title": "Effective generation of interface robustness properties for static analysis\n", "abstract": " A software system interacts with its environment through system interfaces. Robustness of software systems are governed by various temporal properties related to these interfaces, whose violation leads to system crashes and security compromises. These properties can be formally specified for system interfaces and statically verified against a software system. But manually specifying a large number of interface properties for static verification is often inaccurate or incomplete, apart from being cumbersome. In this paper, we propose a novel framework that effectively generates interface properties for static checking from a few generic, high level robustness rules that capture interface behavior. We implement our framework for an existing static analyzer with simple dataflow extensions and apply it on POSIX-API system interfaces used in 10 Redhat-9.0 open source packages. The results show that the framework can\u00a0\u2026", "num_citations": "11\n", "authors": ["186"]}
{"title": "Understanding software application interfaces via string analysis\n", "abstract": " In software systems, different software applications often interact with each other through specific interfaces by exchanging data in string format. For example, web services interact with each other through XML strings. Database applications interact with a database through strings of SQL statements. Sometimes these interfaces between different software applications are complex and distributed. For example, a table in a database can be accessed by multiple methods in a database application and a single method can access multiple tables. In this paper, we propose an approach to understanding software application interfaces through string analysis. The approach first performs a static analysis of source code to identify interaction points (in the form of interface-method-call sites). We then leverage existing string analysis tools to collect all possible string data that can be sent through these different interaction\u00a0\u2026", "num_citations": "11\n", "authors": ["186"]}
{"title": "An exploratory study of logging configuration practice in Java\n", "abstract": " Logging components are an integral element of software systems. These logging components receive the logging requests generated by the logging code and process these requests according to logging configurations. Logging configurations play an important role on the functionality, performance, and reliability of logging. Although recent research has been conducted to understand and improve current practice on logging code, no existing research focuses on logging configurations. To fill this gap, we conduct an exploratory study on logging configuration practice of 10 open-source projects and 10 industrial projects written in Java in various sizes and domains. We quantitatively show how logging configurations are used with respect to logging management, storage, and formatting. We categorize and analyze the change history (1,213 revisions) of logging configurations to understand how the logging\u00a0\u2026", "num_citations": "10\n", "authors": ["186"]}
{"title": "REINAM: reinforcement learning for input-grammar inference\n", "abstract": " Program input grammars (ie, grammars encoding the language of valid program inputs) facilitate a wide range of applications in software engineering such as symbolic execution and delta debugging. Grammars synthesized by existing approaches can cover only a small part of the valid input space mainly due to unanalyzable code (eg, native code) in programs and lacking high-quality and high-variety seed inputs. To address these challenges, we present REINAM, a reinforcement-learning approach for synthesizing probabilistic context-free program input grammars without any seed inputs. REINAM uses an industrial symbolic execution engine to generate an initial set of inputs for the given target program, and then uses an iterative process of grammar generalization to proactively generate additional inputs to infer grammars generalized from these initial seed inputs. To efficiently search for target generalizations\u00a0\u2026", "num_citations": "10\n", "authors": ["186"]}
{"title": "Muldef: Multi-model-based defense against adversarial examples for neural networks\n", "abstract": " Despite being popularly used in many applications, neural network models have been found to be vulnerable to adversarial examples, i.e., carefully crafted examples aiming to mislead machine learning models. Adversarial examples can pose potential risks on safety and security critical applications. However, existing defense approaches are still vulnerable to attacks, especially in a white-box attack scenario. To address this issue, we propose a new defense approach, named MulDef, based on robustness diversity. Our approach consists of (1) a general defense framework based on multiple models and (2) a technique for generating these multiple models to achieve high defense capability. In particular, given a target model, our framework includes multiple models (constructed from the target model) to form a model family. The model family is designed to achieve robustness diversity (i.e., an adversarial example successfully attacking one model cannot succeed in attacking other models in the family). At runtime, a model is randomly selected from the family to be applied on each input example. Our general framework can inspire rich future research to construct a desirable model family achieving higher robustness diversity. Our evaluation results show that MulDef (with only up to 5 models in the family) can substantially improve the target model's accuracy on adversarial examples by 22-74% in a white-box attack scenario, while maintaining similar accuracy on legitimate examples.", "num_citations": "10\n", "authors": ["186"]}
{"title": "Aladdin: Automating Release of Deep-Link APIs on Android\n", "abstract": " Compared to the Web where each web page has a global URL for external access, a specific'page'inside a mobile app cannot be easily accessed unless the user performs several steps from the landing page of this app. Recently, the concept of'deep link'is expected to be a promising solution and has been advocated by major service providers to enable targeting and opening a specific page of an app externally with an accessible uniform resource identifier. In this paper, we present a large-scale empirical study to investigate how deep links are really adopted, over 25,000 Android apps. To our surprise, we find that deep links have quite low coverage, eg, more than 70% and 90% of the apps do not have deep links on app stores Wandoujia and Google Play, respectively. One underlying reason is the mandatory and non-trivial manual efforts of app developers to provide APIs for deep links. We then propose the\u00a0\u2026", "num_citations": "10\n", "authors": ["186"]}
{"title": "Fault localization for firewall policies\n", "abstract": " Firewalls are the mainstay of enterprise security and the most widely adopted technology for protecting private networks. Ensuring the correctness of firewall policies through testing is important. In firewall policy testing, test inputs are packets and test outputs are decisions. Packets with unexpected (expected) evaluated decisions are classified as failed (passed) tests. Given failed tests together with passed tests, policy testers need to debug the policy to detect fault locations (such as faulty rules). Such a process is often time-consuming.To help reduce effort on detecting fault locations, we propose an approach to reduce the number of rules for inspection based on information collected during evaluating failed tests. Our approach ranks the reduced rules to decide which rules should be inspected first. We performed experiments on applying our approach. The empirical results show that our approach can reduce 56\u00a0\u2026", "num_citations": "10\n", "authors": ["186"]}
{"title": "Improving automation in developer testing: State of the practice\n", "abstract": " Developer testing, a common step in software development, involves generating desirable test inputs and checking the behavior of the program unit under test during the execution of the test inputs. Existing industrial developer testing tools include various techniques to address challenges of generating desirable test inputs and checking the behavior of the program unit under test. This paper presents an overview of techniques implemented in industrial developer testing tools to address challenges in improving automation in developer testing. These techniques are summarized from two main aspects: test efficiency (eg, with a focus on cost) and test effectiveness (eg, with a focus on benefit).", "num_citations": "10\n", "authors": ["186"]}
{"title": "\u9762\u5411\u5bf9\u8c61\u5ea6\u91cf\u7efc\u8ff0\n", "abstract": " < \u6b63> \u8f6f\u4ef6\u662f\u4fe1\u606f\u6280\u672f\u7684\u6838\u5fc3, \u56e0\u800c\u7ba1\u7406\u4eba\u5458\u5bf9\u8f6f\u4ef6\u8d28\u91cf\u63a7\u5236\u8d8a\u6765\u8d8a\u91cd\u89c6. \u8fd9\u79cd\u91cd\u89c6\u5f15\u8d77\u4e86\u4e24\u79cd\u6548\u679c:(1) \u8981\u6c42\u65b0\u7684, \u66f4\u597d\u7684\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5\u548c\u6280\u672f;(2) \u5728\u8f6f\u4ef6\u5f00\u53d1\u8fc7\u7a0b\u4e2d, \u8fdb\u884c\u8f6f\u4ef6\u5ea6\u91cf. \u8f6f\u4ef6\u5ea6\u91cf\u53ef\u4ee5\u5e2e\u52a9\u7ba1\u7406\u4eba\u5458\u63a7\u5236, \u5b89\u6392\u8f6f\u4ef6\u5f00\u53d1\u5e76\u5229\u7528\u53cd\u9988\u4fe1\u606f\u5bf9\u8f6f\u4ef6\u8fdb\u884c\u6539\u5584, \u4ece\u800c\u63d0\u9ad8\u8f6f\u4ef6\u8d28\u91cf. \u8f6f\u4ef6\u5ea6\u91cf\u7684\u5fc5\u8981\u6027\u548c\u91cd\u8981\u6027\u5df2\u4e3a\u8f6f\u4ef6\u754c\u6240\u8ba4\u540c. \u4f5c\u4e3a 90 \u5e74\u4ee3\u7684\u9886\u5148\u6280\u672f, \u9762\u5411\u5bf9\u8c61\u7684\u6280\u672f\u5df2\u7ecf\u5728\u8f6f\u4ef6\u4ea7\u4e1a\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7684\u8fd0\u7528, \u9762\u5411\u5bf9\u8c61\u4ea7\u54c1\u4e5f\u5f97\u5230\u4e86\u8fc5\u731b\u7684\u53d1\u5c55. \u9762\u5411\u5bf9\u8c61\u5ea6\u91cf\u662f\u5bf9\u8c61\u6280\u672f\u4e0d\u53ef\u5206\u5272\u7684\u4e00\u90e8\u5206, \u5176\u5728\u9762\u5411\u5bf9\u8c61\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u5177\u6709\u4ee5\u4e0b\u7684\u529f\u7528:", "num_citations": "10\n", "authors": ["186"]}
{"title": "Provable Robust Learning Based on Transformation-Specific Smoothing\n", "abstract": " As machine learning (ML) systems become pervasive, safeguarding their security is critical. Recent work has demonstrated that motivated adversaries could add adversarial perturbations to the test data to mislead ML systems. So far, most research has focused on providing provable robustness guarantees for ML models against a specific Lp norm bounded adversarial perturbation. However, in practice previous work has shown that there are other types of realistic adversarial transformations whose semantic meaning has been leveraged to attack ML systems. In this paper, we aim to provide a unified framework for certifying ML robustness against general adversarial transformations. First, we identify the semantic transformations as different categories: resolvable (e.g., Gaussian blur and brightness) and differentially resolvable transformations (e.g., rotation and scaling). We then provide sufficient conditions and strategies for certifying certain transformations. For instance, we propose a novel sampling-based interpolation approach with estimated Lipschitz upper bound to certify the robustness against differentially resolvable transformations. In addition, we theoretically optimize the smoothing strategies for certifying the robustness of ML models against different transformations. For instance, we show that smoothing by sampling from exponential distribution provides a tighter robustness bound than Gaussian. Extensive experiments on 7 semantic transformations show that our proposed unified framework significantly outperforms the state-of-the-art certified robustness approaches on several datasets including ImageNet.", "num_citations": "9\n", "authors": ["186"]}
{"title": "CoMID: Context-Based Multiinvariant Detection for Monitoring Cyber-Physical Software\n", "abstract": " Cyber-physical software delivers context-aware services through continually interacting with its physical environment and adapting to the changing surroundings. However, when the software's assumptions on the environment no longer hold, the interactions can introduce errors for leading to unexpected behaviors and even system failures. One promising solution to this problem is to conduct runtime monitoring of invariants. Violated invariants reflect latent erroneous states (i.e., abnormal states that could lead to failures). In turn, monitoring when program executions violate the invariants can allow the software to take alternative measures to avoid danger. In this article, we present context-based Multiinvariant detection (CoMID), an approach that automatically infers invariants and detects abnormal states for cyber-physical programs. CoMID consists of two novel techniques, namely context-based trace grouping and\u00a0\u2026", "num_citations": "9\n", "authors": ["186"]}
{"title": "ReWAP: Reducing Redundant Transfers for Mobile Web Browsing via App-Specific Resource Packaging\n", "abstract": " Redundant transfer of resources is a critical issue for compromising the performance of mobile Web applications (a.k.a., apps) in terms of data traffic, load time, and even energy consumption. Evidence demonstrates that the current cache mechanisms are far from satisfactory. With lessons learned from how native apps manage their resources, in this article, we present the ReWAP approach to fundamentally reducing redundant transfers by restructuring the resource loading of mobile Web apps. ReWAP is based on an efficient resource-packaging mechanism where stable resources are encapsulated and maintained into a package, and such a package shall be loaded always from the local storage and updated by explicitly refreshing. By retrieving and analyzing the update of resources, ReWAP maintains resource packages that can accurately identify which resources can be loaded from the local storage for a\u00a0\u2026", "num_citations": "9\n", "authors": ["186"]}
{"title": "Software analytics: achievements and challenges\n", "abstract": " A huge wealth of various data exist in the practice of software development. Further rich data are produced by modern software and services in operation, many of which tend to be data-driven and/or data-producing in nature. Hidden in the data is information about the quality of software and services or the dynamics of software development. Software analytics is to utilize a data-driven approach to enable software practitioners to perform data exploration and analysis in order to obtain insightful and actionable information; such information is used for completing various tasks around software systems, software users, and software development process. This tutorial presents achievements and challenges of research and practice on principles, techniques, and applications of software analytics, highlighting success stories in industry, research achievements that are transferred to industrial practice, and future research\u00a0\u2026", "num_citations": "9\n", "authors": ["186"]}
{"title": "Bert: a tool for behavioral regression testing\n", "abstract": " During maintenance, software is modified and evolved to enhance its functionality, eliminate faults, and adapt it to changed or new platforms. In this demo, we present BERT, a tool for helping developers identify regression faults that they may have introduced when modifying their code. BERT is based on the concept of behavioral regression testing: given two versions of a program, BERT identifies behavioral differences between the two versions through dynamic analysis, in three steps. First, it generates a large number of test inputs that focus on the changed parts of the code. Second, it runs the generated test inputs on the old and new versions of the code and identifies differences in the tests' behavior. Third, it analyzes the identified differences and presents them to the developers. By focusing on a subset of the code and leveraging differential behavior, BERT can provide developers with more detailed\u00a0\u2026", "num_citations": "9\n", "authors": ["186"]}
{"title": "Macro and micro perspectives on strategic software quality assurance in resource constrained environments\n", "abstract": " Software quality assurance (SQA) plays a key role in software development process. Software quality assurance methods include testing, inspection, formal method (program verification, model checking, etc.), static code analysis, and runtime verification, etc. A disciplined approach to meeting benefit, cost, schedule, and quality constraints is in need. In this paper, we propose two perspectives (macro and micro) on strategic software quality assurance in resource constrained environments. We present a survey and discuss a variety of research opportunities and challenges with these two perspectives. Finally we present our research work on test case prioritization based on boundary value coverage to tackle strategic SQA problems with these two perspectives.", "num_citations": "9\n", "authors": ["186"]}
{"title": "Root Cause Localization for Unreproducible Builds via Causality Analysis Over System Call Tracing\n", "abstract": " Localization of the root causes for unreproducible builds during software maintenance is an important yet challenging task, primarily due to limited runtime traces from build processes and high diversity of build environments. To address these challenges, in this paper, we propose RepTrace, a framework that leverages the uniform interfaces of system call tracing for monitoring executed build commands in diverse build environments and identifies the root causes for unreproducible builds by analyzing the system call traces of the executed build commands. Specifically, from the collected system call traces, RepTrace performs causality analysis to build a dependency graph starting from an inconsistent build artifact (across two builds) via two types of dependencies: read/write dependencies among processes and parent/child process dependencies, and searches the graph to find the processes that result in the\u00a0\u2026", "num_citations": "8\n", "authors": ["186"]}
{"title": "Robustra: Training Provable Robust Neural Networks over Reference Adversarial Space\n", "abstract": " Machine learning techniques, especially deep neural networks (DNNs), have been widely adopted in various applications. However, DNNs are recently found to be vulnerable against adversarial examples, ie, maliciously perturbed inputs that can mislead the models to make arbitrary prediction errors. Empirical defenses have been studied, but many of them can be adaptively attacked again. Provable defenses provide provable error bound of DNNs, while such bound so far is far from satisfaction. To address this issue, in this paper, we present our approach named Robustra for effectively improving the provable error bound of DNNs. We leverage the adversarial space of a reference model as the feasible region to solve the min-max game between the attackers and defenders. We solve its dual problem by linearly approximating the attackers\u2019 best strategy and utilizing the monotonicity of the slack variables introduced by the reference model. The evaluation results show that our approach can provide significantly better provable adversarial error bounds on MNIST and CIFAR10 datasets, compared to the state-of-the-art results. In particular, bounded by l\u221e, with \u03f5= 0.1, on MNIST we reduce the error bound from 2.74% to 2.09%; with \u03f5= 0.3, we reduce the error bound from 24.19% to 16.91%.", "num_citations": "8\n", "authors": ["186"]}
{"title": "Detection of multiple-duty-related security leakage in access control policies\n", "abstract": " Access control mechanisms control which subjects (such as users or processes) have access to which resources. To facilitate managing access control, policy authors increasingly write access control policies in XACML. Access control policies written in XACML could be amenable to multiple-duty-related security leakage, which grants unauthorized access to a user when the user takes multiple duties (e.g., multiple roles in role-based access control policies). To help policy authors detect multiple-duty-related security leakage, we develop a novel framework that analyzes policies and detects cases that potentially cause the leakage. In such cases, a user taking multiple roles (e.g., both r 1  and r 2 ) is given a different access decision from the decision given to a user taking an individual role (e.g., r 1  and r 2 , respectively). We conduct experiments on 11 XACML policies and our empirical results show that our\u00a0\u2026", "num_citations": "8\n", "authors": ["186"]}
{"title": "Perspectives on automated testing of aspect-oriented programs\n", "abstract": " Aspect-oriented software development is gaining popularity with the adoption of aspect-oriented languages in writing programs. To reduce the manual effort in assuring the quality of aspect-oriented programs, we have developed a set of techniques and tools for automated testing of aspect-oriented programs. This position paper presents our perspectives on automated testing techniques from three dimensions: testing aspectual behavior or aspectual composition, unit tests or integration tests, and test-input generation or test oracles. We illustrate automated testing techniques primarily through the last dimension in the perspectives. By classifying these automated testing techniques in the perspectives, we provide better understanding of these techniques and identify future directions for automated testing of aspect-oriented programs. This position paper also presents a couple of new techniques that we propose\u00a0\u2026", "num_citations": "8\n", "authors": ["186"]}
{"title": "Software component protocol inference\n", "abstract": " Component-based software development has increasingly gained popularity in industry. While correct component usage is critical to successful reuse of components, the expected component usage is rarely specified explicitly. To address this issue, one recent area of research has been to infer specifications of protocols or sequencing constraints using both static and dynamic techniques. This paper explores the research area of software component protocol inference with a focus on dynamic inference techniques. A framework is proposed to compare the existing dynamic inference techniques. Along with the framework, some static inference techniques are covered in brief discussions. In the end, directions for future work are suggested to push the state of the art forward.", "num_citations": "8\n", "authors": ["186"]}
{"title": "A model-based approach to object-oriented software metrics\n", "abstract": " The need to improve software productivity and software quality has put forward the research on software metrics technology and the development of software metrics tool to support related activities. To support object-oriented software metrics practice effectively, a model-based approach to object-oriented software metrics is proposed in this paper. This approach guides the metrics users to adopt the quality metrics model to measure the object-oriented software products. The development of the model can be achieved by using a top-down approach. This approach explicitly proposes the conception of absolute normalization computation and relative normalization computation for a metrics model. Moreover, a generic software metrics tool\u2014Jade Bird Object-Oriented Metrics Tool (JBOOMT) is designed to implement this approach. The parser-based approach adopted by the tool makes the information of the\u00a0\u2026", "num_citations": "8\n", "authors": ["186"]}
{"title": "PreInfer: Automatic Inference of Preconditions via Symbolic Analysis\n", "abstract": " When tests fail (e.g., throwing uncaught exceptions), automatically inferred preconditions can bring various debugging benefits to developers. If illegal inputs cause tests to fail, developers can directly insert the preconditions in the method under test to improve its robustness. If legal inputs cause tests to fail, developers can use the preconditions to infer failure-inducing conditions. To automatically infer preconditions for better support of debugging, in this paper, we propose PREINFER, a novel approach that aims to infer accurate and concise preconditions based on symbolic analysis. Specifically, PREINFER includes two novel techniques that prune irrelevant predicates in path conditions collected from failing tests, and that generalize predicates involving collection elements (i.e., array elements) to infer desirable quantified preconditions. Our evaluation on two benchmark suites and two real-world open-source\u00a0\u2026", "num_citations": "7\n", "authors": ["186"]}
{"title": "Telemade: A Testing Framework for Learning-Based Malware Detection Systems\n", "abstract": " Learning-based malware detectors may be erroneous due to two inherent limitations. First, there is a lack of differentiability: selected features may not reflect essential differences between malware and benign apps. Second, there is a lack of comprehensiveness: the machine learning (ML) models are usually based on prior knowledge of existing malware (ie, training dataset) so malware can evolve to evade the detection. There is a strong need for an automated framework to help security analysts to detect errors in learning-based malware detection systems. Existing techniques to generate adversarial samples for learning-based systems (that take images as inputs) employ feature mutations based on feature vectors. Such techniques are infeasible to generate adversarial samples (eg, evasive malware) for malware detection system because the synthesized mutations may break the inherent constraints posed by code structures of the malware, causing either crashes or malfunctioning of malicious payloads. To address the challenge, we propose Telemade, a testing framework for learning-based malware detectors.", "num_citations": "7\n", "authors": ["186"]}
{"title": "When program analysis meets mobile security: an industrial study of misusing Android internet sockets\n", "abstract": " Despite recent progress in program analysis techniques to identify vulnerabilities in Android apps, significant challenges still remain for applying these techniques to large-scale industrial environments. Modern software-security providers, such as Qihoo 360 and Pwnzen (two leading companies in China), are often required to process more than 10 million mobile apps at each run. In this work, we focus on effectively and efficiently identifying vulnerable usage of Internet sockets in an industrial setting. To achieve this goal, we propose a practical hybrid approach that enables lightweight yet precise detection in the industrial setting. In particular, we integrate the process of categorizing potential vulnerable apps with analysis techniques, to reduce the inevitable human inspection effort. We categorize potential vulnerable apps based on characteristics of vulnerability signatures, to reduce the burden on static analysis\u00a0\u2026", "num_citations": "6\n", "authors": ["186"]}
{"title": "Improving mobile application security via bridging user expectations and application behaviors\n", "abstract": " To keep malware out of mobile application markets, existing techniques analyze the security aspects of application behaviors and summarize patterns of these security aspects to determine what applications do. However, user expectations (reflected via user perception in combination with user judgment) are often not incorporated into such analysis to determine whether application behaviors are within user expectations. This poster presents our recent work on bridging the semantic gap between user perceptions of the application behaviors and the actual application behaviors.", "num_citations": "6\n", "authors": ["186"]}
{"title": "Making exceptions on exception handling\n", "abstract": " The exception-handling mechanism has been widely adopted to deal with exception conditions that may arise during program executions. To produce high-quality programs, developers are expected to handle these exception conditions and take necessary recovery or resource-releasing actions. Failing to handle these exception conditions can lead to not only performance degradation, but also critical issues. Developers can write formal specifications to capture expected exception-handling behavior, and then apply tools to automatically analyze program code for detecting specification violations. However, in practice, developers rarely write formal specifications. To address this issue, mining techniques have been used to mine common exception-handling behavior out of program code. In this paper, we discuss challenges and achievements in precisely specifying and mining formal exception-handling\u00a0\u2026", "num_citations": "6\n", "authors": ["186"]}
{"title": "Transferring software testing tools to practice\n", "abstract": " Achieving successful technology adoption in practice has often been an important goal for both academic and industrial researchers. However, it is generally challenging to transfer research results into industrial products or into tools that are widely adopted. What are the key factors that lead to practical impact for a research project? This talk presents experiences and lessons learned in successfully transferring tools from two testing projects as collaborative efforts between the academia and industry. In the Pex project (research.microsoft.com/pex) [3], nearly a decade's collaborative efforts between Microsoft Research and academia have led to high-impact tools that are now shipped by Microsoft and adopted by the community. These tools include Fakes [2], a test isolation framework shipped with Visual Studio 2012/2013, IntelliTest, an automatic test generation tool shipped with Visual Studio 2015, and Code Hunt\u00a0\u2026", "num_citations": "5\n", "authors": ["186"]}
{"title": "Reliability Engineering\n", "abstract": " Reliability engineering dates back to reliability studies in the 20th century; since then, various models have been defined and used. Software engineering plays a key role from several viewpoints, but the main concern is that we're moving toward a more connected world, including enterprises and mobile devices. The three articles in this special issue illustrate current trends in this domain.", "num_citations": "5\n", "authors": ["186"]}
{"title": "Educational programming systems for learning at scale\n", "abstract": " Learning programming at scale underlies computer science education ranging from basic programming to advanced software engineering topics. There are strong needs of providing effective system supports for learning programming at scale. Among various desirable characteristics of such system supports, system supports shall allow students to write programs via an online Integrated Development Environment (IDE), allow students to get feedback on how they perform on the given programming exercises, etc. To aim for such effective system supports for learning programming at scale, research teams from Peking University have developed two systems: POP (denoting Peking University Online Programming System) and POJ (denoting Peking University Online Judge System). These two systems have achieved high impact among students around the world (especially those in China). In this paper, we present\u00a0\u2026", "num_citations": "5\n", "authors": ["186"]}
{"title": "Pathways to technology transfer and adoption: achievements and challenges (mini-tutorial)\n", "abstract": " Producing industrial impact has often been one of the important goals of academic or industrial researchers when conducting research. However, it is generally challenging to transfer research results into industrial practices. There are some common challenges faced when pursuing technology transfer and adoption while particular challenges for some particular research areas. At the same time, various opportunities also exist for technology transfer and adoption. This mini-tutorial presents achievements and challenges of technology transfer and adoption in various areas in software engineering, with examples drawn from research areas such as software analytics along with software testing and analysis. This mini-tutorial highlights success stories in industry, research achievements that are transferred to industrial practice, and challenges and lessons learned in technology transfer and adoption.", "num_citations": "5\n", "authors": ["186"]}
{"title": "Automated extraction and validation of security policies from natural-language documents\n", "abstract": " As one of the most fundamental security mechanisms of resources, Access Control Policies (ACP) specify which principals such as users or processes have access to which resources. Ensuring the correct specification and enforcement of ACPs is crucial to prevent security vulnerabilities. However, in practice, ACPs are commonly written in Natural Language (NL) and buried in large documents such as requirements documents, not directly checkable for correctness. It is very tedious and error-prone to manually identify and extract ACPs from these NL documents, and validate NL functional requirements such as use cases against ACPs for detecting inconsistencies. To address these issues, we propose a novel approach, called Text2Policy, that automatically extracts ACPs from NL documents and extracts action steps from NL scenario-based functional requirements (such as use cases). From the extracted ACPs, Text2Policy automatically generates checkable ACPs in specification languages such as XACML. From the extracted action steps, Text2Policy automatically derives access control requests that can be validated against specified or extracted ACPs to detect inconsistencies. To assess the effectiveness of Text2Policy, we conduct three evaluations on the collected ACP sentences from 18 sources and 37 use cases from an open source project called iTrust (including 448 use-case sentences). The results show that Text2Policy effectively extracts ACPs from NL documents and action steps from use cases for detecting issues in the use cases.", "num_citations": "5\n", "authors": ["186"]}
{"title": "JBOOMT: Jade Bird Object-Oriented Metrics Tool\n", "abstract": " Focusing on software productivity and software quality control has spurred the research on software metrics technology. The increasing importance being placed on object-oriented software development has led to the research on the object-oriented software metrics and the development of automated tools to support object-oriented metrics. To effectively aid the software evaluation, a software metrics tool is supposed to support the metrics model. The objective of Jade Bird Object-Oriented Metrics Tool (JBOOMT) is to provide an automated software metrics support for users and managers to measure the design or source code of the object-oriented program and thus evaluate the quality of the software according to the specified hierarchical metrics model. A mechanism is provided for metrics users to customize the preferred metrics model and browse the details of the metrics model. This article introduces the design of JBOOMT and discusses its implementation built in the Jade Bird Program Analysis System (JBPAS).", "num_citations": "5\n", "authors": ["186"]}
{"title": "Graph-based trace analysis for microservice architecture understanding and problem diagnosis\n", "abstract": " Microservice systems are highly dynamic and complex. For such systems, operation engineers and developers highly rely on trace analysis to understand architectures and diagnose various problems such as service failures and quality degradation. However, the huge number of traces produced at runtime makes it challenging to capture the required information in real-time. To address the faced challenges, in this paper, we propose a graph-based microservice trace analysis approach GMTA for understanding architecture and diagnosing various problems. Built on a graph-based representation, GMTA includes efficient processing of traces produced on the fly. It abstracts traces into different paths and further groups them into business flows. To support various analytical applications, GMTA includes an efficient storage and access mechanism by combining a graph database and a real-time analytics database and\u00a0\u2026", "num_citations": "4\n", "authors": ["186"]}
{"title": "Benchmarking Meaning Representations in Neural Semantic Parsing\n", "abstract": " Meaning representation is an important component of semantic parsing. Although researchers have designed a lot of meaning representations, recent work focuses on only a few of them. Thus, the impact of meaning representation on semantic parsing is less understood. Furthermore, existing work\u2019s performance is often not comprehensively evaluated due to the lack of readily-available execution engines. Upon identifying these gaps, we propose UNIMER, a new unified benchmark on meaning representations, by integrating existing semantic parsing datasets, completing the missing logical forms, and implementing the missing execution engines. The resulting unified benchmark contains the complete enumeration of logical forms and execution engines over three datasets\u00d7 four meaning representations. A thorough experimental study on UNIMER reveals that neural semantic parsing approaches exhibit notably different performance when they are trained to generate different meaning representations. Also, program alias and grammar rules heavily impact the performance of different meaning representations. Our benchmark, execution engines and implementation can be found on: https://github. com/JasperGuo/Unimer.", "num_citations": "4\n", "authors": ["186"]}
{"title": "Testing Access Control Policies\n", "abstract": " As software systems become more and more complex, and are deployed to manage a large amount of sensitive information and resources, specifying and managing correct access control policies is critical and yet challenging. Policy testing is an important means to increasing confidence in the correctness of specified policies and their implementations for access control. There are two types of policy testing. In the first type, the artifacts under test are policy specifications and the main testing goal is to assure the correctness of the policy specifications. In the second type, the artifacts under test are policy implementations and the main testing goal is to assure the conformance between the policy specifications and implementations. Both types of policy testing supply typical test inputs (requests) to the artifacts under test and subsequently check test outputs (responses) against expected ones. This article presents recent approaches on policy testing in five main categories: fault models, testing criteria, test generation, test oracles, and model-based testing.", "num_citations": "4\n", "authors": ["186"]}
{"title": "TSS: Transformation-Specific Smoothing for Robustness Certification\n", "abstract": " Recent advances in machine learning (ML) have enabled a plethora of applications in tasks such as image recognition [19] and game playing [36, 46]. Despite all of these advances, ML systems are also found exceedingly vulnerable to adversarial attacks: image recognition systems can be adversarially misled [16, 48, 59], and malware detection systems can be evaded easily [52, 62]. The existing practice of security in ML has fallen into the cycle where new empirical defense techniques are proposed [31, 54], followed by new adaptive attacks breaking these defenses [1, 12, 16, 60]. In response, recent research has attempted to provide provable robustness guarantees for an ML model. Such certification usually follows the form that the ML model is provably robust against", "num_citations": "3\n", "authors": ["186"]}
{"title": "Probabilistic and Systematic Coverage of Consecutive Test-Method Pairs for Detecting Order-Dependent Flaky Tests\n", "abstract": " Software developers frequently check their code changes by running a set of tests against their code. Tests that can nondeterministically pass or fail when run on the same code version are called flaky tests. These tests are a major problem because they can mislead developers to debug their recent code changes when the failures are unrelated to these changes. One prominent category of flaky tests is order-dependent (OD) tests, which can deterministically pass or fail depending on the order in which the set of tests are run. By detecting OD tests in advance, developers can fix these tests before they change their code. Due to the high cost required to explore all possible orders (n! permutations for n tests), prior work has developed tools that randomize orders to detect OD tests. Experiments have shown that randomization can detect many OD tests, and that most OD tests depend on just one other test to fail. However, there was no analysis of the probability that randomized orders detect OD tests. In this paper, we present the first such analysis and also present a simple change for sampling random test orders to increase the probability. We finally present a novel algorithm to systematically explore all consecutive pairs of tests, guaranteeing to detect all OD tests that depend on one other test, while running substantially fewer orders and tests than simply running all test pairs.", "num_citations": "3\n", "authors": ["186"]}
{"title": "A Preliminary Field Study of Game Programming on Mobile Devices\n", "abstract": " TouchDevelop is a new programming environment that allows users to create applications on mobile devices. Applications created with TouchDevelop have continued to grow in popularity since TouchDevelop was first released to public in 2011. This paper presents a field study of 31,699 applications, focusing on different characteristics between 539 game scripts and all other non-game applications, as well as what make some game applications more popular than others to users. The study provides a list of findings on characteristics of game scripts and also implications for improving end-user programming of game applications.", "num_citations": "3\n", "authors": ["186"]}
{"title": "NEGWeb: Detecting Neglected Conditions via Mining Programming Rules from Open Source Code\n", "abstract": " Neglected conditions, also referred as missing paths, are known to be an important class of software defects. Revealing neglected conditions around individual API calls in an application requires the knowledge of programming rules that must be obeyed while reusing those APIs. To mine those implicit programming rules and hence to detect neglected conditions, we develop a novel framework, called NEGWeb, that substantially expands mining scope to billions of lines of open source code available on the web by leveraging a code search engine. We evaluated NEGWeb to detect violations of mined rules in local code bases or open source code bases. In our evaluation, we show that NEGWeb finds three real defects in Java code reported in the literature and also finds three previously unknown defects in a large-scale open source project called Columba (91, 508 lines of Java code) that reuses 541 classes and 2225 methods. We also report a high percentage of real rules among the top 25 reported patterns mined for APIs provided by five popular open source applications.", "num_citations": "3\n", "authors": ["186"]}
{"title": "Improving effectiveness of automated software testing in the absence of specifications\n", "abstract": " This dissertation presents techniques for improving effectiveness of automated software testing in the absence of specifications, evaluates the efficacy of these techniques, and proposes directions for future research.", "num_citations": "3\n", "authors": ["186"]}
{"title": "An infrastructure approach to improving effectiveness of Android UI testing tools\n", "abstract": " Due to the importance of Android app quality assurance, many Android UI testing tools have been developed by researchers over the years. However, recent studies show that these tools typically achieve low code coverage on popular industrial apps. In fact, given a reasonable amount of run time, most state-of-the-art tools cannot even outperform a simple tool, Monkey, on popular industrial apps with large codebases and sophisticated functionalities. Our motivating study finds that these tools perform two types of operations, UI Hierarchy Capturing (capturing information about the contents on the screen) and UI Event Execution (executing UI events, such as clicks), often inefficiently using UIAutomator, a component of the Android framework. In total, these two types of operations use on average 70% of the given test time.", "num_citations": "2\n", "authors": ["186"]}
{"title": "Understanding and finding system setting-related defects in Android apps\n", "abstract": " Android, the most popular mobile system, offers a number of user-configurable system settings (eg, network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first empirical study to understand the characteristics of these setting-related defects (in short as\" setting defects\"), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over three person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate their impact, root causes, and consequences. We find that setting defects have a wide, diverse impact on apps' correctness, and the majority of these defects (\u2248 70.7%) cause non-crash (logic) failures\u00a0\u2026", "num_citations": "2\n", "authors": ["186"]}
{"title": "Grading-Based Test Suite Augmentation\n", "abstract": " Enrollment in introductory programming (CS1) courses continues to surge and hundreds of CS1 students can produce thousands of submissions for a single problem, all requiring timely and accurate grading. One way that instructors can efficiently grade is to construct a custom instructor test suite that compares a student submission to a reference solution over randomly generated or hand-crafted inputs. However, such test suite is often insufficient, causing incorrect submissions to be marked as correct. To address this issue, we propose the Grasa (GRAding-based test Suite Augmentation) approach consisting of two techniques. Grasa first detects and clusters incorrect submissions by approximating their behavioral equivalence to each other. To augment the existing instructor test suite, Grasa generates a minimal set of additional tests that help detect the incorrect submissions. We evaluate our Grasa approach on a\u00a0\u2026", "num_citations": "2\n", "authors": ["186"]}
{"title": "Research methodology on pursuing impact-driven research\n", "abstract": " \u201cBecause anybody who thinks that we\u2019re just here because we\u2019re smart forgets that we\u2019re also privileged, and we have to extend that farther. So we\u2019ve got to educate and help every generation and we all have to keep it up in lots of ways.\u201d\u2013David Notkin, 1955-2013", "num_citations": "2\n", "authors": ["186"]}
{"title": "FACTS: Automated Black-Box Testing of FinTech Systems\n", "abstract": " FinTech, short for``financial technology,''has advanced the process of transforming financial business from a traditional manual-process-driven to an automation-driven model by providing various software platforms. However, the current FinTech-industry still heavily depends on manual testing, which becomes the bottleneck of FinTech industry development. To automate the testing process, we propose an approach of black-box testing for a FinTech system with effective tool support for both test generation and test oracles. For test generation, we first extract input categories from business-logic specifications, and then mutate real data collected from system logs with values randomly picked from each extracted input category. For test oracles, we propose a new technique of priority differential testing where we evaluate execution results of system-test inputs on the system's head (ie, latest) version in the version\u00a0\u2026", "num_citations": "2\n", "authors": ["186"]}
{"title": "Visualizing Path Exploration to Assist Problem Diagnosis for Structural Test Generation\n", "abstract": " Dynamic Symbolic Execution (DSE) is among the most effective techniques for structural test generation, i.e., test generation to achieve high structural coverage. Despite its recent success, DSE still suffers from various problems such as the boundary problem when applied on various programs in practice. To assist problem diagnosis for structural test generation, in this paper, we propose a visualization approach named PexViz. Our approach helps the tool users better understand and diagnose the encountered problems by reducing the large search space for problem root causes by aggregating information gathered through DSE exploration.", "num_citations": "2\n", "authors": ["186"]}
{"title": "A Characteristic Study of Parameterized Unit Tests in. NET Open Source Projects\n", "abstract": " In the past decade, parameterized unit testing has emerged as a promising method to specify program behaviors under test in the form of unit tests. Developers can write parameterized unit tests (PUTs), unit-test methods with parameters, in contrast to conventional unit tests, without parameters. The use of PUTs can enable powerful test generation tools such as Pex to have strong test oracles to check against, beyond just uncaught runtime exceptions. In addition, PUTs have been popularly supported by various unit testing frameworks for .NET and the JUnit framework for Java. However, there exists no study to offer insights on how PUTs are written by developers in either proprietary or open source development practices, posing barriers for various stakeholders to bring PUTs to widely adopted practices in software industry. To fill this gap, we first present categorization results of the Microsoft MSDN Pex Forum posts (contributed primarily by industrial practitioners) related to PUTs. We then use the categorization results to guide the design of the first characteristic study of PUTs in .NET open source projects. We study hundreds of PUTs that open source developers wrote for these open source projects. Our study findings provide valuable insights for various stakeholders such as current or prospective PUT writers (eg, developers), PUT framework designers, test-generation tool vendors, testing researchers, and testing educators.", "num_citations": "2\n", "authors": ["186"]}
{"title": "Mining API usage specifications via searching source code from the web\n", "abstract": " The emergence of the web has revolutionized traditional software development. In modern software development, programmers often reuse or adapt existing frameworks or libraries rather than developing similar artifacts from scratch. Furthermore, programmers often learn how to reuse Application Programming Interfaces (APIs) provided by these frameworks or libraries via searching for relevant code examples from open source code, transforming traditional software development to search-driven development. Therefore, open source code available on the web has become a common platform for sharing", "num_citations": "2\n", "authors": ["186"]}
{"title": "10111 executive summary\u2013practical software testing: Tool automation and human factors\n", "abstract": " The main goal of the seminar``Practical Software Testing: Tool Automation and Human Factors''was to bring together academics working on algorithms, methods, and techniques for practical software testing, with practitioners, interested in developing more soundly-based and well-understood testing processes and practices. The seminar's purpose was to make researchers aware of industry's problems, and practitioners aware of research approaches. The seminar focused in particular on testing automation and human factors. In the week of March 14-19, 2010, 40 researchers from 11 countries (Canada, France, Germany, Italy, Luxembourg, the Netherlands, Sweden, Switzerland, South Africa, United Kingdom, United States) discussed their recent work, and recent and future trends in software testing. The seminar consisted of five main types of presentations or activities: topic-oriented presentations, research-oriented presentations, short self-introduction presentations, tool demos, and working group meetings and presentations.", "num_citations": "2\n", "authors": ["186"]}
{"title": "Identifying security fault reports via text mining\n", "abstract": " A fault-tracking (bug-tracking) system such as Bugzilla contains fault reports (FRs) collected from various sources such as development teams, test teams, and end-users. Software or security engineers manually analyze the FRs to label the subset of FRs that are security fault reports (SFRs), which indicate a security problem. These SFRs generally deserve higher priority in fault fixing than the not-security fault reports (NSFRs). However, this manual process is time consuming and error-prone (eg mislabeling an SFR as an NSFR). To address these important issues, we developed a new approach that applies text mining natural-language descriptions of FRs to train a statistical model on already manually-labeled FRs to identify unlabeled SFRs or SFRs that are manually-mislabeled as NSFRs. A security team can use the model to automate the classification of FRs for large fault databases to reduce the time that they spend on searching for SFRs. We evaluated the model's predictions on a large Cisco software system with over ten million source lines of code. Among a sample of FRs that Cisco software engineers manually labeled as NSFRs, our model successfully classified a high percentage (78%) of the SFRs as verified by a Cisco security team, and predicted their classification as SFRs with a probability of at least 0.98. Our results also indicate that a high percentage (77%) of the SFRs identified by our model is associated with software components that a code-level statistical model predicted to be attack-prone. Such findings provided valuable insights for calling for a future combined approach that exploits both textual information of FRs and code\u00a0\u2026", "num_citations": "2\n", "authors": ["186"]}
{"title": "Toward Systematic Testing of Access Control Policies\n", "abstract": " To facilitate managing access control in a system, access control policies are increasingly written in specification languages such as XACML. A dedicated software component called a Policy Decision Point (PDP) interprets the specified policies, receives access requests, and returns responses to inform whether access should be permitted or denied. To increase confidence in the correctness of specified policies, policy developers can conduct policy testing by supplying typical test inputs (requests) to the PDP and subsequently checking test outputs (responses) against expected ones. Unfortunately, manual testing is tedious and few tools exist for automated testing of XACML policies.In this paper, we present our work toward a framework for systematic testing of access control policies. The framework includes components for policy coverage definition and measurement, request generation, request evaluation, request set minimization, policy property inference, and mutation testing. This framework allows us to evaluate various criteria for test generation and selection, investigate mutation operators, and determine a relationship between structural coverage and fault-detection capability. We have implemented the framework and applied it to various XACML policies. Our experimental results offer valuable insights into choosing mutation operators in mutation testing and choosing coverage criteria in test generation and selection.", "num_citations": "2\n", "authors": ["186"]}
{"title": "Object Oriented Software Metrics Technology\n", "abstract": " 1.1 The Need for Software Metrics 1.2 Definition of Software Metrics 1.3 Classification of Software Metrics", "num_citations": "2\n", "authors": ["186"]}
{"title": "Data-Driven Investigation into Variants of Code Writing Questions\n", "abstract": " To defend against collaborative cheating in code writing questions, instructors of courses with online, asynchronous exams can use the strategy of question variants. These question variants are manually written questions to be selected at random during exam time to assess the same learning goal. In order to create these variants, currently the instructors have to rely on intuition to accomplish the competing goals of ensuring that variants are different enough to defend against collaborative cheating, and yet similar enough where students are assessed fairly. In this paper, we propose data-driven investigation into these variants. We apply our data-driven investigation into a dataset of three midterm exams from a large introductory programming course. Our results show that (1) observable inequalities of student performance exist between variants and (2) these differences are not just limited to score. Our results also\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}
{"title": "Clustering test steps in natural language toward automating test automation\n", "abstract": " For large industrial applications, system test cases are still often described in natural language (NL), and their number can reach thousands. Test automation is to automatically execute the test cases. Achieving test automation typically requires substantial manual effort for creating executable test scripts from these NL test cases. In particular, given that each NL test case consists of a sequence of NL test steps, testers first implement a test API method for each test step and then write a test script for invoking these test API methods sequentially for test automation. Across different test cases, multiple test steps can share semantic similarities, supposedly mapped to the same API method. However, due to numerous test steps in various NL forms under manual inspection, testers may not realize those semantically similar test steps and thus waste effort to implement duplicate test API methods for them. To address this\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}
{"title": "Understanding Challenges in Deploying Deep Learning Based Software: An Empirical Study\n", "abstract": " Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}
{"title": "Quality Assessment for Large-Scale Industrial Software Systems: Experience Report at Alibaba\n", "abstract": " To assure high software quality for large-scale industrial software systems, traditional approaches of software quality assurance, such as software testing and performance engineering, have been widely used within Alibaba, the world's largest retailer, and one of the largest Internet companies in the world. However, there still exists a high demand for software quality assessment to achieve high sustainability of business growth and engineering culture in Alibaba. To address this issue, we develop an industrial solution for software quality assessment by following the GQM paradigm in an industrial setting. Moreover, we integrate multiple assessment methods into our solution, ranging from metric selection to rating aggregation. Our solution has been implemented, deployed, and adopted at Alibaba: (1) used by Alibaba's Business Platform Unit to continually monitor the quality for 60+ core software systems; (2) used by\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}
{"title": "FinExpert: domain-specific test generation for FinTech systems\n", "abstract": " To assure high quality of software systems, the comprehensiveness of the created test suite and efficiency of the adopted testing process are highly crucial, especially in the FinTech industry, due to a FinTech system\u2019s complicated system logic, mission-critical nature, and large test suite. However, the state of the testing practice in the FinTech industry still heavily relies on manual efforts. Our recent research efforts contributed our previous approach as the first attempt to automate the testing process in China Foreign Exchange Trade System (CFETS) Information Technology Co. Ltd., a subsidiary of China\u2019s Central Bank that provides China\u2019s foreign exchange transactions, and revealed that automating test generation for such complex trading platform could help alleviate some of these manual efforts. In this paper, we investigate further the dilemmas faced in testing the CFETS trading platform, identify the importance\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}
{"title": "Aladdin: automating release of Android deep links to in-app content\n", "abstract": " Unlike the Web where each web page has a global URL to reach, a specific \"content page\" inside a mobile app cannot be opened unless the user explores the app with several operations from the landing page. Recently, deep links have been advocated by major companies to enable targeting and opening a specific page of an app externally with an accessible uniform resource identifier (URI). In this paper, we present an empirical study of deep links over 20,000 Android apps, and find that deep links do not get wide adoption among current Android apps, and non-trivial manual efforts are required for app developers to support deep links. To address such an issue, we propose the Aladdin approach and supporting tool to release deep links to access arbitrary locations of existing apps. We evaluate Aladdin with popular apps and demonstrate its effectiveness and performance.", "num_citations": "1\n", "authors": ["186"]}
{"title": "Preliminary analysis of code hunt data set from a contest\n", "abstract": " Code Hunt (https://www. codehunt. com/) from Microsoft Research is a web-based serious gaming platform being popularly used for various programming contests. In this paper, we demonstrate preliminary statistical analysis of a Code Hunt data set that contains the programs written by students (only) worldwide during a contest over 48 hours. There are 259 users, 24 puzzles (organized into 4 sectors), and about 13,000 programs submitted by these users. Our analysis results can help improve the creation of puzzles in a future contest.", "num_citations": "1\n", "authors": ["186"]}
{"title": "Mining usage data from large-scale Android users: challenges and opportunities\n", "abstract": " Mining usage data from a large number of Android users can assist various software engineering tasks. In collaboration with Wandoujia, a leading Android app marketplace in China, we have conducted a large empirical analysis based on mining app usage behaviors collected from millions of Android users. Our empirical findings can provide implications, challenges, and opportunities to app-centric software development, deployment, and maintenance.", "num_citations": "1\n", "authors": ["186"]}
{"title": "Text analytics for security: tutorial\n", "abstract": " Computing systems that make security decisions often fail to take into account human expectations. This failure occurs because human expectations are typically drawn from in textual sources (eg, mobile application description and requirements documents) and are hard to extract and codify. Recently, researchers in security and software engineering have begun using text analytics to create initial models of human expectation. In this tutorial, we provide an introduction to popular techniques and tools of natural language processing (NLP) and text mining, and share our experiences in applying text analytics to security problems. We also highlight the current challenges of applying these techniques and tools for addressing security problems. We conclude the tutorial with discussion of future research directions.", "num_citations": "1\n", "authors": ["186"]}
{"title": "The Pursuit of Practice-Impactful Research\n", "abstract": " In the software engineering research community, there can be different target audiences that researchers\u2019 research aims to produce direct impact for [1]. For example, some researchers conduct their research to produce direct impact on software engineering practices (which broadly include those for both proprietary software and open-source software); some other researchers conduct their research to produce direct impact on other researchers in the research community (sometimes also indirect impact on software engineering practices eventually). While continuing focusing on basic research [2], Microsoft Research has started \u201cMicrosoft Research New Experiences and Technologies, or MSR NExT, an organization of world-class researchers, engineers, and designers devoted to creating potentially disruptive technologies for Microsoft and the world. While NExT will continue to advance the field of computing\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}
{"title": "Learning and Celebration of Software Engineering History and Impact\n", "abstract": " I am wearing my hats of being the SIGSOFT History Liaison and an ACM History Committee Member to write this letter. The main purpose of this letter to inform you that we will initiate the column of \u201cHistory and Impact\u201d in SEN starting from 2015, as I briefly announced in the FSE 2014 SIGSOFT town hall meeting. To make this column a success, your helps and contributions will be strongly needed. In particular, you are strongly encouraged and welcome to submit contributed articles, suggest discussion topics, etc. to this column. Please send emails (to taoxie@ illinois. edu) with your submissions or ideas.The scope of the column is very broad: any topics related to history and impact of software engineering research, practice, and education will be within the scope. Example contributions can be on the formal and heavyweight side, similar to those publications (http://www. sigsoft. org/impact/publications. htm) resulted\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}
{"title": "Paradigm in Verification of Access Control\n", "abstract": " Access control (AC) is one of the most fundamental and widely used requirements for privacy and security. Given a subject's access request on a resource in a system, AC determines whether this request is permitted or denied based on AC policies (ACPs). This position paper introduces our approach to ensure the correctness of AC using verification. More specifically, given a model of an ACP, our approach detects inconsistencies between models, specifications, and expected behaviors of AC. Such inconsistencies represent faults (in the ACP), which we target at detecting before ACP deployment.", "num_citations": "1\n", "authors": ["186"]}
{"title": "Designing fast and scalable policy evaluation engines\n", "abstract": " Most prior research on policies has focused on correctness. While correctness is an important issue, the adoption of policybased computing may be limited if the resulting systems are not implemented efficiently and thus perform poorly. To increase the effectiveness and adoption of policy-based computing, in this paper, we propose fast policy evaluation algorithms that can be adapted to support various policy languages. In this paper, we focus on XACML policy evaluation because XACML has become the de facto standard for specifying access control policies, has been widely used on web servers, and is most complex among existing policy languages. We implemented our algorithms in a policy evaluation system called XEngine and conducted side-by-side comparison with Sun PDP, the industrial standard for XACML policy evaluation. The results show that XEngine is orders of magnitude faster than Sun PDP. The performance difference grows almost linearly with the number of rules in an XACML policy. To our best knowledge, there is no prior work on improving the performance of XACML policy evaluation. This paper represents the first step in exploring this unknown space.", "num_citations": "1\n", "authors": ["186"]}
{"title": "Workshop on testing, analysis and verification of web software (TAV-WEB 2008)\n", "abstract": " TAV-WEB 2008 is the third in a series of workshops that focus on testing, analysis and verification of web software. The goal of these workshops has been to bring together researchers from academic, research, and industrial communities interested in the emerging area of dependable Web software development, to present and discuss their recent research results.", "num_citations": "1\n", "authors": ["186"]}
{"title": "SpotWeb: Characterizing Framework API Usages Through a Code Search Engine\n", "abstract": " The essentials of modern software development (such as low cost and high efficiency) demand software developers to make intensive reuse of the existing open source frameworks or libraries (generally referred as frameworks) available on the web. However, developers often face challenges in reusing these frameworks due to several factors such as the complexity and lack of proper documentation. In this paper, we propose a code-search-engine-based approach that tries to detect hotspots in a given framework; these hotspots are the APIs that are frequently reused. Hotspots can serve as starting points for developers in understanding and reusing the given framework. Our approach also detects deadspots, which are the APIs that are rarely used. Deadspots serve as caveats for developers as there can be difficulties in finding related code examples and are generally less exercised compared to hotspots. We developed a tool, called SpotWeb, for frameworks or libraries written in the Java programming language and used our tool to detect hotspots and deadspots of eight open source frameworks including JUnit, Log4j, Grappa, JGraphT, OpenJGraph, JUNG, BCEL, and Javassit.", "num_citations": "1\n", "authors": ["186"]}
{"title": "A Linguistic Study of Process Modeling Languages\n", "abstract": " Process model or definition is used to describe the process, either software process or business process, by means of a suitable process modeling language. Choosing one or more linguistic paradigms for a process modeling language is an important design decision during process modeling language design. This paper focuses on the study of process modeling languages from linguistic perspective. To understand what key information in process domain needs to be expressed, we propose a process conceptual framework. Existing process modeling languages can be categorized to one or more of the rule-based, state-based, functional, procedural, or object-oriented paradigm based on their linguistic features. Each paradigm has its drawbacks or benefits in describing different aspects of the process. In order to better support process evolution, several linguistic requirements, including generic, reflective, exception handling, and deviation/inconsistency support, are discussed. Especially there is no effective linguistic support for process genericity, we propose a open research question of whether higher-order function genericity could be an effective solution for process genericity. VoiceXML, the Voice Extensible Markup Language, is a language for describing call flows for Interactive Voice Response applications. PASTA, Process and Artifact State Transition Abstraction, is a process modeling language to model complex processes in a graphical, systematic, precise, and structured way. We perform a linguistic analysis on them and propose some potential extensions for them.", "num_citations": "1\n", "authors": ["186"]}
{"title": "Object Oriented Software Quality Evaluation Technology\n", "abstract": " 1.1 The Need for Software Metrics 1.2 Definition of Software Metrics 1.3 Classification and Activities of Software Metrics", "num_citations": "1\n", "authors": ["186"]}
{"title": "C++ Program Information Database for Analysis Tools\n", "abstract": " Program information extracted from source codes is valuable for research in many software engineering fields. Many program analysis tools in these fields usually share some common program information. To support multiple analysis tools based on common program information, it is practical and feasible to store information into database. This paper describes a C++ program information database, which is comprehensive enough to support many analysis tools. To employ the idea of incremental paring, the C++ program information database is linked by multiple incremental databases, which, in turn, are built by extracting information from source codes according to a C++ program conceptual model.", "num_citations": "1\n", "authors": ["186"]}
{"title": "Helping Users Avoid Bugs in GUI Applications\n", "abstract": " \u25cf(Both GUI and non-GUI) applications are buggy\u25cf bug number: Mozilla browser (20,000 open bugs)\u25cf bug life: Linux bugs (average 1.8 yrs, median 1.25 yrs)", "num_citations": "1\n", "authors": ["186"]}
{"title": "Model Checking Grid Policies\n", "abstract": " Grid computing provides an infrastructure to share computing and storage resources of a large number of remotely distributed local sites (eg, computers and storage providers). To facilitate management distributed resources that can be accessible by members of a Virtual Organization (VO), the local sites include various grid policies (called site policies) that manage access control, job priority, and scheduling. In this situation, small faults in the policies may introduce incorrect policy behaviors, which could result in serious consequence. To help specify the site policies, we propose an approach to combine rules of multiple policies based on user-specified properties. In this paper, properties are partial or all of expected policy behaviors for either VO or site policies. Our approach verifies the a combined policy by applying existing combinations in and reports those combinations that could satisfy the specified properties\u00a0\u2026", "num_citations": "1\n", "authors": ["186"]}