{"title": "Observation of a new boson at a mass of 125 GeV with the CMS experiment at the LHC\n", "abstract": " Results are presented from searches for the standard model Higgs boson in proton\u0393\u00c7\u00f4proton collisions at s= 7 and 8 TeV in the Compact Muon Solenoid experiment at the LHC, using data samples corresponding to integrated luminosities of up to 5.1 fb\u0393\u00ea\u00c6 1 at 7 TeV and 5.3 fb\u0393\u00ea\u00c6 1 at 8 TeV. The search is performed in five decay modes: \u256c\u2502\u256c\u2502, ZZ, W+ W\u0393\u00ea\u00c6, \u2567\u00e4+ \u2567\u00e4\u0393\u00ea\u00c6, and b b\u252c\u00bb. An excess of events is observed above the expected background, with a local significance of 5.0 standard deviations, at a mass near 125 GeV, signalling the production of a new particle. The expected significance for a standard model Higgs boson of that mass is 5.8 standard deviations. The excess is most significant in the two decay modes with the best mass resolution, \u256c\u2502\u256c\u2502 and ZZ; a fit to these signals gives a mass of 125.3\u252c\u25920.4 (stat.)\u252c\u25920.5 (syst.) GeV. The decay to two photons indicates that the new particle is a boson with spin different from one.", "num_citations": "17581\n", "authors": ["33"]}
{"title": "The CMS experiment at the CERN LHC\n", "abstract": " The Compact Muon Solenoid (CMS) detector is described. The detector operates at the Large Hadron Collider (LHC) at CERN. It was conceived to study proton-proton (and leadlead) collisions at a centre-of-mass energy of 14 TeV (5.5 TeV nucleon-nucleon) and at luminosities up to 1034 cm\u0393\u00ea\u00c6 2s\u0393\u00ea\u00c6 1 (1027 cm\u0393\u00ea\u00c6 2s\u0393\u00ea\u00c6 1). At the core of the CMS detector sits a high-magneticfield and large-bore superconducting solenoid surrounding an all-silicon pixel and strip tracker, a lead-tungstate scintillating-crystals electromagnetic calorimeter, and a brass-scintillator sampling hadron calorimeter. The iron yoke of the flux-return is instrumented with four stations of muon detectors covering most of the 4\u2567\u00c7 solid angle. Forward sampling calorimeters extend the pseudorapidity coverage to high values (| \u256c\u2556|\u0393\u00eb\u00f1 5) assuring very good hermeticity. The overall dimensions of the CMS detector are a length of 21.6 m, a diameter of 14.6 m and a total weight of 12500 t.", "num_citations": "10789\n", "authors": ["33"]}
{"title": "Agata\u0393\u00c7\u00f6advanced gamma tracking array\n", "abstract": " Abstract The Advanced GAmma Tracking Array (AGATA) is a European project to develop and operate the next generation \u256c\u2502-ray spectrometer. AGATA is based on the technique of \u256c\u2502-ray energy tracking in electrically segmented high-purity germanium crystals. This technique requires the accurate determination of the energy, time and position of every interaction as a \u256c\u2502 ray deposits its energy within the detector volume. Reconstruction of the full interaction path results in a detector with very high efficiency and excellent spectral response. The realisation of \u256c\u2502-ray tracking and AGATA is a result of many technical advances. These include the development of encapsulated highly segmented germanium detectors assembled in a triple cluster detector cryostat, an electronics system with fast digital sampling and a data acquisition system to process the data at a high rate. The full characterisation of the crystals was\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "478\n", "authors": ["33"]}
{"title": "Conceptual design and infrastructure for the installation of the first AGATA sub-array at LNL\n", "abstract": " The first implementation of the AGATA spectrometer consisting of five triple germanium detector clusters has been installed at Laboratori Nazionali di Legnaro, INFN. This setup has two major goals, the first one is to validate the \u256c\u2502-tracking concept and the second is to perform an experimental physics program using the stable beams delivered by the Tandem\u0393\u00c7\u00f4PIAVE-ALPI accelerator complex. A large variety of physics topics will be addressed during this campaign, aiming to investigate both neutron and proton-rich nuclei. The setup has been designed to be coupled with the large-acceptance magnetic-spectrometer PRISMA. Therefore, the in-beam prompt \u256c\u2502 rays detected with AGATA will be measured in coincidence with the products of multinucleon-transfer and deep-inelastic reactions measured by PRISMA. Moreover, the setup is versatile enough to host ancillary detectors, including the heavy-ion detector DANTE\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "152\n", "authors": ["33"]}
{"title": "Blueprinting approach in support of cloud computing\n", "abstract": " Current cloud service offerings, ie, Software-as-a-service (SaaS), Platform-as-a-service (PaaS) and Infrastructure-as-a-service (IaaS) offerings are often provided as monolithic, one-size-fits-all solutions and give little or no room for customization. This limits the ability of Service-based Application (SBA) developers to configure and syndicate offerings from multiple SaaS, PaaS, and IaaS providers to address their application requirements. Furthermore, combining different independent cloud services necessitates a uniform description format that facilitates the design, customization, and composition. Cloud Blueprinting is a novel approach that allows SBA developers to easily design, configure and deploy virtual SBA payloads on virtual machines and resource pools on the cloud. We propose the Blueprint concept as a uniform abstract description for cloud service offerings that may cross different cloud computing layers, ie, SaaS, PaaS and IaaS. To support developers with the SBA design and development in the cloud, this paper introduces a formal Blueprint Template for unambiguously describing a blueprint, as well as a Blueprint Lifecycle that guides developers through the manipulation, composition and deployment of different blueprints for an SBA. Finally, the empirical evaluation of the blueprinting approach within an EC\u0393\u00c7\u00d6s FP7 project is reported and an associated blueprint prototype implementation is presented. View Full-Text", "num_citations": "65\n", "authors": ["33"]}
{"title": "Blueprint template support for engineering cloud-based services\n", "abstract": " Current cloud-based service offerings are often provided as one-size-fits-all solutions and give little or no room for customization. This limits the ability for application developers to pick and choose offerings from multiple software, platform, infrastructure service providers and configure them dynamically and in an optimal fashion to address their application requirements. Furthermore, combining different independent cloud-based services necessitates a uniform description format that facilitates their design, customization, and composition. Hence, there is a need to break down the monolithic offerings into loosely-coupled cloud services offered by multiple providers that can be flexibly customized and (re-)composed in different settings. We propose in this paper the Blueprint concept - a uniform abstract description for cloud service offerings that may cross different cloud computing layers, i.e. software, platform\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["33"]}
{"title": "Instrument Element: a new Grid component that enables the control of remote instrumentation\n", "abstract": " Current grid technologies offer unlimited computational power and storage capacity for scientific research and business activities in heterogeneous areas over the world. Thanks to the grid, different virtual organizations can operate together in order to achieve common goals. However, concrete use cases demand a more close interaction between various types of instruments accessible from the grid, and the classical grid infrastructure, typically composed of computing and storage elements. We cope with this open problem by proposing and realizing the first release of the Instrument Element, i.e., a new grid component that provides the computational/data grid with an abstraction of real instruments, and grid users with a more interactive interface to control them. In this paper, we discuss in detail the proposed software architecture for this new component, then we report some performance results concerning its first\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["33"]}
{"title": "4CaaSt: Comprehensive management of Cloud services through a PaaS\n", "abstract": " The 4CaaSt project aims at developing a PaaS framework that enables flexible definition, marketing, deployment and management of Cloud-based services and applications. The major innovations proposed by 4CaaSt are the blueprint and its lifecycle management, a one stop shop for Cloud services and a PaaS level resource management featuring elasticity. 4CaaSt also provides a portfolio of ready to use Cloud native services and Cloud-aware immigrant technologies.", "num_citations": "36\n", "authors": ["33"]}
{"title": "The many faces of the integration of instruments and the grid\n", "abstract": " Current grid technologies offer unlimited computational power and storage capacity for scientific research and business activities in heterogeneous areas all over the world. Thanks to the grid, different virtual organisations can operate together in order to achieve common goals. However, concrete use cases demand a closer interaction between various types of instruments accessible from the grid on the one hand and the classical grid infrastructure, typically composed of Computing and Storage Elements, on the other. We cope with this open problem by proposing and realising the first release of the Instrument Element (IE), a new grid component that provides the computational/data grid with an abstraction of real instruments, and grid users with a more interactive interface to control them. In this paper we discuss in detail the implemented software architecture for this new component and we present concrete use\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["33"]}
{"title": "Interoperability of the Time of Industry 4.0 and the Internet of Things\n", "abstract": " Industry 4.0 demands a dynamic optimization of production lines. They are formed by sets of heterogeneous devices that cooperate towards a shared goal. The Internet of Things can serve as a technology enabler for implementing such a vision. Nevertheless, the domain is struggling in finding a shared understanding of the concepts for describing a device. This aspect plays a fundamental role in enabling an \u0393\u00c7\u00a3intelligent interoperability\u0393\u00c7\u00a5 among sensor and actuators that will constitute a dynamic Industry 4.0 production line. In this paper, we summarize the efforts of academics and practitioners toward describing devices in order to enable dynamic reconfiguration by machines or humans. We also propose a set of concepts for describing devices, and we analyze how present initiatives are covering these aspects. View Full-Text", "num_citations": "22\n", "authors": ["33"]}
{"title": "Improving the performance of XML based technologies by caching and reusing information\n", "abstract": " The growing synergy between Web services and grid-based technologies is enabling profound, dynamic interactions between applications dispersed in geographic, institutional, and conceptual space. Such deep interoperability requires the simplicity, robustness, and extensibility for which XML has been conceived, making it a natural lingua franca for the network. Along with these advantages, there is a degree of inefficiency that may limit the applicability of XML. Firstly, we investigate the limitations of XML for high-performance and high-interactive distributed computing. Our experimental results clearly show that focusing on parsers, that are routinely used to desterilize XML messages exchanged in these system, we can improve the performance of a generic end to end Web services based solution. Secondly we present a new parser, the cache parser, which uses a cache to reduce the parsing time sender and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "19\n", "authors": ["33"]}
{"title": "On engineering cloud applications-state of the art, shortcomings analysis, and approach\n", "abstract": " Recently, Cloud Computing has become an emerging research topic in response to the shift from product-oriented economy to service-oriented economy and the move from focusing on software/system development to addressing business-IT alignment. From the IT perspectives, there is a proliferation of methods for cloud application development. Such methods have clearly shown considerable shortcomings to provide an efficient solution to deal with major aspects related to cloud applications. One of these major aspects is the multi-tenancy of the Software-as-a-Service (SaaS) components used to compose Service-Based Applications (SBAs) on the cloud. Current SaaS offerings are often provided as monolithic one-size-fits-all solutions and give little or no opportunity for further customization. Monolithic SaaS offerings are more likely to show failure in meeting the business requirements of several consumers. In this paper, we analyze the state-of-the-art of the standardization, methodology, software and product support for SBA development on the cloud, identify some shortcomings, and point out the need of a novel approach for breaking down the monolithic stack of cloud service offerings and providing an effective and flexible solution for SBA designers to select, customize, and aggregate cloud service offerings coming from different providers (25)", "num_citations": "15\n", "authors": ["33"]}
{"title": "Issue in automatic combination of cloud services\n", "abstract": " Current cloud service description languages envision the ability to automatically combine cloud service offerings across multiple abstraction layers, i.e. software, platform, and infrastructure service offerings, to achieve a common shared business goal. However, only little effort has been spent in this direction. This paper formalizes the issue of automatic combination of cloud services showing its computationally intensive nature. In order to overcome this issue we propose a Resource Description Framework (RDF)-based prototype implementation that leverages a batch process for automatically constructing possible combinations of cloud services. Using this approach we are able to analyze possible combinations of cloud services that may fit particular customer needs in a timely fashion.", "num_citations": "12\n", "authors": ["33"]}
{"title": "Towards efficient document content sharing in social networks\n", "abstract": " Social network services have enabled the increasing sharing of digital content (eg, images, videos and audios). However, despite the fact that office documents hold a significant amount of users' digital content, office documents have not yet been sufficiently exploited by social networks. The main reason for this is that existing office document architectures/formats are not open enough for selective access, reuse and commenting of document parts. As a response to this problem we have developed a new document architecture, namely the Semantic Document Architecture (SDArch), which enables the annotation of document content with semantic and social context annotation and provides easy access and reuse of the desired document parts. In this paper we focus on the social context annotation (SCA) that we have introduced to capture implicit information about the usage of document content in the context of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["33"]}
{"title": "Client side estimation of a remote service execution\n", "abstract": " Many use cases, concerning the monitoring and controlling of real physical instruments, demand deep interaction between users and services that virtualize the access to such instruments/devices. In addition, in order to realize high interoperable solutions, SOA-based Web/Grid Service technologies must be adopted. When the access to one of these services is performed via internet using a Web Service call, the remote invocation time becomes critical in order to understand if an instrument can be controlled properly, or the delays introduced by the wire and the serialization/deserialization process are unacceptable. This paper thus presents methodologies and algorithms, based on a 2 k  factorial analysis and a Gaussian Majorization of previous service execution times, which enables the estimation of a generic remote method execution time. Furthermore it suggests three different software architectures, where the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["33"]}
{"title": "The GRIDCC project\n", "abstract": " The GRIDCC [1] project will extend the use of Grid computing to include access to and control of distributed instrumentation. Access to the instruments will be via an interface to a Virtual Instrument Grid Service (VIGS). VIGS is a new concept and its design and implementation, together with middleware that can provide the appropriate quality of service, is a key part of the GRIDCC development plan. An overall architecture for GRIDCC has been defined and some of the application areas, which include distributed power systems, remote control of an accelerator and the remote monitoring of a large particle physics experiment, are briefly discussed.", "num_citations": "7\n", "authors": ["33"]}
{"title": "Towards response time estimation in web services\n", "abstract": " Monitor and control operations demand deep interaction between users and devices, while they require the adoption of high interoperable solutions that only SOA-based Web services can offer. When the access is performed via Internet using Web services calls, the remote invocation time becomes crucial in order to understand if a service can be controlled properly, or the delays introduced by the wire and the serialization/deserialization process are unacceptable. We propose methodologies, based on a 2 factorial analysis and a Gaussian majorization of previous service execution times, which enable the estimation of a generic remote method execution time.", "num_citations": "6\n", "authors": ["33"]}
{"title": "we must keep going i guess\n", "abstract": " Bringing instruments into a Grid: an Empiric Approach Bringing instruments into a grid: An empiric approach \u0393\u00c7\u00f6 Tilburg University Research Portal Skip to main navigation Skip to search Skip to main content Tilburg University Research Portal Logo Contact, Help & FAQ Home Profiles Research Output Research Units Activities Projects Press / Media Prizes / Recognition Bringing instruments into a grid: An empiric approach F. Lelli, G. Maron, S. Orlando, S. Pinter Research output: Contribution to journal \u0393\u00c7\u2551 Article \u0393\u00c7\u2551 Professional 2 Citations (Scopus) Overview Original language English Pages (from-to) 153-159 Journal Transactions on computers Volume 6 Issue number 1 Publication status Published - 2007 Externally published Yes Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Lelli, F., Maron, G., Orlando, S., & Pinter, S. (2007). Bringing instruments into a grid: An empiric approach. Transactions on computers, 6(1), 153-159. Lelli, F. ; Maron, G. ; Orlando, S. ; , -\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["33"]}
{"title": "Bringing instruments to a service-oriented interactive grid\n", "abstract": " Current Grid technologies offer unlimited computational power and storage capacity for scientific research and business activities in heterogeneous areas over the world. Thanks to the Grid, different Virtual Organizations can operate together in order to achieve common goals. However, concrete use cases demand a more close interaction between various types of instruments accessible from the Grid, and the classical Grid infrastructure, typically composed of Computing and Storage Elements. We cope with this open problem by proposing and realizing the first release of the Instrument Element (IE), ie, a new Grid component that provides the Computational/Data Grid with an abstraction of real instruments, and the Grid users with a more interactive interface to control these instruments. In this thesis we describe in detail the proposed software architecture of the IE, also by discussing the functional and non-functional requirements on which its design is based. The non-functional requirements demands not only deep interaction between users and devices to control instruments, but also the adoption of high interoperable solutions, which only Service Oriented Architecture (SOA) based technologies, like Web/Grid Services, can offer. Therefore, in order to solve the trade-off between the necessity of universality/interoperability and performance, we propose a set of solution that improves the performances of a SOA System, in terms of both throughput and latency of service invocations. Moreover, in order to fulfill the Quality of Service (QoS) nonfunctional requirement, we also devise a methodology that allows remote method execution times to be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["33"]}
{"title": "The Swiss National Grid Association and its Experience on a National Grid Infrastructure\n", "abstract": " In the following article we provide our experience with the creation of the Swiss National Grid Association (SwiNG) as well as the establishment of a nation-wide Grid infrastructure based on the ARC Grid middleware. Although not yet fully in production, we already have several scientific user communities in different domains (high energy physics, snow and avalanche related physics, biochemistry, bioinformatics and computer security).", "num_citations": "4\n", "authors": ["33"]}
{"title": "Towards the Development of a Problem Solver for the Monitoring and Control of Instrumentation in a Grid Environment\n", "abstract": " This paper considers the issues involved in developing a generic problem solver to be used within a grid environment for the monitoring and control of instrumentation. The specific feature of such an environment is that the type of data to be processed, as well as the problem, is not always known in advance. Therefore, it is necessary to develop a problem solver architecture that addresses this issue. We propose to analyze the performance of the problem solving algorithms available within the WEKA toolkit and determine a decision tree of the best performing algorithm for a given type of data. For this purpose the algorithms have been tested using 51 datasets either drawn from publicly available repositories or generated in a grid-enabled environment", "num_citations": "4\n", "authors": ["33"]}
{"title": "Supporting domain-specific programming in Web 2.0: a case study of smart devices\n", "abstract": " Web 2.0 communities emerge regularly with the growing need for domain-specific programming over Web APIs. Even though Web mashups provide access to Web APIs, they ignore domain-specific programming needs. On the other hand, developing domain-specific languages (DSLs) is costly and not feasible for such ad hoc communities. We propose User Language Domain (ULD): an intermediate Web-based architecture using a domain-specific embedded languages approach that reduces the cost of DSL development to plugging the Web APIs into a host end user programming language. We have implemented the proposed architecture in the context of smart devices, where we plug the functionality of different Lego Mindstorms devices into a Web-based visual programming language. We expect that several domains, such as smart homes or wearable computers can use the ULD architecture to reduce\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["33"]}
{"title": "The CMS experiment at the CERN LHC\n", "abstract": " The CMS experiment at the CERN LHC \u0393\u00c7\u00f6 Tilburg University Research Portal Skip to main navigation Skip to search Skip to main content Tilburg University Research Portal Logo Contact, Help & FAQ Home Profiles Research Output Research Units Activities Projects Press / Media Prizes / Recognition The CMS experiment at the CERN LHC F. Lelli Research output: Contribution to journal \u0393\u00c7\u2551 Article \u0393\u00c7\u2551 Professional 4644 Citations (Scopus) Overview Original language English Pages (from-to) 1-308 Journal Journal of instrumentation Volume 3 Issue number 8 Publication status Published - 2008 Externally published Yes Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Lelli, F. (2008). The CMS experiment at the CERN LHC. Journal of instrumentation, 3(8), 1-308. Lelli, F. / The CMS experiment at the CERN LHC. In: Journal of instrumentation. 2008 ; Vol. 3, No. 8. pp. 1-308. Lelli, F 2008, 'The CMS experiment '\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["33"]}
{"title": "Grid computing technologies for renewable electricity generator monitoring and control\n", "abstract": " In this paper we discuss the use of real-time Grid computing for the monitoring, control and simulation of renewable electricity generators and their associated electrical networks. We discuss briefly the architectural design of GRIDCC and how we have integrated a number of real (solar, CHP) and simulated conventional power generators into the GRIDCC environment. A local weather station has also been attached to an Instrument Manager to alert experts appropriately when the Solar Array is not generating. The customised remote control and monitoring environment (a virtual control room), distributed using a standard web server, is discussed.", "num_citations": "2\n", "authors": ["33"]}
{"title": "Stock Values and Earnings Call Transcripts: a Dataset Suitable for Sentiment Analysis\n", "abstract": " The dataset reports a collection of earnings call transcripts, the related stock prices, and the related sector index. It contains a total of 188 transcripts, 11970 stock prices, and 1196 sector index values. Furthermore, all of these data originated in the period 2016-2020 and are related to the NASDAQ stock market. The data have been collected using Yahoo Finance and Thomson Reuters Eikon. Specifically, Yahoo Finance offered daily stock prices and traded volume. At the same time, Thomson Reuters Eikon has been used as source for the earnings call transcripts. The dataset can be used as a benchmark for the evaluation of several NLP techniques as well as machine learning algorithms for understanding their potential for financial applications. Moreover, it is also possible to expand the dataset by extending the period in which the data originated following a similar procedure.", "num_citations": "1\n", "authors": ["33"]}
{"title": "Agency in Human-Smart Device Relationships: An Exploratory Study\n", "abstract": " In this paper, we investigate the relationship people have with their smart devices. We use the concept of agency to capture aspects of users\u0393\u00c7\u00d6 sense of mastery as they relate to their device. This study gives preliminary evidence of the existence of two independent dimensions of agency for modeling the interaction between humans and smart devices:(i) user agency and (ii) device agency. These constructs emerged from an exploratory factorial analysis conducted on a survey data collected from 587 participants. In addition, we investigate the correlation between user agency and device agency with background variables of the respondents. Finally, we argue that mapping the users\u0393\u00c7\u00d6 dynamics with their device into user agency and device agency fosters a better understanding of the needs of the users and helps in designing interfaces tailored for the specific capabilities and expectations of the users.", "num_citations": "1\n", "authors": ["33"]}
{"title": "Community support for software development in small groups: the initial steps\n", "abstract": " Communities that support software artifacts are more and more becoming a key success factors for companies and organizations. Members of the community can provide early feedback, patches and support. Following this trend companies release a product under an Open Source license and then sell the support. This paper tries to engineer the process of forming a community around a software artifact in its early stages. We focus our attention in small software such as artifacts produced by small companies or research products where the development is usually carried out by a few developers with limited resources. We present a generic methodology for creating a community around a software project. Our approach has been defined, applied and evaluated in a case study coming from the research filed of distribute applications. Our Empirical results shows that a productive community can be formed in about 4\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["33"]}
{"title": "Using the augmented vector space model to support the knowledge worker in document filtering\n", "abstract": " In the current age the discovery of relevant information is becoming extremely difficult. Users must manually search through the high-recall, low-precision results produced by current search engines. We propose a process which incorporates benefits of automated and manual information extraction. The HSC workbench (Hierarchical Step-wise Categorization) is a tool developed by the Hewlett-Packard Research Group in HP, ESC Galway. It implements this process supporting the user in information filtering tasks, automating segments of the process but giving the user control of the categorization.", "num_citations": "1\n", "authors": ["33"]}
{"title": "Fast Information Transport for an Instrument Enabled Grid\n", "abstract": " Grid-based computing frameworks leverage underutilized processing and storage resources. We present and evaluate a new high-performance, reliable middleware layer that can incorporate instruments into a grid. This Java based messaging system supports remote distributed control and operation of scientific instruments, such as sensors and probes, thereby significantly expanding the grid's capabilities. Various comparative measurements show that our system outperforms the top-ranked publish-sub scribe Java systems in the market. Our sofiware can reach a peak message exchange rate of 900,000 messages per second, with a latency of less than half a millisecond on a 1 GB Ethernet switch.", "num_citations": "1\n", "authors": ["33"]}
{"title": "Bringing instruments into the grid\n", "abstract": " Current Grid technologies offer unlimited computational power and storage capacity for scientific research and business activities in heterogeneous areas over the world. Thanks to the Grid, different Virtual Organizations can operate together in order to achieve common goals. However, concrete use cases demand a more close interaction between various types of instruments accessible from the Grid, and the classical Grid infrastructure, typically composed of Computing and Storage Elements. We cope with this open problem by proposing and realizing the first release of the Instrument Element, ie, a new Grid component that provides the computational/data Grid with an abstraction of real instruments, and Grid users with a more interactive interface to control them. In this poster we discuss in detail the implemented software architecture for this new component and we present concrete use cases, where the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["33"]}
{"title": "with JOpera for Eclipse\n", "abstract": " Thanks to the interoperability offered by Web services standards it is now possible to build large scale distributed software systems by composing reusable services published on the Web. In this presentation we will demonstrate how we combined Eclipse\u0393\u00c7\u00d6s user experience (background model checking and incremental recompilation) with a simple visual composition language to develop the JOpera for Eclipse plugins [1]. With them, it is not required to work with XML-based languages when composing many kinds of different services because their interactions can be specified by literally drawing them in a data and control flow graph. The visual representation is then directly compiled to Java code in order to be executed efficiently. A visual monitor and debugger are also included, so that it is possible to interactively watch the progress of the composition as it runs at the same level of abstraction and with the same visual syntax used to define it.Web services offer a standards-based approach to address many interoperability issues arising when composing distributed software systems out of reusable services. Thanks to the SOAP protocol and WSDL interface description language, an increasingly large number of basic services are being published on the Internet. Clearly, it becomes important to find the right composition abstractions in order to build value added services out of the aggregation of basic ones. Complementing existing approaches based on the XML syntax (eg, BPML, WSBPEL) we have designed a visual syntax for a service composition language [2]. Thus, the data exchanges between the services (data flow), their order of invocation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["33"]}