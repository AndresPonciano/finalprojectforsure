{"title": "Architectural-level risk analysis using UML\n", "abstract": " Risk assessment is an essential part in managing software development. Performing risk assessment during the early development phases enhances resource allocation decisions. In order to improve the software development process and the quality of software products, we need to be able to build risk analysis models based on data that can be collected early in the development process. These models will help identify the high-risk components and connectors of the product architecture, so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product. In this paper, we present a risk assessment methodology which can be used in the early phases of the software life cycle. We use the Unified Modeling Language (UML) and commercial modeling environment Rational Rose Real Time (RoseRT) to obtain UML model statistics. First, for each\u00a0\u2026", "num_citations": "199\n", "authors": ["460"]}
{"title": "Common trends in software fault and failure data\n", "abstract": " The benefits of the analysis of software faults and failures have been widely recognized. However, detailed studies based on empirical data are rare. In this paper, we analyze the fault and failure data from two large, real-world case studies. Specifically, we explore: 1) the localization of faults that lead to individual software failures and 2) the distribution of different types of software faults. Our results show that individual failures are often caused by multiple faults spread throughout the system. This observation is important since it does not support several heuristics and assumptions used in the past. In addition, it clearly indicates that finding and fixing faults that lead to such software failures in large, complex systems are often difficult and challenging tasks despite the advances in software development. Our results also show that requirement faults, coding faults, and data problems are the three most common types of\u00a0\u2026", "num_citations": "178\n", "authors": ["460"]}
{"title": "On the capability of static code analysis to detect security vulnerabilities\n", "abstract": " Context: Static analysis of source code is a scalable method for discovery of software faults and security vulnerabilities. Techniques for static code analysis have matured in the last decade and many tools have been developed to support automatic detection.Objective: This research work is focused on empirical evaluation of the ability of static code analysis tools to detect security vulnerabilities with an objective to better understand their strengths and shortcomings.Method: We conducted an experiment which consisted of using the benchmarking test suite Juliet to evaluate three widely used commercial tools for static code analysis. Using design of experiments approach to conduct the analysis and evaluation and including statistical testing of the results are unique characteristics of this work. In addition to the controlled experiment, the empirical evaluation included case studies based on three open source\u00a0\u2026", "num_citations": "95\n", "authors": ["460"]}
{"title": "Assessing uncertainty in reliability of component-based software systems\n", "abstract": " Many architecture-based software reliability models were proposed in the past. Regardless of the accuracy of these models, if a considerable uncertainty exists in the estimates of the operational profile and components reliabilities then a significant uncertainty exists in calculated software reliability. Therefore, the traditional way of estimating software reliability by plugging point estimates of unknown parameters into the model may not be appropriate since it discards any variance due to uncertainty of the parameters. In this paper we propose a methodology for uncertainty analysis of architecture-based software reliability models suitable for large complex component based applications and applicable throughout the software life cycle. First, we describe different approaches to build the architecture based software reliability model and to estimate parameters. Then, we perform uncertainty analysis using the method of\u00a0\u2026", "num_citations": "90\n", "authors": ["460"]}
{"title": "Large empirical case study of architecture-based software reliability\n", "abstract": " In this paper we present an empirical study of architecture-based software reliability based on a large open source application which consists of 350,000 lines of C code. The goals of our study are to analyze empirically the adequacy, applicability, and accuracy of architecture-based software reliability models. For this purpose we developed innovative approaches to efficiently extract and more accurately analyze a large amount of empirical data. Applying the theoretical results on a large scale field study allows us to test how and when they work, to understand their limitations, and outline the issues that need attention in the future research studies. Thus, our results show that for a subset of failures which can clearly be attributed to single components, both the composite and hierarchical models are very accurate when compared to the actual reliability. However, the assumptions made by the existing architecture\u00a0\u2026", "num_citations": "79\n", "authors": ["460"]}
{"title": "Software reliability estimation under certainty: generalization of the method of moments\n", "abstract": " Traditionally, reliability models of component-based software systems compute the point estimate of system reliability by plugging point estimates of unknown parameters into the model. These models discard the uncertainty of the parameters, that is, do not attempt to answer the question how parameters uncertainties affect the estimates of system reliability. In this paper we focus on uncertainty analysis in software reliability based on method of moments. In particular, we present a generalization of our earlier work that allows us to consider the uncertainty in the operational profile (i.e., the way software is used) in addition to the uncertainty in components failure behavior (i.e., component reliabilities) considered earlier. The method of moments is an approximate analytical method that allows us to generate system reliability moments based on (1) the knowledge of software architecture reflected in the expression of\u00a0\u2026", "num_citations": "68\n", "authors": ["460"]}
{"title": "Empirical characterization of session\u2013based workload and reliability for web servers\n", "abstract": " The growing availability of Internet access has led to significant increase in the use of World Wide Web. If we are to design dependable Web\u2013based systems that deal effectively with the increasing number of clients and highly variable workload, it is important to be able to describe the Web workload and errors accurately. In this paper we focus on the detailed empirical analysis of the session\u2013based workload and reliability based on the data extracted from actual Web logs of eleven Web servers. First, we introduce and rigourously analyze several intra\u2013session and inter\u2013session metrics that collectively describe Web workload in terms of user sessions. Then, we analyze Web error characteristics and estimate the request\u2013based and session\u2013based reliability of Web servers. Finally, we identify the invariants of the Web workload and reliability that apply through all data sets considered. The results presented\u00a0\u2026", "num_citations": "63\n", "authors": ["460"]}
{"title": "Characterization and classification of malicious Web traffic\n", "abstract": " Web systems commonly face unique set of vulnerabilities and security threats due to their high exposure, access by browsers, and integration with databases. This study is focused on characterization and classification of malicious cyber activities aimed at Web systems. The empirical analysis is based on three datasets, each in duration of four to five months, collected by high-interaction honeypots which ran fully functional three-tier Web systems. We first explore the types and prevalence of malicious scans and attacks to Web systems, and the extent to which these malicious activities differ in different periods of time or on Web servers running different services. In addition to descriptive statistical analysis, we include an inferential statistical analysis of the malicious session attributes, such as duration, number of requests and bytes transferred in a session. Then, we use supervised machine learning methods to\u00a0\u2026", "num_citations": "48\n", "authors": ["460"]}
{"title": "Identification of security related bug reports via text mining using supervised and unsupervised classification\n", "abstract": " While many prior works used text mining for automating different tasks related to software bug reports, few works considered the security aspects. This paper is focused on automated classification of software bug reports to security and not-security related, using both supervised and unsupervised approaches. For both approaches, three types of feature vectors are used. For supervised learning, we experiment with multiple classifiers and training sets with different sizes. Furthermore, we propose a novel unsupervised approach based on anomaly detection. The evaluation is based on three NASA datasets. The results showed that supervised classification is affected more by the learning algorithms than by feature vectors and training only on 25% of the data provides as good results as training on 90% of the data. The supervised learning slightly outperforms the unsupervised learning, at the expense of labeling the\u00a0\u2026", "num_citations": "41\n", "authors": ["460"]}
{"title": "Empirical study of session-based workload and reliability for web servers\n", "abstract": " The growing availability of Internet access has led to significant increase in the use of World Wide Web. If we are to design dependable Web-based systems that deal effectively with the increasing number of clients and highly variable workload, it is important to be able to describe the Web workload and errors accurately. In this paper we focus on the detailed empirical analysis of the session-based workload and reliability based on the data extracted from actual Web logs often Web servers. First, we address the data collection process and describe the methods for extraction of workload and error data from Web log files. Then, we introduce and analyze several intra-session and inter-session metrics that collectively describe Web workload in terms of user sessions. Furthermore, we analyze Web error characteristics and estimate the request-based and session-based reliability of Web servers. Finally, we identify the\u00a0\u2026", "num_citations": "41\n", "authors": ["460"]}
{"title": "Analysis of multstep failure modelswith periodic software rejuvenation\n", "abstract": " CiNii \u8ad6\u6587 - Analysis of multstep failure modelswith periodic software rejuvenation CiNii \u56fd\u7acb \u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005 \u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Analysis of multstep failure modelswith periodic software rejuvenation SUZUKI H. \u88ab\u5f15\u7528\u6587\u732e: 2\u4ef6 \u8457\u8005 SUZUKI H. \u53ce\u9332\u520a\u884c\u7269 Advances in Stochastic Modelling Advances in Stochastic Modelling, 85-108, 2002 Notable Publications, Inc \u88ab\u5f15\u7528 \u6587\u732e: 2\u4ef6\u4e2d 1-2\u4ef6\u3092 \u8868\u793a 1 \u96e2\u6563\u6642\u9593\u52d5\u4f5c\u74b0\u5883\u306b\u304a\u3051\u308b\u5468\u671f\u7684\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u82e5\u5316\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u306e\u63a8\u5b9a \u5ca9\u672c \u4e00\u6a39 , \u571f\u80a5 \u6b63 , \u6d77\u751f \u76f4\u4eba \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a\u6280\u8853\u7814\u7a76\u5831\u544a. ED, \u96fb\u5b50\u30c7\u30d0\u30a4\u30b9 103(276), 29-34, 2003-08-21 \u53c2\u8003\u6587\u732e18\u4ef6 2 \u96e2\u6563\u6642\u9593\u52d5\u4f5c\u74b0\u5883\u306b\u304a\u3051\u308b\u5468\u671f\u7684\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u82e5\u5316\u63a8\u5b9a , , \u2026", "num_citations": "41\n", "authors": ["460"]}
{"title": "A contribution towards solving the web workload puzzle\n", "abstract": " World Wide Web, the biggest distributed system ever built, experiences tremendous growth and change in Web sites, users, and technology. A realistic and accurate characterization of Web workload is the first, fundamental step in areas such as performance analysis and prediction, capacity planning, and admission control. Compared to the previous work, in this paper we present more detailed and rigorous statistical analysis of both request and session level characteristics of Web workload based on empirical data extracted from actual logs of four Web servers. Our analysis is focused on exploring phenomena such as self-similarity, long-range dependence, and heavy-tailed distributions. Identification of these phenomena in real data is a challenging task since the existing methods may perform erratically in practice and produce misleading results. We provide more accurate analysis of long-range dependence of\u00a0\u2026", "num_citations": "40\n", "authors": ["460"]}
{"title": "Measurement-based performance analysis of e-commerce applications with web services components\n", "abstract": " Web services are increasingly used to enable interoperability and flexible integration of e-business systems. In this paper we focus on measurement-based performance analysis of an e-commerce application which uses Web services components to execute business operations. In our experiments we use a session-oriented workload generated by a tool developed accordingly to the TPC-W specification. The empirical results are obtained for two different user profiles, Browsing and Ordering, under different workload intensities. Unlike the previous work which was focused on the overall server response time and throughput, we present Web interaction, software architecture, and hardware resource level analysis of the system performance. In particular, we propose a method for extracting component level response times from the application server logs and study the impact of Web services and other components\u00a0\u2026", "num_citations": "40\n", "authors": ["460"]}
{"title": "Detection of dispersed radio pulses: a machine learning approach to candidate identification and classification\n", "abstract": " Searching for extraterrestrial, transient signals in astronomical data sets is an active area of current research. However, machine learning techniques are lacking in the literature concerning single-pulse detection. This paper presents a new, two-stage approach for identifying and classifying dispersed pulse groups (DPGs) in single-pulse search output. The first stage identified DPGs and extracted features to characterize them using a new peak identification algorithm which tracks sloping tendencies around local maxima in plots of signal-to-noise ratio versus dispersion measure. The second stage used supervised machine learning to classify DPGs. We created four benchmark data sets: one unbalanced and three balanced versions using three different imbalance treatments. We empirically evaluated 48 classifiers by training and testing binary and multiclass versions of six machine learning algorithms on each\u00a0\u2026", "num_citations": "38\n", "authors": ["460"]}
{"title": "On parameter tuning in search based software engineering: A replicated empirical study\n", "abstract": " Multiobjective Evolutionary Algorithms are increasingly used to solve optimization problems in software engineering. The choice of parameters for those algorithms usually follows the \"default\" settings, often accepted as \"rule of thumb\" or common wisdom. The fact is that each algorithms needs to be tuned for the problem at hand. Previous work [Arcuri and Fraser, 2011] has shown that variations in parameter values had large effects on the performance of the algorithms. This project seeks to partially replicate the statistical analysis performed by Arcuri and Fraser. We seek to investigate the effects of parameter tuning on the performance of the two algorithms: Indicator-Based Evolutionary Algorithm (IBEA), and Nondominated Sorting Genetic Algorithm (NSGA-II) when applied to the problem of configuring Software Product Lines (SPLs) in the presence of stakeholder preferences such as cost and reliability. The results\u00a0\u2026", "num_citations": "34\n", "authors": ["460"]}
{"title": "Adequacy, accuracy, scalability, and uncertainty of architecture-based software reliability: Lessons learned from large empirical case studies\n", "abstract": " Our earlier research work on applying architecture-based software reliability models on a large scale case study allowed us to test how and when they work, to understand their limitations, and to outline the issues that need future research. In this paper we first present an additional case study which confirms our earlier findings. Then, we present uncertainty analysis of architecture-based software reliability for both case studies. The results show that Monte Carlo method scales better than the method of moments. The sensitivity analysis based on Monte Carlo method shows that (1) small number of parameters contribute to the most of the variation in system reliability and (2) given an operational profile, components' reliabilities have more significant impact on system reliability than transition probabilities. Finally, we summarize the lessons learned from conducting large scale empirical case studies for the purpose of\u00a0\u2026", "num_citations": "34\n", "authors": ["460"]}
{"title": "Exploring fault types, detection activities, and failure severity in an evolving safety-critical software system\n", "abstract": " Many papers have been published on analysis and prediction of software faults and/or failures, but few established the links from software faults (i.e., the root causes) to (potential or observed) failures and addressed multiple attributes. This paper aims at filling this gap by studying types of faults that caused software failures, activities taking place when faults were detected or failures were reported, and the severity of failures. Furthermore, it explores the associations among these attributes and the trends within releases (i.e., pre-release and post-release) and across releases. The results are based on the data extracted from a safety-critical NASA mission, which follows an evolutionary development process. In particular, we analyzed 21 large-scale software components, which together constitute over 8,000 files and millions of lines of code. The main insights include: (1)\u00a0only a few fault types were\u00a0\u2026", "num_citations": "31\n", "authors": ["460"]}
{"title": "UML based severity analysis methodology\n", "abstract": " This paper addresses the problem of assessing the severity based on UML artifacts and uses the cost of failures of software components and connectors as well as failures of system execution scenarios. It proposes a severity assessment methodology that is performed combining three different hazard analysis techniques: functional failure analysis (FFA), failure mode and effect analysis (FMEA), and fault tree analysis (FTA). This process of estimating severity can be automated in development environments supporting UML by annotating the hazard analysis results and the cost of failure information in the UML diagrams. This methodology is useful for reliability-based risk assessment, performance-based risk assessment and requirement-based risk assessment of software systems.", "num_citations": "31\n", "authors": ["460"]}
{"title": "Architecture-based software reliability: Why only a few parameters matter?\n", "abstract": " Uncertainty analysis through sensitivity studies and quantification of the variance of the reliability estimate has become more common in architecture-based software reliability studies. However, up to this point no attempts have been made to explicate the results of such analysis. Our earlier work based on several medium to large scale empirical studies showed that a very few parameters have a significant impact on the variability of system reliability. This paper explains the reasons behind this phenomenon. Unlike related work that considered the impact of the parameters on software reliability either through their model sensitivity or through uncertainty of their estimates, we consider both. Furthermore, we look at all parameters, i.e., components reliabilities and probabilities of transfer of control between components. Based on theoretical and empirical arguments, we justify why a few parameters contribute most of\u00a0\u2026", "num_citations": "26\n", "authors": ["460"]}
{"title": "Entropy as a measure of uncertainty in software reliability\n", "abstract": " The predictive quality of software reliability models is affected by the ability to estimate the correct operational profile. However, building an operational profile is not an easy task, especially for a new product. Therefore, uncertainty analysis of the operational profile and software reliability are of essential importance. In this paper, we present the uncertainty analysis of architecturebased software reliability models [2] using entropy, a well-known concept from information theory [1]. Source entropy that measures the amount of uncertainty inherent in a Markov source is given by [1]", "num_citations": "26\n", "authors": ["460"]}
{"title": "Software requirement risk assessment using UML\n", "abstract": " Summary form only given. Risk assessment is an integral part of software risk management. There are several methods for risk assessment during various phases of software development and at different levels of abstraction. However, there are very few techniques available for assessing risk at the requirements level and those that are available are highly subjective and are not based on any formal design models. Such techniques are human-intensive and highly error prone. This paper presents a methodology that assesses software risk at the requirements level using Unified Modeling Language (UML) specifications of the software at the early development stages. Each requirement is mapped to a specific operational scenario in UML. We determine the possible failure modes of the scenario and find out the complexity of the scenario in each failure mode. The risk factor of a scenario in a failure mode is obtained\u00a0\u2026", "num_citations": "25\n", "authors": ["460"]}
{"title": "Sensitivity of software usage to changes in the operational profile\n", "abstract": " In this paper we present a methodology for uncertainty analysis of the software operational profile suitable for large complex component-based applications and applicable throughout the software life cycle. Within this methodology, we develop a method for studying the sensitivity of software usage to changes in the operational profile based on perturbation theory. This method is then illustrated on three case studies: software developed for the European Space Agency, an e-commerce application, and real-time control software. Results show that components with small execution rates are the most sensitive to the changes in the operational profile. This observation is very important due to the fact that rarely executed components usually handle critical functionalities such as exception handling or recovery.", "num_citations": "23\n", "authors": ["460"]}
{"title": "Methodology for maintainability-based risk assessment\n", "abstract": " A software product spends more than 65% of its lifecycle in maintenance. Software systems with good maintainability can be easily modified to fix faults or to adapt to changing environment. We define maintainability-based risk as a product of two factors: the probability of performing maintenance tasks and the impact of performing these tasks. In this paper, we present a methodology for assessing maintainability-based risk to account for changes in the system requirements. The proposed methodology depends on the architectural artifacts and their evolution through the life cycle of the system. We illustrate the methodology on a case study using UML models", "num_citations": "22\n", "authors": ["460"]}
{"title": "Architectural level risk assessment tool based on UML specifications\n", "abstract": " Recent evidences indicate that most faults in software systems are found in only a few of a system's components [1]. The early identification of these components allows an organization to focus on defect detection activities on high risk components, for example by optimally allocating testing resources [2], or redesigning components that are likely to cause field failures. This paper presents a prototype tool called Architecture-level Risk Assessment Tool (ARAT) based on the risk assessment methodology presented in [3]. The ARAT provides risk assessment based on measures obtained from Unified Modeling Language (UML) artifacts [4]. This tool can be used in the design phase of the software development process. It estimates dynamic metrics [5] and automatically analyzes the quality of the architecture to produce architectural-level software risk assessment [3].", "num_citations": "22\n", "authors": ["460"]}
{"title": "N version programming with majority voting decision: Dependability modeling and evaluation\n", "abstract": " The paper presents dependability model of the N version programming subject to coincident failures on a particular input during a period of execution. The study is based on two dimensional continuous time Markov model and unlike previous works, we carry the analysis in the time domain. The degree to which faults are manifested as coincident and similar failures is an important parameter of the model. In order to produce dependability model for complete fault tolerant system we consider the interactions between the faults in the versions and the faults in the majority voter. Based on the theoretical analysis the impact of the version and majority voter failures to the reliability and safety are discussed.", "num_citations": "21\n", "authors": ["460"]}
{"title": "Performability modeling of n version programming technique\n", "abstract": " The paper presents a detailed, but efficiently solvable model of the N version programming for evaluating reliability and performability over a mission period. Employing a hierarchical decomposition we reduce the model complexity and provide a modeling framework for evaluating the NVP failure and execution time behavior and the operational environment, as well. The failure and execution rates are treated as random variables and the operational profile is analyzed on the microstructure level, looking at probabilities of occurrence, failure and execution rates for each partition of input space. The reliability submodel that represents per run behavior of NVP, includes both functional failures and timing failures thus resulting in system reliability which accounts for performance requirements. The successive runs are modeled by the performance submodel, that represents the iterative nature of the software execution\u00a0\u2026", "num_citations": "20\n", "authors": ["460"]}
{"title": "Using multiclass machine learning methods to classify malicious behaviors aimed at web systems\n", "abstract": " The number of vulnerabilities and attacks on Web systems show an increasing trend and tend to dominate on the Internet. Furthermore, due to their popularity and users ability to create content, Web 2.0 applications have become particularly attractive targets. These trends clearly illustrate the need for better understanding of malicious cyber activities based on both qualitative and quantitative analysis. This paper is focused on multiclass classification of malicious Web activities using three supervised machine learning methods: J48, PART, and Support Vector Machines (SVM). The empirical analysis is based on data collected in duration of nine months by a high interaction honey pot consisting of a three-tier Web system, which included Web 2.0 applications (i.e., a blog and wiki). Our results show that supervised learning methods can be used to efficiently distinguish among multiple vulnerability scan and attack\u00a0\u2026", "num_citations": "19\n", "authors": ["460"]}
{"title": "Analyzing and predicting effort associated with finding and fixing software faults\n", "abstract": " Context: Software developers spend a significant amount of time fixing faults. However, not many papers have addressed the actual effort needed to fix software faults.Objective: The objective of this paper is twofold: (1) analysis of the effort needed to fix software faults and how it was affected by several factors and (2) prediction of the level of fix implementation effort based on the information provided in software change requests.Method: The work is based on data related to 1200 failures, extracted from the change tracking system of a large NASA mission. The analysis includes descriptive and inferential statistics. Predictions are made using three supervised machine learning algorithms and three sampling techniques aimed at addressing the imbalanced data problem.Results: Our results show that (1) 83% of the total fix implementation effort was associated with only 20% of failures. (2) Both post-release failures\u00a0\u2026", "num_citations": "18\n", "authors": ["460"]}
{"title": "Discovering web workload characteristics through cluster analysis\n", "abstract": " In this paper we present clustering analysis of session-based Web workloads of eight Web servers using the intrasession characteristics (i.e., number of requests per session, session length in time, and bytes transferred per session) as variables. We use K-means algorithm and the Mahalanobis distance, and analyze the heavy-tailed behavior of intra-session characteristics and their correlations for each cluster. Our results show that clustering provides an efficient way to classify tens or hundreds thousands of sessions into several coherent classes that efficiently describe Web workloads. These classes reveal phenomena that cannot be observed when studying the workload as a whole.", "num_citations": "18\n", "authors": ["460"]}
{"title": "Applying machine learning to predict software fault proneness using change metrics, static code metrics, and a combination of them\n", "abstract": " Predicting software fault proneness is very important as the process of fixing these faults after the release is very costly and time-consuming. In order to predict software fault proneness, many machine learning algorithms (e.g., Logistic regression, Naive Bayes, and J48) were used on several datasets, using different metrics as features. The question is what algorithm is the best under which circumstance and what metrics should be applied. Related works suggested that using change metrics leads to the highest accuracy in prediction. In addition, some algorithms perform better than others in certain circumstances. In this work, we use three machine learning algorithms (i.e., logistic regression, Naive Bayes, and J48) on three Eclipse releases (i.e., 2.0, 2.1, 3.0). The results showed that accuracy is slightly better and false positive rates are lower, when we use the reduced set of metrics compared to all change metrics\u00a0\u2026", "num_citations": "17\n", "authors": ["460"]}
{"title": "A novel single-pulse search approach to detection of dispersed radio pulses using clustering and supervised machine learning\n", "abstract": " We present a novel two-stage approach that combines unsupervised and supervised machine learning to automatically identify and classify single pulses in radio pulsar search data. In the first stage, we identify astrophysical pulse candidates in the data, which were derived from the Pulsar Arecibo L-Band Feed Array (PALFA) survey and contain 47\u00a0042 independent beams, as trial single-pulse event groups (SPEGs) by clustering single-pulse events and merging clusters that fall within the expected DM and time span of astrophysical pulses. We also present a new peak scoring algorithm, to identify astrophysical peaks in signal-to-noise versus DM curves. Furthermore, we group SPEGs detected at a consistent DM for they were likely emitted by the same source. In the second stage, we create a fully labelled benchmark data set by selecting a subset of data with SPEGs identified (using stage 1 procedures), their\u00a0\u2026", "num_citations": "16\n", "authors": ["460"]}
{"title": "Exploring the missing link: An empirical study of software fixes\n", "abstract": " Many papers have been published on analysis and prediction of software faults and/or failures, but few addressed the software fixes made to correct the faults and prevent failures from reoccurring. This paper contributes towards filling this gap by focusing on empirical characterization of software fixes. The results are based on the data extracted from a safety\u2013critical NASA mission. In particular, 21 large\u2010scale software components (which together constitute over 8000 files and millions of lines of code) were analysed. The unique characteristic of this work is the fact that links were established from software faults (i.e. the root causes) to (potential or observed) failures and consequently to fixes made to correct these faults. Specifically, for the fixes associated with individual failures, the spread across software components and types of software artifacts being fixed was studied. Our results showed that significant number\u00a0\u2026", "num_citations": "16\n", "authors": ["460"]}
{"title": "Towards malware detection via cpu power consumption: Data collection design and analytics\n", "abstract": " This paper presents an experimental design and algorithm for power-based malware detection on general-purpose computers. Our design allows programmatic collection of CPU power profiles for a fixed set of non-malicious benchmarks, first running in an uninfected state and then in an infected state with malware running along with non-malicious software. To characterize power consumption profiles, we use both simple statistical and novel, sophisticated features. We propose an unsupervised, one-class anomaly detection ensemble and compare its perfor-mance with several supervised, kernel-based SVM classifiers (trained on clean and infected profiles) in detecting previously unseen malware. The anomaly detection system exhibits perfect detection when using all features across all benchmarks, with smaller false detection rate than the supervised classifiers. This paper provides a proof of concept that power\u00a0\u2026", "num_citations": "15\n", "authors": ["460"]}
{"title": "Classification of malicious Web sessions\n", "abstract": " The ever increasing number of vulnerabilities and reported attacks on Web systems clearly illustrate the need for better understanding of malicious cyber activities, which will allow better protection, detection, and service recovery in the cyberspace. In this paper we use three supervised machine learning methods, Support Vector Machines (SVM), and decision trees based J48 and PART, to classify attacker activities aimed at Web systems. The empirical analysis is based on four datasets, each in duration of four to five months, collected by high-interaction honeypots. Malicious Web sessions are characterized with forty three different features (i.e., session attributes) extracted from Web server logs. Our results show that the supervised learning methods can be used to efficiently distinguish attack sessions from vulnerability scan sessions, with very high probability of detection and very low probability of false alarms\u00a0\u2026", "num_citations": "14\n", "authors": ["460"]}
{"title": "Modeling web request and session level arrivals\n", "abstract": " This paper is focused on modeling Web request and session level arrival processes. We propose a statistically rigorous approach which includes testing for non-stationarity and Gaussianity, and uses model selection criterion. Furthermore, a goodness of fit test is applied to each candidate model - ARMA, ARIMA, FARIMA, and FGN - and for validation purpose real data is compared with data simulated from the models. The results based on data extracted from six Web servers with different workload intensities show that (1) there is no one-fits-all solution and (2) servers with high workloads have both request and session traffic modeled well with FARIMA model which is capable of capturing both long-range and short-range dependence.", "num_citations": "14\n", "authors": ["460"]}
{"title": "Maintainability based risk assessment in adaptive maintenance context\n", "abstract": " Development of software systems utilizes only 20%-40% of the overall project cost; the rest is consumed by maintenance. Systems with poor maintainability are difficult to modify and to maintain. Maintainability-based risk is defined as a product of two factors: the probability of carrying out maintenance tasks and the impact of these tasks. In this paper, we present a methodology for assessing maintainability-based risk in the context of adaptive maintenance. We demonstrate the methodology on a case study using UML models.", "num_citations": "14\n", "authors": ["460"]}
{"title": "Quantification of attackers activities on servers running Web 2.0 applications\n", "abstract": " The widespread use of Web applications, in conjunction with large number of vulnerabilities, makes them very attractive targets for malicious attackers. The increasing popularity of Web 2.0 applications, such as blogs, wikis, and social sites, makes Web servers even more attractive targets. In this paper we present empirical analysis of attackers activities based on data collected by two high-interaction honeypots which have typical three-tier architectures and include Web 2.0 applications. The contributions of our work include in-depth characterization of different types of malicious activities aimed at Web servers that deploy blog and wiki applications, as well as formal inferential statistical analysis of the malicious Web sessions.", "num_citations": "13\n", "authors": ["460"]}
{"title": "Architectural level maintainability based risk assessment\n", "abstract": " Architectural level maintainability based risk assessment - Espace \u00c9TS ENGLISH Logo La vitrine de diffusion des publications et contributions des chercheurs de l'\u00c9TS RECHERCHER Se connecter Accueil D\u00e9poser Nouveaut\u00e9s Recherche Rep\u00e9rage \u00c0 propos Conditions d'utilisation Politique de d\u00e9p\u00f4t M\u00e9moires et th\u00e8ses Nous joindre Architectural level maintainability based risk assessment Abdel Moez, W., Shaik, I., Gunnalan, R., Shereshevsky, M., Goseva-Popstojanova, K., Ammar, HH, Mili, A. et Fuhrman, C.. 2005. \u00ab Architectural level maintainability based risk assessment \u00bb. Affiche pr\u00e9sent\u00e9e lors de la conf\u00e9rence : 21st IEEE International Conference on Software Maintenance (ICSM 2005) (Budapest , Hungary, Sept. 26, 2005). Le plein texte de ce document n'est pas h\u00e9berg\u00e9 sur ce serveur. Type de document: Affiche Professeur: Professeur Fuhrman, Christopher Affiliation: G\u00e9nie logiciel et des technologies de l':\u2026", "num_citations": "12\n", "authors": ["460"]}
{"title": "Performability and reliability modeling of n version fault tolerant software in real time systems\n", "abstract": " The paper presents a hierarchical modeling approach of the N version programming in a real time environment. The model is constructed in three layers. At the first layer we distinguish the NVP structure from its operational environment. The NVP structure submodel considers both failures of functionality and failures of performance. The operational environment submodel is based on the concept of the operational profile. The second layer consists of a per run reliability and performance submodels. The first considers per run failure probabilities, while the second is responsible for modeling the series of successive runs over a mission. The information contributed by the second layer constitutes third layer models which support the evaluation of a performability and reliability over mission. The work presented, generalizes our previous work as it considers general distributions of the versions' time to failure and\u00a0\u2026", "num_citations": "12\n", "authors": ["460"]}
{"title": "Malware detection using power consumption and network traffic data\n", "abstract": " Even though malware detection is an active area of research, not many works have used features extracted from physical properties, such as power consumption. This paper is focused on malware detection using power consumption and network traffic data collected using our experimental testbed. Seven power-based and eighteen network traffic-based features were extracted and ten supervised machine learning algorithms were used for classification. The main findings include: (1) Among the best performing learners, Random Forest had the highest F-score and close to the highest G-score. (2) Power data extracted from the +12V CPU rails led to better performance than power data from the other three voltage rails. (3) Using only power-based features provided better performance than using only network traffic-based features; using both types of features had the best performance. (4) Feature selection based on\u00a0\u2026", "num_citations": "11\n", "authors": ["460"]}
{"title": "Session reliability of web systems under heavy-tailed workloads: an approach based on design and analysis of experiments\n", "abstract": " While workload characterization and performance of web systems have been studied extensively, reliability has received much less attention. In this paper, we propose a framework for session reliability modeling which integrates the user view represented by the session layer and the system view represented by the service layer. A unique characteristic of the session layer is that, in addition to the user navigation patterns, it incorporates the session length in number of requests and allows us to account for heavy-tailed workloads shown to exist in real web systems. The service layer is focused on the request reliability as it is observed at the service provider side. It considers the multifier web server architecture and the way components interact in serving each request. Within this framework, we develop a session reliability model and solve it using simulation. Instead of the traditional one-factor-at-a-time sensitivity\u00a0\u2026", "num_citations": "11\n", "authors": ["460"]}
{"title": "Empirical analysis of attackers activity on multi-tier Web systems\n", "abstract": " Web-based systems commonly face unique set of vulnerabilities and security threats due to their high exposure, access by browsers, and integration with databases. In this paper we present empirical analysis of attackers activities based on data collected by two high-interaction honeypots. The contributions of our work include: (1) Classification of the malicious traffic to port scans, vulnerability scans, and attacks; (2) Conducting experiments which, in addition to attackers activities aimed at individual components, allowed us to observe and study vulnerability scans and attacks that span multiple system components; and (3) Statistical characterization of the malicious traffic.", "num_citations": "11\n", "authors": ["460"]}
{"title": "Characterization of cyberattacks aimed at integrated industrial control and enterprise systems: a case study\n", "abstract": " Industrial control system (ICS) security has been a topic of research for several years now and the growing interconnectedness with enterprise systems (ES) is exacerbating the existing issues. Research efforts, however, are impeded by the lack of data that integrate both types of systems. This paper presents an empirical analysis of malicious activities aimed at integrated ICS and ES environment using the dataset created and released by the SANS Institute. The contributions of our work include classification of the observed malicious activities according to several criteria, such as the number of steps (i.e., single-step vs. multi-step), targeted technology (i.e., ICS, ES or both), types of cyber-probes and cyberattacks (e.g., port scan, vulnerability scan, information disclosure, code injection, and SQL injection), and protocols used. In addition, we quantified the severity of the attacks' impact on systems. The main empirical\u00a0\u2026", "num_citations": "10\n", "authors": ["460"]}
{"title": "Methodology for architecture level hazard analysis, a survey\n", "abstract": " Summary form only given. A variety of hazard analysis techniques have been proposed for software-based systems. But individually, the techniques are limited in their ability to deal with system complexity, or to derive and prioritize component safety requirements. As the complexity of modern software systems increases, using one technique at different stages of design is becoming increasingly more challenging. The use of object oriented methodology (in analysis, design and coding) in the real-time embedded systems, the birth of new areas for certification such as the space industry and the increasing complexity of software based applications which need to be certified, motivate the need for a new technique. We survey the current hazard analysis techniques, and conclude that a new technique is needed. Also we explore the suitability of Unified Modeling Language (UML) as a foundation of a new architecture\u00a0\u2026", "num_citations": "9\n", "authors": ["460"]}
{"title": "The effect on network flows-based features and training set size on malware detection\n", "abstract": " Although network flows have been used in areas such as network traffic analysis and botnet detection, not many works have used network flows-based features for malware detection. This paper is focused on malware detection based on using features extracted from the network traffic and system logs. We evaluated the performance of four supervised machine learning algorithms (i.e., J48, Random Forest, Naive Bayes, and PART) for malware detection and identified the best learner. Furthermore, we used feature selection based on information gain to identify the smallest number of features needed for classification. In addition, we experimented with training sets of different sizes. The main findings include: (1) Adding network flows-based features improved significantly the performance of malware detection. (2) J48 and PART were the best performing learners, with the highest F-score and G-score values. (3\u00a0\u2026", "num_citations": "8\n", "authors": ["460"]}
{"title": "Uncertainty analysis of software reliability based on method of moments\n", "abstract": " Many architecture\u2013based software reliability models have been proposed in the past [2]. Regardless of the accuracy of the mathematical model used to model software reliability, if considerable uncertainty in components failure data exists (as it usually does) then a significant uncertainty in calculated system reliability exists. Therefore, the traditional approach of computing the point estimate of the system reliability by plugging point estimates of component reliabilities into the model is not appropriate. In order to answer the question how parameters uncertainties propagate into overall system reliability, uncertainty analysis is necessary. Several methods for uncertainty analysis of system characteristics from uncertainties in component characteristics are available [4],[5],[6]. In this short paper we use the method of moments to quantify the propagation of uncertainties (ie propagation of errors) in software reliability. Method of moments is an approximate approach that allows us to generate the moments of system reliability from the moments of component reliabilities.", "num_citations": "8\n", "authors": ["460"]}
{"title": "Malware detection on general-purpose computers using power consumption monitoring: A proof of concept and case study\n", "abstract": " Malware detection is challenging when faced with automatically generated and polymorphic malware, as well as with rootkits, which are exceptionally hard to detect. In an attempt to contribute towards addressing these challenges, we conducted a proof of concept study that explored the use of power consumption for detection of malware presence in a general-purpose computer. The results of our experiments indicate that malware indeed leaves a signal on the power consumption of a general-purpose computer. Specifically, for the case study based on two different rootkits, the data collected at the +12V rails on the motherboard showed the most noticeable increment of the power consumption after the computer was infected. Our future work includes experimenting with more malware examples and workloads, and developing data analytics approach for automatic malware detection based on power consumption.", "num_citations": "7\n", "authors": ["460"]}
{"title": "Accounting for characteristics of session workloads: A study based on partly-open queue\n", "abstract": " Many systems, including Web and Software as a Service (SaaS) are best characterized with session-based workloads. Empirical studies have shown that Web session arrivals exhibit long range dependence and that the number of request in a session is well modeled with skewed or heavy-tailed distributions. However, models that account for session workloads characterized by empirically observed phenomena and studies of their impact on performance metrics are lacking. In this paper, we use partly-open queue to account for session-based workloads in a physically meaningful way and use simulation to analyze the behavior of the Web system under Long Range Dependent (LRD) session arrival process and skewed distribution for the number of requests in a session. Our results show that the percentage of dropped sessions, mean queue length, mean waiting time, and the useful server utilization are all\u00a0\u2026", "num_citations": "7\n", "authors": ["460"]}
{"title": "Experience report: security vulnerability profiles of mission critical software: empirical analysis of security related bug reports\n", "abstract": " While some prior research work exists on characteristics of software faults (i.e., bugs) and failures, very little work has been published on analysis of software applications vulnerabilities. This paper aims to contribute towards filling that gap by presenting an empirical investigation of application vulnerabilities. The results are based on data extracted from issue tracking systems of two NASA missions. These data were organized in three datasets: Ground mission IV&V issues, Flight mission IV&V issues, and Flight mission Developers issues. In each dataset, we identified the security related software bugs and classified them in specific vulnerability classes. Then, we created the vulnerability profiles, i.e., determined where and when the security vulnerabilities were introduced and what were the dominant vulnerabilities classes. Our main findings include: (1) In IV&V issues datasets the majority of vulnerabilities were\u00a0\u2026", "num_citations": "6\n", "authors": ["460"]}
{"title": "A new Markov model of N version programming systems\n", "abstract": " Reliability performance modeling of N version programming is given. The study is based on continuous time Markov model for the general case of N versions. Derived mathematical relations between reliability performances (as a function of version execution time) and modeling parameters enable us to gain a great deal of quantitative results. The obtained results can be used to guide a design of actual systems.< >", "num_citations": "6\n", "authors": ["460"]}
{"title": "Towards malware detection via cpu power consumption: Data collection design and analytics (extended version)\n", "abstract": " This paper presents an experimental design and data analytics approach aimed at power-based malware detection on general-purpose computers. Leveraging the fact that malware executions must consume power, we explore the postulate that malware can be accurately detected via power data analytics. Our experimental design and implementation allow for programmatic collection of CPU power profiles for fixed tasks during uninfected and infected states using five different rootkits. To characterize the power consumption profiles, we use both simple statistical and novel, sophisticated features. We test a one-class anomaly detection ensemble (that baselines non-infected power profiles) and several kernel-based SVM classifiers (that train on both uninfected and infected profiles) in detecting previously unseen malware and clean profiles. The anomaly detection system exhibits perfect detection when using all features and tasks, with smaller false detection rate than the supervised classifiers. The primary contribution is the proof of concept that baselining power of fixed tasks can provide accurate detection of rootkits. Moreover, our treatment presents engineering hurdles needed for experimentation and allows analysis of each statistical feature individually. This work appears to be the first step towards a viable power-based detection capability for general-purpose computers, and presents next steps toward this goal.", "num_citations": "5\n", "authors": ["460"]}
{"title": "Data-Based Analysis of Sampling and Estimation Methods for Vehicle Tracking over Wireless Networks\n", "abstract": " Wireless networks provide the possibility of vehicle tracking using information broadcast by vehicles in a local area. In vehicular safety networks, vehicles include their movement information in broadcast messages, allowing receivers of the messages to track them in real time and detect hazardous situations. There are many choices for sampling, communication and estimation of the movement data. In this paper, we look at four of the main choices that are currently used by industry and in research works. We also introduce a new heuristic estimation method based on our observation of data. To compare these methods, we use several datasets that are publicly available. The methods studied in this paper use either periodic beaconing or error-dependent sampling and communication method, and combine it with either constant-speed or constant-acceleration estimation methods. The results show that while a\u00a0\u2026", "num_citations": "5\n", "authors": ["460"]}
{"title": "Software Architecture Risk Assessment (SARA) Tool\n", "abstract": " The SARA tool provides estimates for maintainability-based risk, reliability-based risk, and requirements-based risk. The tool also provides several of architectural level metrics related to size, coupling, and complexity. The tool extends our earlier Architectural-level Risk Assessment Tool (ARAT) by providing support for more architectural models such as Product line architectures It also provides different perspective of risk assessment other than reliability-based risk such as risk based on maintainability or requirements. The tool can be extended to support performance-based risk, and other forms of risk assessment at the architecture level.", "num_citations": "5\n", "authors": ["460"]}
{"title": "Using maintainability based risk assessment and severity analysis in prioritizing corrective maintenance tasks\n", "abstract": " A software product spends more than 65% of its lifecycle in maintenance. Software systems with good maintainability can be easily modified to fix faults. We define maintainability-based risk as a product of two factors: the probability of performing maintenance tasks and the impact of performing these tasks. In this paper, we present a methodology for assessing maintainability-based risk in the context of corrective maintenance. The proposed methodology depends on the architectural artifacts and their evolution through the life cycle of the system. In order to prioritize corrective maintenance tasks, we combine components' maintainability- based risk with the severity of a failure that may happen as a result of unfixed fault. We illustrate the methodology on a case study using UML models.", "num_citations": "5\n", "authors": ["460"]}
{"title": "Dependability modeling and evaluation of recovery block systems\n", "abstract": " The paper presents performance modeling and evaluation of recovery block systems. In order to produce a dependability model for a complete fault tolerant system we consider the interaction between the faults in the alternatives and the faults in the acceptance test. The study is based on finite state continuous time Markov model, and unlike previous works, we carry out the analysis in the time domain. The undetected and total failure probabilities (safety and reliability), as well as the average recovery block execution time expressions are obtained. Derived mathematical relations between failure probabilities (i.e. reliability and safety) and modeling parameters enable us to gain a great deal of quantitative results.", "num_citations": "5\n", "authors": ["460"]}
{"title": "Verification and validation approaches for model-based software engineering\n", "abstract": " Model-based Software Engineering (MBSwE) and the use of automatic code generation has become popular for safety-critical aerospace applications. For these applications, verification and validation (V&V) is of utmost importance. With models as another layer of artifacts, however, V&V can become more complex in general, as V&V tasks can be carried out at the model level or at the code level. In this short paper, we present a V&V architecture specifically designed for MBSwE, which reflects the interrelationships between the different levels, tasks and tools, and which aims to provide a clear picture on the V&V approaches for MBSwE. We illustrate the architecture with a detailed analysis of two NASA missions and discuss their approaches to model use and understanding, automatic code generation, V&V, and model synchronization.", "num_citations": "4\n", "authors": ["460"]}
{"title": "Guest Editors' Introduction to the Special Section on Evaluation and Improvement of Software Dependability\n", "abstract": " The reliance on the operation of software systems has increased tremendously over the years, spanning almost every aspect of our lives-from shopping and entertainment, to controlling medical devices that sustains human life, controlling airplanes or managing the critical infrastructure. The advances in software engineering, combined with the fast increase of hardware performance and the widespread use of broadband network connectivity led to development of enormous number of useful applications.", "num_citations": "4\n", "authors": ["460"]}
{"title": "Classification of partially labeled malicious Web traffic in the presence of concept drift\n", "abstract": " Attacks to Web systems have shown an increasing trend in the recent past. A contributing factor to this trend is the deployment of Web 2.0 technologies. While work related to characterization and classification of malicious Web traffic using supervised learning exists, little work has been done using semi-supervised learning with partially labeled data. In this paper an incremental semi-supervised algorithm (CSL-Stream) is used to classify malicious Web traffic to multiple classes, as well as to analyze the concept drift and concept evolution phenomena. The work is based on data collected in duration of nine months by a high-interaction honeypot running Web 2.0 applications. The results showed that on completely labeled data semi-supervised learning performed only slightly worse than the supervised learning algorithm. More importantly, multiclass classification of the partially labeled malicious Web traffic (i.e., 50\u00a0\u2026", "num_citations": "3\n", "authors": ["460"]}
{"title": "Estimating the probability of failure when software runs are dependent: An empirical study\n", "abstract": " The assumption of independence among successive software runs, common to many software reliability models, often is a simplification of the actual behavior. This paper addresses the problem of estimating software reliability when the successive software runs are statistically correlated, that is, when an outcome of a run depends on one or more of its previous runs. First, we propose a generalization of our previous work using higher order Markov chain to model a sequence of dependent software runs. Then, we conduct an empirical study for exploring the phenomenon of dependent software runs using three software applications as case studies. Based on two statistical approaches, we show that the outcomes of software runs (i.e., success or failure) for two of the case studies are dependent on the outcome of one or more previous runs, in which case first or higher order Markov chain models are appropriate\u00a0\u2026", "num_citations": "3\n", "authors": ["460"]}
{"title": "Reliability and performability modeling of N-version fault-tolerant software in real-time environment using Markov regenerative stochastic petri nets\n", "abstract": " In this paper we use a hierarchical composition technique in order to assess performability and reliability over a mission of N version fault tolerant software using the class of Markov Regenerative Stochastic Petri Nets (MRSPN). The construction and generation of a large model is avoided and the solution is obtained by interactions and exchange of information between the submodels. The per run reliability submodel and the performance submodel constitute the lower layer. The upper layer supports the assessment of the performability and reliability over a mission. Inputs are the unconditional probability of success derived within the per run reliability submodel and the unconditional probability distribution of execution time derived within the performance submodel in the lower layer. Analyzing the stochastic process underlying the Petri Net we examine the mean time to failure (MTTF) and the distribution of the time to failure as a function of global time (both are reliability measures), as well as the number of successful runs until failure and the number of successful runs per time unit as performability measures. A numerical example is given, too.", "num_citations": "3\n", "authors": ["460"]}
{"title": "Hierarchical decomposition for estimating reliability of fault-tolerant software in mission-critical systems\n", "abstract": " This paper presents a hierarchical modeling approach aimed at reliability assessment over a mission period of the software fault tolerance technique based on N version programming. The model is constructed in three layers wherein submodels represent different parts of an object system and time scale distinctions. Our modeling approach is systematic as opposed to the ad hoc methods used in related works. Moreover, it permits modifications to be flexibly made at a specific level or in a specific submodel. Thus, the work presented here generalizes our previous work as it allows to consider general distributions of the versions time to failure and execution time at the first level. Also, at the third level, instead of the performability model in our previous work, we develop a new model aimed at reliability assessment over a mission period which supports the evaluation of a reliability over mission period in terms of the functional and timeliness requirements.", "num_citations": "3\n", "authors": ["460"]}
{"title": "N-version programming: a unified modeling approach\n", "abstract": " This paper presents an unified approach aimed at modeling the joint behavior of the N version system and its operational environment. Our objective is to develop reliability model that considers both functional and performance requirements which is particularly important for real-time applications. The model is constructed in two steps. First, the Markov model of N version failure and execution behavior is developed. Next, we develop the user-oriented model of the operational environment. In accounting for dependence we use the idea that the influence of the operational environment on versions failures and execution times induces correlation. The model addresses a number of basic issues and yet yields closed-form solutions that provide considerable insight into how reliability is affected by both versions characteristics and the operational environment.", "num_citations": "3\n", "authors": ["460"]}
{"title": "Empirical evaluation of factors affecting distinction between failing and passing executions\n", "abstract": " Information captured in software execution profiles can benefit verification activities by supporting more cost-effective fault localization and execution classification. This paper proposes an experimental design which utilizes execution information to quantify the effect of factors such as different programs and fault inclusions on the distinction between passed and failed execution profiles. For this controlled experiment we use well-known, benchmark-like programs. In addition to experimentation, our empirical evaluation includes case studies of open source programs having more complex fault models. The results show that metrics reflecting distinction between failing and passing executions are affected more by program than by faults included.", "num_citations": "2\n", "authors": ["460"]}
{"title": "Detection of Radio Pulsars in Single-pulse Searches Within and Across Surveys\n", "abstract": " Pulsar detection using machine learning is a challenging problem as it involves extreme class imbalance and strong prioritization of high Recall. This paper is focused on automatic detection of astrophysical pulses in single-pulse searches, both within and across surveys. We use the output from the first stage of our previously developed two-stage Single-Pulse Event Group IDentification approach and focus on the second stage (ie, classification of pulse candidates). Specifically, for the first time in time-domain single-pulse searches we (1) use boosting and deep learning algorithms for within-survey classification and (2) investigate cross-survey classification by using two transfer learning methods, trAdaBoost (instance-based) and fine-tuning (parameter-based). Our experimental results are based on two benchmark data sets, Green Bank Telescope Drift-scan (GBTDrift) and Pulsar Arecibo L-band Feed Array\u00a0\u2026", "num_citations": "1\n", "authors": ["460"]}
{"title": "Using four modalities for malware detection based on feature level and decision level fusion\n", "abstract": " This paper is focused on multimodal approaches to malware detection, which have not been explored widely in related works. We use static code-based features and dynamic power-based, network traffic-based, and system log-based features, and propose multimodal approaches that use feature level and decision level fusion. Our findings include: (1) For all considered learners, power-based features alone were very good predictors; some learners performed well using only network traffic-based features. (2) For most standard supervised learning algorithms, feature level fusion improved all performance metrics. If Recall is the highest priority, Random Forest or J48 with feature level fusion should be selected. (3) The proposed deep neural network with decision level fusion had lower Recall, but higher Precision and (1-FPR) values, which led to comparable F-score and better G-score than the Random\u00a0\u2026", "num_citations": "1\n", "authors": ["460"]}
{"title": "Scalable solutions for automated single pulse identification and classification in radio astronomy\n", "abstract": " Data collection for scientific applications is increasing exponentially and is forecasted to soon reach peta-and exabyte scales. Applications which process and analyze scientific data must be scalable and focus on execution performance to keep pace. In the field of radio astronomy, in addition to increasingly large datasets, tasks such as the identification of transient radio signals from extrasolar sources are computationally expensive. We present a scalable approach to radio pulsar detection written in Scala that parallelizes candidate identification to take advantage of in-memory task processing using Apache Spark on a YARN distributed system. Furthermore, we introduce a novel automated multiclass supervised machine learning technique that we combine with feature selection to reduce the time required for candidate classification. Experimental testing on a Beowulf cluster with 15 data nodes shows that the\u00a0\u2026", "num_citations": "1\n", "authors": ["460"]}
{"title": "Quality of Failure Data\u2013The Good, the Bad, and the Ugly\n", "abstract": " Experimenting with large, realistic applications allows existing theoretical results to be tested and new domains to be explored, which over time will enable evolution of the knowledge and increase the prediction ability in software reliability engineering. One important fact should not be overlooked in this process-the quality of the reliability predictions depends not only on the methods used, but also on the quality of the failure data. One reason for low data quality is due to the fact that in most cases problem and change tracking repositories used today were not designed with failure analysis in mind. Thus, currently collected data should be augmented with empirical observations of other explanatory factors such as for example testing effort, code coverage, and operational usage which will allow more complete prediction models. Another major reason of low data quality is the lack of consistency and discipline in the process of recording the data. For example, studies of both open source and commercial applications have faced difficulties in distinguishing changes made to fix faults from other changes such as enhancements and adding new requirements. We believe that (1) improving the process of collecting and recording the failure data,(2) making real failure data from variety of sources publicly available, and (3) coupling the academic research with practical problems of interest to industry will contribute towards closing the gap in failure analysis.", "num_citations": "1\n", "authors": ["460"]}