{"title": "Probabilistic programming\n", "abstract": " Probabilistic programs are usual functional or imperative programs with two added constructs:(1) the ability to draw values at random from distributions, and (2) the ability to condition values of variables in a program via observations. Models from diverse application areas such as computer vision, coding theory, cryptographic protocols, biology and reliability analysis can be written as probabilistic programs.", "num_citations": "384\n", "authors": ["1871"]}
{"title": "Synergy: a new algorithm for property checking\n", "abstract": " We consider the problem if a given program satisfies a specified safety property. Interesting programs have infinite state spaces, with inputs ranging over infinite domains, and for these programs the property checking problem is undecidable. Two broad approaches to property checking are testing and verification. Testing tries to find inputs and executions which demonstrate violations of the property. Verification tries to construct a formal proof which shows that all executions of the program satisfy the property. Testing works best when errors are easy to find, but it is often difficult to achieve sufficient coverage for correct programs. On the other hand, verification methods are most successful when proofs are easy to find, but they are often inefficient at discovering errors. We propose a new algorithm, Synergy, which combines testing and verification. Synergy unifies several ideas from the literature, including\u00a0\u2026", "num_citations": "294\n", "authors": ["1871"]}
{"title": "Proofs from tests\n", "abstract": " We present an algorithm DASH to check if a program P satisfies a safety property \u03c6. The unique feature of this algorithm is that it uses only test generation operations, and it refines and maintains a sound program abstraction as a consequence of failed test generation operations. Thus, each iteration of the algorithm is inexpensive, and can be implemented without any global may-alias information. In particular, we introduce a new refinement operator WP \u03b1  that uses only the alias information obtained by symbolically executing a test to refine abstractions in a sound manner. We present a full exposition of the DASH algorithm and its theoretical properties. We have implemented DASH in a tool called YOGI that plugs into Microsoft's Static Driver Verifier framework. We have used this framework to run YOGI on 69 Windows Vista drivers with 85 properties and find that YOGI scales much better than SLAM, the current\u00a0\u2026", "num_citations": "213\n", "authors": ["1871"]}
{"title": "Merlin: Specification inference for explicit information flow problems\n", "abstract": " The last several years have seen a proliferation of static and runtime analysis tools for finding security violations that are caused by explicit information flow in programs. Much of this interest has been caused by the increase in the number of vulnerabilities such as cross-site scripting and SQL injection. In fact, these explicit information flow vulnerabilities commonly found in Web applications now outnumber vulnerabilities such as buffer overruns common in type-unsafe languages such as C and C++. Tools checking for these vulnerabilities require a specification to operate. In most cases the task of providing such a specification is delegated to the user. Moreover, the efficacy of these tools is only as good as the specification. Unfortunately, writing a comprehensive specification presents a major challenge: parts of the specification are easy to miss, leading to missed vulnerabilities; similarly, incorrect specifications may\u00a0\u2026", "num_citations": "176\n", "authors": ["1871"]}
{"title": "Boolean programs: A model and process for software analysis\n", "abstract": " CiNii \u8ad6\u6587 - Boolean programs : A model and process for software analysis CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e \u518d\u958b\u306b\u3064\u3044\u3066 Boolean programs : A model and process for software analysis BALL T. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BALL T. \u53ce\u9332\u520a\u884c\u7269 http://research.microsoft.com/slam http://research.microsoft.com/slam Microsoft Research \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Synchronization Verification in System-Level Design with ILP Solvers SAKUNKONCHAK Thanyapat , KOMATSU Satoshi , FUJITA Masahiro IEICE transactions on fundamentals of electronics, communications and computer sciences 89(12), 3387-3396, 2006-12-01 \u53c2\u8003\u6587\u732e30\u4ef6 \u88ab\u5f15\u7528\u2026", "num_citations": "174\n", "authors": ["1871"]}
{"title": "Types as models: Model checking message-passing programs\n", "abstract": " Abstraction and composition are the fundamental issues in making model checking viable for software. This paper proposes new techniques for automating abstraction and decomposition using source level type information provided by the programmer. Our system includes two novel components to achieve this end:(1) a behavioral type-and-effect system for the \u03c0-calculus, which extracts sound models as types, and (2) an assume-guarantee proof rule for carrying out compositional model checking on the types. Open simulation between CCS processes is used as both the subtyping relation in the type system and the abstraction relation for compositional model checking. We have implemented these ideas in a tool---PIPER. PIPER exploits type signatures provided by the programmer to partition the model checking problem, and emit model checking obligations that are discharged using the SPIN model checker. We\u00a0\u2026", "num_citations": "166\n", "authors": ["1871"]}
{"title": "DebugAdvisor: A recommender system for debugging\n", "abstract": " In large software development projects, when a programmer is assigned a bug to fix, she typically spends a lot of time searching (in an ad-hoc manner) for instances from the past where similar bugs have been debugged, analyzed and resolved. Systematic search tools that allow the programmer to express the context of the current bug, and search through diverse data repositories associated with large projects can greatly improve the productivity of debugging This paper presents the design, implementation and experience from such a search tool called DebugAdvisor.", "num_citations": "150\n", "authors": ["1871"]}
{"title": "Automatically refining abstract interpretations\n", "abstract": " Abstract interpretation techniques prove properties of programs by computing abstract fixpoints. All such analyses suffer from the possibility of false errors. We present three techniques to automatically refine such abstract interpretations to reduce false errors: (1) a new operator called interpolated                 widen, which automatically recovers precision lost due to widen, (2) a new way to handle disjunctions that arise due to refinement, and (3) a new refinement algorithm, which refines abstract interpretations that use the join operator to merge abstract states at join points. We have implemented our techniques in a tool Dagger. Our experimental results show our techniques are effective and that their combination is even more effective than any one of them in isolation. We also show that Dagger is able to prove properties of C programs that are beyond current abstraction-refinement tools, such as Slam [4\u00a0\u2026", "num_citations": "124\n", "authors": ["1871"]}
{"title": "Moat: Verifying confidentiality of enclave programs\n", "abstract": " Security-critical applications constantly face threats from exploits in lower computing layers such as the operating system, virtual machine monitors, or even attacks from malicious administrators. To help protect application secrets from such attacks, there is increasing interest in hardware implementations of primitives for trusted computing, such as Intel's Software Guard Extensions (SGX) instructions. These primitives enable hardware protection of memory regions containing code and data, and provide a root of trust for measurement, remote attestation, and cryptographic sealing. However, vulnerabilities in the application itself, such as the incorrect use of SGX instructions or memory safety errors, can be exploited to divulge secrets. In this paper, we introduce a new approach to formally model these primitives and formally verify properties of so-called enclave programs that use them. More specifically, we create\u00a0\u2026", "num_citations": "112\n", "authors": ["1871"]}
{"title": "The Yogi Project: Software Property Checking via Static Analysis and Testing\n", "abstract": " We present Yogi, a tool that checks properties of C programs by combining static analysis and testing. Yogi implements the Dash algorithm which performs verification by combining directed testing and abstraction. We have engineered Yogi in such a way that it plugs into Microsoft\u2019s Static Driver Verifier framework. We have used this framework to run Yogi on 69 Windows Vista drivers with 85 properties. We find that the new algorithm enables Yogi to scale much better than Slam, which is the current engine driving Microsoft\u2019s Static Driver Verifier.", "num_citations": "102\n", "authors": ["1871"]}
{"title": "Counterexample driven refinement for abstract interpretation\n", "abstract": " Abstract interpretation techniques prove properties of programs by computing abstract fixpoints. All such analyses suffer from the possibility of false errors. We present a new counterexample driven refinement technique to reduce false errors in abstract interpretations. Our technique keeps track of the precision losses during forward fixpoint computation, and does a precise backward propagation from the error to either confirm the error as a true error, or identify a refinement so as to avoid the false error.               Our technique is quite simple, and is independent of the specific abstract domain used. An implementation of our technique for affine transition systems is able to prove invariants generated by the StInG tool [19] without doing any specialized analysis for linear relations. Thus, we hope that the technique can work for other abstract domains as well. We sketch how our technique can be used to perform\u00a0\u2026", "num_citations": "92\n", "authors": ["1871"]}
{"title": "Conformance checking for models of asynchronous message passing software\n", "abstract": " We propose a notion of conformance between a specification S and an implementation model I extracted from a message-passing program. In our framework, S and I are CCS processes, which soundly abstract the externally visible communication behavior of a message-passing program. We use the extracted models to check that programs do not get stuck, waiting to receive or trying to send messages in vain. We show that our definition of stuckness and conformance capture important correctness conditions of message-passing software. Our definition of conformance was motivated by the need for modular reasoning over models, leading to the requirement that conformance preserve substi-tutability with respect to stuck-freeness: If I conforms to S, and P is any environment such that P \u2223 S is stuck-free, then it follows that P \u2223 I is stuck-free. We present a simple algorithm for checking if I conforms to S\u00a0\u2026", "num_citations": "77\n", "authors": ["1871"]}
{"title": "Isolator: dynamically ensuring isolation in comcurrent programs\n", "abstract": " In this paper, we focus on concurrent programs that use locks to achieve isolation of data accessed by critical sections of code. We present ISOLATOR, an algorithm that guarantees isolation for well-behaved threads of a program that obey a locking discipline even in the presence of ill-behaved threads that disobey the locking discipline. ISOLATOR uses code instrumentation, data replication, and virtual memory protection to detect isolation violations and delays ill-behaved threads to ensure isolation. Our instrumentation scheme requires access only to the code of well-behaved threads. We have evaluated ISOLATOR on several benchmark programs and found that ISOLATOR can ensure isolation with reasonable runtime overheads. In addition, we present three general desiderata - safety, isolation, and permissiveness - for any scheme that attempts to ensure isolation, and formally prove that ISOLATOR satisfies\u00a0\u2026", "num_citations": "69\n", "authors": ["1871"]}
{"title": "Analyzing access control configurations\n", "abstract": " A facility is described for analyzing access control configurations. In various embodiments, the facility comprises an operating system having resources and identifications of principals, the principals having access control privileges relating to the resources, the access control privileges described by access control metadata; an access control scanner component that receives the access control metadata, determines relationships between the principals and the resources, and emits access control relations information; and an access control inference engine that receives the emitted access control relations information and an access control policy model, analyzes the received information and model, and emits a vulnerability report. In various embodiments, the facility generates an information flow based on access control relations, an access control mechanism model, and an access control policy model; determines\u00a0\u2026", "num_citations": "68\n", "authors": ["1871"]}
{"title": "A design and verification methodology for secure isolated regions\n", "abstract": " Hardware support for isolated execution (such as Intel SGX) enables development of applications that keep their code and data confidential even while running in a hostile or compromised host. However, automatically verifying that such applications satisfy confidentiality remains challenging. We present a methodology for designing such applications in a way that enables certifying their confidentiality. Our methodology consists of forcing the application to communicate with the external world through a narrow interface, compiling it with runtime checks that aid verification, and linking it with a small runtime that implements the narrow interface. The runtime includes services such as secure communication channels and memory management. We formalize this restriction on the application as Information Release Confinement (IRC), and we show that it allows us to decompose the task of proving confidentiality into (a\u00a0\u2026", "num_citations": "67\n", "authors": ["1871"]}
{"title": "Bayesian inference using data flow analysis\n", "abstract": " We present a new algorithm for Bayesian inference over probabilistic programs, based on data flow analysis techniques from the program analysis community. Unlike existing techniques for Bayesian inference on probabilistic programs, our data flow analysis algorithm is able to perform inference directly on probabilistic programs with loops. Even for loop-free programs, we show that data flow analysis offers better precision and better performance benefits over existing techniques. We also describe heuristics that are crucial for our inference to scale, and present an empirical evaluation of our algorithm over a range of benchmarks.", "num_citations": "64\n", "authors": ["1871"]}
{"title": "System to reduce interference in concurrent programs\n", "abstract": " Locks are used to protect variables. All variables protected by a lock are allocated on a page associated with a lock. When a thread (called the owner) acquires the lock, a local copy of the memory page containing the variable is created, the original memory page is protected, and all access of the variable in the owner thread is directed to the local copy. Upon releasing the lock, the changes from the local copy are carried over to the memory page and the memory page is unprotected. Any concurrent access of the variable by non-owner threads triggers an exception handler (due to the protection mechanism) and delays such an access until after the owner thread has finished accessing the variable.", "num_citations": "64\n", "authors": ["1871"]}
{"title": "A behavioral module system for the pi-calculus\n", "abstract": " Distributed message-passing based asynchronous systems are becoming increasingly important. Such systems are notoriously hard to design and test. A promising approach to help programmers design such programs is to provide a behavioral type system that checks for behavioral properties such as deadlock freedom using a combination of type inference and model checking. The fundamental challenge in making a behavioral type system work for realistic concurrent programs is state explosion. This paper develops the theory to design a behavioral module system that permits decomposing the type checking problem, saving exponential cost in the analysis. Unlike module systems for sequential programming languages, a behavioral specification for a module typically assumes that the module operates in an appropriate concurrent context. We identify assume-guarantee reasoning as a fundamental\u00a0\u2026", "num_citations": "55\n", "authors": ["1871"]}
{"title": "Efficient synthesis of probabilistic programs\n", "abstract": " We show how to automatically synthesize probabilistic programs from real-world datasets. Such a synthesis is feasible due to a combination of two techniques: (1) We borrow the idea of ``sketching'' from synthesis of deterministic programs, and allow the programmer to write a skeleton program with ``holes''. Sketches enable the programmer to communicate domain-specific intuition about the structure of the desired program and prune the search space, and (2) we design an efficient Markov Chain Monte Carlo (MCMC) based synthesis algorithm to instantiate the holes in the sketch with program fragments. Our algorithm efficiently synthesizes a probabilistic program that is most consistent with the data. A core difficulty in synthesizing probabilistic programs is computing the likelihood L(P | D) of a candidate program P generating data D. We propose an approximate method to compute likelihoods using mixtures of\u00a0\u2026", "num_citations": "48\n", "authors": ["1871"]}
{"title": "Efficiently sampling probabilistic programs via program analysis\n", "abstract": " Probabilistic programs are intuitive and succinct representations of complex probability distributions. A natural approach to performing inference over these programs is to execute them and compute statistics over the resulting samples. Indeed, this approach has been taken before in a number of probabilistic programming tools. In this paper, we address two key challenges of this paradigm:(i) ensuring samples are well distributed in the combinatorial space of the program, and (ii) efficiently generating samples with minimal rejection. We present a new sampling algorithm Qi that addresses these challenges using concepts from the field of program analysis. To solve the first challenge (getting diverse samples), we use a technique called symbolic execution to systematically explore all the paths in a program. In the case of programs with loops, we systematically explore all paths up to a given depth, and present theorems on error bounds on the estimates as a function of the path bounds used. To solve the second challenge (efficient samples with minimal rejection), we propagate observations backward through the program using the notion of Dijkstra\u2019s weakest preconditions and hoist these propagated conditions to condition elementary distributions during sampling. We present theorems explaining the mathematical properties of Qi, as well as empirical results from an implementation of the algorithm.", "num_citations": "48\n", "authors": ["1871"]}
{"title": "Software debugging recommendations\n", "abstract": " Software debugging recommendation technique embodiments are presented that generally entails creating a database of characterized software bug descriptions and providing software debugging recommendations from the database in response to a query. This can employ a two-phased approach in the search for similar software bugs. The first is a search phase that takes a query as input and returns a ranked list of software bug descriptions that match the query. These bug descriptions can contain a mix of structured and unstructured data. The second phase is a related-information phase that uses the output of the first phase to retrieve a set of related recommendations such as for people, source files, functions and binaries.", "num_citations": "47\n", "authors": ["1871"]}
{"title": "NETRA: seeing through access control\n", "abstract": " We present netra, a tool for systematically analyzing and detecting explicit information-flow vulnerabilities in access-control configurations. Our tool takes a snapshot of the access-control metadata, and performs static analysis on this snapshot. We devise an augmented relational calculus that naturally models both access control mechanisms and information-flow policies uniformly. This calculus is interpreted as a logic program, with a fixpoint semantics similar to Datalog, and produces all access tuples in a given configuration that violate properties of interest. Our analysis framework is programmable both at the model level and at the property level, effectively separating mechanism from policy. We demonstrate the effectiveness of this modularity by analyzing two systems with very different mechanisms for access control---Windows XP and SELinux---with the same specification of information-flow vulnerabilities\u00a0\u2026", "num_citations": "42\n", "authors": ["1871"]}
{"title": "Debugging machine learning tasks\n", "abstract": " Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets.", "num_citations": "39\n", "authors": ["1871"]}
{"title": "Generalized lattice agreement\n", "abstract": " Lattice agreement is a key decision problem in distributed systems. In this problem, processes start with input values from a lattice, and must learn (non-trivial) values that form a chain. Unlike consensus, which is impossible in the presence of even a single process failure, lattice agreement has been shown to be decidable in the presence of failures. In this paper, we consider lattice agreement problems in asynchronous, message passing systems. We present an algorithm for the lattice agreement problem that guarantees liveness as long as a majority of the processes are non-faulty. The algorithm has a time complexity of O (N) message delays, where N is the number of processes. We then introduce the generalized lattice agreement problem, where each process receives a (potentially unbounded) sequence of values from an infinite lattice and must learn a sequence of increasing values such that the union of all\u00a0\u2026", "num_citations": "37\n", "authors": ["1871"]}
{"title": "A model-learner pattern for Bayesian reasoning\n", "abstract": " A Bayesian model is based on a pair of probability distributions, known as the prior and sampling distributions. A wide range of fundamental machine learning tasks, including regression, classification, clustering, and many others, can all be seen as Bayesian models. We propose a new probabilistic programming abstraction, a typed Bayesian model, which is based on a pair of probabilistic expressions for the prior and sampling distributions. A sampler for a model is an algorithm to compute synthetic data from its sampling distribution, while a learner for a model is an algorithm for probabilistic inference on the model. Models, samplers, and learners form a generic programming pattern for model-based inference. They support the uniform expression of common tasks including model testing, and generic compositions such as mixture models, evidence-based model averaging, and mixtures of experts. A formal\u00a0\u2026", "num_citations": "36\n", "authors": ["1871"]}
{"title": "EON: Modeling and analyzing dynamic access control systems with logic programs\n", "abstract": " We present EON, a logic-programming language and tool that can be used to model and analyze dynamic access control systems. Our language extends Datalog with some carefully designed constructs that allow the introduction and transformation of new relations. For example, these constructs can model the creation of processes and objects, and the modification of their security labels at runtime. The information-flow properties of such systems can be analyzed by asking queries in this language. We show that query evaluation in EON can be reduced to decidable query satisfiability in a fragment of Datalog, and further, under some restrictions, to efficient query evaluation in Datalog.", "num_citations": "34\n", "authors": ["1871"]}
{"title": "An empirical study of optimizations in YOGI\n", "abstract": " Though verification tools are finding industrial use, the utility of engineering optimizations that make them scalable and usable is not widely known. Despite the fact that several optimizations are part of folklore in the communities that develop these tools, no rigorous evaluation of these optimizations has been done before. We describe and evaluate several engineering optimizations implemented in the Yogi property checking tool, including techniques to pick an initial abstraction, heuristics to pick predicates for refinement, optimizations for interprocedural analysis, and optimizations for testing. We believe that our empirical evaluation gives the verification community useful information about which optimizations they could implement in their tools, and what gains they can realistically expect from these optimizations.", "num_citations": "33\n", "authors": ["1871"]}
{"title": "Behavioral analysis for message-passing application programs\n", "abstract": " A system and method for modeling a message-passing program module using type annotations is disclosed. The message-passing program module is constructed with operations that communicate with operations of other message-passing program modules in an asynchronous computing environment. Type annotations are communication protocols that represent processes of input and/or output actions that the program module developer expects each operation to perform or take on a selected set of communication channels. During development of the program module, the type annotations are declared at each operation of the program module. Soundness of the type annotations and whether implementation of the program module conforms to the type annotations is checked using a type system. If the program module is well-typed and well-implemented, the type system abstracts a behavioral module of the\u00a0\u2026", "num_citations": "33\n", "authors": ["1871"]}
{"title": "Domain-specific guidance service for software development\n", "abstract": " During software development, both before and after release, information may be collected and stored that may provide insight to developers as a generalized service. For example, data from past debugging sessions, source code in various repositories, bug repositories, discussion groups, and various documents may provide relevant information for software developers to fix current problems when this information is coherently matched with the problem. Using various sources, a system may mine the stored data to give the current developer information related to past code development, and reveal why the code changed throughout previous development. Using sophisticated analyses to identify similar code patterns across multiple large software projects, discovering patterns in normal and abnormal uses of particular software interfaces, and employing other mining techniques, a developer may find domain\u00a0\u2026", "num_citations": "31\n", "authors": ["1871"]}
{"title": "Error detection in web services systems\n", "abstract": " Methods and systems are provided for automatically generating an accurate model of communications processes between disparate computing systems that may be analyzed in an efficient manner for error detection in web services systems. Business Process Execution Language for Web Services (BPEL) descriptions are automatically generated for the BPEL-based executable processes utilized by each communicating computing system in a given web services system. The BPEL abstract process descriptions for each communicating computing system are translated into a combined process model according to a suitable modeling language. The process model is tested by a model checking software application. Communications errors between the disparate computing systems are detected by automatically testing the combined process model according to a variety of potential communications scenarios.", "num_citations": "31\n", "authors": ["1871"]}
{"title": "A compiler and verifier for page access oblivious computation\n", "abstract": " Trusted hardware primitives such as Intel's SGX instructions provide applications with a protected address space, called an enclave, for trusted code and data. However, building enclaves that preserve confidentiality of sensitive data continues to be a challenge. The developer must not only avoid leaking secrets via the enclave's outputs but also prevent leaks via side channels induced by interactions with the untrusted platform. Recent attacks have demonstrated that simply observing the page faults incurred during an enclave's execution can reveal its secrets if the enclave makes data accesses or control flow decisions based on secret values. To address this problem, a developer needs compilers to automatically produce confidential programs, and verification tools to certify the absence of secret-dependent page access patterns (a property that we formalize as page-access obliviousness). To that end, we\u00a0\u2026", "num_citations": "30\n", "authors": ["1871"]}
{"title": "Distributed analytics platform\n", "abstract": " A platform that facilitates software application development, maintenance, and support includes a storage component that receives structured and unstructured data pertaining to at least one application subject to development, maintenance, or support and causes the structured and unstructured data to be stored in a distributed fashion over a plurality of accessible data repositories. The storage component causes the structured and unstructured data to be stored in the data repositories such that the structured and unstructured data is accessible through utilization of a common access format. An executor component executes an analytical process over the structured and unstructured data and generates a first dataset, wherein the storage component causes the first dataset to be stored in at least one of the plurality of accessible data repositories in a format that is accessible by front end analysis applications.", "num_citations": "30\n", "authors": ["1871"]}
{"title": "Fair bisimulation\n", "abstract": " Bisimulations enjoy numerous applications in the analysis of labeled transition systems. Many of these applications are based on two central observations: first, bisimilar systems satisfy the same branching-time properties; second, bisimilarity can be checked efficiently for finite-state systems. The local character of bisimulation, however, makes it difficult to address liveness concerns. Indeed, the definitions of fair bisimulation that have been proposed in the literature sacrifice locality, and with it, also efficient checkability. We put forward a new definition of fair bisimulation which does not suffer from this drawback.               The bisimilarity of two systems can be viewed in terms of a game played between a protagonist and an adversary. In each step of the infinite bisimulation game, the adversary chooses one system, makes a move, and the protagonist matches it with a move of the other system. Consistent with\u00a0\u2026", "num_citations": "27\n", "authors": ["1871"]}
{"title": "Enforcing object protocols by combining static and runtime analysis\n", "abstract": " In this paper, we consider object protocols that constrain interactions between objects in a program. Several such protocols have been proposed in the literature. For many APIs (such as JDOM, JDBC), API designers constrain how API clients interact with API objects. In practice, API clients violate such constraints, as evidenced by postings in discussion forums for these APIs. Thus, it is important that API designers specify constraints using appropriate object protocols and enforce them. The goal of an object protocol is expressed as a protocol invariant. Fundamental properties such as ownership can be expressed as protocol invariants. We present a language, PROLANG, to specify object protocols along with their protocol invariants, and a tool, INVCOP++, to check if a program satisfies a protocol invariant. INVCOP++ separates the problem of checking if a protocol satisfies its protocol invariant (called protocol\u00a0\u2026", "num_citations": "26\n", "authors": ["1871"]}
{"title": "Programming asynchronous layers with CLARITY\n", "abstract": " Asynchronous systems components are hard to write, hard to reason about, and (not coincidentally) hard to mechanically verify. In order to achieve high performance, asynchronous code is often written in an event-driven style that introduces non-sequential control flow and persistent heap data to track pending operations. As a result, existing sequential verification and static analysis tools are ineffective on event-driven code.", "num_citations": "26\n", "authors": ["1871"]}
{"title": "Minimal logic re-synthesis for engineering change\n", "abstract": " We propose an iterative solution to the problem of logic re-synthesis; we begin with a small region for re-synthesis (selected using some criteria), and iteratively expand that region until a solution is obtained. At each stage, we test if this resynthesizing this region alone can realize the new specification. As a second pass, we trim the region iteratively, so that it becomes minimal in the sense that no subset of the current region can realize the change in functionality. However, not all minimal regions are equivalent in terms of their power, area or delay optimality. To compare two different minimal re-synthesis regions, we use a heuristic evaluation criteria for the acceptability of regions for re-synthesis called sensitivity. We compute the sensitivity (or acceptability for resynthesis) for power. This sensitivity criteria is used to pick nodes in the iterative scheme. An iterative algorithm is given for incremental synthesis that begins\u00a0\u2026", "num_citations": "26\n", "authors": ["1871"]}
{"title": "Programming model to detect deadlocks in concurrent programs\n", "abstract": " Described are embodiments for developing a message-passing application program. The program is constructed using stages having a plurality of asynchronous functions, or operations. The operations communicate with other operations of other message-passing programs in a distributed computing environment. The operations also communicate with other operations on other stages of the message-passing application. In order to reduce deadlock errors, a behavioral type signature is appended to the declaration of each operation of the message-passing application program. The behavioral type signature specifies behavioral properties for each operation, such as when an operation should send a message to another operation. A type checker utilizes typing rules and the behavioral type signature to extract an implementation model of each function. The type checker then compares the implementation model to\u00a0\u2026", "num_citations": "21\n", "authors": ["1871"]}
{"title": "Enabling analysis of software source code\n", "abstract": " A source code clarification system is described. In various embodiments, the source code clarification system receives clarified source code and transforms the clarified source code into standard source code or object code that implements asynchronous components. The standard software source code can contain expressions for enabling asynchronous communications. The clarified code can be software source code that is expressed in an imperative language and is capable of static analysis. The clarified source code can contain a coordination primitive that encapsulates interactions between asynchronous components. By using the coordination primitives and events, the clarified source code can express interactions between asynchronous components so that the clarified source code is easier for developers to understand and for static analysis tools to analyze.", "num_citations": "18\n", "authors": ["1871"]}
{"title": "A type system for data-flow integrity on Windows Vista\n", "abstract": " The Windows Vista operating system implements an interesting model of multi-level integrity. We observe that in this model, trusted code must participate in any information-flow attack. Thus, it is possible to eliminate such attacks by statically restricting trusted code. We formalize this model by designing a type system that can efficiently enforce data-flow integrity on Windows Vista. Typechecking guarantees that objects whose contents are statically trusted never contain untrusted values, regardless of what untrusted code runs in the environment. Some of Windows Vista's runtime access checks are necessary for soundness; others are redundant and can be optimized away.", "num_citations": "17\n", "authors": ["1871"]}
{"title": "A quantitative analysis of processor-programmable logic interface.\n", "abstract": " The addition of programmable logic to RISC machines has the potential of exploiting the inherent parallelism of hardware to speedup an application. The authors study the effect of adding a programmable accelerator to DLX, a RISC prototype. They build this model and parameterize the communication overhead between the processor and programmable unit and logic/routing delays inside the programmable unit. They use simulation to evaluate the performance of this model, parameterized by communication overhead and logic delays, by comparing it with the baseline DLX architecture on some sample problems. The methodology is useful in studying the relative importance of the parameters and in projecting the performance of the system, if the programmable logic were to be implemented inside the processor.", "num_citations": "16\n", "authors": ["1871"]}
{"title": "Combining relational learning with SMT solvers using CEGAR\n", "abstract": " In statistical relational learning, one is concerned with inferring the most likely explanation (or world) that satisfies a given set of weighted constraints. The weight of a constraint signifies our confidence in the constraint, and the most likely world that explains a set of constraints is simply a satisfying assignment that maximizes the weights of satisfied constraints. The relational learning community has developed specialized solvers (eg, Alchemy and Tuffy) for such weighted constraints independently of the work on SMT solvers in the verification community. In this paper, we show how to leverage SMT solvers to significantly improve the performance of relational solvers. Constraints associated with a weight of 1 (or 0) are called axioms because they must be satisfied (or violated) by the final assignment. Axioms can create difficulties for relational solvers. We isolate the burden of axioms to SMT solvers and only lazily pass\u00a0\u2026", "num_citations": "15\n", "authors": ["1871"]}
{"title": "Generation and evaluation of test cases for software validation and proofs\n", "abstract": " A \u201cproperty checker\u201d uses light-weight symbolic execution to prove that software programs satisfy safety properties by simultaneously performing program testing and program abstraction. A simple example of safety properties includes conditions that must be satisfied for proper program execution, such as whether an application properly interfaces with API methods or functions. Program tests are an \u201cunder-approximation\u201d of program behavior, and abstractions are an \u201cover-approximation\u201d of the program. This simultaneous testing either finds a test-case that reaches an error state, or finds an abstraction showing that no path in the state space of the program can reach any error state. If a test-case reaches an error state, the property checker has discovered a violation of the safety property. Conversely, if no path in the state space can reach any error state, the property checker has proved that the program satisfies the\u00a0\u2026", "num_citations": "14\n", "authors": ["1871"]}
{"title": "Synthesis and machine learning for heterogeneous extraction\n", "abstract": " We present a way to combine techniques from the program synthesis and machine learning communities to extract structured information from heterogeneous data. Such problems arise in several situations such as extracting attributes from web pages, machine-generated emails, or from data obtained from multiple sources. Our goal is to extract a set of structured attributes from such data.", "num_citations": "13\n", "authors": ["1871"]}
{"title": "Analyzing access control configurations\n", "abstract": " A facility is described for analyzing access control configurations. In various embodiments, the facility comprises an operating system having resources and identifications of principals, the principals having access control privileges relating to the resources, the access control privileges described by access control metadata; an access control scanner component that receives the access control metadata, determines relationships between principals and resources, and emits access control relations information; and an access control inference engine that receives the emitted access control relations information and an access control policy model, analyzes the received information and model, and emits a vulnerability report. In various embodiments, the facility generates an information flow based on access control relations, an access control mechanism model, and an access control policy model; determines, based\u00a0\u2026", "num_citations": "13\n", "authors": ["1871"]}
{"title": "Probabilistic model approximation for statistical relational learning\n", "abstract": " Various technologies described herein pertain to approximating an inputted probabilistic model for statistical relational learning. An initial approximation of formulae included in an inputted probabilistic model can be formed, where the initial approximation of the formulae omits axioms included in the inputted probabilistic model. Further, an approximated probabilistic model of the inputted probabilistic model can be constructed, where the approximated probabilistic model includes the initial approximation of the formulae. Moreover, the approximated probabilistic model and evidence can be fed to a relational learning engine, and a most probable explanation (MPE) world can be received from the relational learning engine. The evidence can comprise existing valuations of a subset of relations included in the inputted probabilistic model. The MPE world can include valuations for the relations included in the inputted\u00a0\u2026", "num_citations": "13\n", "authors": ["1871"]}
{"title": "Computer Aided Verification (17 conf.)\n", "abstract": " This volume contains the proceedings of the International Conference on Computer Aided Verification (CAV), held in Edinburgh, Scotland, July 6\u201310, 2005. CAV 2005 was the seventeenth in a series of conferences dedicated to the advancement of the theory and practice of computer-assisted formal analysis methods for software and hardware systems. The conference covered the spectrum from theoretical results to concrete applications, with an emphasis on practical verification tools and the algorithms and techniques that are needed for their implementation.We received 123 submissions for regular papers and 32 submissions for tool papers. Of these submissions, the Program Committee selected 32 regular papers and 16 tool papers, which formed the technical program of the conference. The conference had three invited talks, by Bob Bentley (Intel), Bud Mishra (NYU), and George C. Necula (UC Berkeley). The\u00a0\u2026", "num_citations": "13\n", "authors": ["1871"]}
{"title": "Techniques to secure computation data in a computing environment\n", "abstract": " Techniques to secure computation data in a computing environment from untrusted code. These techniques involve an isolated environment within the computing environment and an application programming interface (API) component to execute a key exchange protocol that ensures data integrity and data confidentiality for data communicated out of the isolated environment. The isolated environment includes an isolated memory region to store a code package. The key exchange protocol further involves a verification process for the code package stored in the isolated environment to determine whether the one or more exchanged encryption keys have been compromised. If the signature successfully authenticates the one or more keys, a secure communication channel is established to the isolated environment and access to the code package's functionality is enabled. Other embodiments are described and\u00a0\u2026", "num_citations": "12\n", "authors": ["1871"]}
{"title": "Massively empowered classroom: Enhancing technical education in india\n", "abstract": " Students in the developing world are frequently cited as being among the most important beneficiaries of online education initiatives such as massive open online courses (MOOCs). However, very little research has actually been done on the effects of online education in developing contexts. We describe a case study of our experience building and deploying Massively Empowered Classroom (MEC), an experimental project designed to explore how online educational content and techniques in blended learning can be used for undergraduate education in India. Our pilot study of a single course in algorithms extended over two semesters to more than 120 colleges in three state technical universities in India, and reached more than 4000 students. We identified a number of issues that we believe are unique to the Indian educational context. Specifically, we identify four key domains that MOOCs and similar educational initiatives must manage: Content, Incentives, Awareness, and Bandwidth. We believe that similar issues will extend to other developing countries with significant resource constraints.", "num_citations": "9\n", "authors": ["1871"]}
{"title": "Refining abstract interpretations\n", "abstract": " Abstract interpretation techniques prove properties of programs by computing abstract fixpoints. All such analyses suffer from the possibility of false errors. We present a dag-based abstraction refinement technique to automatically refine such abstract interpretations and reduce false errors. This technique refines precision loss due to widen operator by a new interpolated widen operator and refines precision loss due to join operator by disjunctions. We prove the soundness and progress properties of this abstraction refinement procedure.", "num_citations": "9\n", "authors": ["1871"]}
{"title": "Models for contract conformance\n", "abstract": " We have implemented a contract checker for asynchronous, message-passing applications to check that service implementations conform to behavioural contracts. Our contract checker is based on a process algebraic theory of conformance and is implemented on top of a software model checker, Zing. The purpose of this paper is to explain the model construction implemented by our contract checker and how it is related to a mathematical theory of conformance. In addition, we point out current and future research directions in model construction for conformance checking in the presence of channel-passing.", "num_citations": "9\n", "authors": ["1871"]}
{"title": "Behavioral types for structured asynchronous programming\n", "abstract": " Behavioral Types for Structured Asynchronous Programming - Infoscience English Fran\u00e7ais login Home > Behavioral Types for Structured Asynchronous Programming Infoscience Information Usage statistics Files Behavioral Types for Structured Asynchronous Programming Larus, James R. ; Rajamani, Sriram K. ; Rehof, Jakob Year: 2001 Publisher: Microsoft Research Laboratories: UPLARUS Record appears in: Scientific production and competences > I&C - School of Computer and Communication Sciences > IINFCOM > UPLARUS - Prof. Larus Group Peer-reviewed publications Work outside EPFL Technical Reports Published Export as: BibTeX | MARC | MARCXML | DC | EndNote | NLM | RefWorks | RIS View as: MARC | MARCXML | DC Add to your basket: Back to search Record created 2013-12-23, last modified 2020-04-20 Rate this document: 1 2 3 4 5 (Not yet reviewed) Add to personal basket Export as \u2026", "num_citations": "9\n", "authors": ["1871"]}
{"title": "New directions in refinement checking\n", "abstract": " Formal design verification is a methodology for detecting logical errors in systems. In formal design verification, the designer describes a system in a language with a mathematical semantics, and then the system description is analyzed against various correctness requirements. The paradigm is called model checking when the analysis is performed automatically by exhaustive state-sparce exploration. A correctness requirement is usually specified either as a formula expressed in temporal logic, or as an abstract design expressed in the system's description language. If the requirement is specified as an abstract design, the verification problem is called refinement checking. This thesis extends the state-of-art by increasing both the class, and the size of systems on which automatic and semi-automatic refinement checking are viable.", "num_citations": "9\n", "authors": ["1871"]}
{"title": "Asynchronous resilient linearizability\n", "abstract": " We address the problem of implementing a distributed data-structure that can tolerate process crash failures in an asynchronous message passing system, while guaranteeing correctness (linearizability with respect to a given sequential specification) and resiliency (the operations are guaranteed to terminate, as long as a majority of the processes do not fail). We consider a class of data-structures whose operations can be classified into two kinds: update operations that can modify the data-structure but do not return a value and read operations that return a value, but do not modify the data-structure. We show that if every pair of update operations commute or nullify each other, then resilient linearizable replication is possible. We propose an algorithm for this class of data-structures with a message complexity of two message round trips for read operations and O(n) round trips for update operations. We also\u00a0\u2026", "num_citations": "8\n", "authors": ["1871"]}
{"title": "Design rule system for verifying and enforcing design rules in software\n", "abstract": " A software design rule system is provided. The software design rule system can employ a rule language that enables software developers to model valid interactions between multiple, inter-related objects; provide a rule verifier component that determines whether design rules achieve their intended purpose; and provide a rule enforcer component that determines whether the software complies with the specified rules. Software designers can provide design specifications using the rule language that the software design rule system employs. The rule language can specify a program that identifies \u201cauxiliary states\u201d associated with objects in the software that is being developed, transitions between the auxiliary states, and object invariants.", "num_citations": "8\n", "authors": ["1871"]}
{"title": "Runtime monitoring of object invariants with guarantee\n", "abstract": " High level design decisions are never captured formally in programs and are often violated as programs evolve. In this paper, we focus on design decisions in which an object o works correctly only if another object p is in some specific states. Such decisions can be specified as the object invariant of o.             The invariant of o must hold when control is not inside any of o\u2019s methods (i.e. when o is in a steady state). From discussion forums on widely used APIs, it is clear that there are many instances where o\u2019s invariant is violated by the programmer inadvertently changing the state of p when o is in a steady state. Typically, o and p are objects exposed by the API, and the programmer (who is the user of the API), unaware of the dependency between o and p, calls a method of p in such a way that o\u2019s invariant is violated. The fact that the violation occurred is detected much later, when a method of o is called\u00a0\u2026", "num_citations": "8\n", "authors": ["1871"]}
{"title": "Controlling non-determinism for semantic guarantees\n", "abstract": " Concurrent programs are hard to design, develop, and debug. It is widely accepted that we lack good abstractions to design and reason about concurrent programs, and good tools to debug concurrent programs. Recent technology trends, such as the increasing prevalence of multicore processors, make concurrent programming more important than ever. Non-determinism arises in concurrent programs when the order in which threads can execute is unconstrained. While executions of concurrent programs on multiprocessors inherently exhibit non-determinism, the executions on uniprocessors exhibit non-determinism due to the choices of the thread scheduler in the underlying O/S. Undesired non-determinism is a major cause of errors in concurrent programs. Nevertheless, we believe that non-determinism can be safely, permissively, and automatically controlled to tolerate runtime errors in concurrent programs and to provide various desirable semantic guarantees. In this position paper, we sketch some recent work we have done in this direction and outline our longer term goals along the same direction.", "num_citations": "7\n", "authors": ["1871"]}
{"title": "Automatic property checking for software: past, present and future\n", "abstract": " Software validation is a very hard problem. Traditionally, most validation in our industry has been done by testing. Testing is the process of running software on representative inputs and checking if the software behaves as intended. There are various granularities in which testing is performed ranging from unit tests that test small units of the system, to system-wide tests. Over the past decade, automatic property checking tools that use static analysis have started providing a complementary approach to software validation. These tools are intended to augment, rather than replace, testing. These tools do not typically ensure that the software implements intended functionality correctly. Instead, they look for specific kind of errors more thoroughly inside the program by analyzing how control and data flow through the program. This short paper surveys the state of the art in property checking tools and presents the author's\u00a0\u2026", "num_citations": "7\n", "authors": ["1871"]}
{"title": "Guesstimate: a programming model for collaborative distributed systems\n", "abstract": " We present a new programming model GUEESSTIMATE for developing collaborative distributed systems. The model allows atomic, isolated operations that transform a system from consistent state to consistent state, and provides a shared transactional store for a collection of such operations executed by various machines in a distributed system. In addition to \"committed state\" which is identical in all machines in the distributed system, GUESSTIMATE allows each machine to have a replicated local copy of the state (called \"guesstimated state\") so that operations on shared state can be executed locally without any blocking, while also guaranteeing that eventually all machines agree on the sequences of operations executed. Thus, each operation is executed multiple times, once at the time of issue when it updates the guesstimated state of the issuing machine, once when the operation is committed (atomically) to\u00a0\u2026", "num_citations": "6\n", "authors": ["1871"]}
{"title": "A work allocation language with soft constraints\n", "abstract": " This report defines a workflow allocation language with hard and soft constraints and explains the requirements and constraints that led to its current design. The language can be used with most business process and workflow languages. A high-degree of parameterization allows the language to be used with virtually any process languages.", "num_citations": "6\n", "authors": ["1871"]}
{"title": "Online learning versus blended learning: an exploratory study\n", "abstract": " Due to the recent emergence of massive open online courses (MOOCs), students and teachers are gaining unprecedented access to high-quality educational content. However, many questions remain on how best to utilize that content in a classroom environment. In this small-scale, exploratory study, we compared two ways of using a recorded video lecture. In the online learning condition, students viewed the video on a personal computer, and also viewed a follow-up tutorial (a quiz review) on the computer. In the blended learning condition, students viewed the video as a group in a classroom, and received the follow-up tutorial from a live lecturer. We randomly assigned 102 students to these conditions, and assessed learning outcomes via a series of quizzes. While we saw significant learning gains after each session conducted, we did not observe any significant differences between the online and blended\u00a0\u2026", "num_citations": "5\n", "authors": ["1871"]}
{"title": "Identifying concurrency control from a sequential proof\n", "abstract": " The claimed subject matter provides a system and/or a method that facilitates ensuring non-interference between multiple threads that access a shared resource. An interface can receive a portion of sequential code, wherein the portion of sequential code includes a property that is maintained and relied upon when invoked and executed by a sequential client. A synthesizer component can leverage a sequential proof related to the portion of sequential code in order to derive a concurrency control mechanism for a portion of concurrency code that maintains the property when invoked by a concurrent client, wherein the sequential proof identifies a concurrent interference at an execution point that is tolerable for the concurrent client.", "num_citations": "5\n", "authors": ["1871"]}
{"title": "Verification, testing and statistics\n", "abstract": " Programming tools have expanded both in scope of the problems they solve, and in the kinds of techniques they use. Traditionally, programming tools have focused on detecting errors in programs. Recently, this scope has broadened to help with other programming tasks, including inferring specifications, helping diagnose root cause of errors during debugging, and managing knowledge in large projects. Also, traditionally programming tools have been based on either static or dynamic program analysis. Modern programming tools combine static and dynamic program analysis together with techniques from other disciplines such as statistical and probabilistic inference, and information retrieval. This paper reports on some such tools built by the Rigorous Software Engineering group at Microsoft Research India.", "num_citations": "5\n", "authors": ["1871"]}
{"title": "Counterexample driven refinement for abstract interpretation\n", "abstract": " A refinement system automatically identifies whether a detected error in a target system during abstract interpretation is a false error or a true error and adjusts the interpretation to prevent the false error. The target system is represented as a transition system with an initial state and state transitions and a specification that the target system is to satisfy. The refinement system iteratively performs steps of the abstract interpretation using a widening operator. When the state of a step does not satisfy the specification, the refinement system identifies a step whose widening operator was the source of the state that did not satisfy the specification and applies a more precise operator that eliminates the problem with the widening. The refinement system then starts re-performing the steps starting at that step.", "num_citations": "5\n", "authors": ["1871"]}
{"title": "Softalloc: a work allocation language with soft constraints\n", "abstract": " Today's business process orchestration languages such as WS-BPEL and BPML have high-level constructs for specifying flow of control and data, but facilities for allocating tasks to humans are largely missing. This paper presents SoftAlloc, a work allocation language with soft constraints, and explains the requirements and trade-offs that led to its design, in particular, what soft constraints are, and how they enable business process definitions to capture allocation rules, best practices, and organizational goals without rendering the business processes too strict. SoftAlloc combines with virtually any business process language and any conceivable legacy system, while guaranteeing polynomial performance. We present the design, the formal definition, and an evaluation of SoftAlloc.", "num_citations": "5\n", "authors": ["1871"]}
{"title": "Programming model for collaborative distributed systems\n", "abstract": " Described are methods of providing data sharing between applications. The applications run on different computers, communicate via a network, and share a same distributed object. Each application maintains on its computer an invariant copy of the distributed object and a variant copy of the distributed object. Each application performs update operations to the distributed object, where such an update operation issued by a given one of the applications is performed by: executing the update operation on the variant copy maintained by the given application (i) without the given application waiting for the other applications to perform the operation (each invariant copy is guaranteed to converge to a same state) and (ii) at each of the applications, including the given application, executing the update operation on the corresponding invariant copies.", "num_citations": "4\n", "authors": ["1871"]}
{"title": "Software is more than code.\n", "abstract": " This paper reviews the current practice of software engineering and outlines some prospects for developing a more holistic and formally grounded approach.", "num_citations": "4\n", "authors": ["1871"]}
{"title": "Analyzing access control configurations\n", "abstract": " A facility is described for analyzing access control configurations. In various embodiments, the facility comprises an operating system having resources and identifications of principals, the principals having access control privileges relating to the resources, the access control privileges described by access control metadata; an access control scanner component that receives the access control metadata, determines relationships between principals and resources, and emits access control relations information; and an access control inference engine that receives the emitted access control relations information and an access control policy model, analyzes the received information and model, and emits a vulnerability report. In various embodiments, the facility generates an information flow based on access control relations, an access control mechanism model, and an access control policy model; determines, based\u00a0\u2026", "num_citations": "3\n", "authors": ["1871"]}
{"title": "Modeling a data generating process using dyadic Bayesian models\n", "abstract": " There is provided a method and system for modeling a data generating process. The method includes generating a dyadic Bayesian model including a pair of probabilistic functions representing a prior distribution and a sampling distribution, and modeling a data generating process based on the dyadic Bayesian model using observed data. The method includes generating a learner object for the dyadic Bayesian model. The method further includes training the dyadic Bayesian model with the learner object based on the observed data to produce a trained dyadic Bayesian model. The method also includes generating a posterior distribution over parameters based on the trained dyadic Bayesian model. The method also further includes generating a posterior predictive distribution based on the posterior distribution. The method also includes predicting an outcome of observable variables based on the posterior\u00a0\u2026", "num_citations": "3\n", "authors": ["1871"]}
{"title": "Program analysis and machine learning: A win-win deal\n", "abstract": " We give an account of our experiences working at the intersection of two fields: program analysis and machine learning. In particular, we show that machine learning can be used to infer annotations for program analysis tools, and that program analysis techniques can be used to improve the efficiency of machine learning tools.", "num_citations": "3\n", "authors": ["1871"]}
{"title": "Merlin: Specification inference for explicit information flow problems\n", "abstract": " The last several years have seen a proliferation of static and runtime analysis tools for finding security violations that are caused by explicit information flow in programs. Much of this interest has been caused by the increase in the number of vulnerabilities such as cross-site scripting and SQL injections. In fact, these explicit information flow vulnerabilities commonly found in Web applications now outnumber vulnerabilities such as buffer overruns common in type-unsafe languages such as C and C++. Tools checking for these vulnerabilities require a specification to operate. In most cases the task of providing such a specification is delegated to the user. Moreover, the efficacy of these tools is only as good as the specification. Unfortunately, writing a comprehensive specification presents a major challenge: parts of the specification are easy to miss leading to missed vulnerabilities; similarly, incorrect specifications may lead to false positives. This paper proposes Merlin, a new algorithm for automatically inferring explicit information flow specifications from program code. Such specifications greatly reduce manual labor, and enhance the quality of results, while using tools that check for security violations caused by explicit information flow. Beginning with a data propagation graph, which represents interprocedural flow of information in the program, Merlin aims to automatically infer an information flow specification. Merlin models information flow paths in the propagation graph using probabilistic constraints. A na\u0131ve modeling requires an exponential number of constraints, one per path in the propagation graph. For scalability, we approximate these path\u00a0\u2026", "num_citations": "3\n", "authors": ["1871"]}
{"title": "Minimal logic re-synthesis\n", "abstract": " Most problems in logic synthesis are computationally hard and are solved using heuristics. This often makes algorithms un-stable; if the input is changed slightly, the new result of synthesis can be significantly different. A designer can spend much effort hand-optimizing a circuit, so it is desirable to retain as much of this human insight as possible. This motivates the need for incremental synthesis. We propose a re-synthesis algorithm, which allows the designer to designate non-resynthesizable portions of a circuit. We define the concept of minimal change caused by re synthesis, ie given a functional change to the circuit, we examine the minimal change to implement this change. For the evaluation of a region for re-synthesis we present techniques for evaluating the \u201csensitivity\u201d or gain possible with resynthesis of a set of nodes. We conclude with experimental results and future directions.", "num_citations": "3\n", "authors": ["1871"]}
{"title": "Data operations using a proxy encryption key\n", "abstract": " Operating upon encrypted data with a particular data scope. A base encryption key is established and associated with the particular data scope, and then stored in a base encryption key store. That base encryption key store might be managed by an application or service that stores base encryption keys for multiple data scopes. A proxy encryption key acts as a kind of proxy for the base encryption key. The proxy encryption key may be used for frequent operations on encrypted data within the particular data scope. Thus, the principles described herein act as a frequency amplifier that allows key-based operations upon the particular data scope to be performed at much higher frequencies than otherwise would be possible by operating directly using the base encryption key.", "num_citations": "2\n", "authors": ["1871"]}
{"title": "CScale \u2013 A Programming Model for Scalable and Reliable Distributed Applications\n", "abstract": " Today\u2019s connected world demands applications that are responsive, always available, and can service a large number of users. However, the task of writing such applications is daunting, even for experienced developers. We propose CScale, a programming model that attempts to simplify this task. The objective of CScale is to let programmers specify their application\u2019s core logic declaratively without explicitly managing distribution. CScale applications have simple semantics that simplify reasoning about correctness and enable testing and debugging on the single machine. In turn, the CScale runtime manages all aspects of execution of a CScale application on large clusters, including deployment, state management (replication and data partitioning) and fault tolerance. CScale ensures high availability by using distributed wait-free data structures to manage state. CScale does impose some constraints on\u00a0\u2026", "num_citations": "2\n", "authors": ["1871"]}
{"title": "Static and dynamic analysis: better together\n", "abstract": " Static analysis and dynamic analysis have dual properties. Static analysis has high coverage, but is imprecise, and produces false alarms. Dynamic analysis has low coverage, but has high precision. We present lessons learned from three different projects, where we have combined the complementary strengths of static and dynamic analysis to solve interesting problems: 1 The yogi project [2], which combines static abstraction-refinement and directed testing to validate if software components obey safety properties specified as state machines. 1 The netra project [3], which combines static access control configuration analysis and runtime checking to find information flow violations. 1 The clarity programming language [1], where a static analysis together with the clarity compiler and runtime helps guarantee properties that involve reasoning about asynchrony.", "num_citations": "2\n", "authors": ["1871"]}
{"title": "Using trusted execution environments to enable integrity of offline test taking\n", "abstract": " Today, automated assessment in online courses (MOOCs, SPOCS, etc.) is done in a client\u2013server fashion. The only trusted component is the server as it is run by the test creator. The user accesses the automated assessment module by logging into the server using their mobile device. The server sends questions to the user\u2019s device one by one. For each question, the user types their answer into their device, which is then transmitted to the server for evaluation and scoring. This process requires constant internet connectivity on the user\u2019s device, which may be a difficult requirement in developing nations. For evaluating complex assignments, it also places a significant load on the server. This paper aims to address these issues by providing a secure offline solution for the automated assessment problem. An offline approach performs the evaluation and scoring on the user\u2019s mobile device and we must protect\u00a0\u2026", "num_citations": "1\n", "authors": ["1871"]}
{"title": "Robust MCMC Sampling for Probabilistic Programs: Taming the curse of Priors\n", "abstract": " Probabilistic programs allow developers to describe probabilistic models in convenient programming language notations. Consequently, several computational techniques have been proposed over the past few years to infer these implicitly specified probabilistic models. Of these, sampling-based techniques constitute one of the promising approaches. Despite significant progress in the scalability of sampling-based inference tools, the runtime performance of current state-of-the-art techniques strongly depends on the accuracy of priors with respect to real-world observations. In this paper, we propose a novel MCMC samplingbased inference approach, called SmartSample, that combines a post-image analysis with Bayesian estimation to improve the robustness of runtime performance with respect to priors and real-world data. Our empirical evaluation demonstrates that SmartSample outperforms existing state-of\u00a0\u2026", "num_citations": "1\n", "authors": ["1871"]}
{"title": "Secure computation interfaces\n", "abstract": " Applications such as secure Hadoop [1] need to have part of their data and code isolated from privileged software (eg, the operating system), and they need to be able to establish secure communication channels between the isolated code and remote machines. This document investigates the APIs needed to develop these applications. These APIs isolate the applications from the underlying hardware or software that provides \u201csecure computation\u201d[2, 3, 4, 5]. For example, the applications should work independently of whether isolation is implemented using SGX [6], or using Hypervisor-based isolation techniques [2, 3, 4, 5].", "num_citations": "1\n", "authors": ["1871"]}
{"title": "Verification of Confidentiality Properties of Enclave Programs\n", "abstract": " Security-critical applications running in the cloud constantly face threats from exploits in lower computing layers such as the operating system, virtual machine monitors, or even attacks from malicious datacenter administrators. To help protect application secrets from such attacks, there is increasing interest in hardware implementations of primitives for trusted computing, such as Intel\u2019s Software Guard Extensions (SGX). These primitives enable hardware protection of memory regions containing code and data, root of trust for measurement, remote attestation, and cryptographic sealing. However, vulnerabilities in the application itself (eg incorrect use of SGX instructions, memory safety errors) can be exploited to divulge secrets. We introduce Moat, a tool which formally verifies confidentiality properties of applications running on SGX. We create formal models of relevant aspects of SGX, develop several adversary models, and present a verification methodology for proving that an application running on SGX does not contain a vulnerability that causes it to reveal secrets to the adversary. We evaluate Moat on several applications, including a one time password scheme, off-the-record messaging, notary service, and secure query processing.", "num_citations": "1\n", "authors": ["1871"]}
{"title": "Manipulation of User Experience State\n", "abstract": " Techniques for manipulation of user experience state are described. A user experience can include various types of content that a user may consume, such as video content, images, audio content, text documents, and so on. Further, a \u201ccomposition\u201d can be created using various combinations of user experiences, such as still images inset to video content, a navigable map presented with images of geographical locations associated with the map, and so on. In implementations, techniques enable user experiences included as part of a composition to interact such that behaviors associated with one user experience can affect another user experience, and vice-versa.", "num_citations": "1\n", "authors": ["1871"]}