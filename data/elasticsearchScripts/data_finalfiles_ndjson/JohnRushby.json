{"title": "Design and verification of secure systems\n", "abstract": " This paper reviews some of the difficulties that arise in the verification of kernelized secure systems and suggests new techniques for their resolution. It is proposed that secure systems should be conceived as distributed systems in which security is achieved partly through the physical separation of its individual components and partly through the mediation of trusted functions performed within some of those components. The purpose of a security kernel is simply to allow such a 'distributed' system to actually run within a single processor; policy enforcement is not the concern of a security kernel. This approach decouples verification of components which perform trusted functions from verification of the security kernel. This latter task may be accomplished by a new verification technique called 'proof of separability' which explicitly addresses the security relevant aspects of interrupt handling and other issues ignored by\u00a0\u2026", "num_citations": "643\n", "authors": ["1785"]}
{"title": "Noninterference, transitivity, and channel-control security policies\n", "abstract": " We consider noninterference formulations of security policies [7] in which the \u201cinterferes\u201d relation is intransitive. Such policies provide a formal basis for several real security concerns, such as channel control [17, 18], and assured pipelines [4]. We show that the appropriate formulation of noninterference for the intransitive case is that developed by Haigh and Young for \u201cmultidomain security\u201d(MDS)[9, 10]. We construct an \u201cunwinding theorem\u201d[8] for intransitive polices and show that it differs significantly from that of Haigh and Young. We argue that their theorem is incorrect. A companion report [22] presents a mechanically-checked formal specification and verification of our unwinding theorem. We consider the relationship between transitive and intransitive formulations of security. We show that the standard formulations of noninterference and unwinding [7, 8] correspond exactly to our intransitive formulations, specialized to the transitive case. We show that transitive polices are precisely the \u201cmultilevel security\u201d(MLS) polices, and that any MLS secure system satisfies the conditions of the unwinding theorem.", "num_citations": "492\n", "authors": ["1785"]}
{"title": "Bus architectures for safety-critical embedded systems\n", "abstract": " Embedded systems for safety-critical applications often integrate multiple \u201cfunctions\u201d and must generally be fault-tolerant. These requirements lead to a need for mechanisms and services that provide protection against fault propagation and ease the construction of distributed fault-tolerant applications.A number of bus architectures have been developed to satisfy this need. This paper reviews the requirements on these architectures, the mechanisms employed, and the services provided. Four representative architectures (SAFEbusTM, SPIDER,TTA, and FlexRay) are briefly described.", "num_citations": "385\n", "authors": ["1785"]}
{"title": "Partitioning in avionics architectures: Requirements, mechanisms, and assurance\n", "abstract": " Automated aircraft control has traditionally been divided into distinct functions that are implemented separately eg, autopilot, autothrottle, flight management each function has its own fault-tolerant computer system, and dependencies among different functions are generally limited to the exchange of sensor and control data. A by-product of this federated architecture is that faults are strongly contained within the computer system of the function where they occur and cannot readily propagate to affect the operation of other functions. More modern avionics architectures contemplate supporting multiple functions on a single, shared, fault-tolerant computer system where natural fault containment boundaries are less sharply defined. Partitioning uses appropriate hardware and software mechanisms to restore strong fault containment to such integrated architectures. This report examines the requirements for partitioning, mechanisms for their realization, and issues in providing assurance for partitioning. Because partitioning shares some concerns with computer security, security models are reviewed and compared with the concerns of partitioning.Descriptors:", "num_citations": "325\n", "authors": ["1785"]}
{"title": "Using model checking to help discover mode confusions and other automation surprises\n", "abstract": " Automation surprises occur when an automated system behaves differently than its operator expects. If the actual system behavior and the operator's \u2018mental model\u2019 are both described as finite state transition systems, then mechanized techniques known as \u2018model checking\u2019 can be used automatically to discover any scenarios that cause the behaviors of the two descriptions to diverge from one another. These scenarios identify potential surprises and pinpoint areas where design changes, or revisions to training materials or procedures, should be considered. The mental models can be suggested by human factors experts, or can be derived from training materials, or can express simple requirements for \u2018consistent\u2019 behavior. The approach is demonstrated by applying the Mur\u00f8 state exploration system to a \u2018kill-the-capture\u2019 surprise in the MD-88 autopilot.This approach does not supplant the contributions of those\u00a0\u2026", "num_citations": "298\n", "authors": ["1785"]}
{"title": "Formal methods and the certification of critical systems\n", "abstract": " This report was prepared to supplement a forthcoming chapter on formal methods in the FAA Digital Systems Validation Handbook1. Its purpose is to outline the technical basis for formal methods in computer science, to explain the use of formal methods in the speci cation and veri cation of software and hardware requirements, designs, and implementations, to identify the bene ts, weaknesses, and di culties in applying these methods to digital systems used in critical applications, and to suggest factors for consideration when formal methods are o ered in support of certi cation.The report assumes a serious interest in the engineering of critical systems, and a willingness to read occasional mathematical formulas and specialized terminology, but assumes no special background in formal logic or mathematical speci cation techniques.(An appendix provides a rapid introduction to formal logic for those to whom this topic is new.) The discussion should be accessible to most people with an engineering background. It may also be of use to those who develop or advocate formal methods and are interested in their use to support certi cation of critical systems.", "num_citations": "281\n", "authors": ["1785"]}
{"title": "Critical system properties: Survey and taxonomy\n", "abstract": " Computer systems are increasingly employed in circumstances where their failure (or even their correct operation, if they are built to flawed requirements) can have serious consequences.There is a surprising diversity of opinion concerning the properties that such \u2018critical systems\u2019 should possess, and the best methods to develop them. The dependability approach grew out of the tradition of ultra-reliable and fault-tolerant systems, while the safety approach grew out of the tradition of hazard analysis and system safety engineering. Yet another tradition is found in the security community, and there are further specialized approaches in the tradition of real-time systems. In this article are examined the critical properties considered in each approach, and the techniques that have been developed to specify them and to ensure their satisfaction.Since systems are now being constructed that must satisfy several of these\u00a0\u2026", "num_citations": "238\n", "authors": ["1785"]}
{"title": "Formal methods and their role in the certification of critical systems\n", "abstract": " This article describes the rationale for formal methods and considers the benefits, weaknesses, and difficulties in applying these methods to digital systems used in critical applications. It suggests factors for consideration when formal methods are offered in support of certification in a context such as DO-178B (the guidelines for software used on board civil aircraft) [40]. The presentation is intended for those to whom these topics are new. A more technical discussion of formal methods is available as a technical report [42].", "num_citations": "208\n", "authors": ["1785"]}
{"title": "The PVS specification language (beta release)\n", "abstract": " PVS is a Prototype Verification System for specifying and verifying digital systems. The PVS system consists of a specification language, a parser, a typechecker, a prover, specification libraries, and various browsing tools. This document primarily describes the specification language and is meant to be used as a reference manuaL The PVS User Guide [9] is to be consulted for information on how to use the system to develop specifications and proofs, The PVS Proof Checker manual [11] is a reference manual for the commands used to construct proofs.", "num_citations": "196\n", "authors": ["1785"]}
{"title": "User Guide for the PVS Specification and Verification System (Beta Release)\n", "abstract": " The Prototype Veri cation System (PVS) provides an integrated environment for the development and analysis of formal speci cations, and supports a wide range of activities involved in creating, analyzing, modifying, managing, and documenting theories and proofs. This section provides a broad overview of PVS; the facilities provided by the system are discussed in the order a new user is likely to encounter them.", "num_citations": "175\n", "authors": ["1785"]}
{"title": "Generating efficient test sets with a model checker\n", "abstract": " It is well-known that counterexamples produced by model checkers can provide a basis for automated generation of test cases. However when this approach is used to meet a coverage criterion, it generally results in very inefficient test sets having many tests and much redundancy. We describe an improved approach that uses model checkers to generate efficient test sets. Furthermore, the generation is itself efficient, and is able to reach deep regions of the statespace. We have prototyped the approach using the model checkers of our SAL system and have applied it to model-based designs developed in Stateflow. In one example, our method achieves complete state and transition coverage in a Stateflow model for the shift scheduler of a 4-speed automatic transmission with a single test case.", "num_citations": "171\n", "authors": ["1785"]}
{"title": "Kernels for safety\n", "abstract": " Secure systems are often built around a \u201csecurity kernel\u201d\u2014a relatively small and simple component that guarantees the security of the overall system. In this paper we ask whether this approach can be used to ensure system properties other than security\u2014in particular, we are interested in whether \u201csafety\u201d properties can be handled in this way.Our conclusion is that kernelized system structures can provide rigorous guarantees that certain faults of commission will not occur. We give a more precise characterization in terms of the formal statement that can be asserted for a kernelized system and we outline an approach to system design that uses these insights and draws on experience with secure systems in order guarantee certain safety properties.", "num_citations": "144\n", "authors": ["1785"]}
{"title": "Proof of separability a verification technique for a class of security kernels\n", "abstract": " A formal model of \u2018secure isolation\u2019 between the users of a shared computer system is presented. It is then developed into a security verification technique called \u2018Proof of Separability\u2019 whose basis is to prove that the behaviour perceived by each user of the shared system is indistinguishable from that which could be provided by an unshared machine dedicated to his private use.             Proof of Separability is suitable for the verification of security kernels which enforce the policy of isolation; it explicitly addresses issues relating to the interpretation of instructions and the flow of control (including interrupts) which have been ignored by previous treatments.", "num_citations": "132\n", "authors": ["1785"]}
{"title": "An operational semantics for Stateflow\n", "abstract": " We present a formal operational semantics for Stateflow, the graphical Statecharts-like language of the Matlab/Simulink tool suite that is widely used in model-based development of embedded systems. Stateflow has many tricky features but our operational treatment yields a surprisingly simple semantics for the subset that is generally recommended for industrial applications. We have validated our semantics by developing an interpreter that allows us to compare its behavior against the Matlab simulator. We have used the semantics as a foundation for developing prototype tools for formal analysis of Stateflow designs.", "num_citations": "131\n", "authors": ["1785"]}
{"title": "Systematic formal verification for fault-tolerant time-triggered algorithms\n", "abstract": " Many critical real-time applications are implemented as time-triggered systems. We present a systematic way to derive such time-triggered implementations from algorithms specified as functional programs (in which form their correctness and fault-tolerance properties can be formally and mechanically verified with relative ease). The functional program is first transformed into an untimed synchronous system and, then, to its time-triggered implementation. The first step is specific to the algorithm concerned, but the second is generic and we prove its correctness. This proof has been formalized and mechanically checked with the PVS verification system. The approach provides a methodology that can ease the formal specification and assurance of critical fault-tolerant systems.", "num_citations": "125\n", "authors": ["1785"]}
{"title": "Formal verification of algorithms for critical systems\n", "abstract": " The authors describe their experience with formal, machine-checked verification of algorithms for critical applications, concentrating on a Byzantine fault-tolerant algorithm for synchronizing the clocks in the replicated computers of a digital flight control system. The problems encountered in unsynchronized systems and the necessity, and criticality, of fault-tolerant synchronization are described. An overview of one such algorithm and of the arguments for its correctness are given. A verification of the algorithm performed using the authors' EHDM system for formal specification and verification is described. The errors found in the published analysis of the algorithm and benefits derived from the verification are indicated. Based on their experience, the authors derive some key requirements for a formal specification and verification system adequate to the task of verifying algorithms of the type considered. The conclusions\u00a0\u2026", "num_citations": "116\n", "authors": ["1785"]}
{"title": "Runtime certification\n", "abstract": " Software often must be certified for safety, security, or other critical properties. Traditional approaches to certification require the software, its systems context, and all their associated assurance artifacts to be available for scrutiny in their final, completed forms. But modern development practices often postpone the determination of final system configuration from design time to integration time, load time, or even runtime. Adaptive systems go beyond this and modify or synthesize functions at runtime.             Developments such as these require an overhaul to the basic framework for certification, so that some of its responsibilities also may be discharged at integration-, load- or runtime.             We outline a suitable framework, in which the basis for certification is changed from compliance with standards to the construction of explicit goals, evidence, and arguments (generally called an \u201cassurance case\u201d). We\u00a0\u2026", "num_citations": "99\n", "authors": ["1785"]}
{"title": "The MILS component integration approach to secure information sharing\n", "abstract": " To achieve the vision of information superiority, secure and timely sharing of information is needed between geographically separated platforms and users. However, often the producers and consumers of the information, as well as the information itself are separated in different security domains. A COTS marketplace of composable, high assurance components would not only make the vision of cross-domain information sharing achievable, but could also help to make it much more affordable than is currently possible. As part of the Multiple Independent Levels of Security/Safety initiative, AFRL's multi-year High Assurance Middleware for Embedded Systems (HAMES) program is conducting research in integrating trusted components in such a way that the security properties of the system can be predicted. MILS is characterized by a two-level approach to secure system design. At the policy level, a decomposition to\u00a0\u2026", "num_citations": "92\n", "authors": ["1785"]}
{"title": "New challenges in certification for aircraft software\n", "abstract": " We outline the current approach to certification of aircraft software, and the role of DO-178B. We consider evidence for its effectiveness and discuss possible explanations for this. We then describe how changes in aircraft systems and in the air traffic system pose new challenges for certification, chiefly by increasing the extent of interaction and integration.", "num_citations": "91\n", "authors": ["1785"]}
{"title": "Modular certification\n", "abstract": " Airplanes are certified as a whole: there is no established basis for separately certifying some components, particularly software-intensive ones, independently of their specific application in a given airplane. The absence of separate certification inhibits the development of modular components that could be largely \u201cprecertified\u201d and used in several different contexts within a single airplane, or across many different airplanes. In this report, we examine the issues in modular certification of software components and propose an approach based on assume-guarantee reasoning. We extend the method from verification to certification by considering behavior in the presence of failures. This exposes the need for partitioning, and separation of assumptions and guarantees into normal and abnormal cases. We then identify three classes of property that must be verified within this framework: safe function, true guarantees, and controlled failure. We identify a particular assume-guarantee proof rule (due to McMillan) that is appropriate to the applications considered, and formally verify its soundness in PVS. iii", "num_citations": "91\n", "authors": ["1785"]}
{"title": "An overview of formal verification for the time-triggered architecture\n", "abstract": " We describe formal verification of some of the key algorithms in the Time-Triggered Architecture (TTA) for real-time safety-critical control applications. Some of these algorithms pose formidable challenges to current techniques and have been formally verified only in simplified form or under restricted fault assumptions. We describe what has been done and what remains to be done and indicate some directions that seem promising for the remaining cases and for increasing the automation that can be applied. We also describe the larger challenges posed by formal verification of the interaction of the constituent algorithms and of their emergent properties.", "num_citations": "86\n", "authors": ["1785"]}
{"title": "Model checking a fault-tolerant startup algorithm: From design exploration to exhaustive fault simulation\n", "abstract": " The increasing performance of modern model-checking tools offers high potential for the computer-aided design of fault-tolerant algorithms. Instead of relying on human imagination to generate taxing failure scenarios to probe a fault-tolerant algorithm during development, we define the fault behavior of a faulty process at its interfaces to the remaining system and use model checking to automatically examine all possible failure scenarios. We call this approach \"exhaustive fault simulation\". In this paper we illustrate exhaustive fault simulation using a new startup algorithm for the time-triggered architecture (TTA) and show that this approach is fast enough to be deployed in the design loop. We use the SAL toolset from SRI for our experiments and describe an approach to modeling and analyzing fault-tolerant algorithms that exploits the capabilities of tools such as this.", "num_citations": "85\n", "authors": ["1785"]}
{"title": "Models and mechanized methods that integrate human factors into automation design\n", "abstract": " Recent work has shown a convergence between the Human Factors and Formal Methods communities that opens promising new directions for collaborative work in calculating, predicting, and analyzing the behavior of complex aeronautical systems and their operators. Previously it has been shown that fully automatic, finitestate verification techniques can be used to identify likely sources of mode confusion in existing systems; in this paper we focus on use of these techniques in the design of new systems. We use a simple example to demonstrate how automated finite-state techniques can be used to explore autopilot design options, and then suggest additional applications for this technique, including the validation of empirically-derived, minimal mental models of autopilot behavior.", "num_citations": "83\n", "authors": ["1785"]}
{"title": "A trusted computing base for embedded systems\n", "abstract": " The structure of many secure systems has been based on the idea of a security kernel| an operating system nucleus that performs all trusted functions. The di culty with this approach is that the security kernel tends to be rather large, complex, and unstructured. This paper proposes an alternative structure for secure embedded systems. The structure comprises three layers. At the bottom is a Domain Separation Mechanism which is responsible for maintaining isolated\\domains\"(also known as\\processes\" or\\virtual machines\") and for providing controlled channels for their intercommunication. The other resources of the system (for example, devices and the more abstract entities, such as le systems, built upon them) are each controlled by independent resource managers which comprise the second layer of the system. The applications code provides the third layer. Components in both the resource management and applications layers are protected from each other by the domain separation mechanism. The Trusted Computing Base is composed of the domain separation mechanism and a reference validation mechanism associated with each resource.The bene t of this approach is that it leads to a separation of concerns: each component of the embedded system performs a single, well-de ned activity and can be understood (and veri ed) in relative isolation from all other components. Implementation and language issues are also discussed.", "num_citations": "82\n", "authors": ["1785"]}
{"title": "Security requirements specifications: How and what\n", "abstract": " Requirements engineering for information security poses two main challenges: eliciting what are the requirements for a particular system, and figuring out how to specify them in a way that is both perspicuous (to the problem owner) and useful (to the developer). In this short note, I look at some of the challenges in how to specify security requirements, and at what kind of requirements we may expect to encounter in the future.", "num_citations": "75\n", "authors": ["1785"]}
{"title": "Acceptance of formal methods: Lessons from hardware design\n", "abstract": " Despite years of research, the overall impact of formal methods on mainstream software design has been disappointing. By contrast, formal methods are beginning to make real inroads in commercial hardware design. This penetration is the result of sustained progress in automated hardware veri cation methods, an increasing accumulation of success stories from using formal techniques, and a growing consensus among hardware designers that traditional validation techniques are not keeping up with the increasing complexity of designs. For example, validation of a new microprocessor design typically requires as much manpower as the design itself, and the size of validation teams continues to grow. This manpower is employed in writing test cases for simulations that run for months on acres of high-powered workstations. In particular, the notorious FDIV bug in the Intel Pentium processor 13], has galvanized veri cation e orts, not because it was the rst or most serious bug in a processor design, but because it was easily repeatable and because the cost was quanti ed (at over $400 million).Hence, hardware design companies are increasingly looking to new techniques, including formal veri cation, to supplement and sometimes replace conventional validation methods. Indeed, many companies, including industry leaders such as AT&T, Cadence, Hewlett-Packard, IBM, Intel, LSI Logic, Motorola, Rockwell, Texas Instruments, and Silicon Graphics have created formal veri cation groups to help with ongoing designs. 1 In many cases, these groups began by demonstrating the e ectiveness of formal veri cation by nding subtle design errors that were\u00a0\u2026", "num_citations": "72\n", "authors": ["1785"]}
{"title": "Theorem proving for verification\n", "abstract": " The challenges in using theorem proving for verification of parallel systems are to achieve adequate automation, and to allow human guidance to be expressed in terms of the system under examination rather than the mechanisms of the prover. This paper provides an overview of techniques that address these challenges.", "num_citations": "71\n", "authors": ["1785"]}
{"title": "Formal methods and digital systems validation for airborne systems\n", "abstract": " This report has been prepared to supplement a forthcoming chapter on formal methods in the FAA Digital Systems Validation Handbook. Its purpose is as follows: to outline the technical basis for formal methods in computer science; to explain the use of formal methods in the specification and verification of software and hardware requirements, designs, and implementations; to identify the benefits, weaknesses, and difficulties in applying these methods to digital systems used on board aircraft; and to suggest factors for consideration when formal methods are offered in support of certification. These latter factors assume the context for software development and assurance described in RTCA document DO-178B, 'Software Considerations in Airborne Systems and Equipment Certification,' Dec. 1992.", "num_citations": "69\n", "authors": ["1785"]}
{"title": "Formal methods for test case generation\n", "abstract": " The invention relates to the use of model checkers to generate efficient test sets for hardware and software systems. The method provides for extending existing tests to reach new coverage targets; searching* to* some or all of the uncovered targets in parallel; searching in parallel* from* some or all of the states reached in previous tests; and slicing the model relative to the current set of coverage targets. The invention provides efficient test case generation and test set formation. Deep regions of the state space can be reached within allotted time and memory. The approach has been applied to use of the model checkers of SRI's SAL system and to model-based designs developed in Stateflow. Stateflow models achieving complete state and transition coverage in a single test case are reported.", "num_citations": "65\n", "authors": ["1785"]}
{"title": "Formally verified byzantine agreement in presence of link faults\n", "abstract": " This paper shows that deterministic consensus in synchronous distributed systems with link faults is possible, despite the impossibility result of Gray (1978). Instead of using randomization, we overcome this impossibility by moderately restricting the inconsistency that link faults may cause system-wide. Relying upon a novel hybrid fault model that provides different classes of faults for both nodes and links, we provide a formally verified proof that the m+1-round Byzantine agreement algorithm OMH (Lincoln and Rushby (1993)) requires n > 2f/sub l//sup s/ + f/sub l//sup r/ + f/sub l//sup ra/ + 2(f/sub a/ + f/sub s/) + f/sub o/ + f/sub m/ + m nodes for transparently masking at most f/sub l//sup s/ broadcast and f/sub l//sup r/ receive link faults (including at most f/sub l//sup ra/ arbitrary ones) per node in each round, in addition to at most f/sub a/, f/sub s/, f/sub o/, f/sub m/ arbitrary, symmetric, omission, and manifest node faults\u00a0\u2026", "num_citations": "62\n", "authors": ["1785"]}
{"title": "An automated method to detect potential mode confusions\n", "abstract": " Mode confusions are a type of \"automation surprise\"-circumstances where an automated system behaves differently than its operator expects. It is generally accepted that operators develop \"mental models\" for the behavior of automated systems and use these to guide their interaction with the systems concerned, so that an automation surprise results when the actual system behavior diverges from its operator's mental model. Complex systems are often structured into \"modes\" (for example, an autopilot might have different modes for altitude capture, altitude hold, and so on), and their behavior can change significantly across different modes. \"Mode confusion\" arises when the system is in a different mode than that assumed by its operator; this is a rich source of automation surprises, since the operator may interact with the system according to a mental model that is inappropriate for its actual mode. Mode confusions\u00a0\u2026", "num_citations": "62\n", "authors": ["1785"]}
{"title": "A formally verified algorithm for clock synchronization under a hybrid fault model\n", "abstract": " A small modification to the interactive convergence clock synchronization algorithm allows it to tolerate a larger number of simple faults than the standard algorithm, without reducing its ability to tolerate arbitrary or \u201cByzantine\u201d faults. Because the extended caseanalysis required by the new fault model complicates the already intricate argument for correctness of the algorithm, it has been subjected to mechanically-checked formal verification.The fault model examined is similar to the\u201chybrid\u201d one previously used for the problem of distributed consensus: in addition to arbitrary faults, we also admit symmetr~ c(ie, consistent) and manifest(ie, detect able) faults. With n processors, the modified algorithm can withstand a arbitrary, s symmetric, and m manifest faults simultaneously, provided n> 3a+ 2s+ m. A further extension to the fault model includes link faults with bound n> 3a+ 2s+ m+ 1 where t is the maximum, over all\u00a0\u2026", "num_citations": "62\n", "authors": ["1785"]}
{"title": "Model-based reconfiguration: Toward an integration with diagnosis\n", "abstract": " We extend Reiter\u2019s general theory of model-based diagnosis [Reiter, 1987] to a theory of reconfiguration. The generality of Reiter\u2019s theory readily supports an extension in which the problem of reconfiguration is viewed as a close analogue of the problem of diagnosis. Using a reconfiguration predicate rcfg analogous to the abnormality predicate ab, we formulate a strategy for reconfiguration by transforming that for diagnosis. A benefit of this approach is that algorithms for diagnosis can be exploited as algorithms for reconfiguration, thereby promoting an integrated approach to fault detection, identification, and reconfiguration.", "num_citations": "61\n", "authors": ["1785"]}
{"title": "The interpretation and evaluation of assurance cases\n", "abstract": " Assurance cases are a method for providing assurance for a system by giving an argument to justify a claim about the system, based on evidence about its design, development, and tested behavior.In comparison with assurance based on guidelines or standards (which essentially specify only the evidence to be produced), the chief novelty in assurance cases is provision of an explicit argument. In principle, this can allow assurance cases to be more finely tuned to the specific circumstances of the system, and more agile than guidelines in adapting to new techniques and applications. The first part of this report (Chapters 1\u20134) provides an introduction to assurance cases. Although this material should be accessible to all those with an interest in these topics, the examples focus on software for airborne systems, traditionally assured using the DO-178C guidelines and its predecessors. The second part (Chapters 5 and 6) considers the criteria, methods, and tools that may be used to evaluate whether an assurance case provides sufficient confidence that a particular system or service is fit for its intended use.", "num_citations": "60\n", "authors": ["1785"]}
{"title": "Integrated formal verification: Using model checking with automated abstraction, invariant generation, and theorem proving\n", "abstract": " Mechanized formal methods that use both model checking and theorem proving seem to hold most promise for the future. Effective use of both technologies requires they be recast as methods for calculating properties of specifications, rather than merely verifying them. The most valuable properties are those that contribute to the development of invariants and property-preserving abstractions. We outline an architecture for verification tools based on iterated use of such capabilities.", "num_citations": "59\n", "authors": ["1785"]}
{"title": "Formalism in safety cases\n", "abstract": " Suitable formalisms could allow the arguments of a safety case to be checked mechanically. We examine some of the issues in doing so.", "num_citations": "58\n", "authors": ["1785"]}
{"title": "Analyzing cockpit interfaces using formal methods\n", "abstract": " Modern passenger aircraft are highly automated, and problems at the interface between the automation and the pilot are implicated in several accidents. I use a simple example taken from the autopilot of a widely used aircraft type to demonstrate how formal methods can be used to analyze some aspects of these interfaces, and to expose potential problems.This example serves to illustrate the wider thesis that formal methods can find application in domains outside those traditionally associated with it, provided only that the phenomena of interest can be modeled effectively in discrete mathematics.", "num_citations": "57\n", "authors": ["1785"]}
{"title": "Formal specification and verification of a fault-masking and transient-recovery model for digital flight-control systems\n", "abstract": " We present a formal model for fault-masking and transient-recovery among the replicated computers of digital flight-control systems. We establish conditions under which majority voting causes the same commands to be sent to the actuators as those that would be sent by a single computer that suffers no failures. The model and its analysis have been subjected to formal specification and mechanically checked verification using the Ehdm system.", "num_citations": "57\n", "authors": ["1785"]}
{"title": "A less elementary tutorial for the PVS specification and verification system\n", "abstract": " PVS is a veri cation system that provides a speci cation language integrated with support tools and a theorem-prover. It has been used at SRI and elsewhere to perform veri cations of several signi cant algorithms (primarily for faulttolerance) and large hardware designs. This tutorial introduces some of the more powerful strategies provided by the PVS theorem prover. It consists of two parts: the rst extends a previous tutorial by Ricky Butler But93], demonstrating how his proofs may be performed in a more automated manner; the second uses the\\unwinding theorem\" from the noninterference formulation of security to introduce theorem-proving strategies for induction that cannot be demonstrated in the framework of Ricky Butler's example.Using the more powerful strategies of PVS to automate easy proofs (and the easy parts of hard proofs) frees users to concentrate on truly di cult proofs. Automation also makes proofs more robust to changes in the speci cation, thereby facilitating active design exploration and adaptation to changed requirements. This tutorial also shows how speci cations and proofs may be better presented using the LaTEX and PostScript generating facilities of PVS. The PVS les for these examples are available at http://www. csl. sri. com/pvs/examples/csl-95-10. html.", "num_citations": "54\n", "authors": ["1785"]}
{"title": "Automated test generation and verified software\n", "abstract": " Testing remains the principal means of verification in commercial practice and in many certification regimes. Formal methods of verification will coexist with testing and should be developed in ways that improve, supplement, and exploit the value of testing. I describe automated test generation, which uses technology from formal methods to mechanize the construction of test cases, and discuss some of the research challenges in this area.", "num_citations": "52\n", "authors": ["1785"]}
{"title": "Formal verification of the interactive convergence clock synchronization algorithm\n", "abstract": " We describe a formal speci cation and mechanically checked veri cation of the Interactive Convergence Clock Synchronization Algorithm of Lamport and Melliar-Smith 16]. In the course of this work, we discovered several technical aws in the analysis given by Lamport and Melliar-Smith, even though their presentation is unusually precise and detailed. As far as we know, these aws (a ecting the main theorem and four of its ve lemmas) were not detected by the\\social process\" of informal peer scrutiny to which the paper has been subjected since its publication. We discuss the aws in the published proof and give a revised presentation of the analysis that not only corrects the aws in the original, but is also more precise and, we believe, easier to follow. This informal presentation was derived directly from our formal speci cation and veri cation. Some of our corrections to the aws in the original require slight modi cations to the assumptions underlying the algorithm and to the constraints on its parameters, and thus change the external speci cation of the algorithm. The formal analysis of the Interactive Convergence Clock Synchronization Algorithm was performed using the Ehdm formal speci cation and veri cation environment. This application of Ehdm provides a demonstration of some of the capabilities of the system.Note: This second edition of the report presents a revised version of the formal speci cation and veri cation that exploits some of the features introduced into Ehdm since the original veri cation was performed, and also improves the substance of the veri cation in three respects.", "num_citations": "52\n", "authors": ["1785"]}
{"title": "Formal verification of algorithms for critical systems\n", "abstract": " We describe our experience with formal, machinechecked verification of algorithms for critical ap-, placations, concentrating on a Byzantine faulttolerant algorithm for synchronizing the clocks in the replicated computers of a digit al flight control system.First, we explain the problems encountered in unsynchronized systems and the necessity, and criticalityy, of fault-tolerant synchronization. We give an overview of one such algorithm, and of the arguments for its correctness. Next, we describe a verification of the algorithm that we performed using our EH DM system for formal specification and verification. We indicate the errors we found in the published analysis of the algorithm, and other benefits that we derived from the verification.", "num_citations": "49\n", "authors": ["1785"]}
{"title": "Verification diagrams revisited: Disjunctive invariants for easy verification\n", "abstract": " I describe a systematic method for deductive verification of safety properties of concurrent programs. The method has much in common with the \u201cverification diagrams\u201d of Manna and Pnueli [17], but derives from different intuitions. It is based on the idea of strengthening a putative safety property into a disjunction of \u201cconfigurations\u201d that can easily be proved to be inductive. Transitions among the configurations have a natural diagrammatic representation that conveys insight into the operation of the program. The method lends itself to mechanization and is illustrated using a simplified version of an example that had defeated previous attempts at deductive verification.", "num_citations": "48\n", "authors": ["1785"]}
{"title": "Disappearing formal methods*\n", "abstract": " In his book The Invisible Computer [111, Donald Norman describes how most technological innovations begin as very complex products that require highly skilled operators, and become widely accepted and generally useful only as the products manage more of their own functions and present a simple interface to their operators. Early radios, for example, had dozens of controls to adjust such arcane features as regeneration and filter bandwidth, and operators were expected to understand \u201cthe fundamentals of radio reception.\u201d A modern radio, on the other hand, is trivial to operate.Most current formal methods are like those early radios: they are difficult to use and fit awkwardly into traditional software engineering processes. For example, formal methods generally require a formal specification or model that is quite different to the other documents and descriptions produced during design and whose construction is\u00a0\u2026", "num_citations": "46\n", "authors": ["1785"]}
{"title": "Just-in-time certification\n", "abstract": " Traditional, standards-based approaches to certification are hugely expensive, of questionable credibility when development is outsourced, and a barrier to innovation. This paper is a call and a manifesto for new approaches to certification. We start by advocating a goal-based approach in which unconditional claims delivered by formal methods are combined with other evidence in multi-legged cases supported by Bayesian analysis. We then describe the necessity, and the challenge, of extending this to compositional certification and outline promising directions for accomplishing this. Finally, we consider the provocative possibility of adaptive systems in which methods of analysis traditionally used to support certification at design time are instead used for synthesis and monitoring at runtime, and certification is performed \"just-in-time.\"", "num_citations": "45\n", "authors": ["1785"]}
{"title": "Automated test generation with SAL\n", "abstract": " We describe sal-atg, a tool for automated test generation that will be distributed as part of the next release of SAL. Given a SAL specification augmented with Boolean trap variables representing test goals, sal-atg generates an efficient set of tests to drive the trap variables to TRUE; SAL specifications are typically instrumented with trap variables representing structural coverage criteria during automatic translation from a higher-level source notation, such as RSML\u2212 e or Stateflow. We describe extensions to the method of test generation that use conjunctions of trap variables; we show how these can be used to provide boundary coverage and to encode test purposes. We also describe how the output of the tool can be customized to the requirements of the test harness concerned. We describe experiments with sal-atg on realistic examples and preparations for evaluating the quality of tests generated using the experimental framework of Heimdahl, George and Weber [HGW04]. i", "num_citations": "45\n", "authors": ["1785"]}
{"title": "Software verification and system assurance\n", "abstract": " Littlewood introduced the idea that software may be possibly perfect and that we can contemplate its probability of (im)perfection. We review this idea and show how it provides a bridge between correctness, which is the goal of software verification (and especially formal verification), and the probabilistic properties such as reliability that are the targets for system-level assurance. We enumerate the hazards to formal verification, consider how each of these may be countered, and propose relative weightings that an assessor may employ in assigning a probability of perfection.", "num_citations": "43\n", "authors": ["1785"]}
{"title": "Harnessing disruptive innovation in formal verification\n", "abstract": " Technological innovations are sweeping through the field of formal verification. These changes are disruptive to tools based on interactive theorem proving, which needs new ways to integrate the capabilities of novel technologies. I describe two approaches. One is development and use of SMT solvers: these use techniques from theorem proving but apply them in ways that enable model checking, while also supporting highly automated theorem proving. The other is a proposal for an evidential tool bus: a loosely coupled architecture that allows many different verification components to collaborate to solve problems beyond the capability of any single component", "num_citations": "43\n", "authors": ["1785"]}
{"title": "Formal modeling and analysis for interactive hybrid systems\n", "abstract": " An effective strategy for discovering certain kinds of automation surprise and other problems in interactive systems is to build models of the participating (automated and human) agents and then explore all reachable states of the composed system looking for divergences between mental states and those of the automation. Various kinds of model checking provide ways to automate this approach when the agents can be modeled as discrete automata. But when some of the agents are continuous dynamical systems (eg, airplanes), the composed model is a hybrid (ie, mixed continuous and discrete) system and these are notoriously hard to analyze.We describe an approach for very abstract modeling of hybrid systems using relational approximations and their automated analysis using infinite bounded model checking supported by an SMT solver. When counterexamples are found, we describe how additional constraints can be supplied to direct counterexamples toward plausible scenarios that can be confirmed in high-fidelity simulation. The approach is illustrated though application to a known (and now corrected) human-automation interaction problem in Airbus aircraft.", "num_citations": "37\n", "authors": ["1785"]}
{"title": "Runtime efficient state compaction in SPIN\n", "abstract": " Spin is a verification system that can detect errors automatically by exploring the reachable state space of a system. The efficiency of verifiers like Spin depends crucially on the technique used for the representation of states. A number of recent proposals for more compact representations reduce the memory requirements, but cause a considerable increase in execution time. These methods could be used as alternatives when the standard state representation exhausts the memory, but this is exactly when the additional overhead is least affordable.                 We describe a simple but effective state representation scheme that can be used in conjunction with Spin\u2019s normal modes of operation. We compare the idea to Spin\u2019s standard state representation and describe how Spin was modified to support it. Experimental results show that the technique provides a valuable reduction in memory requirements and\u00a0\u2026", "num_citations": "36\n", "authors": ["1785"]}
{"title": "A safety-case approach for certifying adaptive systems\n", "abstract": " A safety case provides an explicit statement of safety claims, a body of evidence concerning the system, and an argument, based on the evidence, that the system satisfies its claims; standards-based methods, in contrast, specify only the evidence to be produced. A reasonable objection to safety cases is that many arguments\u2014especially large, complex ones\u2014can appear plausible, yet harbor flaws. There is a need for tools that can help analyze arguments. Some model-based design tools can do this, but generally operate at a far more detailed level of design than is appropriate for much of safety analysis. Some interactive theorem provers can do it, too, but they generally require notation and skills far removed from those found in aerospace and safety engineering. In this paper we argue that analysis tools based on recent advances in formal methods (SMT solvers, infinite bounded model checkers, and k-induction\u00a0\u2026", "num_citations": "35\n", "authors": ["1785"]}
{"title": "Separation and integration in MILS (The MILS Constitution)\n", "abstract": " We describe the MILS approach to design, construction, integration, and evaluation of secure systems. The crucial feature of the MILS approach is that it separates the problems of enforcing security policy from those of securely sharing resources. MILS design proceeds in two steps: first, we develop a logical security policy architecture in which the system is deconstructed into interacting components in such a way that the trusted components are as simple as possible; second, we allocate components of the policy architecture to resources that are securely shared through mechanisms for logical separation.MILS identifies certain standard resources such as processors, networks, consoles, and file systems and publishes protection profiles for their logical separation; a COTS marketplace is developing that provides components evaluated to these profiles. Standard protection profiles and a marketplace for evaluated policy components (such as guards and filters) are also anticipated. Top-down design of a MILS system pays attention to existing protection profiles and strives to target these where appropriate. MILS construction can then incorporate COTS products evaluated to these protection profiles.", "num_citations": "35\n", "authors": ["1785"]}
{"title": "Toward a multi-method approach to formalizing human-automation interaction and human-human communications\n", "abstract": " Breakdowns in complex systems often occur as a result of system elements interacting in ways unanticipated by analysts or designers. The use of task behavior as part of a larger, formal system model is potentially useful for analyzing such problems because it allows the ramifications of different human behaviors to be verified in relation to other aspects of the system. A component of task behavior largely overlooked to date is the role of human-human interaction, particularly human-human communication in complex human-computer systems. We are developing a multi-method approach based on extending the Enhanced Operator Function Model language to address human agent communications (EOFMC). This approach includes analyses via theorem proving and future support for model checking linked through the EOFMC top level XML description. Herein, we consider an aviation scenario in which an air\u00a0\u2026", "num_citations": "34\n", "authors": ["1785"]}
{"title": "Modeling the human in human factors\n", "abstract": " Human operators use mental models to guide their interaction with automated systems.We can \u201cmodel the human\u201d by constructing explicit descriptions of plausible mental models. Using mechanized formal methods, we can then calculate divergences between the actual system behavior and that suggested by the mental model. These divergences indicate possible automation surprises and other human factors problems and suggest places where the design should be improved.", "num_citations": "34\n", "authors": ["1785"]}
{"title": "The ontological argument in PVS\n", "abstract": " The Ontological Argument, an 11th Century proof of the existence of God, is a good candidate for Fun With Formal Methods as nearly everyone finds the topic interesting. We formalize the Argument in PVS and verify its correctness. The formalization raises delicate questions in formal logic and provides an opportunity to show how these are handled, soundly and efficiently, by the predicatively-subtyped higher-order logic of PVS and its mechanized support. The simplicity of the Argument, coupled to its bold conclusion, raise interesting issues on the interpretation and application of formal methods in the real world.", "num_citations": "33\n", "authors": ["1785"]}
{"title": "Automated deduction and formal methods\n", "abstract": " The automated deduction and model checking communities have developed techniques that are impressively effective when applied to suitable problems. However, these problems seldom coincide exactly with those that arise in formal methods. Using small but realistic examples for illustration, I will argue that effective deductive support for formal methods requires cooperation among different techniques and an integrated approach to language, deduction, and supporting capabilities such as simulation and the construction of invariants and abstractions. Successful application of automated deduction to formal methods will enrich both fields, providing new opportunities for research and use of automated deduction, and making formal methods a truly useful and practical tool.", "num_citations": "33\n", "authors": ["1785"]}
{"title": "The versatile synchronous observer\n", "abstract": " A synchronous observer is an adjunct to a system model that monitors its state variables and raises a signal flag when some condition is satisfied. Synchronous observers provide an alternative to temporal logic as a means to specify safety properties but have the advantage that they are expressed in the same notation as the system model\u2014and thereby lower the mental hurdle to effective use of model checking and other techniques for automated analysis of system models. Model checkers that do use temporal logic can nonetheless employ synchronous observers by checking for properties such as \u201cnever(flag raised).\u201d             The use of synchronous observers to specify properties is well-known; rather less well-known is that they can be used to specify assumptions and axioms, to constrain models, and to specify test cases. The idea underlying these applications is that the basic model generates more\u00a0\u2026", "num_citations": "32\n", "authors": ["1785"]}
{"title": "Formal verification of a fault tolerant clock synchronization algorithm\n", "abstract": " A formal specification and mechanically assisted verification of the interactive convergence clock synchronization algorithm of Lamport and Melliar-Smith is described. Several technical flaws in the analysis given by Lamport and Melliar-Smith were discovered, even though their presentation is unusally precise and detailed. It seems that these flaws were not detected by informal peer scrutiny. The flaws are discussed and a revised presentation of the analysis is given that not only corrects the flaws but is also more precise and easier to follow. Some of the corrections to the flaws require slight modifications to the original assumptions underlying the algorithm and to the constraints on its parameters, and thus change the external specifications of the algorithm.", "num_citations": "32\n", "authors": ["1785"]}
{"title": "Logic and epistemology in safety cases\n", "abstract": " A safety case must resolve concerns of two different kinds: how complete and accurate is our knowledge about aspects of the system (e.g., its requirements, environment, implementation, hazards) and how accurate is our reasoning about the design of the system, given our knowledge.               The first of these is a form of epistemology and requires human experience and insight, but the second can, in principle, be reduced to logic and then checked and automated using the technology of formal methods.               We propose that reducing epistemic doubt is the main challenge in safety cases, and discuss ways in which this might be achieved.", "num_citations": "31\n", "authors": ["1785"]}
{"title": "Model checking Simpson\u2019s four-slot fully asynchronous communication mechanism\n", "abstract": " Simpson\u2019s four-slot fully asynchronous communication mechanism allows single reader and writer processes to access a shared memory in such a way that interference between concurrent reads and writes is avoided, the reader always accesses the most recent data stored by the writer, and neither process need wait for the other. In computer science parlance, it is a means for implementing a wait-free atomic register. We use the SAL model checking environment to examine this mechanism and show that concurrent reads and writes are indeed noninterfering but that access to the most recently written data requires the unattractive assumption that some of the control registers are already atomic. We exhibit counterexamples that reveal incorrect operation when the control registers are not atomic, and also when the mechanism is modified (following a suggestion of Simpson) so that control registers are written only when their values will be changed. We do successfully verify the algorithm when its control registers are assumed to be atomic. The requirement for atomic control registers is unattractive: it means that any application that uses Simpson\u2019s mechanism must be accompanied by separate, strong evidence that its implementation of the control registers satisfies this requirement. We recommend formal examination of alternative algorithms that operate under weaker assumptions. i", "num_citations": "30\n", "authors": ["1785"]}
{"title": "An evidential tool bus\n", "abstract": " Endgame Verifier Examples 1979: Stanford Pascal Verifier and STP used decision procedures for combinations of theories including arithmetic (STP gave rise to Ehdm, then PVS) 1995: PVS used a BDD-based symbolic model checker 1999: PVS used a predicate abstractor 2000: PVS used Mona for WS1S", "num_citations": "29\n", "authors": ["1785"]}
{"title": "Simulink Design Verifier\u2014Applying Automated Formal Methods to Simulink and Stateflow\n", "abstract": " We present Simulink Design Verifier, a tool distributed by The MathWorks to help Simulink users in their verification and validation activities. Simulink Design Verifier automatically generates test input sequences for a model or proves properties about this model. The tool is based on automatic theorem proving and model-checking techniques and aims at a high level of integration with Simulink to make formal verification techniques easier to incorporate into engineers\u2019 workflows. In this presentation, we describe the tool, highlight some of the technical choices that were made during its conception, and discuss user experiences with it.", "num_citations": "28\n", "authors": ["1785"]}
{"title": "Evaluation of an expert system for fault detection, isolation, and recovery in the manned maneuvering unit\n", "abstract": " We explore issues in the specification, verification, and validation of AI-based software using a prototype Fault Detection, Isolation, and Recovery (FDII_) system for the Manned-Maneuvering Unit (MMU). We use the MMU FDIR system, which is implemented in CLIPS, as a vehicle for exploring issues in the semantics of CLIPS-style, rule-based languages, the verification of properties relating to safety and reliability, and the static and dynamic analysis of knowledge-based systems. Our analysis reveals errors and shortcomings in the MMU FDII_ system and raises a number of issues concerning software engineering in CLIPS. In the course of this work we came to realize that the MMU FDIR system does not conform to conventional definitions of AI software, despite the fact that it was intended and indeed presented as an AI system. We discuss this apparent disparity and related questions such as the role of AI techniques in space and aircraft operations and the suitability of rule-based languages such as CLIPS for critical applications.", "num_citations": "28\n", "authors": ["1785"]}
{"title": "Tutorial: Automated formal methods with PVS, SAL, and Yices\n", "abstract": " This full-day tutorial provides an introduction to automated formal methods using modern tools and methods. PVS is a comprehensive system for formal specification and analysis. It provides an attractive specification language based on higher order logic extended with dependent types and structural and predicate subtypes, and includes constructs for recursively defined abstract data types, recursive functions, inductive relations, and tabular specifications, as well as traditional logical formulas. Analysis capabilities include very strong typechecking (which can involve theorem proving), direct execution (at speeds within a factor of five of hand-crafted C), random testing, theorem proving, and symbolic model checking (with predicate abstraction). The PVS theorem prover provides powerful automation including rewriting and decision procedures for real and integer arithmetic, and is scriptable. Properties to be verified\u00a0\u2026", "num_citations": "27\n", "authors": ["1785"]}
{"title": "Mechanized formal methods: progress and prospects\n", "abstract": " In the decade of the 1990s, formal methods have progressed from an academic curiosity at best, and a target of ridicule at worst, to a point where the leading manufacturer of microprocessors has indicated that its next design will be formally verified. In this short paper, I sketch a plausible history of the developments that led to this transformation, present a snapshot of the current state of the practice, and indicate some promising directions for the future. Mindful of the title of this conference, I suggest how formal methods might have an impact on software similar to that which they have had on hardware.", "num_citations": "27\n", "authors": ["1785"]}
{"title": "Using PVS to prove some theorems of David Parnas\n", "abstract": " David Parnas [13] describes some theorems representative of those encountered in support of certification of software for the Darlington nuclear reactor. We describe the verification of these theorems using PVS.", "num_citations": "27\n", "authors": ["1785"]}
{"title": "Understanding and evaluating assurance cases\n", "abstract": " Assurance cases are a method for providing assurance for a system by giving an argument to justify a claim about the system, based on evidence about its design, development, and tested behavior.In comparison with assurance based on guidelines or standards (which essentially specify only the evidence to be produced), the chief novelty in assurance cases is provision of an explicit argument. In principle, this can allow assurance cases to be more finely tuned to the specific circumstances of the system, and more agile than guidelines in adapting to new techniques and applications. The first part of this report (Sections 1\u20134) provides an introduction to assurance cases. Although this material should be accessible to all those with an interest in these topics, the examples focus on software for airborne systems, traditionally assured using the DO-178C guidelines and its predecessors. A brief survey of some existing assurance cases is provided in Section 5. The second part (Section 6) considers the criteria, methods, and tools that may be used to evaluate whether an assurance case provides sufficient confidence that a particular system or service is fit for its intended use. An assurance case cannot provide unequivocal \u201cproof\u201d for its claim, so much of the discussion focuses on the interpretation of such less-than-definitive arguments, and on methods to counteract confirmation bias and other fallibilities in human reasoning.", "num_citations": "26\n", "authors": ["1785"]}
{"title": "Mechanized formal methods: where next?\n", "abstract": " An author who elects to use the phrase \u201cWhere Next?\u201d in a title clearly has predictive or prescriptive intent. In my case it is the latter: I cannot say how formal methods will develop in the next few years, still less whether or how they will be adopted in practice, but I do have views on how they should develop and what ought to encourage their adoption in practice, and this World Congress seems an excellent opportunity to impose these views on the public.", "num_citations": "24\n", "authors": ["1785"]}
{"title": "Trustworthy Self-Integrating Systems\n", "abstract": " Patients in intensive care often have a dozen or more medical devices and sensors attached to them. Each is a self-contained system that operates in ignorance of the others, and their integrated operation as a system of systems that delivers coherent therapy is performed by doctors and nurses. But we can easily imagine a scenario where the devices recognize each other and self-integrate (perhaps under the guidance of a master \u201ctherapy app\u201d) into a unified system. Similar scenarios can be (and are) envisaged for vehicles and roads, and for the devices and services in a home. These self-integrating systems have the potential for significant harm as well as benefit, so as they integrate they should adapt and configure themselves appropriately and should construct an \u201cassurance case\u201d for the utility and safety of the resulting system. Thus, trustworthy self-integration requires autonomous adaptation\u00a0\u2026", "num_citations": "23\n", "authors": ["1785"]}
{"title": "The bell and la padula security model\n", "abstract": " A precise description is given of the Bell and La Padula security model using modern notation. The development faithfully follows that of the original presentation [1, 2]. The paper is intended to provide a basis for more exact, formal, and scientific discussion of the model than has been the case heretofore.", "num_citations": "23\n", "authors": ["1785"]}
{"title": "Mechanized support for assurance case argumentation\n", "abstract": " An assurance case provides an argument that certain claims (usually concerning safety or other critical properties) are justified, based on given evidence concerning the context, design, and implementation of a system. An assurance case serves two purposes: reasoning and communication. For the first, the argument in the case should approach the standards of mathematical proof (though it may be grounded on premises\u2014i.e., evidence\u2014that are equivocal); for the second it must assist human stakeholders to grasp the essence of the case, to explore its details, and to challenge it. Because of the scale and complexity of assurance cases, both purposes benefit from mechanized assistance. We propose simple ways in which an assurance case, formalized in a mechanized verification system to support the first purpose, can be adapted to serve the second.", "num_citations": "22\n", "authors": ["1785"]}
{"title": "Reconfiguration and transient recovery in state machine architectures\n", "abstract": " We consider an architecture for ultra-dependable operation based on synchronized state machine replication, extended to provide transient recovery and reconfiguration in the presence of arbitrary faults. The architecture allows processors suspected of being faulty to be placed on \"probation.\" Processors in this status cannot disrupt other processors, but those that are nonfaulty or recovering from transient faults are able to remain synchronized with the other processors and with each other, can participate in interactively consistent exchange of data (i.e., Byzantine agreement), and can restore damaged state data by loading majority-voted copies from other processors. The processors that are not on probation are able to coordinate membership of their group and to take processors on and off probation. These properties are achieved even if all the processors on probation and some of the others exhibit Byzantine\u00a0\u2026", "num_citations": "22\n", "authors": ["1785"]}
{"title": "The Enhanced HDM system for specification and verification\n", "abstract": " The enhanced HDM system for specification and verification Page 1 \u2022 >' M 2 'ti H dC aL iq e, yr! q 0- N, y .O y ff 0 0 p M -a a o o .ap.. A-' >s v o a, ap)4 Ou 'a u 0 a :a .-o\u2022 y 0 u E. 4 . 0 a ,4., b w '0 M ~' NM ' v b0 J ov v ~ .n M v .9 aa, ff > N L 5 H tv. x O rJ o y v '0 .v' N v .. L \u2022, n a, OO E, .A _ a v a, _ V w .y .' t a .' q L oo O p0 v .0 d o .5 F .d \"g, uv; . _ N O p .~ v M a ~. .b rl 0. a ,yR' 44 -0 7 \u2022v 5 O a A 0 \" -aua, a O o Oa H eo H o eo av VI NO d j N 6 Mu C ro aav O a a . a 50 rM vo O v P ro H o a u 0 a a' y 9 x is a, q Oq O 4. H f .61 or. N u 0a.-a v q gay,oa ,y 5 ~+ M a HO y 0 L .>, M v N 0 a 'H M 0 to .ao . . a 0 \u201e A a N a 0 v M 0 anin ;,CJ n N u a 4,0 to 0 . O a O p 0 O qav \u2022 V Lv , ;A, vu .0 \u2022 -- a.- o o to An uu B u q L av ta g . y, -a g H H \u2022 . o -\u201d > H a C7 y . ~~ A _ pU a\u201e o A 0 .0 A ,2 M q 0 a .a . q o H o H \u2022 as awn (a(J g' c ; >a a y v a) Maui H -a ,n a a1 v ~_ 9. a v o .a -9 a . a. >A 0   N a 'a a q 7 A .- g 4- V n 0 M O v U a o 0 0 w Hr o H av 'OL q ate\u2022 0 N y0. \u2026", "num_citations": "22\n", "authors": ["1785"]}
{"title": "Formal verification of McMillan\u2019s compositional assume-guarantee rule\n", "abstract": " To illustrate some of the power and convenience of its specification language and theorem prover, we use the PVS formal verification system to verify the soundness of a proof rule for assume-guarantee reasoning due to Ken McMillan. i", "num_citations": "21\n", "authors": ["1785"]}
{"title": "Systematic formal verification for fault-tolerant time-triggered algorithms\n", "abstract": " Many critical real-time applications are implemented as time-triggered systems. We present a systematic way to derive a time-triggered implementation from a fault-tolerant algorithm speci ed as a functional program. It is relatively easy to formally and mechanically verify correctness and faulttolerance properties of algorithms expressed in this latter form. The functional program is next transformed into an untimed synchronous system, and then to a time-triggered implementation. The second step is independent of the algorithm concerned and we prove its correctness; the proof has also been formalized and mechanically checked with the PVS veri cation system. This approach provides a methodology that can ease the formal speci cation and assurance of critical fault-tolerant systems.", "num_citations": "21\n", "authors": ["1785"]}
{"title": "Specification, proof checking, and model checking for protocols and distributed systems with PVS\n", "abstract": " Formal methods contribute useful mental frameworks, notations, and systematic methods to the design, documentation, and analysis of computer systems But the primary bene t from speci cally formal methods is that they allow certain questions about a design to be answered by symbolic calculation (eg, by theorem proving or model checking) These symbolic calculations can be used for debugging and design exploration as well as post-hoc veri cation Comparable to the way computational uid dynamics is used in the design of airplanes and jet engines Especially valuable when behavior is complex, as with distributed, concurrent systems| ie, protocols", "num_citations": "21\n", "authors": ["1785"]}
{"title": "Model checking and other ways of automating formal methods\n", "abstract": " My interest is chie y in the second of these and, more particularly, in automation of the calculations concerned. 1 Automation provides repeatability and accuracy and| potentially, at least| speed and economy. The motivation and bene ts here are similar to those provided by mathematical modeling and automated calculation in other engineering elds: computational uid dynamics, for example, allows the aerodynamic properties of airplane designs to be thoroughly explored and analyzed prior to construction.There are many kinds of computer systems, and many di erent properties of those systems that are of interest. There are correspondingly many ways to model computer systems and to calculate their properties. At one end of the spectrum of di erent methods, we can build a simulation or rapid prototype of the system and", "num_citations": "21\n", "authors": ["1785"]}
{"title": "Formal specification and verification for critical systems: Tools, achievements, and prospects\n", "abstract": " Formal speci cation and veri cation use mathematical techniques to help document, specify, design, analyze, or certify computer software and hardware. Mathematically-based notation can provide speci cations that are precise and unambiguous and that can be checked mechanically for certain types of error. Formal veri cation uses theorem proving techniques to establish consistency between one level of formal speci cation and another. This paper describes some of the issues in the design and use of formal speci cation languages and veri cation systems, outlines some examples of the application of formal methods to critical systems, and identi es the bene ts that may be obtained from this technology.", "num_citations": "21\n", "authors": ["1785"]}
{"title": "Formal verification of AI software\n", "abstract": " The application of formal verification techniques to AI software, particularly expert systems, is investigated. Constraint satisfaction and model inversion are identified as two formal specification paradigms for different classes of expert systems. A formal definition of consistency is developed, and the notion of approximate semantics is introduced. Examples are given of how these ideas can be applied in both declarative and imperative forms.", "num_citations": "21\n", "authors": ["1785"]}
{"title": "Considerations in assuring safety of increasingly autonomous systems\n", "abstract": " Recent technological advances have accelerated the development and application of increasingly autonomous (IA) systems in civil and military aviation [34]. IA systems can provide automation of complex mission tasks\u2014ranging across reduced crew operations, air-traffic management, and unmanned, autonomous aircraft\u2014with most applications calling for collaboration and teaming among humans and IA agents. IA systems are expected to provide benefits in terms of safety, reliability, efficiency, affordability, and previously unattainable mission capability. There is also a potential for improving safety by removal of human errors.There are, however, several challenges in the safety assurance of these systems due to the highly adaptive and non-deterministic behavior of these systems, and vulnerabilities due to potential divergence of airplane state awareness between the IA system and humans. These systems must deal with external sensors and actuators, and they must respond in time commensurate with the activities of the system in its environment. One of the main challenges is that safety assurance, currently relying upon authority transfer from an autonomous function to a human to mitigate safety concerns, will need to address their mitigation by automation in a collaborative dynamic context. These challenges have a fundamental, multidimensional impact on the safety assurance methods, system architecture, and V&V capabilities to be employed. The goal of this report is to identify relevant issues to be addressed in these areas, the potential gaps in the current safety assurance techniques, and critical questions that would need to be\u00a0\u2026", "num_citations": "20\n", "authors": ["1785"]}
{"title": "Example of a complementary use of model checking and human performance simulation\n", "abstract": " Aircraft automation designers are faced with the challenge to develop and improve automation such that it is transparent to the pilots using it. To identify problems that may arise between pilots and automation, methods are needed that can uncover potential problems with automation early in the design process. In this paper, simulation and model checking are combined and their respective advantages leveraged to find problematic human-automation interaction using methods that would be available early in the design process. A particular problem of interest is automation surprises, which describe events when pilots are surprised by the actions of the automation. The Tarom flight 381 incident involving the former Airbus automatic speed protection logic, leading to an automation surprise, is used as a common case study. Results of this case study indicate that both methods identified the automation surprise found\u00a0\u2026", "num_citations": "20\n", "authors": ["1785"]}
{"title": "Automated formal methods enter the mainstream\n", "abstract": " This paper outlines the emergence of formal techniques, explaining why they were slow to take on an industrially acceptable form. The contemporary scene, in which formal techniques are increasingly packaged within tools usable by a wide variety of engineers, is reviewed, as are the promising prospects for the future.", "num_citations": "20\n", "authors": ["1785"]}
{"title": "TTA and PALS: Formally verified design patterns for distributed cyber-physical systems\n", "abstract": " Avionics systems in modern and next-generation airborne vehicles combine and integrate various real- time applications to efficiently share the physical resources on board. Many of these real-time applications also need to fulfill fault-tolerance requirements-i.e., the applications have to provide a sufficient level of service even in presence of failures-and this combination of real-time and fault- tolerance requirements elevates avionics to a class of cyber-physical systems of the highest complexity. Consequently, avionics design is challenging for avionics architects and application engineers alike. One way to manage complexity is a division of the overall problem into a hierarchical set of layers connected by well-defined interfaces. The avionics architect may then select the fundamental network architecture, like AFDX, TTP or TTEthernet, and hide their idiosyncrasies-in particular, those concerning the way in which\u00a0\u2026", "num_citations": "19\n", "authors": ["1785"]}
{"title": "Mechanizing formal methods: Opportunities and challenges\n", "abstract": " Mechanization makes it feasible to calculate properties of formally specified systems. This ability creates new opportunities for using formal methods as an exploratory tool in system design. Achieving enough efficiency to make this practical raises challenging problems in automated deduction. These challenges can be met only by approaches that integrate consideration of its mechanization into the design of a specification language.", "num_citations": "19\n", "authors": ["1785"]}
{"title": "On the interpretation of assurance case arguments\n", "abstract": " An assurance case provides a structured argument to establish a claim for a system based on evidence about the system and its environment. I propose a simple interpretation for the overall argument that uses epistemic methods for its evidential or leaf steps and logic for its reasoning or interior steps: evidential steps that cross some threshold of credibility are accepted as premises in a classical deductive interpretation of the reasoning steps. Thus, all uncertainty is located in the assessment of evidence. I argue for the utility of this interpretation.", "num_citations": "18\n", "authors": ["1785"]}
{"title": "Formal verification of transmission window timing for the time-triggered architecture\n", "abstract": " We formally verify the parameters on the timing of message windows in transmitters, receivers, and bus guardians for the Time-Triggered Architecture. i", "num_citations": "18\n", "authors": ["1785"]}
{"title": "Structural embeddings: Mechanization with method\n", "abstract": " The most powerful tools for analysis of formal specifications are general-purpose theorem provers and model checkers, but these tools provide scant methodological support. Conversely, those approaches that do provide a well-developed method generally have less powerful automation. It is natural, therefore, to try to combine the better-developed methods with the more powerful general-purpose tools. An obstacle is that the methods and the tools often employ very different logics. We argue that methods are separable from their logics and are largely concerned with the structure and organization of specifications. We propose a technique called structural embedding that allows the structural elements of a method to be supported by a general-purpose tool, while substituting the logic of the tool for that of the method. We have found this technique quite effective and we provide some examples of its\u00a0\u2026", "num_citations": "18\n", "authors": ["1785"]}
{"title": "Formal methods and their role in digital systems validation for airborne systems\n", "abstract": " This report is based on one prepared as a chapter for the FAA Digital Systems Validation Handbook (a guide to assist FAA certification specialists with advanced technology issues). Its purpose is to explain the use of formal methods in the specification and verification of software and hardware requirements, designs, and implementations; to identify the benefits, weaknesses, and difficulties in applying these methods to digital systems used in critical applications; and to suggest factors for consideration when formal methods are offered in support of certification. The presentation concentrates on the rationale for formal methods and on their contribution to assurance for critical applications within a context such as that provided by DO-178B (the guidelines for software used on board civil aircraft); it is intended as an introduction for those to whom these topics are new.", "num_citations": "18\n", "authors": ["1785"]}
{"title": "The security model of Enhanced HDM\n", "abstract": " The Enhanced HDM Specification and Verification System being developed at SRI International includes an \u201cMLS Checker\u201d that automatically verifies the security of a certain class of system specifications. This paper gives a brief and informal overview of the security model on which the MLS checker is based and discusses its application and its relationship to other security models and to the requirements of the DoD Trusted Computer System Evaluation Criteria.", "num_citations": "18\n", "authors": ["1785"]}
{"title": "Composing Safe Systems\n", "abstract": " Failures in component-based systems are generally due to unintended or incorrect interactions among the components. For safety-critical systems, we may attempt to eliminate unintended interactions, and to verify correctness of those that are intended. We describe the value of partitioning in eliminating unintended interactions, and of assumption synthesis in developing a robust foundation for verification. We show how model checking of very abstract designs can provide mechanized assistance in human-guided assumption synthesis.", "num_citations": "17\n", "authors": ["1785"]}
{"title": "Distributed secure systems: Then and now\n", "abstract": " The early 1980s saw the development of some rather sophisticated distributed systems. These were not merely networked file systems: rather, using remote procedure calls, hierarchical naming, and what would now be called middleware, they allowed a collection of systems to operate as a coherent whole. One such system in particular was developed at Newcastle that allowed pre-existing applications and (Unix) systems to be used, completely unchanged, as components of an apparently standard large (multiprocessor) Unix system. The distributed secure system (DSS) described in our 1983 paper proposed a new way to construct secure systems by exploiting the design freedom created by this form of distributed computing. The DSS separated the security concerns of policy enforcement from those due to resource sharing and used a variety of mechanisms (dedicated components, cryptography, periods\u00a0\u2026", "num_citations": "17\n", "authors": ["1785"]}
{"title": "Formal methods for dependable real-time systems\n", "abstract": " g.[36] John A. Stankovic and Krithi Ramamritham. What is predictability for real-time systems? Real-Time Systems, 2 (4): 247--254, November 1990.(Editorial).[37] Douglas A. Stuart. Implementing a verifier for real-time systems. In Real Time Systems Symposium, pages 62--71, Lake Buena Vista, FL, December 1990. IEEE Computer Society.[14] Farnam Jahanian and Aloysius Ka-Lau Mok. Safety analysis of timing properties in real-time systems. IEEE Transactions on Software Engineering, SE-12 (9): 890--904, September 1986.[15] Ron Koymans. Specifying real-time properties with metric temporal logic. Real-Time Systems, 2 (4): 255--299, November 1990.[16] L. Lamport. Sometime is sometimes not never. In 10th ACM Symposium on Principles of Programming Languages, pages 174--185, Austin, TX, January 1983.[17] L. Lamport and PM Melliar-Smith. Synchronizing clocks in the presence of faults. Journ", "num_citations": "17\n", "authors": ["1785"]}
{"title": "A fault-masking and transient-recovery model for digital flight-control systems\n", "abstract": " We present a formal model for fault-masking and transient-recovery among the replicated computers of digital flight-control systems. We establish conditions under which majority voting causes the same commands to be sent to the actuators as those that would be sent by a single computer that suffers no failures. The model and its analysis have been subjected to formal specification and mechanically checked verification using the EHDM system.", "num_citations": "16\n", "authors": ["1785"]}
{"title": "Validation and Testing of Knowledge-Based Systems How bad can it get?\n", "abstract": " My subtitle is not intended as a slighting reference to the current state of the art for validation and veri cation of knowledge-based systems, but as a reminder that for many systems the important question concerning deployment is not\\how well does it work?\" but\\how badly can it fail?\" This is most obviously the case in safetycritical systems, but it can apply to any system where the consequences of certain types of failure may be incommensurate with the bene ts of normal operation. For conventional software, it is a topic of active debate whether the techniques that are most e ective for ensuring that a system\\works well\" are also the most e ective for showing that it\\cannot go badly wrong.\" At the core of this debate is the question whether safety should be distinguished from reliability. Veri cation and validation for knowledge-based systems has been almost exclusively concerned with showing that these systems can work well. But if knowledgebased systems are to become accepted as components of larger systems that perform critical functions, it will also be necessary to consider the question of how badly they can go wrong. In this regard, it will be useful to consider relevant experience with conventional software and to introduce some of the terms and concepts from\\dependable systems,\"\\software reliability,\" and\\software safety.\"", "num_citations": "16\n", "authors": ["1785"]}
{"title": "Example of a Complementary use of Model Checking and Agent-based Simulation\n", "abstract": " To identify problems that may arise between pilots and automation, methods are needed that can uncover potential problems with automation early in the design process. Such potential problems include automation surprises, which describe events when pilots are surprised by the actions of the automation. In this work, agent-based, hybrid time simulation and model checking are combined and their respective advantages leveraged in an original manner to find problematic human-automation interaction (HAI) early in the design process. The Tarom 381 incident involving the former Airbus automatic speed protection logic, leading to an automation surprise, was used as a common case study for both methodology validation and further analysis. Results of this case study show why model checking alone has difficulty analyzing such systems and how the incorporation of simulation can be used in a complementary\u00a0\u2026", "num_citations": "15\n", "authors": ["1785"]}
{"title": "Modeling and Verification of Parallel Processes: 4th Summer School, MOVEP 2000, Nantes, France, June 19-23, 2000. Revised Tutorial Lectures\n", "abstract": " Daily life relies more and more on safety critical systems, eg in areas such as power plant control, traffic management, flight control, and many more. MOVEP is a school devoted to the broad subject of modeling and verifying software and hardware systems. This volume contains tutorials and annotated bibliographies covering the main subjects addressed at MOVEP 2000. The four tutorials deal with Model Checking, Theorem Proving, Composition and Abstraction Techniques, and Timed Systems. Three research papers give detailed views of High-Level Message Sequence Charts, Industrial Applications of Model Checking, and the use of Formal Methods in Security. Finally, four annotated bibliographies give an overview of Infinite State Space Systems, Testing Transition Systems, Fault-Model-Driven Test Derivation, and Mobile Processes.", "num_citations": "15\n", "authors": ["1785"]}
{"title": "Calculating with requirements\n", "abstract": " The author considers how many issues in requirements engineering can be explored and analyzed using automated formal methods. At present, the tools supporting these analyses are not ideal: considerable knowledge and experience are required to select the most appropriate tool for a given task, to formulate the problem in a suitable manner, and to coax the tool into divulging a useful result. Application of specialized but pragmatically effective theorem proving techniques, and of model checking and related methods, has made it possible to subject formal requirements specifications to several kinds of automated analysis.", "num_citations": "15\n", "authors": ["1785"]}
{"title": "Model-based reconfiguration: Diagnosis and recovery\n", "abstract": " We extend Reiter's general theory of model-based diagnosis to a theory of fault detection, identification, and reconfiguration (FDIR). The generality of Reiter's theory readily supports an extension in which the problem of reconfiguration is viewed as a close analog of the problem of diagnosis. Using a reconfiguration predicate 'rcfg' analogous to the abnormality predicate 'ab,' we derive a strategy for reconfiguration by transforming the corresponding strategy for diagnosis. There are two obvious benefits of this approach: algorithms for diagnosis can be exploited as algorithms for reconfiguration and we have a theoretical framework for an integrated approach to FDIR. As a first step toward realizing these benefits we show that a class of diagnosis engines can be used for reconfiguration and we discuss algorithms for integrated FDIR. We argue that integrating recovery and diagnosis is an essential next step if this technology is to be useful for practical applications.", "num_citations": "14\n", "authors": ["1785"]}
{"title": "Systematic formal verification of interpreters\n", "abstract": " Formal methods have gained acceptance in the hardware field through a pragmatic approach that has succeeded in providing systematic, scalable, highly automated, and cost effective treatments for certain stereotypical problems of practical importance. By identifying stereotypical problems, the effort required to develop effective formal methods has been amortized over many applications. We suggest that formal methods can achieve similar industrial success in selected software applications by following the same principles. As an illustration, we examine approaches to the stereotypical problem of interpreter correctness in the presence of timing differences between the specification and implementation interpreters. In hardware, this corresponds to the problem of verifying microprogrammed, pipelined, or superscalar processors, but it has wider applications to any system-hardware or software-that can be\u00a0\u2026", "num_citations": "13\n", "authors": ["1785"]}
{"title": "Formal methods and critical systems in the real world\n", "abstract": " Programmable computers make it possible to construct systems whose behavior is unimaginably complex. These systems are built because their complexity is believed to confer operational benefits, but this same complexity can harbor unexpected and catastrophic failure modes. The source of these failures can often be traced to software faults\u2014for example, a software bug in the control system of the Therac-25 radiation therapy machine was responsible for the death of three patients and serious injury to several others [Jac89]. Software doesn\u2019t wear out: all software-induced failures are due to design faults, and design faults are largely attributable to the complexity of the designs concerned\u2014complexity that exceeds the intellectual grasp of its own creators. The only way to reduce or eliminate software design faults is to bring the complexity of the software into line with our ability to master that complexity. This might mean choosing not to build certain types of system (such as flight-critical computer control systems for passenger aircraft), and it should mean enhancing the intellectual tools available to software designers. Engineers in established fields use applied mathematics to predict the behavior and properties of their designs with great accuracy. Software engineers, despite the fact that their creations exhibit far more complexity than physical systems, do not generally do this and the practice of the discipline is still at the pre-scientific or craft stage. Unlike most physical systems, the behavior of software admits discontinuities and so interpolation between known points is unreliable: formal logical analysis is needed to address the discrete, formal\u00a0\u2026", "num_citations": "13\n", "authors": ["1785"]}
{"title": "PVS bibliography\n", "abstract": " This bibliography lists most of the publications that I am aware of concerning the development and application of the PVS veri cation system and its predecessors. The les used to create this report, including the BibTEX bibliography, are available at http://www. csl. sri. com/pvs-bib. html. PVS users are encouraged to use the BibTEX entries from these les, which are as accurate, complete, and up to date as I can make them.I have limited this list to publications that should be available in a university or research library. For this reason, I have omitted citations to conference proceedings that were not issued by a technical society or commercial publisher, unless they are on the Web; similarly, technical reports are cited only when they are widely distributed (eg, NASA), or where there is no other source and they are are on the Web (or otherwise freely available, if they predate the Web). Many of the journal and conference papers listed are supported by technical reports that provide much more detail; I am willing to add these if there is demand.", "num_citations": "12\n", "authors": ["1785"]}
{"title": "SAL tutorial: Analyzing the fault-tolerant algorithm OM (1)\n", "abstract": " The resources of SAL allow many kinds of systems to be modeled and analyzed. However, it requires skill and experience to exploit the capabilities of SAL to the best effect in any given problem domain. This tutorial provides an introduction to the use of SAL in modeling and analyzing fault-tolerant systems. The example considered here is a simple variant on the classical one-round Oral Messages algorithm OM (1) for Byzantine agreement and will be familiar to many computer scientists. The SAL model developed here is available for download, so that users can repeat the analyses described, and exercises are suggested for additional experiments. i", "num_citations": "11\n", "authors": ["1785"]}
{"title": "The needham-schroeder protocol in sal\n", "abstract": " The Needham-Schroeder authentication protocol is specified in SAL and its model checker is used to detect the flaw discovered by Gavin Lowe. The SAL simulator is used to further explore the model of the protocol. This provides a simple illustration in the use of SAL for this domain. i", "num_citations": "11\n", "authors": ["1785"]}
{"title": "A separation kernel formal security policy in PVS\n", "abstract": " Greve, Wilding, and Vanfleet [GWV03] present an ACL2 formalization of a security policy for a separation kernel, and validate its utility by using it to support the verification of a simple application. This note reworks their development in PVS and uses the exercise to offer some comparisonns between PVS and ACL2. i", "num_citations": "10\n", "authors": ["1785"]}
{"title": "Combining system properties: A cautionary example and formal examination\n", "abstract": " This report presents a simple example to demonstrate that the composition of a\\fault-tolerant\" service and a\\secure\" service does not necessarily provide a secure and fault-tolerant service.The example is originally due to Peleska, who argues that to achieve the combined service, it is necessary to strengthen one of the individual services to address both concerns. In contrast, I argue that the individual services should be\\deconstructed\" into smaller and weaker components that can be reassembled in di erent ways, and I show that the combined service can be achieved by composing the fault-tolerant service with a weaker version of the secure service. The report also provides an introduction to the use of mechanized formal state exploration methods (speci cally, the Mur system from Stanford) for the purpose of examining and debugging protocols.", "num_citations": "10\n", "authors": ["1785"]}
{"title": "Ubiquitous abstraction: A new approach for mechanized formal verification\n", "abstract": " Ubiquitous abstraction involves the construction of many different abstracted system descriptions at many different points in an analysis, and for several different purposes. This method has great promise as a way to ease difficulties and increase productivity and automation in the formal analysis of concurrent systems. The approach also provides a new way to combine different tools, such as theorem provers and model checkers, though full exploitation of this opportunity requires modification to the tools so that they can exchange symbolic values (e.g. the reachable state set, or a counterexample) rather than merely report the success or failure of their own local analysis. Some of the capabilities described are already integrated in a system called InVeSt (S. Bensalem et al., 1998) and initial experiments with this and other prototypes developed as part of our \"Symbolic Analysis Laboratory\" (SAL) are quite promising.", "num_citations": "9\n", "authors": ["1785"]}
{"title": "Networks are systems\n", "abstract": " The DoD Computer Security Center\u2019s Invitational Workshop on Computer Network Security is concerned with extension of the Criteria in the Center\u2019s \u201cOrange Book\u201d[5] to cover the case of computer networks. My assignment was to prepare a position paper on the role of (Formal) Verification in ensuring network security\u2014but before I could address the issue of verification, I found I had first to consider the notion of \u201cnetwork\u201d: just what is a network, and what does it mean for a network to be \u201csecure\u201d?", "num_citations": "9\n", "authors": ["1785"]}
{"title": "From refutation to verification\n", "abstract": " Model checking has won some industrial acceptance in debugging designs. Theorem proving and formal verification are less popular. An approach built around automated abstractions could integrate theorem proving with model checking in an acceptable way and provide a bridge between refutation and verification.", "num_citations": "8\n", "authors": ["1785"]}
{"title": "Subtypes for specifications\n", "abstract": " Specification languages are best used in environments that provide effective theorem proving. Having such support available, it is feasible to contemplate that typechecking can use the services of the theorem prover. This allows interesting extensions to the type systems provided for specification languages. I describe one such extension called \u201c\u2018predicate subtyping\u201d and illustrate its utility as mechanized in PVS.", "num_citations": "8\n", "authors": ["1785"]}
{"title": "What Use is Verified Software?\n", "abstract": " The world at large cares little for verified software; what it cares about are trustworthy and cost-effective systems that do their jobs well. We examine the value of verified software and of verification technology in the systems context from two perspectives, one analytic, the other synthetic. We propose some research opportunities that could enhance the contribution of the verified software initiative to the practices of systems engineering and assurance.", "num_citations": "7\n", "authors": ["1785"]}
{"title": "Assurance 2.0: A Manifesto\n", "abstract": " System assurance is confronted by significant challenges. Some of these are new, for example, autonomous systems with major functions driven by machine learning and AI, and ultra-rapid system development, while others are the familiar, persistent issues of the need for efficient, effective and timely assurance. Traditional assurance is seen as a brake on innovation and often costly and time consuming. We therefore propose a modernized framework, Assurance 2.0, as an enabler that supports innovation and continuous incremental assurance. Perhaps unexpectedly, it does so by making assurance more rigorous, with increased focus on the reasoning and evidence employed, and explicit identification of defeaters and counterevidence.", "num_citations": "6\n", "authors": ["1785"]}
{"title": "A Mechanically Assisted Examination of Begging the Question in Anselm's Ontological Argument\n", "abstract": " I use mechanized verification to examine several first-and higher-order formalizations of Anselm\u2019s Ontological Argument against the charge of begging the question. I propose three different criteria for a premise to beg the question in fully formal proofs and find that one or another applies to all the formalizations examined. My purpose is to demonstrate that mechanized verification provides an effective and reliable technique to perform these analyses; readers may decide whether the forms of question begging so identified affect their interest in the Argument or its various formalizations.", "num_citations": "6\n", "authors": ["1785"]}
{"title": "How Do We Certify For The Unexpected?\n", "abstract": " By their very nature, loss of control accidents are unanticipated and rare, and their precursors are rare also. Onboard systems to detect and mitigate these precursors must work\u2014and work correctly\u2014when required but must not introduce new malfunctions or unintended functions. How can we provide assurance that software invoked in such rare and unanticipated circumstances is fit for certification? We argue that software systems such as these are but an extreme example of general trends that undermine much of the standards-based approach to software assurance used in aircraft certification. These trends include component-based software, complex integration, continuous modification, and load-and run-time adaptation. We propose that safety cases based on explicit goals, evidence, and argument provide a firmer foundation for assurance, and a framework within which it is possible to address the rare and the\u00a0\u2026", "num_citations": "6\n", "authors": ["1785"]}
{"title": "Formal verification of hybrid Byzantine agreement under link faults\n", "abstract": " We describe an extended fault model for Byzantine Agreement due to Schmid and Weiss. The new fault model extends the previous \u201chybrid\u201d fault model of Thambidurai and Park by the addition of omission-faulty nodes, and by the introduction of link faults. We formally verify the Hybrid Oral Messages Algorithm (OMH) under this new fault model. Our formal verification improves the analysis of Schmid and Weiss a little by increasing the independence of node and link faults, and by making explicit the exact assumptions required in final rounds and in the case OMH (0). We also refute a conjecture of Schmid and Weiss that the analysis can be extended to omission-faulty receivers and introduce a symmetric version of the algorithm that does handle these cases correctly. i", "num_citations": "6\n", "authors": ["1785"]}
{"title": "Dependable computing for critical applications 7\n", "abstract": " Annotation Presenting all 20 of the conferences talks, covers assessing and coping with commercial off-the-shelf components, formal methods, distributed systems, time-triggered architecture, fault tolerance and safety, models of partitioning for integrated modular avionics, dependability evaluation, and probabilistic guarantees. A summary is also provided for a panel on certifying and assessing critical systems. Among the specific topics are building fault-tolerant hardware clocks from commercial components, improving the performance of atomic broadcast protocols using the newsmonger technique, the experimentally validating high-speed systems using physical fault injection, and evaluating dependability using a multi-criteria decision analysis procedure. No mention is made of where or when the conference was held. There is no subject index. Annotation copyrighted by Book News, Inc., Portland, OR.", "num_citations": "6\n", "authors": ["1785"]}
{"title": "Design choices in specification languages and verification systems\n", "abstract": " Design, Choices in, Specification, Languages and, Verification, Systems, John, Rushby, Computer, Science, Laboratory, SRI, International, Menlo, Park,, CA, 94025, USA, Abstract, We, describe, some, of, the, design choices, that, should, be, considered, in, the, development, and, application, of, spec-, ification, languages, and, verification, systems., A, prin-, cipal, issue, is, the, need, to, reconcile, the, desire, for, ex-, pressiveness, in, the, specification language, with the, abil-, ity, to, provide effective, mechanical support., We, argue, that, this, reconciliation, is, assisted, by, a, novel, approach, to, specification, language, design, that, requires, theorem, proving, to, be, used, during typechecking., A, second, key, requirement, is, for, the theorem prover, to, be, specialized, towards, the, needs, of, verification., This, means, that, the theorem prover, must, assist, in, the, rapid, identification, of, the, sources, of, errors, in, incorrect\u00a0\u2026", "num_citations": "6\n", "authors": ["1785"]}
{"title": "Assurance and Assurance Cases.\n", "abstract": " Assurance provides confidence that a system will work as required and not cause harm. Confidence is based on justified beliefs about the system and its environment, and justification can be developed and documented as an assurance case comprised of a structured argument grounded on evidence. For justification to be compelling, the argument must be indefeasible, meaning that we have so thoroughly considered everything that can go wrong (ie, hazards to the system and defeaters to the argument) that there is no new information that could change our assessment.", "num_citations": "5\n", "authors": ["1785"]}
{"title": "Compositional security evaluation: The MILS approach\n", "abstract": " Compositional Security Evaluation: The MILS approach Page 1 Compositional Security Evaluation: The MILS approach John Rushby and Rance DeLong\u2020 Computer Science Laboratory SRI International Menlo Park CA USA \u2020Primary affiliation: LynuxWorks John Rushby, Rance DeLong, SRI Compositional Security Evaluation: 1 Page 2 Systems, Components, and Security \u2022 Security is a system property \u2022 But there is a compelling case to establish a marketplace for security-relevant components \u25e6 Secure file systems, communications subsystems, operating system kernels \u25e6 Filters, downgraders, authentication services \u2022 Want the security of these components to be evaluated \u2022 In such a way that security evaluation for a system built on these is largely based on prior evaluations of the components \u2022 This is an example of compositional assurance John Rushby, Rance DeLong, SRI Compositional Security Evaluation: 2 \u2026", "num_citations": "5\n", "authors": ["1785"]}
{"title": "Mechanized analysis of Anselm\u2019s modal Ontological Argument\n", "abstract": " We use a mechanized verification system, PVS, to examine the argument from Anselm\u2019s Proslogion Chapter III, the so-called \u201cModal Ontological Argument.\u201d We consider several published formalizations for the argument and show they are all essentially similar. Furthermore, we show that the argument is trivial once the modal axioms are taken into account. This work is an illustration of Computational Philiosophy and, in addition, shows how these methods can help detect and rectify errors in modal reasoning.", "num_citations": "4\n", "authors": ["1785"]}
{"title": "The indefeasibility criterion for assurance cases\n", "abstract": " Ideally, assurance enables us to know that our system is safe or possesses other attributes we care about. But full knowledge requires omniscience, and the best we humans can achieve is well-justified belief. So what justification should be considered adequate for a belief in safety? We adopt a criterion from epistemology and argue that assurance should be \u201cindefeasible,\u201d meaning that we must be so sure that all doubts and objections have been attended to that there is no (or, more realistically, we cannot imagine any) new information that would cause us to change our evaluation. We explore application of this criterion to the interpretation and evaluation of assurance cases and derive a strict but practical characterization for a sound assurance case.", "num_citations": "4\n", "authors": ["1785"]}
{"title": "Formal verification of Marzullo\u2019s sensor fusion interval\n", "abstract": " We examine the problem of selecting a best value from a collection of sensor readings, and diagnosing faulty readings in such a collection. We focus on sensor interfaces that return a range of values and describe the \u201cfusion functions\u201d Pf, n (S) of Marzullo and Ff n (S) of Schmid and Schossmaier. We use PVS formally to prove the soundness of Pf, n (S)(ie, it always contains the correct value), from which soundness of Ff n (S) also follows. Ff n (S) is generally to be preferred to Pf, n (S) because it satisfies a \u201cLipschitz Condition\u201d(small changes in sensor readings produce small changes in its output), and is optimal among all such functions. i", "num_citations": "4\n", "authors": ["1785"]}
{"title": "Industrial Practice\n", "abstract": " It is extraordinary that formal methods cause such fierce debate. Some proponents seem committed with an almost religious fervor; some opponents seem hostile beyond all reason. As far as I know, no issue in software engineering causes as much passion, unless it is the use of the goto statement.One reason for this polarization may be that the two sides are arguing from completely different premises. Perhaps the argument has been between those who say that formal methods are essential because they are the only way to gain assurance and those who say formal methods are impossible because they are too expensive. No amount of argument will resolve that difference unless the two sides start to recognize each other's objectives.", "num_citations": "4\n", "authors": ["1785"]}
{"title": "PVS Embeddings of Propositional and Quantified Modal Logic\n", "abstract": " Modal logics allow reasoning about various modes of truth: for example, what it means for something to be possibly true, or to know that something is true as opposed to merely believing it. This report describes embeddings of propositional and quantified modal logic in the PVS verification system. The resources of PVS allow this to be done in an attractive way that supports much of the standard syntax of modal logic, while providing effective automation.", "num_citations": "3\n", "authors": ["1785"]}
{"title": "On the (f) utility of untrusted data sanitization\n", "abstract": " Data sanitization has been studied in the context of architectures for high assurance systems, language-based information flow controls, and privacy-preserving data publication. A range of sanitization strategies has been developed to address the wide variety of data content and contexts that arise in practice. It is therefore tempting to separate the complex downgrading operations into untrusted data sanitizers while leaving the verification of security policy to simpler trusted guards that mediate information flow between different sensitivity levels. We argue that this can be a false economy and may result in more restrictive information flow than is necessary. We also observe that the guarantees provided by language-based declassification algorithms do not hold without exacting requirements for the runtime environment, and that the satisfaction of these requirements is the precise goal of MILS architectures, making\u00a0\u2026", "num_citations": "3\n", "authors": ["1785"]}
{"title": "Formally Verified Hardware Encapsulation Mechanism for Security, Integrity, and Safety\n", "abstract": " Safety-and security-critical systems both require encapsulation of code and data belonging to different applications and sensitivity levels. It must be impossible for a fault or Trojan Horse in one application to affect the operation or real-time performance of another, or for information of one sensitivity level to contaminate that of another. Encapsulation is achieved by the combination of software in an operating system kernel managing hardware protection mechanisms. The critical nature of the applications concerned means that extremely high assurance is required for the correctness of the encapsulation mechanisms. A systematic approach is developed for the formal specification and verification of these encapsulation properties and mechanisms, addressing the interaction between a processor, custom protection hardware, and the kernel software managing them it encompasses both safety and security concerns and may be adjusted for different classes of systems. It is validated by mechanically checked verification. This validation provides a formal guarantee--from kernel interface down through hardware--of both spatial memory and temporal time-slicing encapsulation for the processor.Descriptors:", "num_citations": "3\n", "authors": ["1785"]}
{"title": "LR (k) sparse-parsers and their optimisation\n", "abstract": " A method of syntactic analysis is developed which . . is believed to surpass all known competitors in all major respects. I The method is based upon that associated with the LR(k) grammars but is faster because it bypasses all reduction steps concerned with 'chain' productions. These are freely selected productions which are considered semantically irrelevant and whose right parts consist of just a single symbol. The parses produced by the method are 'sparse' in that they contain no references to chain productions - they are termed 'chain-free' parses. The CFLR(k) grammars are introduced as the largest class which can be -Chain-F-ree parsed from -Le-ft to Right while looking ~ symbols ahead of the current point of the parse. The properties of these grammars are examined in detail and their relationship to the conventional LR(k) grammars is explored. Techniques are presented for testing grammars for the CFLR(k) property and for constructing chain-free parsers for those grammars possessing the property. Methods are also presented for. converting ordinary LR(k) parsers into chain-free parsers. CFLR(k) parsers are more widely applicable than their LR(k) counterparts, are faster 'and provide the same excellent detection of syntactic errors. Unfortunately they also tend to be rather larger. A 'simple optimization is presented which completely'overcomes this single disadvantage without sacrificing any of the advantages of the method. These theoretical techniques are adapted to provide truly practical chain-free parsers based on the conventional SLR and,LALR parsing methods. Detailed consideration is given to use of 'default reductions' and\u00a0\u2026", "num_citations": "3\n", "authors": ["1785"]}
{"title": "From DSS to MILS\n", "abstract": " I outline the principal ideas of the Distributed Secure System (DSS) on which Brian Randell and I collaborated in the early 1980s, its modern manifestation as MILS, and continuing research challenges posed by these architectures.", "num_citations": "2\n", "authors": ["1785"]}
{"title": "FME'96 Tutorial: An Introduction to Some Advanced Capabilities of PVS\n", "abstract": " The speci cation language of PVS builds on classical higher-order logic and the simple theory of types. PVS extends simple type theory with dependent types and predicate subtypes, structures speci cations into parameterized theories, and adds a construction for freely generated abstract data types. These extensions provide a surprisingly rich set of capabilities, and the purpose of this tutorial is to introduce some of them to you. The strategy mechanism of the PVS theorem prover likewise allows rather powerful capabilities to be built on the basic theorem proving resources of propositional calculus, quanti er reasoning, decision procedures, and rewriting; some of these capabilities are also described here. This tutorial is intended to supplement, not replace, the standard PVS tutorials 6, 38], and the other examples available from our web site at http://www. csl. sri. com/pvs/examples/. In particular, you are recommended to work through the various\\phone book\" examples at the beginning of the WIFT Tutorial 6] and the\\airline reservation,\"\\noninterference,\" and\\gcd\" examples in the\\less elementary\" Tutorial 38] before continuing with this one (the speci cation and proof les can be downloaded from the URL mentioned above, and from those given with the references).", "num_citations": "2\n", "authors": ["1785"]}
{"title": "The Future of Formal Methods in Industry\n", "abstract": " The Future of Formal Methods in Industry | Proceedings of the 9th International Conference of Z Usres on The Z Formal Specification Notation ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsZUM '95The Future of Formal Methods in Industry ARTICLE The Future of Formal Methods in Industry Share on Authors: Anthony Hall profile image Anthony Hall View Profile , David Lorge Parnas profile image David Lorge Parnas View Profile , Nico Plat profile image Nico Plat View Profile , John M Rushby profile image John M. Rushby View Profile , Chris T Sennett profile image Chris T. Sennett View Profile Authors Info & Affiliations \u2026", "num_citations": "2\n", "authors": ["1785"]}
{"title": "A mechanically assisted examination of vacuity and question begging in Anselm\u2019s ontological argument\n", "abstract": " I use mechanized verification to examine several first- and higher-order formalizations of Anselm\u2019s Ontological Argument against the charge of begging the question. I propose three different but related criteria for a premise to beg the question in fully formal proofs and find that one or another applies to all the formalizations examined. I also show that all these formalizations entail variants that are vacuous, in the sense that they apply no interpretation to \u201cthan which there is no greater\u201d and are therefore vulnerable to Gaunilo\u2019s refutation. My purpose is to demonstrate that mechanized verification provides an effective and reliable technique to perform these analyses; readers may decide whether the forms of question begging and vacuity so identified affect their interest in the Argument or its various formalizations.", "num_citations": "1\n", "authors": ["1785"]}
{"title": "Automated Integration of Potentially Hazardous Open Systems\n", "abstract": " \u25e6 Eg, if the app works by blocking button presses when an approaching overdose is indicated, then loss of communication could remove the safety function\u25e6 If, on the other hand, it must approve each button press, then loss of communication may affect pain relief but not safety", "num_citations": "1\n", "authors": ["1785"]}
{"title": "Toward a multi-method approach to formalizing human-automation interaction and human-human communications\n", "abstract": " Systems including human and automated agents require methods for verifying and validating that the roles and responsibilities potentially assignable to the human and automated agents do not lead to unsafe situations. Such analyses must consider the conditions that could impact system safety including human behavior and operational procedures, methods of collaboration and regulations. The use of task behavior as part of a larger, formal system model is potentially useful for analyzing such problems because it allows the ramifications of different human behaviors to be verified in relation to other aspects of the system. A component of task behavior largely overlooked to date is the role of human-human interaction, particularly human-human communication in complex humancomputer systems. We are developing a multi-method approach based on extending the Enhanced Operator Function Model language to address human agent communications (EOFMC). This approach includes analyses via theorem proving and model checking linked through the EOFMC top level XML description. Herein, we consider a continuous descent arrival (CDA) scenario including the roles of the pilots monitoring and flying. We then show how we can use the semantics of this extended language to verify properties involving collaborations among these agents, and associated computer systems, necessary to guarantee safety in the CDA context.", "num_citations": "1\n", "authors": ["1785"]}
{"title": "Threatened by a Great Opportunity: Disruptive Innovation in Formal Verification\n", "abstract": " \u2022 Brown and Pike recently did it with sal-inf-bmc\u25e6 Used timeout automata to model timed aspects\u25e6 Statement of theorem discovered systematically using disjunctive invariants (7 disjuncts)\u25e6 Three lemmas proved automatically with 1-induction,\u25e6 Theorem proved automatically using 5-induction\u25e6 Verification takes seconds to check\u25e6 Demo: sal-inf-bmc-v 3-d 5-i-l l0-l l1-l l2 biphase t0\u2022 Adapted verification to 8-N-1 protocol (used in UARTs)\u25e6 Automated proofs more reusable than step-by-step ones\u25e6 Additional lemma proved with 13-induction\u25e6 Theorem proved with 3-induction (7 disjuncts)\u25e6 Revealed a bug in published application note", "num_citations": "1\n", "authors": ["1785"]}
{"title": "Pvs\n", "abstract": " In particular the definition of sqrt below comes from this library. Answers by John Rushby.                            Statement                                         NOT Rational?(sqrt(2))", "num_citations": "1\n", "authors": ["1785"]}
{"title": "Security, safety and partitioning\n", "abstract": " Important aspects of both security and safety are related to process encapsulation and controlled flow of information through known interfaces. Partitioning refers to architectural mechanisms that enforce these attributes. In this paper, we examine formal characterizations of partitioning.", "num_citations": "1\n", "authors": ["1785"]}
{"title": "A Scalable, Reconfigurable, and Dependable Time-Triggered Architecture\n", "abstract": " The research performed in this project began with a study and comparison of time-triggered bus architectures for critical systems and with development of flexible methods for scheduling time-triggered systems. Later work focused on automated synthesis and refutation debugging and developed a new method for bounded model checking over infinite domains based on integration of efficient decision procedures with SAT solving. The method was then extended to automated verification. Implementations of the method are available from SRI in our tools ICS and SAL. The approach pioneered in this project for combining decision procedures with a SAT solver has now become the dominant one for bounded model checking and automated verification.Descriptors:", "num_citations": "1\n", "authors": ["1785"]}
{"title": "From Reviews to Analysis: Challenge and Opportunity Converge\n", "abstract": " The emergence of complex safety-critical systems such as integrated modular avionics challenges the limits of traditional process-based assurance methods based on what DO-178B calls \u201creviews.\u201d The problem is that interaction between subsystems produces too many possible behaviors. Assurance for these cases needs support from what DO-178B calls \u201canalyses\u201d; in particular, tool-supported calculation of (properties of) all possible system behaviors. These calculations can be performed by automated formal methods, but have been infeasible in the past due to inadequate tools and absence of suitable descriptions of the products. Recent advances have produced much more capable tools, and the emergence of \u201cmodel based\u201d approaches to system development has made suitable product descriptions available.", "num_citations": "1\n", "authors": ["1785"]}
{"title": "PARTITIONING IN AVIONICS: REQUIREMENTS, MECHANISMS, AND ASSURANCE.\n", "abstract": " VII, 62 P.: ILL.; INCLUDES BIBLIOGRAPHICAL REFERENCES (P. 53-62). UNITED STATES. UNITED STATES. LANGLEY RESEARCH CENTER. WILLIAM J. HUGHES TECHNICAL CENTER (US) SRI INTERNATIONAL. COVER TITLE.\" MARCH 2000.\" FINAL REPORT.", "num_citations": "1\n", "authors": ["1785"]}
{"title": "Enhancing the utility of formal methods\n", "abstract": " Formal methods contribute useful mental frameworks, notations, and systematic methods to the design and documentation of computer systems, but the primary benefit from specifically formal methods is that they allow certain questions to be settled by symbolic calculation--that is, by formal deduction and related methods such as model checking. In short, it's the tools that make formal methods useful.Formal methods have started to attract mainstream interest in some fields (notably hardware) because their use has become cost-effective: they can now deliver valuable results at an acceptable price. This is due to several factors: a more pragmatic approach (applying formal methods to only part of a design, and for debugging as much as for correctness), systematic application (so that treatments suitable for specific topics--such as cache coherence, or pipeline correctness--have become well developed), and technical\u00a0\u2026", "num_citations": "1\n", "authors": ["1785"]}
{"title": "Natarajan Shankar, Friedrich von Henke Abstract\u2014PVS is the most recent in a series of verifi-cation systems developed at SRI. Its design was strongly influenced, and later\u00a0\u2026\n", "abstract": " PVS is the most recent in a series of verification systems developed at SRI. Its design was strongly influenced, and later refined, by our experiences in developing formal specifications and mechanically checked verifications for the fault-tolerant architecture, algorithms, and implementations of a model\" reliable computing platform\"(RCP) for life-critical digital flight-control applications, and by a collaborative project to formally verify the design of a commercial avionics processor called AAMP5. Several of the formal specifications and verifications performed in support of RCP and AAMP5 are individually of considerable complexity and difficulty. But in order to contribute to the overall goal, it has often been necessary to modify completed verifications to accommodate changed assumptions or requirements, and people other than the original developer have often needed to understand, review, build on, modify, or extract\u00a0\u2026", "num_citations": "1\n", "authors": ["1785"]}