{"title": "Performance analysis of software reliability growth models with testing-effort and change-point\n", "abstract": " In this paper, a scheme for constructing software reliability growth model based on Non-Homogeneous Poisson Process is proposed. The main focus is to provide a method for software reliability modeling, which considers both testing-effort and change-point. In the vast literature, most researchers assume a constant detection rate per fault in deriving their software reliability models. They suppose that all faults have equal probability of being detected during the software testing process, and the rate remains constant over the intervals between fault occurrences. In reality, the fault detection rate strongly depends on the skill of test teams, program size, and software testability. Therefore, it may not be smooth and can be changed. On the other hand, sometimes we have to detect more additional faults in order to reach the desired reliability objective during testing. It is advisable for project managers to purchase new\u00a0\u2026", "num_citations": "312\n", "authors": ["324"]}
{"title": "Analysis of incorporating logistic testing-effort function into software reliability modeling\n", "abstract": " This paper investigates a SRGM (software reliability growth model) based on the NHPP (nonhomogeneous Poisson process) which incorporates a logistic testing-effort function. SRGM proposed in the literature consider the amount of testing-effort spent on software testing which can be depicted as an exponential curve, a Rayleigh curve, or a Weibull curve. However, it might not be appropriate to represent the consumption curve for testing-effort by one of those curves in some software development environments. Therefore, this paper shows that a logistic testing-effort function can be expressed as a software-development/test-effort curve and that it gives a good predictive capability based on real failure-data. Parameters are estimated, and experiments performed on actual test/debug data sets. Results from applications to a real data set are analyzed and compared with other existing models to show that the\u00a0\u2026", "num_citations": "224\n", "authors": ["324"]}
{"title": "An assessment of testing-effort dependent software reliability growth models\n", "abstract": " Over the last several decades, many Software Reliability Growth Models (SRGM) have been developed to greatly facilitate engineers and managers in tracking and measuring the growth of reliability as software is being improved. However, some research work indicates that the delayed S-shaped model may not fit the software failure data well when the testing-effort spent on fault detection is not a constant. Thus, in this paper, we first review the logistic testing-effort function that can be used to describe the amount of testing-effort spent on software testing. We describe how to incorporate the logistic testing-effort function into both exponential-type, and S-shaped software reliability models. The proposed models are also discussed under both ideal, and imperfect debugging conditions. Results from applying the proposed models to two real data sets are discussed, and compared with other traditional SRGM to show\u00a0\u2026", "num_citations": "217\n", "authors": ["324"]}
{"title": "Neural-network-based approaches for software reliability estimation using dynamic weighted combinational models\n", "abstract": " Software reliability is the probability of failure-free software operation for a specified period of time in a specified environment. During the last three decades, many software reliability growth models (SRGMs) have been proposed and analyzed for measuring software reliability growth. SRGMs are mathematical models that represent software failures as a random process and can be used to evaluate development status during testing. However, most of SRGMs depend on some assumptions or distributions. In this paper, we propose an artificial neural-network-based approach for software reliability estimation and modeling. We first explain the neural networks from the mathematical viewpoints of software reliability modeling. We will show how to apply neural network to predict software reliability by designing different elements of neural networks. Furthermore, we will use the neural network approach to build a\u00a0\u2026", "num_citations": "200\n", "authors": ["324"]}
{"title": "Framework for modeling software reliability, using various testing-efforts and fault-detection rates\n", "abstract": " This paper proposes a new scheme for constructing software reliability growth models (SRGM) based on a nonhomogeneous Poisson process (NHPP). The main focus is to provide an efficient parametric decomposition method for software reliability modeling, which considers both testing efforts and fault detection rates (FDR). In general, the software fault detection/removal mechanisms depend on previously detected/removed faults and on how testing efforts are used. From practical field studies, it is likely that we can estimate the testing efforts consumption pattern and predict the trends of FDR. A set of time-variable, testing-effort-based FDR models were developed that have the inherent flexibility of capturing a wide range of possible fault detection trends: increasing, decreasing, and constant. This scheme has a flexible structure and can model a wide spectrum of software development environments, considering\u00a0\u2026", "num_citations": "200\n", "authors": ["324"]}
{"title": "Optimal release time for software systems considering cost, testing-effort, and test efficiency\n", "abstract": " In this paper, we study the impact of software testing effort & efficiency on the modeling of software reliability, including the cost for optimal release time. This paper presents two important issues in software reliability modeling & software reliability economics: testing effort, and efficiency. First, we propose a generalized logistic testing-effort function that enjoys the advantage of relating work profile more directly to the natural flow of software development, and can be used to describe the possible testing-effort patterns. Furthermore, we incorporate the generalized logistic testing-effort function into software reliability modeling, and evaluate its fault-prediction capability through several numerical experiments based on real data. Secondly, we address the effects of new testing techniques or tools for increasing the efficiency of software testing. Based on the proposed software reliability model, we present a software cost\u00a0\u2026", "num_citations": "191\n", "authors": ["324"]}
{"title": "Software reliability analysis by considering fault dependency and debugging time lag\n", "abstract": " Over the past 30 years, many software reliability growth models (SRGM) have been proposed. Often, it is assumed that detected faults are immediately corrected when mathematical models are developed. This assumption may not be realistic in practice because the time to remove a detected fault depends on the complexity of the fault, the skill and experience of personnel, the size of debugging team, the technique(s) being used, and so on. During software testing, practical experiences show that mutually independent faults can be directly detected and removed, but mutually dependent faults can be removed iff the leading faults have been removed. That is, dependent faults may not be immediately removed, and the fault removal process lags behind the fault detection process. In this paper, we will first give a review of fault detection & correction processes in software reliability modeling. We will then illustrate the\u00a0\u2026", "num_citations": "174\n", "authors": ["324"]}
{"title": "Cost-reliability-optimal release policy for software reliability models incorporating improvements in testing efficiency\n", "abstract": " Over the past 30\u00a0years, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products during software development processes. One of the most important applications of SRGMs is to determine the software release time. Most software developers and managers always want to know the date on which the desired reliability goal will be met. In this paper, we first review a SRGM with generalized logistic testing-effort function and the proposed generalized logistic testing-effort function can be used to describe the actual consumption of resources during the software development process. Secondly, if software developers want to detect more faults in practice, it is advisable to introduce new test techniques, tools, or consultants, etc. Consequently, here we propose a software cost model that can be used to formulate realistic total software cost projects and discuss the\u00a0\u2026", "num_citations": "130\n", "authors": ["324"]}
{"title": "A history-based cost-cognizant test case prioritization technique in regression testing\n", "abstract": " Software testing is typically used to verify whether the developed software product meets its requirements. From the result of software testing, developers can make an assessment about the quality or the acceptability of developed software. It is noted that during testing, the test case is a pair of input and expected output, and a number of test cases will be executed either sequentially or randomly. The techniques of test case prioritization usually schedule test cases for regression testing in an order that attempts to increase the effectiveness. However, the cost of test cases and the severity of faults are usually varied. In this paper, we propose a method of cost-cognizant test case prioritization based on the use of historical records. We gather the historical records from the latest regression testing and then propose a genetic algorithm to determine the most effective order. Some controlled experiments are performed to\u00a0\u2026", "num_citations": "125\n", "authors": ["324"]}
{"title": "A contact-resistive random-access-memory-based true random number generator\n", "abstract": " A new type of true random number generator, based on the random telegraph noise of a contact-resistive random access memory device, is proposed in this letter. The random-number generator consists of only a simple bias circuit plus a comparator, leading to small circuit area and low power consumption. By realizing this generator by the 65-nm complementary metal-oxide-semiconductor logic process, the occupied area can be as low as 45 \u03bcm 2 , demonstrating substantial saving in the circuit area.", "num_citations": "120\n", "authors": ["324"]}
{"title": "Estimation and analysis of some generalized multiple change-point software reliability models\n", "abstract": " Software typically undergoes debugging during both a testing phase before product release, and an operational phase after product release. But it is noted that the fault detection and removal processes during software development and operation are different. For example, the fault removal during operation occurs generally at a slower pace than development. In this paper, we derive a powerful, easily deployable technique for software reliability prediction and assessment in the testing and operational phases. We first review how several existing software reliability growth models (SRGM) based on non- homogeneous Poisson processes (NHPP) can be readily derived from a unified theory. With the unified theory, we further incorporate the concept of multiple change-points, i.e. points in time when the software environment changes, into software reliability modeling. Several models are proposed and discussed\u00a0\u2026", "num_citations": "118\n", "authors": ["324"]}
{"title": "An adaptive reliability analysis using path testing for complex component-based software systems\n", "abstract": " With the growing size and complexity of software applications, traditional software reliability methods are insufficient to analyze inter-component interactions of modular software systems. The number of test cases may be extremely large for this application; therefore, it is hard for us to extensively test each software component given resource limitations. In this paper, we propose an adaptive framework of incorporating path testing into reliability estimation for modular software systems. Three estimated methods based on common program structures, namely, sequence, branch, and loop structures, are proposed to calculate the path reliability. Consequently, the derived path reliabilities can be applied to the estimates of software reliability. Some experiments are performed based on two real systems. In addition, the accuracy and correlation with respect to the experiments are investigated by simulation and sensitivity\u00a0\u2026", "num_citations": "105\n", "authors": ["324"]}
{"title": "Enhancing and measuring the predictive capabilities of testing-effort dependent software reliability models\n", "abstract": " Software testing is necessary to accomplish highly reliable software systems. If the project manager can conduct well-planned testing activities, the consumption of related testing-resources will be cost-effective. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed to estimate the reliability growth of software, and they are mostly applicable to the late stages of testing in software development. Thus far, it appears that most SRGMs do not take possible changes of testing-effort consumption rates into consideration. However, in some cases, the policies of testing-resource allocation could be changed or adjusted. Thus, in this paper, we will incorporate the important concept of multiple change-points into Weibull-type testing-effort functions. The applicability and performance of the proposed models are demonstrated through two real data sets. Experimental results show that the\u00a0\u2026", "num_citations": "102\n", "authors": ["324"]}
{"title": "Analysis of a software reliability growth model with logistic testing-effort function\n", "abstract": " We investigate a software reliability growth model (SRGM) based on the Non Homogeneous Poisson Process (NHPP) which incorporates a logistic testing effort function. Software reliability growth models proposed in the literature incorporate the amount of testing effort spent on software testing which can be described by an exponential curve, a Rayleigh curve, or a Weibull curve. However it may not be reasonable to represent the consumption curve for testing effort only by an exponential, a Rayleigh or a Weibull curve in various software development environments. Therefore, we show that a logistic testing effort function can be expressed as a software development/test effort curve and give a reasonable predictive capability for the real failure data. Parameters are estimated and experiments on three actual test/debug data sets are illustrated. The results show that the software reliability growth model with logistic\u00a0\u2026", "num_citations": "101\n", "authors": ["324"]}
{"title": "Software reliability analysis and measurement using finite and infinite server queueing models\n", "abstract": " Software reliability is often defined as the probability of failure-free software operation for a specified period of time in a specified environment. During the past 30 years, many software reliability growth models (SRGM) have been proposed for estimating the reliability growth of software. In practice, effective debugging is not easy because the fault may not be immediately obvious. Software engineers need time to read, and analyze the collected failure data. The time delayed by the fault detection & correction processes should not be negligible. Experience shows that the software debugging process can be described, and modeled using queueing system. In this paper, we will use both finite, and infinite server queueing models to predict software reliability. We will also investigate the problem of imperfect debugging, where fixing one bug creates another. Numerical examples based on two sets of real failure data are\u00a0\u2026", "num_citations": "95\n", "authors": ["324"]}
{"title": "Analysis of test suite reduction with enhanced tie-breaking techniques\n", "abstract": " Test suite minimization techniques try to remove redundant test cases of a test suite. However, reducing the size of a test suite might reduce its ability to reveal faults. In this paper, we present a novel approach for test suite reduction that uses an additional testing criterion to break the ties in the minimization process. We integrated the proposed approach with two existing algorithms and conducted experiments for evaluation. The experiment results show that our approach can improve the fault detection effectiveness of reduced suites with a negligible increase in the size of the suites. Besides, under specific conditions, the proposed approach can also accelerate the process of minimization.", "num_citations": "88\n", "authors": ["324"]}
{"title": "An integration of fault detection and correction processes in software reliability analysis\n", "abstract": " Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment and is widely recognized as one of the most significant aspects of software quality. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed and they can greatly help us to estimate some important measures such as the mean time to failure, the number of remaining faults, defect levels, and the failure intensity, etc. Besides, SRGMs can also help to determine person power needed to support the desired reliability requirements. However, from our studies, most of SRGMs only focus on describing the behavior of fault detection process and assume that faults are fixed immediately upon detection. In fact, this assumption may not be realistic. Thus, in this paper, we will propose a general framework for modeling the software fault detection and\u00a0\u2026", "num_citations": "86\n", "authors": ["324"]}
{"title": "Optimal testing resource allocation, and sensitivity analysis in software development\n", "abstract": " We consider two kinds of software testing-resource allocation problems. The first problem is to minimize the number of remaining faults given a fixed amount of testing-effort, and a reliability objective. The second problem is to minimize the amount of testing-effort given the number of remaining faults, and a reliability objective. We have proposed several strategies for module testing to help software project managers solve these problems, and make the best decisions. We provide several systematic solutions based on a nonhomogeneous Poisson process model, allowing systematic allocation of a specified amount of testing-resource expenditures for each software module under some constraints. We describe several numerical examples on the optimal testing-resource allocation problems to show applications & impacts of the proposed strategies during module testing. Experimental results indicate the advantages\u00a0\u2026", "num_citations": "82\n", "authors": ["324"]}
{"title": "A watermarking-based medical image integrity control system and an image moment signature for tampering characterization\n", "abstract": " In this paper, we present a medical image integrity verification system to detect and approximate local malevolent image alterations (e.g., removal or addition of lesions) as well as identifying the nature of a global processing an image may have undergone (e.g., lossy compression, filtering, etc.). The proposed integrity analysis process is based on nonsignificant region watermarking with signatures extracted from different pixel blocks of interest, which are compared with the recomputed ones at the verification stage. A set of three signatures is proposed. The first two devoted to detection and modification location are cryptographic hashes and checksums, while the last one is issued from the image moment theory. In this paper, we first show how geometric moments can be used to approximate any local modification by its nearest generalized 2-D Gaussian. We then demonstrate how ratios between original and\u00a0\u2026", "num_citations": "78\n", "authors": ["324"]}
{"title": "Optimal resource allocation for cost and reliability of modular software systems in the testing phase\n", "abstract": " Reliability is one of the most important quality attributes of commercial software since it quantifies software failures during the development process. In order to increase the reliability, we should have a comprehensive test plan that ensures all requirements are included and tested. In practice, software testing must be completed within a limited time and project managers should know how to allocate the specified testing-resources among all the modules. In this paper, we present an optimal resource allocation problem in modular software systems during testing phase. The main purpose is to minimize the cost of software development when the fixed amount of testing-effort and a desired reliability objective are given. An elaborated optimization algorithm based on the Lagrange multiplier method is proposed and numerical examples are illustrated. Moreover, sensitivity analysis is also conducted. We analyze the\u00a0\u2026", "num_citations": "74\n", "authors": ["324"]}
{"title": "Phase velocity array tomography of Rayleigh waves in western Sichuan from ambient seismic noise\n", "abstract": " A transportable array with 297 broadband seismic stations was deployed in the western Sichuan (26 N~ 32 N, 100 E~ 105 E) in 2006 by the State Key Laboratory of Earthquake Dynamics, Institute of Geology, China Earthquake Administration. From the ambient noise data recorded at 156 stations of this array to the north of 29 N from January to December of 2007, we have obtained the surface wave empirical Green\u2032 s functions (EGF) using cross-correlation technique and measured the Rayleigh-wave phase velocity dispersion curves for all possible station pairs. These dispersion measurements were then used to invert for phase velocity maps of Rayleigh waves between periods 2~ 35 s. Our results manifest the significant discrepancies between the crustal structures of the Chuandian block, Songpan-Garze block and Sichuan basin, which can be summarized as follows.(1) The phase velocity maps at the short\u00a0\u2026", "num_citations": "68\n", "authors": ["324"]}
{"title": "Enhancing software reliability modeling and prediction through the introduction of time-variable fault reduction factor\n", "abstract": " Over the past three decades, many software reliability models with different parameters, reflecting various testing characteristics, have been proposed for estimating the reliability growth of software products. We have noticed that one of the most important parameters controlling software reliability growth is the fault reduction factor (FRF) proposed by Musa. FRF is generally defined as the ratio of net fault reduction to failures experienced. During the software testing process, FRF could be influenced by many environmental factors, such as imperfect debugging, debugging time lag, etc. Thus, in this paper, we first analyze some real data to observe the trends of FRF, and consider FRF to be a time-variable function. We further study how to integrate time-variable FRF into software reliability growth modeling. Some experimental results show that the proposed models can improve the accuracy of software reliability\u00a0\u2026", "num_citations": "67\n", "authors": ["324"]}
{"title": "An improved decomposition scheme for assessing the reliability of embedded systems by using dynamic fault trees\n", "abstract": " The theories of fault trees have been used for many years because they can easily provide a concise representation of failure behavior of general non-repairable fault tolerant systems. But the defect of traditional fault trees is lack of accuracy when modeling dynamic failure behavior of certain systems with fault-recovery process. A solution to this problem is called behavioral decomposition. A system will be divided into several dynamic or static modules, and each module can be further analyzed using binary decision diagram (BDD) or Markov chains separately. In this paper, we will show a very useful decomposition scheme that independent subtrees of a dynamic module are detected and solved hierarchically. Experimental results show that the proposed method could result in significant saving of computation time without losing unacceptable accuracy. Besides, we also present an analyzing software toolkit: DyFA\u00a0\u2026", "num_citations": "67\n", "authors": ["324"]}
{"title": "Analysis of software reliability modeling considering testing compression factor and failure-to-fault relationship\n", "abstract": " This paper is an attempt to relax and improve the assumptions regarding software reliability modeling. To approximate reality much more closely, we take into account the concepts of testing compression factor and the quantified ratio of faults to failures in the modeling. Numerical examples based on real failure data show that the proposed framework has a fairly good prediction capability. Further, we also address the optimal software release time problem and conduct a detailed sensitivity analysis through the proposed model.", "num_citations": "66\n", "authors": ["324"]}
{"title": "Reliability assessment and sensitivity analysis of software reliability growth modeling based on software module structure\n", "abstract": " Software reliability is an important characteristic for most systems. A number of reliability models have been developed to evaluate the reliability of a software system. The parameters in these software reliability models are usually directly obtained from the field failure data. Due to the dynamic properties of the system and the insufficiency of the failure data, the accurate values of the parameters are hard to determine. Therefore, the sensitivity analysis is often used in this stage to deal with this problem. Sensitivity analysis provides a way to analyzing the impact of the different parameters. In order to assess the reliability of a component-based software, we propose a new approach to analyzing the reliability of the system, based on the reliabilities of the individual components and the architecture of the system. Furthermore, we present the sensitivity analysis on the reliability of a component-based software in order to\u00a0\u2026", "num_citations": "66\n", "authors": ["324"]}
{"title": "Software reliability modeling and cost estimation incorporating testing-effort and efficiency\n", "abstract": " Many studies have been performed on the subject of software reliability but few have explicitly considered the impact of software testing on the reliability process. This paper presents two important issues on software reliability modeling and software reliability economics: testing effort and efficiency. First, we discuss on how to extend the logistic testing-effort function into a general form. The generalized logistic testing-effort function has the advantage of relating the work profile more directly to the natural flow of software development. Therefore, it can be used to describe the actual consumption of resources during the software development process and to obtain a conspicuous improvement in modeling testing-effort expenditures. Furthermore, we incorporate the generalized logistic testing-effort function into software reliability modeling and its fault-prediction capability is evaluated through four numerical experiments\u00a0\u2026", "num_citations": "61\n", "authors": ["324"]}
{"title": "Software reliability analysis and assessment using queueing models with multiple change-points\n", "abstract": " Over the past three decades, many software reliability growth models (SRGMs) have been proposed, and they can be used to predict and estimate software reliability. One common assumption of these conventional SRGMs is to assume that detected faults will be removed immediately. In reality, this assumption may not be reasonable and may not always occur. During debugging, developers need time to reproduce the failure, identify the root causes of faults, fix them, and then re-run the software. From some experiments or observations, the fault correction rate may not be a constant and could be changed at certain points as time proceeds. Consequently, in this paper, we will investigate and study how to apply queueing models to describe the fault detection and correction processes during software development. We propose an extended infinite server queueing model with multiple change-points to predict and\u00a0\u2026", "num_citations": "56\n", "authors": ["324"]}
{"title": "Association of DNA repair gene XRCC1 and XPD polymorphisms with genetic susceptibility to gastric cancer in a Chinese population\n", "abstract": " Background: DNA repair gene polymorphisms can contribute to susceptibility of human cancer, including gastric cancer. Three single nucleotide polymorphisms (SNPs) of xeroderma pigmentosum group D (XPD) and X-ray repair cross complement 1 (XRCC1) genes were genotyped in gastric cancer and control subjects in a population from Southwestern China for their association with susceptibility of gastric cancer risk. Methods: 190 hospital-based cases and 180 matched controls were recruited and blood samples were collected from each of them and amplified with a PCR and DNA sequenced for XPD Asp312Asn, XRCC1 Arg194Trp, and XRCC1 Arg280Gln genotyping. Results: Allelic association analysis of these three SNPs showed that the frequency of XRCC1 194Trp in gastric cancer case and the control was 17.2% and 7.3%, respectively, which was significantly associated with gastric cancer risk (OR\u00a0=\u00a02\u00a0\u2026", "num_citations": "55\n", "authors": ["324"]}
{"title": "Optimal allocation of testing-resource considering cost, reliability, and testing-effort\n", "abstract": " We investigate an optimal resource allocation problem in modular software systems during testing phase. The main purpose is to minimize the cost of software development when the number of remaining faults and a desired reliability objective are given. An elaborated optimization algorithm based on the Lagrange multiplier method is proposed and numerical examples are illustrated. Besides, sensitivity analysis is also conducted. We analyze the sensitivity of parameters of proposed software reliability growth models and show the results in detail. In addition, we present the impact on the resource allocation problem if some parameters are either overestimated or underestimated. We can evaluate the optimal resource allocation problems for various conditions by examining the behavior of the parameters with the most significant influence. The experimental results greatly help us to identify the contributions of each\u00a0\u2026", "num_citations": "55\n", "authors": ["324"]}
{"title": "Fine velocity structure of the Longmenshan fault zone by double-difference tomography\n", "abstract": " Using P-wave travel time data recorded by the Western Sichuan movable seismic array and the earthquake emergency response stations during May 2008 to October 2008, we obained accurate relocation of Wenchuan earthquake sequence and 3D P-wave seismic velocity structure by double-difference tomography. The results show that aftershock relocation by joint-inversion is similar to that based on 1-D velocity model by double-difference relocation; the P-wave velocity of Longmenshan fault zone region in 0~ 15 km is closely correlated with local geology, and the region in 20~ 30 km shows NE-NW cross structure. The south segment of Longmenshan fault zone is imaged as high P-wave velocity region in upper crust, the structure heterogeneity controls the distribution of aftershocks and the direction of propagation. The joint-inversion result also proves the existence of Xiaoyudong-Lixian buried fault, we also\u00a0\u2026", "num_citations": "48\n", "authors": ["324"]}
{"title": "Comparison of weighted grey relational analysis for software effort estimation\n", "abstract": " In recent years, grey relational analysis (GRA), a similarity-based method, has been proposed and used in many applications. However, we found that most traditional GRA methods only consider nonweighted similarity for predicting software development effort. In fact, nonweighted similarity may cause biased predictions, because each feature of a project may have a different degree of relevance to the development effort. Therefore, this paper proposes six weighted methods, including nonweighted, distance-based, correlative, linear, nonlinear, and maximal weights, to be integrated into GRA for software effort estimation. Numerical examples and sensitivity analyses based on four public datasets are used to show the performance of the proposed methods. The experimental results indicate that the weighted GRA can improve estimation accuracy and reliability from the nonweighted GRA. The results also\u00a0\u2026", "num_citations": "48\n", "authors": ["324"]}
{"title": "Design and analysis of GUI test-case prioritization using weight-based methods\n", "abstract": " Testing the correctness of a GUI-based application is more complex than the conventional code-based application. In addition to testing the underlying codes of the GUI application, the space of possible combinations of events with a large GUI-input sequence also requires creating numerous test cases to confirm the adequacy of the GUI testing. Running all GUI test cases and then fixing all found bugs may be time-consuming and delaying the project completion. Hence, it is important to advance the test cases that uncover the most faults as fast as possible in the testing process. Test-case prioritization has been proposed and used in recent years because it can improve the rate of fault detection during the testing phase. However, few studies have discussed the problem of GUI test-case prioritization. In this paper, we propose a weighted-event flow graph for solving the non-weighted GUI test case and ranking GUI\u00a0\u2026", "num_citations": "48\n", "authors": ["324"]}
{"title": "Privacy-preserving data publishing for multiple numerical sensitive attributes\n", "abstract": " Anonymized data publication has received considerable attention from the research community in recent years. For numerical sensitive attributes, most of the existing privacy-preserving data publishing techniques concentrate on microdata with multiple categorical sensitive attributes or only one numerical sensitive attribute. However, many real-world applications can contain multiple numerical sensitive attributes. Directly applying the existing privacy-preserving techniques for single-numerical-sensitive-attribute and multiple-categorical-sensitive-attributes often causes unexpected disclosure of private information. These techniques are particularly prone to the proximity breach, which is a privacy threat specific to numerical sensitive attributes in data publication. In this paper, we propose a privacy-preserving data publishing method, namely MNSACM, which uses the ideas of clustering and Multi-Sensitive\u00a0\u2026", "num_citations": "47\n", "authors": ["324"]}
{"title": "A Chatbot-supported smart wireless interactive healthcare system for weight control and health promotion\n", "abstract": " People who are overweight and obese have a greater risk of developing serious diseases and health conditions. A steadily increasing trend of obesity is not only limited to developed countries, but to developing nations as well. As smartphones have rapidly gained mainstream popularity, mobile applications (apps) are used in public health as intervention to keep track of diets, activity as well as weight, which is deemed more accurate than relying on user's self-report measure, for the sake of weight management. A solution called \u201cSmart Wireless Interactive Healthcare System\u201d (SWITCHes) is developed to facilitate objective data reception and transmission in a real-time manner. Based on the user data acquired from SWITCHes app and the auxiliary data from medical instruments, not only SWITCHes app can engage user with tailored feedback in an interactive way, in terms of artificial intelligence-powered health\u00a0\u2026", "num_citations": "46\n", "authors": ["324"]}
{"title": "Analytical predictions and lattice Boltzmann simulations of intrinsic permeability for mass fractal porous media\n", "abstract": " We derived two new expressions for the intrinsic permeability (k) of fractal porous media. The first approach, the probabilistic capillary connectivity (PCC) model, is based on evaluating the expected value of the cross-sectional area of pores connected along various flow paths in the direction in which the permeability is sought. The other model is a modified version of Marshall's probabilistic approach (MPA) applied to random cross matching of pores present on two parallel slices through a fractal porous medium. The Menger sponge is a three-dimensional mass fractal that represents the complicated pore space geometry of soil and rock. Predictions based on the analytical models were compared with estimates of k derived from lattice Boltzmann method (LBM) simulations of saturated flow in virtual representations of Menger sponges. Overall, the analytically predicted k values matched the k values from the\u00a0\u2026", "num_citations": "46\n", "authors": ["324"]}
{"title": "Hopping effect of hydrogen-doped silicon oxide insert RRAM by supercritical CO2 fluid treatment\n", "abstract": " In this letter, we introduced hydrogen ions into titanium metal doped into SiO 2  thin film as the insulator of resistive random access memory (RRAM) by supercritical carbon dioxide (SCCO) 2  fluid treatment. After treatment, low resistance state split in to two states, we find the insert RRAM, which means it has an operating polarity opposite from normal RRAM. The difference of the insert RRAM is owing to the resistive switching dominated by hydrogen ions, dissociated from OH bond, which was not by oxygen ions as usual. The current conduction mechanism of insert RRAM was hopping conduction due to the metal titanium reduction reaction through SCCO 2 .", "num_citations": "44\n", "authors": ["324"]}
{"title": "Design and analysis of cost-cognizant test case prioritization using genetic algorithm with test history\n", "abstract": " During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults\u00a0\u2026", "num_citations": "44\n", "authors": ["324"]}
{"title": "Electrical and reliability investigation of Cu TSVs with low-temperature Cu/Sn and BCB hybrid bond scheme\n", "abstract": " A wafer-level 3-D integration scheme using Cu through-silicon vias (TSVs) and fine-pitch Cu/Sn-BCB hybrid bonding was developed and investigated with electrical characterization and reliability assessment. The hybrid bonding could be achieved below 250 \u00b0C. Low Kelvin resistance and stable daisy chain resistance were achieved in 5- and 10- \u03bcm TSV test structures across the whole wafer. Without obvious deterioration in reliability test results, the integrated Cu TSV and hybrid bond scheme can be potentially designed for 3-D integration applications.", "num_citations": "42\n", "authors": ["324"]}
{"title": "The prognostic value of EGFR overexpression and amplification in Esophageal squamous cell Carcinoma\n", "abstract": " In view of the prominent role in cancer cell biology and alteration in substantial numbers of ESCC, defining EGFR molecular characteristics relevant to patient prognosis is of great importance. Therefore, we analyzed the protein expression and gene copy variation of the epithelial growth factor receptor (EGFR) in Chinese esophageal squamous cell carcinoma (ESCC) and explored the possible associations with various features of the tumors and survival of the patients. Sections were made from tissue microarray composed of 96 ESCC, and examined for EGFR expression by means of immunohistochemistry (IHC) and for EGFR gene amplification by means of fluorescence in situ hybridization (FISH). The results of IHC were evaluated with six different reported scoring systems. Correlation with clinical features and survival was evaluated using chi-square test and Kaplan\u2013Meier analysis. EGFR overexpression according to scoring system 1 significantly correlated with advanced lymph node involvement (P\u2009=\u20090.046), patient disease specific free survival (DFS) (P\u2009=\u20090.006) and overall survival (OS) (P\u2009=\u20090.007). No such association was observed using other 5 scoring systems (P\u2009>\u20090.05 ). EGFR amplification was associated with lymph node metastasis (P\u2009=\u20090.028), but not correlated with DFS and OS until 20\u00a0months. EGFR IHC overexpression evaluated by scoring system 1 might be suitable to be used in predicting patients survival in ESCC. EGFR gene amplification showed delayed prognostic information after 20\u00a0months.", "num_citations": "41\n", "authors": ["324"]}
{"title": "Endurance Improvement Technology With Nitrogen Implanted in the Interface of  Resistance Switching Device\n", "abstract": " Incorporation of nitrogen as an oxygen-confining layer in the resistance switching reaction region is investigated to improve the reliability of resistance random access memory (RRAM). The switching mechanism can be attributed to the formation and rupture of conduction filaments. A compatible WSiON (around 5 nm) layer is introduced at the interface of tungsten silicon oxide (WSiO x ) and TiN electrode to prevent the randomly diffusing oxygen ions surpassing the storage region of the WSiON layer. The double-layer WSiO x /WSiON memory structure would enhance the endurance over 100 times so as to better confirm the WSiO x  RRAM application of nonvolatile memory.", "num_citations": "40\n", "authors": ["324"]}
{"title": "Inhibition of urinary bladder carcinogenesis by aqueous extract of sclerotia of Polyporus umbellatus fries and polyporus polysaccharide\n", "abstract": " The study aimed to evaluate inhibition effect of sclerotia of Polyporus umbellatus Fries aqueous extract (SPUE) and polyporus polysaccharide (PPS) on bladder cancer, then to measure their effect on mRNA expression of glutathione S-transferase \u03c0 (GSTPi) and NAD(P)H:quinone oxidoreductase 1 (NQO1) in female Fischer-344 rats model. The model rats were induced by N-butyl-N-(4-hydroxybutyl)-nitrosamine (BBN) for a period of 8 weeks and saccharin for 12 weeks. SPUE (50 mg/kg, 250 mg/kg, 500 mg/kg) and PPS (28 mg/kg) were orally administrated to the model rats during the whole study. Compared to the control group, a more preventive effect of SPUE and PPS treatment on bladder cancer was discovered, higher mRNA upregulation of GSTpi and NQO1 was seen in the treatment group. Furthermore, the GSTPi and NQO1 mRNA upregulated level in the low-dose group (SPUE 50 mg/kg) was at maximum\u00a0\u2026", "num_citations": "39\n", "authors": ["324"]}
{"title": "Sensitivity analysis of software reliability for component-based software applications\n", "abstract": " The parameters in these software reliability models are usually directly obtained from the field failure data. Due to the dynamic properties of the system and the insufficiency of the failure data, the accurate values of the parameters are hard to determine. Therefore, the sensitivity analysis is often used in this stage to deal with this problem. Sensitivity analysis provides a way to analyzing the impact of the different parameters. In order to assess the reliability of a component-based software, we propose a new approach to analyzing the reliability of the system, based on the reliabilities of the individual components and the architecture of the system. Furthermore, we present the sensitivity analysis on the reliability of a component-based software in order to determine which of the components affects the reliability of the system most. Finally, three general examples are evaluated to validate and show the effectiveness of the\u00a0\u2026", "num_citations": "39\n", "authors": ["324"]}
{"title": "Age-associated microbiome shows the giant panda lives on hemicelluloses, not on cellulose\n", "abstract": " The giant panda feeds almost exclusively on bamboo, a diet highly enriched in lignin and cellulose, but is characterized by a digestive tract similar to carnivores. It is still large unknown if and how the giant panda gut microbiota contributes to lignin and cellulose degradation. Here we show the giant pandas\u2019 gut microbiota does not significantly contribute to cellulose and lignin degradation. We found that no operational taxonomic unit had a nearest neighbor identified as a cellulolytic species or strain with a significant higher abundance in juvenile than cubs, a very low abundance of putative lignin and cellulose genes existed in part of analyzing samples but a significant higher abundance of genes involved in starch and hemicellulose degradation in juveniles than cubs. Moreover, a significant lower abundance of putative cellulolytic genes and a significant higher abundance of putative \u03b1-amylase and hemicellulase\u00a0\u2026", "num_citations": "38\n", "authors": ["324"]}
{"title": "Low Temperature Improvement Method on  Resistive Random Access Memory Devices\n", "abstract": " To improve the resistive switching properties of the resistive random access memory (RRAM), the supercritical carbon dioxide (SCCO 2 ) fluid is used as a low temperature treatment. In this letter, the Zn:SiO x  thin films are treated by SCCO 2  fluid mixed with pure water. After SCCO 2  fluid treatment, the resistive switching qualities of the Zn:SiO x  thin films are carried out by XPS, fourier transform infrared spectroscopy, and IV measurement. We believe that the SCCO 2 -treated Zn:SiO x  thin film is a proresistive switching properties mising material for RRAM applications due to its compatibility with portable flat panel display.", "num_citations": "38\n", "authors": ["324"]}
{"title": "Defect characterizations of \u03b3-LiAlO2 single crystals\n", "abstract": " LiAlO 2 is a potential substrate for growing III-nitride semiconductors since the lattice mismatch between LiAlO 2 and GaN is only 1.4%. GaN grown on (1 0 0) LiAlO 2 substrates is along the (1 0 1\u00af 0) M-plane which does not have the spontaneous polarization. This paper describes the growth of (1 0 0) LiAlO 2 single crystals using the Czochralski method. The as-grown crystals revealed a serious chemical decomposition problem in the cone area and some cracks in the body of the boule. Cracks indicate strain in the crystal and also relieve the strain. The crystal's structure was identified as the \u03b3-phase by X-ray diffraction analysis. No phase transition was found. The thermal expansion coefficients of LiAlO 2 was found to be 6.50\u00d7 10\u2212 6\u00b0 C\u2212 1 along (1 0 0) axis, and 14.90\u00d7 10\u2212 6\u00b0 C\u2212 1 along (0 0 1) axis. Stress distribution was investigated using an optical polarizer and no residual stress was found. Chemical etching\u00a0\u2026", "num_citations": "38\n", "authors": ["324"]}
{"title": "Mortar: Filling the gaps in data center memory\n", "abstract": " Data center servers are typically overprovisioned, leaving spare memory and CPU capacity idle to handle unpredictable workload bursts by the virtual machines running on them. While this allows for fast hotspot mitigation, it is also wasteful. Unfortunately, making use of spare capacity without impacting active applications is particularly difficult for memory since it typically must be allocated in coarse chunks over long timescales. In this work we propose re-purposing the poorly utilized memory in a data center to store a volatile data store that is managed by the hypervisor. We present two uses for our Mortar framework: as a cache for prefetching disk blocks, and as an application-level distributed cache that follows the memcached protocol. Both prototypes use the framework to ask the hypervisor to store useful, but recoverable data within its free memory pool. This allows the hypervisor to control eviction policies and\u00a0\u2026", "num_citations": "37\n", "authors": ["324"]}
{"title": "The treatment of glioblastoma multiforme through activation of microglia and TRAIL induced by rAAV2-mediated IL-12 in a syngeneic rat model\n", "abstract": " Microglial cells are the predominant immune cells in malignant brain tumors, but tumors may release some factors to reduce their defensive functions. Restoration of the anti-cancer function of microglia has been proposed as a treatment modality for glioblastoma. We examined the effect of intra-cranially administered recombinant adeno-associated virus encoding interleukin-12 (rAAV2/IL12) on transfection efficiency, local immune activity and survival in a rat model of glioblastoma multiforme. F344 rats were injected with rAAV2/IL12 and implanted with syngeneic RG2 cells (glioblastoma cell line). Intracerebral interleukin-12 and interferon-\u03b3 concentrations were determined by ELISA. Activation of microglia was determined by expressions of ED1 and tumor necrosis factor-related apoptosis-inducing ligand (TRAIL) which were evaluated by Western blotting and immunohistochemistry. The proliferation of cancer cells was evaluated with Ki67 immunohistochemistry and apoptosis of cancer cells with TUNEL. The brains treated with rAAV2/IL-12 maintained high expression of interleukin-12 and interferon-\u03b3 for at least two months. In syngeneic tumor model, brains treated with rAAV2/IL12 exhibited more infiltration of activated microglia cells as examined by ED1 and TRAIL stains in the tumor. In addition, the volume of tumor was markedly smaller in AAV2/IL12-treated group and the survival time was significantly longer in this group too. The intra-cerebrally administered rAAV2/IL-12 efficiently induces long lasting expression of IL-12, the greater infiltration of activated microglia cells in the tumor associated improved immune reactions, resulting in the\u00a0\u2026", "num_citations": "37\n", "authors": ["324"]}
{"title": "Improving effort estimation accuracy by weighted grey relational analysis during software development\n", "abstract": " Grey relational analysis (GRA), a similarity-based method, presents acceptable prediction performance in software effort estimation. However, we found that conventional GRA methods only consider non-weighted conditions while predicting effort. Essentially, each feature of a project may have a different degree of relevance in the process of comparing similarity. In this paper, we propose six weighted methods, namely, non-weight, distance-based weight, correlative weight, linear weight, nonlinear weight, and maximal weight, to be integrated into GRA. Three public datasets are used to evaluate the accuracy of the weighted GRA methods. Experimental results show that the weighted GRA performs better precision than the non-weighted GRA. Specifically, the linearly weighted GRA greatly improves accuracy compared with the other weighted methods. To sum up, the weighted GRA not only can improve the\u00a0\u2026", "num_citations": "35\n", "authors": ["324"]}
{"title": "Genistein suppresses psoriasis-related inflammation through a STAT3\u2013NF-\u03baB-dependent mechanism in keratinocytes\n", "abstract": " Psoriasis is a chronic recurrent skin inflammatory disease, and inhibition of inflammation may be an effective means of treating psoriasis. The flavonoid genistein has a clear anti-inflammatory effect. However, the anti-psoriatic effects of genistein and their underlying mechanisms remain unclear. In this study, we investigated the effects of genistein on imiquimod (IMQ)-induced psoriasis-like skin lesions in vivo and explored the mechanisms underlying those effects in vitro. It was found that genistein can significantly improve IMQ-induced pathological scores of cutaneous skin lesions in mice, reduce epidermal thickness, and inhibit the expression of inflammatory factors,including interleukin (IL)-1\u03b2, IL-6, tumour necrosis factor-alpha (TNF-\u03b1), chemokine ligand 2 (CCL2), IL-17 and IL-23. In vitro studies, genistein inhibited the proliferation of human keratinocyte HaCaT cells and inhibited the expression of inflammatory\u00a0\u2026", "num_citations": "34\n", "authors": ["324"]}
{"title": "Blind integrity verification of medical images\n", "abstract": " This paper presents the first method of digital blind forensics within the medical imaging field with the objective to detect whether an image has been modified by some processing (e.g., filtering, lossy compression, and so on). It compares two image features: the histogram statistics of reorganized block-based discrete cosine transform coefficients, originally proposed for steganalysis purposes, and the histogram statistics of reorganized block-based Tchebichef moments. Both features serve as input of a set of support vector machine classifiers built in order to discriminate tampered images from original ones as well as to identify the nature of the global modification one image may have undergone. Performance evaluation, conducted in application to different medical image modalities, shows that these image features can help, independently or jointly, to blindly distinguish image processing or modifications with a\u00a0\u2026", "num_citations": "33\n", "authors": ["324"]}
{"title": "GPS \u7cbe\u5bc6\u5355\u70b9\u5b9a\u4f4d\u6a21\u578b\u53ca\u5176\u5e94\u7528\u5206\u6790\n", "abstract": " \u8ba8\u8bba\u4e86GPS\u7cbe\u5bc6\u5355\u70b9\u5b9a\u4f4d\u4e2d\u76843\u79cd\u5e38\u7528\u6a21\u578b\u2014\u2014\u4f20\u7edf\u6a21\u578b,UofC\u6a21\u578b\u548c\u65e0\u6a21\u7cca\u5ea6\u6a21\u578b.\u5c06\u8fd93\u79cd\u6a21\u578b\u5206\u522b\u5e94\u7528\u5230\u661f\u8f7dGPS\u536b\u661f\u7684\u975e\u5dee\u8fd0\u52a8\u5b66\u5b9a\u8f68\u5b9e\u8df5\u4e2d,\u5e76\u5bf9\u5b9a\u8f68\u7ed3\u679c\u8fdb\u884c\u4e86\u6bd4\u8f83,\u5206\u6790.\u7ed3\u679c\u8868\u660e,3\u79cd\u6a21\u578b\u5728\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u65b9\u9762\u5404\u6709\u4f18\u70b9,\u5e94\u7528\u4e2d\u5e94\u6839\u636e\u5177\u4f53\u60c5\u51b5\u8fdb\u884c\u9009\u62e9.", "num_citations": "33\n", "authors": ["324"]}
{"title": "An artificial neural-network-based approach to software reliability assessment\n", "abstract": " In this paper, we propose an artificial neural- network-based approach for software reliability estimation and modeling. We first explain the network networks from the mathematical viewpoints of software reliability modeling. That is, we will show how to apply neural network to predict software reliability by designing different elements of neural networks. Furthermore, we will use the neural network approach to build a dynamic weighted combinational model. The applicability of proposed model is demonstrated through four real software failure data sets. From experimental results, we can see that the proposed model significantly outperforms the traditional software reliability models.", "num_citations": "33\n", "authors": ["324"]}
{"title": "Software reliability growth models incorporating fault dependency with various debugging time lags\n", "abstract": " Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed and most SRGMs assume that detected faults are immediately corrected. Actually, this assumption may not be realistic in practice. In this paper we first give a review of fault detection and correction processes in software reliability modeling. Furthermore, we show how several existing SRGMs based on NHPP models can be derived by applying the time-dependent delay function. On the other hand, it is generally observed that mutually independent software faults are on different program paths. Sometimes mutually dependent faults can be removed if and only if the leading faults were removed. Therefore, here we incorporate the ideas of fault dependency and time-dependent delay\u00a0\u2026", "num_citations": "33\n", "authors": ["324"]}
{"title": "Joint inversion of receiver function and ambient noise based on Bayesian theory\n", "abstract": " In this study, we present a method for the joint inversion of receiver function and ambient noise based on Bayesian inverse theory (Tarantola, 1987, 2005). The nonlinear inversion method of the complex spectrum ratio of receiver functions (Liu et al., 1996) has been extended to perform the joint inversion of the receiver function and ambient noise with a global scanning of the crustal Poisson's ratio. The forward problem of the Rayleigh\u2010wave phase dispersion is solved in terms of a modified version of the fast generalized R/T method proposed by Pei et al. (2008, 2009). Our numerical tests show that (1) the dependency of inversion results on initial models is removed and the model's parameter is estimated reliably even in the case of using a vertically homogeneous model as the initial guess for the crust structure; (2) because the frequency band consistency of the receiver function with the phase dispersion obtained\u00a0\u2026", "num_citations": "32\n", "authors": ["324"]}
{"title": "Shift register\n", "abstract": " A shift register comprises a plurality of stages,{S n}, n= 1, 2,..., N, N being a positive integer. In one embodiment, each stage S n includes a pull-up circuit having an input for receiving one of a first clock signal, CK1, and a second clock signal, XCK1, an output for responsively outputting an output signal, O n, and an input node Q n, a pull-up control circuit electrically coupled to the input node Q n and configured such that when receiving a first input signal, the pull-up control circuit responsively generates a signal that is provided to the input node Q n to turn on the pull-up circuit, a pull-down circuit electrically coupled to the input node Q n and configured to provide a first voltage to one of the input node Q n and the output of the pull-up circuit, and a pull-down control circuit configured to receive one of a third clock signal, CK2, and a fourth clock signal, XCK2, and responsively generate the first voltage to turn on the pull\u00a0\u2026", "num_citations": "32\n", "authors": ["324"]}
{"title": "The physics of the B factories\n", "abstract": " ScholarWorks@SUNGKYUNKWAN UNIVERSITY: The Physics of the B Factories ScholarWorks@SUNGKYUNKWAN UNIVERSITY LIBRARY Korean Communities & Collections Researchers Titles Issue Date Journals \uac80\uc0c9 Search Detailed Information \uc774\uc804 Cited 208 time in webofscience Cited 279 time in scopus Metadata Downloads The Physics of the B Factoriesopen access Authors Barbero, M[Barbero, M.]; De la Vaissiere, C[De la Vaissiere, C.]; Harada, Y[Harada, Y.]; Kuo, CC[Kuo, CC]; Nagashima, Y[Nagashima, Y.]; Sahu, S[Sahu, S.]; Tosi, S[Tosi, S.]; Bevan, AJ[Bevan, AJ]; Bard, DJ[Bard, DJ]; De la Vaissiere, C[De la Vaissiere, Ch.]; Harrison, PF[Harrison, PF]; Kuo, TL[Kuo, T. -L.]; Nagayama, S[Nagayama, S.]; Saigo, M[Saigo, M.]; Touramanis, C[Touramanis, C.]; Golob, B[Golob, B.]; Barillari, T[Barillari, T.]; De Lesquen, A[De Lesquen, A.]; Harrison, TJ[Harrison, TJ]; Kurashiro, H[Kurashiro, H.]; Nagel, M[Nagel, M.]; \u2026", "num_citations": "31\n", "authors": ["324"]}
{"title": "Optimal weighted combinational models for software reliability estimation and analysis\n", "abstract": " Software is currently a key part of many safety-critical and life-critical application systems. People always need easy- and instinctive-to-use software, but the biggest challenge for software engineers is how to develop software with high reliability in a timely manner. To assure quality, and to assess the reliability of software products, many software reliability growth models (SRGMs) have been proposed in the past three decades. The practical problem is that sometimes these selected SRGMs by companies or software practitioners disagree in their reliability predictions, while no single model can be trusted to provide consistently accurate results across various applications. Consequently, some researchers have proposed to use combinational models for improving the prediction capability of software reliability. In this paper, three enhanced weighted-combinations, namely weighted arithmetic, weighted geometric, and\u00a0\u2026", "num_citations": "31\n", "authors": ["324"]}
{"title": "Load balancing of heterogeneous workloads in memcached clusters\n", "abstract": " Web services, large and small, use in-memory caches like memcached to lower database loads and quickly respond to user requests. These cache clusters are typically provisioned to support peak load, both in terms of request processing capabilities and cache storage size. This kind of worst-case provisioning can be very expensive (eg, Facebook reportedly uses more than 10,000 servers for its cache cluster) and does not take advantage of the dynamic resource allocation and virtual machine provisioning capabilities found in modern public and private clouds. Further, there can be great diversity in both the workloads running on a cache cluster and the types of nodes that compose the cluster, making manual management difficult. This paper identifies the challenges in designing large-scale self-managing caches. Rather than requiring all cache clients to know the key to server mapping, we propose an automated load balancer that can perform line-rate request redirection in a far more dynamic manner. We describe how stream analytic techniques can be used to efficiently detect key hotspots. A controller then guides the load balancer\u2019s key mapping and replication level to prevent overload, and automatically starts additional servers when needed.", "num_citations": "31\n", "authors": ["324"]}
{"title": "Molecular mechanism of cytotoxicity induced by Hsp90-targeted Antp-TPR hybrid peptide in glioblastoma cells\n", "abstract": " Heat-shock protein 90 (Hsp90) is vital to cell survival under conditions of stress, and binds client proteins to assist in protein stabilization, translocation of polypeptides across cell membranes, and recovery of proteins from aggregates. Therefore, Hsp90 has emerged as an important target for the treatment of cancer. We previously reported that novel Antp-TPR hybrid peptide, which can inhibit the interaction of Hsp90 with the TPR2A domain of Hop, induces selective cytotoxic activity to discriminate between normal and cancer cells both in vitro and in vivo. In this study, we investigated the functional cancer-cell killing mechanism of Antp-TPR hybrid peptide in glioblastoma (GB) cell lines. It was demonstrated that Antp-TPR peptide induced effective cytotoxic activity in GB cells through the loss of Hsp90 client proteins such as p53, Akt, CDK4, and cRaf. Antp-TPR also did not induce the up-regulation of Hsp70 and Hsp90\u00a0\u2026", "num_citations": "29\n", "authors": ["324"]}
{"title": "Single nanohole and photoluminescence: nanolocalized and wavelength tunable light source\n", "abstract": " We are first to demonstrate a broadband, nanometer-scale, and background-free light source that is based on photoluminescence of a single nanohole in an Au film. We show that a nanohole with a diameter of as small as 20 nm in a 200-nm thick Au film can be used for this purpose. Further development of the localized source that involves the use of a photon-crystal microcavity with a Q-factor of 100 makes it possible to create a 30-fold enhanced, narrowband tunable light source and with a narrow directivity of the radiation.", "num_citations": "29\n", "authors": ["324"]}
{"title": "Sleep deprivation impairs Ca2+ expression in the hippocampus: ionic imaging analysis for cognitive deficiency with TOF-SIMS\n", "abstract": " Sleep deprivation causes cognitive dysfunction in which impaired neuronal plasticity in hippocampus may underlie the molecular mechanisms of this deficiency. Considering calcium-mediated NMDA receptor subunit 1 (NMDAR1) and neuronal nitric oxide synthase (nNOS) activation plays an important role in the regulation of neuronal plasticity, the present study is aimed to determine whether total sleep deprivation (TSD) would impair calcium expression, together with injury of the neuronal plasticity in hippocampus. Adult rats subjected to TSD were processed for time-of-flight secondary ion mass spectrometry, NMDAR1 immunohistochemistry, nNOS biochemical assay, cytochrome oxidase histochemistry, and the Morris water maze learning test to detect ionic, neurochemical, bioenergetic as well as behavioral changes of neuronal plasticity, respectively. Results indicated that in normal rats, strong calcium\u00a0\u2026", "num_citations": "29\n", "authors": ["324"]}
{"title": "Frequency, characterization, and prognostic analysis of PIK3CA gene mutations in Chinese esophageal squamous cell carcinoma\n", "abstract": " PIK3CA gene mutations are found in numerous cancers but correlate differently with prognosis. Although the frequency of PIK3CA gene mutation in esophageal squamous cell carcinoma (ESCC) has been previously studied, a prognostic analysis has not been reported. Ninety-six surgically resected ESCC tissues were collected from Chinese patients and DNA was extracted. Gene mutations in PIK3CA (exons 9 and 20), EGFR (exons 18, 19, 20 and 21), KRAS (exons 2 and 3), and BRAF (exons 11 and 15) were screened using mutant-enriched liquid chip technology. PIK3CA gene mutations were identified in 12 of 96 ESCC cases (12.5%). No mutations were identified in EGFR, KRAS or BRAF genes in this study. Correlations between clinicopathological features and PIK3CA mutation status were analyzed and finally, patient survival information was used to determine the prognostic significance of PIK3CA mutation\u00a0\u2026", "num_citations": "28\n", "authors": ["324"]}
{"title": "The Reluctant Revolutionary: Dietrich Bonhoeffer's Collision with Prusso-German History\n", "abstract": " Dietrich Bonhoeffer was a uniquely reluctant and distinctly German Lutheran revolutionary. In this volume, the author, an Anglican priest and historian, argues that Bonhoeffer\u2019s powerful critique of Germany\u2019s moral derailment needs to be understood as the expression of a devout Lutheran Protestant. Bonhoeffer gradually recognized the ways in which the intellectual and religious traditions of his own class-the Bildungsb\u00fcrgertum-were enabling Nazi evil. In response, he offered a religiously inspired call to political opposition and Christian witness\u2014which cost him his life. The author investigates Bonhoeffer\u2019s stance in terms of his confrontation with the legacy of Hegelianism and Neo-Rankeanism, and by highlighting Bonhoeffer\u2019s intellectual and spiritual journey, shows how his endeavor to politicially reeducate the German people must be examined in theological terms.", "num_citations": "28\n", "authors": ["324"]}
{"title": "Clinical significance of assessing Her2/neu expression in gastric cancer with dual tumor tissue paraffin blocks\n", "abstract": " One paraffin block is routinely used for human epidermal growth factor receptor 2 (Her2/neu) immunohistochemistry (IHC) assessment. Here, we investigated if picking 2 paraffin blocks for Her2/neu evaluation on 1 slide is an economical, efficient, and practical method, which may reduce false negativity of Her2/neu IHC assessment due to intratumoral heterogeneity. A total of 251 gastric cancer (GC) patients were divided into a cohort using 1 tumor tissue paraffin block (single-block group, n = 132) and a cohort using dual tumor tissue paraffin blocks (dual-block group, n = 119) when evaluating Her2/neu expression status by IHC. In dual-block group, we combined the results from 2 different paraffin blocks and used the higher one as the final score. The number of IHC 1+, 2+, and 3+ specimens in the single-block group was 31 (23.5%), 40 (30.3%), and 19 (14.4%), respectively. The combined final IHC score in the\u00a0\u2026", "num_citations": "27\n", "authors": ["324"]}
{"title": "Optimal allocation of testing resources for modular software systems\n", "abstract": " In this paper, based on software reliability growth models with generalized logistic testing-effort function, we study three optimal resource allocation problems in modular software systems during the testing phase: 1) minimization of the remaining faults when a fixed amount of testing-effort and a desired reliability objective are given; 2) minimization of the required amount of testing-effort when a specific number of remaining faults and a desired reliability objective are given; and 3) minimization of the cost when the number of remaining faults and a desired reliability objective are given. Several useful optimization algorithms based on the Lagrange multiplier method are proposed and numerical examples are illustrated. Our methodologies provide practical approaches to the optimization of testing-resource allocation with a reliability objective. In addition, we also introduce the testing-resource control problem and\u00a0\u2026", "num_citations": "27\n", "authors": ["324"]}
{"title": "Radiation hardness of undoped BGO crystals\n", "abstract": " We measured the radiation hardness of undoped BGO crystals from two different manufacturers. Such crystals are proposed to be used in a small-angle calorimeter of the BELLE detector of the KEK B-factory. Transparency and scintillation light output of the crystals were monitored to see the effect of radiation damage. The crystals show considerable radiation hardness up to 10.2 Mrad equivalent dose, which is much higher than the maximum expected dosage of 500 krad per year of running at BELLE.", "num_citations": "27\n", "authors": ["324"]}
{"title": "Molecular investigation of the interactions of carbon dioxide and methane with kerogen: Application in enhanced shale gas recovery\n", "abstract": " Even with technological advancements such as hydraulic fracturing and horizontal drilling, only 20% of the original gas in shales is recoverable through current industry practices. This low recovery factor is attributed to very low permeability and sorption of much of the gas by solid organic matter. Enhanced gas recovery through carbon dioxide sequestration could be performed either by cyclic injection (huff and puff) of carbon dioxide or fracturing/re-fracturing the formation with a viscosified, foamed or energized carbon dioxide hydraulic fracturing fluid. This work employs molecular modeling to conduct a fundamental investigation of the interactions between carbon dioxide and the solid organic part of the shales, the majority of which is kerogen. In the current work, more realistic molecular models for Type II kerogen in oil-gas window, with active sites were used instead of the graphite based carbon models used in\u00a0\u2026", "num_citations": "26\n", "authors": ["324"]}
{"title": "A study on the applicability of modified genetic algorithms for the parameter estimation of software reliability modeling\n", "abstract": " In order to assure software quality and assess software reliability, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products in the past three decades. In principle, two widely used methods for the parameter estimation of SRGMs are the maximum likelihood estimation (MLE) and the least squares estimation (LSE). However, the approach of these two estimations may impose some restrictions on SRGMs, such as the existence of derivatives from formulated models or the needs for complex calculation. Thus in this paper, we propose a modified genetic algorithm (MGA) to estimate the parameters of SRGMs. Experiments based on real software failure data are performed, and the results show that the proposed genetic algorithm is more effective and faster than traditional genetic algorithms.", "num_citations": "26\n", "authors": ["324"]}
{"title": "Staffing level and cost analyses for software debugging activities through rate-based simulation approaches\n", "abstract": " Research in the field of software reliability, dedicated to the analysis of software failure processes, is quite diverse. In recent years, several attractive rate-based simulation approaches have been proposed. Thus far, it appears that most existing simulation approaches do not take into account the number of available debuggers (or developers). In practice, the number of debuggers will be carefully controlled. If all debuggers are busy, they may not address newly detected faults for some time. Furthermore, practical experience shows that fault-removal time is not negligible, and the number of removed faults generally lags behind the total number of detected faults, because fault detection activities continue as faults are being removed. Given these facts, we apply the queueing theory to describe and explain possible debugging behavior during software development. Two simulation procedures are developed based on\u00a0\u2026", "num_citations": "26\n", "authors": ["324"]}
{"title": "On the differential geometry of flows in nonlinear dynamical systems\n", "abstract": " In order to investigate the geometrical relation between two flows in two dynamical systems, a flow for an investigated dynamical system is called the compared flow and a flow for a given dynamical system is called the reference flow. A surface on which the reference flow lies is termed the reference surface. The time-change rate of the normal distance between the reference and compared flows in the normal direction of the reference surface is measured by a new function (i.e.,  function). Based on the surface of the reference flow, the -order  functions are introduced for the noncontact and -order contact flows in two different dynamical systems. Through the new functions, the geometric relations between two flows in two dynamical systems are investigated without contact between the reference and compared flows. The dynamics for the compared flow with a contact to the reference surface is briefly\u00a0\u2026", "num_citations": "26\n", "authors": ["324"]}
{"title": "Hydraulic fracturing experiments at 1500 m depth in a deep mine: Highlights from the kISMET project\n", "abstract": " In support of the US DOE SubTER Crosscut initiative, we established a field test facility in a deep mine and designed and carried out in situ hydraulic fracturing experiments relevant to enhanced geothermal systems (EGS) in crystalline rock to characterize the stress field, understand the effects of rock fabric on fracturing, and gain experience in monitoring using geophysical methods. The project also included pre-and post-fracturing simulation and analysis, and laboratory measurements and experiments. The kISMET (permeability (k) and Induced Seismicity Management for Energy Technologies) site was established in the West Access Drift of the Sanford Underground Research Facility (SURF) 4757 ft (1450 m) below ground (on the 4850 ft level (4850L)) in phyllite of the Precambrian Poorman Formation. We drilled and continuously cored five near-vertical boreholes in a line on 3 m (10 ft) spacing, deviating the two outermost boreholes slightly to create a five-spot pattern around the test borehole centered in the test volume 40 m below the drift invert (floor) at a total depth of~ 1490 m (4890 ft). Laboratory measurements of core from the center test borehole showed P-wave velocity heterogeneity along each core indicating strong, fine-scale (~ 1 cm or smaller) changes in the mechanical properties more\u00bb", "num_citations": "23\n", "authors": ["324"]}
{"title": "Massively parallel fully coupled implicit modeling of coupled thermal-hydrological-mechanical processes for enhanced geothermal system reservoirs\n", "abstract": " Development of enhanced geothermal systems (EGS) will require creation of a reservoir of sufficient volume to enable commercial-scale heat transfer from the reservoir rocks to the working fluid. A key assumption associated with reservoir creation/stimulation is that sufficient rock volumes can be hydraulically fractured via both tensile and shear failure, and more importantly by reactivation of naturally existing fractures (by shearing) to create the reservoir. The advancement of EGS greatly depends on our understanding of the dynamics of the intimately coupled rock-fracture-fluid system and our ability to reliably predict how reservoirs behave under stimulation and production. In order to increase our understanding of how reservoirs behave under these conditions, we have developed a physics-based rock deformation and fracture propagation simulator by coupling a discrete element model (DEM) for fracturing with a continuum multiphase flow and heat transport model. In DEM simulations, solid rock is represented by a network of discrete elements (often referred as particles) connected by various types of mechanical bonds such as springs, elastic beams or bonds that have more complex properties (such as stress-dependent elastic constants). Fracturing is represented explicitly as broken bonds (microcracks), which form and coalesce into macroscopic fractures when external load is applied. DEM more\u00bb", "num_citations": "23\n", "authors": ["324"]}
{"title": "Impact of some agronomic practices on paddy field soil health under varied ecological conditions: I. Influence of soil moisture\n", "abstract": " The effects of individual and combined additions of urea(100\u03bcg N g^-1soil) and insecticide (triazophos at field rate,FR) under different moisture levles of air-dried soil(AD),50% of water-holding capacity(WHC),100%,WHC and flooded soil(FS) on some selected soil properties in a paddy field soil were examined in a laboratory incubation study.The results indicated that after 21-day incubation at 25\u2103 ,the different moisture levels led to significant changes in the parameters studied,Flooding of soil with distilled waer significantly increased the electron transport system(ETS)/dehydrogenase activity and phenol content of the soil compared to the other moisture levels,while protein and phospholipis behaved differently at varied moisture levels with or without the addition of  urea and /or triazophos.Increased ETS activity was observed with N addition at higher moisture levels thile insecticide incorporation decreased it at all moisture levels as compared to the control(moisture only).The phenol contents slightly decreasd and increased with N and insecticide applications ,respectively,The soil protein contents were found to be unaffected among all the soil treatents at all moisture levels.The soil protein contents were found to be unaffected among all the soil treatments at all moisture levels.However,among different moisture levels,reduced quantities of proteins were estimated at 50% WHC ,suggesting more N-mineralization.Lower quantities of soil biomass phospholipids,among all treatments,were recored at higher moisture levels(100% WHC and FS) than at the loer levels,An overall slight enhancement in phospholipid contents with N and small reduction\u00a0\u2026", "num_citations": "23\n", "authors": ["324"]}
{"title": "Discrete element modeling of deformable pinewood chips in cyclic loading test\n", "abstract": " The design of efficient lignocellulosic biomass feedstock systems is challenging, as current laboratory characterization and design methods were developed primarily for fine powders with relatively low compressibility. The discrete element method (DEM) is gaining prominence as an alternative method for modeling the bulk flow and transport of particulate materials in hoppers and feeders. However, prior DEM simulations investigating the flow of wood chips modeled the particles as simple rigid geometries such as spheres, rods or blocks, and neglected the effects of particle deformability and irregular shapes. As a consequence, those simplified DEM models may not provide enough key diagnosis to help improve the design of biomass feeding and handling equipment. This work presents a bonded-sphere DEM approach for characterizing the mechanical behavior of bulk flexible, deformable pinewood chips in a\u00a0\u2026", "num_citations": "22\n", "authors": ["324"]}
{"title": "Discrete element modeling of rock deformation, fracture network development and permeability evolution under hydraulic stimulation\n", "abstract": " Key challenges associated with the EGS reservoir development include the ability to reliably predict hydraulic fracturing and the deformation of natural fractures as well as estimating permeability evolution of the fracture network with time. We have developed a physics-based rock deformation and fracture propagation simulator by coupling a discrete element model (DEM) for fracturing with a network flow model. In DEM model, solid rock is represented by a network of discrete elements (often referred as particles) connected by various types of mechanical bonds such as springs, elastic beams or bonds that have more complex properties (such as stress-dependent elastic constants). Fracturing is represented explicitly as broken bonds (microcracks), which form and coalesce into macroscopic fractures when external and internal load is applied. The natural fractures are represented by a series of connected line segments. Mechanical bonds that intersect with such line segments are removed from the DEM model. A network flow model using conjugate lattice to the DEM network is developed and coupled with the DEM. The fluid pressure gradient exerts forces on individual elements of the DEM network, which therefore deforms the mechanical bonds and breaks them if the deformation reaches a prescribed threshold value. Such deformation/fracturing in turn more\u00bb", "num_citations": "22\n", "authors": ["324"]}
{"title": "Measurement of direct photon emission in the  decay mode\n", "abstract": " In this paper the KTeV collaboration reports the analysis of 112.1\u00d7 10 3 candidate K L\u2192 \u03c0+ \u03c0\u2212 \u03b3 decays including a background of 671\u00b141 events with the objective of determining the photon production mechanisms intrinsic to the decay process. These decays have been analyzed to extract the relative contributions of the C P violating bremsstrahlung process and the C P conserving M1 and C P violating E1 direct photon emission processes. The M1 direct photon emission amplitude and its associated vector form factor parameterized as| g M 1|(1+ a 1/a 2 (M \u03c1 2\u2212 M K 2)+ 2 M K E \u03b3) have been measured to be| g M 1|= 1.198\u00b10.035 (stat)\u00b10.086 (syst) and a 1/a 2=\u2212 0.738\u00b10.007 (stat)\u00b10.018 (syst) GeV 2/c 2 respectively. An upper limit for the C P violating E1 direct emission amplitude| g E 1|\u2264 0.21 (90% CL) has been found. The overall ratio of direct photon emission (DE) to total photon emission including the\u00a0\u2026", "num_citations": "22\n", "authors": ["324"]}
{"title": "Singlet charge 2/3 quark hiding the top quark: Fermilab Tevatron and CERN LEP implications\n", "abstract": " If c and t quarks are strongly mixed with a weak singlet charge 2/3 quark, B (t\u2192 l\u03bd+ X) could be suppressed via the t\u2192 cH 0 mode; thereby, the top quark could still hide below M W, whereas the heavy quark signal observed at the Fermilab Tevatron is due to the dominantly singlet quark Q. This may occur without affecting the small m c value. Demanding m Q\u2243 175 GeV and m t\u2272 M W, we find that B (t\u2192 l\u03bd+ X) cannot be too suppressed. The heavy quark Q decays via W, H, and Z bosons. The latter can lead to b-tagged Z+ 4 jet events, while the strong c-Q mixing is reflected in a sizable Q\u2192 sW fraction. Z\u2192 tc decay occurs at the tree level and may be at the 10\u2212 3 order, leading to the signature of Z\u2192 l\u03bdbc, all isolated and with large p T, at 10\u2212 5 order.", "num_citations": "22\n", "authors": ["324"]}
{"title": "Inhibition of bromodomain-containing protein 9 for the prevention of epigenetically-defined drug resistance\n", "abstract": " Bromodomain-containing protein 9 (BRD9), an epigenetic \u201creader\u201d of acetylated lysines on post-translationally modified histone proteins, is upregulated in multiple cancer cell lines. To assess the functional role of BRD9 in cancer cell lines, we identified a small-molecule inhibitor of the BRD9 bromodomain. Starting from a pyrrolopyridone lead, we used structure-based drug design to identify a potent and highly selective in vitro tool compound 11, (GNE-375). While this compound showed minimal effects in cell viability or gene expression assays, it showed remarkable potency in preventing the emergence of a drug tolerant population in EGFR mutant PC9 cells treated with EGFR inhibitors. Such tolerance has been linked to an altered epigenetic state, and 11 decreased BRD9 binding to chromatin, and this was associated with decreased expression of ALDH1A1, a gene previously shown to be important in drug\u00a0\u2026", "num_citations": "21\n", "authors": ["324"]}
{"title": "A diet diverse in bamboo parts is important for Giant panda (Ailuropoda Melanoleuca) metabolism and health\n", "abstract": " The aim of this study was to determine the metabolic response in giant pandas (Ailuropoda melanoleuca) to the consumption of certain parts of bamboo above ground growth. Giant pandas were provisioned with three species of bamboo: Phyllostachys bissetii, of which they only consume the culm (culm group); Bashania fargesii, of which they only consume the leaves (leaf group); and Qiongzhuea opienensis, of which they only consume the shoots (shoot group). The \u201cculm\u201d group absorbed the highest amount of calories and fiber, but was in short energy supply (depressed tricarboxylic acid cycle activity), and high fiber level diet might reduce the digestibility of protein. The \u201cculm\u201d and \u201cleaf\u201d groups absorbed less protein, and had a lower rate of body mass growth than the \u201cshoot\u201d group. Digestion of fiber requires energy input and yields low caloric extraction from the culm and leaf, and protein intake is important for\u00a0\u2026", "num_citations": "21\n", "authors": ["324"]}
{"title": "Independent prognostic role of PD-L1 expression in patients with esophageal squamous cell carcinoma\n", "abstract": " Accumulating evidence has shown that PD-L1 expression is associated with clinicopathological features in various human malignancies. We searched for correlations between PD-L1 expression and clinicopathological data in esophageal squamous cell carcinoma (ESCC) patients. PD-L1 expression in primary tumors from 278 patients was evaluated using immunohistochemistry (IHC) in ESCC tissue microarray. Survival curves were constructed by using the Kaplan-Meier method. Univariate and multivariate Cox proportional hazard regression models were performed to identify associations with outcome variables. Overall, tumoral PD-L1 expression (\u2265 10%, 20% or 30% as cut-off value) was associated with favorable DFS and OS upon multivariate analysis. When the patients stratified into stage I-II (168, 60.4%) and stage III-IV (110, 39.6%), or with lymph node metastasis (133, 47.8%), the prognostic role was not\u00a0\u2026", "num_citations": "21\n", "authors": ["324"]}
{"title": "Fine structure of middle and upper crust of the Longmenshan Fault zone from short period seismic ambient noise\n", "abstract": " On 12 May 2008, the Mw7. 9 earthquake occurred on the Longmenshan (LMS) fault zone in Sichuan province of China. Geological surveys, inversion of the seismic rupture process and aftershock relocation indicate that this temblor resulted from a thrust on a high-angle fault with right-slip component. The study of the fine velocity structure of this fault and earthquake-generating environment is of great importance to understanding the tectonic setting and dynamic process of the Wenchuan earthquake as well as the dynamics of the east edge of the Tibet Plateau. Seismic ambient noise tomography has been proved to be a new and efficient way to study the crust and upper mantle structure in recent years. This method extracts surface wave Green's Function between station pairs from continuous ambient noise data, uses the surface wave tomography and S wave inversion method to obtain the velocity structure. Since\u00a0\u2026", "num_citations": "21\n", "authors": ["324"]}
{"title": "A study of enhanced MC/DC coverage criterion for software testing\n", "abstract": " The coverage criteria of verification techniques play an important role in software development and testing. The goal is to reduce the size of test suites to economize on time, and to ensure whether all statements (or conditions) are covered. We can use these criteria to track test progress, assess current situations, predict emerging events, and so on. Thus we can take the necessary actions upon early indications that testing activity is falling behind. Modified condition/decision coverage (MC/DC) was proposed by NASA in 1994, and had been widely adopted and discussed since then. As evident from the definition, MC/DC criterion is used to judge whether each Boolean operator can be satisfied or not. However, we find that the selected test cases sometimes may not be able to satisfy the original definition of MC/DC under some Boolean expressions. In this paper, we will propose a simple but useful method which\u00a0\u2026", "num_citations": "21\n", "authors": ["324"]}
{"title": "Mensuration of concrete cracks using digital close-range photographs\n", "abstract": " The elements of concrete structures suffering from chemical reactions trend to be an expansion phenomenon, which lead to cracks. The concrete structures of inferior quality caused by cracks have much influence on the structures durability and strength.The non-contact measurement is employed because of the physical limitations in manual measurements. This paper deals with semi-automatic crack feature extraction from digital close-range images to infer the relationship between concrete expansion and crack width. Only few manual seed points are required on crotches of crack features. Then, the shape and position may be illustrated with automatic feature extraction in two-dimensional image space. In the process of crack searching, a Gaussian filter is applied in advance to smooth the noise along crack profiles to improve the reliability and precision. Subsequently, the edges of cracks are determined by\u00a0\u2026", "num_citations": "21\n", "authors": ["324"]}
{"title": "Pragmatic study of parametric decomposition models for estimating software reliability growth\n", "abstract": " Numerous stochastic models for the software failure phenomenon based on Nonhomogeneous Poisson Process (NHPP) have been proposed in the last three decades (1968-98). Although these models are quite helpful for software developers and have been widely applied at industrial organizations or research centers, we still need to do more work on examining/estimating the parameters of existing software reliability growth models (SRGMs). We investigate and account for three possible trends of software fault detection phenomena during the testing phase: increasing, decreasing and steady state. We present empirical results from quantitative studies on evaluating the fault detection process and develop a valid time-variable fault detection rate model which has the inherent flexibility of capturing a wide range of possible fault detection trends. The applicability of the proposed model and the related methods of\u00a0\u2026", "num_citations": "21\n", "authors": ["324"]}
{"title": "Integrating generalized Weibull-type testing-effort function and multiple change-points into software reliability growth models\n", "abstract": " In modern societies, software is everywhere and we need software to be reliable. In practice, during software development processes, software reliability assessment can greatly help managers to understand effectiveness of consumed testing-effort and deploy testing-resource. In the 1970s-2000, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of software products. In this paper, the concept of multiple change-points is incorporated into Weibull-type testing-effort dependent SRGM because the consumption phenomenon of testing resource may vary at some moments. The performance and application of proposed models are demonstrated through one real data set. The experimental results show that the models give an excellence performance on failure prediction. Besides, we also discuss the optimal release time problems based on reliability requirement\u00a0\u2026", "num_citations": "20\n", "authors": ["324"]}
{"title": "Numerical simulation applications in the design of EGS Collab Experiment 1\n", "abstract": " The United States Department of Energy, Geothermal Technologies Office (GTO) is funding a collaborative investigation of enhanced geothermal systems (EGS) processes at the meso-scale. This study, referred to as the EGS Collab project, is a unique opportunity for scientists and engineers to investigate the creation of fracture networks and circulation of fluids across those networks under in-situ stress conditions. The EGS Collab project is envisioned to comprise three experiments and the site for the first experiment is on the 4850 Level (4,850 feet below ground surface) in phyllite of the Precambrian Poorman formation, at the Sanford Underground Research Facility, located at the former Homestake Gold Mine, in Lead, South Dakota. Principal objectives of the project are to develop a number of intermediate-scale field sites and to conduct well-controlled in situ experiments focused on rock fracture behavior and permeability enhancement. Data generated during these experiments will be compared against predictions of a suite of computer codes specifically designed to solve problems involving coupled thermal, hydrological, geomechanical, and geochemical processes. Comparisons between experimental and numerical simulation results will provide code developers with direction for improvements and verification of process models, build confidence in the suite of available numerical tools, and ultimately identify critical future development needs for the geothermal modeling community. Moreover, conducting thorough comparisons of models, modelling approaches, measurement approaches and measured data, via the EGS Collab project, will\u00a0\u2026", "num_citations": "19\n", "authors": ["324"]}
{"title": "Evaluation and analysis of incorporating Fuzzy Expert System approach into test suite reduction\n", "abstract": " ContextSoftware has become increasingly important in our modern society. However, when new features are developed due to user requests, such requests could make the sizes of test-case pools bigger. Many techniques are proposed to solve this problem, such as test suite reduction. However, the ability to expose faults may be weakened when reducing the sizes of the test suites. In this paper, we propose some methods using fuzzy logic in order to improve existing test-suite reduction techniques.ObjectiveThe main purpose of this research is to use a Fuzzy Expert System approach in order to enhance the effectiveness of fault detection during software testing.MethodIncorporating a Fuzzy Expert System into traditional test suite reduction techniques is presented and studied. More objective criteria are used in order to compare the performance of our proposed and selected test suite reduction methods. Some\u00a0\u2026", "num_citations": "19\n", "authors": ["324"]}
{"title": "The use of mutation-specific antibodies in predicting the effect of EGFR-TKIs in patients with non-small-cell lung cancer\n", "abstract": " Purpose                 We aimed to quantify the epidermal growth factor receptor (EGFR) mutation in tumors and to analyze its prediction of EGFR-tyrosine kinase inhibitor (EGFR-TKI) treatment efficacy in EGFR mutation-positive non-small-cell lung cancer (NSCLC) patients.                                               Methods                 We examined EGFR mutation status in 124 lung cancer samples by direct sequencing and amplification refractory mutation system. Among them, 41 were appropriate to quantify the expression of mutant EGFR proteins using immunohistochemistry (IHC) with mutation-specific antibodies. The quantification was determined by both the staining intensity and the proportion of stained tumor cells.                                               Results                 The median progression-free survival (PFS) in patients with a high score for mutant EGFR expression was 18.0\u00a0months (95\u00a0% CI 16.0\u201320.0), which was\u00a0\u2026", "num_citations": "19\n", "authors": ["324"]}
{"title": "Evaluation and application of bounded generalized Pareto analysis to fault distributions in open source software\n", "abstract": " In general, one of the most important aspects of software development and project management is how to make predictions and assessments of quality and reliability for developed products. Project data usually will be systematically collected and analyzed during the process of software development. Practically, it would be helpful if developers could identify the most error-prone modules early so that they can optimize testing-resource allocation and increase fault detection effectiveness accordingly. In the past, many research studies revealed the applicability of the Pareto principle to software systems, and some of them reported that the Pareto distribution (PD) model can be used to predict the fault distribution of software. In this paper, a special form of the Generalized PD model, named the Bounded Generalized Pareto distribution (BGPD) model, is further proposed to investigate the fault distributions of Open\u00a0\u2026", "num_citations": "19\n", "authors": ["324"]}
{"title": "Study on Czochralski growth and defects of LiAlO2 single crystals\n", "abstract": " The LiAlO 2 single crystal was grown by the conventional Czochralski method under ambient pressure with pull rate at 3.0 mm/min and rotation rate at 25 rpm by a home-made furnace. In order to examine the defects of the LiAlO 2 crystal, the surfaces were polished, and etched; these surfaces included the (1 0 0),(1\u00af 0 0),(0 1 0), and (0 0 1) faces. Analysis with an optical microscope showed two domains on the (1\u00af 0 0) and the (0 1 0) surfaces. Moreover, some inclusions were observed on the (0 1 0) face where the etch pit density was 1.3\u20131.6\u00d7 10 4 cm\u2212 2. The experimental results showed that there are different etching characteristics and patterns on these four surfaces of the specimen.", "num_citations": "19\n", "authors": ["324"]}
{"title": "A GPU-accelerated package for simulation of flow in nanoporous source rocks with many-body dissipative particle dynamics\n", "abstract": " Mesoscopic simulations of hydrocarbon flow in source shales are challenging, in part due to the heterogeneous shale pores with sizes ranging from a few nanometers to a few micrometers. Additionally, the sub-continuum fluid\u2013fluid and fluid\u2013solid interactions in nano- to micro-scale shale pores, which are physically and chemically sophisticated, must be captured. To address those challenges, we present a GPU-accelerated package for simulation of flow in nano- to micro-pore networks with a many-body dissipative particle dynamics (mDPD) mesoscale model. Based on a fully distributed parallel paradigm, the code offloads all intensive workloads on GPUs. Other advancements, such as smart particle packing and no-slip boundary condition in complex pore geometries, are also implemented for the construction and the simulation of the realistic shale pores from 3D nanometer-resolution stack images. Our code is\u00a0\u2026", "num_citations": "18\n", "authors": ["324"]}
{"title": "The necessity for iteration in the application of numerical simulation to EGS: Examples from the EGS Collab test bed 1\n", "abstract": " Mark White1, Tim Johnson1, Tim Kneafsey2, Doug Blankenship3, Pengcheng Fu4, Hui Wu4, Ahmad Ghassemi5, Jianrong Lu5, Hai Huang6, Ghanashyam Neupane6, Curt Oldenburg2, Christine Doughty2, Bud Johnston7, Philip Winterfeld8, Ryan Pollyea9, Richard Jayne9, Adam Hawkins10, Yuran Zhang10, and EGS Collab Team11 1Energy and Environment Directorate, Pacific Northwest National Laboratory, Richland, WA 99352, USA 2Earth and Environmental Sciences, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA 3Energy Research, Sandia National Laboratory, Albuquerque, NM, 87185, USA 4Atmospheric, Earth and Energy Division, Lawrence Livermore National Laboratory, Livermore, CA 94550, USA 5Petroleum and Geologic Engineering, The University of Oklahoma, Norman, OK 94550, USA 6Energy and Environment Science and Technology Directorate, Idaho National Laboratory, Idaho Falls, ID 7Geothermal Technologies Program, National Renewable Energy Laboratory, Golden, CO 80401, USA 8Petroleum Engineering, Colorado School of Mines, Golden, CO 80401, USA 9Geosciences, Virginia Tech, Blacksburg, VA 24061, USA 10TomKat Center for Sustainable Energy, Geothermal Program, Stanford University, Stanford, CA 94305, USA Email: mark. white@ pnnl. gov", "num_citations": "18\n", "authors": ["324"]}
{"title": "An introduction to the EGS Collab Project\n", "abstract": " The development of Enhanced Geothermal Systems (EGS) requires an ability to accurately predict the flow rates and temperatures of the production wells. While simple in concept, EGS is complicated by the heterogeneity and complexity of fracture pathways that can lead to channeling, short-circuiting, and premature thermal breakthrough. The EGS Collab project will", "num_citations": "18\n", "authors": ["324"]}
{"title": "Improvement of Resistive Switching Characteristics by Thermally Assisted Forming Process for-Based Structure\n", "abstract": " After high-temperature forming (HTF) and room-temperature forming treatments, the resistive switching behavior gets some improvements. The switching ratio is enhanced as the device undergoes the HTF process. Through the conduction mechanism analyses in the high-resistance state, Schottky emission and Frenkel-Poole emission are fitted. In addition, after HTF treatment, the resistance of conductive filament decreases. The different high-resistance-state characteristics in the two devices can be attributed to more oxygen ions generated by the serious damage during HTF. Finally, the switching behavior activated by low-temperature forming process is employed to confirm the model.", "num_citations": "18\n", "authors": ["324"]}
{"title": "Clinical utility of a combination of lipoarabinomannan, 38-kDa, and 16-kDa antigens as a diagnosis tool for tuberculosis\n", "abstract": " The aim of this study was to evaluate the diagnostic value of tests detecting antibodies against lipoarabinomannan (LAM), 38-kDa, and 16-kDa antigens for Mycobacterium tuberculosis (MTB). Sera from 160 tuberculosis (TB) patients and 150 non-TB healthy controls were subjected to simultaneous detection of antibodies against LAM, 38-kDa, and 16-kDa antigens using protein chips. The diagnostic value of the 3 TB antigens, alone or combined, was evaluated. Results showed that LAM and 38-kDa antigens had the highest positive rates in the TB patients. Tests showing any single positive antibody, 2 positive antibodies, and 3 positive antibodies had a sensitivity of 93.1%, 51.3%, and 15.6%, and a specificity of 81.3%, 96.6%, and 99.3%, respectively. The positive predictive value of tests showing any 2 positive antibodies and 3 positive antibodies was 94.2% and 96.1%, respectively. Combined detection of a\u00a0\u2026", "num_citations": "18\n", "authors": ["324"]}
{"title": "A study of improving the accuracy of software effort estimation using linearly weighted combinations\n", "abstract": " An accurate prediction of software effort has been the goal of successful software project management for more than thirty years. In order to achieve this goal, many software effort estimation methods have been proposed. Unfortunately, none of these methods developed thus far have been able to offer consistent prediction accuracy in all cases. In this paper, therefore, we integrate several software effort estimation methods and assign linear weights for combinations. It is noted that the weight assignment is based on the outcome of single methods. Seven public datasets and three criteria are used to evaluate the accuracy of our combinational models. Experimental results show that the proposed combination models are a useful method for improving estimation accuracy.", "num_citations": "18\n", "authors": ["324"]}
{"title": "Reliability prediction and assessment of fielded software based on multiple change-point models\n", "abstract": " In this paper, we investigate some techniques for reliability prediction and assessment of fielded software. We first review how several existing software reliability growth models based on non-homogeneous Poisson processes (NHPPs) can be readily derived based on a unified theory for NHPP models. Furthermore, based on the unified theory, we can incorporate the concept of multiple change-points into software reliability modeling. Some models are proposed and discussed under both ideal and imperfect debugging conditions. A numerical example by using real software failure data is presented in detail and the result shows that the proposed models can provide fairly good capability to predict software operational reliability.", "num_citations": "18\n", "authors": ["324"]}
{"title": "Reference architecture of intelligent appliances for the elderly\n", "abstract": " This paper describes reference architecture of consumer electronic and assistive appliances and services designed to improve the quality of life of the elderly. The architecture serves as a foundation for the identification, definition and standardization of internal and external interfaces of such appliances. It is a basis for component-based design and supports a framework for building different types of appliance, and different versions of each type, by integrating configurable and evolvable components in a systematic, verifiable way.", "num_citations": "18\n", "authors": ["324"]}
{"title": "Wavelength routing with spare reconfiguration for all-optical WDM networks\n", "abstract": " This study presents a wavelength-routing scheme with spare reconfiguration (SR) to construct dependable all-optical wavelength-division-multiplexing (WDM) networks. Path protection using shared spare lightpaths is a general wavelength-routing method for reducing blocking probability while minimizing demand for spare resources. However, in a dynamic traffic environment, this method may still yield a poor performance because a wavelength on a link is very likely to be continuously held by a spare lightpath and to be unable to be assigned to the working lightpath of a new connection. This study develops a spare reconfiguration mechanism with wavelength reassignment (SR_WR) and path reassignment (SR_PR) to make the spare dynamic and thus further reduce the blocking probability. The proposed wavelength routing with SR proceeds in three stages and has polynomial time complexity. Extensive\u00a0\u2026", "num_citations": "18\n", "authors": ["324"]}
{"title": "Multilevel solution of the elastohydrodynamic lubrication of concentrated contacts in spiroid gears\n", "abstract": " This paper presents a new method for analyzing the lubrication performance of spiroid gears, which combines the elastohydrodynamic lubrication analysis with LTCA. The LTCA provides the geometry of teeth surfaces at the vicinity of contact point, normal load, lubricant entrainment velocity, etc. Multilevel techniques are used to solve the elastohydrodynamic lubrication of concentrated contacts with high ellipticity ratio under heavy load in spiroid gears. Finally, a pair of spiroid gears used in aircraft is analyzed. Some interesting new conclusions are presented.", "num_citations": "18\n", "authors": ["324"]}
{"title": "An architecture-based multi-objective optimization approach to testing resource allocation\n", "abstract": " Software systems are widely employed in society. With a limited amount of testing resource available, testing resource allocation among components of a software system becomes an important issue. Most existing research on the testing resource allocation problem takes a single-objective optimization approach, which may not adequately address all the concerns in the decision-making process. In this paper, an architecture-based multi-objective optimization approach to testing resource allocation is proposed. An architecture-based model is used for system reliability assessment, which has the advantage of explicitly considering system architecture over the reliability block diagram (RBD)-based models, and has good flexibility to different architectural alternatives and component changes. A system cost modeling approach which is based on well-developed software cost models is proposed, which would be a\u00a0\u2026", "num_citations": "17\n", "authors": ["324"]}
{"title": "Incorporating imperfect debugging into software fault processes\n", "abstract": " For the traditional SRGMs, it is assumed that a detected fault is immediately removed and is perfectly repaired with no new faults being introduced. In reality, it is impossible to remove all faults from the fault correction process and have a fault-free effect on the software development environment. In order to relax this perfect debugging assumption, we introduce the possibility of imperfect debugging phenomenon. Furthermore, most of the traditional SRGMs have focused on the failure detection process. Consideration of fault correction process in the existing models is limited. However, to achieve desired level of software quality, it is very important to apply powerful technologies for removing the errors in the fault correction process. Therefore, we divide these processes into different two nonhomogeneous Poisson processes (NHPPs). Moreover, these models are considered to be more practical to depict the fault\u00a0\u2026", "num_citations": "17\n", "authors": ["324"]}
{"title": "Defects and acoustic properties of \n", "abstract": " A potential piezoelectric crystal LiAlO2 with (100) orientation is grown by means of the Czochralski pulling method. The as-grown crystal is identified as a single phase with good uniformity by x-ray diffraction pattern. (001) Transmission electron microscope image showed a unique cross-hatched pattern which reveals a superlattice structure. Several cubic LiAlO2 specimen, 10.0mm\u00d710.0mm\u00d710.0mm were manufactured to characterize its elastic properties. The time-based pulse-echo transmission technique was employed to measure the acoustic velocities of longitudinal and transverse modes. The elastic constants of LiAlO2 were extracted from the acoustic velocity measurements at different propagation directions. It was found that the acoustic velocities of LiAlO2 are much higher than the current piezoelectric crystals, including quartz, LiNbO3, and Langasite family materials.", "num_citations": "16\n", "authors": ["324"]}
{"title": "First measurement of the T-violating muon polarization in the decay K+\u2192 \u03bc+ \u03bd\u03b3\n", "abstract": " We present the result of the first measurement of the T-violating transverse muon polarization PT in the decay K+\u2192\u03bc+\u03bd\u03b3. This polarization is sensitive to new sources of CP-violation in the Higgs sector. Using data accumulated in the period 1996\u20131998 we have obtained PT=(\u22120.64\u00b11.85(stat)\u00b10.10(syst))\u00d710\u22122 which is consistent with no T-violation in this decay.", "num_citations": "16\n", "authors": ["324"]}
{"title": "Rebuilding of the lithosphere beneath the western margin of Ordos: Evidence from multiscale seismic tomography\n", "abstract": " Probing the fine structure of the lithosphere-asthenosphere system is important for understanding the evolution of the western Ordos block. We perform high-resolution multi-scale teleseismic traveltime tomography with P-wave arrival time recorded by 678 stations of Himalayan Scientific Seismic Array and 26 stations of a regional seismic network in the western Ordos region. Totally 160, 000 P-wave relative traveltimes from 9, 641 teleseismic events recorded by 704 seismic stations are available. The CRUST1. 0 model is used to correct the contribution of crust structure to the teleseismic relative travel times. We use the multi-scale seismic tomography method with sparsity constraint to obtain the P-wave velocity structure of the upper mantle to depth of 800 km. Our results show that there exist a clear boundary between Alxa and Ordos along 104 E in upper mantle. Along the 38 N tectonic line, there is a different\u00a0\u2026", "num_citations": "15\n", "authors": ["324"]}
{"title": "Dietary resources shape the adaptive changes of cyanide detoxification function in giant panda (Ailuropoda melanoleuca)\n", "abstract": " The functional adaptive changes in cyanide detoxification in giant panda appear to be response to dietary transition from typical carnivore to herbivorous bear. We tested the absorption of cyanide contained in bamboo/bamboo shoots with a feeding trial in 20 adult giant pandas. We determined total cyanide content in bamboo shoots and giant panda\u2019s feces, levels of urinary thiocyanate and tissue rhodanese activity using color reactions with a spectrophotometer. Rhodanese expression in liver and kidney at transcription and translation levels were measured using real-time RT-PCR and immunohistochemistry, respectively. We compared differences of rhodanese activity and gene expressions among giant panda, rabbit (herbivore) and cat (carnivore), and between newborn and adult giant pandas. Bamboo shoots contained 3.2 mg/kg of cyanide and giant pandas absorbed more than 65% of cyanide. However\u00a0\u2026", "num_citations": "15\n", "authors": ["324"]}
{"title": "A new Pythium species isolated from vegetable fields and analysis by rDNA ITS sequence.\n", "abstract": " Pythium guangxiense sp. nov., isolated from vegetable fields in Nanning, Guangxi guangxi Subject Category: Geographic Entities", "num_citations": "15\n", "authors": ["324"]}
{"title": "Reliability analysis using weighted combinational models for web-based software\n", "abstract": " In the past, some researches suggested that engineers can use combined software reliability growth models (SRGMs) to obtain more accurate reliability prediction during testing. In this paper, three weighted combinational models, namely, equal, linear, and nonlinear weight, are proposed for reliability estimation of web-based software. We further investigate the estimation accuracy of using genetic algorithm to determine the weight assignment for the proposed models. Preliminary result shows that the linearly and nonlinearly weighted combinational models have better prediction capability than single SRGM and equally weighted combinational model for web-based software.", "num_citations": "15\n", "authors": ["324"]}
{"title": "Chromosomal and genomic variations in esophageal squamous cell carcinoma: a review of technologies, applications, and prospections\n", "abstract": " Esophageal squamous cell carcinoma (ESCC) is one of the most common malignant tumors with poor prognosis worldwide. The poor prognosis is due to the advanced stage at the time of diagnosis and the limited clinical staging lacking significant molecular biomarkers to effectively stratify patients for treatment options. As cancer is a disease of genome instability and a resulting of accumulation of genetic alteration, mounting chromosomal and genomic technologies were developed and progressed rapidly which could be used for characterizing patients in genomics level. In this review, we summarized applications of multiple technologies and research progress at chromosomal and genomic level in ESCC.", "num_citations": "14\n", "authors": ["324"]}
{"title": "Spontaneous hemorrhage of a parathyroid adenoma into the mediastinum\n", "abstract": " ObjectiveTo describe the case of a previously healthy 56-year-old woman, who presented with acute, nontraumatic pain in the left side of the neck and mild dysphagia.MethodsWe report the results of the physical examination, imaging studies, and clinical laboratory studies. In addition, we describe the patient\u2019s hospital course after surgical intervention.ResultsA patient who sought medical attention because of acute, nontraumatic neck pain and dysphagia was subsequently found to have acute extracapsular hemorrhage of a parathyroid adenoma. Computed tomography and magnetic resonance imaging studies revealed a mass effect beginning in the neck and extending into the mediastinum. Surgical exploration of the neck and histopathologic evaluation confirmed the diagnosis of spontaneous rupture of a parathyroid adenoma with associated hemorrhage.ConclusionExtracapsular hemorrhage of a parathyroid\u00a0\u2026", "num_citations": "14\n", "authors": ["324"]}
{"title": "An open-label, multicentre study of levocetirizine for the treatment of allergic rhinitis and urticaria in Taiwanese patients\n", "abstract": " Levocetirizine has been shown in observational studies in the west as an effective and satisfactory therapy for patients with allergic respiratory and skin disease. An open-label, multicentre observational study was conducted to investigate the patients\u2019 perception of levocetirizine in the treatment of allergic rhinitis (AR) and urticaria in Taiwanese patients. Three hundred and thirty-three patients (236 AR and97 urticaria patients) attending out-patient clinics of medical centres across Taiwan were included in the study. Patients were treated with levocetirizine 5 mg once daily (AR patients for 2-4 weeks and urticaria patients for 2-6 weeks) and at the end of treatment, they evaluated for symptoms of disease, perception of change in symptoms, global efficacy and tolerability, global preference over previous antiallergic treatment, change in quality of sleep/daily activities, and safety and adverse events (AEs). Levocetirizine markedly improved the symptoms of AR and urticaria; with 70-75% of AR patients and 60-80% of urticaria patients reporting complete or marked improvements in individual symptoms. Asthma symptoms were completely or markedly improved in 44% of patients with AR and concomitant asthma.", "num_citations": "14\n", "authors": ["324"]}
{"title": "The AMS facility at Xi'an AMS Centre\n", "abstract": " [en] The Institute of Earth Environment of the Chinese Academy of Sciences and Xi'an Jiaotong University have jointly created an AMS center, under the support of Ministry of Science and Technology, Chinese Academy of Sciences and Ministry of Education. The Xi'an 3MV multi-element AMS system will be a national experimental platform for earth environmental studies, archeology, biomedical research and other fields related to the AMS technique. This paper describes the AMS's major components with their unique features, including body grounded ion source accepting both CO 2 and solid samples; low energy injector with precisely timing and fast switching; low radiation accelerator containing the accelerator tubes with combined magnetic permanent and declined electrostatic field suppression of secondary electrons; high energy analyzing system with high dispersion and slit stabilization. The excellent results of the acceptance tests for 10 Be, 14 C, 26 Al and 129 I are listed. The 14 C precision of 0.2% and the background for ratio 10 Be/9 Be of 3.65 x 10-15 were demonstrated.(authors)", "num_citations": "14\n", "authors": ["324"]}
{"title": "The kinematic analysis of basketball three point shoot after high intensity program\n", "abstract": " The purpose of this study was to analyze kinetic and kinematic characteristics of three points shooting by high speed camera. Basketball players have to finish the high intensity program which was designed from simulative basketball games. The high intensity testing program includes dribbling, sprint, slide, jump shooting and three points shooting. The results of the experiments indicated that elbow, wrist, hip and ankle joints angle velocities would decrease, except the knee joint, after the high intensity program. The knee angle of take-off would also increase. It indicated that the upper limb joints angular velocity would decrease and players had to increase knee joint angular velocity to maintain original power. The time from take-off to ball release also decreased which means that there was a change in the coordinates in knee joint and elbow joint. After high intensity program the elbow and knee joints extension were closed to produce more power for the shot.", "num_citations": "14\n", "authors": ["324"]}
{"title": "Urinary profiles of luteinizing hormone, estrogen and progestagen during the estrous and gestational periods in giant pandas (Ailuropda melanoleuca)\n", "abstract": " Luteinizing hormone (LH) is one of the main pituitary hormones that regulate ovulation, however its role has not been studied in giant panda. In this study, we developed an ELISA method for the detection of panda urinary LH. We analyzed urinary hormones of 24 female pandas during 36 breeding periods, we found females could easily be impregnated if the first mating occurred within 10 hours after LH peak. We also found the patterns of the ratios of urinary LH and progestagen in pandas that bred and successfully gave birth were significantly different from those that bred but failed to give birth. These data was the first to provide the urinary LH profiles during the estrous and gestational periods in pandas, and demonstrated that the appearance of the urinary LH peak indicated the timing of ovulation. The LH detection together with estrogen analysis makes the window for successful mating narrower than previously\u00a0\u2026", "num_citations": "13\n", "authors": ["324"]}
{"title": "The method and accuracy of documentation of intraoperative fluids management in liver transplantation recipients\n", "abstract": " Background: In liver transplantation, blood loss can be massive, requiring timely and rapid fluid resuscitation. Maintaining proper documentation of fluids during such situations can be difficult and may often lead to counting errors. We report our method of documentation of fluid management during liver transplantation.Matreial/Methods: Each unit of red blood cells (125 cc) that comes from the blood bank had a serial number of 10 Arabic numbers which were verified and double-checked. Each unit was then numbered and labeled as encircled absolute numbers (eg, 1, 2, 3). Both the encircled number and the serial number of the bag were recorded in the anesthesia chart. Each liter of crystalloids and colloids were similarly numbered and labeled in sequence for ease of calculation. At the end of the operation, the nurse anesthetist ascertains that the number of units of blood products used matched with the number of\u00a0\u2026", "num_citations": "13\n", "authors": ["324"]}
{"title": "A liquid chromatography/tandem mass spectrometry method for determination of aristolochic acid\u2010I in rat plasma\n", "abstract": " A sensitive, rapid and specific liquid chromatography\u2013electrospray ionization\u2013tandem mass spectrometry method was developed and validated for the determination of aristolochic acid\u2010I (AA\u2010I) in rat plasma. Finasteride was used as the internal standard (IS). The analyte was separated on a Zorbax Eclipse XDB\u2010C18 column by isocratic elution with methanol\u201010\u2009mM ammonium acetate (75:25, v/v, pH = 7.3) at a flow rate of 0.25\u2009mL/min, and analyzed by mass spectrometry in positive multiple reaction monitoring mode. The precursor\u2010to\u2010product ion transitions of m/z 359.0 \u2192 298.2 and m/z 373.1 \u2192 305.2 were used to detect AA\u2010I and IS, respectively. Good linearity was achieved over a range of 0.4\u2013600\u2009ng/mL. Intra\u2010 and inter\u2010day precisions measured as relative standard deviation were less than 13.5%, and accuracy ranged from 94.2 to 97.5%. The developed method was successfully applied in the\u00a0\u2026", "num_citations": "13\n", "authors": ["324"]}
{"title": "A modified genetic algorithm for parameter estimation of software reliability growth models\n", "abstract": " In this paper, we propose a modified genetic algorithm (MGA) with calibrating fitness functions, weighted bit mutation, and rebuilding mechanism for the parameter estimation of software reliability growth models (SRGMs). An example using a real failure data is given to demonstrate the performance of proposed method. Experimental result shows that MGA is effective for estimating the parameters of SRGM.", "num_citations": "13\n", "authors": ["324"]}
{"title": "Measuring and assessing software reliability growth through simulation-based approaches\n", "abstract": " In the past decade, several rate-based simulation approaches were proposed to predict software failure process. But most of them did not take the number of available debuggers into consideration and this may not be reasonable. In practice, the number of debuggers is always limited and controlled. If all debuggers or developers are busy, the new detected faults should be willing to wait (for a long time to be corrected and removed). Besides, practical experiences also show that the fault removal time is non-negligible and the number of removed faults generally lags behind the total number of detected faults. Based on these facts, in this paper, we will apply queueing theory to describe and explain the possible debugging behavior during software development. Two simulation procedures are developed based on G/G/ infin and G/G/m queueing models. The proposed methods will be illustrated with real software\u00a0\u2026", "num_citations": "13\n", "authors": ["324"]}
{"title": "Effects of some management practices on electron transport system (ETS) activity in paddy soil\n", "abstract": " Electron transport system(ETS)/dehydrogenase activity in a paddy field soil was measured under a variety of incubation conditions using the reduction of 2-(p-iodophenyl-3-(p-nitrophenyl)-5-phenyl tetrazolium chloride(INT) to iodonitrotetrazolium formazan(INTF).The results exhibited a high positive correlation between the ETS activity and the incubation temperature and soil moisture,Dehydrogenase/ETS activity  displayed a negative correlation with insecticide concentrations,and the activity affected adversely as the concentration of the insecticide increased.The hiher doses,5 and 10 field rates(1 field rate=1500mL ha^-1),of insecticide significantly inhibited ETS activity,while lower rates failed to produce any significant reducing effect,Inorganic N( as urea) of concentrations from 0 to 100ug N g^-1 soil showed a positive response to ETS activity,However,at concentrations of 200 and 400ug N g^-1,the activity was reduced significantly.", "num_citations": "13\n", "authors": ["324"]}
{"title": "Prognostic impact and potential interaction of EGFR and c-Met in the progression of esophageal squamous cell carcinoma\n", "abstract": " This study is to examine EGFR and c-Met variation in precancerous lesion, early esophageal squamous cell carcinoma (ESCC), and advanced ESCC and to explore their prognostic significance. EGFR and c-Met were detected by immunohistochemistry (IHC) and fluorescence in situ hybridization (FISH). Of 158 endoscopy resection (ER) specimens, c-Met high expression and FISH positive were 44.9 and 12.6\u00a0%, respectively. EGFR high expression and FISH positive were 2.5 and 19.6\u00a0%, respectively. Of 84 surgical specimens, c-Met high expression and FISH positive were 50 and 8.3\u00a0%, respectively. EGFR high expression and FISH positive were 7.1 and 28.5\u00a0%, respectively. A significant correlation was observed between c-Met and EGFR FISH positive both in ER (P\u2009<\u20090.001) and surgical specimens (P\u2009=\u20090.029). Patients with EGFR high expression had poorer disease-free survival (DFS) and overall\u00a0\u2026", "num_citations": "12\n", "authors": ["324"]}
{"title": "Carbon dioxide sequestration and hydrocarbons recovery in the gas rich shales: An insight from the molecular dynamics simulations\n", "abstract": " Recent technological advances like horizontal drilling and hydraulic fracturing have made recovery of gas possible from the ultra low permeable shale plays in the United States. However, recoveries from these gas shales still tend to be in the range of single digits. It is believed that more than 80% of the generated hydrocarbons still remain in these tight formations. In order to increase the recovery from these tight shale plays, an enhanced recovery procedure using supercritical carbon dioxide as the injection fluid is recommended in this paper. In the current research, molecular dynamics simulations (MDS) have been performed on the kerogen-methane-carbon dioxide system to understand the absorption-adsorption-desorption phenomena of the super critical carbon dioxide fluid.", "num_citations": "12\n", "authors": ["324"]}
{"title": "Medical image integrity control and forensics based on watermarking\u2014Approximating local modifications and identifying global image alterations\n", "abstract": " In this paper we present a medical image integrity verification system that not only allows detecting and approximating malevolent local image alterations (e.g. removal or addition of findings) but is also capable to identify the nature of global image processing applied to the image (e.g. lossy compression, filtering ...). For that purpose, we propose an image signature derived from the geometric moments of pixel blocks. Such a signature is computed over regions of interest of the image and then watermarked in regions of non interest. Image integrity analysis is conducted by comparing embedded and recomputed signatures. If any, local modifications are approximated through the determination of the parameters of the nearest generalized 2D Gaussian. Image moments are taken as image features and serve as inputs to one classifier we learned to discriminate the type of global image processing. Experimental results\u00a0\u2026", "num_citations": "12\n", "authors": ["324"]}
{"title": "Performance assessment and reliability analysis of dependable and distributed computing systems based on BDD and recursive merge\n", "abstract": " System reliability evaluation, sensitivity analysis, failure frequency analysis, importance measures, and optimal design are important issues that have become research topics for distributed dependable computing. Finding all of the Minimal File Spanning Trees (MFST) and avoiding repeatedly computing the redundant MFSTs have been key techniques for evaluating the reliability of a distributed computing system (DCS) in previous works. However, identifying all of the disjointed MFSTs is difficult and time consuming for large-scale networks. Although existing algorithms have been demonstrated to work well on medium-scale networks, they have two inherent drawbacks. First, they do not support efficient manipulation of Boolean algebra. The sum-of-disjoint-products method used by these algorithms is inefficient when dealing with large Boolean functions. Second, the tree-based partitioning algorithm does not\u00a0\u2026", "num_citations": "12\n", "authors": ["324"]}
{"title": "Medical image tamper approximation based on an image moment signature\n", "abstract": " In this paper we propose a medical image integrity verification system that not only allows detecting and localizing one alteration, but also provides an approximation of this latter. For that purpose, we suggest the embedding of an image signature or digest derived from Geometric moments of image pixel blocks. Image integrity verification is then conducted by comparing this embedded signature to the recomputed one. This signature helps to approximate modifications by determining the parameters of the nearest generalized 2D Gaussian. Experimental results with local image modification illustrate the overall performances of our method.", "num_citations": "12\n", "authors": ["324"]}
{"title": "Test suite reduction analysis with enhanced tie-breaking techniques\n", "abstract": " Test suite minimization techniques try to remove redundant test cases of a test suite. However, reducing the size of a test suite might reduce its ability to reveal faults. Most of prior works which address this problem affect some extent of suite size reduction. In this paper, we present a novel approach for test suite reduction that uses additional testing criterion to break the ties in the minimization process. We performed an experiment with the Siemens suite subject programs. The experiment results show that, compared to existing approaches, the proposed approach can improve the fault detection effectiveness of reduced suites with negligible increase in the size of the suites. Besides, the proposed approach can also accelerate the process of minimization.", "num_citations": "12\n", "authors": ["324"]}
{"title": "Reliability modeling incorporating error processes for internet-distributed software\n", "abstract": " The paper proposes several improvements to conventional software reliability growth models (SRGMs) to describe actual software development processes by eliminating an unrealistic assumption that detected errors are immediately corrected. A key part of the proposed models is the \"delay-effect factor\", which measures the expected time lag in correcting the detected faults during software development. To establish the proposed model, we first determine the delay-effect factor to be included In the actual correction process. For the conventional SRGMs, the delay-effect factor is basically non-decreasing. This means that the delayed effect becomes more significant as time moves forward. Since this phenomenon may not be reasonable for some applications, we adopt a bell-shaped curve to reflect the human learning process in our proposed model. Experiments on a real data set for Internet-distributed software has\u00a0\u2026", "num_citations": "12\n", "authors": ["324"]}
{"title": "Flower bud development in apple trees as related to node formation\n", "abstract": " The sequential differentiation of leaf, bract, lateral flower, sepal, petal, stamen and carpel primordia in apple buds was examined. Under favorable conditions the minimum time required for differentiation of the lateral floral primordia, the sepal, petal, stamen and carpel for the king flower was less than a week, 1, 1, 2 and 2 weeks, respectively. The progress of node formation in vegetative and floral buds before bud differentiation was shown to be a negative asymptotic curve. There was no subsequent node formation in vegetative buds. Floral buds increased in node number, during the first 3-4 weeks of development; no further increment occurred after formation of the\" sepal\" stage. The theory of plastochron for flower induction is discussed", "num_citations": "12\n", "authors": ["324"]}
{"title": "EGS Collab Earth Modeling: Integrated 3D Model of the Testbed\n", "abstract": " The EGS Collab project is conducting a series of stimulation and interwell flow tests in an intermediate scale (~ 10-20 m) testbed located on the 4850 level in the Sanford Underground Research Facility (SURF) in Lead, South Dakota. The testbed consists of eight~ 200 ft (~ 60 m) HQ-diameter (9.6 cm) boreholes that are drilled into the Poorman Formation from the western rib of the West Access Drift. Of the eight boreholes, one borehole is used as an injection/stimulation well, another sub-parallel borehole located about 10 m away from the injection well is used as a production well, and rest of the other boreholes are used as monitoring wells. For 3D visualization of the testbed as well as to provide spatially accurate and consistent parameter data for geomechanical and geophysical process modeling tasks, we are using Leapfrog Software (Seequent Limited). For analysis of fractures and creation of conditioned stochastic discrete fracture network (CS-DFN) model of the testbed, we are using FracMan (Golder Associates Inc.). As of mid-May 2019, the EGS Collab Leapfrog database contains the general layout of the drifts/shafts at the 4850 and 4100 levels; generalized geologic framework model of a domain that spans both the 4850 and 4100 levels; the more\u00bb", "num_citations": "11\n", "authors": ["324"]}
{"title": "Poor efficacy response to trastuzumab therapy in advanced gastric cancer with homogeneous HER2 positive and non-intestinal type\n", "abstract": " ResultsThe HER2 homogeneous group and HER2 heterogeneous group showed no statistical difference in RR (46.4% vs 55.0%, P= 0.558), PFS (5.80 vs 6.30 months, P= 0.804) and OS (16.00 vs 16.00 months, P= 0.787). The Lauren intestinal group and Lauren non-intestinal group demonstrated no discrepancy in PFS (6.00 vs 6.00 months, P= 0.912) and OS (16.50 vs 14.00 months, P= 0.227). However, by combining HER2 heterogeneity and Lauren classification, PFS and OS of HER2 homogeneous/Lauren non-intestinal subgroup was the shortest among the 4 subgroups (P= 0.012 and P= 0.037), which was much shorter than the other patients (PFS: 3.00 vs 6.30 months, P= 0.003; OS: 4.50 vs 16.50 months, P= 0.004). Univariate and multivariate analysis showed that HER2 heterogeneity combined with Lauren classification was an independent prognostic factor in both PFS (P= 0.031 and P= 0.002) and OS (P\u00a0\u2026", "num_citations": "11\n", "authors": ["324"]}
{"title": "Laboratory measurement of critical state hydraulic fracture geometry\n", "abstract": " Understanding the geometry of a hydraulic fracture is key to predicting its behavior and performance. Physical measurement of field hydraulic fracture geometries beyond the borehole is difficult and typically cost prohibitive with the only published examples being mine-back studies and cores. Laboratory-scale hydraulic fracturing experiments can more accurately measure the fracture geometry due to smaller specimen size and improved monitoring capabilities. This paper presents laboratory work where hydraulic fracture treatments were performed using epoxy injection such that a propagating fracture could be stabilized and preserved at near-critical state. Constant backpressure was applied after hydraulic breakdown but before cessation of fracture extension to maintain near-critical state geometry. Preliminary results are presented giving measurement of fracture dimensions, including aperture, at the millimeter\u00a0\u2026", "num_citations": "11\n", "authors": ["324"]}
{"title": "A study of applying the bounded Generalized Pareto distribution to the analysis of software fault distribution\n", "abstract": " Software is currently a key part of many safety-critical applications. But the main problem facing the computer industry is how to develop a software with (ultra) high reliability on time, and assure the quality of software. In the past, some researchers reported that the Pareto distribution (PD) and the Weibull distribution (WD) models can be used for software reliability estimation and fault distribution modeling. In this paper we propose a modified PD model to predict and assess the software fault distribution. That is, we suggest using a special form of the Generalized Pareto distribution (GPD) model, named the bounded Generalized Pareto distribution (BGPD) model. We will show that the BGPD model eliminates several modeling issues that arise in the PD model, and perform detailed comparisons based on real software fault data. Experimental result shows that the proposed BGPD model presents very high fitness to\u00a0\u2026", "num_citations": "11\n", "authors": ["324"]}
{"title": "CARATS: a computer-aided reliability assessment tool for software based on object-oriented design\n", "abstract": " With the growth of complexity in the software system, to deliver reliable software products on time becomes a critical issue. Many software reliability growth models (SRGMs) have been proposed in the past three decades. However, most software reliability assessment processes and parameter estimations of models depend on the computations of the general-purposed numerical software. In this paper, we will present a powerful computer-aided reliability assessment tool for software based on object-oriented analysis and design-CARATS. CARATS can use both traditional SRGMs and neural-network methods to assess software reliability. This would greatly help project managers to make decisions during software development life cycle. Due to the characteristics of the special-purposed and object-oriented design, CARATS can analyze the software reliability easily, and is also more flexible to adopt different\u00a0\u2026", "num_citations": "11\n", "authors": ["324"]}
{"title": "Software release time management: How to use reliability growth models to make better decisions\n", "abstract": " In late years, due to the significance of software application, professional testing of software becomes an increasingly important task. Once all detected faults are removed, project managers can begin to determine when to stop testing. Software reliability has important relations with many aspects of software, including the structure, the operational environment, and the amount of testing. Actually, software reliability analysis is a key factor of software quality and can be used for planning and controlling the testing resources during development. Over the past three decades, many software reliability growth models (SRGMs) have been proposed. For most traditional SRGMs, one common assumption is that the fault detection rate is a constant over time. However, the fault detection process in the operational phase is different from that in the testing phase. Thus, in this paper, we use the testing compression factor (TCF) to\u00a0\u2026", "num_citations": "11\n", "authors": ["324"]}
{"title": "EGS collab project: Status, tests, and data\n", "abstract": " The EGS (Enhanced Geothermal Systems) Collab project is performing stimulation and flow experiments in highly-monitored and well-characterized intermediate-scale (approximately10 to 20 meter) field test beds at a depth of approximately 1,500 meters in the Sanford Underground Research Facility (SURF) in the Black Hills of South Dakota. Our fracture stimulation and interwell flow tests are performed to better understand processes that control formation of effective subsurface heat exchangers that are critical to the development and success of EGS. Different EGS Collab stimulations will be performed under dissimilar stress conditions to produce data for model comparisons that better differentiate stimulation mechanisms and the evolution of permeability enhancement in crystalline rock. EGS Collab experiments provide a means of testing tools, concepts, and strategies that could later be employed under\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "The role of numerical simulation in the design of stimulation and circulation experiments for the EGS Collab project\n", "abstract": " The United States Department of Energy, Geothermal Technologies Office (GTO) is funding a collaborative investigation of enhanced geothermal systems (EGS) processes at the meso-scale. This study, referred to as the EGS Collab project, is a unique opportunity for scientists and engineers to investigate the creation of fracture networks and circulation of fluids across those networks under in-situ stress conditions. The EGS Collab project is envisioned to comprise three experiments and the site for the first experiment is on the 4850 Level in phyllite of the Precambrian Poorman formation, at the Sanford Underground Research Facility, located at the former Homestake Gold Mine, in Lead, South Dakota. Principal objectives of the project are to develop a number of intermediate-scale field sites and to conduct well-controlled in situ experiments focused on rock fracture behavior and permeability enhancement. Data generated during these experiments will be compared against predictions of a suite of computer codes specifically designed to solve problems involving coupled thermal, hydrological, geomechanical, and geochemical processes. Comparisons between experimental and numerical simulation results will provide code developers with direction for improvements and verification of process models, build confidence in the suite of available numerical tools, and ultimately identify critical future development needs for the geothermal modeling community. Moreover, conducting thorough comparisons of models, modelling approaches, measurement approaches and measured data, via the EGS Collab project, will serve to identify techniques that\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "Reliability analysis of on-demand service-based software systems considering failure dependencies\n", "abstract": " Service-based software systems (SBSSs) are widely deployed due to the growing trend of distributed computing and cloud computing. It is important to ensure high quality of an SBSS, especially in a strongly competitive market. Existing works on SBSS reliability usually assumed independence of service failures. However, the fact that resource sharing exists in different levels of SBSS operations invalidates this assumption. Ignorance of failure dependencies have been discussed as potentially affecting system reliability predictions and lowering the benefits of design diversity, as typically seen in high-reliability systems. In this paper, we propose a reliability framework that incorporates failure dependence modeling, system reliability modeling, as well as reliability analysis for individual services and for failure sources. The framework is also capable of analyzing the internal structures of popular software fault tolerant\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "An improved Pareto distribution for modelling the fault data of open source software\n", "abstract": " In the modern society, software plays a very important role in many application systems. Consequently, the main goal of project managers and software engineers is to deliver reliable software within very limited resource, time and budget during the software development life cycle. Presently, it is widely recognized that open source software (OSS) has developed as a new (and novel) form of both personal and aggregation production. In the past, some research has shown that the traditional Pareto distribution (PD) and the Weibull distribution models can be used to describe the distribution of software faults of OSS. However, there could be a negative value for the cumulative probability of the traditional PD model in some cases. In this paper, based on our past studies, a modified Pareto\u2010based distribution model, called the single\u2010change\u2010point 2\u2010parameter generalized PD (SCP\u20102GPD) model is proposed. The\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "Reliability evaluation of service-oriented architecture systems considering fault-tolerance designs\n", "abstract": " Service-oriented architecture (SOA) provides an elastic and automatic way to discover, publish, and compose individual services. SOA enables faster integration of existing software components from different parties, makes fault tolerance (FT) feasible, and is also one of the fundamentals of cloud computing. However, the unpredictable nature of SOA systems introduces new challenges for reliability evaluation, while reliability and dependability have become the basic requirements of enterprise systems. This paper proposes an SOA system reliability model which incorporates three common fault-tolerance strategies. Sensitivity analysis of SOA at both coarse and fine grain levels is also studied, which can be used to efficiently identify the critical parts within the system. Two SOA system scenarios based on real industrial practices are studied. Experimental results show that the proposed SOA model can be used to accurately depict the behavior of SOA systems. Additionally, a sensitivity analysis that quantizes the effects of system structure as well as fault tolerance on the overall reliability is also studied. On the whole, the proposed reliability modeling and analysis framework may help the SOA system service provider to evaluate the overall system reliability effectively and also make smarter improvement plans by focusing resources on enhancing reliability-sensitive parts within the system.", "num_citations": "10\n", "authors": ["324"]}
{"title": "Tricyclic graphs with exactly two main eigenvalues\n", "abstract": " An eigenvalue of a graph G is called a main eigenvalue if it has an eigenvector the sum of whose entries is not equal to zero. Let G                 0 be the graph obtained from G by deleting all pendant vertices and \u03b4(G) the minimum degree of vertices of G. In this paper, all connected tricyclic graphs G with \u03b4(G                 0) \u2265 2 and exactly two main eigenvalues are determined.", "num_citations": "10\n", "authors": ["324"]}
{"title": "A study of applying extended pie technique to software testability analysis\n", "abstract": " During the software development process, data that has been gained from the testing phase can help developers to predict software reliability more precisely. But the testing stage usually takes more and more effort due to the growing complexity of software. How to build software that can be tested efficiently has become an important topic in addition to enhancing and developing new testing methods. Thus, research on software testability has been developed variously. In the past, a dynamic technique for estimating program testability was proposed and called propagation, infection, and execution (PIE) analysis. Previous research studies show that PIE analysis can complement software testing. However, this technique requires a lot of computational overhead in estimating the testability of software components. In this paper, we propose an Extended PIE (EPIE) technique to accelerate the traditional PIE analysis\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "Software reliability prediction and analysis using queueing models with multiple change-points\n", "abstract": " Over the past three decades, many software reliability growth models (SRGMs) were proposed and they are aimed at predicting and estimating software reliability. One common assumption of these conventional SRGMs is that detected faults will be removed immediately. In reality, this assumption may not be reasonable and may not always occur. Developers need time to identify the root causes of detected faults and then fix them. Besides, during debugging the fault correction rate may not be a constant and could be changed at some certain points as time proceeds. Consequently, in this paper, we will explore and study how to apply queueing model to investigate the fault correction process during software development. We propose an extended infinite server queueing model with multiple change-points to predict and assess software reliability. Experimental results based on real failure data show that proposed\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "A study of modified testing-based fault localization method\n", "abstract": " In software development and maintenance, locating faults is generally a complex and time-consuming process. In order to effectively identify the locations of program faults, several approaches have been proposed. Similarity-aware fault localization (SAFL) is a testing-based fault localization method that utilizes testing information to calculate the suspicion probability of each statement. Dicing is also another method that we have used. In this paper, our proposed method focuses on predicates and their influence, instead of on statements in traditional SAFL. In our method, fuzzy theory, matrix calculating, and some probability are used. Our method detects the importance of each predicate and then provides more test data for programmers to analyze the fault locations. Furthermore, programmers will also gain some important information about the program in order to maintain their program accordingly. In order to\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "Software reliability prediction and assessment using both finite and infinite server queueing approaches\n", "abstract": " Over the past 30 years, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of software. In fact, effective debugging is not easy because the fault may not be immediately obvious. In the past, some researchers ever used an infinite server queueing (ISO) model to describe the software debugging behavior. An infinite-server queueing model is considered where access of customers to service is controlled by a gate and the gate is open only if all servers are free. However, the finite server queueing (FSQ) model is first advantageously modeled as an infinite-server system. Thus, in this paper, we show how to incorporate both FSQ and ISQ models into software reliability estimation and prediction. In addition, we also consider the factor of perfect/imperfect debugging. Experimental results show that the proposed framework to incorporate both fault detection and\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "Comparison and assessment of improved grey relation analysis for software development effort estimation\n", "abstract": " The goal of software project planning is to provide a framework that allows project manager to make reasonable estimates of the resources. In fact, software development is highly unpredictable - only 10% of projects on time and budget. Thus, it is very important for software project managers to accurately and precisely estimate software development effort since the resources are limited. One of the most widely used approaches of software effort estimation is the analogy method. Since the method of analogy is constructed on the foundation of distance-based similarity, there are still some drawbacks and restrictions for application. For example, the anomalistic and outlying values will influence the function to determine similarity. Contrarily, grey relational analysis (GRA) is a distinct measurement from the traditional distance scale and can dig out the realistic law from small-sample data. In this paper, we show how to\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "Software reliability prediction and analysis during operational use\n", "abstract": " In reality, the fault detection (or correction) phenomenon and software reliability estimation in the operational phase are different from that in the testing phase. The fault removal continues at a slower rate during the operational phase. In this paper, we will investigate some techniques for software reliability prediction and measurement in the operational phase. We first review how some software reliability growth models (SRGMs) based on non-homogeneous Poisson processes (NHPPs) can be readily derived based on a unified theory for NHPP models. Under this general framework, we can not only verify some conventional SRGMs but also derive some new SRGMs that can be used for software reliability measurement in the operational phase. That is, based on the unified theory, we can incorporate the concept of multiple changepoints into software reliability modeling. We can formularize and simulate the fault\u00a0\u2026", "num_citations": "10\n", "authors": ["324"]}
{"title": "Performance of undoped BGO crystals under extremely high dose conditions\n", "abstract": " We checked the performance of undoped BGO crystals under extremely high dose conditions (dosage up to 88 Mrad). The light outputs of the crystals drop by \u223c30% after receiving 10 Mrad of \u03b3-ray dose but remain stable throughout the rest of the test. The self-recovery behavior of BGO under 20 Mrad has also been studied. The reduced variation in light output from pre-radiated BGOs, together with a fixed recovery limit, indicate that a pre-radiation process may be helpful to stabilize the BGO performance in a low dose environment.", "num_citations": "10\n", "authors": ["324"]}
{"title": "Characteristics of mesenchymal stem cells isolated from bone marrow of giant panda\n", "abstract": " In present study, we report on bone marrow (BM) mesenchymal stem cells (MSCs) that are isolated from giant pandas. Cells were collected from the BM of two stillborn giant pandas. The cells were cultured and expanded in 10% fetal bovine serum medium. Cell morphology was observed under an inverted microscopy, and the proliferation potential of the cells was evaluated by counting cell numbers for eight consecutive days. Differentiation potentials of the cells were determined by using a variety of differentiation protocols for osteocytes, adipocytes, neuron cells, and cardiomyocytes. Meanwhile, the specific gene expressions for MSCs or differentiated cells were analyzed by RT-PCR. The isolated cells exhibited a fibroblast-like morphology; expressed mesenchymal specific markers such as cluster of differentiation 73 (CD73), SRY (sex determining region Y)-box 2 (SOX-2), guanine nucleotide-binding protein-like\u00a0\u2026", "num_citations": "9\n", "authors": ["324"]}
{"title": "Reconfigurable active drive: An fpga accelerated storage architecture for data-intensive applications\n", "abstract": " (SSD) architecture for data-intensive applications called Reconfigurable Active Drive (RAD). In the RAD architecture, an FPGA utilizes parallel data transfer of flash memory to improve the data throughput for streaming applications. The storage architecture is designed to match the I/O bandwidth of the FPGA and is capable of fully parallelizing data access and data processing. RAD is modelled and evaluated using an SSD simulator with applications implemented on FPGAs. Compared with reconfigurable computing (RC) and pure software (SW) approaches, RAD achieves up to 159\u00d7 and 541\u00d7 speedups respectively in terms of the end-to-end throughput.", "num_citations": "9\n", "authors": ["324"]}
{"title": "Reliability and sensitivity analysis of embedded systems with modular dynamic fault trees\n", "abstract": " Fault trees theories have been used in years because they can easily provide a concise representation of failure behavior of general non-repairable fault-tolerant systems. But the defect of traditional fault trees is lack of accuracy when modeling dynamic failure behavior of certain systems with fault-recovery process. A solution to this problem is called behavioral decomposition. A system will be divided into several dynamic or static modules, and each module can be further analyzed using BDD or Markov chains separately. In this paper, we will show a decomposition scheme that independent subtrees of a dynamic module are detected and solved hierarchically for saving computation time of solving Markov chains without losing unacceptable accuracy when assessing components sensitivities. In the end, we present our analyzing software toolkit that implements our enhanced methodology.", "num_citations": "9\n", "authors": ["324"]}
{"title": "The EGS Collab hydroshear experiment at the Sanford Underground Research Facility\u2013Siting criteria and evaluation of candidate sites\n", "abstract": " The objective of the EGS Collab project is to establish a suite of intermediate-scale (~ 10-20 m) field test beds coupled with stimulation and interwell flow tests to provide a basis to better understand fracture stimulation methods, resulting fracture geometries, and processes that control heat transfer between rock and stimulated fractures. Experiment 1 of the project is being conducted at a depth of~ 1.5 km in the Sanford Underground Research Facility (SURF) on the 4850 Level (ft. below the ground surface). The stimulation method planned for Experiment 2 of this project is hydroshearing of an existing natural fracture. In siting this experiment, there are several key geologic criteria that need to be met. These include: 1) the fracture should be least 10 m in length so that it could be intersected by two boreholes that are spaced that far apart; 2) it should be optimally oriented relevant to the stress field so that it is critically stressed; 3) the site should have appropriate stress conditions (not too shallow a depth); 4) the fracture should have sufficient permeability to allow for prestimulation flow testing, but not too high a permeability that would preclude permeability enhancement via shear stimulation; 5) the fracture should not intersect other features (permeable fractures, boreholes, adjacent drifts) that could serve as major leak-off zones; 6) the site should not be complicated by geology (a single lithology would be preferred). In addition, there are logistical criteria that will also influence site selection, including: 1) site availability/access, 2) logistical support, eg, power, internet, water, operating rail system; 3) drift size and orientation compatible with drilling and\u00a0\u2026", "num_citations": "8\n", "authors": ["324"]}
{"title": "High amplification of FGFR1 gene is a delayed poor prognostic factor in early stage ESCC patients\n", "abstract": " Amplification of the fibroblast growth factor receptor 1 (FGFR1) is believed to predict response to FGFR inhibitors. The aim of this study was to investigate the frequency and the prognostic impact of FGFR1 amplification in patients with resected esophageal squamous cell carcinoma (ESCC) by using fluorescent in situ hybridization. Microarrayed paraffin embedded blocks were constructed, and the cohort of tissues came from 506 patients with ESCC. FGFR1 high amplification (FGFR1 high) was defined by an FGFR1/centromere 8 ratio of\u2265 2.0, or average number of FGFR1 signals/tumor cell nucleus\u2265 6.0, or percentage of tumor cells containing\u2265 15 FGFR1 signals, or large cluster in\u2265 10% of cancer cells. FGFR1 low amplification was defined by\u2265 5 FGFR1 signals in\u2265 50% of cancer cells. Kaplan-Meier curves with log-rank tests and Cox proportional hazards model were used to analyze patients\u2019 survival. Among\u00a0\u2026", "num_citations": "8\n", "authors": ["324"]}
{"title": "Software reliability analysis considering the variation of testing-effort and change-point\n", "abstract": " It is commonly recognized that software development is highly unpredictable and software quality may not be easily enhanced after software product is finished. During the software development life cycle (SDLC), project managers have to solve many technical and management issues, such as high failure rate, cost over-run, low quality, late delivery, etc. Consequently, in order to produce robust and reliable software product (s) on time and within budget, project managers and developers have to appropriately allocate limited development-and testing-effort and time. In the past, the distribution of testing-effort or manpower can typically be described by the Weibull or Rayleigh model. Practically, it should be noticed that development environments or methods could change due to some reasons. Thus when we plan to perform software reliability modeling and prediction, these changes or variations occurring in the\u00a0\u2026", "num_citations": "8\n", "authors": ["324"]}
{"title": "Enhanced n-version programming and recovery block techniques for web service systems\n", "abstract": " In recent years, web services (WS\u2019s) have been widely used to support interoperable machine-to-machine interaction over a network. In order to ensure a reliable WS system, a number of fault tolerance designs have been proposed. It is known that network connection and hardware devices may fail. In addition, the acceptance test (AT) as well as the decision mechanism (DM), which are common in fault tolerance designs, could also fail unexpectedly. Such uncertainties may affect the reliability of a WS-based system but have not yet been carefully considered in reliability modeling. Therefore, we propose extended NVP (ENVP) and extended RB (ERB) for the reliability analysis. Various operations of ENVP and ERB are discussed, and a simulation procedure is implemented to evaluate the system reliability and the failure probability of fault-tolerant WS-based systems. The experimental results show a high degree of\u00a0\u2026", "num_citations": "8\n", "authors": ["324"]}
{"title": "A wireless and sensitive detection of octachlorostyrene using modified AuNPs as signal-amplifying tags\n", "abstract": " A wireless, remote query octachlorostyrene (OCS) biosensor was fabricated by coating a mass-sensitive magnetoelastic ribbon with anti-OCS antibody. In response to a time-varying magnetic field, the magnetoelastic sensor mechanically vibrates at a characteristic resonance frequency which inversely depends on the sensor mass loading. As the magnetoelastic film is magnetostrictive itself, the vibrations launch magnetic flux that can be remotely detected using a pickup coil. Au nanoparticles (NPs) were used to amplify the mass loading. In a sample solution containing OCS target and OCS-modified AuNPs (OCS\u2013AuNPs), both OCS and OCS\u2013AuNPs react with the anti-OCS antibody immobilized on the sensor surface in a competition mode. The bound OCS\u2013AuNPs amount is inversely proportional to the OCS target concentration. The reduction of bound OCS\u2013AuNPs induced by free OCS results in significant\u00a0\u2026", "num_citations": "8\n", "authors": ["324"]}
{"title": "The effectiveness of jet-grout slabs and cross-walls in restricting wall movements in deep excavations\n", "abstract": " This paper presents observations made at some well-instrumented deep excavations during the construction of the Taipei Rapid Transit Systems. Jet-grout slabs and in-situ cross-walls were used inside these excavations to reduce the lateral displacements of the retaining walls. Based upon these case histories, the effectiveness of these two types of support systems in restricting lateral displacements of retaining walls for deep excavations in soft clay is assessed.", "num_citations": "8\n", "authors": ["324"]}
{"title": "Susceptibility of Chinese alfalfa cultivars to Verticillium wilt\n", "abstract": " Twenty six alfalfa cultivars (25 Medicago sativa and one M. falcata) from China were compared to the seven standard cultivars from North America and Europe for resistance to Verticillium wilt caused by Ve rticillium albo-atru m. Except for the cultivar E Qi, all 25 Chinese cultivars were susceptible to Verticillium wilt with low numbers of resistant plants (less than 21.7% in the population of each cultivar) and high disease severity index (greater than 3.43). The cultivar E Qi was moderately resistant to Verticillium wilt with 43.4% of resistant plants, which was similar to the Swedish cultivar Vertus (43.3%) but lower than the Canadian cultivar Barrier (60.2%). Since there is no report of Verticillium wilt of alfalfa in China or Taiwan, the susceptible nature of Chinese alfalfa cultivars suggests that a regulatory measure may be required to prevent this destructive disease from entering into disease free regions via pathogeninfected alfalfa seed or hay.", "num_citations": "8\n", "authors": ["324"]}
{"title": "Measurement of radiation damage on an optical reflector\n", "abstract": " We measured the radiation damage on an optical white fluorocarbon reflector called Goretex, which is to be used for aerogel threshold counters and crystal calorimeters of the BELLE detector of the KEK B-factory. Reflectance of the Goretex surface was monitored to see any effect of the radiation damage. Maximum equivalent dose was 8.6 Mrad. No radiation damage is observed within measurement errors.", "num_citations": "8\n", "authors": ["324"]}
{"title": "Prognostic significance of c-MYC amplification in esophageal squamous cell carcinoma\n", "abstract": " BackgroundWe investigated the frequency of c-MYC amplification in esophageal squamous cell carcinoma (ESCC), including both stage I to II and III to IVa disease, and evaluated the correlation of c-MYC amplification with clinicopathologic variables and outcome.MethodsIn 259 ESCCs resected at Zhongshan Hospital, Fudan University, from January 2007 to November 2010, c-MYC amplification was analyzed by using tissue microarray, with fluorescence in situ hybridization assay.Resultsc-MYC gene amplification was found in 43.2% (112 of 259) of patients with ESCC. Significant differences were found between c-MYC amplification and patient age\u00a0(p\u00a0= 0.009) and lymph node metastasis (p\u00a0= 0.046). The median follow-up period was 33 months (range: 4\u00a0to\u00a0102 months). A survival difference was found between\u00a0patients with different c-MYC status. Among 112 patients with c-MYC amplification, a significantly\u00a0\u2026", "num_citations": "7\n", "authors": ["324"]}
{"title": "Design and implementation of a dynamic healthcare system for weight management and health promotion\n", "abstract": " Overweight and obesity have become the major public health challenges globally, since late 20 th  century and they are apparently on the rise in some upper middle- and high-income countries. Over past few years, the obesity-related research has been turning to use mobile wireless device to deliver the weight management intervention, with the tremendously increased use of mobile phones. Mobile health (mHealth)-related smartphone application (app) is brought to facilitate user to self-monitor the diet, physical activity and body weight for improving accuracy and reducing burdens. A dynamic healthcare solution called \u201cIntelligent Weight Management System\u201d (iWMS) has been developed to support user to collect and transmit the objective data in real time. User can follow the advice from healthcare professionals to not only make the changes in lifestyle and behavior, but also communicate easily with healthcare\u00a0\u2026", "num_citations": "7\n", "authors": ["324"]}
{"title": "Queueing-theory-based models for software reliability analysis and management\n", "abstract": " Software reliability is one of the most important internal attributes of software systems. Over the past three decades, many software reliability growth models have been proposed and discussed. Some research has also shown that the fault detection and removal processes of software can be described and modeled using an infinite-server queueing system. But, there is practically no company that can afford unlimited resources to test and correct detected faults in the real world. Consequently, the number of debuggers should be limited, not infinite. In this paper, we propose an extended finite-server-queueing (EFSQ) model to analyze the fault removal process of the software system. Numerical examples based on real project data are illustrated. Evaluation results show that our proposed EFSQ model has a fairly accurate prediction capability of software reliability and also depict the real-life situation of software\u00a0\u2026", "num_citations": "7\n", "authors": ["324"]}
{"title": "Tumor containing fragment number influences immunohistochemistry positive rate of HER2 in biopsy specimens of gastric cancer\n", "abstract": " HER2 assessment in biopsy specimens of gastric cancer (GC) is challenging because of the intratumoral heterogeneity. False negative results may be get because of limited biopsy material. The aim of this study is to explore how tumor-containing fragment number and biopsy specimen number affect HER2 immunohistochemistry (IHC) positive rate. Eight hundred and\u00a0ninety biopsy specimens and 459 paired resected specimens were collected. IHC staining of HER2 was performed. HER2 IHC positive (scored 3+) rate was compared based on tumor-containing fragment number, biopsy specimen number, average size and tumor tissue proportion of tumor-containing fragments. The positive predictability of biopsy specimens to resected specimens was analyzed based on tumor fragment number. HER2 IHC positive rates were 2.0, 3.5, 7.0, 13.2, 17.1, and 15.9% when tumor fragment numbers were 1, 2, 3, 4, 5 and 6 respectively. The rate rose with the increase of tumor fragment number (P\u2009=\u20090.004). ROC curve analysis showed that biopsy specimens exhibited positive predictability when tumor fragment number reached 3, but showed better performance when the number was \u22654 (P\u2009<\u20090.05). After fragment number reached 4, no statistic differences were reached in either HER2 IHC positive rate or positive predictability with further increase of the number (P\u2009>\u20090.05). HER2 IHC positive rate was not associated with biopsy number (P\u2009=\u20090.127), average size of tumor fragments (P\u2009=\u20090.397), and tumor tissue proportion of tumor fragments (P\u2009=\u20090.825) directly. The number of tumor-containing fragments influences HER2 IHC positive (scored 3\u00a0\u2026", "num_citations": "7\n", "authors": ["324"]}
{"title": "On the main signless Laplacian eigenvalues of a graph\n", "abstract": " A signless Laplacian eigenvalue of a graph  is called a main signless Laplacian eigenvalue if it has an eigenvector the sum of whose entries is not equal to zero. In this paper, we first give the necessary and sufficient conditions for a graph with one main signless Laplacian eigenvalue or two main signless Laplacian eigenvalues, and then characterize the trees and unicyclic graphs with exactly two main signless Laplacian eigenvalues, respectively.", "num_citations": "7\n", "authors": ["324"]}
{"title": "Pore-scale simulation of coupled reactive transport and dissolution in fractures and porous media using the level set interface tracking method\n", "abstract": " A level set simulation methodology developed for modeling coupled reactive transport and structure evolution has been applied to dissolution in fracture apertures and porous media. The coupled processes such as fluid flow, reactant transport and dissolution at the solid-liquid interfaces are handled simultaneously. The reaction-induced evolution of solid-liquid interfaces is captured using the level set method, with the advantage of representing the interface with sub-grid scale resolution. The coupled processes are simulated for several geometric models of fractures and porous media under various flow conditions and reaction rates. Quantitative relationships between permeability and porosity are obtained from some of the simulation results and compared with analytical constitutive relations (ie, the conventional cubic law and the Carman-Kozeny law) based on simplified pore space geometries and reaction induced geometric evolutions. The drastic deviation of the simulation results from these analytical theories is explained by the development of large local concentration gradients of reactants within fracture apertures and individual pores observed in the simulation results and consequently the complex geometric evolution patterns of fracture apertures and pores due to mineral dissolution. The simulation results support the argument that traditional constitutive relations based on simplified geometries and conditions have limited applicability in predicting field more\u00bb", "num_citations": "7\n", "authors": ["324"]}
{"title": "Method and system for assessing and analyzing software reliability\n", "abstract": " A method for assessing and analyzing software reliability comprises the steps of: collecting failure data from a software system during a testing period; providing a reliability model having a testing compression factor, wherein the reliability model is used to fit the failure data; providing an estimation function derived from the reliability model; obtaining the value of the testing compression factor in accordance with the estimation function; and using the testing compression factor to determine the efficiency of test cases.", "num_citations": "7\n", "authors": ["324"]}
{"title": "Integrating path testing with software reliability estimation using control flow graph\n", "abstract": " In this paper, we propose a new approach to calculate a pathpsilas reliability along three basic programming structures, which include sequential, branching, and looping structures. The pathpsilas reliability is then taken to approximate the systempsilas reliability, and an example is evaluated to validate and show the effectiveness of proposed method. The numerical example yields several findings. First, the proposed method can be effectively used in the early stages of testing. Second, the path reliability is highly relative to system reliability, and the estimated error of the proposed method is acceptable. Third, with higher test coverage on path testing, the accuracy of system reliability estimation can be further improved. Therefore, the proposed method is a viable alternative method for architecture-based software reliability modeling.", "num_citations": "7\n", "authors": ["324"]}
{"title": "Software reliability modeling withweibull-type testing-effort and multiple change-points\n", "abstract": " Software reliability is defined as the probability of failure-free software operation for a specified period of time in a specified environment. Over the past 30 years, many software reliability growth models (SRGMs) have been proposed for estimation of reliability growth of products during software development processes. SRGMs proposed in the literature took into consideration the amount of testing-effort spent on software testing which can be depicted as a Weibull-type curve. However, in reality, the consumption rate of testing-effort expenditures may not be a constant and could be changed at some time points. Therefore, in this paper, we will incorporate the concept of multiple change-points into the Weibull-type testing-effort function. New model is proposed and the applicability of proposed model is demonstrated through real software failure data set. Our experimental results show that the proposed model has a\u00a0\u2026", "num_citations": "7\n", "authors": ["324"]}
{"title": "Models and methods for urban power distribution network planning\n", "abstract": " The models, methods and their application experiences of a practical GIS(geographic information system)-based computer decision-making support system of urban power distribution network planning with seven subsystems, termed CNP, are described. In each subsystem there is at least one or one set of practical mathematical methobs. Some new models and mathematical methods have been introduced. In the development of GNP the idea of cognitive system engineering has been insisted on, which claims that human and computer intelligence should be combined together to solve the complex engineering problems cooperatively. Practical applications have shown that not only the optimal plan can be automatically reached with many complicated factors considered, but also the computation,analysis and graphic drawing burden can be released considerably.", "num_citations": "7\n", "authors": ["324"]}
{"title": "Measurement of radiation damage on an epoxy-based optical glue\n", "abstract": " We measured the radiation damage on an optical glue called Eccobond-24, which is a candidate for CsI and BGO crystal calorimeters of the BELLE detector of the KEK B-factory. Absorption spectrophotometry in the range 300\u2013800 nm was used to monitor the radiation damage. The maximum equivalent dose was 1.64 Mrad. The glue shows effects of damage, but is acceptable for the radiation level in the above-mentioned experiment.", "num_citations": "7\n", "authors": ["324"]}
{"title": "GNE-371, a potent and selective chemical probe for the second bromodomains of human transcription-initiation-factor TFIID subunit 1 and transcription-initiation-factor TFIID\u00a0\u2026\n", "abstract": " The biological functions of the dual bromodomains of human transcription-initiation-factor TFIID subunit 1 (TAF1(1,2)) remain unknown, although TAF1 has been identified as a potential target for oncology research. Here, we describe the discovery of a potent and selective in vitro tool compound for TAF1(2), starting from a previously reported lead. A cocrystal structure of lead compound 2 bound to TAF1(2) enabled structure-based design and structure\u2013activity-relationship studies that ultimately led to our in vitro tool compound, 27 (GNE-371). Compound 27 binds TAF1(2) with an IC50 of 10 nM while maintaining excellent selectivity over other bromodomain-family members. Compound 27 is also active in a cellular-TAF1(2) target-engagement assay (IC50 = 38 nM) and exhibits antiproliferative synergy with the BET inhibitor JQ1, suggesting engagement of endogenous TAF1 by 27 and further supporting the use of 27\u00a0\u2026", "num_citations": "6\n", "authors": ["324"]}
{"title": "The prognostic significance of MCL1 copy number gain in esophageal squamous cell carcinoma\n", "abstract": " BackgroundMCL1 copy number variations have been reported to be associated with cancer prognosis in several cancers. However, the role of MCL1 gain has not yet been determined in esophageal squamous cell carcinomas (ESCC).MethodsFluorescence in situ hybridization (FISH) for MCL1 was performed on 262 ESCC samples using tissue microarray (TMA).ResultsThe median age of ESCC patients was 62 years (range 37\u201383), with frequencies between women (16.4%) and men (83.6%). Of the 262 tumors, 77 tumors (29.4%) had high MCL1 gain. In the multivariate analysis, lymph node metastasis (HR: 3.236, P< 0.001 for DFS; HR: 3.501, P< 0.001 for OS) and clinical stage (HR: 3.388, P< 0.001 for DFS; HR: 3.616, P< 0.001 for OS) were identified as independent worse prognostic factors. Interestingly, among patients without lymph node metastasis or stage I-II patients, high MCL1 gain was associated with\u00a0\u2026", "num_citations": "6\n", "authors": ["324"]}
{"title": "Enhanced recovery in Shales: molecular investigation of CO2 energized fluid for re-fracturing shale formations\n", "abstract": " The liquid and gas rich shales are low permeability, low porosity but high organic content reservoirs. They need effective sub-surface, in-situ stimulation for economic production. Sometimes due to ineffective initial completion, the shale wells don't produce well. An effective re-stimulation or refracturing can shoot up the production from these mature low producing wells. The fracture fluid plays a key role in determining the fate of such reservoirs. Typically, slickwater or gelled water is used as a fracture-fluid. However, the energized fluids, energized with carbon dioxide (CO 2) has shown to increase the performance. They reduce the water volumes, problems associated with water usage and have superior proppant transport capabilities. The Molecular Dynamics (MD) Simulations technique is used in the current work to understand the interaction between carbon dioxide and hydrocarbons rich Type II kerogen in the\u00a0\u2026", "num_citations": "6\n", "authors": ["324"]}
{"title": "A study of applying severity-weighted greedy algorithm to software test case prioritization during testing\n", "abstract": " Regression testing is a very useful technique for software testing. Traditionally, there are several techniques for test case prioritization; two of the most used techniques are Greedy and Additional Greedy Algorithm (GA and AGA). However, it can be found that they may not consider the severity while prioritizing test cases. In this paper, an Enhanced Additional Greedy Algorithm (EAGA) is proposed for test case prioritization. Experiments with eight subject programs are performed to investigate the effects of different techniques under different criteria and fault severity. Experimental results show that proposed EAGA perform well than other techniques.", "num_citations": "6\n", "authors": ["324"]}
{"title": "Evaluation and analysis of spectrum-based fault localization with modified similarity coefficients for software debugging\n", "abstract": " During the process of fault localization, the spectrum-based techniques are frequently used and widely studied since they can automatically and effectively localize the faults of software and be implemented easily. So far most of spectrum-based fault localization techniques have relied heavily on the use of similarity coefficients. However, we noticed that existing similarity coefficients for fault localization may lack measure(s) to properly reflect the relationship between failing and passing test cases. It also has to note that the failing test cases usually are expected to provide more information to the similarity coefficients than the passing test cases. In order to evaluate the importance of failing and passing test cases in the similarity coefficients, a number of modified similarity coefficients in fault localization are presented and discussed. The modified similarity coefficients which are assigned the weights of the failing and/or\u00a0\u2026", "num_citations": "6\n", "authors": ["324"]}
{"title": "Experimental demonstration of slow self-collimated beams through a coupled zigzag-box resonator in a two-dimensional photonic crystal\n", "abstract": " We report on the experimental realization of slow self-collimated beams by using a 10-coupled zigzag-box resonator in a two-dimensional photonic crystal. The speeds of the beams are reduced to less than 0.023c at resonant frequencies where the transmission exhibit peak values. The dispersion relation and the group velocities of the beams in the coupled resonator are well described by the tight-binding model. Time-domain simulations of self-collimated pulses passing though the coupled resonator are also performed to demonstrate the slowing down the speed of the pulses. Our result could be useful in implementing devices to control self-collimated beams in time domain.", "num_citations": "6\n", "authors": ["324"]}
{"title": "Crystal Structure, Thermal Stability and Corrosion Inhibition of N, N-Diethylammonium O, O\u2032-Di (4-methylphenyl) dithiophosphate\n", "abstract": " [Et2NH2][(4-MeC6H4O) 2PS2](N, N-diethylammonium O, O\u2032-di (4-methylphenyl) dithiophosphate, NOP) was synthesized and characterized by elemental analysis, IR, TG-DSC, and single crystal X-ray diffraction analysis. The thermal stability and corrosion inhibition were investigated. The crystallographic structure analysis reveals that the title compound crystallizes in monoclinic system, P 21/n space group, a= 1.4241 (3) nm, b= 2.0087 (4) nm, c= 1.4366 (3) nm, V= 3.9599 (15) nm3, Dc= 1.286 Mg/m3, Z= 8, F (000)= 1.632, \u03bc (MoK\u03b1)= 0.360 mm-1, S= 1.001,(\u0394/\u03c3) max= 0.001, R1= 0.077 6, wR2= 0.174 9 (I> 2\u03c3 (I)). There are two independent subunits in the asymmetric structure unit of NOP, and infinite one\ue011 dimensional chain structure is formed via\ue00a NH\u2026 S hydrogen bonds between the subunits. The corrosion inhibition study showed that the NOP could effectively restrain corrosion of Q235 steel in H2SO4 solution\u00a0\u2026", "num_citations": "6\n", "authors": ["324"]}
{"title": "Isolation of a capnophilic Escherichia coli strain from an empyemic patient\n", "abstract": " Isolation of a capnophilic Escherichia coli strain from an empyemic patient \u00d7 Close The Infona portal uses cookies, ie strings of text saved by a browser on the user's device. The portal can access those files and use them to remember the user's data, such as their chosen settings (screen view, interface language, etc.), or their login data. By using the Infona portal the user accepts automatic saving and using this information for portal operation purposes. More information on the subject can be found in the Privacy Policy and Terms of Service. By closing this window the user confirms that they have read the information on cookie usage, and they accept the privacy policy and the way cookies are used by the portal. You can change the cookie settings in your browser. I accept Polski English Login or register account remember me Password recovery INFONA - science communication portal INFONA Search advanced \u00d7 to , /\u2026", "num_citations": "6\n", "authors": ["324"]}
{"title": "Optimal resource allocation and sensitivity analysis for modular software testing\n", "abstract": " Software testing takes place almost continuously throughout the software developmental life-cycle. Actually, module testing is the most detailed form of testing to be performed. The software programmer is generally responsible for testing the modules of the program, ensuring that each performs the function for which it was designed. Therefore, managers should know how to allocate the specified testing-resources among all the modules and develop quality software with high reliability. We present the optimal policies of testing-resource allocation for modular software systems. Our methodologies provide practical approaches to the optimization of testing-resource allocation with a reliability objective. Some theorems and numerical examples for the optimal testing-effort allocation policies are demonstrated. Besides, sensitivity analysis is also discussed in detail. Using the proposed strategies for module testing\u00a0\u2026", "num_citations": "6\n", "authors": ["324"]}
{"title": "Optimizing job reliability through contention-free, distributed checkpoint scheduling\n", "abstract": " A datacenter that consists of hundreds or thousands of servers can provide virtualized environments to a large number of cloud applications and jobs that value the requirement of reliability very differently. Checkpointing a virtual machine (VM) is a proven technique to improve reliability. However, existing checkpoint scheduling techniques for enhancing reliability of distributed systems fails to achieve satisfactory results, either because they tend to offer the same, fixed reliability to all jobs, or because their solutions are tied up to specific applications and rely on centralized checkpoint control mechanisms. In this work, we first show that reliability can be significantly improved through contention-free scheduling of checkpoints. Then, inspired by the Carrier Sense Multiple Access (CSMA) protocol in wireless congestion control, we propose a novel framework for distributed and contention-free scheduling of VM\u00a0\u2026", "num_citations": "5\n", "authors": ["324"]}
{"title": "Analysis of a Fault-Tolerant Framework for Reliability Prediction of Service-Oriented Architecture Systems\n", "abstract": " Service-oriented architecture (SOA) has become an increasingly popular\u00a0choice for building software application in the last years. An SOA system is an elastic structure that utilizes services discovery and integrates these services to perform specified functions. In general, reliability is a critical system attribute when evaluating the quality of a well-built software applications. But it has to be noted that the phenomenon of error propagation could have significant impacts on system reliability. Propagated errors may be masked or propagated to the system interface, which can thereby lead to a system failure. Much research on reliability evaluation for SOA systems have been proposed in the past. However, most of these studies have neglected the phenomenon of error propagation and the issue of link failure. In this article, we take a different view of error propagation, fault tolerance, and the failure behavior of links\u00a0\u2026", "num_citations": "5\n", "authors": ["324"]}
{"title": "An adaptive IO prefetching approach for virtualized data centers\n", "abstract": " Cloud and data center applications often make heavy use of virtualized servers, where flash-based solid-state drives (SSDs) have become popular alternatives over hard drives for data-intensive applications. Traditional data prefetching focuses on applications running on bare metal systems using hard drives. In contrast, virtualized systems using SSDs present different challenges for data prefetching. Most existing prefetching techniques, if applied unchanged in such environments, are likely to either fail to fully utilize SSDs, interfere with virtual machine I/O requests, or cause too much overhead if run in every virtualized instance.In this work, we demonstrate that data prefetching, when run in a virtualization-friendly manner can provide significant performance benefits for a wide range of data-intensive applications. We have designed and developed VIO-prefetching, consisting of accurate prediction of application\u00a0\u2026", "num_citations": "5\n", "authors": ["324"]}
{"title": "A patient with hemodialysis-related hyperammonemic encephalopathy: a delayed presentation of congenital arterioportal fistulas.\n", "abstract": " We present a 32-year-old woman who developed hyperammonemic encephalopathy during hemodialysis. She was rather well before 2009 when receiving peritoneal dialysis due to chronic interstitial nephritis. Due to a refractory peritonitis, the treatment was shifted to hemodialysis in January 2009. About 1 year later, she was found with consciousness disturbance during hemodialysis then admitted to the hospital because of hyperammonemia (165 \u03bcg/dl). During hospitalization, the patient's abdominal Doppler sonography showed a hepatofugal flow in the portal trunk while the hepatic artery angiography demonstrated multiple intrahepatic arterioportal fistulas. Her general condition was improved after the treatment of lactulose and hepatic artery embolization. With the occurrence of arterioportal fistulas induced portal hypertension, we speculated that the portal-systemic shunt was enhanced during hemodialysis because of venous hypotension which then resulted in the transient hyperammonemia. To the best of our knowledge, this is the first patient who developed hemodialysis-related hyperammonemic encephalopathy due to multiple arterioportal fistulas.", "num_citations": "5\n", "authors": ["324"]}
{"title": "Effects of paeoniflorin on isolated thoracic aorta rings of rats and its possible mechanism\n", "abstract": " Objective: To investigate the relaxative characteristics of paeoniflorin on thoracic aortic artery in rat and its mechanism. Method: We perfused the isolated rings and observed the response of NA-induced artery contraction to paeoniflorin under the Ca2+-contained and Ca2+-free bath solutions. In the same way the effects of paeoniflorin on the vascular smooth muscle were observed by adding KCl (60 mmol. L-1), and the effect on the contraction of the vascular smooth muscle depending on the intrace11ular calcium and extracellular calcium were also observed by adding NA. We also observed the effect of paeoniflorin on the contraction of rings induced by NA in the presence of L-NAME. Result: Peoniflorin relaxed rat aorta rings precontracted by NA in a dose-dependent manner. But had no effect on the aorta\u2019s contraction induced by intracellular calcium and extracellular calcium as well as KCL (60 mmol. L-1), the relaxant effect could be attenuated by L-NAME. There was significant change in the group of paeoniflorin (P 0.05). Conclusion: The results indicate that paeoniflorin relaxes vascular smooth muscle in an endothelium-dependent manner. The mechanisms seem to be related with promoting synthesis and release of NO, but not for opening Ca2+ activated K+ channel as well as the inhibition of Ca2+ influx and release of Ca2+ from intracellular stores.", "num_citations": "5\n", "authors": ["324"]}
{"title": "Bit-plane arithmetic coding for Laplacian source\n", "abstract": " For bit-plane coding of random source, we establish a relationship between the probability density function of an arbitrary non-negative distribution and the bit-plane symbol probability. Applied to Laplacian source, the relationship gives a simple, closed form of bit-plane symbol probability, which is a superset of the probability assignment rule adopted in the MPEG-4 Audio Scalable Lossless Coding (SLS) international standard. For bit-plane arithmetic coding of Laplacian source, we propose an algorithm and apply it to MPEG-4 SLS. Experimental results show that without changing the encoder/decoder complexity, the proposed algorithm consistently improved the compression ratio over SLS for all the 51 MPEG test sequences. The overall improvement in the compression ratio is 0.11%.", "num_citations": "5\n", "authors": ["324"]}
{"title": "Medical image integrity control seeking into the detail of the tampering\n", "abstract": " In this paper, we propose a system which aims at verifying integrity of medical images. It not only detects and localizes alterations, but also seeks into the details of the image modification to understand what occurred. For that latter purpose, we developed an image signature which allows our system to approximate modifications by a simple model, a door function of similar dimensions. This signature is partly based on a linear combination of the DCT coefficients of pixel blocks. Protection data is attached to the image by watermarking. Whence, image integrity verification is conducted by comparing this embedded data to the recomputed one from the observed image. Experimental results with malicious image modification illustrate the overall performances of our system.", "num_citations": "5\n", "authors": ["324"]}
{"title": "An analysis of adherence to ARV treatment among people living with HIV/AIDS in rural area of China and strategy for its improvement [J]\n", "abstract": " Objective To understand the factors influencing the adherence to ARV treatment among people living with HIV/AIDS in rural areas, and develop strategy for its improvemet. Method The adherence to ARV treatment and related impact factors were analyzed among 501 HIV carriers/AIDS patients coming from rural areas of Henan and Anhui province. Result The total non-adherence rate was 44.8% among all the subjects studied, of which the non-adherence in respect with time was 34.0%(165/486) and that in respect with dosage was 10.8%. The factor which influenced the adherence involved therapeutic and behavioral aspects; the former included treatment regimen and drug forms and the latter included being busy with farm work (67.3%), forgetting to take medicine (60%) and working in the city far from home (13.33%). Conclusion (1) The adherence of ARV treatment is only about 50% in rural area of China.(2) The factor which influence the adherence to ARV treatment among HIV carriers/AIDS patients living in rural areas were related with their living and working habits, but ARV treatment regimen and drug forms also play a significant role.(3) It is essential to strengthen ARV treatment education for the improvement of adherence to ARV treatment in rural areas.", "num_citations": "5\n", "authors": ["324"]}
{"title": "Software and Hardware co-design for MP3 Decoder\n", "abstract": " The RISC architecture is a load-store architecture that its data processing operations execute only on registers. The signal processing application requires many data computations. It takes many resources to execute data movements and operations. Therefore, the performance of the signal processing application is limited on traditional RISC architecture. In this paper, ARM processor and MP3 decoder are selected as the RISC architecture platform and signal processing application, respectively. Moreover, because the signal processing application can not execute efficiently on traditional RISC architecture, both software optimization and architecture enhancement will be used to improve the performance of the system. The experimental results have shown great performance improvements in the system", "num_citations": "5\n", "authors": ["324"]}
{"title": "Component-based Medical and Assistive Devices and Systems\n", "abstract": " This paper discusses foundation for component-based design and integration, development and quality assurance, and certification of medical devices and systems. In addition to medical devices used in hospitals and clinics by professionals, we also need to be concerned with assistive devices, which are used by na\u00efve users in and out of homes. Examples include medicine dispensers that can help to ensure correctness and enforce compliance of medication schedules, and monitors that can record and process vital-sign signals, detect irregularities, and send notifications. The division between assistive and medical devices is often blurry and future medical care and health delivery system will contain both. The paper first discusses needed information technologies (IT) and sciences for each of the topics listed above. The roadmap at end of the paper suggests a plan to acquire them.", "num_citations": "5\n", "authors": ["324"]}
{"title": "Influence of cadmium, lead and zinc on the growth and metal content in ryegrass\n", "abstract": " A greenhouse experiment was conducted to evaluate the effects of cadmium (Cd), lead (Pb) and zinc (Zn) on the growth and heavy metal content in ryegrass plants. The metals were applied separately at three different levels ie Cd at 10, 30 and 60 Mew g/g-1; Pb at 50, 100 and 150 Mew g/g-1 and Zn at 75, 150 and 225 Mew g/g-1 soil. Results demonstrated a significant inhibitory effect of metals on the growth and dry matter yields of ryegrass plants. The concentrations of metals both in roots and shoots increased significantly with Cd, Pb and Zn addition to the soil. The adverse effects of metals were more pronounced on roots than on the above ground plant parts. Among the tested metals, Pb displayed the greatest suppressing effect followed by Cd and Zn, indicating their relative toxicity in the order: Pb Cd Zn. It was suggested that the solubility of lead compound, presence of acetate in the soil medium, low pH of the soil and the relative tolerance of ryegrass to Cd were the factors contributing to the toxicity trend observed in the study.", "num_citations": "5\n", "authors": ["324"]}
{"title": "Pollen Analysis of Taiwan Pliocene (II)-Yunshuichi Section\n", "abstract": " Fifty-seven sample taken from the Yunshuichi section in southwestern Taiwan are palynologically analyzed; six new genera, fifty-two new species, five new records and two new combinations are proposed. The new angiospermous taxa in this study are compared with Muller's review. These new taxa accord with the time course of appearance with several areas outside Taiwan, but Fagraea and Patrinia types have not been sufficiently documented in the past. Some taxa, e.g. Tilia, Pterocarya, Carya, Ephedra, etc., are absent in the present flora of Taiwan, but are still living in southeastern mainland China; therefore the Pliocene vegetation of Taiwan might be, in part, similar to the present vegetation of southeastern mainland China. Two pollen assemblages, i.e., Subalpine-Temperate and Subtropical-Tropical assemblage are utilized to subdivide the pollen zones. Seven pollen zones are determined, the climate\u00a0\u2026", "num_citations": "5\n", "authors": ["324"]}
{"title": "Oral tartar emetic treatment of schistosomiasis japonica.\n", "abstract": " Some 3, 000 patients, of all ages from 5 years and of each sex, suffering from different stages of Schistosoma japonicum schistosoma japonicum Subject Category: Organism Names", "num_citations": "5\n", "authors": ["324"]}
{"title": "Software reliability prediction and management: A multiple change\u2010point model approach\n", "abstract": " It is commonly recognized that software development is highly unpredictable and software quality may not be easily enhanced after software product is finished. During the software development life cycle (SDLC), project managers have to solve many technical and management issues, such as high failure rate, cost over\u2010run, low quality, and late delivery. Consequently, in order to produce robust and reliable software product(s) on time and within budget, project managers and developers have to appropriately allocate limited time, manpower, development, and testing effort. In the past, the distribution of testing effort or manpower can typically be described by the Weibull or Rayleigh model. Practically, it should be noticed that development environments or methods could be changed due to some reasons. Thus, when we plan to perform software reliability modeling and prediction, these changes or variations\u00a0\u2026", "num_citations": "4\n", "authors": ["324"]}
{"title": "Treating TH2-Mediated diseases by inhibition of bromodomains\n", "abstract": " The invention provides methods for treating Th2 cytokine mediated diseases by inhibiting bromodomain function.", "num_citations": "4\n", "authors": ["324"]}
{"title": "An improved high\u2010output cell microarray technology\n", "abstract": " Aims Cell microarray (CMA) is a high\u2010throughput scientific research tool, which has greatly accelerated many analyses based at the cellular level. However, there are few described methods for constructing CMAs. Here, we introduce a new, simple, high\u2010output CMA method that is applicable to a broad range of cellular samples.   Methods In this method, a recipient block (length, 3.6\u00a0cm; width, 2.7\u00a0cm; depth, 2\u00a0cm) with 40 dot markers was moulded using a transparent plastic box. Adenocarcinoma cells were collected from malignant pleural effusions, cell cylinders were moulded with plastic piping and the cylinders were manually arrayed one by one into the corresponding location of the 60\u00a0\u00b0C pre\u2010softened recipient block using the guide holes drilled with a steel needle. We constructed a 40\u2010cylinder CMA to prove this method. The expression of cytokeratin 7 (CK7) in the CMA was examined to confirm antigen\u00a0\u2026", "num_citations": "4\n", "authors": ["324"]}
{"title": "Diagnostic application of PIK3CA mutation analysis in Chinese esophageal cancer patients\n", "abstract": " The PIK3CA gene mutation was found to associate with prognosis and might affect molecular targeted therapy in esophageal carcinoma (EC). The aim of this study is to compare different methods for analyzing the PIK3CA gene mutation in EC. Genomic DNA was extracted from 106 surgically resected EC patient tissues. The PIK3CA mutation status (exons 9 and 20) were screened by mutant-enrich liquid chip (ME-Liquidchip), Sanger sequencing, and pyrosequencing. And all samples with mutations were independently reassessed using amplification refractory mutation system (ARMS) methods again. PIK3CA mutation rates were identified as 11.3% (12/106) by ME-Liquidchip. 10 mutations occurred in exon 9 and 2 in exon 20, including G1624A:E542K (n\u2009=\u20094), G1633A:E545K (n\u2009=\u20096) and A3140G:H1047R (n\u2009=\u20092). The results were further verified by ARMS methods. Among these 12 cases characterized for\u00a0\u2026", "num_citations": "4\n", "authors": ["324"]}
{"title": "\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u53ef\u80fd\u6ee1\u610f\u5ea6\u7684\u7fa4 AHP \u5224\u65ad\u77e9\u9635\u96c6\u7ed3\u65b9\u6cd5\n", "abstract": " \u9488\u5bf9\u5c42\u6b21\u5206\u6790\u6cd5 (AHP) \u5224\u65ad\u77e9\u9635\u7fa4\u51b3\u7b56\u95ee\u9898, \u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5: \u5b9a\u4e49\u53ef\u80fd\u5ea6\u548c\u6ee1\u610f\u5ea6\u6307\u6807, \u5206\u522b\u8861\u91cf\u5c5e\u6027\u6392\u5e8f\u7ea6\u675f\u4e0b\u5408\u6210\u77e9\u9635\u7684\u4e00\u81f4\u6027\u7a0b\u5ea6\u548c\u5408\u6210\u77e9\u9635\u4e0e\u7fa4\u7ec4\u5224\u65ad\u77e9\u9635\u7684\u5dee\u5f02\u7a0b\u5ea6; \u5229\u7528\u6a21\u7cca\u4e92\u8865\u5224\u65ad\u77e9\u9635\u7684\u7ebf\u6027\u548c\u8fde\u7eed\u6027, \u8ba1\u7b97\u4e0e\u7fa4\u7ec4\u5224\u65ad\u77e9\u9635\u5dee\u5f02\u6700\u5c0f\u7684\u6700\u4f18\u53ef\u80fd\u5ea6\u77e9\u9635\u548c\u65e0\u7ea6\u675f\u7684\u6700\u4f18\u6ee1\u610f\u5ea6\u77e9\u9635; \u91c7\u7528\u4e24\u4e2a\u6700\u4f18\u77e9\u9635\u4e0a\u4e09\u89d2\u5143\u7d20\u7684\u51e0\u4f55\u5e73\u5747, \u83b7\u53d6\u6ee1\u8db3\u6700\u4f18\u53ef\u80fd\u6ee1\u610f\u5ea6\u7684\u7fa4\u51b3\u7b56\u5408\u6210\u77e9\u9635. \u6700\u540e\u901a\u8fc7\u7b97\u4f8b\u8868\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027.", "num_citations": "4\n", "authors": ["324"]}
{"title": "Detection of KRAS gene mutation and its clinical significance in colorectal adenocarcinoma\n", "abstract": " To explore the clinical significance of KRAS mutation detection in colorectal adenocarcinoma. Paraffin-embedded tissue specimens were obtained from 440 patients with colorectal adenocarcinoma. The genomic DNA was extracted. Mutations of exon 2 of KRAS gene were examined by PCR and direct sequencing. Somatic mutations of KRAS gene were identified in 146 cases, with the mutation rate of 33.2%(146/440). Among these 146 patients, KRAS mutation involved codon 12 in 118 patients, including 35G> A (Gly12Asp, 62 cases), 35G> T (Gly12Val, 35 cases), 34G> T (Gly12Cys, 9 cases), 34G> A (Gly12Ser, 6 cases), 35G> C (Gly12Ala, 5 cases), and 34G> C (Gly12Arg, 1 case); in 27 patients the mutation involved codon 13, including 38G> A (Gly13Asp, 25 cases), 38G> C (Gly13 Val, 1 case) and 37G> T (Gly13 Cys, 1 case); and in one patient, the mutation involved codon 14 with 40G> A (Val14Ile). The status of KRAS or codon 12 mutations in colorectal adenocarcinoma was related to patients' gender (P= 0.021 and P= 0.030, respectively), and this significant correlation to females was conserved in clinical stage III (P= 0.007 and P= 0.003, respectively), but not in stages I, II, and IV. The status of KRAS or codon 12 mutations was also related to tumor stage. Between stage II and stage IV, the mutation rate of KRAS and codon 12 showed significant difference (P= 0.028 and 0.034, respectively). Between stage III and stage IV, only the codon 12 mutation rate showed significant difference (P= 0.011). Codon 13 mutation was not related to tumor stage. About one third of patients with colorectal adenocarcinoma have KRAS gene mutation, which\u00a0\u2026", "num_citations": "4\n", "authors": ["324"]}
{"title": "An investigation of classification-based algorithms for modified condition/decision coverage criteria\n", "abstract": " During software development, white-box testing is used to examine the internal design of the program. One of the most important aspects of white-box testing is the code coverage. Among various test coverage measurements, the Modified Condition/Decision Coverage (MC/DC) is a structural coverage measure and can be used to assess the adequacy and quality of the requirements-based testing (RBT) process. NASA has proposed a method to select the needed test cases for satisfying this criterion. However, there may have some flaws in NASA's method. That is, the selected test cases may not satisfy the original definition of the MC/DC criterion in some particular situations and perhaps can not detect errors completely. On the other hand, NASA's method may be hard to detect some operator errors. For example, we may not be able to detect the incorrectly coding or for xor in some cases. Additionally, this method\u00a0\u2026", "num_citations": "4\n", "authors": ["324"]}
{"title": "Thermo-hydro-mechanical modeling of working fluid injection and thermal energy extraction in EGS fractures and rock matrix\n", "abstract": " Development of enhanced geothermal systems (EGS) will require creation of a reservoir of sufficient volume to enable commercial-scale heat transfer from the reservoir rocks to the working fluid. A key assumption associated with reservoir creation/stimulation is that sufficient rock volumes can be hydraulically fractured via both tensile and shear failure, and more importantly by reactivation of naturally existing fractures (by shearing), to create the reservoir. The advancement of EGS greatly depends on our understanding of the dynamics of the intimately coupled rock-fracture-fluid-heat system and our ability to reliably predict how reservoirs behave under stimulation and production. Reliable performance predictions of EGS reservoirs require accurate and robust modeling for strongly coupled thermal-hydrological-mechanical (THM) processes. Conventionally, these types of problems have been solved using operator-splitting methods, usually by coupling a subsurface flow and heat transport simulators with a solid mechanics simulator via input files. An alternative approach is to solve the system of nonlinear partial differential equations that govern multiphase fluid flow, heat transport, and rock mechanics simultaneously, using a fully coupled, fully implicit solution procedure, in which all solution variables (pressure, enthalpy, and rock displacement fields) are solved simultaneously. This paper describes numerical simulations used to investigate the poro-and thermal-more\u00bb", "num_citations": "4\n", "authors": ["324"]}
{"title": "Peer\u2013to\u2013Peer Video\u2013On\u2013Demand service in NUWeb\n", "abstract": " Because of the continuing growth of video quality and ease of producing video content, traditional centralised video streaming approaches have difficulty in coping with the increasing users' demand in the age of Web 2.0. On the other hand, Peer\u2013to\u2013Peer (P2P) video streaming that only involve peers cannot guarantee reliable Quality of Service (QoS) because of the transient nature of peers. To achieve both scalability and reliability, we present a proxy\u2013assisted P2P Video\u2013On\u2013Demand (VOD) system based on the Net User's Web (NUWeb). In our system, client nodes are self\u2013organised into a mesh\u2013like P2P system to actively participate in the video delivery. A Grey Relation Analysis (GRA)\u2013based peer selection and a deadline\u2013based piece scheduling are proposed to effectively select suitable parents and timely switch parents to provide high video quality. Based on a structured P2P, proxies are also partially\u00a0\u2026", "num_citations": "4\n", "authors": ["324"]}
{"title": "Method for generating test cases for software program\n", "abstract": " The method for generating test cases for a software program includes the step of setting a plurality of reference points in accordance with a sentence of the software program. The tracing pairs each including an initial test case as well as its adjacent vertex are set if one of them is among the reference points and the other one is not among the reference points. The essential test cases are chosen from the tracing pairs.", "num_citations": "4\n", "authors": ["324"]}
{"title": "Blind forensics in medical imaging based on Tchebichef image moments\n", "abstract": " In this paper, we present a blind forensic approach for the detection of global image modifications like filtering, lossy compression, scaling and so on. It is based on a new set of image features we proposed, called Histogram statistics of Reorganized Block-based Tchebichef moments (HRBT) features, and which are used as input of a set of classifiers we learned to discriminate tampered images from original ones. In this article, we compare the performances of our features with others proposed schemes from the literature in application to different medical image modalities (MRI, X-Ray ...). Experimental results show that our HRBT features perform well and in some cases better than other features.", "num_citations": "4\n", "authors": ["324"]}
{"title": "Malicious Code Behavior Analysis Based on DynamoRIO\n", "abstract": " This paper proposes a method based on dynamic binary analysis to analyze malicious code behavior and designs and implements a prototype malicious behavior analysis system based on DynamoRIO. Experimental results show that the system can capture Application Programming Interface (API) functions calling sequence and transfer parameter information completely. Based on correlative analysis of the calling sequence and the parameter information, malicious behaviors which cover files, the registry, services, processes, threads and so on are identified.", "num_citations": "4\n", "authors": ["324"]}
{"title": "Method for Estimating Software Development Effort\n", "abstract": " A method for estimating software development effort comprises the steps of: generating a database containing a plurality of source softwares; calculating the Grey relational coefficients between the software to be developed and a source software in the database for each feature they exhibit; calculating the weights for each Grey relational coefficient; multiplying each Grey relational coefficient with the corresponding weight; calculating the Grey relational grade by summing up the products produced in the multiplying step; calculating the Grey relational grades for all remaining source softwares in the database; and comparing the Grey relational grades to estimate the effort for developing the software to be developed.", "num_citations": "4\n", "authors": ["324"]}
{"title": "Design and analysis of a fully non-blocking quantum switch\n", "abstract": " A switching device is required to build a realistic network which can handle the explosive growth of network traffic. However, the drawback of the switching device is mainly focused on the blocking problem. Previous studies have presented the solutions in the quantum context to avoid blocking at the expense of increasing packet loss and the exponential size of quantum gates. In this paper, we design a fully non-blocking quantum switch using the linear number O(N 2 ) of quantum SWAP gates. Comparisons with a previously proposed quantum self-routing packet switch is also presented in terms of the hardware complexity, the propagation delay and packet loss probability", "num_citations": "4\n", "authors": ["324"]}
{"title": "Dependable WDM networks with reconfigurable edge-disjoint p-cycles\n", "abstract": " In this paper, we propose a fault tolerant mechanism on the optical WDM network design with edge-disjoint p-cycles (EDPC) that is reconfigurable according failure scenarios. EDPC is a special type of traditional p-cycles, that is, no common edges are allowed to exist between any two p-cycles. One p-cycle provides guaranteed protection against to single link failure, but it can't protect against simultaneous failures to two links on one p-cycle. If failures of multiple links happen in such way, all working spans protected by the p-cycle will crumble down. Our method called local reconfiguration recovers the protection environment to enable its protection ability. The results show that the EDPC method can tolerate more link failures and improve the restoration efficiency for traditional p-cycles with the decrease of two working units for every two p-cycles. When the protection environment crumbles down, the local\u00a0\u2026", "num_citations": "4\n", "authors": ["324"]}
{"title": "Modeling and prediction of software operational reliability\n", "abstract": " In this paper, how several existing software reliability growth models based on Non-homogeneous Poisson Processes (NHPPs) are derived based on a unified theory for NHPP models is described. Within this general framework, existing NHPP models are versified and new NHPP models are derived. The approach includes a number of known models under different conditions. Based on these approaches, a method of estimating and computing software reliability growth during operational phase is shown and using this method, the transitions from testing phase to operational phase is described. That is, a method for predicting the fault detection rate to reflect changes in the user\u2019s operational environments is proposed. The proposed method offers a quantitative analysis on software failure behavior in field operation and provides useful feedback information to the development process.", "num_citations": "4\n", "authors": ["324"]}
{"title": "Paleomagnetic Study on the Oil-Gas Geology of the Ordos Basin [J]\n", "abstract": " The results of paleomagnetic research show that the Ordos Basin was located within the paleolatitude of 14~ 20 N during the Cambrian and Early-middle Ordovician Periods with a warmer and moist climate, of which the littoral-neritic sedimentary strata in the north and south margins were very rich in organic materials, being favorable fields to find Early Paleozoic oil and gas. From the Late Ordovician to Early Carboniferous, the Ordos Basin may have a large-scale NS horizontal tectonic displacement. In the Late Carboniferous and Permian Period, the location of Ordos Basin was at around 20 N with widespread limnogenic deposits, being important source rocks of Coal-oil and coal-gas. From the Triassic to Jurassic Period, the Ordos Basin was located at 24~ 31 N. And the change of arid and humid climate and the differential tectonic rotations among the Ordos Block and other blocks resulted in favorable conditions for oil and gas formation and accumulation within the marginal areas of the basin. Thus, much more Mesozoic oil and gas reources should be found hopefully.", "num_citations": "4\n", "authors": ["324"]}
{"title": "Studies on the period of flower bud differentiation of apple trees\n", "abstract": " Pascal 002 Biological and medical sciences/002A Fundamental and applied biological sciences. Psychology/002A32 Agronomy. Soil science and plant productions/002A32E Economic plant physiology/002A32E06 Growth and development/002A32E06D Flowering, floral biology, reproduction patterns", "num_citations": "4\n", "authors": ["324"]}
{"title": "Fusion proteins and methods for identifying bromodomain inhibiting compounds\n", "abstract": " Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.) 2013-03-15", "num_citations": "3\n", "authors": ["324"]}
{"title": "Microseismic Correlation and Cluster Analysis of DOE EGS Collab Data\n", "abstract": " Enhanced geothermal systems (EGS) can enrich the US energy portfolio by increasing the amount of renewable baseload power available to the electrical grid. To advance EGS technological readiness, meso-scale field studies were funded by the Geothermal Technologies Office (GTO) to validate subsurface reservoir models using observed geophysical data, borehole characterization data, and fluid flow information. The EGS Collab initiative is putting this collaborative multi-Laboratory and multi-university experiment into practice at the Sanford Underground Research Facility (SURF), formerly the Homestake Mine, in South Dakota. At the first site, established approximately 1.5 kilometers beneath the ground surface, 18 3-component accelerometers, four 3-component geophones, and two strings of 12-sensor hydrophones were deployed in 6 observation wells and in shallow boreholes near the drift to monitor fluid movement from the injection borehole to the production borehole.Here we present results using data from the microseismic monitoring network to correlate and cluster microseismic events occurring before, during and after fluid injection stimulation tests between May-July 2018. Using microearthquakes identified using traditional short-term-average/long-term-average (STA/LTA) techniques, we correlate the known microseismicity with one another to characterize the similarity of seismic events within the fracture network. A comprehensive examination of the locations of these clustered events with complementary fracture information can inform us as to the character of the microearthquakes with respect to the developing and existing\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "Imaging Hydraulic Fracture Propagation Using Semi-Permanent Continuous Active Seismic Source Monitoring: Results from the DOE EGS Collab Experiment\n", "abstract": " Engineering subsurface permeability, through the creation of new fractures or re-activation of existing networks, is a grand challenge problem in the geosciences with applications to enhanced geothermal system (EGS) development and unconventional oil and gas production. No routine measurement approach is capable of mapping induced fracture networks away from the wellbore; while microseismic event (MEQ) clouds are used as a proxy for identifying the stimulated region, studies suggest that such hypocenter distributions, while informative, do not fully delineate the region with flowing fractures. An alternative strategy is the collection of high repeatability active seismic data that capture changes in seismic velocity and attenuation; such an approach has the advantage that changes can be imaged in aseismic regions.", "num_citations": "3\n", "authors": ["324"]}
{"title": "Quasistatic Discrete Element Modeling of Hydraulic and Thermal Fracturing Processes in Shale and Low-Permeability Crystalline Rocks\n", "abstract": " A quasistatic discrete element model (DEM) has been developed and coupled with heat conduction model and network flow models to provide mechanistically based simulations of crack propagations within low-permeability heterogeneous rocks under hydraulic or thermal stimulations. For crack growth in hot brittle crystalline rocks, driven by thermal cooling, the coupled DEM\u2013heat conduction model clearly predicts that a quasihierarchical array of subparallel cracks, oriented along the direction of the temperature gradient, is formed under small-to-moderately large thermally generated strain load conditions. The coupled DEM\u2013network flow model enables realistic simulations of hydraulic fracturing process in low-permeability rocks with both local heterogeneity due to variations of the mineral fabric and larger-scale stratigraphic heterogeneities that can be conveniently incorporated into the model. The simulation\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "Stochastic modelling and simulation approaches to analysing enhanced fault tolerance on service\u2010based software systems\n", "abstract": " Presently, service\u2010based software systems (SBSSs) have been heavily deployed to fulfil the functionalities of cloud computing and are widely used in many other application fields. Additionally, maintaining functionality and quality of service levels becomes increasingly important for SBSSs; this is because system operational failures may cause great financial loss to an organization. Fault tolerance (FT) is usually used to provide continuous and reliable system service delivery when failures occur. However, the reliability and performance of FT should be carefully analysed because of the overhead of invoking redundant services. It is also noted that the single point of failure on the FT adjudicators as well as the failure correlation also hamper the benefits of FT in SBSSs. To address these problems, this paper proposes two approaches, the stochastic modelling approach and the simulation approach, for analysing the\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "Optimizing job reliability via contention-free, distributed scheduling of vm checkpointing\n", "abstract": " Checkpointing a virtual machine (VM) is a proven technique to improve the reliability in modern datacenters. Inspired by the CSMA protocol in wireless congestion control, we propose a novel framework for distributed and contention-free scheduling of VM checkpointing to offer reliability as a transparent, elastic service in datacenters. In this work, we quantify the reliability in closed form by studying system stationary behaviors, and maximize the job reliability through utility optimization. We implement a proof-of-concept prototype based on our design. Evaluation results show that the proposed checkpoint scheduling can significantly reduce the performance interference from checkpointing and improve reliability by as much as one order of magnitude over contention-oblivious scheme.", "num_citations": "3\n", "authors": ["324"]}
{"title": "Understanding reliability implication of hardware error in virtualization infrastructure\n", "abstract": " Hardware errors are no longer the exceptions in modern cloud data centers. Although virtualization provides software failure isolation across different virtual machines (VM), the virtualization infrastructure including the hypervisor and privileged VMs remains vulnerable to hardware errors. Making matters worse is that such errors are unlikely bounded by virtualization boundary and may lead to loss of work in multiple guest VMs due to unexpected and/or mishandled failures. To understand reliability implication of hardware errors in virtualized systems, in this paper we develop a simulation-based framework that enables a comprehensive fault injection study on the hypervisor with a wide range of configurations. Our analysis shows that, in current systems, many hardware errors can propagate through various paths for an extended time before an observed failure (eg, whole system crash). We further discuss the challenges of designing error tolerance techniques for the hypervisor.", "num_citations": "3\n", "authors": ["324"]}
{"title": "Ambient noise tomography for wavespeed and anisotropy in the crust of southwestern China\n", "abstract": " The primary objective of this thesis is to improve our understanding of the crustal structure and deformation in the southeastern Tibetan Plateau and adjacent regions using surface wave tomography. Green's functions for Rayleigh and Love waves are extracted from ambient noise interferometry. Using the Green's functions, we first conduct traditional traveltime tomography for the two shear wavespeeds Vsv and Vsh Their differences are measured as radial anisotropy. We then conduct Eikonal tomography to study azimuthal anisotropy in the crust. Our tomography results are well consistent with geology in the study region. In the Sichuan Basin, low wavespeed and positive radial anisotropy (Vsh> Vsv) in the upper crust reflect thick sedimentary layers at surface; high wavespeed and small radial anisotropy in the middle and lower crust reflect a cold and rigid basin root. Little azimuthal anisotropy is observed in the Basin, indicating small internal deformation. In the Tibetan Plateau, we observe widespread low wavespeed zones with positive anisotropy in the middle and lower crust, which may reflect combined effects of weakened rock mechanism and horizontal flow in the deep crust of southeastern Tibet. The northern part of the Central Yunnan block, which geographically coincides with the inner zone of the Emeishan flood basalt, reveals relatively higher wavespeeds than the surrounding regions and little radial anisotropy throughout the entire crust. We speculate that the high wavespeeds and small radial anisotropy are due to combined effects of the remnants of intruded material from mantle with sub-vertical structures and channel flow with sub\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "An octachlorostyrene electrochemical immunosensor: double amplification strategies with immobilization of nano-Au and Au nanoparticle labels\n", "abstract": " Anti-octachlorostyrene (OCS) antibody was derived from an immune rabbit preparation. An OCS immunosensor was constructed by immobilizing the anti-OCS antibody on a glassy carbon electrode coated with chitosan and gold nanoparticles (AuNPs, \u223c5 nm, represented as AuNP05). Large-sized AuNPs (\u223c90 nm, represented as AuNP90) were used as the electrochemical label. The AuNP90-labeled OCS competes with the target OCS for the limited antibody molecules immobilized on the sensor surface. The amount of bound AuNP90 is inversely proportional to the OCS concentration. OCS was quantified based on the bound AuNP90 which was detected by differential pulse voltammetry (DPV), i.e. the AuNP90 was firstly electrooxidized in 0.1 M HCl to produce AuCl4\u2212, then the reduction current of AuCl4\u2212 was detected. The immobilized AuNP05 increases the loading of anti-OCS antibody. Both the immobilized\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "Software quality assurance methodologies and techniques\n", "abstract": " Software quality assurance (SQA) is a planned and systematic pattern of actions necessary to provide adequate confidence that a software product conforms to requirements during software development. SQA consists of methodologies and techniques of assessing the software development processes and methods, tools, and technologies used to ensure the quality of the developed software. SQA is typically achieved through the use of well-defined standard practices, including tools and processes, for quality control to ensure the integrity and reliability of software. This special issue presents new research works along these directions, and we received 21 submissions and accepted five of them after a thorough peerreview process. The acceptance rate of this special issue is around 24%. The resultant collection provides a number of useful results. These accepted papers cover a broad range of topics in the research field of SQA, including software validation, verification, and testing, SQA modeling, certification, evaluation, and improvement, SQA standards and models, SQA case studies, data analysis and risk management.", "num_citations": "3\n", "authors": ["324"]}
{"title": "A study of using two-parameter generalized pareto model to analyze the fault distribution of open source software\n", "abstract": " In the modern society, software plays a very important part in many security-critical or mission-critical systems. Consequently, the main goal of project managers and software engineers is to develop and deliver reliable software within very limited resource, time, and budget. In the past, some research reports showed that the Weibull distribution (WD) and the Pareto distribution (PD) models can be used to describe the distribution of software faults. In this paper, based on our previous study, we further propose and show how the two-parameter generalized Pareto distribution (2-GPD) can be used to model the distribution of software faults. Some mathematical properties of proposed model are analyzed and presented. Experiments based on open source software (OSS) are performed and discussed in detail. Evaluation results show that the proposed 2-GPD model eliminates some issues in modeling that arise in the\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "Adjoint tomography using Green's functions from ambient noise\n", "abstract": " We model empirical Green's Functions (EGFs) from ambient noise interferometry using spectral-element method (SEM) by putting point force at the station locations of a temporary array (Oct. 2003-Sept. 2004) in the southeastern Tibet and the Sichuan Basin. The synthetic EGFs calculated from a 3-D model by Yao et al.(2008) well capture the characteristics of the waveforms of observed EGFs in the period band of 10-40 s. We also benchmark this 3-D model with a different dataset of regional seismic events. The data and 3-D synthetics traveltime misfits histogram distribution of 5,141 measurements centers around zero, presenting a significant improvement from the the initial three-layered model (Crust2. 0), which produces synthetics 3s faster than the data overall. This result is consistent with the existence of the ubiquitous low velocity zones in the mid-lower crust. The regional seismic events mostly locate on the\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "9.1 Models for thermal transport properties of oil shale\n", "abstract": " Models for estimating the heat capacity and thermal conductivity of Green River oil shale as a function of temperature and grade are presented. Oil shale heat capacity is based on the weighted sum of the temperature-dependent heat capacities of the components of formation (mineral, kerogen, coke) and their expected transformations. The mineralogical component is based on published heat capacities of the individual minerals. The heat capacity of kerogen is an adaptation of a model for the heat capacity of coal. Oil shale pyrobitumen and coke heat capacities are based on thermal studies of coke. This model is compared to approximately 200 laboratory heat capacity measurements on oil shale. The thermal conductivity of low-grade oil shale is constrained by measurements of limestone while the thermal conductivity of high-grade oil shale is constrained by the reported thermal conductivities of coal. The thermal\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "Teleseismic P-wave tomography of the crust and upper mantle in Longmenshan area, west Sichuan\n", "abstract": " The 3D P-wave velocity structure of the crust and upper mantle within the depth range of 400 km was obtained by using teleseismic traveltimes data recorded by West Sichuan Seismic Array distributed in Longmenshan region and non-linear tomography inversion technique. For adapting to the complicated structure, the fast marching traveltime calculation method and Tarantola's inversion method was used. Our results show the tectonic differences of the crustal and upper-mantle structure among the blocks of Chuandian, Songpan-Garze and Sichuan basin. Our results show that: 1) the crustal structure of the study area correlates with the surface geological features. The Sichuan basin is imaged as a high-velocity feature, while the Songpan-Garze and Chuandian black as low-velocity feature. The lithosphere thickness of Sichuan basin has lateral variations from 250 km in south part to 100 km in north part. There\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "A protein chip detecting 5 antibodies against mycobacterium tuberculosis: evaluation in diagnosis of tuberculosis [J]\n", "abstract": " Objective To detect 5 antibodies against tuberculoprotein, LAM, CFP10, ESAT-6 and 2 proteins at molecular weight of 38 kD and 16 kD by a protein chip and explore its clinical value in diagnosis of tuberculosis. Methods The serum antibodies from 50 tuberculosis patients and 80 non-tuberculosis patients were analyzed by this protein chip, and the diagnostic value of the test was evaluated. Results As was shown in the 130 clinical samples, the sensitivity, specificity, accuracy and Youden index of the diagnostic test were 92.0%, 77.5%, 83.1% and 69.5% respectively. Conclusion The tuberculosis protein chip assay is of high value in diagnosis of both pulmonary and non-pulmonary tuberculosis patients, and has good coincidence with gold standard. It is time-saving and easy to sampling and practicing, and can be regarded as supplemental item for non-pulmonary tuberculosis.", "num_citations": "3\n", "authors": ["324"]}
{"title": "Effect of fault dependency and debugging time lag on software error models\n", "abstract": " In this paper, we first show how several existing SRGMs based on NHPP models can be comprehensively derived by applying the time-dependent delay function. Moreover, for most conventional SRGMs, they assume that detected errors are immediately corrected. But this assumption may not be realistic in practice. Therefore, we incorporate the ideas of failure dependency and time-dependent delay function into software reliability growth modeling. New SRGMs are proposed and numerical illustrations based on real data set are presented. Evaluation results show that the proposed framework to incorporate both failure dependency and time-dependent delay function for SRGM has a fairly accurate prediction capability.", "num_citations": "3\n", "authors": ["324"]}
{"title": "Considering fault dependency and debugging time lag in reliability growth modeling during software testing\n", "abstract": " Since the early 1970s tremendous growth has been seen in the research of software reliability growth modeling. In general, software reliability growth models (SRGMs) are applicable to the late stages of testing in software development and they can provide useful information about how to improve the reliability of software products. For most existing SRGMs, most researchers assume that faults are immediately detected and corrected. However, in practice, this assumption may not be realistic and satisfied. In this paper we first give a review of fault detection and correction processes in SRGMs. We show how several existing SRGMs based on NHPP models can be comprehensively derived by applying the time-dependent delay function. Furthermore, we show how to incorporate both failure dependency and time-dependent delay function into software reliability growth modeling. We present stochastic reliability\u00a0\u2026", "num_citations": "3\n", "authors": ["324"]}
{"title": "Key\n", "abstract": " Priority date (The priority date is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the date listed.) 1996-09-06", "num_citations": "3\n", "authors": ["324"]}
{"title": "Conceptual design of the Extreme Forward Calorimeter\n", "abstract": " The advantages of installing an extreme forward calorimeter (EFC) at BELLE are discussed. Conceptual designs of the extreme forward calorimeter as well as results from a GEANT simulation of the expected performances are presented.", "num_citations": "3\n", "authors": ["324"]}
{"title": "Studies on the Excimer Fluorescence and the Steric Tacticity of Polyphenylsilsesquioxanes\n", "abstract": " The steric tacticity of polyphenylsilsesquioxanes (PPS) was studied by exeimer fluorescence spectroscopy. It was found that the fluorescence of PPS come mainly from its intramolecular excimers, The experimental results indicated that the steric-structure of PPS is most probably cisisotactic rather than the cis-syndiotactic as suggested in the literature. With the increase in defect content monomer fluorescence intensity increases because of the enhanced mobility of phenyl groups. Molecular weights and molecular weight distributions do not affect the fluorescence spectra of PPS.", "num_citations": "3\n", "authors": ["324"]}
{"title": "Coupled Physics Simulation of Expansive Reactions in Concrete with the Grizzly Code\n", "abstract": " The Grizzly code is being developed under the US Department of Energy\u2019s Light Water Sustainability program as a tool to model aging mechanisms and their effects on the integrity of critical nuclear power plant components. An important application for Grizzly is the modeling of aging in concrete structures, which can be due to a number of mechanisms. Initial focus in this area has been on modeling expansive reactions due to alkali-silica reactions or radiation-induced volumetric expansion. Grizzly is an inherently multiphysics modeling platform that naturally permits including the effects of multiple coupled physics in a simulation. Models have been developed for transport of heat and moisture in concrete, and these have been coupled and used as inputs to models for expansive reactions. This paper summarizes this capability, and demonstrates it on a representative structure.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Growth and Characterization of In x Ga 1\u2212 x N Multiple Quantum Wells Without Phase Separation\n", "abstract": " Efficient conversion of photon energy into electricity is a crucial step toward a sustainable solar-energy economy. Likewise, solid-state lighting devices are gaining prominence because of benefits such as reduced energy consumption and reduced toxicity. Among the various semiconductors investigated, In                   x                 Ga1\u2013x                 N alloys or superlattices are fervently pursued because of their large range of bandgaps between 0.65\u00a0eV and 3.4\u00a0eV. This paper reports on the fabrication of multiple quantum wells on LiGaO2 (001) substrates by plasma-assisted molecular beam epitaxy. Metal modulated epitaxy was utilized to prevent formation of metal droplets during the growth. Streaky patterns, seen in reflection high-energy electron diffraction, indicate two-dimensional growth throughout the device. Postdeposition characterization using scanning electron microscopy also showed smooth surfaces\u00a0\u2026", "num_citations": "2\n", "authors": ["324"]}
{"title": "Performance analysis of cell-phone worm spreading in cellular networks through opportunistic communications\n", "abstract": " Starting on the 15th of June 2020 we wiil introduce a new policy for reviewers. Reviewers who provide timely and substantial comments will receive a discount voucher entitling them to an APC reduction. Vouchers (worth of 25 EUR or 50 EUR, depending on the review quality) will be assigned to reviewers after the final decision of the reviewed paper is given. Vouchers issued to specific individuals are not transferable.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Corelation between Expression of AQP1, AQP5 mRNA and Lung Injury in Rats with Lung-defensive Sluggishness Syndrome and Effect of Rhubarb on It [J]\n", "abstract": " Objective: To observe the correlation between the expression of AQP1, AQP5 mRNA and the lung injury in the rats with lung sluggishness syndrome and the regulative effect of rhubarb on it, to explore the material foundation of the lung defensive sluggishness syndrom and the mechanism of rhubarb in the prevention and treatmentof the lung injury. Method: Lipopolysaccharide (LPS) was injected into SD rats to perform lung-defensivesluggishness syndrome. Sixty rats was randomly divided into 5 groups: control group, lung-defensive sluggishnessgroup, natural recovery after 5 days group, rhubarb prevention group (9 g\u00b7 kg-1\u00b7 d-1 for 2 days before modelestablishment) and rhubarb treatment group (9 g\u00b7 kg-1\u00b7 d-1for 5 days after model establishment). Physiological andpathological staining was performed and real-time PCR was used to observe the expression of AQP1, AQP5 mRNA inthe pulmonary tissue. Result: The pulmonary edema could be showed obviously in the lung-defensive sluggishnessmodel and rhubarb could alleviate it. Compared with the control group, expression of AQP1, AQP5 mRNA of themodel group was decreased, thus the natural recovery after 5 days group was elevated; no significantly difference canbe seen between rhubarb prevention group and rhubarb treatment group. Conclusion: The pulmonary edema andabnormal expression of AQP1, AQP5 mRNA can be showed in lung-defensive sluggishness model and the rhubarb caninhibit them, which is one of the important mechanisms in protecting the lung injury of lung-defensive sluggishnesssyndrome.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Effect of Rhei Radix et Rhigoma on AQP_ (1, 5) mRNA Expression of Alveolar Type \u2161 Cell Injured by LPS [J]\n", "abstract": " Objective: To observe effect of Rhei Radix et Rhigoma on AQP1, 5 mRNA expression of alveolar type \u2161cell (AT\u2161) injured by LPS. Method: Primary cultured alveolar type \u2161cells were divided into 5 groups: control group: 2mL DMEM cultured medium was added; LPS injured cell model group (acute lung injury cell model group): 10 \u03bcg\u00b7 mL-1 LPS was added to injure AT \u2161 cell; serum containing Rhei Radix et Rhigoma intervention group was divided into 3 groups: each group was added the total volume of 5%, 10%, 20% serum containing Rhei Radix et Rhigoma. Which were collected After 4 hours of culture the mRNA expression of AQP1, AQP5 was detected by Real-Time PCR. Result: Compared with the control group, AQP1 mRNA expression of LPS injured cell model group was significantly lower (P0. 05). Compared with the LPS group, 20% of total volume of serum containing Rhei Radix et Rhigoma intervention group could increase AQP1 mRNA expression significantly, but not return to the normal level. Compared with the control group, AQP5 mRNA expression of LPS injured cell model group was increased. Rhei Radix et Rhigoma intervention groups could decrease AQP5 mRNA expression, but not significantly. Conclusion: The protective mechanism of Rhei Radix et Rhigoma on acute lung injury cell model by LPS may be associated with the increased AQP1 mRNA expression and the decreased AQP5 mRNA expression.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Energy Consumption Analysis and Improvement Strategy for Cluster-Head Rotation in Wireless Sensor Networks [J]\n", "abstract": " To design a more energy-efficient cluster head rotation (CHR) strategy for wireless sensor networks, this paper built an analytical model for the energy consumption of contention-based CHR, analyzed the energy consumption ratio between data gathering and CHR, and proposed a schedule-based CHR, which could remove the energy consumption in CHR. The simulation results show that the schedule-based CHR obviously improves energy efficiency and energy consumption balance in the cluster.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Modeling of mechanical interactions of proppant and hydraulic fractures for in-situ oil shale retorting\n", "abstract": " Several in-situ oil shale retorting strategies require creation of either vertical or horizontal hydraulic fractures and injection of proppant to facilitate the flow of generated hydrocarbon fluid. An important issue is to reliably model the mechanical interactions between proppants and hydraulic fractures during heating and to quantify/predict the degree of proppant embedment into the shale matrix and associated reduction in fracture aperture under both thermal stress and confining stress (ie overburden). An extended 2D discrete element model (DEM) that incorporates the effect of plastic deformation of oil shale was developed and applied to the problem of modeling proppant-fracture mechanical interactions. The softening of the shale rock due to retorting and the development of localized plasticity zones near the surface of fracture walls was shown to be critical to the degree of proppant embedment and fracture closure\u00a0\u2026", "num_citations": "2\n", "authors": ["324"]}
{"title": "Modeling of calcite precipitation driven by bacteria-facilitated urea hydrolysis in a flow column using a fully coupled, fully implicit parallel reactive transport simulator\n", "abstract": " One approach for immobilizing subsurface metal contaminants involves stimulating the in situ production of mineral phases that sequester or isolate contaminants. One example is using calcium carbonate to immobilize strontium. The success of such approaches depends on understanding how various processes of flow, transport, reaction and resulting porosity-permeability change couple in subsurface systems. Reactive transport models are often used for such purpose. Current subsurface reactive transport simulators typically involve a de-coupled solution approach, such as operator-splitting, that solves the transport equations for components and batch chemistry sequentially, which has limited applicability for many biogeochemical processes with fast kinetics and strong medium property-reaction interactions. A massively parallel, fully coupled, fully implicit reactive transport simulator has been developed based\u00a0\u2026", "num_citations": "2\n", "authors": ["324"]}
{"title": "HiCloud: Taming clouds on the clouds\n", "abstract": " The proposed project is aimed to extend current computationally-intensive weather forecasting research on geographically distributed resources for faster results delivery and improved scalability. To achieve this goal we take advantage of the emerging cloud computing paradigm [1],[2],[3],[4],[5] to enable very large-scale weather forecasting applications in the proposed cloud infrastructure. Such an integration is promising but challenging due to increasing complexity and overheads in resource management and job scheduling. This project involves interdisciplinary and multisite collaboration. The main tasks are as follows.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Reliable Hierarchical Peer-to-Peer File Sharing Systems\n", "abstract": " This paper presents an approach for hierarchical peer-to-peer (P2P) file-sharing systems in the design of the fault tolerance. In the hierarchical P2P system, peer nodes are organized into groups, and each group has a special peer node called superpeer to manage the regular peer nodes in the group. With this hierarchical characteristic, the proposed approach utilizes the superpeer node to tolerate the failures of the regular peer nodes. Unlike traditional P2P fault-tolerant approaches, the proposed fault-tolerant approach neither replicates the contents of the sharing files nor adds redundant overlay connections. Instead, it keeps track of the file retrievals in the superpeer node to support the fault tolerance. The cost of tracking the file retrievals is far smaller than the cost of replicating the file content. Furthermore, the proposed approach also considers the fault tolerance of the superpeer node by using the registration\u00a0\u2026", "num_citations": "2\n", "authors": ["324"]}
{"title": "Abrupt changes in an 8000-year precipitation reconstruction for Nevada, the western USA\n", "abstract": " A scanningt-test algorithm for detecting multiple time-scale abrupt changes in the level of a time-series was used to analyze an 8000 year time series of annual precipitation which was reconstructed from tree rings for the Nevada Climate Division 3 in the western USA. The tree ring samples were gathered from eight states in the southwestern USA. Twenty-two change-points were identified by the algorithm and these were used to partition the tree-ring series into twenty-three relatively Wet/Normal/Dry episodes. These twenty-three episodes were collaborated by a coherency analysis of abrupt changes between the precipitation reconstruction series and the TIC/\u03b418O records from cored sediments of Pyramid Lake in Nevada, and by comparison with published results from related studies. These episodes were also compared with studies of the global climate change and with records of climate change in\u00a0\u2026", "num_citations": "2\n", "authors": ["324"]}
{"title": "Tolerating multiple faults in wdm networks withoutwavelength conversion\n", "abstract": " This paper addresses the problem of tolerating as many faults as possible in wavelength division multiplexing (WDM) networks without the capability of wavelength conversion. The problem of finding the maximum number of faults that can be tolerated is modeled as a constrained ring cover set problem, which is a decomposition problem with exponential complexity. The Face Decomposition Algorithm (FDA) that can tolerate one or more faults is proposed. From the results, we know that the maximum number of faults tolerated can be extended from one significantly under various network topologies.", "num_citations": "2\n", "authors": ["324"]}
{"title": "A dynamical redirection approach to enhancing mobile IP with fault tolerance in cellular systems\n", "abstract": " The paper investigates the reliability of Mobile IP in a cellular system. To support the node mobility, a lot of mobility agents are deployed in the packet network of a cellular system to retain the continuous network connectivity while mobile nodes move their locations. If a mobility agent fails, the mobile nodes under its coverage area will be affected. The network connectivity of these mobile nodes will be disrupted. The main goal of this paper is to present an efficient approach to tolerating the failures of mobile agents. Once failures occur in a mobility agent, its in-progress and new-arrival workload can be redirected to other mobility agents. The mobile nodes under the mobility agent's coverage area do not lose their data executive ability. The overhead of the proposed fault-tolerant approach is also measured in terms of the performance degradation on other mobility agents by using an M/G/m/m queueing mode. The\u00a0\u2026", "num_citations": "2\n", "authors": ["324"]}
{"title": "Impact of Some Agronomic Practices on Paddy Field Soil Health Under Varied Ecological Conditions: \u2161. Influence of Soile Temperature\n", "abstract": " A  21-d incubation experiment was conducted under controlled laboratory conditions to study the effects of elevated temperatures(10,25,and 40\u2103)on some microbiological and biochemical properties in flooded paddy soil amended or unamended with urea at 100\u03bcg N g^-1 soil and /or insecticide(triazophos) at field rate(FR).Enhancements in temperature led to increase the electron transport system(ETS)/dehydrogenase activity and phospholipid contents of the soil,while soil organic matter phenol and protein contents decreased with increasing tenperature with or wihtout the addition of inputs.An incerease of temperature from 10\u2103 to 25 or 40\u2103 enhanced the ETS activity 2 folds (on avergae for all soils),while the inclusion of N and insecticide increased and decreased it ,respectively,compared to the control.The soil phenol and protein contents were highly correlated with temperatures(for all soils,r=-0.936 and -0.971,respectively)and the additions of N and insecticide produced slight reductions and enhancemetns in them,respectively,At a particular tempeature,the soil protein contents remained unaffected among all the soil treatments.An overall slight increase in phospholipid contents with N and a small decline with insecticide addition were noticed against the untreated soil.the toxcity of fertilizer and insecticide decrased as the incubation temperature increased suggesting faster degradation of agrochemicals with raising temperature.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Assessment of two sulfonylurea herbicides on soil microbial biomass N and N-mineralization\n", "abstract": " Assessment of two sulfonylurea herbicides on soil microbial biomass N and N- mineralization FAO_logo home-icon English Espa\u00f1ol Fran\u00e7ais \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u4e2d\u6587 \u0420\u0443\u0441\u0441\u043a\u0438\u0439 home-icon Translate with Google Access the full text NOT AVAILABLE Lookup at Google Scholar google-logo Bibliographic information Language : English Type : Summary In AGRIS since : 2004 Volume : 41 Issue : 1-2 Start Page : 187 End Page : 203 All titles : \" Assessment of two sulfonylurea herbicides on soil microbial biomass N and N- mineralization \" Other : \" Summaries (Ar, En) \" \" 4 tables; 34 ref. \" Save as: AGRIS_AP RIS EndNote(XML) Assessment of two sulfonylurea herbicides on soil microbial biomass N and N- mineralization Loading... Paper Written Paper Assessment of two sulfonylurea herbicides on soil microbial biomass N and N- mineralization [2001] El-Ghamry, AM (Mansoura Univ. (Egypt). Faculty of Agriculture) Xu, JM Huang, CY the --\u2026", "num_citations": "2\n", "authors": ["324"]}
{"title": "EFC-A small angle BGO calorimeter for BELLE\n", "abstract": " A pure BGO electromagnetic calorimeter at small angles has been designed and prototyped for the BELLE detector at the KEK B-factory. It has the capability of detecting minimum ionizing particles in addition to having good energy and position resolution. Physics motivation, design solutions, simulations, and test results are reported.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Spent Electrons in BELLE\n", "abstract": " Spent electrons and positrons are generated and simulated through the beam pipes using DECAY TURTLE. The spectra of their position and momentum before they reach the sensitive volume of BELLE are calculated and installed in the BELLE Full Simulator (TDR design). One can invoke these distributions in the Full Simulator to study the trigger rate and radiation damage. Direct hit rates on and radiation damage of the Extreme Forward Calorimeter (EFC) are estimated.", "num_citations": "2\n", "authors": ["324"]}
{"title": "Application of chemically modified probe-atomic absorption spectrometry (CMPAAS)-I. Determination of Bi in copper alloy and lead by trioctylphosphine oxide-coated tungsten\u00a0\u2026\n", "abstract": " A trioctylphosphine oxide (TOPO) coated tungsten probe has been prepared and used for the determination of Bi and the \u03bcg/g level in copper alloy and lead. Bismuth is preconcentrated on the probe which is placed in a graphite-cup furnace for AA determination of bismuth. The method has good sensitivity and minimises analysis time. Microgram per gram levels of Bi can be determined with good precision and accuracy.", "num_citations": "2\n", "authors": ["324"]}
{"title": "EFFECT OF HEAT ON BUD-BREAK IN APPLE\n", "abstract": " Apple shoots of cultivars requiring low, intermediate and high chilling were excised from trees in the orchard in late November 1979, packed in damp peat moss and stored at 4 C. Shoot samples were removed from the peat at approximately monthly intervals through early April, held at 25 C in containers of water and observed over a 3 week period. Similar samples, chilled for various time periods, were placed in waterproof plastic bags, immersed in water and given a heat treatment of 39 C for 2 hours to test the ability of heat to overcome dormancy due to insufficient chilling.(Longer heating periods of 4 and 6 hours at 39 were tried without additional benefit). Observations were made for bud break as well as vigor of growth from brusting buds.", "num_citations": "2\n", "authors": ["324"]}
{"title": "An Empirical Study on Factors Influencing Consumer Adoption Intention of an AI-Powered Chatbot for Health and Weight Management.\n", "abstract": " The research of mobile health (mHealth) application interventions has attracted considerable attention among researchers. The convenience and ubiquity of smartphones makes them an ideal vehicle by which to use mHealth APPs for the self-monitoring of one's health throughout the day. This study utilized the tenets of the extended unified theory of acceptance and use of technology (UTAUT2) as our theoretical foundation. We also considered innovativeness and network externality in seeking to investigate the determinants of one's intention to adopt a chatbot for health and weight management. The health chatbot running on the Line\u2122 APP platform features artificial intelligence (AI) technology to facilitate accurate analysis and health consultations in near real-time. In the analysis of 415 responses, the proposed model explained 87.1% of variance in behavioral intention. Habit was the independent variable with\u00a0\u2026", "num_citations": "1\n", "authors": ["324"]}
{"title": "Multiphysics Platform for Simulation of Concrete Degradation\n", "abstract": " This report documents the development of computational tools for concrete degradation processes and their effects on structures in the BlackBear multiphysics simulation environment. This project has developed capabilities for nonlinear mechanical constitutive modeling of concrete, and for simulation of multiple interacting chemical species involved in important concrete degradation mechanisms, coupled with heat and moisture transport models. Two nonlinear mechanical constitutive models implemented in BlackBear: a simple damage mechanics model, and a more comprehensive damaged plasticity model. Also, a coupled species transport model based on the Nernst-Planck equations has been implemented.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Discrete and continuum plane-hopper flow simulations with experimental validation using x-ray imaging, arch profilometry and wall pressure measurements\n", "abstract": " The flow of spherical glass beads, approximately cubic wood crumbles\u00ae, and wood microchips are investigated using a combination of discrete element modeling (DEM), finite element analysis (FEA), and physical experiments using a custom plane-flow hopper in which the flow is characterized using x-ray imaging, profilometry to measure arching geometry and pressure measurements at the wall. The purpose of this study is to investigate fundamental flow behavior, and for this purpose the simulations and physical tests are specially designed. During the physical tests and simulations, the hopper geometry begins as a V shape with the two opposing side walls touching at the bottom of the V. Basic tests are performed by first filling the hopper with material, and then rotating the opposing side walls inward about their common intersection axis to cause an approximately uniform stress condition throughout most of the hopper. The side walls then slide upward to effectively increase the size of the opening at the bottom of the hopper until the arch of material in the hopper breaks, and all of the material falls out of the hopper. More advanced tests are also performed in which the initial rotation of the walls is slightly reversed to decrease the more\u00bb", "num_citations": "1\n", "authors": ["324"]}
{"title": "Light water reactor sustainability program: Survey of models for concrete degradation\n", "abstract": " Concrete is widely used in the construction of nuclear facilities because of its structural strength and its ability to shield radiation. The use of concrete in nuclear facilities for containment and shielding of radiation and radioactive materials has made its performance crucial for the safe operation of the facility. As such, when life extension is considered for nuclear power plants, it is critical to have predictive tools to address concerns related to aging processes of concrete structures and the capacity of structures subjected to age-related degradation. The goal of this report is to review and document the main aging mechanisms of concern for concrete structures in nuclear power plants (NPPs) and the models used in simulations of concrete aging and structural response of degraded concrete structures. This is in preparation for future work to develop and apply models for aging processes and response of aged NPP concrete structures in the Grizzly code. To that end, this report also provides recommendations for developing more robust predictive models for aging effects of performance of concrete.", "num_citations": "1\n", "authors": ["324"]}
{"title": "An updated research in the \u201capplicable technology\u201d of vernacular architecture in arid area with the view of resource pattern\n", "abstract": " According to analyzing the basic pattern of construction resources in arid area, this paper summaries a series of coping strategies about the traditional vernacular architecture under the limited construction resources. It points out that regional\" applicable technology\" plays an important positive role in developing the traditional vernacular architecture, supporting and sustaining the ecological security pattern of rural settlement. The thesis updates the direction of possibly existed earth applicable technology with contemporary applicable technology's development and the study of cases. It is a positive way to vernacular architecture in arid area for reference in the future.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Preliminary research on the role of rho family RhoA, RhoB, RhoC mRNA expression in endometriosis\n", "abstract": " Objective: To detect Rho family (RhoA, RhoB, RhoC) mRNA expression and study the role of Rho family in endometriosis. Method: Quantified RT-PCR technique was used in the detection. Result: RhoA and RhoC mRNA expression in endometriosis was lower than that in normal endometrium markedly (P0. 05, vs normal). RhoB mRNA expression didn't show any distinction between the groups. Conclusion: Endometriosis may be related to the low expression of RhoA and RhoC.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Network protocol reverse parsing technique based on dataflow analysis\n", "abstract": " Reverse parsing unknown network protocol is of great significance in many network security applications. Most of the existing protocol reverse parsing methods can not handle the encryption protocol or get the semantic information of the protocol field. To solve this problem, a network protocol parsing technique based on dataflow analysis was proposed. According to the data flow recording tool developed on Pin platform, it could parse the network protocol with the aid of the dependence analysis based data flow tracking technology, as well as obtain the protocol format and semantic information of each protocol field. The experimental results show that the technique can parse out the protocol format correctly, especially for the encryption protocol, and extract the program behavior semantics of each protocol field.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Comparative performance evaluation of applying extended PIE technique to accelerate software testability analysis\n", "abstract": " The rapid development of technology provides high performance and reliability for the hardware system; based on this, software engineers can focus their developed software on more convenience and ultra-high reliability. To reach this goal, the testing stage of software development life cycle usually takes more time and effort due to the growing complexity of the software. How to build software that can be tested efficiently has become an important topic in addition to enhancing and developing new testing methods. Thus, research on software testability has been conducted and various methods have been developed. In the past, a dynamic technique for estimating program testability was proposed and called propagation, infection and execution (PIE) analysis. Previous research studies have shown that PIE analysis can complement software testing. However, this method requires a lot of computational overhead in\u00a0\u2026", "num_citations": "1\n", "authors": ["324"]}
{"title": "The influences of immiscible N_2 WAG flooding in low permeability reservoir of Ordos Basin\n", "abstract": " Aim To research the infuences of immiscible N2 WAG flooding in low permeability reservoir of Ordos Basin, and provide the basis for optimizing the embodiment. Methods The physical simulation experiments and theoretical studies were uesed to analyse the effcets of different injection patterns and the WAG radio and the number of WAG slugs to the displacement efficiency. Results The final displacement efficiency of M2 WAG after gas injection is 10.42% more than the M1 WAG after water flooding; under the condition of the same injected volume of N2, the combination of the 1\u2236 2 WAG radio and two WAG slug is the optimal injection case. Conclusion For the low permeability reservoir of Ordos Basin, immiscible N2 WAG flooding is a good measure to improve the displacement effect after water flooding; the advanced gas (N2) injection can not only maintain the reservior pressure to enhance the oil recovery, but save the water resource.", "num_citations": "1\n", "authors": ["324"]}
{"title": "\u9e2d\u809d\u708e\u75c5\u6bd2 GD \u682a\u7684\u5206\u79bb\u4e0e RT-PCR \u9274\u5b9a\u5206\u6790\n", "abstract": " \u672c\u7814\u7a76\u5206\u79bb\u4e861\u682a\u9e2d\u809d\u708e\u75c5\u6bd2(duck hepatitis virus,DHV),\u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u5206\u578b\u7814\u7a76.\u53d6\u5e7f\u4e1c\u67d0\u9e2d\u573a\u7591\u4f3cDHV\u81f4\u6b7b8\u65e5\u9f84\u96cf\u9e2d\u809d\u810f,\u809d\u810f\u7ec4\u7ec7\u5904\u7406\u540e\u7ecf\u9e2d\u80da\u63a5\u79cd\u5206\u79bb\u7eaf\u5316\u75c5\u6bd2.\u5206\u79bb\u6bd2\u682a\u7ecf\u52a8\u7269\u56de\u5f52\u8bd5\u9a8c,ELD50\u6d4b\u5b9a,\u4e2d\u548c\u8bd5\u9a8c\u7b49\u786e\u5b9a\u4e3a\u8840\u6e05\u2160\u578bDHV,\u540c\u65f6\u901a\u8fc7RT-PCR\u68c0\u6d4b\u53ca\u5e8f\u5217\u5206\u6790,\u4ece\u57fa\u56e0\u5206\u578b\u89d2\u5ea6\u4e5f\u9a8c\u8bc1\u5206\u79bb\u6bd2\u682a\u4e3aDHV-\u2160,\u5e76\u5c06\u8be5\u5206\u79bb\u6bd2\u547d\u540d\u4e3aGD\u682a.", "num_citations": "1\n", "authors": ["324"]}
{"title": "\u4e24\u682a\u732a\u7e41\u6b96\u4e0e\u547c\u5438\u5f81\u75c5\u6bd2\u7684\u5206\u79bb\u9274\u5b9a\u53ca\u9057\u4f20\u53d8\u5f02\u5206\u6790\n", "abstract": " \u4ece\u5e7f\u4e1c\u7701\u53d1\u75c5\u732a\u573a\u91c7\u96c6\u7684\u7ec4\u7ec7\u548c\u8840\u6e05\u4e2d\u5206\u79bb\u5230\u4e24\u682aPRRSV,\u75c5\u6599\u7ecfRT-PCR\u68c0\u6d4b\u4e3a\u9633\u6027,\u901a\u8fc7Marc-145\u7ec6\u80de\u8fdb\u884c\u4f20\u4ee3,\u53ef\u4ea7\u751f\u660e\u663e\u7ec6\u80de\u75c5\u53d8,\u901a\u8fc7\u95f4\u63a5\u514d\u75ab\u8367\u5149\u53ef\u68c0\u6d4b\u5230\u8367\u5149\u4fe1\u53f7,\u4e24\u682a\u75c5\u6bd2\u5206\u522b\u547d\u540d\u4e3aZH-GD\u682a\u548cZS-GD\u682a.\u5bf9\u4e24\u682a\u75c5\u6bd2\u7684ORF5\u548cNsp2\u9ad8\u53d8\u533a\u8fdb\u884c\u5e8f\u5217\u6d4b\u5b9a\u548c\u5206\u6790,\u7ed3\u679c\u8868\u660e,PRRSV ZH-GD\u682a\u548cZS-GD\u682a\u7684\u6838\u82f7\u9178\u5e8f\u5217\u4e0e\u6b27\u6d32\u578b\u4ee3\u8868\u682aLV\u682a\u95f4\u7684\u76f8\u4f3c\u6027\u76f8\u5bf9\u8f83\u8fdc,\u4e0e\u7f8e\u6d32\u682a\u7ecf\u5178\u6bd2\u682aVR-2332\u95f4\u7684\u76f8\u4f3c\u6027\u5206\u522b\u4e3a88.6%\u548c88.1%,\u4e0e\u4e2d\u56fd\u7ecf\u5178\u7f8e\u6d32\u6bd2\u682aCH-1a\u95f4\u7684\u76f8\u4f3c\u6027\u5206\u522b\u4e3a94.4%\u548c93.0%.\u5728Nsp2\u4e0a\u670930\u4e2a\u6c28\u57fa\u9178\u7684\u7f3a\u5931,\u4e0eJXA1,XH-GD\u7b49\u9ad8\u81f4\u75c5\u6027\u53d8\u5f02\u682a\u7684Nsp2\u7f3a\u5931\u4f4d\u7f6e\u4e00\u81f4.\u5206\u79bb\u7684\u4e24\u682aPRRSV\u5747\u5c5e\u4e8e\u7f8e\u6d32\u578b\u7684\u53d8\u5f02\u682aPRRSV.", "num_citations": "1\n", "authors": ["324"]}
{"title": "An investigation into whether the NHPP framework is suitable for software reliability prediction and estimation\n", "abstract": " Many software reliability growth models (SRGMs) based on non-homogeneous Poisson process (NHPP) framework have been proposed for estimating the reliability growth of products. However, some concerns regarding the properties of NHPP framework were exposed and discussed while the NHPP models have received considerable attention. Two main concerns are (I) the variance of an NHPP-based model grows as software testing proceeds, which was considered an unreasonable NHPP property for describing software failure behavior; and (II) the numbers of failures observed in disjoint time intervals are independent, which may fails in the early stage of software testing. With regard to Concern (I), we will justify the validity of NHPP framework through a mathematical perspective, i.e. the process of parameter estimation for NHPP models. Considering Concern (II), we will explain why NHPP SRGMs are still\u00a0\u2026", "num_citations": "1\n", "authors": ["324"]}
{"title": "Changes of Caulis Aristolochiae manshuriensis extract-induced chronic nephrotoxicity and relationship between changes and plasma concentration\n", "abstract": " Objective: To observe the renal changes of aristolochic acid-induced nephrotoxicity by long-term using different dose of Caulis Aristolochiae Manshuriensis Extract (CAME) in rats, and investigate the relationship between the changes and blood aristolochic acid-A concentration. Method: Male SD rats were randomized into 5 groups: normal diet control group, low-salt diet control group, Low-, Mid-, and High-dose groups. Renal function and morphological changes in renal tissue were observed at 4 and 6 weeks after treatment with CAME, respectively. The aristolochic acid-A concentration in blood was determined by LC-MS-MS method. Result: Compared with low-salt diet control group, the urine volumes in 3 groups were increased at the 2nd weeks. At the 4th weeks, the urine volumes were decreased; other renal changes in low-and mid-dose groups were not obvious; the levels of urine protein and BUN were remarkably increased in high-dose group. At the 6th weeks, the urine volume was decreased in high-dose group and the levels of urine protein, Crea and BUN were significantly increased. Morphologic changes consisted of vacuolization of renal tubular cells, hyalinization of small arterioles and the light renal interstitial fibrosis. All the injury changes in High-dose group were more severe than those in low-and mid-dose groups. The parameters in two control groups had no statistical significance. The plasma aristolochic acid-A concentration was very low. Conclusion: Long-term use of CAME in rats results in renal function and morphological changes, which correlated with time and dosage of used CAME and might be independence to\u00a0\u2026", "num_citations": "1\n", "authors": ["324"]}
{"title": "Comparative Analysis on Soil Physi-Chemical Properties and the Tree Growth in Taiwania flousiana Plantations and Successive Rotation Plantations of Cunninghamia lanceolata [J]\n", "abstract": " Soil physi-chemical properties and the tree growth were compared in successive rotation plantations of Cunninghamia lanceolata (CR Chinese Fir) in tree different ages (8-year-old, 11-year-old and 14-year-old), and Taiwania flousiana plantations with the same age pattern which were afforested in previous slash of Chinese Fir. The results showed that soil bulk densities were 0. 893-1. 112, 0. 797-1. 256 and 0. 598-1. 211 g. cm-3 respectively in 8, 11, 14-year-old Taiwania flousiana plantations soils (0~ 100 cm), which were lower 0. 9%-6. 9% than those in CR Chinese Fir plantations. Total soil porosity in the three ages of Taiwania flousiana plantations was respectively 45. 69%-65. 33%, 46. 45%-68. 74% and 39. 82%-68. 26%, and all higher 6. 6%-8. 7% than that in the CR Chinese Fir plantations. The soil ventilation degree was respectively 49. 57%-59. 71%, 44. 55%-66. 68% and 42. 38%-66. 16%, and also higher 14. 2%-24. 4% than that in the CR Chinese Fir plantations. The soil saturation moisture capacity was respectively 3. 43%-11. 43%, 4. 15%-19. 73% and 2. 28%-12. 23%, and higher 12. 4%-14. 9% than that in the CR Chinese Fir plantations; The saturated water contents were 46. 89%-66. 50%, 40. 90%-68. 09% and 42. 36%-98. 15% respectively, and higher 12. 4%-14. 9% than that in the CR Chinese Fir plantations. pH values of forest soil (0-40 cm layer) were 4. 30-5. 10, 4. 40-4. 80 and 4. 13-4. 55 respectively in 8, 11, 14-years-old of Taiwania flousiana plantations, which were lower 1. 3%-6. 3% than that in the CR Chinese Fir plantations, and the organic matter contents were 46. 03-71. 45, 34. 96-68. 67 and 25. 59-79. 38 g\u00b7 kg-1\u00a0\u2026", "num_citations": "1\n", "authors": ["324"]}
{"title": "Quantifying the Influences of Imperfect Debugging on Software Development Using Simulation Approach\n", "abstract": " Practical experiences indicate that imperfect debugging actually exists in software development. In addition to inherent faults, additional faults may be introduced into software system during debugging process. Therefore, the debugging team should be staffed with more personnel to fix the introduced faults and ensure the quality of software system. To address this problem, we apply G/G/m queueing model to describe debugging behavior under imperfect debugging environment. Based on the proposed simulation framework, we investigate the influences of imperfect debugging on staffing needs. The application of the proposed framework will be illustrated through a real data set. From the simulation results, project managers can be aware of the relationship between the staffing needs and the degree of imperfect debugging.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Market based resource allocation for differentiated quality service levels\n", "abstract": " The emergence of the Grid makes it possible for researchers and scientists to access and share computation and storage resources across the organization boundaries. In a Grid environment, individual users have their own service requirements; that is, they may demand different levels of quality of service (QoS) in terms of availability, reliability, capacity, performance, security, and so on. Each QoS property imposes various constraints and performance tradeoffs. Because of the complexity of managing client-specific QoS requirements and the dynamism inherent in supply and demand for resources, even highly experienced system administrators find it difficult to manage the resource allocation process. In the real world, markets are used to allocate resources when there are competing interests, with a common currency used as the means to place a well-understood and comparable value on items. Given the nature of distributed resource management, it is natural to combine economic methods and resource management in Grid computing to provide differentiated quality service levels. A market-based model is appealing in Grids because it matches the reality of the situation where clients are in competition for scarce resources. It holds the hope that it will provide a simple, robust mechanism for determining", "num_citations": "1\n", "authors": ["324"]}
{"title": "Facial rehabilitation with implant-retained prostheses: a 16-year perspective\n", "abstract": " METHODS:103 patients underwent reconstruction for auricular (56), orbital (38) and nasal (9) defects. Eighteen underwent auricular reconstruction after failed autologous procedures, while the others presented after trauma, burns or tumor extirpation. All patients with orbital and nasal defects had undergone tumor extirpation.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Biogenic Porosity and its Lattice Boltzmann Method Permeability in the Karst Biscayne Aquifer\n", "abstract": " We focus on two major problems in the study of paleokarst of the Biscayne aquifer in southeastern Florida:(1) current conceptual models of karst aquifers do not adequately characterize much oTthe eogenetic macroporc system within the carbonate rocks of the Biscayne aquifer, and (2) standard laboratory core-anaiysis methods cannot be used lo accurately measure the permeability of highly macroporous carbonate core samples.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Development Report on the Idaho National Laboratory Sitewide Three-Dimensional Aquifer Model\n", "abstract": " A sub-regional scale, three-dimensional flow model of the Snake River Plain Aquifer was developed to support remediation decisions for Waste Area Group 10, Operable Unit 10 08 at the Idaho National Laboratory (INL) Site. This model has been calibrated primarily to water levels and secondarily to groundwater velocities interpreted from stable isotope disequilibrium studies and the movement of anthropogenic contaminants in the aquifer from facilities at the INL. The three-dimensional flow model described in this report is one step in the process of constructing a fully three-dimensional groundwater flow and contaminant transport model as prescribed in the Idaho National Engineering and Environmental Laboratory Operable Unit 10-08 Sitewide Groundwater Model Work Plan. An updated three-dimensional hydrogeologic conceptual model is presented along with the geologic basis for the conceptual model. Sediment-dominated three-dimensional volumes were used to represent the geology and constrain groundwater flow as part of the conceptual model. Hydrological, geochemical, and geological data were summarized and evaluated to infer aquifer behavior. A primary observation from development and evaluation of the conceptual model was that relative to flow on a regional scale, the aquifer can be treated with steady-state conditions. Boundary conditions developed for the three-dimensional flow model are presented along with more\u00bb", "num_citations": "1\n", "authors": ["324"]}
{"title": "Grignard reagents-benzylmagnesium synthesized using a sonochemistry method [J]\n", "abstract": " Grignard reagent benzylmagnesium chloride was synthesized using a sonochemistry method. The effects of solvent types, temperature, raw material mol ratio, magnesite powder states and stir situation on the yield of products under fixed ultrasonic power and frequency was studied. Based on the studies the orthogonal experiment was carried out to optimize the factors. Gas chromatogram was performed to analyze the products qualitatively and quantitatively, showing the optimum conditions: fresh magnesium powder and benzyl chloride as raw material, solvent being THF, ether or solution of THF and ether, water bath temperature 20 C and unstirred, and the yield 81%.", "num_citations": "1\n", "authors": ["324"]}
{"title": "The cost of transparency: grid-based file access on the avaki data grid\n", "abstract": " Grid computing has been a hot topic for a decade. Several systems have been developed. Despite almost a decade of research and tens of millions of dollars spent, uptake of grid technology has been slow. Most deployed grids are based on a toolkit approach that requires significant software modification or development. An operating system technique used for over 30 years has been to reduce application complexity by providing transparency, e.g. file systems mask details of devices, virtual machines mask finite memory, etc. It has been argued that providing transparency in a grid environment is too costly in terms of performance. This paper examines that question in the context of data grids by measuring the performance of a commercially available data grid product \u2013 the Avaki Data Grid (ADG). We present the architecture of the ADG, describe our experimental setup, and provide performance results\u00a0\u2026", "num_citations": "1\n", "authors": ["324"]}
{"title": "Full restoration of multiple faults in WDM networks without wavelength conversion\n", "abstract": " This paper addresses the problem of achieving full restoration and tolerating as many faults as possible in wavelength division multiplexing (WDM) networks without the capability of wavelength conversion. The problem of finding the maximum number of faults that can be tolerated is modeled as a constrained ring cover set problem, which is a decomposition problem with exponential complexity. The face decomposition algorithm (FDA) that can tolerate one or more faults is proposed. From the results, we know that the maximum number of faults tolerated can be extended from one significantly under various network topologies.", "num_citations": "1\n", "authors": ["324"]}
{"title": "\u5f71\u54cd\u786b\u9178\u751f\u4ea7\u50ac\u5316\u5242\u4f4e\u6e29\u6d3b\u6027\u7684\u51e0\u4e2a\u56e0\u7d20\n", "abstract": " \u7814\u7a76\u4e86\u7528\u78b3\u5316\u6bcd\u6db2\u5236\u5907\u7684\u4f4e\u6e29\u786b\u9178\u50ac\u5316\u8346\u4e2d\u94ef, \u67b7, \u9502\u7b49\u78b1\u91d1\u5c5e\u5143\u7d20\u7684\u52a9\u50ac\u5316\u4f5c\u7528\u548c\u7194\u76d0\u7528\u91cf\u5bf9\u50ac\u5316\u8346\u4f4e\u6e29\u6d3b\u6027\u7684\u5f71\u54cd. \u86c4\u679c\u8868\u660e, \u94ef\u7684\u4f4e\u6e29\u52a9\u50ac\u5316\u6548\u679c\u6700\u4f73, \u5176\u7528\u91cf\u5b9c\u63a7\u5236\u5728 Cs/V= 0.3~ o. 6. \u5f53 Cs/V= o. 6 \u65f6, 410\u2103 \u7684 SO: \u8f6c\u5316\u7387\u8fed 42.5%, \u6bd4\u65e0\u94ef\u50ac\u5316\u5242\u7684\u8f6c\u5316\u7387\u9ad8 9.5%. \u94c0\u7684\u4f4e\u6e29\u52a9\u50ac\u725d\u4f5c\u7528\u6bd4\u949d\u4f4e, \u5b9c\u91c7\u53d6\u63aa\u65bd, \u9002\u5f53\u63d0\u9ad8\u6bcd\u6db2\u7684 Cs/Rb. \u5728\u591a\u79cd\u78b1\u91d1\u5c5e\u5143\u7d20\u6df7\u5408\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b, \u9502\u5143\u7d20\u6709\u8f83\u597d\u7684\u52a9\u50ac\u5316\u4f5c\u7528, \u5176\u6700\u4f73\u7528\u91cf\u4e3a \u201c/V \u4e00 0.4, \u6b64\u65f6 410\u2103 \u7684 s02 \u8f6c\u5316\u7387\u9009 46%. \u6789\u76d0\u7684\u6dfb\u52a0\u987a\u5e8f\u5bf9\u50ac\u5316\u8346\u4f4e\u6e29\u6d3b\u6027\u4ea6\u6709\u4e00\u5b9a\u5f71\u54cd. \u53d1\u73b0\u6d3b\u6027\u86c6\u4efd\u542b\u91cf\u65f6\u50ac\u5316\u8346\u4f4e\u6e29\u6d3b\u6027\u548c\u9ad8\u6e29\u6d3b\u6027\u5f71\u54cd\u8f83\u5927, M/V \u5b9c\u5c0f\u4e8e 4.5.", "num_citations": "1\n", "authors": ["324"]}
{"title": "CAD approach for plastic mould parting\n", "abstract": " A CAD approach which can optimize and automate the parting direction determination is presented. The approach is based on the geometrical and topological information of the solid modelling of the plastic moulded part in order to select a pair of optimal parting directions of a two-plate mould which minimizes the number of side cores. The shell of a part is divided into inter-influential regions and non-influential faces in the mould design point of view. Through analyzing and computing the accessibility direction cones of the inter-influential regions, the optimal parting directions can be determined automatically.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Surgical Approach in Total Knee Arthroplasty\n", "abstract": " Total knee arthroplasy (TKA) has been found to be safe and effective in well over 90% of patients who undergo the procedure. Medial parapatellar approach (MPA) has been the standard approach, and correct patellar tracking is critical to the success of the procedure.  Although there has been great concern regarding disruption of patellar blood supply, a lateral release was necessary around 51% of TKAs. We have used lateral parapatellar approach (LPA) exclusively in all total knee arthroplasties, including genu valgum, genu varum and revision TKA, to improve both patellar vascularity and patellar traching since October 1996.  The LPA has virtually eliminated patellar maltracking and wound healing problems which are often associated with MPA. The possibility of excellent closure of arthrotomy wound in LPA in most cases implicated that lateral release in some TKAs with MPA could be unnecessary.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Recovery of uranium from loaded resin of processing in-situ leaching solution by alkaline elution system\n", "abstract": " Sodium carbonate-ammonium sulfate solution is used as an eluant to elute uranium from loaded resin in the compact moving-bed column. Uranium concentration is up to about 10 g/L in eluate. The eluate is then heated to precipitate uranium and recover chemicals. The prepared yellow cake contains 60% of moisture and >60% of uranium (based on dried yellow cake). About 70% of CO{sub 2} and nearly all NH{sub 3} are recovered by condensing and absorbing from released gases during heating process of the eluate. The recovered chemicals and mother liquor after precipitation can be recycled for reuse.", "num_citations": "1\n", "authors": ["324"]}
{"title": "Optimal wall temperature control of a heat exchanger\n", "abstract": " An optimal wall temperature control of a single-duct heat exchanger is studied. The wall temperature which is the control variable is assumed to be uniform in space but a function of time. The system considered is a distributed parameter system where the fluid temperature is a function of time and a spatial coordinate. The control is to force optimally the temperature profile of the fluid from the initial state to a new desired steady temperature profile in a finite time. Two different performance indexes are considered. One is the integral of the absolute deviation of the system temperature profile at any moment from the new desired steady state profile. Both are to be minimized. Optimal solutions are obtained by two methods, namely, linear programming and a variational technique. AuthorDescriptors:", "num_citations": "1\n", "authors": ["324"]}