{"title": "The impact of information security events to the stock market: A systematic literature review\n", "abstract": " Information security is a highly critical aspect of information systems. Although the literature regarding security assurance is vast, the research on economic consequences of security incidents is quite limited. The purpose of this systematic review is to search, collect and classify event studies related to information security impact on stock prices. In total, 37 related papers conducting 45 studies were found by the systematic search of bibliographic sources. The majority (75.6%) of these studies report statistical significance of the impact of security events to the stock prices of firms.", "num_citations": "125\n", "authors": ["175"]}
{"title": "Selective fusion of heterogeneous classifiers\n", "abstract": " There are two main paradigms in combining different classification algorithms: Classifier Selection and Classifier Fusion. The first one selects a single model for classifying a new instance, while the latter combines the decisions of all models. The work presented in this paper stands in between these two paradigms aiming to tackle the disadvantages and benefit from the advantages of both. In particular, this paper proposes the use of statistical procedures for the selection of the best subgroup among different classification algorithms and the subsequent fusion of the decision of the models in this subgroup with simple methods like Weighted Voting. Extensive experimental results show that the proposed approach, Selective Fusion, improves over simple selection and fusion methods, leading to performance comparable with the state-of-the-art heterogeneous classifier combination method of Stacking, without the\u00a0\u2026", "num_citations": "119\n", "authors": ["175"]}
{"title": "Clinical effect of stereotyped B-cell receptor immunoglobulins in chronic lymphocytic leukaemia: a retrospective multicentre study\n", "abstract": " BackgroundAbout 30% of cases of chronic lymphocytic leukaemia (CLL) carry quasi-identical B-cell receptor immunoglobulins and can be assigned to distinct stereotyped subsets. Although preliminary evidence suggests that B-cell receptor immunoglobulin stereotypy is relevant from a clinical viewpoint, this aspect has never been explored in a systematic manner or in a cohort of adequate size that would enable clinical conclusions to be drawn.MethodsFor this retrospective, multicentre study, we analysed 8593 patients with CLL for whom immunogenetic data were available. These patients were followed up in 15 academic institutions throughout Europe (in Czech Republic, Denmark, France, Greece, Italy, Netherlands, Sweden, and the UK) and the USA, and data were collected between June 1, 2012, and June 7, 2013. We retrospectively assessed the clinical implications of CLL B-cell receptor immunoglobulin\u00a0\u2026", "num_citations": "106\n", "authors": ["175"]}
{"title": "An empirical study on views of importance of change impact analysis issues\n", "abstract": " Change impact analysis is a change management activity that previously has been studied much from a technical perspective. For example, much work focuses on methods for determining the impact of a change. In this paper, we present results from a study on the role of impact analysis in the change management process. In the study, impact analysis issues were prioritised with respect to criticality by software professionals from an organisational perspective and a self-perspective. The software professionals belonged to three organisational levels: operative, tactical and strategic. Qualitative and statistical analyses with respect to differences between perspectives as well as levels are presented. The results show that important issues for a particular level are tightly related to how the level is defined. Similarly, issues important from an organisational perspective are more holistic than those important from a self\u00a0\u2026", "num_citations": "89\n", "authors": ["175"]}
{"title": "Categorical missing data imputation for software cost estimation by multinomial logistic regression\n", "abstract": " A common problem in software cost estimation is the manipulation of incomplete or missing data in databases used for the development of prediction models. In such cases, the most popular and simple method of handling missing data is to ignore either the projects or the attributes with missing observations. This technique causes the loss of valuable information and therefore may lead to inaccurate cost estimation models. On the other hand, there are various imputation methods used to estimate the missing values in a data set. These methods are applied mainly on numerical data and produce continuous estimates. However, it is well known that the majority of the cost data sets contain software projects with mostly categorical attributes with many missing values. It is therefore reasonable to use some estimating method producing categorical rather than continuous values. The purpose of this paper is to investigate\u00a0\u2026", "num_citations": "79\n", "authors": ["175"]}
{"title": "Not all IGHV3-21 chronic lymphocytic leukemias are equal: prognostic considerations\n", "abstract": " An unresolved issue in chronic lymphocytic leukemia (CLL) is whether IGHV3-21 gene usage, in general, or the expression of stereotyped B-cell receptor immunoglobulin defining subset #2 (IGHV3-21/IGLV3-21), in particular, determines outcome for IGHV3-21-utilizing cases. We reappraised this issue in 8593 CLL patients of whom 437 (5%) used the IGHV3-21 gene with 254/437 (58%) classified as subset #2. Within subset #2, immunoglobulin heavy variable (IGHV)-mutated cases predominated, whereas non\u2013subset #2/IGHV3-21 was enriched for IGHV-unmutated cases (P = .002). Subset #2 exhibited significantly shorter time-to-first-treatment (TTFT) compared with non\u2013subset #2/IGHV3-21 (22 vs 60 months, P = .001). No such difference was observed between non\u2013subset #2/IGHV3-21 vs the remaining CLL with similar IGHV mutational status. In conclusion, IGHV3-21 CLL should not be axiomatically\u00a0\u2026", "num_citations": "70\n", "authors": ["175"]}
{"title": "Temporal trends in chronic Total occlusion interventions in Europe: 17 626 procedures from the European registry of chronic Total occlusion\n", "abstract": " Background: The study focuses on the evolution of practice, procedural outcomes, and in-hospital complications of chronic total occlusion percutaneous coronary intervention in Europe.   Methods and Results: Data from 17\u2009626 procedures enrolled in European Registry of Chronic Total Occlusion between January 2008 and June 2015 were assessed. The mean patient age was 63.9\u00b110.9 years; 85% were men. Procedural success increased from 79.7% to 89.3% through the study period. Patients enrolled during the years had increasing comorbidities and lesion complexity (J-CTO score [Multicenter CTO Registry of Japan] increased from 1.76\u00b11.03 in 2008 to 2.17\u00b10.91 in 2015; P for trend, <0.001). Retrograde approach utilization steadily increased from 10.1% in 2008 to 29.9% in 2015 (P for trend, <0.001). Antegrade dissection reentry adoption was low, not exceeding 5.5%. In-hospital mortality decreased\u00a0\u2026", "num_citations": "62\n", "authors": ["175"]}
{"title": "Clustering classifiers for knowledge discovery from physically distributed databases\n", "abstract": " Most distributed classification approaches view data distribution as a technical issue and combine local models aiming at a single global model. This however, is unsuitable for inherently distributed databases, which are often described by more than one classification models that might differ conceptually. In this paper we present an approach for clustering distributed classifiers in order to discover groups of similar classifiers and thus similar databases with respect to a specific classification task. We also show that clustering distributed classifiers as a pre-processing step for classifier combination enhances the achieved predictive performance of the ensemble.", "num_citations": "58\n", "authors": ["175"]}
{"title": "Prioritization of issues and requirements by cumulative voting: A compositional data analysis framework\n", "abstract": " Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized\u00a0\u2026", "num_citations": "55\n", "authors": ["175"]}
{"title": "PuReD-MCL: a graph-based PubMed document clustering methodology\n", "abstract": " Motivation: Biomedical literature is the principal repository of biomedical knowledge, with PubMed being the most complete database collecting, organizing and analyzing such textual knowledge. There are numerous efforts that attempt to exploit this information by using text mining and machine learning techniques. We developed a novel approach, called PuReD-MCL (Pubmed Related Documents-MCL), which is based on the graph clustering algorithm MCL and relevant resources from PubMed.                    Methods: PuReD-MCL avoids using natural language processing (NLP) techniques directly; instead, it takes advantage of existing resources, available from PubMed. PuReD-MCL then clusters documents efficiently using the MCL graph clustering algorithm, which is based on graph flow simulation. This process allows users to analyse the results by highlighting important clues, and finally to\u00a0\u2026", "num_citations": "54\n", "authors": ["175"]}
{"title": "Towards an integrated platform for big data analysis\n", "abstract": " The amount of data in the world is expanding rapidly. Every day, huge amounts of data are created by scientific experiments, companies, and end users\u2019 activities. These large data sets have been labeled as \u201cBig Data\u201d, and their storage, processing and analysis presents a plethora of new challenges to computer science researchers and IT professionals. In addition to efficient data management, additional complexity arises from dealing with semi-structured or unstructured data, and from time critical processing requirements. In order to understand these massive amounts of data, advanced visualization and data exploration techniques are required.             Innovative approaches to these challenges have been developed during recent years, and continue to be a hot topic for research and industry in the future. An investigation of current approaches reveals that usually only one or two aspects are addressed\u00a0\u2026", "num_citations": "44\n", "authors": ["175"]}
{"title": "Towards an affordable brain computer interface for the assessment of programmers\u2019 mental workload\n", "abstract": " This paper provides a proof of concept for the use of wearable technology, and specifically wearable Electroencephalography (EEG), in the field of Empirical Software Engineering. Particularly, we investigated the brain activity of Software Engineers (SEngs) while performing two distinct but related mental tasks: understanding and inspecting code for syntax errors. By comparing the emerging EEG patterns of activity and neural synchrony, we identified brain signatures that are specific to code comprehension. Moreover, using the programmer's rating about the difficulty of each code snippet shown, we identified neural correlates of subjective difficulty during code comprehension. Finally, we attempted to build a model of subjective difficulty based on the recorded brainwave patterns. The reported results show promise towards novel alternatives to programmers\u2019 training and education. Findings of this kind may\u00a0\u2026", "num_citations": "43\n", "authors": ["175"]}
{"title": "Validation and interpretation of Web users\u2019 sessions clusters\n", "abstract": " Understanding users\u2019 navigation on the Web is important towards improving the quality of information and the speed of accessing large-scale Web data sources. Clustering of users\u2019 navigation into sessions has been proposed in order to identify patterns and similarities which are then managed in the context of Web users oriented applications (searching, e-commerce, etc.). This paper deals with the problem of assessing the quality of user session clusters in order to make inferences regarding the users\u2019 navigation behavior. A common model-based clustering algorithm is used to result in clusters of Web users\u2019 sessions. These clusters are validated by using a statistical test, which measures the distances of the clusters\u2019 distributions to infer their dissimilarity and distinguishing level. Furthermore, a visualization method is proposed in order to interpret the relation between clusters. Using real data sets, we illustrate\u00a0\u2026", "num_citations": "39\n", "authors": ["175"]}
{"title": "Optimal exact experimental designs with correlated errors through a simulated annealing algorithm\n", "abstract": " Simulated annealing (SA) is a stochastic optimization method with principles taken from the physical process called \u201cannealing\u201d which aims to bring a solid to its ground state or a state of minimum energy. SA is known as a simple heuristic tool suitable for providing direct or approximate solutions to a wide variety of combinatorial problems. This paper is concerned with the problem of determining optimal exact experimental designs with n observations and k two-level factors assuming the existence of correlated errors with a known correlation structure. A simulated annealing algorithm has been developed and applied for the search of D- and A-optimal designs. An extensive discussion regarding the right choices of the initial parameters is presented and a method of self-improvement of the algorithm is suggested via a series of repeated executions. Finally, a version of the SA algorithm is used to find optimal exact\u00a0\u2026", "num_citations": "39\n", "authors": ["175"]}
{"title": "Investigating the applicability of agility assessment surveys: A case study\n", "abstract": " ContextAgile software development has become popular in the past decade without being sufficiently defined. The Agile principles can be instantiated differently which creates different perceptions of Agility. This has resulted in several frameworks being presented in the research literature to evaluate the level of Agility. However, the evidence of their actual use in practice is limited.ObjectiveThe objective is to identify online surveys that assess/profile Agility in practice, and to evaluate the surveys in an industrial setting.MethodThe Agility assessment surveys were identified through searching the web. Then, they were explored and two surveys were identified as most promising for our objective. The selected surveys were evaluated in a case study with three Agile teams in a software consultancy company.ResultsEach team and its customer separately judged the team's Agility. This outcome was compared with the\u00a0\u2026", "num_citations": "38\n", "authors": ["175"]}
{"title": "Reliability and validity of the adapted Greek version of scoliosis research society\u201322 (SRS-22) questionnaire\n", "abstract": " The SRS-22 is a valid instrument for the assessment of the health related quality of life of patients with Idiopathic scoliosis. The SRS-22 questionnaire was developed in USA and has been widely used in the English speaking countries. Recently it has been translated and validated in many other languages. The purpose of this study is to evaluate the reliability and validity of the adapted Greek version of the refined Scoliosis Research Society-22 Questionnaire. Following the steps of cross \u2013 cultural adaptation the adapted Greek version of the SRS-22 questionnaire and a validated Greek version of the SF-36 questionnaire were mailed to 68 patients treated surgically for Idiopathic Scoliosis. 51 out of the 68 patients returned the 1st set of questionnaires, while a second set was emailed to 30 randomly selected patients of the first time responders. 20 out of the 30 patients returned the 2nd set. The mean age at the time of operation was16,2 years and the mean age at the time of evaluation was 21,2 years. Descriptive statistics for content analysis were calculated. Reliability assessment was determined by estimating Cronbach's \u03b1 and intraclass correlation coefficient (ICC) respectively. Concurrent validity was evaluated by comparing SRS-22 domains with relevant domains in the SF-36 questionnaire using Pearson's Correlation Coefficient (r). The calculated Cronbach's \u03b1 of internal consistency for three of the corresponding domains (pain 0.85; mental health 0.87; self image 0.83) were very satisfactory and for two domains (function/activity 0.72 and satisfaction 0.67) were good. The ICC of all domains of SRS-22 questionnaire was high (ICC>0.70\u00a0\u2026", "num_citations": "38\n", "authors": ["175"]}
{"title": "WIVSS: a new methodology for scoring information systems vulnerabilities\n", "abstract": " Vulnerabilities of information systems constitute an ever-increasing problem that IT security management must solve. As the number of vulnerabilities is growing exponentially, their ranking and prioritization is a crucial task for organizations and researchers that are involved with the security of computer systems. The open standard to score and rank the vulnerabilities is the Common Vulnerability Scoring System (CVSS) while the focus of this research is to investigate ways to improve it by achieving higher diversity of values and better accuracy. In this paper it is introduced a new vulnerability scoring system, called WIVSS (Weighted Impact Vulnerability Scoring System). The methodology uses a different approach to score vulnerabilities, depending on the different impact of vulnerabilities characteristics. The methodology WIVSS is applied to the most recent 9455 vulnerabilities and the results show improvement in\u00a0\u2026", "num_citations": "36\n", "authors": ["175"]}
{"title": "Sensors talk and humans sense towards a reciprocal collective awareness smart city framework\n", "abstract": " Smart city infrastructures provide unique opportunities for innovative applications developing and testing. Sensor city installations offer the ground for experimenting with user-oriented services, which at the same time can test and improve the infrastructure itself. The proposed work summarizes principles and methodology for and experiment, entitled SEN2SOC which will bridge sensor measurements and social networks interactions via natural language generation for supporting smart city services. SEN2SOC aims at exploiting the SmartSantander infrastructure in a sensor to social reciprocal fashion such that the sensor measurements will be and communicated to the public (citizens, authorities, etc), while social networks users activities in relevance to sensors social postings will be analyzed and summarized both to verify sensors reporting and to develop collective aware applications.", "num_citations": "34\n", "authors": ["175"]}
{"title": "Desmoglein-3/\u03b3-catenin and E-cadherin/\u00df-catenin differential expression in oral leukoplakia and squamous cell carcinoma\n", "abstract": " Objective                 The purpose of this study was to investigate gene/protein expression alterations of intercellular connections\u2019 components in oral leukoplakia (OLs) and squamous-cell carcinoma (OSCCs).                                               Materials and methods                 Expression of desmogleins-2,3 (Dsg2/Dsg3), E-cadherin, and their cytoplasmic ligand, \u03b2/\u03b3-catenins were quantitatively assessed in HSC-3 cells growing as monolayer cultures (ML)/multicellular aggregates (MCAs), using RT-PCR/Western blot, whereas their localization was detected by immunofluorescence. Furthermore, their expression was semi-quantitatively investigated in tissues from 25 OLs/25 OSCCs, using automated immunohistochemistry.                                               Results                 The steady-state levels of Dsg3 RNA transcripts increased as HSC-3 cells enter their exponential phase of growth, before a dramatic decrease to\u00a0\u2026", "num_citations": "31\n", "authors": ["175"]}
{"title": "Model-based cluster analysis for web users sessions\n", "abstract": " One of the main issues in Web usage mining is the discovery of patterns in the navigational behavior of Web users. Standard approaches, such as clustering of users\u2019 sessions and discovering association rules or frequent navigational paths, do not generally allow to characterize or quantify the unobservable factors that lead to common navigational patterns. Therefore, it is necessary to develop techniques that can discover hidden and useful relationships among users as well as between users and Web objects. Correspondence Analysis (CO-AN) is particularly useful in this context, since it can uncover meaningful associations among users and pages. We present a model-based cluster analysis for Web users sessions including a novel visualization and interpretation approach which is based on CO-AN.", "num_citations": "29\n", "authors": ["175"]}
{"title": "Assessment of vulnerability severity using text mining\n", "abstract": " Software1 vulnerabilities are closely associated with information systems security, a major and critical field in today's technology. Vulnerabilities constitute a constant and increasing threat for various aspects of everyday life, especially for safety and economy, since the social impact from the problems that they cause is complicated and often unpredictable. Although there is an entire research branch in software engineering that deals with the identification and elimination of vulnerabilities, the growing complexity of software products and the variability of software production procedures are factors contributing to the ongoing occurrence of vulnerabilities, Hence, another area that is being developed in parallel focuses on the study and management of the vulnerabilities that have already been reported and registered in databases. The information contained in such databases includes, a textual description and a\u00a0\u2026", "num_citations": "28\n", "authors": ["175"]}
{"title": "A multi-target approach to estimate software vulnerability characteristics and severity scores\n", "abstract": " Software vulnerabilities constitute a great risk for the IT community. The specification of the vulnerability characteristics is a crucial procedure, since the characteristics are used as input for a plethora of vulnerability scoring systems. Currently, the determination of the specific characteristics -that represent each vulnerability- is a process that is performed manually by the IT security experts. However, the vulnerability description can be very informative and useful to predict vulnerability characteristics. The primary goal of this research is the enhancement, the acceleration and the support of the manual procedure of the vulnerability characteristic assignment. To achieve this goal, a model, which combines texts analysis and multi-target classification techniques was developed. This model estimates the vulnerability characteristics and subsequently, calculates the vulnerability severity scores from the predicted\u00a0\u2026", "num_citations": "27\n", "authors": ["175"]}
{"title": "MeSHy: Mining unanticipated PubMed information using frequencies of occurrences and concurrences of MeSH terms\n", "abstract": " MotivationPubMed is the most widely used database of biomedical literature. To the detriment of the user though, the ranking of the documents retrieved for a query is not content-based, and important semantic information in the form of assigned Medical Subject Headings (MeSH) terms is not readily presented or productively utilized. The motivation behind this work was the discovery of unanticipated information through the appropriate ranking of MeSH term pairs and, indirectly, documents. Such information can be useful in guiding novel research and following promising trends.MethodsA web-based tool, called MeSHy, was developed implementing a mainly statistical algorithm. The algorithm takes into account the frequencies of occurrences, concurrences, and the semantic similarities of MeSH terms in retrieved PubMed documents to create MeSH term pairs. These are then scored and ranked, focusing on their\u00a0\u2026", "num_citations": "27\n", "authors": ["175"]}
{"title": "Gene functional annotation by statistical analysis of biomedical articles\n", "abstract": " BackgroundFunctional annotation of genes is an important task in biology since it facilitates the characterization of genes relationships and the understanding of biochemical pathways. The various gene functions can be described by standardized and structured vocabularies, called bio-ontologies. The assignment of bio-ontolgy terms to genes is carried out by means of applying certain methods to datasets extracted from biomedical articles. These methods originate from data mining and machine learning and include maximum entropy or support vector machines (SVM).PurposeThe aim of this paper is to propose an alternative to the existing methods for functionally annotating genes. The methodology involves building of classification models, validation and graphical representations of the results and reduction of the dimensions of the dataset.MethodsClassification models are constructed by Linear discriminant\u00a0\u2026", "num_citations": "26\n", "authors": ["175"]}
{"title": "Offshore insourcing: A case study on software quality alignment\n", "abstract": " Background: Software quality issues are commonly reported when off shoring software development. Value-based software engineering addresses this by ensuring key stakeholders have a common understanding of quality. Aim: This work seeks to understand the levels of alignment between key stakeholders on aspects of software quality for two products developed as part of an offshore in sourcing arrangement. The study further aims to explain the levels of alignment identified. Method: Representatives of key stakeholder groups for both products ranked aspects of software quality. The results were discussed with the groups to gain a deeper understanding. Results: Low levels of alignment were found between the groups studied. This is associated with insufficiently defined quality requirements, a culture that does not question management and conflicting temporal reflections on the product's quality\u00a0\u2026", "num_citations": "25\n", "authors": ["175"]}
{"title": "Assessment of Chlamydia psittaci shedding and environmental contamination as potential sources of worker exposure throughout the mule duck breeding process\n", "abstract": " Chlamydia psittaci is an obligate intracellular bacterium responsible for avian chlamydiosis, otherwise known as psittacosis, a zoonotic disease that may lead to severe atypical pneumonia. This study was conducted on seven mule duck flocks harboring asymptomatic birds to explore the circulation and persistence of C. psittaci during the entire breeding process and assess the potential sources of worker exposure. Cloacal swabs and air samples were taken on each occasion requiring humans to handle the birds. In parallel, environmental samples, including dust, water, and soil, were collected. Specific real-time PCR analyses revealed the presence of C. psittaci in all flocks but with three different shedding patterns involving ducks about the age of 4, 8, and 12 weeks with heavy, moderate, and low excretion levels, respectively. Air samples were only positive in flocks harboring heavy shedders. Dust in flocks with\u00a0\u2026", "num_citations": "23\n", "authors": ["175"]}
{"title": "Impact metrics of security vulnerabilities: Analysis and weighing\n", "abstract": " The number of vulnerabilities discovered and reported during the recent decades is enormous, making an improved ranking and prioritization of vulnerabilities\u2019 severity a major issue for information technology (IT) management. Although several methodologies for ranking and scoring vulnerabilities have been proposed, the Common Vulnerability Scoring System (CVSS) is the open standard with wide acceptance from the information security community. Recently, the Weighted Impact Vulnerability Scoring System (WIVSS) has been proposed as an alternative scoring methodology, which assigns different weights to impact factors of vulnerability in order to achieve higher diversity of values and thus improvement in flexibility of ranking in comparison to CVSS. The purpose of this paper is to expand the idea of WIVSS by defining the sets of weights which provide higher diversity of values. For this reason, an algorithm\u00a0\u2026", "num_citations": "23\n", "authors": ["175"]}
{"title": "Performance and effectiveness trade\u2010off for checkpointing in fault\u2010tolerant distributed systems\n", "abstract": " Checkpointing has a crucial impact on systems' performance and fault\u2010tolerance effectiveness: excessive checkpointing results in performance degradation, while deficient checkpointing incurs expensive recovery. In distributed systems with independent checkpoint activities there is no easy way to determine checkpoint frequencies optimizing response\u2010time and fault\u2010tolerance costs at the same time. The purpose of this paper is to investigate the potentialities of a statistical decision\u2010making procedure. We adopt a simulation\u2010based approach for obtaining performance metrics that are afterwards used for determining a trade\u2010off between checkpoint interval reductions and efficiency in performance. Statistical methodology including experimental design, regression analysis and optimization provides us with the framework for comparing configurations, which use possibly different fault\u2010tolerance mechanisms\u00a0\u2026", "num_citations": "23\n", "authors": ["175"]}
{"title": "All-or-nothing transforms using quasigroups\n", "abstract": " In this paper we suggest a new transformation scheme for All-Or-Nothing encryption, originally suggested by Rivest. The new transform concerns the use of quasigroups for the preprocessing of the data before any ordinary encryption method. We describe a method of constructing random quasigroups and we propose a way of using the advantages of quasigroup in Rivest's method. This combination makes the method faster and maintains the advantages against brute-force attacks.", "num_citations": "23\n", "authors": ["175"]}
{"title": "Requirements and architecture design principles for a smart city experiment with sensor and social networks integration\n", "abstract": " Smart city infrastructures offer unique testbeds ground for innovative experimentation and services offering. Sensors networks in cities with integrated social networks activities can improve people-centric services, while improving infrastructures setting. This work summarizes the principles and priorities chosen in a smart city experiment, entitled SEN2SOC which bridges sensor measurements and social networks interactions for supporting smart city services. SEN2SOC prioritizes requirements along particular categories which cover data collection, users sensing along with applications implementation and architectural concerns. These requirements are correlated with the suggested components in an architecture which is flexible enough in order to permit various activities control flow in terms of data preprocessing, conditions detection, statistical analysis as well as applications development and social data mining.", "num_citations": "21\n", "authors": ["175"]}
{"title": "Inter-transaction association rules mining for rare events prediction\n", "abstract": " Rare events prediction is a very interesting and critical issue that has been approached within various contexts by research areas, such as statistics and machine learning. Data mining has provided a set of tools to treat this problem when the size as well as the inherent features of the data, such as noise, randomness and special data types, become an issue for the traditional methods. Transaction databases that contain sets of events require special approaches in order to extract valuable temporal knowledge. Sequential analysis aims to discover patterns or rules describing the temporal structure of data. In this paper we propose an approach that extends sequential analysis to predict rare events in transaction databases. We utilize the framework of inter-transaction association rules, which associate events across a window of transactions. The proposed algorithm produces rules for the accurate and timely prediction of a userspecified rare event, such as a network intrusion or an engine failure.", "num_citations": "21\n", "authors": ["175"]}
{"title": "Non-linear correlation of content and metadata information extracted from biomedical article datasets\n", "abstract": " Biomedical literature databases constitute valuable repositories of up to date scientific knowledge. The development of efficient machine learning methods in order to facilitate the organization of these databases and the extraction of novel biomedical knowledge is becoming increasingly important. Several of these methods require the representation of the documents as vectors of variables forming large multivariate datasets. Since the amount of information contained in different datasets is voluminous, an open issue is to combine information gained from various sources to a concise new dataset, which will efficiently represent the corpus of documents. This paper investigates the use of the multivariate statistical approach, called Non-Linear Canonical Correlation Analysis (NLCCA), for exploiting the correlation among the variables of different document representations and describing the documents with only one\u00a0\u2026", "num_citations": "20\n", "authors": ["175"]}
{"title": "Incorporating resting state dynamics in the analysis of encephalographic responses by means of the Mahalanobis\u2013Taguchi strategy\n", "abstract": " The analysis of encephalographic responses has mostly been attempted via signal analytic techniques aiming at revealing the useful information from recordings which are considered as contaminated by the ubiquitous ongoing (or background) brain activity. There is continuously accumulating evidence for the existence of well-defined resting-state-networks (RSNs) in the brain, which play a crucial role in the generation of spontaneous activity and the associated neural responses. Hence, the signal plus noise is no longer a valid model and the ongoing fluctuations may influence the response.We introduce here the use of a multivariate statistical methodology, known as Mahalanobis\u2013Taguchi (MT) strategy, which can be tailored to the spontaneous fluctuations so as to optimize the subsequent response detection. A subject-specific version of the MT strategy that combines the original methodology with a clustering\u00a0\u2026", "num_citations": "18\n", "authors": ["175"]}
{"title": "A-optimal incomplete block designs with unequal block sizes for comparing test treatments with a control\n", "abstract": " A-optimal designs for comparing v test treatments with a control in b blocks of unequal sizes are considered. A new class of designs is defined, namely the BTIUB designs, which can be considered as an extension of the BTIB designs of Bechhofer and Tamhane in the case of blocks with unequal sizes. Some conditions for the existence and construction of BTIUB designs are given. Finally an algorithm for the construction of A-optimal BTIUB designs is developed and it is applied in the case of two block sizes. Tables of some A-optimal BTIUB designs are given.", "num_citations": "17\n", "authors": ["175"]}
{"title": "A novel single-trial methodology for studying brain response variability based on archetypal analysis\n", "abstract": " The objective of this paper is to present a methodology for deriving an intelligible synopsis of single-trial (ST) variability in brain responses. An algorithmic procedure, relying on temporal patterning and built over archetypal analysis, is introduced. Archetypical brain waves are first derived from the ensemble of brain responses and then used to unfold the observed variability. Using these archetypes as anchor points, homogeneous groups of ST-responses are detected and contrasted with each other. The new methodology incorporates steps for organizing the variability and presenting it by means of low-dimensional maps. Magnetoencephalographic responses from a visual stimulation paradigm are used for demonstrating and validating the approach. The results show that a small number of archetypes is sufficient for describing reliably the response variability. The groups of ST-responses, delineated around these\u00a0\u2026", "num_citations": "16\n", "authors": ["175"]}
{"title": "PREVENT: An algorithm for mining intertransactional patterns for the prediction of rare events\n", "abstract": " In this paper we propose a data mining technique for the efficient prediction of rare events, such as heat waves, network intrusions and engine failures, using inter transactional patterns. Data mining is a research area that attempts to assist the decision makers with a set of tools to treat a wide range of real world problems that the traditional statistical and mathematical approaches are not enough in terms of efficiency and computational performance. Transaction databases, such as the ones in this paper that contain sets of events, require special approaches in order to extract valuable temporal knowledge. We utilize the framework of inter-transaction association rules, which associate events across a window of transactions. We propose an approach that extends sequential analysis to predict rare events in transaction databases. We formulate the problem of rare events prediction and we propose PREVENT, an algorithm that produces inter-transactional patterns for the fast and accurate prediction of a user-specified rare event. Finally, we provide experimental results and suggest some ideas for future research.", "num_citations": "15\n", "authors": ["175"]}
{"title": "Multiple objective optimization of sampling designs for forest inventories using random search algorithms\n", "abstract": " In forest inventories, various sampling techniques, also known as sampling designs, are used for the collection of the necessary information, which will provide precise sample estimates of the tree population characteristics at a low cost. The traditional single objective optimization in sampling is usually achieved either by minimizing the variance of the sample estimates assuming constraints on the cost of all the measurements in the sample or by minimizing the cost assuming constraints on the variance. Another quite realistic approach is to consider the optimization as a multiple objective optimization problem where the cost and the variance are simultaneously minimized functions. This paper proposes variations of a random search algorithm known as simulated annealing algorithm (SAA) for finding optimal sampling designs. The algorithm is first described for the single objective optimization problem and then is\u00a0\u2026", "num_citations": "15\n", "authors": ["175"]}
{"title": "Internet based auctions: a survey on models and applications\n", "abstract": " Web based auctions and negotiations have become quite popular due to their implementation and integration in electronic commerce applications. Online auctions have become an effective approach in the buying and selling process, employed in the rapidly emerging Internet-based electronic commerce platforms. The goal of this paper is to outline research efforts in relation to online Internet-based auctions modeling and applications. The key research topics in the area are identified and the paper focus on the issues of auction modeling, use of agents in Web-based auction implementations, auctions computational and combinatorial analysis as well as on security issues involved in online auctioning. The most popular implementations, applications and servers are also referenced and discussed.", "num_citations": "15\n", "authors": ["175"]}
{"title": "Exploiting the temporal patterning of transient VEP signals: A statistical single-trial methodology with implications to brain\u2013computer interfaces (BCIs)\n", "abstract": " BackgroundWhen visual evoked potentials (VEPs) are deployed in brain\u2013computer interfaces (BCIs), the emphasis is put on stimulus design. In the case of transient VEPs (TVEPs) brain responses are never treated individually, i.e. on a single-trial (ST) basis, due to their poor signal quality. Therefore their main characteristic, which is the emergence during early latencies, remains unexplored.New methodFollowing a pattern-analytic methodology, we investigated the possibility of using single-trial TVEP responses to differentiate between the different spatial locations where a particular visual stimulus appeared and decide whether it was attended or unattended by the subject.ResultsCovert spatial attention modulates the temporal patterning of TVEPs in such a way that a brief ST-segment, from a single synthesized sensor, is sufficient for a Mahalanobis-Taguchi (MT) system to decode subject's intention\u00a0\u2026", "num_citations": "14\n", "authors": ["175"]}
{"title": "Associating the severity of vulnerabilities with their description\n", "abstract": " Software vulnerabilities constitute a major problem for today\u2019s world, which relies more than ever to technological achievements. The characterization of vulnerabilities\u2019 severity is an issue of major importance in order to address them and extensively study their impact on information systems. That is why scoring systems have been developed for the ranking of vulnerabilities\u2019 severity. However, the severity scores are based on technical information and are calculated by combining experts\u2019 assessments. The motivation for the study conducted in this paper was the question of whether the severity of vulnerabilities is directly related to their description. Hence, the associations of severity scores and individual characteristics with vulnerability descriptions\u2019 terms were studied using Text Mining, Principal Components and correlation analysis techniques, applied to all vulnerabilities registered in the National\u00a0\u2026", "num_citations": "13\n", "authors": ["175"]}
{"title": "Analyzing measurements of the R statistical open source software\n", "abstract": " Software quality is one of the main goals of effective programming. Although it has a quite ambiguous meaning, quality can be measured by several metrics, which have been appropriately formulated through the years. Software measurement is a particularly important procedure, as it provides meaningful information about the software artifact. This procedure is even more emerging when we refer to open source software, where the need for shared knowledge is crucial for the maintenance and evolution of the code. A paradigm of open source project where code quality is especially important is the scientific language R. This paper aims to perform measurements on the R statistical open source software, examine the relationships among the observed metrics and special attributes of the R software and search for certain characteristics that define its behavior and structure. For this purpose, a random sample of 508 R\u00a0\u2026", "num_citations": "13\n", "authors": ["175"]}
{"title": "An investigation of software effort phase distribution using compositional data analysis\n", "abstract": " One of the most significant problems faced by project managers is to effectively distribute the project resources and effort among the various project activities. Most importantly, project success depends on how well, or how balanced, the work effort is distributed among the project phases. This paper aims to obtain useful information regarding the correlation of the composition of effort attributed in phases for around 1,500 software projects of the ISBSG R11 database based on a promising statistical method called Compositional Data Analysis (CoDA). The motivation for applying this analysis is the observation that certain types of project data (effort distributions and attributes) do not relate in a direct way but present a spurious correlation. Effort distribution is compared to the project life-cycle activities, organization type, language type, function points and other prime project attributes. The findings are beneficial for\u00a0\u2026", "num_citations": "13\n", "authors": ["175"]}
{"title": "Diagnostic accuracy of high-risk HPV DNA genotyping for primary cervical cancer screening and triage of HPV-positive women, compared to cytology: preliminary results of the\u00a0\u2026\n", "abstract": " PurposeThe purpose of the presented PIPAVIR (persistent infections with human papillomaviruses; http://www. pipavir. com) subanalysis is to assess the performance of high-risk (hr) HPV-DNA genotyping as a method of primary cervical cancer screening and triage of HPV positive women to colposcopy compared to liquid-based cytology (LBC) in an urban female population.MethodsWomen, aged 30\u201360, provided cervicovaginal samples at the Family-Planning Centre, Hippokratio Hospital of Thessaloniki, Greece, and the Department of Gynecology and Obstetrics in Mare Klinikum, Kiel, Germany. Cytology and HPV genotyping was performed using LBC and HPV Multiplex Genotyping (MPG), respectively. Women positive for cytology [atypical squamous cells of undetermined significance (ASC-US) or worse] or hrHPV were referred for colposcopy.ResultsAmong 1723/1762 women included in the final analysis\u00a0\u2026", "num_citations": "12\n", "authors": ["175"]}
{"title": "A multivariate statistical framework for the analysis of software effort phase distribution\n", "abstract": " ContextIn software project management, the distribution of resources to various project activities is one of the most challenging problems since it affects team productivity, product quality and project constraints related to budget and scheduling.ObjectiveThe study aims to (a) reveal the high complexity of modelling the effort usage proportion in different phases as well as the divergence from various rules-of-thumb in related literature, and (b) present a systematic data analysis framework, able to offer better interpretations and visualisation of the effort distributed in specific phases.MethodThe basis for the proposed multivariate statistical framework is Compositional Data Analysis, a methodology appropriate for proportions, along with other methods like the deviation from rules-of-thumb, the cluster analysis and the analysis of variance. The effort allocations to phases, as reported in around 1500 software projects of the\u00a0\u2026", "num_citations": "12\n", "authors": ["175"]}
{"title": "Towards analytical evaluation of professional competences in human resource management\n", "abstract": " Managers of enterprises concern with a major challenge for optimal management of human resources based on availability of domain experts and highly qualified personnel. The process of allocating right people to the right positions in a right time is a key to success. To achieve this goal, managers need to deploy evaluation tools integrated with the gap analysis method. This paper presents the concept and implementation details of an in-house developed software tool for competence evaluation of domain specific competencies and selection of professionals. A generic mathematical representation of competences in this project makes the software tool applied in a wide variety of organizations. A standard competence model has been first defined in this project with 5 main competence categories and related sub-categories including over 70 competence questionnaires in different managerial and employee levels\u00a0\u2026", "num_citations": "12\n", "authors": ["175"]}
{"title": "A framework for capturing, statistically modeling and analyzing the evolution of software models\n", "abstract": " This paper presents a new methodological framework for capturing and statistically modeling the evolution of models in model-driven software development. The framework captures the changes between revisions of models in terms of both low-level (internal) and high-level (developer-visible) edit operations applied between revisions. In our approach, evolution is modeled statistically by using ARMA, GARCH and mixed ARMA-GARCH models. Forecasting and simulation aspects of these time series models are thoroughly assessed. The suitability of the framework is shown by applying it to a large set of design models of real Java systems. Our analysis shows that mixed ARMA-GARCH models are superior to ARMA models.A main motivation for, and application of, the resulting statistical models is to control the generation of realistic model histories which are intended to be used for testing model versioning tools\u00a0\u2026", "num_citations": "10\n", "authors": ["175"]}
{"title": "Empirical extension of a classification framework for addressing consistency in model based development\n", "abstract": " ContextConsistency constitutes an important aspect in practical realization of modeling ideas in the process of software development and in the related research which is diverse. A classification framework has been developed, in order to aid the model based software construction by categorizing research problems related to consistency. However, the framework does not include information on the importance of classification elements.ObjectiveThe aim was to extend the classification framework with information about the relative importance of the elements constituting the classification. The research question was how to express and obtain this information.MethodA survey was conducted on a sample of 24 stakeholders from academia and industry, with different roles, who answered a quantitative questionnaire. Specifically, the respondents prioritized perspectives and issues using an extended hierarchical voting\u00a0\u2026", "num_citations": "10\n", "authors": ["175"]}
{"title": "A simulated annealing approach for multimedia data placement\n", "abstract": " Multimedia applications are characterized by their strong timing requirements and constraints and thus multimedia data storage is a critical issue in the overall system's performance and functionality. This paper describes multimedia data representation models that effectively guide data placement towards the improvement of the Quality of Presentation for the considered multimedia applications. The performance of both constructive placement and iterative improvement placement algorithms is evaluated and discussed. Emphasis is given on placement schemes which are based on the simulated annealing optimization algorithm. A placement policy, based on a self-improving version of the simulated annealing (SISA) algorithm is applied and evaluated. Performance of the placement policies is experimentally evaluated on a simulated tertiary storage subsystem. As proven by the experimentation, the proposed\u00a0\u2026", "num_citations": "10\n", "authors": ["175"]}
{"title": "An evolutionary algorithm for A-optimal incomplete block designs\n", "abstract": " Evolutionary algorithms are heuristic stochastic search and optimization techniques with principles taken from natural genetics. They are procedures mimicking the evolution process of an initial population through genetic transformations. This paper is concerned with the problem of finding A-optimal incomplete block designs for multiple treatment comparisons represented by a matrix of contrasts. An evolutionary algorithm for searching optimal, or nearly optimal, incomplete block designs is described in detail. Various examples regarding the application of the algorithm to some well-known problems illustrate the good performance of the algorithm", "num_citations": "10\n", "authors": ["175"]}
{"title": "An adaptive model for competences assessment of IT professionals\n", "abstract": " Emerging technologies such as Big Data and Cloud Computing in the field of information technology imposes further needs (requests) for professional competences in organizations and IT companies. The ultimate goal is to comply with industrial changes characterizedby adaptive solutions for fostering human-machine interactions. Here competence and job knowledge play a great role in organizations. This paper discusses the concept ofan adaptive competence profiling platform in the context of EU funded project ComProFITS. The main goal is (i) reinforcing competence analytics, and (ii) improving the quality of personnel selection and job performance in the IT sector. This project reflects the results of the research and development activities based on needs analysis with a Spanish IT company.", "num_citations": "9\n", "authors": ["175"]}
{"title": "Applied multiresponse metamodeling for queuing network simulation experiments: problems and perspectives\n", "abstract": " A complete performance evaluation study of a simulated system should consider possible alternatives and response predictions to potential parameter changes. Simulation sensitivity analysis and metamodeling constitute an efficient approach for this kind of problems. However, this approach is usually despised, mainly because, a sophisticated methodological treatment is required. Such a methodology should take into account, peculiarities, inherent to queuing network models, as for example, multiple responses, large number of model parameters, many qualitative parameters etc. This work aims to illustrate the combined use of the proper statistical techniques to cope with this sort of problems and to show the need for a sound methodological framework that will bring this approach closer to the queuing network simulation practice.", "num_citations": "9\n", "authors": ["175"]}
{"title": "Human papillomavirus E7 protein detection as a method of triage to colposcopy of HPV positive women, in comparison to genotyping and cytology. Final results of the PIPAVIR study\n", "abstract": " The objective of the presented cross\u2010sectional\u2010evaluation\u2010screening study is the clinical evaluation of high\u2010risk(hr)HPVE7\u2010protein detection as a triage method to colposcopy for hrHPV\u2010positive women, using a newly developed sandwich\u2010ELISA\u2010assay. Between 2013\u20102015, 2424 women, 30\u201060 years old, were recruited at the Hippokratio Hospital, Thessaloniki/Greece and the Im Mare Klinikum, Kiel/Germany, and provided a cervical sample used for Liquid Based Cytology, HPV DNA genotyping, and E7 detection using five different E7\u2010assays: \u201crecomWell HPV16/18/45KJhigh\u201d, \u201crecomWell HPV16/18/45KJlow\u201d, \u201crecomWell HPV39/51/56/59\u201d, \u201crecomWell HPV16/31/33/35/52/58\u201d and \u201crecomWell HPVHRscreen\u201d (for 16,18,31,33,35,39,45,51,52,56,58,59 E7), corresponding to different combinations of hrHPVE7\u2010proteins. Among 1473 women with eligible samples, those positive for cytology (ASCUS+ 7.2%), and/or\u00a0\u2026", "num_citations": "8\n", "authors": ["175"]}
{"title": "Clustering web information services\n", "abstract": " The explosive growth of the Web scale has drastically increased information circulation and dissemination rates. As the number of both Web users and Web sources grows significantly everyday, crucial data management issues, such as clustering on the Web, should be addressed and analyzed. Clustering has been proposed towards improving both the information availability and the Web users\u2019 personalization. Clusters on the Web are either users\u2019 sessions or Web information sources, which are managed in a variation of applications and implementations testbeds. This chapter focuses on the topic of clustering information over the Web, in an effort to overview and survey on the theoretical background and the adopted practices of most popular emerging and challenging clustering research efforts. An up-to-date survey of the existing clustering schemes is given, to be of use for both researchers and practitioners\u00a0\u2026", "num_citations": "8\n", "authors": ["175"]}
{"title": "On the necessity of multiple university rankings\n", "abstract": " Nowadays university rankings are ubiquitous commodities; a plethora of them is published every year by private enterprises, state authorities and universities. University rankings are very popular to governments, journalists, university administrations and families as well. At the same time, they are heavily criticized as being very subjective and contradictory to each other. University rankings have been studied with respect to political, educational and data management aspects. In this paper, we focus on a specific research question regarding the alignment of some well-known such rankings, ultimately targeting to investigate the usefulness of the variety of all these rankings. First, we describe in detail the methodology to collect and homogenize the data and, second, we statistically analyze these data to examine the correlation among the different rankings. The results show that despite their statistically significant\u00a0\u2026", "num_citations": "7\n", "authors": ["175"]}
{"title": "Quantification of interacting runtime qualities in software architectures: Insights from transaction processing in client\u2013server architectures\n", "abstract": " ContextArchitecture is fundamental for fulfilling requirements related to the non-functional behavior of a software system such as the quality requirement that response time does not degrade to a point where it is noticeable. Approaches like the Architecture Tradeoff Analysis Method (ATAM) combine qualitative analysis heuristics (e.g. scenarios) for one or more quality metrics with quantitative analyses. A quantitative analysis evaluates a single metric such as response time. However, since quality metrics interact with each other, a change in the architecture can affect unpredictably multiple quality metrics.ObjectiveThis paper introduces a quantitative method that determines the impact of a design change on multiple metrics, thus reducing the risks in architecture design. As a proof of concept, the method is applied on a simulation model of transaction processing in client server architecture.MethodFactor analysis is\u00a0\u2026", "num_citations": "7\n", "authors": ["175"]}
{"title": "An application of quasigroups in all-or-nothing transform\n", "abstract": " All-Or-Nothing (AON) is an encryption mode for block ciphers with the property that an adversary must decrypt the entire ciphertext in order to determine any plaintext block. In this article, we present a new encryption scheme with the AON property, based on operations defined by quasigroups. The proposed procedure is a reliable and secure preprocessing step to any other common encryption mode, aiming to slow down the brute force searches against block ciphers.", "num_citations": "7\n", "authors": ["175"]}
{"title": "Survival analysis for the duration of software projects\n", "abstract": " In the area of software engineering various methods have been proposed in order to predict the cost of a software project in terms of the effort or of the productivity. An important feature which is closely related to the cost is the duration of a software project. In this paper we deal with the problem of studying and modeling the distribution of the time from specification until delivery of a software product. Specifically, we investigate the use of a statistical methodology known from biostatistics as survival analysis. The purpose of such an analysis is to describe the distribution of the duration and also to identify important factors that affect it. The great advantage of survival analysis is that we can utilize information not only from the completed projects in a dataset but also from ongoing projects. The general principles of the methodology are described with examples from applications to known data sets", "num_citations": "7\n", "authors": ["175"]}
{"title": "Methods of constructing A-efficient BTIUB designs\n", "abstract": " In this paper some methods of constructing balanced treatment incomplete block designs with two different block sizes (called BTIUB designs), are given in order to compare test treatments with a control in unequal blocks. These methods are based on the use of the incidence matrices of block designs with known patterns, such as balanced incomplete block designs and group divisible designs. Each method produces at least one A-optimal BTIUB design, while almost all of the designs constructed, within reasonable limits of the parameters, for practical situations, are highly A-efficient.", "num_citations": "7\n", "authors": ["175"]}
{"title": "ACID Sim Tools: A simulation framework for distributed transaction processing architectures\n", "abstract": " Modern network centric information systems implement highly distributed architectures that usually include multiple application servers. Application design is mainly based on the fundamental object-oriented principles and the adopted architecture matches the logical decomposition of applications (into several tiers like presentation, logic and data) to their software and hardware structuring. The provided recovery solutions ensure an at-mostonce service request processing by an existing transaction processing infrastructure. However, in published works performance evaluation of transaction processing aspects is focused on the computational model of database servers. Also, there are no available tools which enable exploring the performance and availability trade-offs that arise when applying different combinations of concurrency control, atomic commit and recovery protocols. This paper introduces ACID Sim Tools, a publicly available tool and at the same time an open source framework for interactive and batch-mode simulation of transaction processing architectures that adopt the basic assumptions of an object-based computational model.", "num_citations": "6\n", "authors": ["175"]}
{"title": "A probabilistic validation algorithm for web users' clusters\n", "abstract": " Cluster analysis is one of the most important aspects in the data mining process for discovering groups and identifying interesting distributions or patterns over the considered data sets. In the context of Web data mining, model-based clustering algorithms are often used to cluster similar users' sessions in order to determine Website access behaviors. An important issue in cluster analysis is the evaluation of clustering results to find the partitioning that best fits the underlying data. In this paper, we present a novel validation technique for model based clustering approaches.", "num_citations": "6\n", "authors": ["175"]}
{"title": "Modeling the effect of the badges gamification mechanism on personality traits of Stack Overflow users\n", "abstract": " Technical Question & Answering (Q&A) websites have become promising sources of information for a wide range of topics and the most popular is Stack Overflow. Aiming to attract the continuous users\u2019 interest, Stack Overflow enabled the gamification system. By rewarding users\u2019 participation and contribution to the community, the site provides motivation to increase their participation. A well-known gamification process is the badge awards. In this paper, we propose a methodology to model and analyze the effect of the badge award gamification process on user\u2019s personality traits based on data recorded before and after award time, employing the Big Five personality Factors. Using statistical univariate analysis and mainly multivariate cluster analysis, we show that: (1) On the average, users\u2019 personality scores for each one of the individual personality traits are not significantly affected by the badge award; (2) When\u00a0\u2026", "num_citations": "5\n", "authors": ["175"]}
{"title": "Software technologies skills: A graph-based study to capture their associations and dynamics\n", "abstract": " Software design and development technologies evolve very fast and in unpredicted rates, posing many challenges for programmers who strive to use them properly and to be up-to-date, especially since software development demands teamwork and collaboration. As a result, Question and Answer (Q&A) sites, like Stack Overflow, have seen large growth. The questions are characterized by tags, which support developers to easily trace their topic of interest. Very often, these tags refer to technologies that are connected or serve the same purpose. This work is motivated by the fact that despite the volume of questions and technologies change over time, tags inter-connections carry insightful information since they can be utilized to monitor the technology trends and their dynamics given technologies fast simultaneous evolution over time. This work recognizes the value of such connections, to reveal associations of\u00a0\u2026", "num_citations": "5\n", "authors": ["175"]}
{"title": "Synthetic metrics for evaluating runtime quality of software architectures with complex tradeoffs\n", "abstract": " Runtime quality of software, such as availability and throughput, depends on architectural factors and execution environment characteristics (e.g. CPU speed, network latency). Although the specific properties of the underlying execution environment are unknown at design time, the software architecture can be used to assess the inherent impact of the adopted design decisions on runtime quality. However, the design decisions that arise in complex software architectures exhibit non trivial interdependences. This work introduces an approach that discovers the most influential factors, by exploiting the correlation structure of the analyzed metrics via factor analysis of simulation data. A synthetic performance metric is constructed for each group of correlated metrics. The variability of these metrics summarizes the combined factor effects hence it is easier to assess the impact of the analyzed architecture decisions on the\u00a0\u2026", "num_citations": "5\n", "authors": ["175"]}
{"title": "Construction of generalized binary proper efficiency-balanced block designs with two different replication numbers\n", "abstract": " Necessary and sufficient conditions for a generalized binary proper block design with two different replication numbers to be efficiency-balanced (EB) are given. Binary block designs such as balanced incomplete block designs, group divisible designs, balanced treatment incomplete block designs and balanced bipartite block designs are used here to construct generalized binary proper EB designs with two different replication numbers whose existence has been an open problem.", "num_citations": "5\n", "authors": ["175"]}
{"title": "A novel feature selection method based on comparison of correlations for human activity recognition problems\n", "abstract": " In human activity recognition studies it is important to identify an optimal set with the minimum number of features that will potentially improve the recognition rate. In the current paper we introduce a promising feature selection method that exploits the differences on the correlation structure of the features, between the different classes of the target variable. Using the recordings of triaxial accelerometers and gyroscopes, we extracted several features and created subsets according to the activities performed. For each subset, we calculated the pairwise correlation coefficients of the features and compared the feature correlations of different subsets. By identifying the significantly different correlations we ranked the variables participating in those correlations based on their frequency of appearance and thus created a subset of features that will optimize the performance of a classification algorithm. The method\u00a0\u2026", "num_citations": "4\n", "authors": ["175"]}
{"title": "Investigation of the relationship between sleep disorders and xerostomia\n", "abstract": " Objectives To investigate the relationship between sleep disorders, morning hyposalivation, and subjective feeling of dry mouth.   Materials and methods A cross-sectional, observational, clinical study was carried out in a homogenous population sample which consists of Greek male soldiers without any medical history. After the application of oral modified Schirmer test, the sample was divided into a study group (n\u2009=\u200963) (MST\u2009<\u200925\u00a0mm/3\u00a0min) and a control group (n\u2009=\u2009110) (MST\u2009\u2265\u200925\u00a0mm/3\u00a0min). In order to assess daytime sleepiness, risk of obstructive sleep apnea (OSA), sleep quality, sleep bruxism (SB), and subjective feeling of dry mouth, all the participants filled in the following scales in Greek version: Epworth Sleepiness Scale (ESS), Pittsburgh Sleep Quality Index (PSQI), Berlin Questionnaire (BQ), a SB questionnaire, and Xerostomia Inventory (XI) respectively. In every subgroup that came of ESS\u00a0\u2026", "num_citations": "4\n", "authors": ["175"]}
{"title": "The COMALAT approach to individualized E-learning in job-specific language competences\n", "abstract": " COMALAT (Competence Oriented Multilingual Adaptive Language Assessment and Training) project aims to strengthen    the mobility of young workers across Europe, by improving job\u2010specific language competence tailored individually to    particular needs. In this work we will concentrate on the COMALAT learning management system (LMS), which is a\u00a0language    learning system for Vocational Education and Training (VET). COMALAT LMS aims at providing learning material as an Open    Educational Resource (OER) and is capable of self\u2010adapting to the needs of different learners. Each learner is treated    individually in acquiring new language skills related to job\u2010specific competences. In addition, it is specifically    tailored towards addressing competence areas, and therefore it is not a\u00a0generic language learning platform. We discuss    some technical details of the COMALAT platform and present the\u00a0\u2026", "num_citations": "4\n", "authors": ["175"]}
{"title": "Regression-based statistical bounds on software execution time\n", "abstract": " Our work aims at facilitating the schedulability analysis of non-critical systems, in particular those that have soft real-time constraints, where WCETs can be replaced by less stringent probabilistic bounds, which we call Maximal Execution Times (METs). In our approach, we can obtain adequate probabilistic execution time models by separating the non-random input data dependency from a modeling error that is purely random. To achieve this, we propose to take advantage of the rich set of available statistical model-fitting techniques, in particular linear regression. Although certainly the proposed technique cannot directly achieve extreme probability levels that are usually expected for WCETs, it is an attractive alternative for MET analysis, since it can arguably guarantee safe probabilistic bounds. We demonstrate our method on a JPEG decoder running on an industrial SPARC V8 processor.", "num_citations": "4\n", "authors": ["175"]}
{"title": "Software product quality in global software development: Finding groups with aligned goals\n", "abstract": " The development of a software product in an organization involves various groups of stakeholders who may prioritize the qualities of the product differently. This paper presents an empirical study of 65 individuals in different roles and in different locations, including on shoring, outsourcing and off shoring, prioritizing 24 software quality aspects. Hierarchical cluster analysis is applied to the prioritization data, separately for the situation today and the ideal situation, and the composition of the clusters, regarding the distribution of the inherent groupings within each of them, is analyzed. The analysis results in observing that the roles are not that important in the clustering. However, compositions of clusters regarding the onshore-offshore relationships are significantly different, showing that the offshore participants have stronger tendency to cluster together. In conclusion, stakeholders seem to form clusters of aligned\u00a0\u2026", "num_citations": "4\n", "authors": ["175"]}
{"title": "The coming of age for big data in systems radiobiology, an engineering perspective\n", "abstract": " As high-throughput approaches in biological and biomedical research are transforming the life sciences into information-driven disciplines, modern analytics platforms for big data have started to address the needs for efficient and systematic data analysis and interpretation. We observe that radiobiology is following this general trend, with -omics information providing unparalleled depth into the biomolecular mechanisms of radiation response\u2014defined as systems radiobiology. We outline the design of computational frameworks and discuss the analysis of big data in low-dose ionizing radiation (LDIR) responses of the mammalian brain. Following successful examples and best practices of approaches for the analysis of big data in life sciences and health care, we present the needs and requirements for radiation research. Our goal is to raise awareness for the radiobiology community about the new technological\u00a0\u2026", "num_citations": "3\n", "authors": ["175"]}
{"title": "Is the Market Value of Software Vendors Affected by Software Vulnerability Announcements?\n", "abstract": " Nowadays, information security constitutes an urgent issue for businesses and researchers. The security vulnerabilities existing in computer systems are sources of different problems. An indirect and emerging issue, regarding the economic consequences of the vulnerabilities, is the impact of software vulnerability announcements to the stock price of the responsible software vendors. The scope of this paper is the study of the stock market reaction when vulnerability announcements occur and the correlation analysis between the impact of these events and vulnerability severity according to scoring systems. To find the impact to the stock market, the well-established in economics event study methodology was used. The dataset in this research was collected from the US-CERT (United States Computer Emergency Readiness Team) website, consisting of year\u2019s 2014 records while the total number of\u00a0\u2026", "num_citations": "3\n", "authors": ["175"]}
{"title": "A Structural Equation Modeling Approach of the Toll-Like Receptor Signaling Pathway in Chronic Lymphocytic Leukemia\n", "abstract": " Gene pathway identification is an open and active research area that has attracted the interest not only of biomedical scientists but also of a large number of researchers from disciplines related to knowledge discovery from biological data. In this paper, we used Structural Equation Modeling (SEM) in order to statistically investigate the Toll-Like Receptor (TLR) signaling pathway in Chronic Lymphocytic Leukemia (CLL). Specifically, we used Path Analysis, a special case of SEM which is a statistical technique for testing and confirming causal relations based on data and qualitative assumptions. The dataset consists of Real Time PCR data for 84 genes relevant to the TLR signaling pathway, that were obtained from 192 patients with CLL that have been classified based on the mutational status of their clonotypic antigen receptors as mutated CLL (M-CLL) or unmutated CLL (U-CLL). The causal effects among genes\u00a0\u2026", "num_citations": "3\n", "authors": ["175"]}
{"title": "A simulation process for asynchronous event processing systems: Evaluating performance and availability in transaction models\n", "abstract": " Simulation is essential for understanding the performance and availability behavior of complex systems, but there are significant difficulties when trying to simulate systems with multiple components, which interact with asynchronous communication. A systematic process is needed, in order to cope with the complexity of asynchronous event processing and the failure semantics of the interacting components. We address this problem by introducing an approach that combines formal techniques for faithful representation of the complex system effects and a statistical analysis for simultaneously studying multiple simulation outcomes, in order to interpret them. Our process has been successfully applied to a synthetic workload for distributed transaction processing. We outline the steps followed towards generating a credible simulation model and subsequently we report and interpret the results of the applied statistical\u00a0\u2026", "num_citations": "3\n", "authors": ["175"]}
{"title": "A Study on Workload Characterization for a Web Proxy Server.\n", "abstract": " The popularity of the World-Wide-Web has increased dramatically in the past few years. Web proxy servers have an important role in reducing server loads, network traffic, and client request latencies. This paper presents a detailed workload characterization study of a busy Web proxy server. The study aims in identifying the major characteristics which will improve modelling of Web proxy accessing. A set of log files is processed for workload characterization. Throughout the study, emphasis is given on identifying the criteria for a Web caching model. A statistical analysis, based on the previous criteria, is presented in order to characterize the major workload parameters. Results of this analysis are presented and the paper concludes with a discussion about workload characterization and content delivery issues.", "num_citations": "3\n", "authors": ["175"]}
{"title": "Similarity based distributed classification\n", "abstract": " Most distributed knowledge discovery approaches view data distribution as a technical issue and combine local models aiming at a single global model. This however, is unsuitable for inherently distributed databases, which often produce models that differ semantically. In this paper we present an approach for distributed classification that uses the pairwise similarity of local models in order to produce a better model for each of the distributed databases. This is achieved by averaging the decisions of all local models weighted by their similarity with the model induced from the origin of the unlabelled data.", "num_citations": "3\n", "authors": ["175"]}
{"title": "Optimal designs with a single two-level factor and n autocorrelated observations\n", "abstract": " The problem of finding an optimal design with a single two-level factor and n observations, under the presence of correlation between errors, is considered. Assuming errors following a first order autoregressive model, the exact D- and A-optimal designs for the distinct cases of positive and negative correlation were found and their optimality was proved explicitly.", "num_citations": "3\n", "authors": ["175"]}
{"title": "A Strong Seasonality Pattern for Covid-19 Incidence Rates Modulated by UV Radiation Levels\n", "abstract": " The Covid-19 pandemic has required nonpharmaceutical interventions, primarily physical distancing, personal hygiene and face mask use, to limit community transmission, irrespective of seasons. In fact, the seasonality attributes of this pandemic remain one of its biggest unknowns. Early studies based on past experience from respiratory diseases focused on temperature or humidity, with disappointing results. Our hypothesis that ultraviolet (UV) radiation levels might be a factor and a more appropriate parameter has emerged as an alternative to assess seasonality and exploit it for public health policies. Using geographical, socioeconomic and epidemiological criteria, we selected twelve North-equatorial-South countries with similar characteristics. We then obtained UV levels, mobility and Covid-19 daily incidence rates for nearly the entire 2020. Using machine learning, we demonstrated that UV radiation strongly associated with incidence rates, more so than mobility did, indicating that UV is a key seasonality indicator for Covid-19, irrespective of the initial conditions of the epidemic. Our findings can inform the implementation of public health emergency measures, partly based on seasons in the Northern and Southern Hemispheres, as the pandemic unfolds into 2021. View Full-Text", "num_citations": "2\n", "authors": ["175"]}
{"title": "Establishment of computational biology in Greece and Cyprus: Past, present, and future\n", "abstract": " We review the establishment of computational biology in Greece and Cyprus from its inception to date and issue recommendations for future development. We compare output to other countries of similar geography, economy, and size\u2014based on publication counts recorded in the literature\u2014and predict future growth based on those counts as well as national priority areas. Our analysis may be pertinent to wider national or regional communities with challenges and opportunities emerging from the rapid expansion of the field and related industries. Our recommendations suggest a 2-fold growth margin for the 2 countries, as a realistic expectation for further expansion of the field and the development of a credible roadmap of national priorities, both in terms of research and infrastructure funding.", "num_citations": "2\n", "authors": ["175"]}
{"title": "Study of gene expressions' correlation structures in subgroups of Chronic Lymphocytic Leukemia Patients\n", "abstract": " In chronic lymphocytic leukemia (CLL) the interaction of leukemic cells with the microenvironment ultimately affects patient outcome. CLL cases can be divided in two subgroups with different clinical course based on the mutational status of the immunoglobulin heavy variable (IGHV) genes: mutated CLL (M-CLL) and unmutated CLL (U-CLL). Since in CLL, the differentiated relation of genes between the two subgroups is of greater importance than the individual gene behavior, this paper investigates the differences between the groups\u2019 gene interactions, by comparing their correlation structures. Fisher\u2019s test and Zou\u2019s confidence intervals are employed to detect differences of correlation coefficients. Afterwards, networks created by the genes participating in most differences are estimated with the use of structural equation models (SEM). The analysis is enhanced with graph modeling in order to visualize the between\u00a0\u2026", "num_citations": "2\n", "authors": ["175"]}
{"title": "Understanding specialized ribosomal protein functions and associated ribosomopathies by navigating across sequence, literature, and phenotype information resources\n", "abstract": " The ubiquitously expressed ribosomal proteins (RPs) constitute essential components of the ribosome, the organelle responsible for protein production in the cell. Historically, these proteins have been solely viewed as building blocks necessary for maintaining the structural integrity of the translational machinery. Recently, however, many studies have emerged challenging this traditional notion. Through various interactions with specific proteins and mRNA cis-regulatory elements, ribosomal proteins exert unforeseen control over translation and cellular homeostasis. Moreover, perturbation of their expression levels results in the development of diseases with highly tissue-specific symptoms, commonly known as ribosomopathies. The present study aims to describe the multifunctional roles of RPs in the biomedical literature and their association with disease phenotypes. We show that the level of literature coverage\u00a0\u2026", "num_citations": "2\n", "authors": ["175"]}
{"title": "An experience-based framework for evaluating alignment of software quality goals\n", "abstract": " Efficient quality management of software projects requires knowledge of how various groups of stakeholders involved in software development prioritize the product and project goals. Agreements or disagreements among members of a team may originate from inherent groupings, depending on various professional or other characteristics. These agreements are not easily detected by conventional practices (discussions, meetings, etc.) since the natural language expressions are often obscuring, subjective, and prone to misunderstandings. It is therefore essential to have objective tools that can measure the alignment among the members of a team; especially critical for the software development is the degree of alignment with respect to the prioritization goals of the software product. The paper proposes an experience-based framework of statistical and graphical techniques for the systematic study of\u00a0\u2026", "num_citations": "2\n", "authors": ["175"]}
{"title": "A controlled experiment investigation on the impact of an instructional tool for personalized learning\n", "abstract": " We describe a controlled experiment concerning the use of a learning aid during the instructional procedure. The core issue of investigation is whether this instructional aid can augment the cognitive transfer of the learners by personalizing the offered knowledge. A controlled experiment was performed with the participation of 79 students. The results have shown that for the transfer of simple information this \"lesson sheet\" does not provide any statistically significant advantage, yet for complex information a statistically significant better performance is observed for the student group that used the tool.", "num_citations": "2\n", "authors": ["175"]}
{"title": "Maximal software execution time: a regression-based approach\n", "abstract": " This work aims at facilitating the schedulability analysis of non-critical systems, in particular those that have soft real-time constraints, where worst-case execution times (WCETs) can be replaced by less stringent probabilistic bounds, which we call maximal execution times (METs). To this end, it is possible to obtain adequate probabilistic execution time models by separating the non-random dependency on input data from a modeling error that is purely random. The proposed approach first utilizes execution time multivariate measurements for building a multiple regression model and then uses the theory related to confidence bounds of coefficients, in order to estimate the upper bound of execution time. Although certainly our method cannot directly achieve extreme probability levels that are usually expected for WCETs, it is an attractive alternative for MET analysis, since it can arguably guarantee safe\u00a0\u2026", "num_citations": "1\n", "authors": ["175"]}
{"title": "CLL: Different ages, dfferent antigen receptor profiles\n", "abstract": " BALIAKAS P. MINGA E. HADZIDIMITRIOU A. TSANOUSA A. SUTTON L. SCARFO L. DAVIS Z. YAN J. PLEVOV\u00c1 Karla SANDBERG Y. JUHL F. BOUDJOGRA M. TZENOU T. CHATZOULI M. AGATHANGELIDIS A. CHU Ch. GARDINER A. SMEDBY K. MANSOURI L. PEDERSEN L. POIRON C. GIUDICELLI V. TICH\u00dd Boris PANAGIOTIDIS P. JULIUSSON G. ANAGNOSTOPOULOS A. ANGELIS L. LEFRANC M. GEISLER Ch. LANGERAK A. POSP\u00cd\u0160ILOV\u00c1 \u0160\u00e1rka CHIORAZZI N. BELESSI Ch. DAVI F. OSCIER D. DARZENTAS Nikos ROSENQUIST R. GHIA P. STAMATOPOULOS K.", "num_citations": "1\n", "authors": ["175"]}
{"title": "Discovering patterns of correlation and similarities in software project data with the Circos visualization tool\n", "abstract": " Software cost estimation based on multivariate data from completed projects requires the building of efficient models. These models essentially describe relations in the data, either on the basis of correlations between variables or of similarities between the projects. The continuous growth of the amount of data gathered and the need to perform preliminary analysis in order to discover patterns able to drive the building of reasonable models, leads the researchers towards intelligent and time-saving tools which can effectively describe data and their relationships. The goal of this paper is to suggest an innovative visualization tool, widely used in bioinformatics, which represents relations in data in an aesthetic and intelligent way. In order to illustrate the capabilities of the tool, we use a well known dataset from software engineering projects.", "num_citations": "1\n", "authors": ["175"]}
{"title": "Automatic extraction of structure, content and usage data statistics of web sites\n", "abstract": " In this paper we present a web mining tool which automatically extracts the structure, content and usage data statistics of web sites. This work inspired by the fact that web mining consists of three axes: web structure mining, web content mining and web usage mining. Each one of those axes is using the structure, content and usage data respectively. The scope is to use the developed multi-thread web crawler as a tool to automatically extract from web pages data that are associated with each one of those three axes in order afterwards to compute several useful descriptive statistics and apply advanced mathematical and statistical methods. A description of our system is provided as well as some experimentation results.", "num_citations": "1\n", "authors": ["175"]}
{"title": "Simulation metamodeling for the design of reliable object based systems\n", "abstract": " Replication is a suitable approach for the provision of fault tolerance and load balancing in distributed systems. Object replication takes place on the basis of well-designed interaction protocols that preserve object state consistency in an application transparent manner. The published analytic performance models may only be applied in single-server process replication schemes and are not suitable for schemes composed of miscellaneous policies, such as those arising in object based systems. In this work we make use of a simulation metamodeling approach that allows the comparative evaluation of composite fault tolerance schemes, on the basis of small size uniform experimental designs. Our approach opens the possibility to take into account different design concerns in a combined manner (eg fault tolerance combined with load balancing and multithreading). We provide results in terms of a case system study that reveals a dependence of the optimal adjustments on the system load level. This finding suggests the device of dynamically adjusted fault tolerance schemes.", "num_citations": "1\n", "authors": ["175"]}
{"title": "A Controlled Experiment Concerning Traditional and Distance Learning of UML Sequence Diagram\n", "abstract": " Modern software companies are faced with the problem of training or updating their software engineers in UML topics. Because of limited time availability, often this becomes a difficult and expensive task. The study presented in this paper is a controlled experiment, concerning the ability to learn UML Sequence Diagrams, by means of two different approaches: The Traditional learning approach, learning in a classroom with the presence of a teacher and the Distance learning approach using web-based instructional material. The latter offers more flexibility to a software company, since the trainees need not interrupt their scheduled assignments. The core issue of investigation is whether Distance learning can be as effective as Traditional learning, when a software engineer must learn the UML language. A controlled experiment, comparing the two learning approaches, was performed with the participation of 49 students, separated into two groups. The results of the study show that there is no statistically significant difference between the two groups, indicating that Traditional place-based and Distance learning courses can both be effective methods for delivering course information. Therefore, the Distance learning approach seems to be appropriate for teaching UML.", "num_citations": "1\n", "authors": ["175"]}
{"title": "Alternative m-estimators of location and their linear convex combinations\n", "abstract": " Some alternative M estimators of location are studied. Their \u03c8-functions are defined by a single algebraic expression for all values of the independent variable and they combine good robustness and performance properties. A re-descending M-estiriiator and an M-estimator with monotone \u03c8 are introduced and their linear convex combinations are considered. As a result a class of estimators decreasing to a positive number is obtained with 50% breakdown point. The breakdown point of this class is calculated via a generalization of a Huber's theorem. A simulation study was performed in order to examine the finite sample behavior of the proposed estimators.", "num_citations": "1\n", "authors": ["175"]}