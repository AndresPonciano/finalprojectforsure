{"title": "An exploratory study of performance regression introducing code changes\n", "abstract": " Performance is an important aspect of software quality. In fact, large software systems failures are often due to performance issues rather than functional bugs. One of the most important performance issues is performance regression. Examples of performance regressions are response time degradation and increased resource utilization. Although performance regressions are not all bugs, they often have a direct impact on users' experience of the system. Due to the possible large impact of performance regressions, prior research proposes various automated approaches that detect performance regressions. However, the detection of performance regressions is conducted after the fact, i.e., after the system is built and deployed in the field or dedicated performance testing environments. On the other hand, there exists rich software quality research that examines the impact of code changes on software quality; while\u00a0\u2026", "num_citations": "33\n", "authors": ["731"]}
{"title": "Towards the Use of the Readily Available Tests from the Release Pipeline as Performance Tests. Are We There Yet?\n", "abstract": " Performance is one of the important aspects of software quality. Performance issues exist widely in software systems, and the process of fixing the performance issues is an essential step in the release cycle of software systems. Although performance testing is widely adopted in practice, it is still expensive and time-consuming. In particular, the performance testing is usually conducted after the system is built in a dedicated testing environment. The challenges of performance testing make it difficult to fit into the common DevOps process in software development. On the other hand, there exist a large number of tests readily available, that are executed regularly within the release pipeline during software development. In this paper, we perform an exploratory study to determine whether such readily available tests are capable of serving as performance tests. In particular, we would like to see whether the performance of\u00a0\u2026", "num_citations": "12\n", "authors": ["731"]}
{"title": "A novel algorithm for encrypted traffic classification based on sliding window of flow's first N packets\n", "abstract": " Network applications are getting more and more prevalent along with the development and the widespread use of encrypted network applications. However, traffic classification methods may need to be improved to realize more stable classification in a more sufficient way. Here, we proposed a novel Sliding Window First N Packets algorithm for the encrypted network traffic classification. With this method, one could evidently reduce the flow characteristics feature dimension, as well as the number of packets in each traffic flow. The experimental results show that under a reduced dimension of encrypted traffic flow characteristics and also a reduced number of each flow data packets, average classification accuracy using the Sliding Window First N Packets algorithm we proposed is more than 95%. By using our approach, one can achieve a general increase of the traffic classification accuracy by about 3% compared\u00a0\u2026", "num_citations": "12\n", "authors": ["731"]}
{"title": "Using black-box performance models to detect performance regressions under varying workloads: an empirical study\n", "abstract": " Performance regressions of large-scale software systems often lead to both financial and reputational losses. In order to detect performance regressions, performance tests are typically conducted in an in-house (non-production) environment using test suites with predefined workloads. Then, performance analysis is performed to check whether a software version has a performance regression against an earlier version. However, the real workloads in the field are constantly changing, making it unrealistic to resemble the field workloads in predefined test suites. More importantly, performance testing is usually very expensive as it requires extensive resources and lasts for an extended period. In this work, we leverage black-box machine learning models to automatically detect performance regressions in the field operations of large-scale software systems. Practitioners can leverage our approaches to complement or\u00a0\u2026", "num_citations": "7\n", "authors": ["731"]}
{"title": "Performance regression detection in DevOps\n", "abstract": " Performance is an important aspect of software quality. The goals of performance are typically defined by setting upper and lower bounds for response time and throughput of a system and physical level measurements such as CPU, memory and I/O. To meet such performance goals, several performance-related activities are needed in development (Dev) and operations (Ops). In fact, large software system failures are often due to performance issues rather than functional bugs. One of the most important performance issues is performance regression. Although performance regressions are not all bugs, they often have a direct impact on users\u2019 experience of the system. The process of detection of performance regressions in development and operations is faced with challenges. First, the detection of performance regression is conducted after the fact, i.e., after the system is built and deployed in the field or dedicated\u00a0\u2026", "num_citations": "1\n", "authors": ["731"]}