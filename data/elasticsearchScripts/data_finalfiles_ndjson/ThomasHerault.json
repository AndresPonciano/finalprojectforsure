{"title": "Approximate probabilistic model checking\n", "abstract": " Symbolic model checking methods have been extended recently to the verification of probabilistic systems. However, the representation of the transition matrix may be expensive for very large systems and may induce a prohibitive cost for the model checking algorithm. In this paper, we propose an approximation method to verify quantitative properties on discrete Markov chains. We give a randomized algorithm to approximate the probability that a property expressed by some positive LTL formula is satisfied with high confidence by a probabilistic system. Our randomized algorithm requires only a succinct representation of the system and is based on an execution sampling method. We also present an implementation and a few classical examples to demonstrate the effectiveness of our approach.", "num_citations": "370\n", "authors": ["521"]}
{"title": "Probabilistic model checking of the CSMA/CD protocol using PRISM and APMC\n", "abstract": " Carrier Sense Multiple Access/Collision Detection (CSMA/CD) is the protocol for carrier transmission access in Ethernet networks (international standard IEEE 802.3). On Ethernet, any Network Interface Card (NIC) can try to send a packet in a channel at any time. If another NIC tries to send a packet at the same time, a collision is said to occur and the packets are discarded. The CSMA/CD protocol was designed to avoid this problem, more precisely to allow a NIC to send its packet without collision. This is done by way of a randomized exponential backoff process. In this paper, we analyse the correctness of the CSMA/CD protocol, using techniques from probabilistic model checking and approximate probabilistic model checking. The tools that we use are PRISM and APMC. Moreover, we provide a quantitative analysis of some CSMA/CD properties.", "num_citations": "65\n", "authors": ["521"]}
{"title": "Hierarchical QR factorization algorithms for multi-core clusters\n", "abstract": " This paper describes a new QR factorization algorithm which is especially designed for massively parallel platforms combining parallel distributed nodes, where a node is a multi-core processor. These platforms represent the present and the foreseeable future of high-performance computing. Our new QR factorization algorithm falls in the category of the tile algorithms which naturally enables good data locality for the sequential kernels executed by the cores (high sequential performance), low number of messages in a parallel distributed setting (small latency term), and fine granularity (high parallelism). Each tile algorithm is uniquely characterized by its sequence of reduction trees. In the context of a cluster of nodes, in order to minimize the number of inter-processor communications (aka, \u201ccommunication-avoiding\u201d), it is natural to consider hierarchical trees composed of an \u201cinter-node\u201d tree which acts on top of\u00a0\u2026", "num_citations": "61\n", "authors": ["521"]}
{"title": "Fault tolerance techniques for high-performance computing\n", "abstract": " This chapter provides an introduction to resilience methods. The emphasis is on checkpointing, the de-facto standard technique for resilience in High Performance Computing. We present the main two protocols, namely coordinated checkpointing and hierarchical checkpointing. Then we introduce performance models and use them to assess the performance of theses protocols. We cover the Young/Daly formula for the optimal period and much more! Next we explain how the efficiency of checkpointing can be improved via fault prediction or replication. Then we move to application-specific methods, such as ABFT. We conclude the chapter by discussing techniques to cope with silent errors (or silent data corruption).", "num_citations": "55\n", "authors": ["521"]}
{"title": "Scalability and parallelization of monte-carlo tree search\n", "abstract": " Monte-Carlo Tree Search is now a well established algorithm, in games and beyond. We analyze its scalability, and in particular its limitations and the implications in terms of parallelization. We focus on our Go program MoGo and our Havannah program Shakti. We use multicore machines and message-passing machines. For both games and on both type of machines we achieve adequate efficiency for the parallel version. However, in spite of promising results in self-play there are situations for which increasing the time per move does not solve anything. Therefore parallelization is not a solution to all our problems. Nonetheless, for problems where the Monte-Carlo part is less biased than in the game of Go, parallelization should be quite efficient, even without shared memory.", "num_citations": "54\n", "authors": ["521"]}
{"title": "APMC 3.0: Approximate verification of discrete and continuous time Markov chains\n", "abstract": " In this paper, we give a brief overview of APMC (approximate probabilistic model checker). APMC implements approximate probabilistic verification of probabilistic systems. It is based on Monte-Carlo method and the theory of randomized approximation schemes and allows to verify extremely large models without explicitly representing the global transition system. To avoid the state-space explosion phenomenon, APMC gives an accurate approximation of the satisfaction probability of the property instead of the exact value, but using only a very small amount of memory. The version of APMC we present can handle efficiently both discrete and continuous time probabilistic systems", "num_citations": "54\n", "authors": ["521"]}
{"title": "QR factorization of tall and skinny matrices in a grid computing environment\n", "abstract": " Previous studies have reported that common dense linear algebra operations do not achieve speed up by using multiple geographical sites of a computational grid. Because such operations are the building blocks of most scientific applications, conventional supercomputers are still strongly predominant in high-performance computing and the use of grids for speeding up large-scale scientific problems is limited to applications exhibiting parallelism at a higher level. We have identified two performance bottlenecks in the distributed memory algorithms implemented in ScaLAPACK, a state-of-the-art dense linear algebra library. First, because ScaLA-PACK assumes a homogeneous communication network, the implementations of ScaLAPACK algorithms lack locality in their communication pattern. Second, the number of messages sent in the ScaLAPACK algorithms is significantly greater than other algorithms that\u00a0\u2026", "num_citations": "48\n", "authors": ["521"]}
{"title": "On the combination of silent error detection and checkpointing\n", "abstract": " In this paper, we revisit traditional check pointing and rollback recovery strategies, with a focus on silent data corruption errors. Contrarily to fail-stop failures, such latent errors cannot be detected immediately, and a mechanism to detect them must be provided. We consider two models: (i) errors are detected after some delays following a probability distribution (typically, an Exponential distribution), (ii) errors are detected through some verification mechanism. In both cases, we compute the optimal period in order to minimize the waste, i.e., the fraction of time where nodes do not perform useful computations. In practice, only a fixed number of checkpoints can be kept in memory, and the first model may lead to an irrecoverable failure. In this case, we compute the minimum period required for an acceptable risk. For the second model, there is no risk of irrecoverable failure, owing to the verification mechanism, but the\u00a0\u2026", "num_citations": "42\n", "authors": ["521"]}
{"title": "A model for large scale self-stabilization\n", "abstract": " We introduce a new model for distributed algorithms designed for large scale systems that need a low-overhead solution to allow the processes to communicate with each other. We assume that every process can communicate with any other process provided it knows its identifier, which is usually the case in e.g. a peer to peer system, and that nodes may arrive or leave at any time. To cope with the large number of processes, we limit the memory usage of each process to a small constant number of variables, combining this with previous results concerning failure detectors and resource discovery. We illustrate the model with a self-stabilizing algorithm that builds and maintains a spanning tree topology. We provide a formal proof of the algorithm and the results of experiments on a cluster.", "num_citations": "34\n", "authors": ["521"]}
{"title": "Optimal checkpointing period: Time vs. energy\n", "abstract": " This short paper deals with parallel scientific applications using non-blocking and periodic coordinated checkpointing to enforce resilience. We provide a model and detailed formulas for total execution time and consumed energy. We characterize the optimal period for both objectives, and we assess the range of time/energy trade-offs to be made\u00a0by instantiating the model with a set of realistic scenarios for Exascale systems. We give a particular emphasis to I/O transfers, because the relative cost of communication is expected to dramatically increase, both in terms of latency and consumed energy, for future Exascale platforms.", "num_citations": "33\n", "authors": ["521"]}
{"title": "Multithreading in the PLASMA Library\n", "abstract": " Parallel Linear Algebra Software for Multicore Architectures (PLASMA) is a numerical software library for solving problems in dense linear algebra on systems of multicore processors and multisocket systems of multicore processors [1]. PLASMA offers routines for solving a wide range of problems in dense linear algebra such as nonsymmetric, symmetric, and symmetric positive definite systems of linear equations, least square problems, singular value problems, and eigenvalue problems (currently only symmetric eigenvalue problems). PLASMA solves these problems in real and complex arithmetic and in single and double precision. PLASMA is designed to give high efficiency on homogeneous multicore processors and multisocket systems of multicore processors. As of today, the majority of such systems are on-chip symmetric multiprocessors with classic super-scalar processors as their building blocks (x86 and\u00a0\u2026", "num_citations": "29\n", "authors": ["521"]}
{"title": "Supple: a flexible probabilistic data dissemination protocol for wireless sensor networks\n", "abstract": " We propose a flexible proactive data dissemination approach for data gathering in self-organized Wireless Sensor Networks (WSN). Our protocol Supple, effectively distributes and stores monitored data in WSNs such that it can be later sent to or retrieved by a sink. Supple empowers sensors with the ability to make on the fly forwarding and data storing decisions and relies on flexible and self-organizing selection criteria, which can follow any predefined distribution law. Using formal analysis and simulation, we show that Supple is effective in selecting storing nodes that respect the predefined distribution criterion with low overhead and limited network knowledge.", "num_citations": "27\n", "authors": ["521"]}
{"title": "Probabilistic verification of sensor networks.\n", "abstract": " Sensor networks are networks consisting of miniature and low-cost systems with limited computation power and energy. Thanks to the low cost of the devices, one can spread a huge number of sensors into a given area to monitor, for example, physical changes of the environment. Typical applications are in defense, environment, and design of ad-hoc networks areas. In this paper, we address the problem of verifying the correctness of such networks through a case study. We model a simple sensor network whose aim is to detect an event in a bounded area (such as a fire in a forest). The behavior of the network is probabilistic, so we use Approximate Probabilistic Model Checker (APMC), a tool that allows to approximately check the correctness of extremely large probabilistic systems, to verify it.", "num_citations": "26\n", "authors": ["521"]}
{"title": "Running parallel applications with topology-aware grid middleware\n", "abstract": " The concept of topology-aware grid applications is derived from parallelized computational models of complex systems that are executed on heterogeneous resources, either because they require specialized hardware for certain calculations, or because their parallelization is flexible enough to exploit such resources. Here we describe two such applications, a multi-body simulation of stellar evolution, and an evolutionary algorithm that is used for reverse-engineering gene regulatory networks. We then describe the topology-aware middleware we have developed to facilitate the \"modeling-implementing-executing\" cycle of complex systems applications. The developed middleware allows topology-aware simulations to run on geographically distributed clusters with or without firewalls between them. Additionally, we describe advanced coallocation and scheduling techniques that take into account the applications\u00a0\u2026", "num_citations": "24\n", "authors": ["521"]}
{"title": "Distribution, approximation and probabilistic model checking\n", "abstract": " APMC is a model checker dedicated to the quantitative verification of fully probabilistic systems against LTL formulas. Using a Monte-Carlo method in order to efficiently approximate the verification of probabilistic specifications, it could be used naturally in a distributed framework. We present here the tool and its distribution scheme, together with extensive performance evaluation, showing the scalability of the method, even on clusters containing 500+ heterogeneous workstations.", "num_citations": "20\n", "authors": ["521"]}
{"title": "Easy stabilization with an agent\n", "abstract": " The paper presents a technique for achieving stabilization in distributed systems. This technique, called agent-stabilization, uses an external tool, the agent, that can be considered as a special message created by a lower layer. Basically, an agent performs a traversal of the network and if necessary, modifies the local states of the nodes, yielding stabilization.", "num_citations": "19\n", "authors": ["521"]}
{"title": "Brief announcement: Self-stabilizing spanning tree algorithm for large scale systems\n", "abstract": " We introduce a self-stabilizing algorithm that builds and maintains a spanning tree topology on any large scale system. We assume that the existing topology is a complete graph and that nodes may arrive or leave at any time. To cope with the large number of processes of a grid or a peer to peer system, we limit the memory usage of each process to a small constant number of variables, combining this with previous results concerning failure detectors and resource discovery.", "num_citations": "18\n", "authors": ["521"]}
{"title": "Symbolic range analysis of pointers\n", "abstract": " Alias analysis is one of the most fundamental techniques that compilers use to optimize languages with pointers. However, in spite of all the attention that this topic has received, the current state-of-the-art approaches inside compilers still face challenges regarding precision and speed. In particular, pointer arithmetic, a key feature in C and C++, is yet to be handled satisfactorily. This paper presents a new alias analysis algorithm to solve this problem. The key insight of our approach is to combine alias analysis with symbolic range analysis. This combination lets us disambiguate fields within arrays and structs, effectively achieving more precision than traditional algorithms. To validate our technique, we have implemented it on top of the LLVM compiler. Tests on a vast suite of benchmarks show that we can disambiguate several kinds of C idioms that current state-of-the-art analyses cannot deal with. In particular, we\u00a0\u2026", "num_citations": "16\n", "authors": ["521"]}
{"title": "Evaluating complex MAC protocols for sensor networks with APMC\n", "abstract": " In this paper we present an analysis of a MAC (Medium Access Control) protocol for wireless sensor networks. The purpose of this protocol is to manage wireless media access by constructing a Time Division Media Access (TDMA) schedule. APMC (Approximate Probabilistic Model Checker) is a tool that uses approximation-based verification techniques in order to analyse the behavior of complex probabilistic systems. Using APMC, we approximately computed the probabilities of several properties of the MAC protocol being studied, thus giving some insights about it performance.", "num_citations": "15\n", "authors": ["521"]}
{"title": "Revisiting the double checkpointing algorithm\n", "abstract": " Fast check pointing algorithms require distributed access to stable storage. This paper revisits the approach base upon double check pointing, and compares the blocking algorithm of Zheng, Shi and Kal\u00e9, with the non-blocking algorithm of Ni, Meneses and Kal\u00e9, in terms of both performance and risk. We also extend their model proposed to assess the impact of the overhead associated to non-blocking communications. We then provide a new peer-to-peer check pointing algorithm, called the triple check pointing algorithm, that can work at constant memory, and achieves both higher efficiency and better risk handling than the double check pointing algorithm. We provide performance and risk models for all the evaluated protocols, and compare them through comprehensive simulations.", "num_citations": "14\n", "authors": ["521"]}
{"title": "DSL-lab: a low-power lightweight platform to experiment on domestic broadband internet\n", "abstract": " This article presents the design and building of DSL-Lab, a platform to experiment on distributed computing over broadband domestic Internet. Experimental platforms such as PlanetLab and Grid'5000 are promising methodological approaches to study distributed systems. However, both platforms focus on high-end service and network deployments only available on a restricted part of the Internet, leaving aside the possibility for researchers to experiment in conditions close to what is usually available with domestic connection to the Internet. DSL-Lab is a complementary approach to PlanetLab and Grid'5000 to experiment with distributed computing in an environment closer to how Internet appears, when applications are run on end-user PCs. DSL-Lab is a set of 40 low-power and low-noise nodes, which are hosted by participants, using the participants' xDSL or cable access to the Internet. The objective is to\u00a0\u2026", "num_citations": "13\n", "authors": ["521"]}
{"title": "Pointer disambiguation via strict inequalities\n", "abstract": " The design and implementation of static analyses that disambiguate pointers has been a focus of research since the early days of compiler construction. One of the challenges that arise in this context is the analysis of languages that support pointer arithmetics, such as C, C++ and assembly dialects. This paper contributes to solve this challenge. We start from an obvious, yet unexplored, observation: if a pointer is strictly less than another, they cannot alias. Motivated by this remark, we use abstract interpretation to build strict less-than relations between pointers. We construct a program representation that bestows the Static Single Information (SSI) property onto our dataflow analysis. SSI gives us a sparse algorithm, whose correctness is easy to ensure. We have implemented our static analysis in LLVM. It runs in time linear on the number of program variables, and, depending on the benchmark, it can be as much as\u00a0\u2026", "num_citations": "11\n", "authors": ["521"]}
{"title": "Recent Advances in Parallel Virtual Machine and Message Passing Interface: 10th European PVM/MPI Users' Group Meeting, Venice, Italy, September 29-October 2, 2003, Proceedings\n", "abstract": " Themessagepassingparadigmisconsideredthemoste? ectivewaytodevelop-? cient parallel applications. PVM (Parallel Virtual Machine) and MPI (Message Passing Interface) are the most frequently used tools for programming message passing applications. This volume includes the selected contributions presented at the 10th-ropean PVM/MPI Users\u2019 Group Meeting (Euro PVM/MPI 2003), which was held in Venice, Italy, September 29\u2013October 2, 2003. The conference was jointly organized by the Department of Computer Science of the Ca\u2019Foscari University of Venice, Italy and the Information Science and Technologies Institute of the National Research Council (ISTI-CNR), Pisa, Italy. TheconferencewaspreviouslyheldinLinz, Austria (2002), Santorini, Greece (2001), Balatonfured, \u0308 Hungary (2000), Barcelona, Spain (1999), Liverpool, UK (1998), and Krakow, Poland (1997). The? rst three conferences were devoted to PVM and were held in Munich, Germany (1996), Lyon, France (1995), and Rome, Italy (1994). The conference has become a forum for users and developers of PVM, MPI, and other message passing environments. Interactions between these groups has proved to be very useful for developing new ideas in parallel computing, and for applying some of those already existent to new practical? elds. The main topics of the meeting were evaluation and performance of PVM and MPI, ext-sions, implementations and improvements of PVM and MPI, parallel algorithms using the message passing paradigm, and parallel applications in science and engineering. In addition, the topics of the conference were extended to include Grid computing\u00a0\u2026", "num_citations": "11\n", "authors": ["521"]}
{"title": "Performance and reliability trade-offs for the double checkpointing algorithm\n", "abstract": " Fast checkpointing algorithms require distributed access to stable storage. This paper revisits the approach based upon double checkpointing, and compares the blocking algorithm of Zheng, Shi and Kal\u00e9 [23], with the non-blocking algorithm of Ni, Meneses and Kal\u00e9 [15] in terms of both performance and risk. We also extend the model proposedcan provide a better efficiency in [23, 15] to assess the impact of the overhead associated to non-blocking communications. In addition, we deal with arbitrary failure distributions (as opposed to uniform distributions in [23]). We then provide a new peer-to-peer checkpointing algorithm, called the triple checkpointing algorithm, that can work without additional memory, and achieves both higher efficiency and better risk handling than the double checkpointing algorithm. We provide performance and risk models for all the evaluated protocols, and compare them through comprehensive simulations.", "num_citations": "9\n", "authors": ["521"]}
{"title": "Replication is more efficient than you think\n", "abstract": " This paper revisits replication coupled with checkpointing for fail-stop errors. Replication enables the application to survive many fail-stop errors, thereby allowing for longer checkpointing periods. Previously published works use replication with the no-restart strategy, which works as follows:(i) compute the application Mean Time To Interruption (MTTI) M as a function of the number of processor pairs and the individual processor Mean Time Between Failures (MTBF);(ii) use checkpointing period [EQUATION] \u00e0 la Young/Daly, where C is the checkpoint duration; and (iii) never restart failed processors until the application crashes. We introduce the restart strategy where failed processors are restarted after each checkpoint. We compute the optimal checkpointing period [EQUATION] for this strategy, which is much larger than [EQUATION], thereby decreasing I/O pressure. We show through simulations that using\u00a0\u2026", "num_citations": "8\n", "authors": ["521"]}
{"title": "Accelerating the cosmic microwave background map-making procedure through preconditioning\n", "abstract": " Estimation of the sky signal from sequences of time ordered data is one of the key steps in cosmic microwave background (CMB) data analysis, commonly referred to as the map-making problem. Some of the most popular and general methods proposed for this problem involve solving generalised least-squares (GLS) equations with non-diagonal noise weights given by a block-diagonal matrix with Toeplitz blocks. In this work, we study new map-making solvers potentially suitable for applications to the largest anticipated data sets. They are based on iterative conjugate gradient (CG) approaches enhanced with novel, parallel, two-level preconditioners. We apply the proposed solvers to examples of simulated non-polarised and polarised CMB observations and a set of idealised scanning strategies with sky coverage ranging from a nearly full sky down to small sky patches. We discuss their implementation for\u00a0\u2026", "num_citations": "8\n", "authors": ["521"]}
{"title": "SDN-based Wi-Fi Direct clustering for cloud access in campus networks\n", "abstract": " Mobile cloud is changing the way to enroll teaching activities in a university campus. Lectures and lab sessions can be carried out directly from tablets in a classroom by accessing a server in the cloud. In this paper, we address the problem of high-density cloud access with wireless devices in campus networks. We propose to use Wi-Fi direct clustering to solve the problem of quality of service (QoS) degradation when a high number of wireless devices want to access a content in the cloud at the same time. A centralized software-defined network controller is used in our proposed architecture to capture the network state and organize the Wi-Fi Direct groups. The optimized number of clusters can be calculated in function of the number of devices in the room. By simulations, we show that we can provide a better QoS in terms of download time and application\u2019s throughput by reducing the interference in this\u00a0\u2026", "num_citations": "7\n", "authors": ["521"]}
{"title": "Fault-local stabilization: the shortest path tree\n", "abstract": " We present a fault-local solution to the shortest path tree problem in a rooted network. We consider the case where a transient fault corrupts f nodes (f is unknown, but inferior to half the size of the network) after the tree has been constructed. Our solution allows to recover in less than O (f) time units. If an upper bound k on the number of corrupted nodes is known, the memory space needed depends only on k.", "num_citations": "6\n", "authors": ["521"]}
{"title": "SAFE-OS: A secure and usable desktop operating system\n", "abstract": " Containment of application execution is a key security feature of operating systems. Without strong containment, an attacker who compromises one process may take control of the whole machine. Virtualization technology has been widely used in server systems to strongly isolate various applications or services in different virtual machines; its usage in desktop systems which are much more interactive (interactions with the user and between applications) is a challenging task. In this paper we describe SAFE-OS, a desktop operating system using virtualization technology. SAFE-OS provides a high level of isolation between processes while maintaining a standard user interface that abstracts the underlying complexity.", "num_citations": "5\n", "authors": ["521"]}
{"title": "Cell assisted APMC\n", "abstract": " In this paper, we give an overview of APMC-CA (cell assisted approximate probabilistic model checker). APMC-CA is a new version of APMC dedicated to the cell processor. We show that using the cell architecture, we achieve better performances than APMC 3.0.", "num_citations": "5\n", "authors": ["521"]}
{"title": "Emulation platform for high accuracy failure injection in grids\n", "abstract": " In the process of developping grid applications, people need to often evaluate the robustness of their work. Two common approaches are, simulation where one can evaluates his software and predict behaviors under conditions usually unachievable in a laboratory experiment and experimentation where the actual application is launched on an actual grid. However simulation could ignore unpredictable behaviors due to the abstraction done and experimation does not guarantee a controlled and reproducible environment. In this chapter, we propose an emulation platform for parallel and distributed systems including grids where both the machines and the network are virtualized at a low level. The use of virtual machines allows us to test highly accurate failure injection since we can \u201cdestroy\u201d virtual machines and, network virtualization provides low-level network emulation. Failure accuracy is a criteria that notes how realistic a fault is. The accuracy of our framework is evaluated through a set of micro benchmarks and a very stable P2P system call Pastry since we are very interested in the publication system and resources finding of grid systems.", "num_citations": "4\n", "authors": ["521"]}
{"title": "On the complexity of a self-stabilizing spanning tree algorithm for large scale systems\n", "abstract": " Many large scale systems, like grids and structured peer to peer systems, operate on a constrained topology. Since underlying networks do not expose the real topology to the applications, an algorithm should build and maintain a virtual topology for the application. This algorithm has to bootstrap the system and react to the arrival and departures of processes. In a previous article, we introduced a computing model designed for scalability in which we gave a self-stabilizing algorithm that builds a spanning tree. At that time, we provided a proof of stabilization and performance measurements of a prototypal implementation. In this work, we present a probabilistic method to evaluate the theoretical performances of algorithms in this model, and provide a probabilistic analysis of the convergence time of the algorithm.", "num_citations": "4\n", "authors": ["521"]}
{"title": "Software-defined Events through PAPI\n", "abstract": " PAPI has been used for almost two decades as an abstraction and standardization layer for profiling hardware-specific performance metrics. However, application developers-and profiling software packages-are quite often interested in information beyond hardware counters, such as the behavior of libraries used by the software that is being profiled. So far, accessing this information has required interfacing directly with the libraries on a case-by-case basis, or low-level binary instrumentation. In this paper, we introduce the new Software-Defined Event (SDE) component of PAPI which aims to enable PAPI to serve as an abstraction and standardization layer for events that originate in software layers as well. Extending PAPI to include SDEs enables monitoring of both types of performance events-hardware-and software-related events-in a uniform way, through the same consistent PAPI interface. Furthermore\u00a0\u2026", "num_citations": "3\n", "authors": ["521"]}
{"title": "Determining the optimal redistribution for a given data partition\n", "abstract": " The classical redistribution problem aims at optimally scheduling communications when moving from an initial data distribution to a target distribution where each processor will host a subset of data items. However, modern computing platforms are equipped with a powerful interconnection switch, and the cost of a given communication is (almost) independent of the location of its sender and receiver. This leads to generalizing the redistribution problem as follows: find the optimal one-toone mapping of the subsets of data items onto the processors for which the cost of the redistribution is minimal. This paper studies the complexity of this generalized problem. We provide optimal algorithms and evaluate their gain over classical redistribution through simulations. We also show the NP-hardness of the problem to find the optimal data partition and processor permutation (defined by new subsets) that minimize the cost of\u00a0\u2026", "num_citations": "3\n", "authors": ["521"]}
{"title": "Determining the optimal redistribution\n", "abstract": " The classical redistribution problem aims at optimally scheduling communications when moving from an initial data distribution \\Dini to a target distribution \\Dtar where each processor  will host a subset  of data items. However, modern computing platforms are equipped with a powerful interconnection switch, and the cost of a given communication is (almost) independent of the location of its sender and receiver. This leads to generalizing the redistribution problem as follows: find the optimal permutation  of processors such that  will host the set , and for which the cost of the redistribution is minimal. This report studies the complexity of this generalized problem. We provide optimal algorithms and evaluate their gain over classical redistribution through simulations. We also show the NP-hardness of the problem to find the optimal data partition and processor permutation (defined by new subsets ) that minimize the cost of redistribution followed by a simple computation kernel.", "num_citations": "3\n", "authors": ["521"]}
{"title": "A rollback-recovery protocol on peer to peer systems\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "3\n", "authors": ["521"]}
{"title": "Message relaying techniques for computational grids and their relations to fault tolerant message passing for the Grid\n", "abstract": " In order to execute without modification Message Passing distributed applications on a computational grid, one has to address many issues. The first to come is how let processes of two different clusters communicate. In this work, we study the performances of relaying techniques (passing messages to a middle-tier) to solve this issue. When using relays, messages and most of the nondeterministic behavior of nodes pass through the relays during the execution. This provides the ability to implement fault tolerance at the relay level using pessimistic message logging techniques. We also evaluate the overhead of this logging and study how relays should be designed and fault tolerance protocols composed to provide a full fault-tolerant Message Passing Interface library for computational grids.", "num_citations": "2\n", "authors": ["521"]}
{"title": "Table de hachage distribu\u00e9e autostabilisante\n", "abstract": " Nous pr\u00e9sentons un algorithme autostabilisant qui construit et entretient une table de hachage distribu\u00e9e dans un environnement pair \u00e0 pair. L'utilisation d'un mod\u00e8le dans lequel les processus ne connaissent pas leurs voisins a priori le rend utilisable sur des syst\u00e8mes \u00e0 grande \u00e9chelle. Ses performances en nombre de messages sont de l'ordre des r\u00e9f\u00e9rences du domaine (notamment Chord), la r\u00e9plication des donn\u00e9es est \u00e9galement assur\u00e9e. La base sur lequel il est construit permet d'en donner une preuve formelle de correction.", "num_citations": "1\n", "authors": ["521"]}