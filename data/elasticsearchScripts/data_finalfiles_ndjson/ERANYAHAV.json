{"title": "code2vec: Learning distributed representations of code\n", "abstract": " We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them.   We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities\u00a0\u2026", "num_citations": "464\n", "authors": ["283"]}
{"title": "code2seq: Generating sequences from structured representations of code\n", "abstract": " The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present ${\\rm {\\scriptsize CODE2SEQ}}$: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree (AST) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art NMT models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.", "num_citations": "276\n", "authors": ["283"]}
{"title": "On the practical computational power of finite precision rnns for language recognition\n", "abstract": " While Recurrent Neural Networks (RNNs) are famously known to be Turing complete, this relies on infinite precision in the states and unbounded computation time. We consider the case of RNNs with finite precision whose computation time is linear in the input length. Under these limitations, we show that different RNN variants have different computational power. In particular, we show that the LSTM and the Elman-RNN with ReLU activation are strictly stronger than the RNN with a squashing activation and the GRU. This is achieved because LSTMs and ReLU-RNNs can easily implement counting behavior. We show empirically that the LSTM does indeed learn to effectively use the counting mechanism.", "num_citations": "186\n", "authors": ["283"]}
{"title": "Verifying safety properties of concurrent Java programs using 3-valued logic\n", "abstract": " We provide a parametric framework for verifying safety properties of concurrent Java programs. The framework combines thread-scheduling information with information about the shape of the heap. This leads to error-detection algorithms that are more precise than existing techniques. The framework also provides the most precise shape-analysis algorithm for concurrent programs. In contrast to existing verification techniques, we do not put a bound on the number of allocated objects. The framework even produces interesting results when analyzing Java programs with an unbounded number of threads. The framework is applied to successfully verify the following properties of a concurrent program: \u2022Concurrent manipulation of linked-list based ADT preserves the ADT datatype invariant [19]. \u2022The program does not perform inconsistent updates due to interference. \u2022The program does not reach a deadlock. \u2022The\u00a0\u2026", "num_citations": "155\n", "authors": ["283"]}
{"title": "Automatic inference of memory fences\n", "abstract": " We addresses the problem of automatic verification and fence inference in concurrent programs running under relaxed memory models. Modern architectures implement relaxed memory models in which memory operations may be reordered and executed non-atomically. Instructions called memory fences are provided to the programmer, allowing control of this behavior. To ensure correctness of many algorithms, the programmer is often required to explicitly insert memory fences into her program. However, she must use as few fences as possible, or the benefits of the relaxed architecture may be lost. It is our goal to help automate the fence insertion process. We present an algorithm for automatic inference of memory fences in concurrent programs, relieving the programmer from this complex task. Given a finite-state program, a safety specification and a description of the memory model our algorithm computes a\u00a0\u2026", "num_citations": "136\n", "authors": ["283"]}
{"title": "Tracelet-based code search in executables\n", "abstract": " We address the problem of code search in executables. Given a function in binary form and a large code base, our goal is to statically find similar functions in the code base. Towards this end, we present a novel technique for computing similarity between functions. Our notion of similarity is based on decomposition of functions into tracelets: continuous, short, partial traces of an execution. To establish tracelet similarity in the face of low-level compiler transformations, we employ a simple rewriting engine. This engine uses constraint solving over alignment constraints and data dependencies to match registers and memory addresses between tracelets, bridging the gap between tracelets that are otherwise similar. We have implemented our approach and applied it to find matches in over a million binary functions. We compare tracelet matching to approaches based on n-grams and graphlets and show that tracelet\u00a0\u2026", "num_citations": "134\n", "authors": ["283"]}
{"title": "A general path-based representation for predicting program properties\n", "abstract": " Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning.  We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens.  We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages.  We evaluate our approach on the tasks of\u00a0\u2026", "num_citations": "129\n", "authors": ["283"]}
{"title": "Chameleon: adaptive selection of collections\n", "abstract": " Languages such as Java and C#, as well as scripting languages like Python, and Ruby, make extensive use of Collection classes. A collection implementation represents a fixed choice in the dimensions of operation time, space utilization, and synchronization. Using the collection in a manner not consistent with this fixed choice can cause significant performance degradation. In this paper, we present CHAMELEON, a low-overhead automatic tool that assists the programmer in choosing the appropriate collection implementation for her application. During program execution, CHAMELEON computes elaborate trace and heap-based metrics on collection behavior. These metrics are consumed on-thefly by a rules engine which outputs a list of suggested collection adaptation strategies. The tool can apply these corrective strategies automatically or present them to the programmer. We have implemented CHAMELEON\u00a0\u2026", "num_citations": "121\n", "authors": ["283"]}
{"title": "Extracting automata from recurrent neural networks using queries and counterexamples\n", "abstract": " We present a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN. We do this using Angluin\u2019s\\lstar algorithm as a learner and the trained RNN as an oracle. Our technique efficiently extracts accurate automata from trained RNNs, even when the state vectors are large and require fine differentiation.", "num_citations": "120\n", "authors": ["283"]}
{"title": "Deriving linearizable fine-grained concurrent objects\n", "abstract": " Practical and efficient algorithms for concurrent data structures are difficult to construct and modify. Algorithms in the literature are often optimized for a specific setting, making it hard to separate the algorithmic insights from implementation details. The goal of this work is to systematically construct algorithms for a concurrent data structure starting from its sequential implementation. Towards that goal, we follow a construction process that combines manual steps corresponding to high-level insights with automatic exploration of implementation details. To assist us in this process, we built a new tool called Paraglider. The tool quickly explores large spaces of algorithms and uses bounded model checking to check linearizability of algorithms.", "num_citations": "116\n", "authors": ["283"]}
{"title": "Typestate-based semantic code search over partial programs\n", "abstract": " We present a novel code search approach for answering queries focused on API-usage with code showing how the API should be used. To construct a search index, we develop new techniques for statically mining and consolidating temporal API specifications from code snippets. In contrast to existing semantic-based techniques, our approach handles partial programs in the form of code snippets. Handling snippets allows us to consume code from various sources such as parts of open source projects, educational resources (eg tutorials), and expert code sites. To handle code snippets, our approach (i) extracts a possibly partial temporal specification from each snippet using a relatively precise static analysis tracking a generalized notion of typestate, and (ii) consolidates the partial temporal specifications, combining consistent partial information to yield consolidated temporal specifications, each of which captures\u00a0\u2026", "num_citations": "110\n", "authors": ["283"]}
{"title": "Statistical similarity of binaries\n", "abstract": " We address the problem of finding similar procedures in stripped binaries. We present a new statistical approach for measuring the similarity between two procedures. Our notion of similarity allows us to find similar code even when it has been compiled using different compilers, or has been modified. The main idea is to use similarity by composition: decompose the code into smaller comparable fragments, define semantic similarity between fragments, and use statistical reasoning to lift fragment similarity into similarity between procedures. We have implemented our approach in a tool called Esh, and applied it to find various prominent vulnerabilities across compilers and versions, including Heartbleed, Shellshock and Venom. We show that Esh produces high accuracy results, with few to no false positives -- a crucial factor in the scenario of vulnerability search in stripped binaries.", "num_citations": "103\n", "authors": ["283"]}
{"title": "Partial-coherence abstractions for relaxed memory models\n", "abstract": " We present an approach for automatic verification and fence inference in concurrent programs running under relaxed memory models. Verification under relaxed memory models is a hard problem. Given a finite state program and a safety specification, verifying that the program satisfies the specification under a sufficiently relaxed memory model is undecidable. For stronger models, the problem is decidable but has non-primitive recursive complexity. In this paper, we focus on models that have store-buffer based semantics, e.g., SPARC TSO and PSO. We use abstract interpretation to provide an effective verification procedure for programs running under this type of models. Our main contribution is a family of novel partial-coherence abstractions, specialized for relaxed memory models, which partially preserve information required for memory coherence and consistency. We use our abstractions to automatically\u00a0\u2026", "num_citations": "93\n", "authors": ["283"]}
{"title": "Dynamic synthesis for relaxed memory models\n", "abstract": " Modern architectures implement relaxed memory models which may reorder memory operations or execute them non-atomically. Special instructions called memory fences are provided, allowing control of this behavior. To implement a concurrent algorithm for a modern architecture, the programmer is forced to manually reason about subtle relaxed behaviors and figure out ways to control these behaviors by adding fences to the program. Not only is this process time consuming and error-prone, but it has to be repeated every time the implementation is ported to a different architecture. In this paper, we present the first scalable framework for handling real-world concurrent algorithms running on relaxed architectures. Given a concurrent C program, a safety specification, and a description of the memory model, our framework tests the program on the memory model to expose violations of the specification, and\u00a0\u2026", "num_citations": "84\n", "authors": ["283"]}
{"title": "Verifying safety properties using separation and heterogeneous abstractions\n", "abstract": " In this paper, we show how separation (decomposing a verification problem into a collection of verification subproblems) can be used to improve the efficiency and precision of verification of safety properties. We present a simple language for specifying separation strategies for decomposing a single verification problem into a set of subproblems. (The strategy specification is distinct from the safety property specification and is specified separately.) We present a general framework of heterogeneous abstraction that allows different parts of the heap to be abstracted using different degrees of precision at different points during the analysis. We show how the goals of separation (i.e., more efficient verification) can be realized by first using a separation strategy to transform (instrument) a verification problem instance (consisting of a safety property specification and an input program), and by then utilizing heterogeneous\u00a0\u2026", "num_citations": "65\n", "authors": ["283"]}
{"title": "On the bottleneck of graph neural networks and its practical implications\n", "abstract": " Since the proposal of the graph neural network (GNN) by Gori et al. (2005) and Scarselli et al. (2008), one of the major problems in training GNNs was their struggle to propagate information between distant nodes in the graph. We propose a new explanation for this problem: GNNs are susceptible to a bottleneck when aggregating messages across a long path. This bottleneck causes the over-squashing of exponentially growing information into fixed-size vectors. As a result, GNNs fail to propagate messages originating from distant nodes and perform poorly when the prediction task depends on long-range interaction. In this paper, we highlight the inherent problem of over-squashing in GNNs: we demonstrate that the bottleneck hinders popular GNNs from fitting long-range signals in the training data; we further show that GNNs that absorb incoming edges equally, such as GCN and GIN, are more susceptible to over-squashing than GAT and GGNN; finally, we show that prior work, which extensively tuned GNN models of long-range problems, suffers from over-squashing, and that breaking the bottleneck improves their state-of-the-art results without any tuning or additional weights. Our code is available at https://github.com/tech-srl/bottleneck/ .", "num_citations": "59\n", "authors": ["283"]}
{"title": "Similarity of binaries through re-optimization\n", "abstract": " We present a scalable approach for establishing similarity between stripped binaries (with no debug information). The main challenge in binary similarity, is to establish similarity even when the code has been compiled using different compilers, with different optimization levels, or targeting different architectures. Overcoming this challenge, while avoiding false positives, is invaluable to the process of reverse engineering and the process of locating vulnerable code.", "num_citations": "55\n", "authors": ["283"]}
{"title": "Predicate abstraction for relaxed memory models\n", "abstract": " We present a novel approach for predicate abstraction of programs running on relaxed memory models. Our approach consists of two steps.             First, we reduce the problem of verifying a program P running on a memory model M to the problem of verifying a program P                                M                that captures an abstraction of M as part of the program.             Second, we present a new technique for discovering predicates that enable verification of P                                M               . The core idea is to extrapolate from the predicates used to verify P under sequential consistency. A key new concept is that of cube extrapolation: it successfully avoids exponential state explosion when abstracting P                                M               .             We implemented our approach for the x86 TSO and PSO memory models and showed that predicates discovered via extrapolation are powerful enough to verify several challenging\u00a0\u2026", "num_citations": "49\n", "authors": ["283"]}
{"title": "Correctness-preserving derivation of concurrent garbage collection algorithms\n", "abstract": " Constructing correct concurrent garbage collection algorithms is notoriously hard. Numerous such algorithms have been proposed, implemented, and deployed - and yet the relationship among them in terms of speed and precision is poorly understood, and the validation of one algorithm does not carry over to others.As programs with low latency requirements written in garbagecollected languages become part of society's mission-critical infrastructure, it is imperative that we raise the level of confidence in the correctness of the underlying system, and that we understand the trade-offs inherent in our algorithmic choice.In this paper we present correctness-preserving transformations that can be applied to an initial abstract concurrent garbage collection algorithm which is simpler, more precise, and easier to prove correct than algorithms used in practice--but also more expensive and with less concurrency. We then\u00a0\u2026", "num_citations": "48\n", "authors": ["283"]}
{"title": "Abstract semantic differencing via speculative correlation\n", "abstract": " We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence. We focus on infinite-state numerical programs, and use abstract interpretation to compute an over-approximation of program differences.", "num_citations": "46\n", "authors": ["283"]}
{"title": "Firmup: Precise static detection of common vulnerabilities in firmware\n", "abstract": " We present a static, precise, and scalable technique for finding CVEs (Common Vulnerabilities and Exposures) in stripped firmware images. Our technique is able to efficiently find vulnerabilities in real-world firmware with high accuracy. Given a vulnerable procedure in an executable binary and a firmware image containing multiple stripped binaries, our goal is to detect possible occurrences of the vulnerable procedure in the firmware image. Due to the variety of architectures and unique tool chains used by vendors, as well as the highly customized nature of firmware, identifying procedures in stripped firmware is extremely challenging. Vulnerability detection requires not only pairwise similarity between procedures but also information about the relationships between procedures in the surrounding executable. This observation serves as the foundation for a novel technique that establishes a partial correspondence\u00a0\u2026", "num_citations": "45\n", "authors": ["283"]}
{"title": "Exploiting social navigation\n", "abstract": " We present an effective Sybil attack against social location based services. Our attack is based on creating a large number of reputed \"bot drivers\", and controlling their reported locations using fake GPS reports. We show how this attack can be used to influence social navigation systems by applying it to Waze - a prominent social navigation application used by over 50 million drivers. We show that our attack can fake traffic jams and dramatically influence routing decisions. We present several techniques for preventing the attack, and show that effective mitigation likely requires the use of additional carrier information.", "num_citations": "45\n", "authors": ["283"]}
{"title": "Effective abstractions for verification under relaxed memory models\n", "abstract": " We present a new abstract interpretation based approach for automatically verifying concurrent programs running on relaxed memory models. Our approach is based on three key insights: (i) Although the behaviors of relaxed memory models (e.g., TSO and PSO) are naturally captured by store buffers, directly using such encodings substantially decreases the accuracy of program analysis due to shift operations on buffer contents. The scalability and accuracy of program analysis can be greatly improved by eliminating the expensive shifting of store buffer contents, and we present a new abstraction of the memory model that accomplishes this goal. (ii) The precision of the analysis can be further improved by an encoding of store buffer sizes using leveraged knowledge of the abstract interpretation domain. (iii) A novel source-to-source transformation that realizes the above two techniques makes it possible to use of\u00a0\u2026", "num_citations": "44\n", "authors": ["283"]}
{"title": "Extracting code from programming tutorial videos\n", "abstract": " The number of programming tutorial videos on the web increases daily. Video hosting sites such as YouTube host millions of video lectures, with many programming tutorials for various languages and platforms. These videos contain a wealth of valuable information, including code that may be of interest. However, two main challenges have so far prevented the effective indexing of programming tutorial videos:(i) code in tutorials is typically written on-the-fly, with only parts of the code visible in each frame, and (ii) optical character recognition (OCR) is not precise enough to produce quality results from videos. We present a novel approach for extracting code from videos that is based on:(i) consolidating code across frames, and (ii) statistical language models for applying corrections at different levels, allowing us to make corrections by choosing the most likely token, combination of tokens that form a likely line\u00a0\u2026", "num_citations": "42\n", "authors": ["283"]}
{"title": "Abstract semantic differencing for numerical programs\n", "abstract": " We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence when no difference exists.             We focus on computing semantic differences in numerical programs where the values of variables have no a-priori bounds, and use abstract interpretation to compute an over-approximation of program differences. Computing differences and establishing equivalence under abstraction requires abstracting relationships between variables in the two programs. Towards that end, we first construct a correlating program in which these relationships can be tracked, and then use a correlating abstract domain to compute a sound approximation of these relationships. To better establish equivalence between correlated variables and precisely\u00a0\u2026", "num_citations": "42\n", "authors": ["283"]}
{"title": "Typestate verification: Abstraction techniques and complexity results\n", "abstract": " We consider the problem of typestate verification for shallow programs; i.e., programs where pointers from program variables to heap-allocated objects are allowed, but where heap-allocated objects may not themselves contain pointers. We prove a number of results relating the complexity of verification to the nature of the finite state machine used to specify the property. Some properties are shown to be intractable, but others which appear to be quite similar admit polynomial-time verification algorithms. Our results serve to provide insight into the inherent complexity of important classes of verification problems. In addition, the program abstractions used for the polynomial-time verification algorithms may be of independent interest.", "num_citations": "39\n", "authors": ["283"]}
{"title": "CGCExplorer: a semi-automated search procedure for provably correct concurrent collectors\n", "abstract": " Concurrent garbage collectors are notoriously hard to design, implement, and verify. We present a framework for the automatic exploration of a space of concurrent mark-and-sweep collectors. In our framework, the designer specifies a set of\" building blocks\" from which algorithms can be constructed. These blocks reflect the designer's insights about the coordination between the collector and the mutator. Given a set of building blocks, our framework automatically explores a space of algorithms, using model checking with abstraction to verify algorithms in the space.", "num_citations": "32\n", "authors": ["283"]}
{"title": "Estimating types in binaries using predictive modeling\n", "abstract": " Reverse engineering is an important tool in mitigating vulnerabilities in binaries. As a lot of software is developed in object-oriented languages, reverse engineering of object-oriented code is of critical importance. One of the major hurdles in reverse engineering binaries compiled from object-oriented code is the use of dynamic dispatch. In the absence of debug information, any dynamic dispatch may seem to jump to many possible targets, posing a significant challenge to a reverse engineer trying to track the program flow. We present a novel technique that allows us to statically determine the likely targets of virtual function calls. Our technique uses object tracelets \u2013 statically constructed sequences of operations performed on an object \u2013 to capture potential runtime behaviors of the object. Our analysis automatically pre-labels some of the object tracelets by relying on instances where the type of an object is known\u00a0\u2026", "num_citations": "31\n", "authors": ["283"]}
{"title": "Structural language models of code\n", "abstract": " We address the problem of any-code completion-generating a missing piece of source code in a given program without any restriction on the vocabulary or structure. We introduce a new approach to any-code completion that leverages the strict syntax of programming languages to model a code snippet as a tree-structural language modeling (SLM). SLM estimates the probability of the program\u2019s abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous techniques that have severely restricted the kinds of expressions that can be generated in this task, our approach can generate arbitrary code in any programming language. Our model significantly outperforms both seq2seq and a variety of structured approaches in generating Java and C# code. Our code, data, and trained models are available at http://github. com/tech-srl/slm-code-generation/. An online demo is available at http://AnyCodeGen. org.", "num_citations": "26\n", "authors": ["283"]}
{"title": "Adversarial examples for models of code\n", "abstract": " Neural models of code have shown impressive results when performing tasks such as predicting method names and identifying certain kinds of bugs. We show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code using adversarial examples. The main idea of our approach is to force a given trained model to make an incorrect prediction, as specified by the adversary, by introducing small perturbations that do not change the program\u2019s semantics, thereby creating an adversarial example. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving the desired prediction with respect to the model\u2019s inputs, while holding the model weights constant, and following the gradients to slightly modify the input code.  We show that our DAMP attack is effective across three\u00a0\u2026", "num_citations": "26\n", "authors": ["283"]}
{"title": "Synthesis of memory fences via refinement propagation\n", "abstract": " We address the problem of fence inference in infinite-state concurrent programs running on relaxed memory models such as TSO and PSO. We present a novel algorithm that can automatically synthesize the necessary fences for infinite-state programs.             Our technique is based on two main ideas: (i) verification with numerical domains: we reduce verification under relaxed models to verification under sequential consistency using integer and boolean variables. This enables us to combine abstraction refinement over booleans with powerful numerical abstractions over the integers. (ii) synthesis with refinement propagation: to synthesize fences for a program P, we combine abstraction refinements used for successful synthesis of programs coarser than P into a new candidate abstraction for P. This \u201cproof reuse\u201d approach dramatically reduces the time required to discover a proof for P.             We\u00a0\u2026", "num_citations": "24\n", "authors": ["283"]}
{"title": "Programming not only by example\n", "abstract": " Recent years have seen great progress in automated synthesis techniques that can automatically generate code based on some intent expressed by the programmer, but communicating this intent remains a major challenge. When the expressed intent is coarse-grained (for example, restriction on the expected type of an expression), the synthesizer often produces a long list of results for the programmer to choose from, shifting the heavy-lifting to the user. An alternative approach, successfully used in end-user synthesis, is programming by example (PBE), where the user leverages examples to interactively and iteratively refine the intent. However, using only examples is not expressive enough for programmers, who can observe the generated program and refine the intent by directly relating to parts of the generated program. We present a novel approach to interacting with a synthesizer using a granular interaction\u00a0\u2026", "num_citations": "23\n", "authors": ["283"]}
{"title": "Compiler Optimization of C++ Virtual Function Calls.\n", "abstract": " We describe two generic optimization techniques to improve run-time performance of C++ virtual function calls: type specification and type prediction. Both involve program analysis that results in a set of call sites to be optimized, and code transformations that replace the original dispatching mechanism in these sites by more efficient call expressions. We implement two special cases. The first is a type-specification optimization, called unique name, that requires static global view of the whole program in order to substitute indirect virtual function call sites by direct calls. The other is a type-prediction kind of optimization, referred to as single type prediction, that uses type-profiling information to replace virtual function calls by conditional expressions which involve direct function calls.", "num_citations": "23\n", "authors": ["283"]}
{"title": "Learning deterministic weighted automata with queries and counterexamples\n", "abstract": " We present an algorithm for extraction of a probabilistic deterministic finite automaton (PDFA) from a given black-box language model, such as a recurrent neural network (RNN). The algorithm is a variant of the exact-learning algorithm L*, adapted to a probabilistic setting with noise. The key insight is the use of conditional probabilities for observations, and the introduction of a local tolerance when comparing them. When applied to RNNs, our algorithm often achieves better word error rate (WER) and normalised distributed cumulative gain (NDCG) than that achieved by spectral extraction of weighted finite automata (WFA) from the same networks. PDFAs are substantially more expressive than n-grams, and are guaranteed to be stochastic and deterministic - unlike spectrally extracted WFAs.", "num_citations": "22\n", "authors": ["283"]}
{"title": "A formal hierarchy of RNN architectures\n", "abstract": " We develop a formal hierarchy of the expressive capacity of RNN architectures. The hierarchy is based on two formal properties: space complexity, which measures the RNN's memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine. We place several RNN variants within this hierarchy. For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016). We also show how these models' expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions. Our results build on the theory of \"saturated\" RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy. Experimental findings from training unsaturated networks on formal languages support this conjecture.", "num_citations": "20\n", "authors": ["283"]}
{"title": "Programming with \u201cbig code\u201d\n", "abstract": " The vast amount of code available on the web is increasing on a daily basis. Open-source hosting sites such as GitHub contain billions of lines of code. Community question-answering sites provide millions of code snippets with corresponding text and metadata. The amount of code available in executable binaries is even greater. Collectively, these increasing amounts of code have been referred to as \u201cBig Code\u201d. In this monograph, we cover some of the recent research trends on leveraging \u201cBig Code\u201d for performing various programming tasks that are difficult to accomplish with traditional techniques.", "num_citations": "20\n", "authors": ["283"]}
{"title": "Asynchronous assertions\n", "abstract": " Assertions are a familiar and widely used bug detection technique. Traditional assertion checking, however, is performed synchronously, imposing its full cost on the runtime of the program. As a result, many useful kinds of checks, such as data structure invariants and heap analyses, are impractical because they lead to extreme slowdowns. We present a solution that decouples assertion evaluation from program execution: assertions are checked asynchronously by separate checking threads while the program continues to execute. Our technique guarantees that asynchronous evaluation always produces the same result as synchronous evaluation, even if the program concurrently modifies the program state. The checking threads evaluate each assertion on a consistent snapshot of the program state as it existed at the moment the assertion started. We implemented our technique in a system called Strobe, which\u00a0\u2026", "num_citations": "20\n", "authors": ["283"]}
{"title": "Leveraging a corpus of natural language descriptions for program similarity\n", "abstract": " Program similarity is a central challenge in many programming-related applications, such as code search, clone detection, automatic translation, and programming education.", "num_citations": "19\n", "authors": ["283"]}
{"title": "Similarity of binaries\n", "abstract": " A computer implemented method of estimating a similarity of binary records comprising executable code, comprising converting a first binary record and a second binary record to a first intermediate representation (IR) and a second IR respectively, decomposing each of the first IR and the second IR to a plurality of strands which are partial dependent chains of program instructions, calculating a probability score for each of the plurality of strands of the first IR to have an equivalent counterpart in the second IR by comparing each strand of the first IR to one or more strands of the second IR, adjusting the probability score for each strand according to a significance value calculated for each strand and calculating a similarity score defining a functional similarity between the first IR and the second IR by aggregating the adjusted probability score of the plurality of strands.", "num_citations": "18\n", "authors": ["283"]}
{"title": "Towards neural decompilation\n", "abstract": " We address the problem of automatic decompilation, converting a program in low-level representation back to a higher-level human-readable programming language. The problem of decompilation is extremely important for security researchers. Finding vulnerabilities and understanding how malware operates is much easier when done over source code. The importance of decompilation has motivated the construction of hand-crafted rule-based decompilers. Such decompilers have been designed by experts to detect specific control-flow structures and idioms in low-level code and lift them to source level. The cost of supporting additional languages or new language features in these models is very high. We present a novel approach to decompilation based on neural machine translation. The main idea is to automatically learn a decompiler from a given compiler. Given a compiler from a source language S to a target language T , our approach automatically trains a decompiler that can translate (decompile) T back to S . We used our framework to decompile both LLVM IR and x86 assembly to C code with high success rates. Using our LLVM and x86 instantiations, we were able to successfully decompile over 97% and 88% of our benchmarks respectively.", "num_citations": "16\n", "authors": ["283"]}
{"title": "Statistical reconstruction of class hierarchies in binaries\n", "abstract": " We address a fundamental problem in reverse engineering of object-oriented code: the reconstruction of a program's class hierarchy from its stripped binary. Existing approaches rely heavily on structural information that is not always available, eg, calls to parent constructors. As a result, these approaches often leave gaps in the hierarchies they construct, or fail to construct them altogether. Our main insight is that behavioral information can be used to infer subclass/superclass relations, supplementing any missing structural information. Thus, we propose the first statistical approach for static reconstruction of class hierarchies based on behavioral similarity. We capture the behavior of each type using a statistical language model (SLM), define a metric for pairwise similarity between types based on the Kullback-Leibler divergence between their SLMs, and lift it to determine the most likely class hierarchy. We\u00a0\u2026", "num_citations": "15\n", "authors": ["283"]}
{"title": "Cross-supervised synthesis of web-crawlers\n", "abstract": " A web-crawler is a program that automatically and systematically tracks the links of a website and extracts information from its pages. Due to the different formats of websites, the crawling scheme for different sites can differ dramatically. Manually customizing a crawler for each specific site is time consuming and error-prone. Furthermore, because sites periodically change their format and presentation, crawling schemes have to be manually updated and adjusted. In this paper, we present a technique for automatic synthesis of web-crawlers from examples. The main idea is to use hand-crafted (possibly partial) crawlers for some websites as the basis for crawling other sites that contain the same kind of information. Technically, we use the data on one site to identify data on another site. We then use the identified data to learn the website structure and synthesize an appropriate extraction scheme. We iterate this\u00a0\u2026", "num_citations": "15\n", "authors": ["283"]}
{"title": "Neural reverse engineering of stripped binaries using augmented control flow graphs\n", "abstract": " We address the problem of reverse engineering of stripped executables, which contain no debug information. This is a challenging problem because of the low amount of syntactic information available in stripped executables, and the diverse assembly code patterns arising from compiler optimizations. We present a novel approach for predicting procedure names in stripped executables. Our approach combines static analysis with neural models. The main idea is to use static analysis to obtain augmented representations of call sites; encode the structure of these call sites using the control-flow graph (CFG) and finally, generate a target name while attending to these call sites. We use our representation to drive graph-based, LSTM-based and Transformer-based architectures. Our evaluation shows that our models produce predictions that are difficult and time consuming for humans, while improving on existing\u00a0\u2026", "num_citations": "13\n", "authors": ["283"]}
{"title": "Structural language models for any-code generation\n", "abstract": " We address the problem of Any-Code Generation (AnyGen)-generating code without any restriction on the vocabulary or structure. The state-of-the-art in this problem is the sequence-to-sequence (seq2seq) approach, which treats code as a sequence and does not leverage any structural information. We introduce a new approach to AnyGen that leverages the strict syntax of programming languages to model a code snippet as tree structural language modeling (SLM). SLM estimates the probability of the program's abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes. We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node. Unlike previous structural techniques that have severely restricted the kinds of expressions that can be generated, our approach can generate arbitrary expressions in any programming language. Our model significantly outperforms both seq2seq and a variety of existing structured approaches in generating Java and C# code. We make our code, datasets, and models available online.", "num_citations": "13\n", "authors": ["283"]}
{"title": "Synthesis of forgiving data extractors\n", "abstract": " We address the problem of synthesizing a robust data-extractor from a family of websites that contain the same kind of information. This problem is common when trying to aggregate information from many web sites, for example, when extracting information for a price-comparison site.", "num_citations": "13\n", "authors": ["283"]}
{"title": "Lossless separation of web pages into layout code and data\n", "abstract": " A modern web page is often served by running layout code on data, producing an HTML document that enhances the data with front/back matters and layout/style operations. In this paper, we consider the opposite task: separating a given web page into a data component and a layout program. This separation has various important applications: page encoding may be significantly more compact (reducing web traffic), data representation is normalized across web designs (facilitating wrapping, retrieval and extraction), and repetitions are diminished (expediting site updates and redesign).", "num_citations": "12\n", "authors": ["283"]}
{"title": "Pattern-based synthesis of synchronization for the C++ memory model\n", "abstract": " We address the problem of synthesizing efficient and correct synchronization for programs running under the C++ relaxed memory model. Given a finite-state program P and a safety property S such that P satisfies S under a sequentially consistent (SC) memory model, our approach automatically eliminates concurrency errors in P due to the relaxed memory model, by creating a new program P with additional synchronization. Our approach works by automatically exploring the space of programs that can be created from P by adding synchronization operations. To explore this (vast) space, our algorithm: (i) explores bounded error traces to detect memory access patterns that can occur under the C++ memory model but not under SC, and (ii) eliminates these error traces by adding appropriate synchronization operations. We implemented our approach using CDSCHECKER as an oracle for detecting error traces and\u00a0\u2026", "num_citations": "12\n", "authors": ["283"]}
{"title": "Neural reverse engineering of stripped binaries\n", "abstract": " We address the problem of reverse engineering of stripped executables which contain no debug information. This is a challenging problem because of the low amount of syntactic information available in stripped executables, and due to the diverse assembly code patterns arising from compiler optimizations. We present a novel approach for predicting procedure names in stripped executables. Our approach combines static analysis with encoder-decoder-based models. The main idea is to use static analysis to obtain enriched representations of API call sites; encode a set of sequences of these call sites by traversing the Control-Flow Graph; and finally, attend to the encoded sequences while decoding the target name. Our evaluation shows that our model performs predictions that are difficult and time consuming for humans, while improving on the state-of-the-art by 20%.", "num_citations": "11\n", "authors": ["283"]}
{"title": "Abstraction-guided synthesis of synchronization\n", "abstract": " We present a novel framework for automatic inference of efficient synchronization in concurrent programs, a task known to be difficult and error-prone when done manually. Our framework is based on abstract interpretation and can infer synchronization for infinite state programs. Given a program, a specification, and an abstraction, we infer synchronization that avoids all (abstract) interleavings that may violate the specification, but permits as many valid interleavings as possible. Combined with abstraction refinement, our framework can be viewed as a new approach for verification where both the program and the abstraction can be modified on-the-fly during the verification process. The ability to modify the program, and not only the abstraction, allows us to remove program interleavings not only when they are known to be invalid, but also when they cannot be verified using the given abstraction. We\u00a0\u2026", "num_citations": "10\n", "authors": ["283"]}
{"title": "Asynchronous assertions\n", "abstract": " A snapshot of an application executing on a processor is taken in response to detecting an assertion in a running application. The assertion is evaluated based on the snapshot asynchronously while allowing the application to continue executing. The results of the assertion evaluation are returned to the application.", "num_citations": "10\n", "authors": ["283"]}
{"title": "Symbolic automata for static specification mining\n", "abstract": " We present a formal framework for static specification mining. The main idea is to represent partial temporal specifications as symbolic automata \u2013 automata where transitions may be labeled by variables, and a variable can be substituted by a letter, a word, or a regular language. Using symbolic automata, we construct an abstract domain for static specification mining, capturing both the partialness of a specification and the precision of a specification. We show interesting relationships between lattice operations of this domain and common operators for manipulating partial temporal specifications, such as building a more informative specification by consolidating two partial specifications.", "num_citations": "9\n", "authors": ["283"]}
{"title": "Context-sensitive dynamic bloat detection system that uses a semantic profiler to collect usage statistics\n", "abstract": " Methods and apparatus are provided for a context-sensitive dynamic bloat detection system. A profiling tool is disclosed that selects an appropriate collection implementation for a given application. The disclosed profiling tool uses semantic profiling together with a set of collection selection rules to make an informed choice. A collection implementation, such as an abstract data entity, is selected for a given program by obtaining collection usage statistics from the program. The collection implementation is selected based on the collection usage statistics using a set of collection selection rules. The collection implementation is one of a plurality of interchangeable collection implementations having a substantially similar logical behavior for substantially all collection types. The collection usage statistics indicate how the collection implementation is used in the given program. One or more suggestions can be generated\u00a0\u2026", "num_citations": "8\n", "authors": ["283"]}
{"title": "A structural model for contextual code changes\n", "abstract": " We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program\u2019s Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task.  We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs\u00a0\u2026", "num_citations": "7\n", "authors": ["283"]}
{"title": "From programs to interpretable deep models and back\n", "abstract": " We demonstrate how deep learning over programs is used to provide (preliminary) augmented programmer intelligence. In the first part, we show how to tackle tasks like code completion, code summarization, and captioning. We describe a general path-based representation of source code that can be used across programming languages and learning tasks, and discuss how this representation enables different learning algorithms. In the second part, we describe techniques for extracting interpretable representations from deep models, shedding light on what has actually been learned in various tasks.", "num_citations": "7\n", "authors": ["283"]}
{"title": "Neural edit completion\n", "abstract": " We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program's Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task. We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs, Transformers\u00a0\u2026", "num_citations": "5\n", "authors": ["283"]}
{"title": "Code searching and ranking\n", "abstract": " A method, computerized apparatus and computer program product for providing a code segment in response to a query, the method comprising using at least one hardware processor for: receiving a multiplicity of code segments and meta data related to the code segments; analyzing each code segment of the multiplicity of code segments, said analyzing comprising: semantically analyzing the code segment to obtain a first rank, structurally analyzing the code segment to obtain a second rank, and analyzing the meta data associated with the code segment to obtain a third rank; combining the first rank, second rank and third rank into a total rank associated with the code segment; receiving a query; matching the query to each of the multiplicity of code segments to identify matching code segments; and providing the matching code segments in accordance with total ranks associated with each of the matching code\u00a0\u2026", "num_citations": "5\n", "authors": ["283"]}
{"title": "Hardware and Software: Verification and Testing\n", "abstract": " This volume contains the proceedings of the 10th Haifa Verification Conference (HVC 2014). The conference was hosted by IBM Research-Haifa and took place during November 18\u201320, 2014. It was the tenth event in this series of annual conferences dedicated to advancing the state of the art and state of the practice in verification and testing. The conference provided a forum for researchers and practitioners from academia and industry to share their work, exchange ideas, and discuss the future directions of testing and verification for hardware, software, and complex hybrid systems. Overall, HVC 2014 attracted 43 submissions in response to the call for papers. Each submission was assigned to at least three members of the Program Committee and in many cases additional reviews were solicited from external experts. The Program Committee selected 21 papers for presentation. In addition to the 21 contributed\u00a0\u2026", "num_citations": "5\n", "authors": ["283"]}
{"title": "Searching and ranking of code in videos\n", "abstract": " A method comprising: receiving a multiplicity of videos from a source; for each video: receiving meta data related to the video; extracting from the video a video frame containing computer code; identifying a region of interest (ROI) within the video frame; performing OCR of the ROI to extract a code segment; analyzing the code segment by: semantically analyzing the code segment to obtain a first rank, structurally analyzing the code segment to obtain a second rank, and analyzing the meta data to obtain a third rank; and combining the first rank, second rank and third rank into a total rank associated with the code segment; receiving a query; matching the query to each code segment to identify matching code segments and associated videos; and providing the associated videos in accordance with total ranks associated with the matching code segments.", "num_citations": "4\n", "authors": ["283"]}
{"title": ": Data-Driven Disjunctive Abstraction\n", "abstract": " We address the problem of computing an abstraction for a set of examples, which is precise enough to separate them from a set of counterexamples. The challenge is to find an over-approximation of the positive examples that does not represent any negative example. Conjunctive abstractions (e.g., convex numerical domains) and limited disjunctive abstractions, are often insufficient, as even the best such abstraction might include negative examples. One way to improve precision is to consider a general disjunctive abstraction.                 We present , a new algorithm for learning general disjunctive abstractions. Our algorithm is inspired by widely used machine-learning algorithms for obtaining a classifier from positive and negative examples. In contrast to these algorithms which cannot generalize from disjunctions,  obtains a disjunctive abstraction that minimizes the number of disjunctions. The result\u00a0\u2026", "num_citations": "4\n", "authors": ["283"]}
{"title": "Code similarity via natural language descriptions\n", "abstract": " Semantic Relatedness of Code Fragments Page 1 Code Similarity via Natural Language Descriptions Meital Ben Sinai & Eran Yahav Technion \u2013 Israel Institute of Technology Off the Beaten Track, Jan 2015 www.like2drops.com 1/30 Page 2 >7M users >17M repositories Google code, programming blogs, documentation sites\u2026 3M registered users >8M questions >14M answers Lots of snippets out there Dec \u201814 2/30 OBT'15 - Code Similarity via Natural Language Descriptions - Meital Ben Sinai & Eran Yahav Page 3 \u25b6 The code is not organized \u25b6 Cannot accomplish even simple tasks (which are increasingly improving in other domains) Similarity: Images VS. Programs 3/30 OBT'15 - Code Similarity via Natural Language Descriptions - Meital Ben Sinai & Eran Yahav Page 4 \u25b6 Images already have some solutions \u25b6 Find somewhere on the web The Grand Canal, Venice, Italy Similarity: Images VS. Programs 3/30 '- /\u2026", "num_citations": "4\n", "authors": ["283"]}
{"title": "Finding rare numerical stability errors in concurrent computations\n", "abstract": " A numerical algorithm is called stable if an error, in all possible executions of the algorithm, does not exceed a predefined bound. Introduction of concurrency to numerical algorithms results in a significant increase in the number of possible computations of the same result, due to different possible interleavings of concurrent threads. This can lead to instability of previously stable algorithms, since rounding can result in a larger error than expected for some interleavings. Such errors can be very rare, since the particular combination of rounding can occur in only a small fraction of interleavings. In this paper, we apply the cross-entropy method--a generic approach to rare event simulation and combinatorial optimization--to detect rare numerical instability in concurrent programs. The cross-entropy method iteratively samples a small number of executions and adjusts the probability distribution of possible scheduling\u00a0\u2026", "num_citations": "4\n", "authors": ["283"]}
{"title": "Preserving correctness under relaxed memory models\n", "abstract": " This thesis addresses the problem of automatic verification and fence inference in concurrent programs running under relaxed memory models. Modern architectures implement relaxed memory models in which memory operations may be reordered and executed non-atomically. Instructions called memory fences are provided to the programmer, allowing control of this behavior. To ensure correctness of many algorithms, the programmer is often required to explicitly insert memory fences into her program. However, she must use as few fences as possible, or the benefits of the relaxed architecture may be lost. It is our goal to help automate the fence insertion process.We present a framework for automatic inference of memory fences in concurrent programs, relieving the programmer from this complex task. The framework consists of two parts:* An algorithm that given a finite-state program, a safety specification and a description of the memory model computes a set of ordering constraints that guarantee the correctness of the program under the memory model. The computed constraints are maximally permissive: removing any constraint from the solution would permit an execution violating the specification. These constraints are then realized as additional fences in the input program.* A family of novel partial-coherence abstractions, specialized for relaxed memory models. These abstractions allow us to extend the applicability of the algorithm to programs that are infinite-state under the relaxed memory model, even when they were finite-state under the\" standard\" sequentially consistent model.", "num_citations": "4\n", "authors": ["283"]}
{"title": "Shallow finite state verification\n", "abstract": " We consider the problem of verifying finite state properties of shallow programs; ie, programs where pointers from program variables to heap-allocated objects are allowed, but where heap-allocated objects may not themselves contain pointers. We prove a number of results relating the complexity of such verification problems to the nature of the finite state machine used to specify the property. Some properties are shown to be intractable, but others which appear to be quite similar admit polynomial-time verification algorithms. While there has been much progress on many aspects of automated program verification, we are not aware of any previous work relating the difficulty of finite state verification to properties of the finite state automaton. Our results serve to provide insight into the inherent complexity of important classes of verification problems. In addition, the program abstractions used for the polynomial-time verification algorithms may be of independent interest.", "num_citations": "4\n", "authors": ["283"]}
{"title": "How Attentive are Graph Attention Networks?\n", "abstract": " Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GATs can only compute a restricted kind of attention where the ranking of attended nodes is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how_attentive_are_gats .", "num_citations": "3\n", "authors": ["283"]}
{"title": "Thinking Like Transformers\n", "abstract": " What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.", "num_citations": "2\n", "authors": ["283"]}
{"title": "Programming with \u201cBig Code\u201d\n", "abstract": " The vast amount of code available on the web is increasing on a daily basis. Open-source hosting sites such as GitHub contain billions of lines of code. Community question-answering sites provide millions of code snippets with corresponding text and metadata. The amount of code available in executable binaries is even greater. In this talk, I will cover recent research trends on leveraging such \u201cbig code\u201d for program analysis, program synthesis and reverse engineering. We will consider a range of semantic representations based on symbolic automata\u00a0[11, 15], tracelets\u00a0[3], numerical abstractions\u00a0[13, 14], and textual descriptions\u00a0[1, 22], as well as different notions of code similarity based on these representations.                 To leverage these semantic representations, we will consider a number of prediction techniques, including statistical language models\u00a0[19, 20], variable order Markov models\u00a0[2], and\u00a0\u2026", "num_citations": "2\n", "authors": ["283"]}
{"title": "Automatic verification and synthesis for weak memory models\n", "abstract": " Techniques are provided for automatic verification and inference of memory fences in concurrent programs that can bound the store buffers that are used to model relaxed memory models. A method is provided for determining whether a program employing a relaxed memory model satisfies a safety specification. An abstract memory model is obtained of the relaxed memory model. The abstract memory model represents concrete program states of the program as a finite number of abstract states. The safety specification is evaluated for the program on the abstract memory model having the finite number of abstract states. Fence positions at one or more locations can be determined to ensure that the safety specification is satisfied.", "num_citations": "2\n", "authors": ["283"]}
{"title": "Static Analysis: 18th International Symposium, SAS 2011. Venice, Italy, September 14-16, 2011. Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 18th International Symposium on Static Analysis, SAS 2011, held in Venice, Italy, in September 2011. The 22 revised full papers were selected from 67 submissions. Also included in this volume are the abstracts of the invited talks that were given at the symposium by renowned experts in the field. The papers address all aspects of static analysis, including abstract domains, abstract interpretation, abstract testing, data flow analysis, bug detection, program transformation, program verification, security analysis and type checking.", "num_citations": "2\n", "authors": ["283"]}
{"title": "Property-Guided Verification of Concurrent Heap-Manipulating Programs\n", "abstract": " We address the problem of verifying properties of concurrent and sequential programs written in languages, such as Java, that make extensive use of the heap to allocate\u2014and deallocate\u2014new objects and threads. We present a framework for the verification of sequential and concurrent Java programs.", "num_citations": "2\n", "authors": ["283"]}
{"title": "Programming with a read-eval-synth loop\n", "abstract": " A frequent programming pattern for small tasks, especially expressions, is to repeatedly evaluate the program on an input as its editing progresses. The Read-Eval-Print Loop (REPL) interaction model has been a successful model for this programming pattern. We present the new notion of Read-Eval-Synth Loop (RESL) that extends REPL by providing in-place synthesis on parts of the expression marked by the user. RESL eases programming by synthesizing parts of a required solution. The underlying synthesizer relies on a partial solution from the programmer and a few examples.  RESL hinges on bottom-up synthesis with general predicates and sketching, generalizing programming by example. To make RESL practical, we present a formal framework that extends observational equivalence to non-example specifications.  We evaluate RESL by conducting a controlled within-subjects user-study on 19\u00a0\u2026", "num_citations": "1\n", "authors": ["283"]}
{"title": "Programming by predicates: a formal model for interactive synthesis\n", "abstract": " Program synthesis is the problem of computing from a specification a program that implements it. New and popular variations on the synthesis problem accept specifications in formats that are easier for the human synthesis user to provide: input\u2013output example pairs, type information, and partial logical specifications. These are all partial specification formats, encoding only a fraction of the expected behavior of the program, leaving many matching programs. This transition into partial specification also changes the mode of work for the user, who now provides additional specifications until they are happy with the synthesis result. Therefore, synthesis becomes an iterative, interactive process. We present a formal model for interactive synthesis, parameterized by an abstract domain of predicates on programs. The abstract domain is used to describe both the iterative refinement of the specifications and reduction of the\u00a0\u2026", "num_citations": "1\n", "authors": ["283"]}
{"title": "Generating Tests by Example.\n", "abstract": " Property-based testing is a technique combining parametric tests with value generators, to create an efficient and maintainable way to test general specifications. To test the program, property-based testing randomly generates a large number of inputs defined by the generator to check whether the test-assertions hold. We present a novel framework that synthesizes property-based tests from existing unit tests. Projects often have a suite of unit tests that have been collected over time, some of them checking specific and subtle cases. Our approach leverages existing unit tests to learn property-based tests that can be used to increase value coverage by orders of magnitude. Further, we show that our approach:(i) preserves the subtleties of the original test suite; and (ii) produces properties that cover a greater range of inputs than those in the example set.", "num_citations": "1\n", "authors": ["283"]}
{"title": "Differential program analysis\n", "abstract": " The evolution of software is an emerging research topic, receiving much attention and focus, as every line of code written today is more likely to belong to an existing piece of software. Identifying and modeling the semantic impact of these patches in a sound and precise way is a paramount goal yet to be fulfilled. This work is an effort to generate sound, precise and scalable analysis methods for modeling the differences and measuring the similarity between programs.This work first focuses on computing semantic differences in numerical programs where the values of variables have no a-priori bounds, and uses abstract interpretation to compute an over-approximation of program differences. Computing differences and establishing equivalence under abstraction requires abstracting relationships between variables in the two programs. Towards that end, a correlating program is first constructed, in which these relationships can be tracked, and then a correlating abstract domain is used to compute a sound approximation of these relationships. The construction of the correlating program relies on syntactic similarity. To better establish equivalence between correlated variables and precisely capture differences, the abstract domain has to represent non-convex information using a partially-disjunctive abstract domain. To balance precision and cost of this representation, the domain over-approximates numerical information while preserving equivalence between correlated variables by dynamically partitioning the disjunctive state according to equivalence criteria.", "num_citations": "1\n", "authors": ["283"]}
{"title": "Analysis and Synthesis with\" Big Code\".\n", "abstract": " The vast amount of code available on the web is increasing on a daily basis. Open-source hosting sites such as GitHub contain billions of lines of code. Community question-answering sites provide millions of code snippets with corresponding text and metadata. The amount of code available in executable binaries is even greater. In this lecture series, I will cover recent research trends on leveraging such \u201cbig code\u201d for program analysis, program synthesis and reverse engineering. We will consider a range of semantic representations based on symbolic automata [55, 63], tracelets [28], numerical abstractions [61, 58], and textual descriptions [82, 1], as well as different notions of code similarity based on these representations. To leverage these semantic representations, we will consider a number of prediction techniques, including statistical language models [66, 73], variable order Markov models [18], and other distance-based and model-based sequence classification techniques. Finally, we discuss applications of these techniques including semantic code search in both source code [55] and stripped binaries [28], code completion and reverse engineering [43].", "num_citations": "1\n", "authors": ["283"]}