{"title": "Supermon: A high-speed cluster monitoring system\n", "abstract": " Supermon is a flexible set of tools for high speed, scalable cluster monitoring. Node behavior can be monitored much faster than with other commonly used methods (e.g., rstatd). In addition, Supermon uses a data protocol based on symbolic expressions (S-expressions) at all levels of Supermon, from individual nodes to entire clusters. This contributes to Supermon's scalability and allows it to function in a heterogeneous environment. This paper presents the Supermon architecture and discuss initial performance measurements on a cluster of heterogeneous Alpha-processor based nodes.", "num_citations": "221\n", "authors": ["1725"]}
{"title": "An image-free opto-mechanical system for creating virtual environments and imaging neuronal activity in freely moving Caenorhabditis elegans\n", "abstract": " Non-invasive recording in untethered animals is arguably the ultimate step in the analysis of neuronal function, but such recordings remain elusive. To address this problem, we devised a system that tracks neuron-sized fluorescent targets in real time. The system can be used to create virtual environments by optogenetic activation of sensory neurons, or to image activity in identified neurons at high magnification. By recording activity in neurons of freely moving C. elegans, we tested the long-standing hypothesis that forward and reverse locomotion are generated by distinct neuronal circuits. Surprisingly, we found motor neurons that are active during both types of locomotion, suggesting a new model of locomotion control in C. elegans. These results emphasize the importance of recording neuronal activity in freely moving animals and significantly expand the potential of imaging techniques by providing a mean to stabilize fluorescent targets.", "num_citations": "139\n", "authors": ["1725"]}
{"title": "The CCA component model for high\u2010performance scientific computing\n", "abstract": " The Common Component Architecture (CCA) is a component model for high\u2010performance computing, developed by a grass\u2010roots effort of computational scientists. Although the CCA is usable with CORBA\u2010like distributed\u2010object components, its main purpose is to set forth a component model for high\u2010performance, parallel computing. Traditional component models are not well suited for performance and massive parallelism. We outline the design pattern for the CCA component model, discuss our strategy for language interoperability, describe the development tools we provide, and walk through an illustrative example using these tools. Performance and scalability, which are distinguishing features of CCA components, affect choices throughout design and implementation. Copyright \u00a9 2005 John Wiley & Sons, Ltd.", "num_citations": "108\n", "authors": ["1725"]}
{"title": "The ghost in the machine: Observing the effects of kernel operation on parallel application performance\n", "abstract": " The performance of a parallel application on a scalable HPC system is determined by user-level execution of the application code and system-level (OS kernel) operations. To understand the influences of system-level factors on application performance, the measurement of OS kernel activities is key. We describe a technology to observe kernel actions and make this information available to application-level performance measurement tools. The benefits of merged application and OS performance information and its use in parallel performance analysis are demonstrated, both for profiling and tracing methodologies. In particular, we focus on the problem of kernel noise assessment as a stress test of the approach. We show new results for characterizing noise and introduce new techniques for evaluating noise interference and its effects on application execution. Our kernel measurement and noise analysis\u00a0\u2026", "num_citations": "75\n", "authors": ["1725"]}
{"title": "Microfluidic devices for analysis of spatial orientation behaviors in semi-restrained Caenorhabditis elegans\n", "abstract": " This article describes the fabrication and use of microfluidic devices for investigating spatial orientation behaviors in nematode worms (Caenorhabditis elegans). Until now, spatial orientation has been studied in freely moving nematodes in which the frequency and nature of encounters with the gradient are uncontrolled experimental variables. In the new devices, the nematode is held in place by a restraint that aligns the longitudinal axis of the body with the border between two laminar fluid streams, leaving the animal's head and tail free to move. The content of the fluid streams can be manipulated to deliver step gradients in space or time. We demonstrate the utility of the device by identifying previously uncharacterized aspects of the behavioral mechanisms underlying chemotaxis, osmotic avoidance, and thermotaxis in this organism. The new devices are readily adaptable to behavioral and imaging studies involving fluid borne stimuli in a wide range of sensory modalities.", "num_citations": "67\n", "authors": ["1725"]}
{"title": "Analysis of microbenchmarks for performance tuning of clusters\n", "abstract": " Microbenchmarks, i.e. very small computational kernels, have become commonly used for quantitative measures of node performance in clusters. For example, a commonly used benchmark measures the amount of time required to perform a fixed quantum of work. Unfortunately, this benchmark is one of many that violate well known rules from sampling theory, leading to erroneous, contradictory or misleading results. At a minimum, these types of benchmarks can not be used to identify time-based activities that may interfere with and hence limit application performance. Our original and primary goal remains to identify noise in the system due to periodic activities that are not part of user application code. We discuss why the 'fixed quantum of work' benchmark provides data that is of limited use for analysis; and we show code for, discuss, and analyze results from a microbenchmark which follows good rules of\u00a0\u2026", "num_citations": "57\n", "authors": ["1725"]}
{"title": "Introduction to concurrency in programming languages\n", "abstract": " Illustrating the effect of concurrency on programs written in familiar languages, this text focuses on novel language abstractions that truly bring concurrency into the language and aid analysis and compilation tools in generating efficient, correct programs. It also explains the complexity involved in taking advantage of concurrency with regard to program correctness and performance. The book describes the historical development of current programming languages and the common threads that exist among them. It also contains several chapters on design patterns for parallel programming and includes quick reference guides to OpenMP, Erlang, and Cilk. Ancillary materials are available on the book's website.", "num_citations": "41\n", "authors": ["1725"]}
{"title": "Performance analysis of parallel programs via message-passing graph traversal\n", "abstract": " The ability to understand the factors contributing to parallel program performance are vital for understanding the impact of machine parameters on the performance of specific applications. We propose a methodology for analyzing the performance characteristics of parallel programs based, on message-passing traces of their execution on a set of processors. Using this methodology, we explore how perturbations in both single processor performance and the messaging layer impact the performance of the traced run. This analysis provides a quantitative description of the sensitivity of applications to a variety of performance parameters to better understand the range of systems upon which an application can be expected to perform well. These performance parameters include operating system, interference and variability in message latencies within the interconnection network layer.", "num_citations": "36\n", "authors": ["1725"]}
{"title": "Optimizing the tracking efficiency for cosmic ray muon tomography\n", "abstract": " We have built a detector capable of locating high Z objects in the sampling (middle) region of the detector. As atomic number increases, radiation length rapidly decreases, yielding larger variance in scattering angle. Cosmic ray muon tomography works by tracking muons above the sampling region, and tracking them below the region as well. The difference between the two trajectories yield information, via the muon scattering variance, of the materials contained within the sampling region [Borozdin, K, et al., 2003]. One of most important aspects of cosmic ray tomography is minimizing exposure time. The cosmic ray flux is about 1 cm -2  min -1 , and the goal is to use them for detecting high-density materials as quickly as possible. This involves using all of the information possible to reconstruct tracks with redundant detectors. Detector scattering residuals yield a low precision measurement of muon energy. Knowing\u00a0\u2026", "num_citations": "31\n", "authors": ["1725"]}
{"title": "Bridging the language gap in scientific computing: the Chasm approach\n", "abstract": " Chasm is a toolkit providing seamless language interoperability between Fortran 95 and C++. Language interoperability is important to scientific programmers because scientific applications are predominantly written in Fortran, while software tools are mostly written in C++. Two design features differentiate Chasm from other related tools. First, we avoid the common\u2010denominator type systems and programming models found in most Interface Definition Language (IDL)\u2010based interoperability systems. Chasm uses the intermediate representation generated by a compiler front\u2010end for each supported language as its source of interface information instead of an IDL. Second, bridging code is generated for each pairwise language binding, removing the need for a common intermediate data representation and multiple levels of indirection between the caller and callee. These features make Chasm a simple system that\u00a0\u2026", "num_citations": "31\n", "authors": ["1725"]}
{"title": "Performance technology for parallel and distributed component software\n", "abstract": " This work targets the emerging use of software component technology for high\u2010performance scientific parallel and distributed computing. While component software engineering will benefit the construction of complex science applications, its use presents several challenges to performance measurement, analysis, and optimization. The performance of a component application depends on the interaction (possibly nonlinear) of the composed component set. Furthermore, a component is a \u2018binary unit of composition\u2019 and the only information users have is the interface the component provides to the outside world. A performance engineering methodology and development approach is presented to address evaluation and optimization issues in high\u2010performance component environments. We describe a prototype implementation of a performance measurement infrastructure for the Common Component Architecture\u00a0\u2026", "num_citations": "29\n", "authors": ["1725"]}
{"title": "Right-weight kernels: an off-the-shelf alternative to custom light-weight kernels\n", "abstract": " DOE has put forth a considerable effort into light-weight kernels for high performance computing, yet there has been a lack of acceptance and use, perhaps due to the limited support for hardware and software. The arguments for light-weight kernels have been based on the problem of interference, i.e. changes in application performance that occur when the operating system pre-empts the application. Nevertheless, using existing, well supported operating systems for HPC systems has been quite successful. The problems with the standard operating systems remain, however, although their impact on applications is still not quantified. At LANL, we have undertaken a research program to determine whether Linux and/or Plan 9 can be used to realize the benefits of light-weight kernels while maintaining the benefits of a full-featured operating system. Specifically, we are evaluating measures that quantify what is \"good\"\u00a0\u2026", "num_citations": "28\n", "authors": ["1725"]}
{"title": "TAUoverSupermon: Low-Overhead Online Parallel Performance Monitoring\n", "abstract": " Online application performance monitoring allows tracking performance characteristics during execution as opposed to doing so post-mortem. This opens up several possibilities otherwise unavailable such as real-time visualization and application performance steering that can be useful in the context of long-running applications. As HPC systems grow in size and complexity, the key challenge is to keep the online performance monitor scalable and low overhead while still providing a useful performance reporting capability. Two fundamental components that constitute such a performance monitor are the measurement and transport systems. We adapt and combine two existing, mature systems - TAU and Supermon - to address this problem. TAU performs the measurement while Supermon is used to collect the distributed measurement state. Our experiments show that this novel approach leads to very\u00a0\u2026", "num_citations": "25\n", "authors": ["1725"]}
{"title": "Cosmic-ray muon tomography and its application to the detection of high-Z materials\n", "abstract": " Each minute, about 10000 muons rain down on every square meter of Earth. These charged elementary particles are produced by cosmic rays striking the upper atmosphere. Millions of highly penetrative muons pass through our bodies, cars and houses daily. Penetrating the objects, muons interact with atoms of different materials, mainly electromagnetically. They are more strongly deflected, or scattered, by high-Z materials, including nuclear materials, like uranium and plutonium, and gamma-ray shielding materials, like lead, tungsten or gold. This allowed us to develop a technique which uses multiple scattering of cosmic-ray muons to detect shielded packages of nuclear materials in a background of normal cargo. The advantages of this technique are that it is passive, does not deliver any radiation dose above background, selective to high-z dense materials, and is suitable for large amount of shielding. Physical basis of the technique, current status of our research and its possible perspectives will be discussed.", "num_citations": "25\n", "authors": ["1725"]}
{"title": "Semi-automatic extraction of software skeletons for benchmarking large-scale parallel applications\n", "abstract": " The design of high-performance computing architectures requires performance analysis of large-scale parallel applications to derive various parameters concerning hardware design and software development. The process of performance analysis and benchmarking an application can be done in several ways with varying degrees of fidelity. One of the most cost-effective ways is to do a coarse-grained study of large-scale parallel applications through the use of program skeletons. The concept of a\" program skeleton\" that we discuss in this paper is an abstracted program that is derived from a larger program where source code that is determined to be irrelevant is removed for the purposes of the skeleton. In this work, we develop a semi-automatic approach for extracting program skeletons based on compiler program analysis. We demonstrate correctness of our skeleton extraction process by comparing details from\u00a0\u2026", "num_citations": "24\n", "authors": ["1725"]}
{"title": "A performance interface for component-based applications\n", "abstract": " This work targets the emerging use of software component technology for high-performance scientific parallel and distributed computing. While component software engineering will benefit the construction of complex science applications, its use presents several challenges to performance optimization. A component application is composed of a set of components, thus, application performance depends on the interaction (possibly non-linear) of the component set. Furthermore, a component is a \"binary unit of composition\" and the only information users have is the interface the component provides to the outside world. An interface for component performance measurement and query is presented to address optimization issues. We describe the performance component design and an example demonstrating its use for runtime performance tuning.", "num_citations": "22\n", "authors": ["1725"]}
{"title": "Performance measurement of applications with GPU acceleration using CUDA\n", "abstract": " Multi-core accelerators offer significant potential to improve the performance of parallel applications. However, tools to help the parallel application developer understand accelerator performance and its impact are scarce. An approach is presented to measure the performance of GPU computations programmed using CUDA and integrate this information with application performance data captured with the TAU Performance System. Test examples are shown to validate the measurement methods. Results for a case study of the GPU-accelerated NAMD molecular dynamics application application are given.", "num_citations": "21\n", "authors": ["1725"]}
{"title": "A prototype notebook-based environment for computational tools\n", "abstract": " The Virtual Notebook Environment (ViNE) is a platform-independent, web-based interface designed to support a range of scientific activities across distributed, heterogeneous computing platforms. ViNE provides scientists with a web-based version of the common paper-based lab notebook, but in addition, it provides support for collaboration and management of computational experiments. Collaboration is supported with the web-based approach, which makes notebook material generally accessible and with a hierarchy of security mechanisms that screen that access. ViNE provides uniform, system-transparent access to data, tools, and programs throughout the scientist's computing infrastructure. Computational experiments can be launched from ViNE using a visual specification language. The scientist is freed from concerns about inter-tool connectivity, data distribution, or data management details. ViNE also provides support for dynamically linking analysis results back into the notebook content.", "num_citations": "19\n", "authors": ["1725"]}
{"title": "High performance computing with clouds\n", "abstract": " Infrastructure services (Infrastructure-as-a-service), provided by cloud vendors allows any user to provision a large number of compute instances fairly easily by utilizing the virtual resources to perform data/compute intensive works. More often the unique opportunities for high performance computing with the clouds benefit the users in economical way. In this paper, the current trend of using cloud computing for High Performance Commuting is illustrated by discussing why and how clouds can be good candidate for HPC. Some HPC applications (mostly commercial) that has been deployed with clouds are also summarized and some benchmarking done for the performance analysis for HPC with clouds are discussed as well. Again, the very recently released HPC-as-Service by Penguin Computing, an optimized cloud service specifically for HPC is discussed and compared with common general purpose cloud\u00a0\u2026", "num_citations": "16\n", "authors": ["1725"]}
{"title": "Wool: A workflow programming language\n", "abstract": " Workflows offer scientists a simple but flexible programming model at a level of abstraction closer to the domain-specific activities that they seek to perform. However, languages for describing workflows tend to be highly complex, or specialized towards a particular domain, or both. WOOL is an abstract workflow language with human-readable syntax, intuitive semantics, and a powerful abstract type system. WOOL workflows can be targeted to almost any kind of runtime system supporting data-flow computation. This paper describes the design of the WOOL language and the implementation of its compiler, along with a simple example runtime. We demonstrate its use in an image-processing workflow.", "num_citations": "14\n", "authors": ["1725"]}
{"title": "Static analysis techniques for semiautomatic synthesis of message passing software skeletons\n", "abstract": " The design of high-performance computing architectures requires performance analysis of large-scale parallel applications to derive various parameters concerning hardware design and software development. The process of performance analysis and benchmarking an application can be done in several ways with varying degrees of fidelity. One of the most cost-effective ways is to do a coarse-grained study of large-scale parallel applications through the use of program skeletons. The concept of a \u201cprogram skeleton\u201d that we discuss in this article is an abstracted program that is derived from a larger program where source code that is determined to be irrelevant is removed for the purposes of the skeleton. In this work, we develop a semiautomatic approach for extracting program skeletons based on compiler program analysis. We demonstrate correctness of our skeleton extraction process by comparing details from\u00a0\u2026", "num_citations": "13\n", "authors": ["1725"]}
{"title": "ForOpenCL: transformations exploiting array syntax in Fortran for accelerator programming\n", "abstract": " Emerging GPU architectures for high performance computing are well suited to a data-parallel programming model. This paper presents preliminary work examining a programming methodology that provides Fortran programmers with access to these emerging systems. We use array constructs in Fortran to show how this infrequently exploited, standardised language feature is easily transformed to lower-level accelerator code. The transformations in ForOpenCL are based on a simple mapping from Fortran to OpenCL. We demonstrate, using a stencil code solving the shallow-water fluid equations, that the performance of the ForOpenCL compiler-generated transformations is comparable with that of hand-optimised OpenCL code.", "num_citations": "12\n", "authors": ["1725"]}
{"title": "Rapid prototyping frameworks for developing scientific applications: A case study\n", "abstract": " In this paper, we describe a Python-based framework for the rapid prototyping of scientific applications. A case study was performed using a problem specification developed for Marmot, a project at the Los Alamos National Laboratory aimed at re-factoring standard physics codes into reusable and extensible components. Components were written in Python, ZPL, Fortran, and C++ following the Marmot component design. We evaluate our solution both qualitatively and quantitatively by comparing it to a single-language version written in C.", "num_citations": "12\n", "authors": ["1725"]}
{"title": "An energy-based interaction model for population opinion dynamics with topic coupling\n", "abstract": " We introduce a new, and quite general variational model for opinion dynamics based on pairwise interaction potentials and a range of opinion evolution protocols ranging from random interactions to global synchronous flows in the opinion state space. The model supports the concept of topic \u201ccoupling\u201d, allowing opinions held by individuals to be changed via indirect interaction with others on different subjects. Interaction topology is governed by a graph that determines interactions. Our model, which is really a family of variational models, has, as special cases, many of the previously established models for the opinion dynamics. After introducing the model, we study the dynamics of the special case in which the potential is either a tent function or a constructed bell-like curve. We find that even in these relatively simple potential function examples there emerges interesting behavior. We also present results of\u00a0\u2026", "num_citations": "11\n", "authors": ["1725"]}
{"title": "Pink: A 1024-node single-system image linux cluster\n", "abstract": " This work describes our experience of designing and building Pink, a 1024-node (2048 processor) Myrinet-based single-system image Linux cluster that was installed in January 2003 at the Los Alamos National Laboratory. At the time of its installation, Pink was the largest single-system image Linux cluster in the world, and was based entirely on open-source software - from the BIOS up. Pink was the proof-of-concept prototype for Lightning, a production 1408-node (2816 processor) cluster that begin operation at LANL. Lightning is currently number 6 on the Top500 list. In This work we examine the issues that were encountered and the problems that needed to be overcome in order to scale a cluster to this size. We also present some performance numbers that demonstrate the scalability and manageability of the cluster software suite.", "num_citations": "9\n", "authors": ["1725"]}
{"title": "Information extraction from muon radiography data\n", "abstract": " Scattering muon radiography was proposed recently as a technique of detection and 3-d imaging for dense high-Z objects. High-energy cosmic ray muons are deflected in matter in the process of multiple Coulomb scattering. By measuring the deflection angles we are able to reconstruct the configuration of high-Z material in the object. We discuss the methods for information extraction from muon radiography data. Tomographic methods widely used in medical images have been applied to a specific muon radiography information source. Alternative simple technique based on the counting of high-scattered muons in the voxels seems to be efficient in many simulated scenes. SVM-based classifiers and clustering algorithms may allow detection of compact high-Z object without full image reconstruction. The efficiency of muon radiography can be increased using additional informational sources, such as momentum estimation, stopping power measurement, and detection of muonic atom emission.", "num_citations": "9\n", "authors": ["1725"]}
{"title": "Computational experiments using distributed tools in a web-based electronic notebook environment\n", "abstract": " Scientific computational environments should provide high-level support for the integrated and systematic use of tools familiar in the laboratory setting, including lab notebooks, instruments, experiments, and analysis tools. Further, they should shield the user from the complexities of the underlying computational platform. We report here on the Virtual Notebook Environment, ViNE, which provides the web-based equivalent of the standard (paper) lab notebook, adding features for sharing, security, and collaboration. More significantly, ViNE allows scientists to represent, manage, and execute computational experiments that involve the sequenced use of data, computational tools, and programs distributed across the World Wide Web. ViNE hides system-level complexities, freeing the scientist from concerns about inter-tool connectivity, data distribution, data management, and machine idiosyncrasies. It thus extends the\u00a0\u2026", "num_citations": "9\n", "authors": ["1725"]}
{"title": "An efficient class of direct search surrogate methods for solving expensive optimization problems with CPU-time-related functions\n", "abstract": " In this paper, we characterize a new class of computationally expensive optimization problems and introduce an approach for solving them. In this class of problems, objective function values may be directly related to the computational time required to obtain them, so that, as the optimal solution is approached, the computational time required to evaluate the objective is significantly less than at points farther away from the solution. This is motivated by an application in which each objective function evaluation requires both a numerical fluid dynamics simulation and an image registration process, and the goal is to find the parameter values of a predetermined reference image by comparing the flow dynamics from the numerical simulation and the reference image through the image comparison process. In designing an approach to numerically solve the more general class of problems in an efficient way, we\u00a0\u2026", "num_citations": "8\n", "authors": ["1725"]}
{"title": "Unsupervised segmentation for inflammation detection in histopathology images\n", "abstract": " Acute inflammation of the placenta is associated with an increased rate of perinatal morbidity and mortality. This paper presents a novel method for analyzing digitized images of hematoxylin and eosin (H&E) stained histology slides to detect and quantify inflammatory polymorphonuclear leukocytes (PMLs). The first step is isolating the tissue by removing the background and red blood cells via color thresholding. Next, an iterative threshold technique is used to differentiate tissue from nuclei. Nuclei are further segmented to distinguish between nuclei which match morphological characteristics of PMLs and those which do not, such as connective tissue fibroblasts as well as chorion and amnion epithelium. Quantification of the remaining nuclei, many of which are likely neutrophils, are associated with amniotic fluid proteomic markers of infection and inflammation, as well as histological grading of neutrophils in\u00a0\u2026", "num_citations": "8\n", "authors": ["1725"]}
{"title": "Co-array Python: A parallel extension to the Python language\n", "abstract": " A parallel extension to the Python language is introduced that is modeled after the Co-Array Fortran extensions to Fortran 95. A new Python module, CoArray, has been developed to provide co-array syntax that allows a Python programmer to address co-array data on a remote processor. An example of Jacobi iteration using the CoArray module is shown and corresponding performance results are presented.", "num_citations": "8\n", "authors": ["1725"]}
{"title": "Co-array collectives: Refined semantics for co-array fortran\n", "abstract": " Co-array notation provides a compact syntax for programming parallel programs. Co-array Fortran (CAF) introduced and implements this notation, and CAF is currently proposed as an extension to the Fortran language standard. We believe that co-array notation requires a revised semantic definition beyond that specified by CAF for both pragmatic reasons within Fortran and to make the notation attractive for incorporation into other programming languages. The revised semantics make the language model easier to understand and reduces the potential for programmer error. Furthermore, these revised semantics allow CAF to be extended to capture collective operations in co-array notation.", "num_citations": "7\n", "authors": ["1725"]}
{"title": "Cafe: Coarray Fortran extensions for heterogeneous computing\n", "abstract": " Emerging hybrid accelerator architectures are often proposed for inclusion as components in an exascale machine, not only for performance reasons but also to reduce total power consumption. Unfortunately, programmers of these architectures face a daunting and steep learning curve that frequently requires learning a new language (e.g., OpenCL) or adopting a new programming model. Furthermore, the distributed (and frequently multi-level) nature of the memory organization of clusters of these machines provides an additional level of complexity. This paper presents preliminary work examining how Fortran coarray syntax can be extended to provide simpler access to accelerator architectures. This programming model integrates the Partitioned Global Address Space (PGAS) features of Fortran with some of the more task-oriented constructs in OpenMP 4.0 and OpenACC. It also includes the potential for\u00a0\u2026", "num_citations": "6\n", "authors": ["1725"]}
{"title": "Workflow representation and runtime based on lazy functional streams\n", "abstract": " Workflows are a successful model for building both distributed and tightly-coupled programs based on a dataflow-oriented coordination of computations. Multiple programming languages have been proposed to represent workflow-based programs in the past. In this paper, we discuss a representation of workflows based on lazy functional streams implemented in the strongly typed language Haskell. Our intent is to demonstrate that streams are an expressive intermediate representation for higher-level workflow languages. By embedding our stream-based workflow representation in a language such as Haskell, we also gain with minimal effort the strong type system provided by the base language, the rich library of built-in functional primitives, and most recently, rich support for managing concurrency at the language level.", "num_citations": "6\n", "authors": ["1725"]}
{"title": "Life with ed: A case study of a linuxBIOS/BProc cluster\n", "abstract": " In this paper, we describe experiences with our 127-no de/161-processor Alpha cluster testbed, Ed. Ed is unique for two distinct reasons. First, we have replaced the standard BIOS on the cluster nodes with the Lin-uxBIOS which loads Linux directly from non-volatile memory (Flash RAM). Second, the operating system provides a single-system image of the entire cluster, much like a traditional supercomputer. We will discuss the advantages of such a cluster, including time to boot (101 seconds for 100 nodes), upgrade (same as time to boot), and start processes (2.4 seconds for 15,000 pro-cesses). Additionally, we have discovered that certain predictions about the nature of terascale clusters, such as the need for hierarchical structure, are false. Finally, we argue that to achieve true scalability, terascale clusters must be built in the way of Ed.", "num_citations": "6\n", "authors": ["1725"]}
{"title": "OnRamp: enabling a new component-based development paradigm\n", "abstract": " Often the adoption of component-based scientific software requires the developer to abandon comfortable practices and embrace an unfamiliar software methodology. OnRamp provides a mechanism for the developer to generate CCA components, through commented markup in their original software, and keep their familiar software development practices. The developer uses these annotations to identify which methods in their code belong to which Port interfaces, and which Ports belong to which CCA components. Taken by itself, the markup is sufficient to create a skeleton of components representing the exported functionality of the original code, but because the entire code is available to OnRamp the implementation can also be generated. The functionality of the original code is wrapped in annotation-specified components, exporting part or all of its original functionality. OnRamp provides a model for\u00a0\u2026", "num_citations": "5\n", "authors": ["1725"]}
{"title": "High-Performance Component Software Systems\n", "abstract": " The increasing complexity of software systems for computational science and engineering has led to the adoption of a now-common methodology from business and industrial computing: software components. A component in this context is an independently deploy-able unit of software, which can be composed together with other components to form a complete program for solving some class of problems [63]. Components must have well-defined interfaces to allow this composition, but the implementation within the component is not specified. This situation implies programming language independence and, as long as the interfaces do not change, the ability to substitute updated or superior implementations. Other desirable features include scalability of the number of interacting components, scalable parallelism both within and between components, and minimality. In this context, minimality means that the\u00a0\u2026", "num_citations": "5\n", "authors": ["1725"]}
{"title": "Identifying change patterns in software history\n", "abstract": " Traditional algorithms for detecting differences in source code focus on differences between lines. As such, little can be learned about abstract changes that occur over time within a project. Structural differencing on the program's abstract syntax tree reveals changes at the syntactic level within code, which allows us to further process the differences to understand their meaning. We propose that grouping of changes by some metric of similarity, followed by pattern extraction via antiunification will allow us to identify patterns of change within a software project from the sequence of changes contained within a Version Control System (VCS). Tree similarity metrics such as a tree edit distance can be used to group changes in order to identify groupings that may represent a single class of change (e.g., adding a parameter to a function call). By applying antiunification within each group we are able to generalize from families of concrete changes to patterns of structural change. Studying patterns of change at the structural level, instead of line-by-line, allows us to gain insight into the evolution of software.", "num_citations": "4\n", "authors": ["1725"]}
{"title": "An open domain-extensible environment for simulation-based scientific investigation (odessi)\n", "abstract": " In scientific domains where discovery is driven by simulation modeling there are found common methodologies and procedures applied for scientific investigation. ODESSI (Open Domain-extensible Environment for Simulation-based Scientific Investigation) is an environment to facilitate the representation and automatic conduction of scientific studies by capturing common methods for experimentation, analysis, and evaluation used in simulation science. Specific methods ODESSI will support include parameter studies, optimization, uncertainty quantification, and sensitivity analysis. By making these methods accessible in a programmable framework, ODESSI can be used to capture and run domain-specific investigations. ODESSI is demonstrated for a problem in the neuroscience domain involving computational modeling of human head electromagnetics for conductivity analysis and source localization.", "num_citations": "4\n", "authors": ["1725"]}
{"title": "A measurement and simulation methodology for parallel computing performance studies\n", "abstract": " Disciplined application of system measurement and performance simulation is a powerful method for understanding the behavior of parallel programs and computers. The current state-of-the-art in parallel program performance analysis is focused on interconnection network and processor performance. The presence of operating system interference, although recognized as a source of performance degradation, has not been formally considered in the analysis process.", "num_citations": "4\n", "authors": ["1725"]}
{"title": "Computational experiments using distributed tools in a web-based electronic notebook environment\n", "abstract": " Computational environments used by scientists should provide high-level support for scientific processes that involve the integrated and systematic use of familiar abstractions from a laboratory setting, including notebooks, instruments, experiments, and analysis tools. However, doing so while hiding the complexities of the underlying computational platform is a challenge. ViNE is a web-based electronic notebook that implements a high-level interface for applying computational tools in scientific experiments in a location- and platform-independent manner. Using ViNE, a scientist can specify data and tools and construct experiments that apply them in well-defined procedures. ViNE's implementation of the experiment abstraction offers the scientist easy-to-understand framework for building scientific processes. This paper discusses how ViNE implements computational experiments in distributed\u00a0\u2026", "num_citations": "4\n", "authors": ["1725"]}
{"title": "Loss of community identity in opinion dynamics models as a function of inter-group interaction strength\n", "abstract": " Recent technological changes have increased connectivity between individuals around the world leading to higher frequency interactions between members of communities that would be otherwise distant and disconnected. This paper examines a model of opinion dynamics in interacting communities and studies how increasing interaction frequency affects the ability for communities to retain distinct identities versus falling into consensus or polarized states in which community identity is lost. We also study the effect (if any) of opinion noise related to a tendency for individuals to assert their individuality in homogenous populations. Our work builds on a model we developed previously [11] where the dynamics of opinion change is based on individual interactions that seek to minimize some energy potential based on the differences between opinions across the population.", "num_citations": "3\n", "authors": ["1725"]}
{"title": "Image denoising by regularization on characteristic graphs\n", "abstract": " This paper introduces improvements to a now classical family of image denoising methods through rather minimal changes to the way derivatives are computed. In particular, we ask, and answer, the question \u201cHow much can we improve the common denoising methods by local, completely non-parametric modifications to image graphs?\u201d We present the concept of non-parametric characteristic graph representations of images and detail two such graph constructions. Their use in image denoising is demonstrated within a regularization framework. The results are compared with those of more traditional approaches of Tikhonov, total variation and L1TV regularization. We show that in some denoising scenarios our methods perform more favorably in preserving intensity levels and geometric details of object boundaries. They are particularly useful for denoising images with both smooth and discontinuous intensity variations, preserving detail to the pixel level.", "num_citations": "3\n", "authors": ["1725"]}
{"title": "Performance tool integration in a gpu programming environment: Experiences with tau and hmpp\n", "abstract": " Application development environments offering high-level programming support for accelerators will need to integrate instrumentation and measurement capabilities to enable full, consistent performance views for analysis and tuning. We describe early experiences with the integration of a parallel performance system (TAU) and accelerator performance tool (TAUcuda) with the HMPP Workbench for programming GPU accelerators using CUDA. A description of the design approach is given, and two case studies are reported to demonstrate our development prototype. A new version of the technology is now being created based on the lessons learned from the research work.", "num_citations": "2\n", "authors": ["1725"]}
{"title": "Reliability of automated neutrophil quantitation in digitized H&E stained slides: Pilot analysis of correlation with amniotic fluid proteomics score\n", "abstract": " Background: The diagnosis of intraamniotic infection is considered clinically relevant to maternal, fetal, neonatal and childhood morbidity and mortality. However, intraobserver variability, even among expert pathologists, remains problematic, apart from 0.96 agreement on \u201cpresent v absent\u201d(Pediatr Dev Pathol. 2003; 6 (5): 435-48), with \u201cconsensus\u201d(rather than valid biomarkers) being the gold standard. An automated and reliable method for quantitation of neutrophils would be a useful diagnostic tool.Methods: We sampled 10 cases with amniotic fluid proteomics (AFMR) scores, 2 each of scores 0-4 as per Buhimschi et al, BJOG. 2009 Jan; 116 (2): 257-67. Two images were taken at random of extraplacental membranes; each image was analyzed at both 10x and 20x magnification. The image analytic technique begins with scanned slides in the form of color (RGB) images. We first segment the image to separate\u00a0\u2026", "num_citations": "2\n", "authors": ["1725"]}
{"title": "804: A novel method of assessing cervical collagen integrity utilizing image segmentation analysis\n", "abstract": " ObjectiveCervical shortening is a early predictor of spontaneous preterm birth, and its pathogenesis and pathophysiology are poorly understood. Collagen histologic staining reflects collagen integrity. We describe a novel method of analyzing cervical collagen integrity using image segmentation analysis and correlate it with urogenital culture status.Study DesignAsymptomatic women with a transvaginal cervical length 25 mm between 16-24 weeks underwent cervical M. hominis and U. urealyticum cultures and a micro-cervical biopsy using The Bard\u00ae Monopty\u00ae Instrument (CR Bard, Inc., Murray Hill, NJ). Cervical biopsies were stained with hematoxylin and eosin (H&E) and examined by a single reviewer blinded to clinical data. One 40X photomicrograph was taken of each biopsy. The R channel (collagen stains red-pink with H&E) histogram was extracted and its mean, standard deviation, skew and kurtosis\u00a0\u2026", "num_citations": "2\n", "authors": ["1725"]}
{"title": "A Gentle Migration Path to Component-Based Programming\n", "abstract": " Publisher SummaryThe chapter presents a simplified component programming model that provides a gentle migration path to component-based application development. Most existing component models (CCA INtegration framework [CCAIN]) require a relatively large leap from current software practices. The simple CCAIN proposed in the chapter allows scientific software developers to take an evolutionary and staged approach for component usage rather than a revolutionary plan that requires extensive software rewriting. The proposed model still requires a movement toward increased software modularization, but this, to a large extent, is a good idea and is happening anyway. Paradoxically, software components are widely recognized to provide real benefits to the scientific computing community, yet are generally ignored in practice. The chapter addresses this discrepancy by proposing a simplified CCAIN that\u00a0\u2026", "num_citations": "2\n", "authors": ["1725"]}
{"title": "Variable Cognition in ABM Decision-Making: An application to livestock vaccine choice.\n", "abstract": " Modeling realistic human decision-making is an important feature of good policy design processes. The use of an agent-based modelling framework allows for quantitative human decision-models that assume fully rational agents. This research introduces a dynamic human decision-making sub-model. The parameterisation of human memory and \u2018rationality\u2019 in a decision-making model represents an important extension of decision-making in ABMs. A data driven model of herd movement within a dynamic natural environment is the context for evaluating the cognitive decision-making model. The natural and human environments are linked via memory and rationality that affect herdsmen decision-making to vaccinate cattle using a once-for-life vaccine (Rift Valley fever) and an annual booster vaccine (Contagious Bovine Pleuropneumonia). The simulation model uses environmental data from Samburu county, Kenya from 2004 to 2015. The cognitive parameters of memory and \u2018rationality\u2019 are shown to successfully differentiate between vaccination decisions that are characterised by annual and once-for-life choices. The preliminary specifications and findings from the dynamic cognition - pastoralist agent-based model (PastoralScape) indicate that the model offers much to livestock vaccination modelling among small-scale herders.", "num_citations": "1\n", "authors": ["1725"]}
{"title": "Array types for a graph processing language\n", "abstract": " Graphs are frequently represented and manipulated in the form of arrays encoding their adjacency matrices. Programs that operate on these arrays, for example, using linear algebra operations to encode graph algorithm primitives, can benefit from optimization techniques such as operation and loop fusion, data layout tuning, and other classical compilation methods. To support the construction of sound transformations and optimizations we present HAL (Hierarchical Array Language), a typed language for array processing. HAL provides first class index spaces, programming via type-directed transformations, and an expressive type-system that allows for encoding data structure properties. We show laws that can be derived for HAL to establish a basis for many semantics preserving transformations. This work also briefly introduces methods for expressing computations over these types via higher order functions.", "num_citations": "1\n", "authors": ["1725"]}
{"title": "Blob indentation identification via curvature measurement\n", "abstract": " This paper presents a novel method for identifying indentations on the boundary of solid 2D shape. It uses the signed curvature at a set of points along the boundary to identify indentations and provides one parameter for tuning the selection mechanism for discriminating indentations from other boundary irregularities. An efficient implementation is described based on the Fourier transform for calculating curvature from a sequence of points obtained from the boundary of a binary blob.", "num_citations": "1\n", "authors": ["1725"]}
{"title": "Deriving program transformations by demonstration\n", "abstract": " Automatic code transformation in which transformations are tuned for specific applications and contexts are difficult to achieve in an accessible manner. In this paper, we present an approach to build application specific code transformations. Our approach is based on analysis of the abstract syntax representation of exemplars of the essential change to the code before and after the transformation is applied. This analysis entails a sequence of steps to identify the change, determine how to generalize it, and map it to term rewriting rules for the Stratego term rewriting system. The methods described in this paper assume programs are represented in a language-neutral term format, allowing tools based on our methods to be applied to programs written in the major languages used by computational scientists utilizing high performance computing systems.", "num_citations": "1\n", "authors": ["1725"]}
{"title": "Twig: A configurable domain-specific language\n", "abstract": " Programmers design, write, and understand programs with a high-level structure in mind. Existing programming languages are not very good at capturing this structure because they must include low-level implementation details. To address this problem we introduce Twig, a programming language that allows for domain-specific logic to be encoded alongside low-level functionality. Twig's language is based on a simple, formal calculus that is amenable to both human and machine reasoning. Users may introduce rules that rewrite expressions, allowing for user-defined optimizations. Twig can also incorporate procedures written in a variety of low-level languages. Our implementation supports C and Python, but our abstract model can accommodate other languages as well. We present Twig's design and formal semantics and discuss our implementation. We demonstrate Twig's use in two different domains, multi\u00a0\u2026", "num_citations": "1\n", "authors": ["1725"]}
{"title": "141: Collagen integrity of the uterine cervix reflects amniotic fluid cytokine profile\n", "abstract": " ObjectiveTo determine if there is a correlation between the histological staining characteristics of cervical collagen and amniotic fluid (AF) cytokines in asymptomatic women with an ultrasonographic short cervix in the midtrimester.Study DesignAsymptomatic women with a transvaginal cervical length 25mm between 16-24 weeks underwent a micro-cervical biopsy and amniocentesis. AF cytokine concentrations were assayed using the Bio-Plex multianalyte detection and quantitation system (Bio-Rad, Hercules, CA.). Cervical biopsy specimens were stained with hematoxylin and eosin (H&E) and examined by a single reviewer blinded to clinical and assay data. One 40X photomicrograph was taken of each biopsy. The R channel (collagen stains red-pink with H&E stain) histogram was extracted and its mean, standard deviation, skew and kurtosis calculated. A single collagen staining factor score (CFS) was extracted\u00a0\u2026", "num_citations": "1\n", "authors": ["1725"]}
{"title": "The design of a general method for constructing coupled scientific simulations\n", "abstract": " The field of computing is driven at some level by the desire to automate complex tasks. Scientific computing-where computers routinely generate solutions to large mathematical models that would be impossible to complete by hand-is a rich source of complex tasks. The immense size of scientific applications has forced the development of increasingly complex high performance computing platforms. Unfortunately, as these computing systems grow, the task of programming and controlling them becomes nearly as complex as the scientific problems themselves. Programming tools are desperately needed to automate common tasks. In recent years, high performance systems have grown to a point where it is feasible for them to execute more than a single scientific model at a time. Most physical systems are composed of smaller sub-systems which interact to produce effects in the overall system. For example, the ocean is not an isolated system: subtle changes in the atmosphere or sea-ice will change the behavior of the ocean, and similarly the ocean will affect the atmosphere and sea-ice. Scientists working in many different disciplines have already created models of many of isolated sub-systems. They now want to model the more complex interactions between such sub-systems. The most obvious way to do this is not to construct a single, monolithic model of the larger system, but to combine the smaller sub-system models in a manner that mimics the real-world interactions between them. If the scientist has implemented the sub-system", "num_citations": "1\n", "authors": ["1725"]}
{"title": "Automated component creation for legacy C++ and fortran codes.\n", "abstract": " A significant amount of work has been spent creating component models and programming environments, but little support exists for automation in the process of creating components from existing codes. To entice users to adopt the component-based paradigm over traditional programming models, integration of legacy codes must be as simple and fast as possible, We present a system for automating the IDL generation stage of component development based on source code analysis of legacy C, Ct-4 and Fortran codes using the Program Database Toolkit. Together with IDL compilation tools such as Babel, we provide an alternative to hand-written IDL code for legacy applications and libraries. In addition to generating IDL, we propose an XML-based method for specifying meta-data related to type mapping and wrapper generation that can be shared between our tools and IDL compilers. The component model of choice for this work is the Common Component Architecture (CCA) using the Scientific Interface Definition Language (SIDL), though the concepts presented can be applied to other models.", "num_citations": "1\n", "authors": ["1725"]}
{"title": "INTERLACE: An interoperation and linking architecture for computational engines\n", "abstract": " To aid in building high-performance computational environments, INTERLACE offers a framework for linking reusable computational engines in a heterogeneous distributed system. The INTERLACE model provides clients with access to computational servers which interface with \u201cwrapped\u201d computational engines. An API is defined for establishing the server interconnection requirements including data organization and movement, and command invocation. This provides an abstraction layer above the server middleware upon which servers and clients may be constructed. The INTERLACE framework has been demonstrated by building a distributed computational environment with MatLab engines.", "num_citations": "1\n", "authors": ["1725"]}