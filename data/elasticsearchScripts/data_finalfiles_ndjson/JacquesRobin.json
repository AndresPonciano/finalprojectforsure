{"title": "An overview of SURGE: A reusable comprehensive syntactic realization component\n", "abstract": " !#\"% $'& () &10) & 2 () 3# $5 4'() &10) &1687) &19A@ B6C 7) $8@ B0D 4E2 4E&1 (F 0) 7G2\u00a9() 7) &H9C IQPR7F S% $8@ UTB& 7) $'&V 2\u00a9 P'7) $ RIQ () 0W S &1 () &X 9RI@ B6RY7) $'&#@ B (X ab $ dcfeD 2 75 g I\u00a9 TBP'hCi8@ p2r q% 6s@ BtQ&1 (vu 0v@ B7vwQx\u00c4 y\u00c5&HS\u00c9 \u00c7DI\u00d1 () \u00d6EeD \u00dcr&5 2 () &\u2022 iEIQ7) $ \u00e1@ B6'9R&HiR7) &197) I\u00e2 \u00e0A2\u00a9 7) $8 TB&1&16\u00ea\u00eb3\u00ed\u00e0\u00c5u &1I S% 6\u00e1 \u00ec\u00eeIQ (% $ R&1 (% YQPs@ B9d2\u00a9 6'31&V 2 6R90) P'4R4dI\u00d1 () 7\u2022 9'P'(v@ B6RY\u00ef 7) $ RIQ0) &A w\u00d1& 2 () 01e", "num_citations": "181\n", "authors": ["1533"]}
{"title": "Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design, implementation and evaluation\n", "abstract": " Automatically summarizing vast amounts of on-line quantitative data with a short natural language paragraph has a wide range of real-world applications. However, this specific task raises a number of difficult issues that are quite distinct from the generic task of language generation: conciseness, complex sentences, floating concepts, historical background, paraphrasing power and implicit content.", "num_citations": "157\n", "authors": ["1533"]}
{"title": "Generating concise natural language summaries\n", "abstract": " Summaries typically convey maximal information in minimal space. In this paper, we describe an approach to summary generation that opportunistically folds information from multiple facts into a single sentence using concise linguistic constructions. Unlike previous work in generation, how information gets added into a summary depends in part on constraints from how the text is worded so far. This approach allows the construction of concise summaries, containing complex sentences that pack in information. The resulting summary sentences are, in fact, longer than sentences generated by previous systems. We describe two applications we have developed using this approach, one of which produces summaries of basketball games (STREAK) while the other (PLANDOC) produces summaries of telephone network planning activity; both systems summarize input data as opposed to full text. The applications\u00a0\u2026", "num_citations": "121\n", "authors": ["1533"]}
{"title": "Controlling content realization with functional unification grammars\n", "abstract": " Standard Functional Unification Grammars (FUGs) provide a structurally guided top-down control regime for sentence generation. When using FUGs to perform content realization as a whole, including lexical choice, this regime is no longer appropriate for two reasons: (1) the unification of non-lexicalized semantic input with an integrated lexico-grammar requires mapping \u201cfloating\u201d semantic elements which can trigger extensive backtracking and (2) lexical choice requires accessing external constraint sources on demand to preserve the modularity between conceptual and linguistic knowledge.             We introduce two control tools that we have implemented for FUGs to address these limitations: bk-class, a form of dependency-directed backtracking to efficiently process \u201cfloating\u201d constraints and external, a co-routine mechanism allowing a FUG to cooperate with external constraint sources during unification\u00a0\u2026", "num_citations": "90\n", "authors": ["1533"]}
{"title": "Empirically designing and evaluating a new revision-based model for summary generation\n", "abstract": " We present a system for summarizing quantitative data in natural language, focusing on the use of a corpus of basketball game summaries, drawn from on-line news services, to empirically shape the system design and to evaluate our approach. Our initial corpus analysis revealed characteristics of textual summaries that challenge the capabilities of current language generation systems. In order to meet these challenges, we developed a revision-based model for summary generation and implemented it in our prototype system streak. A second, detailed corpus analysis was used to identify and encode the revision rules of the system. Finally, we carried out a quantitative evaluation, using several test corpora, to measure the robustness of the new revision-based model. Our results show that our new model improves both coverage and extensibility of the traditional language generation model.", "num_citations": "88\n", "authors": ["1533"]}
{"title": "Generating cross-references for multimedia explanation\n", "abstract": " When explanations include multiple media, such as text and illustrations, a reference to an object can be made through a combination of media. We call part of a presentation that references material elsewhere a cross-reference. We are concerned here with how textual expressions can refer to parts of accompanying illustrations. The illustration to which a cross-reference refers should also satisfy the specific goal of identifying an object for the user. Thus, producing an effective cross-reference not only involves text generation, but may also entail modifying or replacing an existing illustration and in some cases, generating an illustration where previously none was needed. In this paper, we describe the different types of cross-references that COMET (Coordinated Multimedia Explanation Testbed) generates and show the roles that both its text and graphics generators play in this process.", "num_citations": "62\n", "authors": ["1533"]}
{"title": "Evaluating the portability of revision rules for incremental summary generation\n", "abstract": " This paper presents a quantitative evaluation of the portability to the stock market domain of the revision rule hierarchy used by the system STREAK to incrementally generate newswire sports summaries. The evaluation consists of searching a test corpus of stock market reports for sentence pairs whose (semantic and syntactic) structures respectively match the triggering condition and application result of each revision rule. The results show that at least 59% of all rule classes are fully portable, with at least another 7% partially portable.", "num_citations": "60\n", "authors": ["1533"]}
{"title": "SURGE: a comprehensive plug-in syntactic realization component for text generation\n", "abstract": " Departamento de Inform atica Universidade Federal de Pernambuco Caixa Postal 7851, Cidade Universit aria Recife, PE 50740-540 Brazil Fax:+ 55-81 271-8438) surge is a syntactic realization front-end for natural language generation systems. It is aimed to be used as a reusable tool encapsulating a rich knowledge of English syntax. We explain how the wide coverage necessary to reach the goal of reusability has been reached through (1) a pragmatic approach of integrating several linguistic theories as required, and (2) a level of abstraction for the input description that does not overcommit the other components of the generator to speci c ontological perspectives. The paper describes how a generator can interact with surge by preparing input speci cations. The input description language is presented and motivated and di erent methods to construct the speci cations are contrasted. Finally, the coverage of surge is discussed and techniques to extend it are presented, including a short description of the development tools provided to the grammar writer. The development and maintenance of the surge system over the past decade have demonstrated that (1) syntactic realization becomes di cult to manage as the grammar grows,(2) wide coverage requires attention to categories and constructs often ignored in traditional grammatical theories (eg, dates, addresses, person names), and (3) integration into many di erent systems has strongly motivated a relatively low level of abstraction as an interface between domain-speci c system components and a reusable syntactic realization component.", "num_citations": "59\n", "authors": ["1533"]}
{"title": "Tailoring lexical choice to the user's vocabulary in multimedia explanation generation\n", "abstract": " In this paper, we discuss the different strategies used in COMET (COordinated Multimedia Explanation Testbed) for selecting words with which the user is familiar. When pictures cannot be used to disambiguate a word or phrase, COMET has four strategies for avoiding unknown words. We give examples for each of these strategies and show how they are implemented in COMET.", "num_citations": "43\n", "authors": ["1533"]}
{"title": "A revision-based generation architecture for reporting facts in their historical context\n", "abstract": " Natural language reports generated by existing systems ignore the historical context of the facts and events they relate. In this chapter, I argue that going beyond this limitation requires abandoning the pipelined architecture of existing report generators. I propose a new architecture in which a first draft of the report is organized around new information and then incrementally revised to opportunistically add related historical information. This type of information-adding revision allows to elaborate inside a clause or a nominal while taking into account surface structure constraints from any other portion of the report. In addition to providing the additional flexibility required to convey historical information, the proposed architecture constitutes an interesting testbed to investigate a wide range of open questions concerning content planning below the sentence level, generation with revision and generation architecture.", "num_citations": "33\n", "authors": ["1533"]}
{"title": "Detecting complex changes during metamodel evolution\n", "abstract": " Evolution of metamodels can be represented at the finest grain by the trace of atomic changes: add, delete, and update elements. For many applications, like automatic correction of models when the metamodel evolves, a higher grained trace must be inferred, composed of complex changes, each one aggregating several atomic changes. Complex change detection is a challenging task since multiple sequences of atomic changes may define a single user intention and complex changes may overlap over the atomic change trace. In this paper, we propose a detection engine of complex changes that simultaneously addresses these two challenges of variability and overlap. We introduce three ranking heuristics to help users to decide which overlapping complex changes are likely to be correct. We describe an evaluation of our approach that allow reaching full recall. The precision is improved by our\u00a0\u2026", "num_citations": "29\n", "authors": ["1533"]}
{"title": "Detecting complex changes and refactorings during (Meta) model evolution\n", "abstract": " Evolution of metamodels can be represented at the finest grain by the trace of atomic changes such as add, delete, and update of elements. For many applications, like automatic correction of models when the metamodel evolves, a higher grained trace must be inferred, composed of complex changes, each one aggregating several atomic changes. Complex change detection is a challenging task since multiple sequences of atomic changes may define a single user intention and complex changes may overlap over the atomic change trace.In this paper, we propose a detection engine of complex changes that simultaneously addresses these two challenges of variability and overlap. We introduce three ranking heuristics to help users to decide which overlapping complex changes are likely to be correct. In our approach, we record the trace of atomic changes rather than computing them with the difference between\u00a0\u2026", "num_citations": "27\n", "authors": ["1533"]}
{"title": "Generating summaries of workflow diagrams\n", "abstract": " FLOWDOC is a prototype text generator that summarizes information from work flow graphs in a business re-engineering context. A richer ontology than is typically used allows generalization of input data during content selection, and combination of data during sentence planning.", "num_citations": "25\n", "authors": ["1533"]}
{"title": "Metamodel and constraints co-evolution: A semi automatic maintenance of ocl constraints\n", "abstract": " Metamodels are core components of modeling languages to define structural aspects of a business domain. As a complement, OCL constraints are used to specify detailed aspects of the business domain, e.g. more than 750 constraints come with the UML metamodel. As the metamodel evolves, its OCL constraints may need to be co-evolved too. Our systematic analysis shows that semantically different resolutions can be applied depending not only on the metamodel changes, but also on the user intent and on the structure of the impacted constraints. In this paper, we investigate the reasons that lead to apply different resolutions. We then propose a co-evolution approach that offers alternative resolutions while allowing the user to choose the best applicable one. We evaluated our approach on the evolution of the UML case study. The results confirm the need of alternative resolutions along with user\u00a0\u2026", "num_citations": "20\n", "authors": ["1533"]}
{"title": "Corpus analysis for revision-based generation of complex sentences\n", "abstract": " The complex sentences of newswire reports contain oating content units that appear to be opportunistically placed where the form of the surrounding text allows. We present a corpus analysis that identi ed precise semantic and syntactic constraints on where and how such information is realized. The result is a set of revision tools that form the rule base for a report generation system, allowing incremental generation of complex sentences.", "num_citations": "20\n", "authors": ["1533"]}
{"title": "Looking at both the present and the past to efficiently update replicas of web content\n", "abstract": " Since Web sites are autonomous and independently updated, applications that keep replicas of Web data, such as Web warehouses and search engines, must periodically poll the sites and check for changes. Since this is a resource-intensive task, in order to keep the copies up-to-date, it is important to devise efficient update schedules that adapt to the change rate of the pages and avoid visiting pages not modified since the last visit. In this paper, we propose a new approach that learns to predict the change behavior of Web pages based both on the static features and change history of pages, and refreshes the copies accordingly. Experiments using real-world data show that our technique leads to substantial performance improvements compared to previously proposed approaches.", "num_citations": "18\n", "authors": ["1533"]}
{"title": "Lexical choice in natural language generation\n", "abstract": " In this paper we survey the issue of lexical choice in natural language generation. We first define lexical choice as the choice of open-class lexical items (whether phrasal patterns or individual words) appropriate to express the content units of the utterance being generated in a given situation of enunciation. We then distinguish between five major classes of constraints on lexical choice: grammatical, inter-lexical, encyclopedic, interpersonal and discursive. For each class we review how they have been represented and used in existing generators. We conclude by pointing out the common limitations of these generators with respect to lexical choice as well as some directions for future research in the field.", "num_citations": "17\n", "authors": ["1533"]}
{"title": "Constraint Programming Architectures: Review and a New Proposal.\n", "abstract": " Most automated reasoning tasks with practical applications can be automatically reformulated into a constraint solving task. A constraint programming platform can thus act as a unique, underlying engine to be reused for multiple automated reasoning tasks in intelligent agents and systems. We identify six key requirements for such platform: expressive task modeling language, rapid solving method customization and combination, adaptive solving method, user-friendly solution explanation, efficient execution, and seamless integration within larger systems and practical applications. We then propose a novel, model-driven, component and rule-based architecture for such a platform that better satisfies as a whole this set of requirements than those of currently available platforms.", "num_citations": "15\n", "authors": ["1533"]}
{"title": "A prototype implementation of an orthographic software modeling environment\n", "abstract": " Orthographic Software Modeling (OSM) is a view-centric software engineering approach that aims to leverage the orthographic projection metaphor used in the visualization of physical objects to visualize software systems. Although the general concept of OSM does not prescribe specific sets of views, a concrete OSM environment has to be specific about the particular views to be used in a particular project. At the University of Mannheim we are developing a prototype OSM environment, nAOMi, that supports the views defined by the KobrA 2.0 method, a version of KobrA adapted for OSM. In this paper we provide an overview of the KobrA 2.0 metamodel underpinning nAOMi and give a small example of its use to model a software system.", "num_citations": "14\n", "authors": ["1533"]}
{"title": "User-needs analysis and design methodology for an automated document generator\n", "abstract": " Telephone network planning engineers routinely study feeder routes within the telephone network in order to create and refine network capacity expansion (relief) plans. In doing so they make use of a powerful software tool called LEIS                                    TM                  \u2014 PLAN                 1 We are developing an extension to PLAN called PLANDoc that will automatically generate natural language narratives documenting the engineers\u2019 use of PLAN. In this paper, we present the user-needs analysis and design methodology we have used in developing the PLANDoc system. We describe our interviews with various end users to determine if such a system would be desirable and what design factors would make it useful. We show how we model the system on a set of iteratively-revised human-generated narratives. The model narratives determine the function and architecture of the documentation system, and\u00a0\u2026", "num_citations": "14\n", "authors": ["1533"]}
{"title": "Adaptive CHR meets CHR\u2228: An extended refined operational semantics for CHR\u2228 based on justifications\n", "abstract": " Adaptive constraint processing with Constraint Handling Rules (CHR) allows the application of intelligent search strategies to solve Constraint Satisfaction Problems (CSP), but these search algorithms have to be implemented in the host language of adaptive CHR, which is currently Java. On the other hand, CHR\u2228 enables to explicitly formulate search in CHR, using disjunctive bodies to model choices. However, a naive implementation for handling disjunctions, in particular chronological backtracking (as implemented in Prolog) might cause \u201cthrashing\u201d due to an inappropriate order of decisions. To avoid this, a first combination of adaptive CHR and CHR\u2228 is presented to offer a more efficient embedded search mechanism to handle disjunctions. Therefore the refined operational semantics of CHR is extended for disjunctions and adaptation. 1", "num_citations": "13\n", "authors": ["1533"]}
{"title": "ORCAS: Towards a CHR-Based Model-Driven Framework of Reusable Reasoning Components.\n", "abstract": " We present the long term vision and current stage of the ORCAS project which goal is to develop an easily extensible framework of reusable fine-grained automated reasoning components. It innovates in software engineering by putting forward the first integration of model-driven, component-based, aspect-oriented and formal development techniques. It innovates in automated reasoning by proposing to build the most varied AI applications and inference engines (eg, for deduction, abduction, inheritance, belief revision, constraint solving, optimization and induction) by assembling components, each one encapsulating a CHRv knowledge base and reusing a CHRv inference engine as a universally shared server component.", "num_citations": "13\n", "authors": ["1533"]}
{"title": "Can ontologies improve web search engine effectiveness before the advent of the semantic web?\n", "abstract": " The advent of the semantic web will not be immediate, due to a shortage of knowledge engineering manpower to annotate a critical mass of web documents with concepts from ontologies. In this paper, we argue that widecoverage linguistic ontologies have an alternative usage to improve web search effectiveness while the web remains predominantly textual. They can be used by search engines to perform simple semantic processing, such as automatically expanding the query keywords with alternate wordings of the same or immediately neighboring concepts. We present a conclusive empirical experiment that shows that sizable relative effectiveness gains can be achieved by various expansion strategies for both unbounded and bounded retrieval result sets. This experiment also shows that the effectiveness of such ontology-boosted search on a textual web with no semantic annotations remains far away from the very high precision and recall promise of the semantic web.", "num_citations": "12\n", "authors": ["1533"]}
{"title": "Automatic generation and revision of natural language report summaries providing historical background\n", "abstract": " Summarization applications raise several challenging issues for language generation systems. To address them, I propose a new generation model where an initial draft conveying only the essential information is incrementally revised to include additional background information. The generator streak, implementing this model, relies on revision operations specifying the various ways a draft can be transformed in order to concisely accommodate a new piece of information. These operations are, for the most part, domain-independent.", "num_citations": "12\n", "authors": ["1533"]}
{"title": "A unified semantics for Constraint Handling Rules in transaction logic\n", "abstract": " Reasoning on Constraint Handling Rules (CHR) programs and their executional behaviour is often ad-hoc and outside of a formal system. This is a pity, because CHR subsumes a wide range of important automated reasoning services. Mapping CHR to Transaction Logic () combines CHR rule specification, CHR rule application, and reasoning on CHR programs and CHR derivations inside one formal system which is executable. This new  semantics obviates the need for disjoint declarative and operational semantics.", "num_citations": "9\n", "authors": ["1533"]}
{"title": "Avalia\u00e7\u00e3o Emp\u00edrica da Expans\u00e3o de Consultas Baseada em um Thesaurus: aplica\u00e7\u00e3o em um engenho de busca na Web.\n", "abstract": " Neste artigo, s\u00e3o avaliados empiricamente os ganhos da precis\u00e3o, cobertura e medida-F, obtidos a partir do uso de v\u00e1rias estrat\u00e9gias de expans\u00e3o de consultas submetidas a um engenho de busca da Web. Estas expans\u00f5es foram realizadas de forma autom\u00e1tica e baseadas em um thesaurus: WordNet. Os resultados desta avalia\u00e7\u00e3o mostraram que a expans\u00e3o com sin\u00f4nimos e/ou hiper\u00f4nimos de primeiro n\u00edvel melhora a medida-F e a cobertura. Eles tamb\u00e9m mostraramm que a expans\u00e3o com sin\u00f4nimos associados ao significado mais comum das palavras-chave das consultas melhora a medida-F, a cobertura e a precis\u00e3o para os primeiros 20, 30, 40 e 50 resultados ordenados da consulta.Abstract: In this paper, we empirically measure the precision, recall and F-measure gains, captured from usage of several query expansion strategies applied to a web search engine. This expansion was done automatically\u00a0\u2026", "num_citations": "8\n", "authors": ["1533"]}
{"title": "User-needs analysis and design methodology for an automated documentation generator\n", "abstract": " Telephone network planning engineers routinely study feeder routes within the telephone network in order to create and refine network capacity expansion (relief) plans. In doing so they make use of a powerful software tool called LEIS-PLAN\u2122. We are developing an extension to PLAN called PLANDoc that will automatically generate natural language narratives documenting the engineers\u2019 use of PLAN. In this paper, we present the user-needs analysis and design methodology we have used in developing the PLANDoc system. We describe our interviews with various end users to determine if such a system would be desirable and what design factors would make it useful. We show how we model the system on a set of iteratively-revised human-generated narratives. The model narratives determine the function and architecture of the documentation system, and they inform the development of the system components.", "num_citations": "8\n", "authors": ["1533"]}
{"title": "Concurrent transaction frame logic formal semantics for uml activity and class diagrams\n", "abstract": " We propose Concurrent Transaction Frame Logic (CTFL) as a language to provide formal semantics to UML activity and class diagrams. CTFL extends first-order Horn logic with object-oriented class hierarchy and object definition terms, and with five new logical connectives that declaratively capture temporal and concurrency constraints on updates and transactions. CTFL has coinciding, sound and refutation complete proof and model theories. CTFL allows using a single language to (1) formally describe the semantics of both activity and class diagrams, (2) verify UML models based on these two diagrams using theorem proving and (3) implement the model as an executable, object-oriented logic program.", "num_citations": "7\n", "authors": ["1533"]}
{"title": "Mapping UML class diagrams to object-oriented logic programs for formal model-driven development\n", "abstract": " MODELOG aims at automatically mapping UML class, object, statechart, activity and collaboration diagrams adorned with Object-Constraint Language expressions to non-monotonic, dynamic, object-oriented logic programs in Concurrent Transaction Frame Logic (CTFL). Coupled with the Flora-2 inference engine for CTFL, MODELOG will fill five gaps in the current UML-based infrastructure for the Common Warehouse Meta-model, Model-Driven Architecture and Semantic Web visions:(1) automated data transformation transactions specified using the Meta-Object Facility for data warehousing and mining,(2) automated UML model transformations for refinement and refactoring,(3) formal verification of UML models,(4) complete UML model compiling into running code and (5) deductive and abductive inference in intelligent agents leveraging UML semantic web ontologies. In this paper, we present the MODELOG mapping of UML class diagrams to structural CTFL clauses.", "num_citations": "6\n", "authors": ["1533"]}
{"title": "HYSSOP: Natural Language Generation Meets Knowledge Discovery in Databases\n", "abstract": " In this paper, we present HYSSOP, a system that generates natural language hypertext summaries of insights resulting from a knowledge discovery process. We discuss the synergy between the two technologies underlying HYSSOP: Natural Language Generation (NLG) and Knowledge Discovery in Databases (KDD). We first highlight the advantages of natural language hypertext as a summarization medium for KDD results, showing the gains that it provides over charts and tables in terms of conciseness, expressive versatility and ease of interpretation for decision makers. Second, we highlight how KDD technologies, and in particular OLAP and data mining, can implement key tasks of automated natural language data summary generation, in a more domain-independent and scalable way than the human written heuristic rule approach of previous systems.", "num_citations": "6\n", "authors": ["1533"]}
{"title": "Using OLAP and data mining for content planning in natural language generation\n", "abstract": " We present a new approach to content determination and discourse organization in Natural Language Generation (NLG). This approach relies on two decision-support oriented database technologies, OLAP and data mining, and it can be used for any NLG application involving the textual summarization of quantitative data. It improves on previous approaches to content planning for NLG in quantitative domains by providing: (1) application domain independence, (2) efficient, variable granularity insight search in high dimensionality data spaces, (3) automatic discovery of surprising, counter-intuitive data, and (4) tailoring of output text organization towards different, declaratively speci- fied, analytical perspectives on the input data.", "num_citations": "6\n", "authors": ["1533"]}
{"title": "Measuring the effect of centroid size on web search precision and recall\n", "abstract": " We present a controlled experiment that measures the effect, on retrieval effectiveness and efficiency, of varying the maximum keyword number used to represent documents in Web search engines. We defined a general methodology to quantitatively evaluate the impact of a given design option on the retrieval effectiveness-efficiency trad-offs, and apply to this particular case of centroid size. The resulting data suggests that in the general case, the most appropriate centroid size is 100 Introduction The exponential growth of the Web makes more crucial everyday the development of powerful, user-friendly tools to cope with information overload. The most widely used of such tools, and perhaps the only ones to have made inroads in the everyday life of the general public, are Web search engines. While these engines may significantly vary in the details of the Information Retrieval (IR) techniques they use, the great majority of them nonetheless share the same basic principles. One such princ...", "num_citations": "6\n", "authors": ["1533"]}
{"title": "Default reasoning in CHR\u2228\n", "abstract": " CHR\u2228 has emerged as a versatile knowledge representation language, usable for an unparalleled variety of automated reasoning tasks: constraint solving, optimization, classification, subsumption, classical deduction, abduction, truth-maintenance, belief revision, belief update and planning. In this paper, we add default reasoning to this list, by showing how to represent default logic theories in CHR\u2228. We then discuss how to leverage this representation together with the well-know correspondence between default logic and Negation As Failure (NAF) in logic programming, to propose an extension CHR\u2228, naf of CHR\u2228 allowing NAF in the rule heads.", "num_citations": "5\n", "authors": ["1533"]}
{"title": "Content aggregation in natural language hypertext summarization of OLAP and data mining discoveries\n", "abstract": " We present a new approach to paratactic content aggregation in the context of generating hypertext summaries of OLAP and data mining discoveries. Two key properties make this approach innovative and interesting:(1) it encapsulates aggregation inside the sentence planning component, and (2) it relies on a domain independent algorithm working on a data structure that abstracts from lexical and syntactic knowledge.", "num_citations": "5\n", "authors": ["1533"]}
{"title": "Generating newswire report leads with historical information: A draft and revision approach\n", "abstract": " In this paper I investigate the issue of providing historical background in computer-generated reports. I rst observe that ignoring this issue is the most drastic limitation of existing report generation systems. I then present an empirical corpus analysis of basketball summaries aimed at discovering the speci c means by which historical information is conveyed in human-generated reports. This analysis resulted in a set of data that forms the basis for the implementation of streak, a system generating basketball report leads with historical information. This data shows that building such a system cannot be based on the single-pass pipelined architecture of previous systems. Instead, I propose an entirely new architecture in which generation proceeds in two passes. The rst pass builds a report draft containing only the basic facts. The second pass incrementally revises this draft to include additional facts providing the historical background. Independently of the issue of historical information, I show that this architecture also allows the generation of more complex and syntactically diverse sentences than previous generation systems. I also explain how the information-adding revision framework de ned by this architecture di ers from previous work in generation with revision. I then delimit the additional research I propose to pursue within this framework. I conclude by showing how the proposed work will make several signi cant contributions to four natural language generation topics: report generation, complex sentence generation, generation architecture and generation with revision.", "num_citations": "5\n", "authors": ["1533"]}
{"title": "Adaptive CHR Meets CHR\u2228\n", "abstract": " Adaptive constraint processing with Constraint Handling Rules (CHR) allows the application of intelligent search strategies to solve Constraint Satisfaction Problems (CSP), but these search algorithms have to be implemented in the host language of adaptive CHR which is currently Java. On the other hand, CHR\u2009\u2228\u2009 enables to explicitly formulate search in CHR, using disjunctive bodies to model choices. However, a naive implementation for handling disjunctions, in particular chronological backtracking (as implemented in Prolog), might cause \u201cthrashing\u201d due to an inappropriate order of decisions. In order to avoid this, a first combination of adaptive CHR and CHR\u2009\u2228\u2009 is presented to offer a more efficient embedded search mechanism to handle disjunctions. Therefore, the refined operational semantics of CHR is extended for disjunctions and adaptation.", "num_citations": "4\n", "authors": ["1533"]}
{"title": "SKDQL: A structured language to specify knowledge discovery processes and queries\n", "abstract": " Tools and techniques used for automatic and smart analysis of huge data repositories of industries, governments, corporations and scientific institutes are the subjects dealt by the field of Knowledge Discovery in Databases (KDD). In MATRIKS context, a framework for KDD, SKDQL (Structured Knowledge Discovery Query Language) is the proposal of a structured language for KDD specification, following SQL patterns within an open and extensible architecture, supporting heterogeneity, interaction and increment of KDD process, with resources for accessing, cleaning, transforming, deriving and mining data, beyond knowledge manipulation.", "num_citations": "3\n", "authors": ["1533"]}
{"title": "Control in Functional Unification Grammars for Text Generation\n", "abstract": " Standard Functional Unification Grammars (FUGs) provide a structurally guided top-down control regime for text generation that is not appropriate for handling non-structural and dynamic constraints. We introduce two control tools that we have implemented for FUGs to address these limitations: bk-class, a tool to limit search by using a form of dependency-directed backtracking and external, a co-routine mechanism allowing a FUG to cooperate with dynamic constraint sources. We show how these tools complement the top-down regime of FUGs to enhance lexical choice.", "num_citations": "3\n", "authors": ["1533"]}
{"title": "Online Verification through Model Checking of Medical Critical Intelligent Systems\n", "abstract": " Software systems based on Artificial Intelligence (AI) and Machine Learning (ML) are being widely adopted in various scenarios, from online shopping to medical applications. When developing these systems, one needs to take into account that they should be verifiable to make sure that they are in accordance with their requirements. In this work we propose a framework to perform online verification of ML models, through the use of model checking. In order to validate the proposal, we apply it to the medical domain to help qualify medical risk. The results reveal that we can efficiently use the framework to determine if a patient is close to the multidimensional decision boundary of a risk score model. This is particularly relevant since patients in these circumstances are the ones more likely to be misclassified. As such, our framework can be used to help medical teams make better informed decisions.", "num_citations": "2\n", "authors": ["1533"]}
{"title": "Um Ambiente para Desenvolvimento de Gram\u00e1ticas Computacionais para o Portugu\u00eas.\n", "abstract": " Neste trabalho, apresentamos um ambiente (m\u00e9todo e ferramentas) para desenvolvimento de gram\u00e1ticas computacionais para gera\u00e7\u00e3o (GCGs), para serem usadas como componentes tipo plug-in em aplica\u00e7\u00f5es de gera\u00e7\u00e3o de texto em portugu\u00eas. O m\u00e9todo \u00e9 baseado na tradu\u00e7\u00e3o de Gram\u00e1ticas de Unifica\u00e7\u00e3o Funcional (FUGs) para uma extens\u00e3o das Gram\u00e1ticas de Cl\u00e1usulas Definidas (DCGs). As FUGs facilitam a especifica\u00e7\u00e3o das regras diretamente por ling\u00fcistas enquanto que as DCGs permitem uma eficiente implementa\u00e7\u00e3o numa linguagem de programa\u00e7\u00e3o declarativa. Na discuss\u00e3o do m\u00e9todo, apresenta-se uma mini-gram\u00e1tica exemplo que serve como base no desenvolvimento das primeiras vers\u00f5es de GCGs do portugu\u00eas. Este trabalho \u00e9 direcionado para pesquisadores de ling\u00fc\u00edstica (computacional e te\u00f3rica) e de ci\u00eancias de computa\u00e7\u00e3o.", "num_citations": "2\n", "authors": ["1533"]}
{"title": "Increasing profitability: voice-based browsing to recommendation system web services\n", "abstract": " We present a mediator Web service to provide voice-based access to recommendation systems through voice portals. The mediator allows requests for recommendation to be forwarded from the voice portal to the recommendation system Web service and uses natural language generation to summarizes the recommendation results and potential follow-ups as a VoiceXML dialog that is sent back to the portal. Since the mediator must be independent of the recommendation application, the semantics of the concepts used in the two services is reconciled by ontology-mediation: the mediator publishes its general recommendation system\u2019s ontology of entities and queries and the recommendation system Web service, in turn, attaches specific entities and services it provides, as specialization of concepts of the published general ontology that the mediator is able to convey as a natural language dialog.", "num_citations": "1\n", "authors": ["1533"]}
{"title": "Inductive Object-Oriented Logic Programming\n", "abstract": " In many of its practical applications, such as natural language processing, automatic programming, expert systems, semantic web ontologies and knowledge discovery in databases, Inductive Logic Programming (ILP) is not used to substitute but rather to complement manual knowledge acquisition. This manual acquisition is increasingly done using hybrid languages integrating objects with rules or relations. Since using a common representation language for both manually encoded and ILP learned knowledge is key to their seamless integration, this raises the issue of using such hybrid languages for induction. In this paper, we present Cigolf, an ILP system that uses the object-oriented logic language Flora for knowledge representation. Cigolf takes as input a background knowledge base, a set of examples, and a learning bias specification, all represented in Flora. It translates this objectoriented input into a relational input specification for the ILP system Aleph. It then uses a tabled Prolog version of Aleph to induce new knowledge and translates back this learned knowledge into Flora. We describe the issues raised by this bi-directional translation process and the solution we adopted. We also compare the respective performance of Cigolf and Aleph on a few ILP benchmarks to assess the overhead associated with using an object-oriented logic representation language instead of a purely logic one for learning tasks.", "num_citations": "1\n", "authors": ["1533"]}