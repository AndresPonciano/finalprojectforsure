{"title": "Software testing and analysis: process, principles, and techniques\n", "abstract": " Market_Desc:\u00b7 IT professionals\u00b7 Students and Instructors of Computer Science Special Features:\u00b7 Promotes a vision of software testing & analysis that is integrated into modern software engineering practice\u00b7 Provides balanced coverage of software testing & analysis approaches, not oriented toward ultra-high reliability or high-speed development approaches\u00b7 Covers techniques that are suitable for near-term application, with sufficient technical background to indicate how and when to apply them\u00b7 Presents software testing and static analysis techniques in a coherent framework as complementary approaches for achieving adequate quality at acceptable cost. About The Book: Software Testing & Analysis teaches readers how to test and analyze software to achieve an acceptable level of quality at an acceptable cost. Readers will be able to minimize software failures, increase quality, and effectively manage costs. By incorporating modern topics and strategies, this book will be the standard software-testing textbook. Software Testing and Analysis integrates software testing and analysis techniques into modern software development practice.", "num_citations": "596\n", "authors": ["264"]}
{"title": "Automatic generation of software behavioral models\n", "abstract": " Dynamic analysis of software systems produces behavioral models that are useful for analysis, verification and testing.", "num_citations": "525\n", "authors": ["264"]}
{"title": "A unified high-level Petri net formalism for time-critical systems\n", "abstract": " Petri nets are a powerful formalism for the specification and analysis of concurrent systems. Thanks to their flexibility, they have been extended and modified in several ways in order to match the requirements of specific application areas. In particular, since Petri nets easily model control flow, some extensions have been proposed to deal with functional aspects while others have taken timing issues into account. Unfortunately, so far little has been done to integrate these aspects, that are crucial in the case of time-critical systems. In this paper, we introduce a high-level Petri net formalism (ER nets) which can be used to specify control, function, and timing issues. In particular, we discuss how time can be modeled via ER nets by providing a suitable axiomatization. Then, we use ER nets to define a time notation (called \u0422\u0412 nets), which is shown to generalize most time Petri net-based formalisms which appeared in the\u00a0\u2026", "num_citations": "500\n", "authors": ["264"]}
{"title": "An empirical evaluation of fault-proneness models\n", "abstract": " Planning and allocating resources for testing is difficult and it is usually done on an empirical basis, often leading to unsatisfactory results. The possibility of early estimation of the potential faultiness of software could be of great help for planning and executing testing activities. Most research concentrates on the study of different techniques for computing multivariate models and evaluating their statistical validity, but we still lack experimental data about the validity of such models across different software applications. The paper reports on an empirical study of the validity of multivariate models for predicting software fault-proneness across different applications. It shows that suitably selected multivariate models can predict fault-proneness of modules of different software packages.", "num_citations": "181\n", "authors": ["264"]}
{"title": "On formalizing UML with high-level Petri nets\n", "abstract": " Object-oriented methodologies are increasingly used in software development. Despite the proposal of several formally based models, current object-oriented practice is still dominated by informal methodologies, like Booch, OMT, and UML. Unfortunately, the lack of dynamic semantics of such methodologies limits the possibility of early analysis of specifications.               This paper indicates the feasibility of ascribing formal semantics to UML by defining translation rules that automatically map UML specifications to high-level Petri nets. This paper illustrates the method through the hurried philosophers problem, that is first specified by using (a subset of) UML, and then mapped onto high-level Petri nets. The paper indicates how UML specifications can be verified by discussing properties of the hurried philosophers problem that can be verified on the derived highlevel Petri net.", "num_citations": "162\n", "authors": ["264"]}
{"title": "Automated testing of classes\n", "abstract": " Programs developed with object technologies have unique features  that often make traditional testing methods inadequate.  Consider,  for instance, the dependence between the state of an object and the  behavior of that object: The outcome of a method executed by an  object often depends on the state of the object when the method is  invoked.  It is therefore crucial that techniques for testing of  classes exercise class methods when the method's receiver is in  different states.  The state of an object at any given time depends  on the sequence of messages received by the object up to that time.  Thus, methods for testing object-oriented software should identify  sequences of method invocations that are likely to uncover potential  defects in the code under test.  However, testing methods for  traditional software do not provide this kind of information.In this paper, we use data flow analysis, symbolic execution, and\u00a0\u2026", "num_citations": "138\n", "authors": ["264"]}
{"title": "Automatic steering of behavioral model inference\n", "abstract": " Many testing and analysis techniques use finite state models to validate and verify the quality of software systems. Since the specification of such models is complex and time-consuming, researchers defined several techniques to extract finite state models from code and traces. Automatically generating models requires much less effort than designing them, and thus eases the verification and validation of large software systems. However, when models are inferred automatically, the precision of the mining process is critical. Behavioral models mined with imprecise processes can include many spurious behaviors, and can thus compromise the results of testing and analysis techniques that use those models.", "num_citations": "131\n", "authors": ["264"]}
{"title": "Using symbolic execution for verifying safety-critical systems\n", "abstract": " Safety critical systems require to be highly reliable and thus special care is taken when verifying them in order to increase the confidence in their behavior. This paper addresses the problem of formal verification of safety critical systems by providing empirical evidence of the practical applicability of symbolic execution and of its usefulness for checking safety-related properties. In this paper, symbolic execution is used for building an operational model of the software on which safety properties, expressed by means of a Path Description Language (PDL), can be assessed.", "num_citations": "127\n", "authors": ["264"]}
{"title": "Dynamic analysis for diagnosing integration faults\n", "abstract": " Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component\u00a0\u2026", "num_citations": "124\n", "authors": ["264"]}
{"title": "Automatic recovery from runtime failures\n", "abstract": " We present a technique to make applications resilient to failures. This technique is intended to maintain a faulty application functional in the field while the developers work on permanent and radical fixes. We target field failures in applications built on reusable components. In particular, the technique exploits the intrinsic redundancy of those components by identifying workarounds consisting of alternative uses of the faulty components that avoid the failure. The technique is currently implemented for Java applications but makes little or no assumptions about the nature of the application, and works without interrupting the execution flow of the application and without restarting its components. We demonstrate and evaluate this technique on four mid-size applications and two popular libraries of reusable components affected by real and seeded faults. In these cases the technique is effective, maintaining the\u00a0\u2026", "num_citations": "108\n", "authors": ["264"]}
{"title": "A general way to put time in Petri nets\n", "abstract": " Petri nets have been widely used in the past to model vari&s types of concurrent systems, including real-time systems. To do so, a variety of ad-hoc solutions have been proposed in order to allow time to be taken into account in specifications. In this paper, we initially survey these proposals and then we illustrate a new proposal (TEI nets), that unifies and generalizes all pre, vious ones. Next, we discuss how these and possibly other enrichments of the original model can be formally defined. This will be do, ne in terms of a high-level net model, called ER nets. The ability of ER nets to express a variety of behaviors justifies our choice to use them as a kernel formalism of a specification support environment that we are currently developing. In this environment, it will be possible to define new graphical notations for formal specifications and give them formal semantics in terms of the underlying ER model. This allows the\u00a0\u2026", "num_citations": "108\n", "authors": ["264"]}
{"title": "Autoblacktest: Automatic black-box testing of interactive applications\n", "abstract": " Automatic test case generation is a key ingredient of an efficient and cost-effective software verification process. In this paper we focus on testing applications that interact with the users through a GUI, and present AutoBlackTest, a technique to automatically generate test cases at the system level. AutoBlackTest uses reinforcement learning, in particular Q-Learning, to learn how to interact with the application under test and stimulate its functionalities. The empirical results show that AutoBlackTest can execute a relevant portion of the code of the application under test, and can reveal previously unknown problems by working at the system level and interacting only through the GUI.", "num_citations": "105\n", "authors": ["264"]}
{"title": "Dynamic detection of cots component incompatibility\n", "abstract": " The development of COTS-based systems shifts the focus of testing and verification from single components to component integration. Independent teams and organizations develop COTS components without referring to specific systems or interaction patterns. Developing systems that reuse COTS components (even high-quality ones) therefore presents new compatibility problems. David Garlan, Robert Allen, and John Ockerbloom (1995) reported that in their experience, integrating four COTS components took 10 person-years (rather than the one planned person-year), mainly because of integration problems. According to Barry Boehm and Chris Abts (1999), three of the four main problems with reusing COTS products are absence of control over their functionality, absence of control over their evolution, and lack of design for interoperability. Our proposed technique, called behavior capture and test, detects\u00a0\u2026", "num_citations": "102\n", "authors": ["264"]}
{"title": "Automatic workarounds for web applications\n", "abstract": " We present a technique that finds and executes workarounds for faulty Web applications automatically and at runtime. Automatic workarounds exploit the inherent redundancy of Web applications, whereby a functionality of the application can be obtained through different sequences of invocations of Web APIs. In general, runtime workarounds are applied in response to a failure, and require that the application remain in a consistent state before and after the execution of a workaround. Therefore, they are ideally suited for interactive Web applications, since those allow the user to act as a failure detector with minimal effort, and also either use read-only state or manage their state through a transactional data store. In this paper we focus on faults found in the access libraries of widely used Web applications such as Google Maps. We start by classifying a number of reported faults of the Google Maps and YouTube\u00a0\u2026", "num_citations": "95\n", "authors": ["264"]}
{"title": "Compatibility and regression testing of COTS-component-based software\n", "abstract": " Software engineers frequently update COTS components integrated in component-based systems, and can often chose among many candidates produced by different vendors. This paper tackles both the problem of quickly identifying components that are syntactically compatible with the interface specifications, but badly integrate in target systems, and the problem of automatically generating regression test suites. The technique proposed in this paper to automatically generate compatibility and prioritized test suites is based on behavioral models that represent component interactions, and are automatically generated while executing the original test suites on previous versions of target systems.", "num_citations": "95\n", "authors": ["264"]}
{"title": "Mining behavior models from user-intensive web applications\n", "abstract": " Many modern user-intensive applications, such as Web applications, must satisfy the interaction requirements of thousands if not millions of users, which can be hardly fully understood at design time. Designing applications that meet user behaviors, by efficiently supporting the prevalent navigation patterns, and evolving with them requires new approaches that go beyond classic software engineering solutions. We present a novel approach that automates the acquisition of user-interaction requirements in an incremental and reflective way. Our solution builds upon inferring a set of probabilistic Markov models of the users' navigational behaviors, dynamically extracted from the interaction history given in the form of a log file. We annotate and analyze the inferred models to verify quantitative properties by means of probabilistic model checking. The paper investigates the advantages of the approach referring to a Web\u00a0\u2026", "num_citations": "89\n", "authors": ["264"]}
{"title": "An introduction to software testing\n", "abstract": " The development of large software systems is a complex and error prone process. Faults might occur at any development stage and they must be identified and removed as early as possible to stop their propagation and reduce verification costs. Quality engineers must be involved in the development process since the very early phases to identify required qualities and estimate their impact on the development process. Their tasks span over the whole development cycle and go beyond the product deployment through maintenance and post mortem analysis. Developing and enacting an effective quality process is not a simple task, but it requires that we integrate many quality-related activities with product characteristics, process organization, available resources and skills, and budget constraints.This paper discusses the main characteristics of a good quality process, then surveys the key testing phases and\u00a0\u2026", "num_citations": "81\n", "authors": ["264"]}
{"title": "PLCTools: design, formal validation, and code generation for programmable controllers\n", "abstract": " Strong timing requirements and complex interactions with controlled elements complicate the design and validation of software controllers. Different techniques have been proposed to cope with these problems during the different development steps: for example, differential equations for modeling controlled elements, the IEC 1131-3 notations for designing the software controller, and formal models for validating the design, but no definitive solutions have been proposed yet. The paper describes PLCTOOLS, a toolbox that exploits all the aforementioned techniques to supply an integrated environment for the design, formal validation, and automatic code generation of software controllers.", "num_citations": "80\n", "authors": ["264"]}
{"title": "Behavior capture and test: Automated analysis of component integration\n", "abstract": " Component-based technology is increasingly adopted to speed up the development of complex software through component reuse. Unfortunately, the lack of complete information about reused components, and the complex interaction patterns among components can lead to subtle problems that throw new verification challenges. Good components are often re-used many times, sometimes within product lines, in other cases across different products. The reuse of components provides a lot of information that could be useful for verification. In this paper, we show how to automatically analyze component interactions by collecting information about components' behavior during testing and field execution, and then using the collected information for checking the compatibility of components when updated or reused in new products. The paper illustrates the main problems in developing the idea, proposes original\u00a0\u2026", "num_citations": "69\n", "authors": ["264"]}
{"title": "Deriving models of software fault-proneness\n", "abstract": " The effectiveness of the software testing process is a key issue for meeting the increasing demand of quality without augmenting the overall costs of software development. The estimation of software fault-proneness is important for assessing costs and quality and thus better planning and tuning the testing process. Unfortunately, no general techniques are available for estimating software fault-proneness and the distribution of faults to identify the correct level of test for the required quality. Although software complexity and testing thoroughness are intuitively related to the costs of quality assurance and the quality of the final product, single software metrics and coverage criteria provide limited help in planning the testing process and assuring the required quality. By using logistic regression, this paper shows how models can be built that relate software measures and software fault-proneness for classes of\u00a0\u2026", "num_citations": "67\n", "authors": ["264"]}
{"title": "Improving UML with Petri nets\n", "abstract": " UML is the OMG standard notation for object-oriented modeling. It is easy, graphical and appealing, but in several cases still too imprecise. UML is strong as modeling means, supplies several different diagrammatic notations for representing the different aspects of a system under development, but lacks simulation and verifiability capabilities. This drawback comes from its semi-formal nature: UML is extremely precise and wide if we consider syntactical aspects, but its semantics is as precise as those of informal notations. Scientists and users, together with standardization efforts (UML 2.0), are trying to overcome this problem, but as side effect, they are also limiting the intrinsic flexibility of UML. Moreover, several formalization efforts concentrated on its static elements (for example, inheritance), leaving dynamic semantics almost untouched.In this paper we propose the paring of UML dynamic models with high-level\u00a0\u2026", "num_citations": "67\n", "authors": ["264"]}
{"title": "Towards self-adaptive service-oriented architectures\n", "abstract": " Web services, service-oriented, and service-discovery architectures help developers solve complex business cases, reduce costs, risks, and time-to-market. The task of developers is challenged by the difficulty of guaranteeing interoperability with target web services, since the lack of information about the interaction protocol of dynamically discovered web services may lead to unexpected runtime failures. This paper proposes an approach to design self-adaptive service-oriented architectures. The approach enables clients to automatically adapt their behavior to alternative web services that provide compatible functionality through different interaction protocols. It uses an infrastructure that traces the successful interactions of the web services, automatically synthesize models that approximate the interaction protocols, and steer client-side adaptations at runtime.", "num_citations": "63\n", "authors": ["264"]}
{"title": "Automatic generation of oracles for exceptional behaviors\n", "abstract": " Test suites should test exceptional behavior to detect faults in error-handling code. However, manually-written test suites tend to neglect exceptional behavior. Automatically-generated test suites, on the other hand, lack test oracles that verify whether runtime exceptions are the expected behavior of the code under test.", "num_citations": "62\n", "authors": ["264"]}
{"title": "Introducing formal specification methods in industrial practice\n", "abstract": " Formal specification methods are not often applied in industrial projects, despite their advantages and the maturity of theories and tools. The scarce familiarity of practitioners with formal notations and the difficulties of their use are main causes of the limited success of formal specification methods.Approaches based on the use of popular front-end notations formally defined with mappings on formal models can solve practical problems. However the absence of flexibility of the mappings proposed so far limits the applicability of such approaches to the few environments that match exactly these solutions.", "num_citations": "62\n", "authors": ["264"]}
{"title": "Formal verification with confidence intervals to establish quality of service properties of software systems\n", "abstract": " Formal verification is used to establish the compliance of software and hardware systems with important classes of requirements. System compliance with functional requirements is frequently analyzed using techniques such as model checking, and theorem proving. In addition, a technique called quantitative verification supports the analysis of the reliability, performance, and other quality-of-service (QoS) properties of systems that exhibit stochastic behavior. In this paper, we extend the applicability of quantitative verification to the common scenario when the probabilities of transition between some or all states of the Markov models analyzed by the technique are unknown, but observations of these transitions are available. To this end, we introduce a theoretical framework, and a tool chain that establish confidence intervals for the QoS properties of a software system modelled as a Markov chain with uncertain\u00a0\u2026", "num_citations": "61\n", "authors": ["264"]}
{"title": "Interclass testing of object oriented software\n", "abstract": " The characteristics of object-oriented software affect type and relevance of faults. In particular the state of the objects may cause faults that cannot be easily revealed with traditional testing techniques. This paper proposes a new technique for interclass testing, that is, the problem of deriving test cases for suitably exercising interactions among clusters of classes. The proposed technique uses data-flow analysis for deriving a suitable set of test case specifications for interclass testing. The paper then shows how to automatically generate feasible test cases that satisfy the derived specifications using symbolic execution and automated deduction. Finally, the paper demonstrates the effectiveness of the proposed technique by deriving test cases for a microscope controller developed for the European Space Laboratory of the Columbus Orbital Facility.", "num_citations": "61\n", "authors": ["264"]}
{"title": "Graph models for reachability analysis of concurrent programs\n", "abstract": " The problem of analyzing concurrent systems has been investigated by many researchers, and several solutions have been proposed. Among the proposed techniques, reachability analysis\u2014systematic enumeration of reachable states in a finite-state model\u2014is attractive because it is conceptually simple and relatively straightforward to automate and can be used in conjunction with model-checking procedures to check for application-specific as well as general properties. This article shows that the nature of the translation from source code to a modeling formalism is of greater practical importance than the underlying formalism. Features identified as pragmatically important are the representation of internal choice, selection of a dynamic or static matching rule, and the ease of  applying reductions. Since combinatorial explosion is the primary impediment to application of reachability analysis, a particular concern in\u00a0\u2026", "num_citations": "59\n", "authors": ["264"]}
{"title": "A technique for verifying component-based software\n", "abstract": " Component-based software systems raise new problems for the testing community: the reuse of components suggests the possibility of reducing testing costs by reusing information about the quality of the software components. This paper addresses the problem of testing evolving software systems, i.e., systems obtained by modifying and/or substituting some of their components. The paper proposes a technique to automatically identify behavioral differences between different versions of the system, to deduce possible problems from inconsistent behaviors. The approach is based on the automatic distilling of invariants from in-field executions. The computed invariants are used to monitor the behavior of new components, and to reveal unexpected interactions. The event generated while monitoring system executions are presented to software engineers who can infer possible problems of the new versions.", "num_citations": "58\n", "authors": ["264"]}
{"title": "Teste e an\u00e1lise de software: processos, princ\u00edpios e t\u00e9cnicas\n", "abstract": " Neste livro amplamente ilustrado, a autora, professora de terapia intensiva, demonstra como entender e interpretar o ECG \u00e0 beira do leito do paciente.", "num_citations": "56\n", "authors": ["264"]}
{"title": "Giving semantics to SA/RT by means of high-level timed Petri nets\n", "abstract": " In the IPTES project a dual language approach is proposed for overcoming both the problems derived from the use of a user-friendly, high-level, but not-formally-defined language and from a lower-level, formal, but difficult-to-use language. The approach uses a user-friendly, high-level language as user interface and a lower-level, formal language asmachine language. In this way the users can both access the IPTES environment through a nice interface and can profit from non-ambiguity-checks and proofs algorithms based on the formal kernel machine language. The correspondence between the two languages is built-in in the IPTES environment that provides a transparent mapping mechanism that relates the users specifications expressed by means of the high-level interface-language with the formal definitions expressed in the formal machine language.               This paper presents the mapping\u00a0\u2026", "num_citations": "55\n", "authors": ["264"]}
{"title": "AutoBlackTest: a tool for automatic black-box testing\n", "abstract": " In this paper we present AutoBlackTest, a tool for the automatic generation of test cases for interactive applications. AutoBlackTest interacts with the application though its GUI, and uses reinforcement learning techniques to understand the interaction modalities and to generate relevant testing scenarios. Early results show that the tool has the potential of automatically discovering bugs and generating useful system and regression test suites.", "num_citations": "53\n", "authors": ["264"]}
{"title": "Supporting test suite evolution through test case adaptation\n", "abstract": " Software systems evolve during development and maintenance, and many test cases designed for the early versions of the system become obsolete during the software lifecycle. Repairing test cases that do not compile due to changes in the code under test and generating new test cases to test the changed code is an expensive and time consuming activity that could benefit from automated approaches. In this paper we propose an approach for automatically repairing and generating test cases during software evolution. Differently from existing approaches to test case generation, our approach uses information available in existing test cases, defines a set of heuristics to repair test cases invalidated by changes in the software, and generate new test cases for evolved software. The results obtained with a prototype implementation of the technique show that the approach can effectively maintain evolving test suites\u00a0\u2026", "num_citations": "52\n", "authors": ["264"]}
{"title": "Inferring state-based behavior models\n", "abstract": " Dynamic analysis helps to extract important information about software systems useful in testing, debugging and maintenance activities. Popular dynamic analysis techniques synthesize either information on the values of the variables or information on relations between invocation of methods. Thus, these approaches do not capture the important relations that exist between data values and invocation sequences. In this paper, we introduce a technique, called GK-tail, for generating models that represent the interplay between program variables and method invocations. GK-tail extends the k-tail algorithm for extracting finite state automata from execution traces, to the case of finite state automata with parameters. The paper presents the technique and the results of some preliminary experiments that indicate the potentialities of the proposed approach.", "num_citations": "50\n", "authors": ["264"]}
{"title": "Using high-level Petri nets for testing concurrent and real-time systems\n", "abstract": " This paper faces the problem of testing concurrent and real-time programs. First, it tackles the problem of extending the testing activity to the specification phases, in order to reduce software development costs, increasing the reliability of the produced software. In particular, it proposes the use of the same model, based on Petri nets, for representing both the specifications and the implementations. Thus testing tools and techniques based on such model can be used for validating both the specifications and the implementations.Then, it proposes adequacy criteria for the suggested model. Such criteria are based on coverage measures of the topology of Petri nets. Some adequacy criteria extend in a natural way the criteria used for testing sequential programs, others are strictly related to peculiar problems of concurrent and real-time software. They allow the production of test data for analyzing the specifications and the implementations. Moreover since the same model is used to represent both specifications and implementations, tests derived for the specifications can also be used for testing the implementation, ie the adequacy criteria can be used not only for deriving structural tests, but also functional test.", "num_citations": "49\n", "authors": ["264"]}
{"title": "Cross-checking oracles from intrinsic software redundancy\n", "abstract": " Despite the recent advances in automatic test generation, testers must still write test oracles manually. If formal specifications are available, it might be possible to use decision procedures derived from those specifications. We present a technique that is based on a form of specification but also leverages more information from the system under test. We assume that the system under test is somewhat redundant, in the sense that some operations are designed to behave like others but their executions are different. Our experience in this and previous work indicates that this redundancy exists and is easily documented. We then generate oracles by cross-checking the execution of a test with the same test in which we replace some operations with redundant ones. We develop this notion of cross-checking oracles into a generic technique to automatically insert oracles into unit tests. An experimental evaluation shows\u00a0\u2026", "num_citations": "48\n", "authors": ["264"]}
{"title": "Automated test oracles: A survey\n", "abstract": " Software testing is an essential activity of software development, and oracles are a key pillar of testing. The increasing size of test suites, the growing availability of test case generators that produce enormous amount of test cases, and the repeated execution of large amounts of test cases require automated oracles. Although studied since the late 1970s, in the last decade, test oracles and techniques to automatically generate test oracles have attracted a lot of attention and have witnessed an impressive growth.In this chapter, we survey test oracles with particular attention to their automation. We overview the main evolution of the technology, and we propose some criteria to classify the different approaches. We present the main techniques to automatically generate test oracles classified according to the required information, and we discuss different forms of checkable oracles, defined as oracles expressed in a form\u00a0\u2026", "num_citations": "48\n", "authors": ["264"]}
{"title": "Petri nets and software engineering\n", "abstract": " Software engineering and Petri net theory are disciplines of different nature. Research on software engineering focuses on a problem domain, i.e., the development of complex software systems, and tries to find a coherent set of solutions to cope with the different aspects of the problem, while research on Petri nets investigates applications and properties of a specific model (Petri nets).             When Petri nets can solve some problems of software development, the two disciplines meet with mutual benefits: software engineers may find useful solutions, while Petri net experts may find new stimuli and challenges in their domain.             Petri nets and software engineering have similar age: Karl Adam Petri wrote his thesis in 1962, while the term \u201csoftware engineering\u201d was coined in 1968 at a NATO conference held in Germany. The two disciplines met several times in the past forty years with alternate fortune\u00a0\u2026", "num_citations": "46\n", "authors": ["264"]}
{"title": "Translating code comments to procedure specifications\n", "abstract": " Procedure specifications are useful in many software development tasks. As one example, in automatic test case generation they can guide testing, act as test oracles able to reveal bugs, and identify illegal inputs. Whereas formal specifications are seldom available in practice, it is standard practice for developers to document their code with semi-structured comments. These comments express the procedure specification with a mix of predefined tags and natural language. This paper presents Jdoctor, an approach that combines pattern, lexical, and semantic matching to translate Javadoc comments into executable procedure specifications written as Java expressions. In an empirical evaluation, Jdoctor achieved precision of 92% and recall of 83% in translating Javadoc into procedure specifications. We also supplied the Jdoctor-derived specifications to an automated test case generation tool, Randoop. The\u00a0\u2026", "num_citations": "44\n", "authors": ["264"]}
{"title": "Combining symbolic execution and search-based testing for programs with complex heap inputs\n", "abstract": " Despite the recent improvements in automatic test case generation, handling complex data structures as test inputs is still an open problem. Search-based approaches can generate sequences of method calls that instantiate structured inputs to exercise a relevant portion of the code, but fall short in building inputs to execute program elements whose reachability is determined by the structural features of the input structures themselves. Symbolic execution techniques can effectively handle structured inputs, but do not identify the sequences of method calls that instantiate the input structures through legal interfaces. In this paper, we propose a new approach to automatically generate test cases for programs with complex data structures as inputs. We use symbolic execution to generate path conditions that characterise the dependencies between the program paths and the input structures, and convert the path\u00a0\u2026", "num_citations": "43\n", "authors": ["264"]}
{"title": "In-field healing of integration problems with COTS components\n", "abstract": " Developers frequently integrate complex COTS frameworks and components in software applications. COTS products are often only partially documented, and developers may misuse technologies and introduce integration faults, as witnessed by the many entries in fault repositories. Once identified, common integration problems and their fixes are usually documented in forums and fault repositories on the Web, but this does not prevent them to occur in the field when COTS products are reused. In this paper, we propose a methodology and a self- healing technology that can reduce the occurrence of infield failures caused by common integration problems that are identified and documented by COTS developers. Our methodology supports COTS developers in producing healing connectors for common misuses of COTS products. Our technology produces information that facilitate debugging and patching of\u00a0\u2026", "num_citations": "40\n", "authors": ["264"]}
{"title": "Ensuring interoperable service-oriented systems through engineered self-healing\n", "abstract": " Many modern software systems dynamically discover and integrate third party libraries, components and services that comply with standard APIs. Compliance with standard APIs facilitates dynamic binding, but does not always guarantee full behavioral compatibility. For instance, problems that derive from behavior incompatibility are quite frequent in service-oriented applications that dynamically bind service implementations that match API specifications.", "num_citations": "39\n", "authors": ["264"]}
{"title": "Structural coverage of feasible code\n", "abstract": " Infeasible execution paths reduce the precision of structural testing coverage and limit the industrial applicability of structural testing criteria. In this paper, we propose a technique that combines static and dynamic analysis approaches to identify infeasible program elements that can be eliminated from the computation of structural coverage to obtain accurate coverage data. The main novelty of the approach stems from its ability to identify a relevant number of infeasible elements, that is, elements that belong statically to the code, but cannot be executed under any input condition. The technique can also generate new test cases that execute uncovered elements, thus increasing the structural coverage of the program. The experimental results obtained on a prototype implementation for computing accurate branch coverage and reported in this paper indicate that the technique can effectively improve structural\u00a0\u2026", "num_citations": "37\n", "authors": ["264"]}
{"title": "Formal interpreters for diagram notations\n", "abstract": " The article proposes an approach for defining extensible and flexible formal interpreters for diagram notations with significant dynamic semantics. More precisely, it addresses semi-formal diagram notations that have precisely-defined syntax, but informally defined (dynamic) semantics. These notations are often flexible to fit the different needs and expectations of users. Flexibility comes from the incompleteness or informality of the original definition and results in different interpretations.The approach defines interpreters by means of a mapping onto a semantic domain. Two sets of rules define the correspondences between the elements of the diagram notation and those of the semantic domain, and between events and states of the semantic domain and visual annotations on the elements of the diagram notation.Flexibility also leads to notation families, that is, sets of notations that share core concepts, but present\u00a0\u2026", "num_citations": "36\n", "authors": ["264"]}
{"title": "Mutant operators for testing concurrent java programs\n", "abstract": " Mutation testing is a fault-based testing technique that has been widely studied in the last decades. One reason for the interest of the scientific community in mutation testing is its flexibility. It can be applied to programs at unit and integration testing levels, as well as to software specifications written in a variety of different languages. A fundamental issue to make mutation testing work for a given language or environment is the set of mutant operators used to create the mutants. In this paper a set o mutant operator is proposed for the Java programming language, and more specifically, aiming at exercising aspects of concurrency and synchronization of that language.", "num_citations": "36\n", "authors": ["264"]}
{"title": "Dynamic data flow testing of object oriented systems\n", "abstract": " Data flow testing has recently attracted new interest in the context of testing object oriented systems, since data flow information is well suited to capture relations among the object states, and can thus provide useful information for testing method interactions. Unfortunately, classic data flow testing, which is based on static analysis of the source code, fails to identify many important data flow relations due to the dynamic nature of object oriented systems. In this paper, we propose a new technique to generate test cases for object oriented software. The technique exploits useful inter-procedural data flow information extracted dynamically from execution traces for object oriented systems. The technique is designed to enhance an initial test suite with test cases that exercise complex state based method interactions. The experimental results indicate that dynamic data flow testing can indeed generate test cases that\u00a0\u2026", "num_citations": "34\n", "authors": ["264"]}
{"title": "Automatically repairing test cases for evolving method declarations\n", "abstract": " When software systems evolve, for example due to fault fixes, modification of functionalities or refactoring activities, test cases may become obsolete thus generating wrong results or even not being executable or compilable. Maintaining test cases is expensive and time consuming, and often test cases are discarded by software developers due to high maintenance costs. This paper presents TestCareAssistant, a technique that combines data-flow analysis with program diffing for automatically repairing test cases that become obsolete because of changes in method declarations (addition, removal, or type modification of parameters or return values). The paper illustrates the efficacy of TestCareAssistant by analyzing the impact of method declarations changes on the executability of test cases, and by presenting the preliminary results of applying TestCareAssistant to repair 22 test cases.", "num_citations": "34\n", "authors": ["264"]}
{"title": "Towards autonomic service-oriented applications\n", "abstract": " The integration of third-party web services can solve complex business problems and can reduce risks, costs and time-to-market. However, the task of the integrators is challenged by services that are maintained by different organisations, and that may evolve dynamically and autonomously. The impossibility of statically determining which service implementation will be bound at runtime may lead to unexpected failures. This paper presents a novel approach for designing self-adaptive service-oriented applications, which autonomously react to changes in the implementation of the services, automatically detect possible integration mismatches and dynamically execute suitable adaptation strategies. The solution proposed in this paper is based on a taxonomy of integration faults that helps developers anticipate potential mismatches between discovered web services and applications, and design test cases and\u00a0\u2026", "num_citations": "34\n", "authors": ["264"]}
{"title": "Self-healing by means of automatic workarounds\n", "abstract": " We propose to use automatic workarounds to achieve self-healing in software systems. We observe that software systems of significant complexity, especially those made of components, are often redundant, in the sense that the same functionality and the same state-transition can be obtained through multiple sequences of operations. This redundancy is the basis to construct effective workarounds for component failures. In particular, we assume that failures can be detected and intercepted together with a trace of the operations that lead to the failure. Given the failing sequence, the system autonomically executes one or more alternative sequences that are known to have an equivalent behavior. We argue that such workarounds can be derived with reasonable effort from many forms of specifications, that they can be effectively prioritized either statically or dynamically, and that they can be deployed at run time in a\u00a0\u2026", "num_citations": "34\n", "authors": ["264"]}
{"title": "An empirical evaluation of object oriented metrics in industrial setting\n", "abstract": " Advances in distributed object technologies (eg, the Common Object Request Broker Architecture [15] and the Enterprise Java Bean Specification [19]) dramatically impact the development process of distributed software applications. In particular, time for providing new distributed services is decreasing because applications are not built from scratch any longer. Rather, they are developed based on pre-existing middle tier software (middleware) and integrate components and services provided off-the-shelf by third parties [9]. The increasing demand for rapid provision of new products entails rigid constraints on the activities of quality assurance for this class of applications. It becomes crucial to optimize the allocation of resources for testing and analysis to meet the required quality goals, while reducing time-to-market. Measuring the fault-proneness of the software may facilitate the allocation of resources for testing and analysis. If the distribution of faults in the software can be accurately estimated in advance, resources can be allocated accordingly, ie, more resources to the more fault-prone parts of the software. For example, in the case of code inspection, more thorough inspection sessions could be scheduled for the more fault-prone modules. Although fault-proneness cannot be directly measured, it can be estimated based on other measurable attributes of the software, based on expected correlations between such attributes and fault-proneness. Many software metrics have been proposed for this purpose (eg,[14, 10, 20]). However, the best predictors of fault-proneness may vary according to the class of applications and the target application\u00a0\u2026", "num_citations": "34\n", "authors": ["264"]}
{"title": "Timed high-level nets\n", "abstract": " Petri nets have been widely used for modeling and analyzing concurrent systems. Several reasons contribute to their success: the simplicity of the model, the immediate graphical representation, the easy modeling of asynchronous aspects, the possibility of reasoning about important properties such as reachability, liveness, boundedness. However, the original model fails in representing two important features: complex functional aspects, such as conditions which rule the flow of control, and time. Due to that, two different classes of extensions of Petri nets have been proposed: high-level nets and timed Petri nets. High-level nets allow the representation of functional aspects in full details, but do not provide a means for representing time; on the other hand, timed Petri nets have been thought for time representation, but they do not provide a means for representing detailed functinal aspects. Thus, these two\u00a0\u2026", "num_citations": "34\n", "authors": ["264"]}
{"title": "Localizing faults in cloud systems\n", "abstract": " By leveraging large clusters of commodity hardware, the Cloud offers great opportunities to optimize the operative costs of software systems, but impacts significantly on the reliability of software applications. The lack of control of applications over Cloud execution environments largely limits the applicability of state-of-the-art approaches that address reliability issues by relying on heavyweight training with injected faults. In this paper, we propose LOUD, a lightweight fault localization approach that relies on positive training only, and can thus operate within the constraints of Cloud systems. LOUD relies on machine learning and graph theory. It trains machine learning models with correct executions only, and compensates the inaccuracy that derives from training with positive samples, by elaborating the outcome of machine learning techniques with graph theory algorithms. The experimental results reported in this\u00a0\u2026", "num_citations": "33\n", "authors": ["264"]}
{"title": "Gk-tail+ an efficient approach to learn software models\n", "abstract": " Inferring models of program behavior from execution samples can provide useful information about a system, also in the increasingly common case of systems that evolve and adapt in their lifetime, and without requiring large developers' effort. Techniques for learning models of program behavior from execution traces shall address conflicting challenges of recall, specificity and performance: They shall generate models that comprehensively represent the system behavior (recall) while limiting the amount of illegal behaviors that may be erroneously accepted by the model (specificity), and should infer the models within a reasonable time budget to process industrial scale systems (performance). In our early work, we designed GK-tail, an approach that can infer guarded finite state machines that model the behavior of object-oriented programs in terms of sequences of method calls and constraints on the parameter\u00a0\u2026", "num_citations": "33\n", "authors": ["264"]}
{"title": "Healing Web applications through automatic workarounds\n", "abstract": " We develop the notion of automatic workaround in the context of Web applications. A workaround is a sequence of operations, applied to a failing component, that is equivalent to the failing sequence in terms of its intended effect, but that does not result in a failure. We argue that workarounds exist in modular systems because components often offer redundant interfaces and implementations, which in turn admit several equivalent sequences of operations. In this paper, we focus on Web applications because these are good and relevant examples of component-based (or service-oriented) applications. Web applications also have attractive technical properties that make them particularly amenable to the deployment of automatic workarounds. We propose an architecture where a self-healing proxy applies automatic workarounds to a Web application server. We also propose a method to generate equivalent\u00a0\u2026", "num_citations": "33\n", "authors": ["264"]}
{"title": "Generation of integration tests for self-testing components\n", "abstract": " Internet software tightly integrates classic computation with communication software. Heterogeneity and complexity can be tackled with a\u00a0component-based approach, where components are developed by application experts and integrated by domain experts. Component-based systems cannot be tested with classic approaches but present new problems. Current techniques for integration testing are based upon the component developer providing test specifications or suites with their components. However, components are often being used in ways not envisioned by their developers, thus the packaged test specifications and suites cannot be relied upon. Often this results in conditions being placed upon a\u00a0components use, however, what is required is a method for allowing test suites to be adapted for new situations. In this paper, we propose an approach for implementing self-testing components, which\u00a0\u2026", "num_citations": "33\n", "authors": ["264"]}
{"title": "Towards industrially relevant fault-proneness models\n", "abstract": " Estimating software fault-proneness early, i.e., predicting the  probability of software modules to be faulty, can help in reducing  costs and increasing effectiveness of software analysis and testing.  The many available static metrics provide important information, but  none of them can be deterministically related to software  fault-proneness. Fault-proneness models seem to be an interesting  alternative, but the work on these is still biased by lack of  experimental validation.         This paper discusses barriers and problems in using software  fault-proneness in industrial environments, proposes a method for  building software fault-proneness models based on logistic  regression and cross-validation that meets industrial needs, and  provides some experimental evidence of the validity of the proposed  approach.", "num_citations": "33\n", "authors": ["264"]}
{"title": "Toward formalizing structured analysis\n", "abstract": " Real-time extensions to structured analysis (SA/RT) are popular in industrial practice. Despite the large industrial experience and the attempts to formalize the various \u201cdialects,\u201d SA/RT notations are still imprecise and ambiguous. This article tries to identify the semantic problems of the requirements definition notation defined by Hatley and Pirbhai, one of the popular SA/RT \u201cdialects,\u201d and discusses possible solutions. As opposed to other articles that give their own interpretation, this article does not propose a specific semantics for the notation. This article identifies imprecisions, i.e., missing or partial information about features of the notation; it discusses ambiguities, i.e., elements of the definition that allow at least two different  (\u201creasonable\u201d) interpretations of features of the notation; and it lists extensions, i.e., features not belonging to the notation, but required by many industrial users and often supported by CASE\u00a0\u2026", "num_citations": "33\n", "authors": ["264"]}
{"title": "A Petri net and logic approach to the specification and verification of real time systems\n", "abstract": " La presente simulazione \u00e8 stata realizzata sulla base delle regole riportate nel DM 598/2018 e allegata Tabella A. Cineca non si assume alcuna responsabilit\u00e0 in merito all\u2019uso che il diretto interessato o terzi faranno della simulazione. Si specifica inoltre che la simulazione contiene calcoli effettuati con dati e algoritmi di pubblico dominio e deve quindi essere considerata come un mero ausilio al calcolo svolgibile manualmente o con strumenti equivalenti. Informazioni sui dati: vengono considerati tutti i prodotti in stato definitivo. Per i prodotti indicizzati wos/scopus, l\u2019anno di riferimento e la tipologia sono quelli riportati in banca-dati.", "num_citations": "33\n", "authors": ["264"]}
{"title": "High-level timed Petri nets as a kernel for executable specifications\n", "abstract": " One of the goals of the IPTES environment is to provide a highly usable and formally based specification support environment for real-time applications. Therefore the environment is built upon a formal language that provides a sound, and mathematically well-defined kernel for IPTES. The language provides a means for formulating unambiguous specifications that can be formally verified at any stage of the project. The ability of verifying the specifications from the early stages of the project is very important for revealing errors when their correction can be done at a much lower cost compared with the cost of removing the same errors in later phases.               The formal kernel of IPTES is a class of high-level Petri nets, called HLTPNs (High-Level Timed Petri Nets), that allow specifications to be executed, simulated, tested and formally proved.               HLTPNs come in two forms: the internal form (HLTPN\u00a0\u2026", "num_citations": "33\n", "authors": ["264"]}
{"title": "Enhancing symbolic execution with built-in term rewriting and constrained lazy initialization\n", "abstract": " Symbolic execution suffers from problems when analyzing programs that handle complex data structures as their inputs and take decisions over non-linear expressions. For these programs, symbolic execution may incur invalid inputs or unidentified infeasible traces, and may raise large amounts of false alarms. Some symbolic executors tackle these problems by introducing executable preconditions to exclude invalid inputs, and some solvers exploit rewrite rules to address non linear problems. In this paper, we discuss the core limitations of executable preconditions, and address these limitations by proposing invariants specifically designed to harmonize with the lazy initialization algorithm. We exploit rewrite rules applied within the symbolic executor, to address simplifications of inverse relationships fostered from either program-specific calculations or the logic of the verification tasks. We present a symbolic\u00a0\u2026", "num_citations": "32\n", "authors": ["264"]}
{"title": "Achieving cost-effective software reliability through self-healing\n", "abstract": " Heterogeneity, mobility, complexity and new application domains raise new software reliability issues that cannot be met cost-effectively only with classic software engineering approaches. Self-healing systems can successfully address these problems, thus increasing software reliability while reducing maintenance costs. Self-healing systems must be able to automatically identify runtime failures, locate faults, and find a way to bring the system back to an acceptable behavior. This paper discusses the challenges underlying the construction of self-healing systems with particular focus on functional failures, and presents a set of techniques to build software systems that can automatically heal such failures. It introduces techniques to automatically derive assertions to effectively detect functional failures, locate the faults underlying the failures, and identify sequences of actions alternative to the failing sequence to bring the system back to an acceptable behavior.", "num_citations": "32\n", "authors": ["264"]}
{"title": "A survey of recent trends in testing concurrent software systems\n", "abstract": " Many modern software systems are composed of multiple execution flows that run simultaneously, spanning from applications designed to exploit the power of modern multi-core architectures to distributed systems consisting of multiple components deployed on different physical nodes. We collectively refer to such systems as concurrent systems. Concurrent systems are difficult to test, since the faults that derive from their concurrent nature depend on the interleavings of the actions performed by the individual execution flows. Testing techniques that target these faults must take into account the concurrency aspects of the systems. The increasingly rapid spread of parallel and distributed architectures led to a deluge of concurrent software systems, and the explosion of testing techniques for such systems in the last decade. The current lack of a comprehensive classification, analysis and comparison of the many\u00a0\u2026", "num_citations": "31\n", "authors": ["264"]}
{"title": "Towards self-protecting enterprise applications\n", "abstract": " Enterprise systems must guarantee high availability and reliability to provide 24/7 services without interruptions and failures. Mechanisms for handling exceptional cases and implementing fault tolerance techniques can reduce failure occurrences, and increase dependability. Most of such mechanisms address major problems that lead to unexpected service termination or crashes, but do not deal with many subtle domain dependent failures that do not necessarily cause service termination or crashes, but result in incorrect results. In this paper, we propose a technique for developing selfprotecting systems. The technique proposed in this paper observes values at relevant program points. When the technique detects a software failure, it uses the collected information to identify the execution contexts that lead to the failure, and automatically enables mechanisms for preventing future occurrences of failures of the\u00a0\u2026", "num_citations": "31\n", "authors": ["264"]}
{"title": "Testing object-oriented software\n", "abstract": " The best approach to testing object-oriented software depends on many factors: the application-under-test, the development approach, the organization of the development and quality assurance teams, the criticality of the application, the development environment and the implementation language(s), the use of design and language features, project timing and resource constraints. Nonetheless, we can outline a general approach that works in stages from independent consideration of classes and their features to consideration of their interactions. A coherent strategy would include three main phases: intraclass, interclass, and system and acceptance testing.", "num_citations": "31\n", "authors": ["264"]}
{"title": "A framework for testing object-oriented components\n", "abstract": " The terms \u201ccomponent\u201d and \u201ccomponent-based software engineering\u201d are relatively recent. Although there is broad agreement on the meaning of these terms, different authors have sometimes used slightly different interpretations. Following Brown and Wellnau [4], here we view a component as \u201ca replaceable software unit with a set of contractually-specified interfaces and explicit context dependencies only.\u201d A component is a program or a set of programs developed by one organization and deployed by one or more other organizations, possibly in different application domains. To date, many component systems are designed and developed using object technology. The same is true of middleware architectures that support the deployment of components, such as Java Beans [3], CORBA [1], and DCOM [5]. Although there is consensus that the issues in component-based software engineering cannot be solved merely by object technology [4, 14], these technologies will clearly play a fundamental role in the production of software components [15]. It is quite likely that developers will use with increasing frequency object technologies in the design and implementation of components. For this reason, in the sequel we focus on testing of components that consist of an object or a set of cooperating objects. In addition, we consider messages sent to a component as method invocations on the objects contained in the component.Programs developed with object technologies have unique features that often make traditional testing techniques inadequate. An important feature of these programs is that the behavior of a method may depend on the state of the\u00a0\u2026", "num_citations": "31\n", "authors": ["264"]}
{"title": "A technique for designing robotic control systems based on Petri nets\n", "abstract": " Traditional techniques for designing the software of complex real-time control systems offer limited support for validating design before producing final code. The need of early validation of the design of control software is increasing, due to the growing complexity of the software components. This is especially the case of advanced robot controllers, due to the strict timing requirements of run time elaboration of information from exteroceptive sensors. The use of formal methods can increase the possibility of validating the design. This paper presents an approach for designing software for robotic controllers based on the use of a specialized notation formally defined by means of Petri nets (CONTROL NETS). The proposed notation is easy to use in the specific domain and it benefits from the validation techniques available for Petri nets. Benefits of CONTROL NETS, with respect to a traditional approach based on\u00a0\u2026", "num_citations": "31\n", "authors": ["264"]}
{"title": "Reusing constraint proofs in program analysis\n", "abstract": " Symbolic analysis techniques have largely improved over the years, and are now approaching an industrial maturity level. One of the main limitations to the scalability of symbolic analysis is the impact of constraint solving that is still a relevant bottleneck for the applicability of symbolic techniques, despite the dramatic improvements of the last decades. In this paper we discuss a novel approach to deal with the constraint solving bottleneck. Starting from the observation that constraints may recur during the analysis of the same as well as different programs, we investigate the advantages of complementing constraint solving with searching for the satisfiability proof of a constraint in a repository of constraint proofs. We extend recent proposals with powerful simplifications and an original canonical form of the constraints that reduce syntactically different albeit equivalent constraints to the same form, and thus facilitate the\u00a0\u2026", "num_citations": "30\n", "authors": ["264"]}
{"title": "Measuring software redundancy\n", "abstract": " Redundancy is the presence of different elements with the same functionality. In software, redundancy is useful (and used) in many ways, for example for fault tolerance and reliability engineering, and in self-adaptive and self-checking programs. However, despite the many uses, we still do not know how to measure software redundancy to support a proper and effective design. If, for instance, the goal is to improve reliability, one might want to measure the redundancy of a solution to then estimate the reliability gained with that solution. Or one might compare alternative solutions to choose the one that expresses more redundancy and therefore, presumably, more reliability. We first formalize a notion of redundancy whereby two code fragments are considered redundant when they achieve the same functionality with different executions. On the basis of this abstract and general notion, we then develop a concrete\u00a0\u2026", "num_citations": "30\n", "authors": ["264"]}
{"title": "Generating effective integration test cases from unit ones\n", "abstract": " Unit testing aims to ensure that methods correctly implement the specified and implied pre- and post-conditions, while integration testing ensures that modules correctly follow interaction protocols. While the generation of unit test cases has been explored extensively in the literature, there is still little work on the generation of integration test cases. In this paper we present a new technique to generate integration test cases that leverages existing unit test cases. Our key observation is that both, unit and integration testing, use method calls as the atoms to construct test cases from. Unit tests contain information on how to instantiate classes in meaningful ways, how to construct arguments for method calls, and what the resulting system state should be after calling methods with those arguments. We use this information to construct more complex test cases that focus on class interactions rather than on individual state\u00a0\u2026", "num_citations": "30\n", "authors": ["264"]}
{"title": "Augusto: Exploiting popular functionalities for the generation of semantic gui tests with oracles\n", "abstract": " Testing software applications by interacting with their graphical user interface (GUI) is an expensive and complex process. Current automatic test case generation techniques implement explorative approaches that, although producing useful test cases, have a limited capability of covering semantically relevant interactions, thus frequently missing important testing scenarios. These techniques typically interact with the available widgets following the structure of the GUI, without any guess about the functions that are executed.", "num_citations": "29\n", "authors": ["264"]}
{"title": "Symbolic execution of programs with heap inputs\n", "abstract": " Symbolic analysis is a core component of many automatic test generation and program verication approaches. To verify complex software systems, test and analysis techniques shall deal with the many aspects of the target systems at different granularity levels. In particular, testing software programs that make extensive use of heap data structures at unit and integration levels requires generating suitable input data structures in the heap. This is a main challenge for symbolic testing and analysis techniques that work well when dealing with numeric inputs, but do not satisfactorily cope with heap data structures yet. In this paper we propose a language HEX to specify invariants of partially initialized data structures, and a decision procedure that supports the incremental evaluation of structural properties in HEX. Used in combination with the symbolic execution of heap manipulating programs, HEX prevents the\u00a0\u2026", "num_citations": "29\n", "authors": ["264"]}
{"title": "Automatic testing of GUI\u2010based applications\n", "abstract": " Testing GUI\u2010based applications is hard and time consuming because it requires exploring a potentially huge execution space by interacting with the graphical interface of the applications. Manual testing can cover only a small subset of the functionality provided by applications with complex interfaces, and thus, automatic techniques are necessary to extensively validate GUI\u2010based systems. This paper presents AutoBlackTest, a technique to automatically generate test cases at the system level. AutoBlackTest uses reinforcement learning, in particular Q\u2010learning, to learn how to interact with the application under test and stimulate its functionalities. When used to complement the activity of test designers, AutoBlackTest reuses the information in the available test suites to increase its effectiveness. The empirical results show that AutoBlackTest can sample better than state of the art techniques the behaviour of the\u00a0\u2026", "num_citations": "29\n", "authors": ["264"]}
{"title": "Enhancing structural software coverage by incrementally computing branch executability\n", "abstract": " Structural code coverage criteria have been studied since the early seventies, and now they are well supported by commercial and open-source tools and are commonly embedded in several advanced industrial processes. Most industrial applications still refer to simple criteria, like statement and branch coverage, and consider complex criteria, like modified condition decision coverage, only rarely and often driven by the requirements of certification agencies. The industrial value of structural criteria is limited by the difficulty of achieving high coverage, due to both the complexity of deriving test cases that execute specific uncovered elements and the presence of many infeasible elements in the code. In this paper, we propose a technique that both generates test cases that execute yet uncovered branches and identifies infeasible branches that can be eliminated from the computation of the branch coverage\u00a0\u2026", "num_citations": "27\n", "authors": ["264"]}
{"title": "Analyzing refinements of state based specifications: the case of TB nets\n", "abstract": " We describe how formal specifications given in terms of a high-level timed Petri net formalism (TB nets) can be analyzed to check the temporal properties of bounded invariance (the systems stays in a given state until time \u03c4) and bounded response (the system will enter a given state within time \u03c4). In particular, we concentrate on specifications given in a hierarchical, top-down manner, where one specification level refines a more abstract level. Our goal is to define the conditions under which the properties that are proven to hold at a given abstraction level are preserved at the next refined level. To do so, we define the concept of correct refinement, and we show that bounded invariance and bounded response are preserved by a correct refinement. We also provide a set of constructive rules that may be applied to refine a net in such a way that the resulting net is a correct refinement.", "num_citations": "27\n", "authors": ["264"]}
{"title": "Reproducing concurrency failures from crash stacks\n", "abstract": " Reproducing field failures is the first essential step for understanding, localizing and removing faults. Reproducing concurrency field failures is hard due to the need of synthesizing a test code jointly with a thread interleaving that induce the failure in the presence of limited information from the field. Current techniques for reproducing concurrency failures focus on identifying failure-inducing interleavings, leaving largely open the problem of synthesizing the test code that manifests such interleavings. In this paper, we present ConCrash, a technique to automatically generate test codes that reproduce concurrency failures that violate thread-safety from crash stacks, which commonly summarize the conditions of field failures. ConCrash efficiently explores the huge space of possible test codes to identify a failure-inducing one by using a suitable set of search pruning strategies. Combined with existing techniques for\u00a0\u2026", "num_citations": "26\n", "authors": ["264"]}
{"title": "Validating timing requirements for time basic net specifications\n", "abstract": " This article deals with the validation of formal requirement specifications of real-time systems. Formally specified requirements can be validated by both direct execution and by proving properties at the specification level. We first discuss how the timing characteristics of a system can be specified by a formalism based on high-level Petri nets. Then we show how its temporal properties can be proven by means of a symbolic execution-based proof method for time reachability analysis. Depending on the complexity of the model of the specified system, the method may contain some undecidable steps and require interaction with the user. In many practical cases, however, the method can be performed mechanically and has acceptable response times.", "num_citations": "26\n", "authors": ["264"]}
{"title": "Efficient analysis of event processing applications\n", "abstract": " Complex event processing (CEP) middleware systems are increasingly adopted to implement distributed applications: they not only dispatch events across components, but also embed part of the application logic into declarative rules that detect situations of interest from the occurrence of specific pattern of events. While this approach simplifies the development of large scale event processing applications, writing the rules that correctly capture the application domain arguably remains a difficult and error prone task, which fundamentally lacks consolidated tool support.", "num_citations": "25\n", "authors": ["264"]}
{"title": "Automatic workarounds: Exploiting the intrinsic redundancy of web applications\n", "abstract": " Despite the best intentions, the competence, and the rigorous methods of designers and developers, software is often delivered and deployed with faults. To cope with imperfect software, researchers have proposed the concept of self-healing for software systems. The ambitious goal is to create software systems capable of detecting and responding \u201cautonomically\u201d to functional failures, or perhaps even preempting such failures, to maintain a correct functionality, possibly with acceptable degradation. We believe that self-healing can only be an expression of some form of redundancy, meaning that, to automatically fix a faulty behavior, the correct behavior must be already present somewhere, in some form, within the software system either explicitly or implicitly. One approach is to deliberately design and develop redundant systems, and in fact this kind of deliberate redundancy is the essential ingredient of many fault\u00a0\u2026", "num_citations": "25\n", "authors": ["264"]}
{"title": "Handling software faults with redundancy\n", "abstract": " Software engineering methods can increase the dependability of software systems, and yet some faults escape even the most rigorous and methodical development process. Therefore, to guarantee high levels of reliability in the presence of faults, software systems must be designed to reduce the impact of the failures caused by such faults, for example by deploying techniques to detect and compensate for erroneous runtime conditions. In this chapter, we focus on software techniques to handle software faults, and we survey several such techniques developed in the area of fault tolerance and more recently in the area of autonomic computing. Since practically all techniques exploit some form of redundancy, we consider the impact of redundancy on the software architecture, and we propose a taxonomy centered on the nature and use of redundancy in software systems. The primary utility of this taxonomy is\u00a0\u2026", "num_citations": "25\n", "authors": ["264"]}
{"title": "PLCTools: Graph transformation meets PLC design\n", "abstract": " This paper presents PLCTOOLS, a formal environment for designing and simulating programmable controllers. Control models are specified with IEC FED (Function Block Diagram), and translated into functionally equivalent HLTPNs (High-Level Timed Petri Nets), through MetaEnv, for analysis and simulation and obtained results are presented in terms of suitable animations of FED blocks.The peculiarity with FBD is that it does not come with a fixed set of syntactic elements; it allows users to add as many new blocks as they want. Consequently, each time users want to add a new FBD block with PLCTOOLS, they must provide the concrete syntax, to add it to the library of available blocks, but also the associated HLTPN, to allow MetaEnv to build the formal representation.", "num_citations": "25\n", "authors": ["264"]}
{"title": "JBSE: A symbolic executor for java programs with complex heap inputs\n", "abstract": " We present the Java Bytecode Symbolic Executor (JBSE), a symbolic executor for Java programs that operates on complex heap inputs. JBSE implements both the novel Heap EXploration Logic (HEX), a symbolic execution approach to deal with heap inputs, and the main state-of-the-art approaches that handle data structure constraints expressed as either executable programs (repOk methods) or declarative specifications. JBSE is the first symbolic executor specifically designed to deal with programs that operate on complex heap inputs, to experiment with the main state-of-the-art approaches, and to combine different decision procedures to explore possible synergies among approaches for handling symbolic data structures.", "num_citations": "24\n", "authors": ["264"]}
{"title": "Bidirectional symbolic analysis for effective branch testing\n", "abstract": " Structural coverage metrics, and in particular branch coverage, are popular approaches to measure the thoroughness of test suites. Unfortunately, the presence of elements that are not executable in the program under test and the difficulty of generating test cases for rare conditions impact on the effectiveness of the coverage obtained with current approaches. In this paper, we propose a new approach that combines symbolic execution and symbolic reachability analysis to improve the effectiveness of branch testing. Our approach embraces the ideal definition of branch coverage as the percentage of executable branches traversed with the test suite, and proposes a new bidirectional symbolic analysis for both testing rare execution conditions and eliminating infeasible branches from the set of test objectives. The approach is centered on a model of the analyzed execution space. The model identifies the frontier\u00a0\u2026", "num_citations": "24\n", "authors": ["264"]}
{"title": "Kriging-based self-adaptive cloud controllers\n", "abstract": " Cloud technology is rapidly substituting classic computing solutions, and challenges the community with new problems. In this paper we focus on controllers for cloud application elasticity, and propose a novel solution for self-adaptive cloud controllers based on Kriging models. Cloud controllers are application specific schedulers that allocate resources to applications running in the cloud, aiming to meet the quality of service requirements while optimizing the execution costs. General-purpose cloud resource schedulers provide sub-optimal solutions to the problem with respect to application-specific solutions that we call cloud controllers. In this paper we discuss a general way to design self-adaptive cloud controllers based on Kriging models. We present Kriging models, and show how they can be used for building efficient controllers thanks to their unique characteristics. We report experimental data that confirm\u00a0\u2026", "num_citations": "23\n", "authors": ["264"]}
{"title": "Recent advances in automatic black-box testing\n", "abstract": " Research in black-box testing has produced impressive results in the past 40 years, addressing many aspects of the problem that span from integration with the development process, to test case generation and execution. In the past few years, the research in this area has focused mostly on the automation of black-box approaches to improve applicability and scalability. This chapter surveys the recent advances in automatic black-box testing, covering contributions from 2010 to 2014, presenting the main research results and discussing the research trends.", "num_citations": "23\n", "authors": ["264"]}
{"title": "Assurance of self-adaptive controllers for the cloud\n", "abstract": " In this paper we discuss the assurance of self-adaptive controllers for the Cloud, and we propose a taxonomy of controllers based on the supported assurance level. Self-adaptive systems for the Cloud are commonly built by means of controllers that aim to guarantee the required quality of service while containing costs, through a careful allocation of resources. Controllers determine the allocation of resources at runtime, based on the inputs and the status of the system, and referring to some knowledge, usually represented as adaptation rules or models. Assuring the reliability of self-adaptive controllers account to assuring that the adaptation rules or models represent well the system evolution. In this paper, we identify different categories of control models based on the assurance approaches. We introduce two main dimensions that characterize control models. The dimensions refer to the flexibility and\u00a0\u2026", "num_citations": "23\n", "authors": ["264"]}
{"title": "Designing self-adaptive service-oriented applications\n", "abstract": " In this paper, we present a self-adaptive approach for service-oriented applications that combines novel techniques into a traditional sense-plan-act control loop, where the subject system is connected to a controller that in turn feeds commands back into the subject system. Our control loop works as follows: The invocation of a service triggers monitoring mechanisms. Such mechanisms identify changes in the invoked services that may depend on server-side implementation updates or dynamically discovered services.", "num_citations": "23\n", "authors": ["264"]}
{"title": "Ingegneria del software\n", "abstract": " Fornire i concetti relativi ai moderni processi di produzione del software, i principi dell\u2019ingegneria dei requisiti, delle architetture software, delle metodologie di progettazione. Illustrare le principali notazioni di specifica. Presentare metodi e strumenti per la verifica e la convalida.", "num_citations": "23\n", "authors": ["264"]}
{"title": "Self-test components for highly reconfigurable systems\n", "abstract": " Verification of component-based systems presents new challenges not yet completely addressed by existing testing techniques. This paper proposes a new approach for automatically testing highly reconfigurable component-based systems, i.e., systems that can be obtained by changing some components. The paper presents an industrial case that motivates our research and proposes a testing infrastructure that tracks run-time information for components. The collected information is used for automatic testing new versions of existing components and new configurations of existing systems.", "num_citations": "22\n", "authors": ["264"]}
{"title": "A toolbox for automating visual software engineering\n", "abstract": " Visual diagrammatic (VD) notations have always been widely used in software engineering. Such notations have been used to syntactically represent the structure of software systems, but they usually lack dynamic semantics, and thus provide limited support to software engineers. In contrast, formal models would provide rigorous semantics, but the scarce adaptability to different application domains precluded their large industrial application. Most attempts tried to formalize widely used VD notations by proposing a mapping to a formal model, but they all failed in addressing flexibility, that is, the key factor of the success of VD notations.               This paper presents MetaEnv, a toolbox for automating visual software engineering. MetaEnv augments VD notations with customizable dynamic semantics. Traditional meta-CASE tools support flexibility at syntactic level; MetaEnv augments them with semantic\u00a0\u2026", "num_citations": "22\n", "authors": ["264"]}
{"title": "Constructing multi-formalism state-space analysis tools: Using rules to specify dynamic semantics of models\n", "abstract": " State-space analysis techniques have been developed for several representations of concurrent systems, but each tool or technique has typically been targeted to a single design or program notation. We describe an approach to constructing multi-formalism state-space analysis tools for heterogeneous system descriptions, using a shared\" inframodel\" that represents only the essential information for interpretation by tool components that can be customized to reflect the semantics of each formalism. The (operational) semantics of each formalism, as well as interactions between components described in different formalisms, is described separately through rules governing enabling, matching, and firing of transitions. This results in more natural and compact internal representations, and more efficient analysis, than a purely translational approach.", "num_citations": "22\n", "authors": ["264"]}
{"title": "Symbolic execution of concurrent systems using petri nets\n", "abstract": " Techniques for analyzing sequential programs in order to improve their reliability have been widely studied in the past. Among the most interesting analysis techniques, we consider symbolic execution. However, analysis techniques for concurrent programs, and in particular symbolic execution, are still an open research area. In this paper, we define a method for symbolic execution of concurrent systems, based on an extension of the Petri net formalism, called EF nets. EF nets are a powerful, highly expressive and general formalism. Depending on the level of abstraction of actions and predicates that one associates to the transitions of the net, EF nets can be used as a high-level specification formalism for concurrent systems, or as a lower level internal representation of concurrent programs. Thus, the model is not dependent on a particular concurrent programming language, but it is flexible enough to be the kernel\u00a0\u2026", "num_citations": "21\n", "authors": ["264"]}
{"title": "Link: exploiting the web of data to generate test inputs\n", "abstract": " Applications that process complex data, such as maps, personal data, book information, travel data, etc., are becoming extremely common. Testing such applications is hard, because they require realistic and coherent test inputs that are expensive to generate manually and difficult to synthesize automatically. So far the research on test case generation techniques has focused mostly on generating test sequences and synthetic test inputs, and has payed little attention to the generation of complex test inputs.", "num_citations": "20\n", "authors": ["264"]}
{"title": "On the right objectives of data flow testing\n", "abstract": " This paper investigates the limits of current data flow testing approaches from a radically novel viewpoint, and shows that the static data flow techniques used so far in data flow testing to identify the test objectives fail to represent the universe of data flow relations entailed by a program. This paper compares the data flow relations computed with static data flow approaches with the ones observed while executing the program. To this end, the paper introduces a dynamic data flow technique that collects the data flow relations observed during testing. The experimental data discussed in the paper suggest that data flow testing based on static techniques misses many data flow test objectives, and indicate that the amount of missing objectives (false negatives) can be more limiting than the amount of infeasible data flow relations identified statically (false positives). This opens a new area of research of (dynamic) data\u00a0\u2026", "num_citations": "18\n", "authors": ["264"]}
{"title": "SHIWS: A self-healing integrator for web services\n", "abstract": " The integration of third-party Web services is challenged by the difficulty of keeping consistency between software systems that are maintained by different organizations and may evolve dynamically and independently, because of both changes in service implementation and dynamic discovery of new services. Self-adaptive applications have been recognized as viable solutions for dealing with systems where size and complexity increase beyond the ability of humans to respond manually, coherently and timely to environmental and system changes. In this paper, we propose a tool supported solution that exploits a self-adaptive approach, based on a mechanism for revealing possible runtime mismatches between requested and provided services, and for dynamically adapting the client application accordingly.", "num_citations": "18\n", "authors": ["264"]}
{"title": "A formal design notation for real-time systems\n", "abstract": " The development of real-time systems is based on a variety of different methods and notations. Despite the purported benefits of formal methods, informal techniques still play a predominant role in current industrial practice. Formal and informal methods have been combined in various ways to smoothly introduce formal methods in industrial practice. The combination of real-time structured analysis (SA-RT) with Petri nets is among the most popular approaches, but has been applied only to requirements specifications. This paper extends SA-RT to specifications of the detailed design of embedded real-time systems, and combines the proposed notation with Petri nets.", "num_citations": "18\n", "authors": ["264"]}
{"title": "Exception handlers for healing component-based systems\n", "abstract": " To design effective exception handlers, developers must predict at design time the exceptional events that may occur at runtime, and must implement the corresponding handlers on the basis of their predictions. Designing exception handlers for component-based software systems is particularly difficult because the information required to build handlers is distributed between component and application developers. Component developers know the internal details of the components but ignore the applications, while application developers own the applications but cannot access the details required to implement handlers in components. This article addresses the problem of automatically healing the infield failures that are caused by faulty integration of OTS components. In the article, we propose a technique and a methodology to decouple the tasks of component and application developers, who will be able to\u00a0\u2026", "num_citations": "17\n", "authors": ["264"]}
{"title": "Contextual integration testing of classes\n", "abstract": " This paper tackles the problem of structural integration testing of stateful classes. Previous work on structural testing of object-oriented software exploits data flow analysis to derive test requirements for class testing and defines contextual def-use associations to characterize inter-method relations. Non-contextual data flow testing of classes works well for unit testing, but not for integration testing, since it misses definitions and uses when properly encapsulated. Contextual data flow analysis approaches investigated so far either do not focus on state dependent behavior, or have limited applicability due to high complexity.               This paper proposes an efficient structural technique based on contextual data flow analysis to test state-dependent behavior of classes that aggregate other classes as part of their state.", "num_citations": "17\n", "authors": ["264"]}
{"title": "SLA protection models for virtualized data centers\n", "abstract": " Enterprise services must satisfy strong requirements that are coded in agreements with customers, commonly called service level agreements (SLA). To satisfy SLAs in critical conditions, conventional data centers are often greatly over-dimensioned, wasting resources and raising service costs. Virtualized data centers (VDC) provide an opportunity to significantly reduce over-dimensioning, and so reduce service costs without negatively affecting service agreements, through dynamic adaptation. In this paper, we discuss the problems involved in creating self-adaptive enterprise services in virtualized data centers, and we investigate solution strategies. We envision a set of models that help adaptation controllers to identify suitable reactions to changes in service level agreement and environmental execution conditions. We introduce models at different abstraction levels, to support the evaluation of the impacts of\u00a0\u2026", "num_citations": "16\n", "authors": ["264"]}
{"title": "An exploratory study of field failures\n", "abstract": " Field failures, that is, failures caused by faults that escape the testing phase leading to failures in the field, are unavoidable. Improving verification and validation activities before deployment can identify and timely remove many but not all faults, and users may still experience a number of annoying problems while using their software systems.This paper investigates the nature of field failures, to understand to what extent further improving in-house verification and validation activities can reduce the number of failures in the field, and frames the need of new approaches that operate in the field.We report the results of the analysis of the bug reports of five applications belonging to three different ecosystems, propose a taxonomy of field failures, and discuss the reasons why failures belonging to the identified classes cannot be detected at design time but shall be addressed at runtime. We observe that many faults (70\u00a0\u2026", "num_citations": "15\n", "authors": ["264"]}
{"title": "Automatic test case evolution\n", "abstract": " Software systems evolve incrementally both during and after development, and many test cases become obsolete while software evolves. Updating test suites in the context of software evolution is a complex and time consuming activity. This article focuses on the problem of updating test suites automatically, and identifies eight scenarios that allow either to repair test cases or to use test cases to generate new ones, and proposes eight test evolution algorithms that automatically repair and generate test cases by adapting existing ones. This article presents a framework, TestCareAssistant (TCA), that implements the algorithms to support the evolution of test suites written in Java. The framework has been extensively evaluated on five different open source projects where it has been applied to repair 138 broken test cases, and to generate the test cases for 727 new classes and 2462 new methods. The results obtained\u00a0\u2026", "num_citations": "15\n", "authors": ["264"]}
{"title": "Merlot: A tool for analysis of real-time specifications\n", "abstract": " Real-time systems are becoming increasingly important in the everyday life. The use of such systems for critical applications requires tools and techniques for increasing correctness and reliability of the final product. In this paper, we describe a toolset (Merlot) for analysing real-time system specifications. Merlot allows the automatic verification of temporal properties for a large set of specifications and requires interaction with the user only when the complexity of the specification overcomes a reasonable automatable level. Merlot has been built with the aim of verifying both the feasibility of the approach and the applicability of the analysis techniques to real problems, that is to identify classes of problems that can be analysed with little or no interaction with the user in a reasonable amount of time.< >", "num_citations": "15\n", "authors": ["264"]}
{"title": "SUSHI: a test generator for programs with complex structured inputs\n", "abstract": " Random and search-based test generators yield realistic test cases based on program APIs, but often miss structural test objectives that depend on non-trivial data structure instances; Whereas symbolic execution can precisely characterise those dependencies but does not compute method sequences to instantiate them. We present SUSHI, a high-coverage test case generator for programs with complex structured inputs. SUSHI leverages symbolic execution to generate path conditions that precisely describe the relationship between program paths and input data structures, and converts the path conditions into the fitness functions of search-based test generation problems. A solution for the search problem is a legal method sequence that instantiates the structured inputs to exercise the program paths identified by the path condition. Our experiments indicate that SUSHI can distinctively complement current\u00a0\u2026", "num_citations": "14\n", "authors": ["264"]}
{"title": "Datec: Contextual data flow testing of java classes\n", "abstract": " Many mature development processes use structural coverage metrics to monitor the quality of testing. Studies suggest that commonly used control flow testing criteria poorly address state-based behavior of object oriented software. This paper presents DaTeC, a tool that provides useful coverage information of Java object states by implementing a novel contextual data flow testing approach.", "num_citations": "14\n", "authors": ["264"]}
{"title": "Automatic generation of runtime failure detectors from property templates\n", "abstract": " Fine grained error or failure detection is often indispensable for precise, effective, and efficient reactions to runtime problems. In this chapter we describe an approach that facilitates automatic generation of efficient runtime detectors for relevant classes of functional problems. The technique targets failures that commonly manifest at the boundaries between the components that form the system. It employs a model-based specification language that developers use to capture system-level properties extracted from requirements specifications. These properties are automatically translated into assertion-like checks and inserted in all relevant locations of the systems code.               The main goals of our research are to define useful classes of system-level properties, identify errors and failures related to the violations of those properties, and produce assertions capable of detecting such violations. To this end we\u00a0\u2026", "num_citations": "14\n", "authors": ["264"]}
{"title": "A SOA based self-adaptive personal mobility manager\n", "abstract": " Service oriented applications integrate heterogenous Web services, which are deployed and maintained by different providers, and can evolve dynamically and autonomously. Autonomous changes of web services manifest only at run time, and may lead to unexpected failures. This paper proposes an approach to designing self-adaptive applications, which can react to changes in the implementation of services, thus avoiding unexpected failures. The architecture proposed in this paper automatically detects possible integration mismatches, and dynamically executes suitable adaptation strategies. The paper presents an application, the personal mobility manager, which integrates heterogeneous Web services, identifies failures that derive from dynamic changes in the integrated services, and illustrates the self-adaptive design solution which can prevent run time failures", "num_citations": "14\n", "authors": ["264"]}
{"title": "Integration testing of procedural object-oriented languages with polymorphism\n", "abstract": " Object oriented features like information hiding, inheritance, polymorphism, and dynamic binding, present new problems, that cannot be adequately solved with traditional testing approaches. In this paper we address the problem of integration testing of procedural object oriented systems in the presence of polymorphism. The kind of polymorphism addressed is inclusion polymorphism, as provided by languages like C++, Eiffel, CLOS, Ada95, and Java. This paper proposes a technique for testing combinations of polymorphic calls. The technique applies a data-flow approach to newly defined sets of polymorphic definitions and uses, ie, definitions and uses that depend on polymorphic calls. The proposed testing criteria allow for selecting execution paths that can reveal failures due to incorrect combinations of polymorphic calls. The paper describes the technique and illustrates its efficacy through an example.", "num_citations": "14\n", "authors": ["264"]}
{"title": "PLC programming languages: A formal approach\n", "abstract": " This paper introduces describes how to integrate standard editing and code generation functionalities offered by most tools supporting the IEC standard 1131-3 with capabilities for modelling and simulating the plant and its interactions with the digital controller. The 1131-3 notations (in particular Functional Block Diagrams) are complemented with differential equations that describe the behaviour of the plant and with an underlying formal model, which supports the analysis of functional and timing properties.", "num_citations": "14\n", "authors": ["264"]}
{"title": "Generation of multi-formalism state-space analysis tools\n", "abstract": " As software evolves from early architectural sketches to final code, a variety of representations are appropriate. Moreover, at most points in development, different portions of a software system are at different stages in development, and consequently in different representations. State-space analysis techniques (reachability analysis, model checking, simulation, etc.) have been developed for several representations of concurrent systems, but each tool or technique has typically been targeted to a single design or program notation. We describe an approach to constructing space analysis tools using a core set of basic representations and components. Such a tool generation approach differs from translation to a common formalism. We need not map every supported design formalism to a single internal form that completely captures the original semantics; rather, a shared\" inframodel\" represents only the essential\u00a0\u2026", "num_citations": "14\n", "authors": ["264"]}
{"title": "Automatic GUI testing of desktop applications: an empirical assessment of the state of the art\n", "abstract": " Testing software applications interacting with their graphical user interface, in short GUI testing, is both important, since it can reveal subtle and annoying bugs, and expensive, due to myriads of possible GUI interactions. Recent attempts to automate GUI testing have produced several techniques that address the problem from different perspectives, sometimes focusing only on some specific platforms, such as Android or Web, and sometimes targeting only some aspects of GUI testing, like test case generation or execution. Although GUI test case generation techniques for desktop applications were the first to be investigated, this area is still actively researched and its state of the art is continuously expanding.", "num_citations": "13\n", "authors": ["264"]}
{"title": "Heuristically matching solution spaces of arithmetic formulas to efficiently reuse solutions\n", "abstract": " Many symbolic program analysis techniques rely on SMT solvers to verify properties of programs. Despite the remarkable progress made in the development of such tools, SMT solvers still represent a main bottleneck to the scalability of these techniques. Recent approaches tackle this bottleneck by reusing solutions of formulas that recur during program analysis, thus reducing the number of queries to SMT solvers. Current approaches only reuse solutions across formulas that are equivalent to, contained in or implied by other formulas, as identified through a set of predefined rules, and cannot reuse solutions across formulas that differ in their structure, even if they share some potentially reusable solutions. In this paper, we propose a novel approach that can reuse solutions across formulas that share at least one solution, regardless of their structural resemblance. Our approach exploits a novel heuristic to efficiently\u00a0\u2026", "num_citations": "13\n", "authors": ["264"]}
{"title": "Adaptive runtime verification for autonomic communication infrastructures\n", "abstract": " Autonomic communication and autonomic computing can solve many problems in managing complex network and computer systems, as well as network applications, where computing and networking coexist. Autonomic applications must be able to diagnose and repair their own faults automatically. In particular, they must be able to monitor the execution state, understand the behavior of the application and of the executing environment, and interpret monitored data to identify faults and select a repairing strategy. Assertions have been extensively studied in software engineering for identifying deviations from the expected behaviors and thus signal anomalous outcomes. Unfortunately, classic assertions are defined statically at development time and cannot capture unpredictable changes and evolutions in the execution environment. Thus, they do not easily adapt to autonomic applications. The paper proposes a\u00a0\u2026", "num_citations": "13\n", "authors": ["264"]}
{"title": "LEMMA: a language for easy medical models analysis\n", "abstract": " The quality of health care systems and processes is becoming a prominent problem and more and more efforts are devoted to define methodologies and tools to measure and assure quality of care. New methods are required to optimize health care processes to guarantee high quality standards within (limited) available resources. Resource optimizations able to preserve the quality of treatments require good models of medical processes. This paper presents LEMMA, a new notation to model medical processes. LEMMA provides physicians with intuitive graphical elements to design their models. At the same time a high level timed Petri net corresponding to the designed model is built automatically. In this way, LEMMA models are ascribed formal semantics and can be executed and analyzed automatically. The dual language approach followed in this paper allows physicians to gain all the\u00a0\u2026", "num_citations": "13\n", "authors": ["264"]}
{"title": "A tool for analysing high-level timed Petri nets\n", "abstract": " The IPTES toolset and methodology have been developed for supporting speci cations, design and implementation of real time systems. Such systems are often used in safety critical applications and may thus require intensive testing and analysis before being released for their nal use. The IPTES toolset provides analysis mechanisms based on execution and animation of speci cations. Such capabilities may be enough for extensive analysis of many hard real-time systems. However, in some cases, part of the system may require additional analysis. New techniques for the analysis of temporal properties based on the IPTES formal kernel model (High-Level Timed Petri Nets) have been studied and developed within the IPTES project. This report describes a rst prototype that automatically proofs temporal properties for High-Level Timed Petri Nets. It includes the description of the main design issued, an end-user manual and an experience report that describes the use of the prototype on some initial examples.", "num_citations": "13\n", "authors": ["264"]}
{"title": "Effectiveness and challenges in generating concurrent tests for thread-safe classes\n", "abstract": " Developing correct and efficient concurrent programs is difficult and error-prone, due to the complexity of thread synchronization. Often, developers alleviate such problem by relying on thread-safe classes, which encapsulate most synchronization-related challenges. Thus, testing such classes is crucial to ensure the reliability of the concurrency aspects of programs. Some recent techniques and corresponding tools tackle the problem of testing thread-safe classes by automatically generating concurrent tests. In this paper, we present a comprehensive study of the state-of-the-art techniques and an independent empirical evaluation of the publicly available tools. We conducted the study by executing all tools on the JaConTeBe benchmark that contains 47 well-documented concurrency faults. Our results show that 8 out of 47 faults (17%) were detected by at least one tool. By studying the issues of the tools and the\u00a0\u2026", "num_citations": "12\n", "authors": ["264"]}
{"title": "Test-and-adapt: An approach for improving service interchangeability\n", "abstract": " Service-oriented applications do not fully benefit from standard APIs yet, and many applications fail to use interchangeably all the services that implement a standard service API. This article presents an approach to develop adaptation strategies that improve service interchangeability for service-oriented applications based on standard APIs. In our approach, an adaptation strategy consists of sets of parametric adaptation plans (called test-and-adapt plans), which execute test cases to reveal the occurrence of interchangeability problems, and activate runtime adaptors according to the test results. Throughout this article, we formalize the structure of the parametric test-and-adapt plans and of their execution semantics, present an algorithm for identifying correct execution orders through sets of test-and-adapt plans, provide empirical evidence of the occurrence of interchangeability problems for sample applications and\u00a0\u2026", "num_citations": "12\n", "authors": ["264"]}
{"title": "Iterative model-driven development of adaptable service-based applications\n", "abstract": " Flexibility and interoperability make web services well suited for designing highly-customizable reactive service-based applications, that is interactive applications that can be rapidly adapted to new requirements and environmental conditions. This is the case, for example of personal data managers that many users tailor to their needs to meet different usage conditions and requests.", "num_citations": "12\n", "authors": ["264"]}
{"title": "Adaptive integration of third-party web services\n", "abstract": " Service based computing allows clients to dynamically bind services, and providers to modify the service implementation independently from their clients. The impossibility of statically determining which service implementation will be bound at runtime may lead to unexpected client-side failures. This position paper suggests a scenario in which service-based applications autonomously react to changes in the implementation of the used services, automatically detect possible integration mismatches, and dynamically execute suitable adaptation strategies. The proposed solution exploits ideas from autonomic computing and self-managed software. We propose a design methodology based on the definition of both test cases, to automatically diagnose service mismatches, and adaptation strategies, to overcome the revealed problems. We introduce a general runtime infrastructure that automatically embeds the test\u00a0\u2026", "num_citations": "12\n", "authors": ["264"]}
{"title": "Quantifying the complexity of dataflow testing\n", "abstract": " It is common belief that dataflow testing criteria are harder to satisfy than statement and branch coverage. As motivations, several researchers indicate the difficulty of finding test suites that exercise many dataflow relations and the increased impact of infeasible program paths on the maximum coverage rates that can be indeed obtained. Yet, although some examples are given in research papers, we lack data on the validity of these hypotheses. This paper presents an experiment with a large sample of object oriented classes and provides solid empirical evidence that dataflow coverage rates are steadily lower than statement and branch coverage rates, and that the uncovered dataflow elements do not generally depend on the feasibility of single statements.", "num_citations": "11\n", "authors": ["264"]}
{"title": "Graph transformations and software engineering: Success stories and lost chances\n", "abstract": " Textual as well as visual and diagrammatic notations are essential in software engineering, and are used in many different contexts. Chomsky grammars are the key tool to handle textual notations, and find many applications for textual languages. Visual and diagrammatic languages add spatial dimensions that reduce the applicability of textual grammars and call for new tools.Graph transformation systems have been studied for over 40 years and are a powerful tool to deal with syntax, semantics and transformation of diagrammatic notations. The enormous importance of visual and diagrammatic languages and the strong support that graph transformation provide to the manipulation of diagrammatic notations would suggest a big success of graph transformation in software engineering.Graph transformation systems find their application both as language generating devices and specification means for system\u00a0\u2026", "num_citations": "10\n", "authors": ["264"]}
{"title": "Protecting SLAs with surrogate models\n", "abstract": " In this paper, we propose the use of surrogate models to avoid or limit violations of the service level agreements (protect SLAs) of enterprise applications executed within virtualized data centers (VDCs).", "num_citations": "10\n", "authors": ["264"]}
{"title": "A toolset for automated failure analysis\n", "abstract": " Classic fault localization techniques can automatically provide information about the suspicious code blocks that are likely responsible for observed failures. This information is useful, but not sufficient to completely understand the causes of failing executions, which still require further (time-consuming) investigations to be exactly identified. A useful and comprehensive source of information is frequently given by the set of unexpected events that have been observed during failures. Sequences of unexpected events are usually simple to be interpret, and testers can guess the expected correct sequences of events from the faulty sequences. In this paper, we present a tool that automatically identifies anomalous events that likely caused failures, filters the possible false positives, and presents the resulting data by building views that show chains of cause-effect relations, i.e., views that show when anomalous events are\u00a0\u2026", "num_citations": "10\n", "authors": ["264"]}
{"title": "A formal framework for developing adaptable service-based applications\n", "abstract": " Web services are open, interoperable, easy to integrate and reuse, and are extensively used in many application domains. Research and best practices have produced excellent support for developing large-scale web-based applications implementing complex business processes.               Flexibility and interoperability of web services make them well suited also for highly-customizable reactive service-based applications, that is interactive applications which serve few users, and can be rapidly adapted to new requirements and environmental conditions. This is the case, for example of personal data managers tailored to the needs of few specific users who want to adapt them to different conditions and requests. Classic development approaches that require experts of web service technologies do not well support this class of applications which call for rapid individual customization and adaptation by non\u00a0\u2026", "num_citations": "10\n", "authors": ["264"]}
{"title": "Validation of concurrent Ada\u2122 programs using symbolic execution\n", "abstract": " Symbolic execution is a well known technique for analyzing sequential programs. It has a set of important applications: it can be used for verifying the correctness of a particular path for all the input data which cause the execution of that path. It can support testing tools identifying the constraints that characterize the set of data which exercize a particular execution path. With suitable assertions it can provide a verification mechanism. Finally, it can be used as a basis for a documentation tool.             In this paper we propose an extension of sequential symbolic execution for Ada tasking. A net based formalism, EF net, is used for representing the Ada task system. EF nets are suitable for representing all the aspects of Ada tasking, except for time related commands, which are not considered in this paper. Two symbolic execution algorithms are then defined on EF nets. The first one, called SEA, suitable for the\u00a0\u2026", "num_citations": "10\n", "authors": ["264"]}
{"title": "SEIM: static extraction of interaction models\n", "abstract": " The quality of systems that integrate Web services provided by independent organizations depends on the ways the systems interact with the services, ie, on their interaction protocols, which are not always easy to deduce by inspecting the code. Accurate models of interaction protocols provide a comprehensive view of the interactions, and support manual and automatic analysis of corner cases that are often difficult to discover, and are responsible from many subtle failures.", "num_citations": "9\n", "authors": ["264"]}
{"title": "Self-healing strategies for component integration faults\n", "abstract": " Software systems increasingly integrate Off-The-Shelf (OTS) components. However, due to the lack of knowledge about the reused OTS components, this integration is fragile and can cause in the field a lot of failures that result in dramatic consequences for users and service providers, e.g. loss of data, functionalities, money and reputation. As a consequence, dynamic and automatic fixing of integration problems in systems that include OTS components can be extremely beneficial to increase their reliability and mitigate these risks. In this paper, we present a technique for enhancing component-based systems with capabilities to self-heal common integration faults by using a predetermined set of healing strategies. The set of faults that can be healed has been determined from the analysis of the most frequent integration bugs experienced by users according to data in bug repositories available on Internet. An\u00a0\u2026", "num_citations": "9\n", "authors": ["264"]}
{"title": "Adaptive REST applications via model inference and probabilistic model checking\n", "abstract": " In this paper we present a novel approach for adaptive REST Web applications that focuses on adaptation against changes in the navigational behaviour of users. The proposed solution exploits the Web server's log file to infer a Markov model that captures the navigational behaviour of system users over time probabilistically. The model is inferred incrementally as soon as new requests are issued to the server, and is analysed periodically to verify quantitative properties by means of probabilistic model checking. The results of the run-time verification trigger ad-hoc adaptation policies, which adjust the application to the user behaviours captured by the inferred model. The paper discusses the advantages of adopting probabilistic model checking for Web applications in terms of incrementality, retroactivity and efficiency, and illustrates these characteristics as well as the applicability of the approach with a practical\u00a0\u2026", "num_citations": "8\n", "authors": ["264"]}
{"title": "RAW: runtime automatic workarounds\n", "abstract": " Faults in Web APIs may escape the testing process, and therefore affect thousands of Web applications. As a consequence, users of these applications might suffer from related failures for a long time until proper fixes are released by the Web API developers. In this paper we present RAW, a tool that tries to find workarounds automatically and at runtime, thereby reducing the negative impact of faults in Web applications. Runtime and automatically deployed workarounds serve as a temporary relief for application users while proper fixes are developed and released.", "num_citations": "8\n", "authors": ["264"]}
{"title": "Property decomposition to speed up analysis\n", "abstract": " The complexity and criticity of many real time systems suggest that the temporal reachability analysis technique can find a role in the industrial development of such systems. Unfortunately, the costs of time reachability analysis inhibits the systematic application of this kind of analysis in the early stages of development, when long verification sessions slow down the development process. Moreover, the large reachability space built for proving temporal properties reduces the size of specification for which temporal reachability analysis can be applied. We show how some easily accessible information about the system to be developed and the required properties can largely reduce in many interesting cases the size of the reachability space to be explored. In this way temporal reachability analysis can be used for verifying fairly large specifications and can be applied at the early stages of development without\u00a0\u2026", "num_citations": "8\n", "authors": ["264"]}
{"title": "Verifying LTL properties of bytecode with symbolic execution\n", "abstract": " Bytecode languages are at a very desirable degree of abstraction for performing formal analysis of programs, but at the same time pose new challenges when compared with traditional languages. This paper proposes a methodology for bytecode analysis which harmonizes two well-known formal verification techniques, model checking and symbolic execution. Model checking is a property-guided exploration of the system state space until the property is proved or disproved, producing in the latter case a counterexample execution trace. Symbolic execution emulates program execution by replacing concrete variable values with symbolic ones, so that the symbolic execution along a path represents the potentially infinite numeric executions that may occur along that path. We propose an approach where symbolic execution is used for building a possibly partial model of the program state space, and on-the-fly model checking is exploited for verifying temporal properties on it. The synergy of the two techniques yields considerable potential advantages: symbolic execution allows for modeling the state space of infinite-state software systems, limits the state explosion, and fosters modular verification; model checking provides fully automated verification of reachability properties of a program. To assess these potential advantages, we report our preliminary experience with the analysis of a safety-critical software system.", "num_citations": "7\n", "authors": ["264"]}
{"title": "Behavior capture and test for controlling the quality of component-based integrated systems\n", "abstract": " Complex software systems are seldom designed from scratch; rather they are often designed by assembling basic components. Software tools are not an exception: They are often obtained by assembling simple tools that provide basic functionality. From the verification viewpoint, basic tools do not differ from components: in both cases they can be seen as black box elements that provide a set of services for the embedding system or tool. Complex systems and tools are often provided in many different versions and configurations that are obtained by adding and replacing various components. Let us consider for example a complex CASE tool that runs on several platforms and is distributed in various versions for different user profiles. Many graphics and system components may be used for distinct platforms. For instance, the simple configurations distributed free-of-charge may include only a simple set of basic tools, while the educational and the professional editions may include a wider set of basic tools that provide extended functionality. Unfortunately, behavioral differences among components may cause subtile failures difficult to reveal and remove. On of the main goals of verification is to check the completeness and compatibility of the services provided by the components to reveal possible conflicts, thus supporting efficient verification of different tool configurations and versions.The use of components in running systems produces a lot of useful information about the components\u2019 behavior that could be used to check for the compatibility between different components and between components and embedding systems. Unfortunately this\u00a0\u2026", "num_citations": "7\n", "authors": ["264"]}
{"title": "A practical approach to formal design of real-time systems\n", "abstract": " Formal methods are being increasingly used in engineering industrial software. They are mostly used for specifying and verifying software requirements, but seldom in later development phases. This paper tries to bridge the gap between formal requirements specification and final code by introducing a formally defined design notation. The proposed design notation extends structured analysis specification notations with constructs derived from POSIX real-time extensions. The design notation proposed in this paper is formally defined by means of high-level timed Petri nets, and can be formally analyzed using tools and techniques available for Petri nets. Automatic tools for editing and verifying design specifications given in terms of the notation proposed in this paper have been implemented and the notation has been successfully validated on industrial case-studies.", "num_citations": "7\n", "authors": ["264"]}
{"title": "Measuring software testability modulo test quality\n", "abstract": " Comprehending the degree to which software components support testing is important to accurately schedule testing activities, train developers, and plan effective refactoring actions. Software testability estimates such property by relating code characteristics to the test effort. The main studies of testability reported in the literature investigate the relation between class metrics and test effort in terms of the size and complexity of the associated test suites. They report a moderate correlation of some class metrics to test-effort metrics, but suffer from two main limitations:(i) the results hardly generalize due to the small empirical evidence (datasets with no more than eight software projects); and (ii) mostly ignore the quality of the tests. However, considering the quality of the tests is important. Indeed, a class may have a low test effort because the associated tests are of poor quality, and not because the class is easier to test\u00a0\u2026", "num_citations": "6\n", "authors": ["264"]}
{"title": "Energy-based anomaly detection a new perspective for predicting software failures\n", "abstract": " The ability of predicting failures before their occurrence is a fundamental enabler for reducing field failures and improving the reliability of complex software systems. Recent research proposes many techniques to detect anomalous values of system metrics, and demonstrates that collective anomalies are a good symptom of failure-prone states. In this paper (i) we observe the analogy of complex software systems with multi-particle and network systems, (ii) propose to use energy-based models commonly exploited in physics and statistical mechanics to precisely reveal failure-prone behaviors without training with seeded errors, and (iii) present some preliminary experimental results that show the feasibility of our approach.", "num_citations": "6\n", "authors": ["264"]}
{"title": "An RBM anomaly detector for the cloud\n", "abstract": " Failures are unavoidable in complex software systems, and the intrinsic characteristics of cloud systems amplify the problem. Predicting failures before their occurrence by detecting anomalies in system metrics is a viable solution to enable failure preventing or mitigating actions. The most promising approaches for predicting failures exploit statistical analysis or machine learning to reveal anomalies and their correlation with possible failures. Statistical analysis approaches result in far too many false positives, which severely hinder their practical applicability, while accurate machine learning approaches need extensive training with seeded faults, which is often impossible in operative cloud systems. In this paper, we propose EmBeD, Energy-Based anomaly Detection in the cloud, an approach to detect anomalies at runtime based on the free energy of a Restricted Boltzmann Machine (RBM) model. The free energy\u00a0\u2026", "num_citations": "6\n", "authors": ["264"]}
{"title": "From graph transformation to software engineering and back\n", "abstract": " Software engineers usually represent problems and solutions using graph-based notations at different levels of abstractions. These notations are often semi-formal, but the use of graph transformation techniques can support reasoning about graphs in many ways, and thus can largely enhance them.               Recent work indicates many applications of graph transformation to software engineering and opens new research directions. This paper aims primarily at illustrating how graph transformation can help software engineers, but it also discusses how software engineering can ameliorate the practical application of graph transformation technology and its supporting tools.", "num_citations": "6\n", "authors": ["264"]}
{"title": "Towards extensible graphical formalisms\n", "abstract": " Discusses how to tailor a graphical notation on top of a kernel formal (graphical) specification language. The goal is to allow an environment supporting formal specifications written in a kernel formal notation to be extended to support additional, application domain-oriented graphical notations. The semantics of the newly defined notation is given by a translation scheme into the kernel notation. Our approach is founded on high-level Petri nets, the kernel formalism, and graph grammars, which define the new graphical notations and their translation into target nets. The paper provides examples of the definition of Statecharts on top of the kernel formalism.< >", "num_citations": "6\n", "authors": ["264"]}
{"title": "Predicting failures in multi-tier distributed systems\n", "abstract": " Many applications are implemented as multi-tier software systems, and are executed on distributed infrastructures, like cloud infrastructures, to benefit from the cost reduction that derives from dynamically allocating resources on-demand. In these systems, failures are becoming the norm rather than the exception, and predicting their occurrence, as well as locating the responsible faults, are essential enablers of preventive and corrective actions that can mitigate the impact of failures, and significantly improve the dependability of the systems. Current failure prediction approaches suffer either from false positives or limited accuracy, and do not produce enough information to effectively locate the responsible faults.In this paper, we present PreMiSE, a lightweight and precise approach to predict failures and locate the corresponding faults in multi-tier distributed systems. PreMiSE\u00a0blends anomaly-based and signature\u00a0\u2026", "num_citations": "5\n", "authors": ["264"]}
{"title": "Model-driven generation of runtime checks for system properties\n", "abstract": " Creating runtime monitors for interesting properties is an important research problem. Existing approaches to runtime verification require specifications that not only define the property to monitor, but also contain details of the implementation, sometimes even requiring the implementation to add special variables or methods for monitoring. Often intuitive properties such as \u201cevent X should only happen when objects A and B agree\u201d have to be translated by developers into complex specifications, for example, pre- and post-conditions on several methods that only in concert express this simple property. In most specification languages, the result of this manual translation are specifications that are so strongly tailored to the program at hand and the objects involved that, even if the property occurs again in a similar program, the whole translation process has to be repeated to create a new specification. In this\u00a0\u2026", "num_citations": "5\n", "authors": ["264"]}
{"title": "Can Graph Grammars Make Formal Methods More Human?\n", "abstract": " Formal methods are scarcely used in industrial applications. Industrial strength tools and educational effort do not significantly help in promoting formal methods. Main obstacles to the industrial application of formal methods are lack of flexibility, lack of specialization, and difficulties in interpreting analysis results. This paper argues that graph grammars can help overcoming such limitations. The paper suggests that translation rules based on graph grammars are a natural merging of formalizations based on graph grammars and rule based approaches; thus it indicates a challenging practical application domain for graph grammars.", "num_citations": "5\n", "authors": ["264"]}
{"title": "Software design of robot controllers with petri nets: a case-study\n", "abstract": " Traditional techniques for designing complex real-time control software offer limited support for validating design before producing a final code. The need of early validation of the design of control software is increasing, due to the growing complexity of the software components and the strict timing requirements of run time control algorithms. The use of formal methods may greatly improve the design specification and validation processes. This paper presents a case-study that illustrates an approach for designing real-time control software with a specialized notation, formally defined by means of Petri nets. The formal approach is evaluated by comparison with a traditional approach based on the structured analysis.", "num_citations": "5\n", "authors": ["264"]}
{"title": "Customizable notations for kernel formalisms\n", "abstract": " Rigorous formal methods and intuitive graphical notations can greatly enhance the development of complex computer systems. Formal methods guarantee non-ambiguity and support powerful analysis techniques. Intuitive graphical notations facilitate the communications between engineers preventing errors due to misunderstandings. Unfortunately, tools and techniques based on formal methods do not usually support adequate graphical notations; while tools and methods based on powerful graphical notations often lack formal foundations. This paper proposes a technique that allows kernel formalisms to be accessed through powerful graphical notations. The proposed technique allows graphical notations to be tailored to the needs of the specific application domain. This paper focuses on the tool support.", "num_citations": "5\n", "authors": ["264"]}
{"title": "Formal specification and timing analysis of high-integrity real-time systems\n", "abstract": " We motivate the need for formal specification and verification of specifications in the case of high-integrity real-time systems. After a review of sample approaches, we concentrate the attention on Petri nets augmented to support timing requirements. We illustrate a very general formalism and an associated timing analysis procedure. We then illustrate a restriction of the general model and a specialized analysis procedure. Concepts are mainly illustrated via examples and informal descriptions. The reader is directed to the published literature for formal details.", "num_citations": "5\n", "authors": ["264"]}
{"title": "On the role of software reliability in software engineering\n", "abstract": " We place reliability in the context of other relevant software qualities and try to define it rigorously. Then we discuss two complementary approaches to reliability: the constructive approach, which tries to produce a-priori reliable software, and the analytic approach, which tries to measure reliability by inspecting software a-posteriori, after its development.             The paper reviews two relevant technologies that may provide a constructive contribution to improving software reliability: formal specifications and programming languages. Although our emphasis is on constructive approaches, we briefly review the principles and techniques of software validation, that can be used to check software reliability after development.", "num_citations": "5\n", "authors": ["264"]}
{"title": "Petri nets as a support to symbolic execution of concurrent Ada programs\n", "abstract": " In order to provide a basis for a general testing and verification environment, a model of concurrency based on the Petri nets formalism, and a translation algorithm from a subset of the Ada task system to the proposed formalism are given. Symbolic execution of concurrent programs is discussed, with reference to the considered Ada subset, on the basis of the proposed formalism. Problems due to scheduling and synchronization are considered, and solutions are proposed. The algorithm provides enough information for test data selection, allows the reproducible execution of a concurrent program, and can detect unfeasible rendezvous sequences. (ESA)", "num_citations": "5\n", "authors": ["264"]}
{"title": "Voice and data performance measurements in L-express net\n", "abstract": " L-Express is a protocol for Local Area Networks based on a single bus topology. It utilizes a simple and efficient virtual token access scheme which provides ordered and collision-free transmission.", "num_citations": "5\n", "authors": ["264"]}
{"title": "Mining finitestate automata with annotations\n", "abstract": " Many software design and verification techniques assume the availability of some kind of behavioral models of the systems under analysis [2, 4, 10, 11, 28]. Unfortunately, manually specifying and maintaining behavioral models is expensive and error prone, and requires specific skills that are not always available in development teams. This reduces the applicability of model-based approaches in industrial projects. The problem of generating behavioral models can be solved with techniques that automatically generate behavioral models by mining program executions and thus reducing the effort required to generate models [3, 5, 18, 20, 24].Many of these techniques generate finite state automata (FSA) that model the relations between sequences of events, but do not capture information about the attributes that are associated with the events, like parameters of method calls, thus missing details that may be\u00a0\u2026", "num_citations": "4\n", "authors": ["264"]}
{"title": "Inference of behavioral models that support program analysis\n", "abstract": " The use of models to study the behavior of systems is common to all fields: from wind-tunnel to Navier-Stokes equations to circuit diagrams to finite models of buildings, engineers in all disciplines construct and analyze models [68]. A behavioral model formalizes and abstracts the view of a system and gives insight about the behavior of the system being developed. In the software field, behavioral models can support software engineering tasks. In particular, models that represent the behavior of the program during its execution can be used to reason about questions like:\u201cWhat did happen during program executions?, How program should have behave?, What will happen afterwards?\u201d. Relevant uses of behavioral models are included in all the main analysis and testing activities: models are used in program comprehension to complement the information available in specifications, are used in testing to ease test case generation, used as oracles to verify the correctness of the executions, and are used as failure detection to automatically identify anomalous behaviors. Unfortunately, it is extremely effort demanding to produce and maintain behavioral models. Fortunately, when behavioral models are not part of specifications, automated approaches can automatically derive behavioral models from programs. The degree of completeness and soundness of the generated models depends from the kind of inferred model and the quality of the data available for the inference. When model inference techniques do not work well or the data available for the inference are poor, the many testing and analysis techniques based on these models will necessarily\u00a0\u2026", "num_citations": "4\n", "authors": ["264"]}
{"title": "Toward deeply adaptive societies of digital systems\n", "abstract": " Modern societies are pervaded by computerized, heterogeneous devices designed for specific purposes, but also more and more often capable of interacting with other devices for entirely different purposes. For example, a cell phone could be used to purchase a train ticket on-line that could later be printed by a vending machine at the train station. This type of open environment is what we call a society of digital systems. In this paper, we outline the characteristics of societies of digital systems, and argue that they call for a new approach to cope with unforeseen interactions, possible incompatibilities, failures, and emergent behaviors. We argue that designers can not assume a closed or homogeneous world, and must instead naturally accommodate dynamic adaptations. Furthermore, self-adaptability, that is, the ability to adapt autonomically to a changing environment, also poses problems, as different adaptation\u00a0\u2026", "num_citations": "4\n", "authors": ["264"]}
{"title": "Petri nets as semantic domain for diagram notations\n", "abstract": " This paper summarizes the work carried out by the authors during the last years. It proposes an approach for defining extensible and flexible formal interpreters for diagram notations based on high-level timed Petri nets.The approach defines interpreters by means of two sets of rules. The first set specifies the correspondences between the elements of the diagram notation and those of the semantic domain (Petri nets); the second set transforms events and states of the semantic domain into visual annotations on the elements of the diagram notation. The feasibility of the approach is demonstrated through MetaEnv, a prototype tool that allows users to implement special-purpose interpreters.", "num_citations": "4\n", "authors": ["264"]}
{"title": "Fundamental Approaches to Software Engineering: 6th International Conference, FASE 2003, Held as Part of the Joint European Conferences on Theory and Practice of Software\u00a0\u2026\n", "abstract": " This book constitutesnbsp; the refereed proceedings of the 6th International Conference on Fundamental Approaches to Software Engineering, FASE 2003, held in Warsaw, Poland, in April 2003. The 20 revised full papers presented together with a keynote paper were carefully reviewed and selected from 89 submissions. The papers are organized in topical sections on software components, mobile computing, aspects and web applications, software measurements, formal verficiation, analysis and testing, and model integration and extension.", "num_citations": "4\n", "authors": ["264"]}
{"title": "A New Timed petri Net Model for Hardware Representation\n", "abstract": " The use of modified Petri nets for design representation is proposed. Since Petri nets in their basic form show only the causal dependencies among the events, but do not provide any concept of time, we resort to a high-level net model called Environment/Relationship (ER) net which is suitable for a unified representation and manipulation of time and data. The use of ER nets in hardware description is demonstrated by means of a simple but significant example: this allows the comparison of ER nets with other methods used to this purpose. In particular ER nets are able to remove all the ambiguities that intrinsically arise in other techniques. On the basis of such case study, general rules for mapping hardware specification languages into ER nets are provided.", "num_citations": "4\n", "authors": ["264"]}
{"title": "A net-based model for time-dependent systems\n", "abstract": " A high level Petri net based model for dealing with time dependent systems, real time and stochastic time systems is defined. The basic model is called Environment Relation nets (ER) in which the information bearing environment entities are used as tokens. The most significant time oriented extensions of Petri nets, both stochastic and nonstochastic, proposed in the literature are shown to be particular cases of ER nets, thus assessing the generality of this model. (ESA)", "num_citations": "4\n", "authors": ["264"]}
{"title": "Coverage-driven test generation for thread-safe classes via parallel and conflict dependencies\n", "abstract": " Thread-safe classes are common in concurrent object-oriented programs. Testing such classes is important to ensure the reliability of the concurrent programs that rely on them. Recently, researchers have proposed the automated generation of concurrent (multi-threaded) tests to expose concurrency faults in thread-safe classes (thread-safety violations). However, generating fault-revealing concurrent tests within an affordable time-budget is difficult due to the huge search space of possible concurrent tests. In this paper, we present DepCon, an approach to effectively reduce the search space of concurrent tests by means of both parallel and conflict dependency analyses. DepCon is based on the intuition that only methods that can both interleave (parallel dependent) and access the same shared memory locations (conflict dependent) can lead to thread-safety violations when concurrently executed. DepCon\u00a0\u2026", "num_citations": "3\n", "authors": ["264"]}
{"title": "Reusing solutions modulo theories\n", "abstract": " In this paper we propose an approach for reusing formula solutions to reduce the impact of Satisfiability Modulo Theories (SMT) solvers on the scalability of symbolic program analysis. SMT solvers can efficiently handle huge expressions in relevant logic theories, but they still represent a main bottleneck to the scalability of symbolic analyses, like symbolic execution and symbolic model checking. Reusing proofs of formulas solved during former analysis sessions can reduce the amount of invocations of SMT solvers, thus mitigating the impact of SMT solvers on symbolic program analysis. Early approaches to reuse formula solutions exploit equivalence and inclusion relations among structurally similar formulas, and are strongly tighten to the specific target logics. In this paper, we present an original approach that reuses both satisfiability and unsatisfiability proofs shared among many formulas beyond only equivalent\u00a0\u2026", "num_citations": "3\n", "authors": ["264"]}
{"title": "Improving interaction with services via probabilistic piggybacking\n", "abstract": " Modern service oriented applications increasingly include publicly released services that impose novel and compelling requirements in terms of scalability and support to clients with limited capabilities such as mobile applications. To meet these requirements, service oriented applications require a careful optimisation of their provisioning mechanisms. In this paper we investigate a novel technique that optimises the interactions between providers and clients called probabilistic piggybacking. In our approach we automatically infer a probabilistic model that captures the behaviour of clients and predicts the future service requests. The provider exploits this information by piggybacking each message toward clients with the response of the predicted next request, minimizing both the amount of exchanged messages and the client latency. The paper focuses on REST services and illustrates the technique with a\u00a0\u2026", "num_citations": "3\n", "authors": ["264"]}
{"title": "Design for testability for highly reconfigurable component-based systems\n", "abstract": " Highly-reconfigurable component-based systems, i.e., systems that are built form existing components and are distributed in many versions and configurations, are becoming increasingly popular.The design and verification of such systems presents new challenges. In this paper we propose a design approach that facilitates analysis and testing of different configurations by identifying and tracking relations among requirements, logic components and resources. The approach proposed in the paper allows for easily identifying different dependencies among resources, components and requirements and thus spotting the tests that must be re-executed to assure the desired level of quality.", "num_citations": "3\n", "authors": ["264"]}
{"title": "Hierarchical Decomposition of High Level Timed Petri Nets\n", "abstract": " In IPTES, real-time system speci cations expressed in SA/SD-RT are internally represented by means of High Level Timed Petri Nets (HLTPNs). Petri nets present several widely recognized advantages, however their usefulness is limited by the absence of hierarchical decomposition mechanisms.The absence of hierarchical decomposition mechanisms has at least three main consequences on the usability of Petri nets for the speci cations of real size systems: fast growth of the speci cations that become quickly unreadable: impossibility of mapping the hierarchical aspects of speci cations expressed in a hierarchical language (eg SA/SD-RT) directly onto Petri nets; no support for reducing the analysis e ort by means of divide and conquer strategies based on a suitable hierarchy. This report presents a formal hierarchical decomposition mechanism for HLTPNs, that it can enhance existing analysis techniques, support mapping from hierarchical speci cation languages, and improve the readability of large speci cations. This report de nes the conditions under which the properties that are proven to hold at a given abstraction level are preserved at the next re ned level. To do so, we de ne the concept of correct re nement, and we show that interesting temporal properties are preserved by correct re nements. We also provide a set of constructive rules that may be applied to re ne a net in such a way that the resulting net is a correct re nement. Finally the report sketches the main features of a rst prototype built as part of the IPTES project.", "num_citations": "3\n", "authors": ["264"]}
{"title": "Peer Review: Trust and Prejudice\n", "abstract": " A sound review process is critical in contemporary scientific communities. The current discussion on peer review in the software engineering community is centered mainly around conferences, and focuses mostly on 'implementation' issues, like blind reviews, rebuttals, deadlines, with little attention to the ultimate goal of the review process, the external conditions that bias the process, and the role of journals. In this short note, I would like to remind the community that review is a means not the goal. I overview the goals of reviews, discuss process and environment biases, highlight advantages and limitations of the current approaches, compare the review processes of conferences and journals, and present my vision about a possible healthy evolution of software engineering conferences and journals.", "num_citations": "2\n", "authors": ["264"]}
{"title": "Semantic-based analysis of Javadoc comments\n", "abstract": " Developers often document their code with semi-structured comments such as Javadoc. Such comments are a form of specification, and often document the intended behavior of a code unit, as well as its preconditions. The goal of our project is to analyze Javadoc comments to generate assertions aiming to verify that a software unit indeed behaves as expected. Existing works with this goal mainly rely on syntactic-based techniques to match natural language terms in comments to elements in the code under test. In this paper we show the limitations of syntax-based techniques, and we present our roadmap to semantically analyze Javadoc comments.", "num_citations": "2\n", "authors": ["264"]}
{"title": "Towards an engineering methodology for multi-model scientific simulations\n", "abstract": " Complex physical phenomena are characterized by sub-systems that continuously interact with each other, and that can be modeled with different computational models. To study such phenomena we need to integrate the heterogeneous computational models of the different sub-systems to precisely analyze the interactions between the various aspects that characterize the phenomenon as a whole. While efficient methods and consolidated software tools are available to build and simulate single models, the problem of devising a general and effective approach to integrate heterogeneous models has been studied only recently and is still largely an open issue. In this paper, we propose an engineering methodology to automate the process of integrating heterogeneous computational models. The methodology is based on the novel idea of capturing the relevant information about the different models and their\u00a0\u2026", "num_citations": "2\n", "authors": ["264"]}
{"title": "On the integration of software testing and formal analysis\n", "abstract": " The software industry favors dynamic testing over static analysis of software, because traditional static software analysis techniques do not adequately balance automation, precision and scalability. Recently several researchers have combined static and dynamic techniques to overcome these problems. Undergoing efforts include concolic execution, testing-based correctness prove, execution driven abstract interpretation and dynamic invariant generation.             This paper summarizes the state of the art about combining dynamic testing and static analysis, and designs a roadmap towards a modern approach to software V&V that enhances dynamic testing with static analysis techniques. In particular, this paper surveys the most promising approaches to combine dynamic testing and static program analysis. It classifies the techniques against a framework of combination patterns, to facilitate the identification\u00a0\u2026", "num_citations": "2\n", "authors": ["264"]}
{"title": "Dynamic analysis for integration faults localization\n", "abstract": " Software developers usually integrate third party components to build systems providing distinct functionalities like graph drawing, or persistence capabilities. Developers often use grey-box components: software modules they do not know in details because provided without source code or incomplete specifications or both. Lack of source code and specifications makes the integration of such modules difficult and often causes faults that lead to critical failures if not detected at testing time. Lack of such informations complicates faults localization too. Existing static analysis and debugging techniques rely on source code or specifications, for this reason their applicability is often limited when grey-box components are used. Dynamic analysis techniques do not need such information: they monitor components interfaces, thus being applicable even when this information is missing. Dynamic analysis approaches identify violations of models inferred from data recorded during monitored executions. Unfortunately these techniques suffers from limitations too: they are capable of identifying only specific kinds of faults, their results are often affected by false positives, and they present scalability issues depending on the huge amount of data collected during training or on the identification of many violations during debugging. This paper presents Behaviour Capture and Test (BCT), a dynamic analysis technique that overcomes the limitations of the existing dynamic analysis techniques. BCT uses different kinds of models to localize different types of faults, it prunes false positives, it incrementally builds models to save disk space and guides developers when\u00a0\u2026", "num_citations": "2\n", "authors": ["264"]}
{"title": "An empirical evaluation of data flow testing of Java classes\n", "abstract": " This paper tackles the problem of structural integration testing of stateful classes. Previous work on structural testing of object-oriented software exploits data flow analysis to derive test requirements for class testing and defines contextual def-use associations to characterize inter-method relations. Non-contextual data flow testing of classes works well for unit testing, but not for integration testing, since it misses definitions and uses when properly encapsulated. Contextual data ow analysis approaches investigated so far either do not focus on state dependent behavior, or have limited applicability due to high complexity. This paper proposes an efficient structural technique based on contextual data ow analysis to test state-dependent behavior of classes that aggregate other classes as part of their state.", "num_citations": "2\n", "authors": ["264"]}
{"title": "Software Engineering and Petri Nets\n", "abstract": " Software Engineering and Petri Nets Mission Statement Page 1 www.lta.disco.unimib.it \u00a9 2003 GregorEngels, Mauro Pezz\u00e8 credits This material is taken from the Tutorial on Software Engineering and Petri presented at ICATPN 2003 by Gregor Engels and Mauro Pezz\u00e8 Software Engineering and Petri Nets Mauro Pezz\u00e8 Universit\u00e0 degli Studi di Milano - Bicocca ACPN - September 2003 \u00a9 Engels Pezz\u00e8 Software Engineering and Petri Nets 2 Mission Statement understanding the prospects for Petri nets in software engineering research and practice what are the opportunities for Petri Nets? what are the barriers and constraints? understanding today\u2019s software engineering problems not an introduction to software engineering although we introduce some aspects of it not an introduction to Petri nets (some) familiarity with Petri nets is expected Page 2 ACPN - September 2003 \u00a9 Engels Pezz\u00e8 Software Engineering and -\u2026", "num_citations": "2\n", "authors": ["264"]}
{"title": "The Maturity of Software Engineering\n", "abstract": " What can software engineering learn from other engineering disciplines? A lot-only fools believe they have nothing to learn. But the question raised by Ebert is different: it is not what we can learn from other engineering disciplines, but whether SE can reach maturity by learning from them. In many cases, traditional engineering disciplines do not behave much better than SE. I have been surprised to learn through experience how often civil engineers fail to deliver buildings on time and within budget. We tend to overestimate how well traditional engineers can predict their costs, timing, and product quality, and underestimate our own abilities with regard to well-engineered software.Traditional engineering disciplines present a degree of repeatability and measurability based on well-defined methods, processes, and artifacts, and this makes them mature, according to Ebert. But are SE processes repeatable and\u00a0\u2026", "num_citations": "2\n", "authors": ["264"]}
{"title": "A formal model for quality assurance in coloproctologic surgery.\n", "abstract": " In the last 30 years the advanced countries experienced a large growth of financial and social resources devoted to Health Care (HC)[1] The need of better co-ordinating activities of a composite large set of specialists, and the need of avoiding further growth of financial and social resources without reducing benefits, stimulated the growth of Health Care Evaluation (HCE) and Quality Assurance (QA) studies. In this paper we show how system analysis and precise assessment of the clinical process can be largely enhanced by using formal techniques. The proposed methodology takes advantage of positive experiences in computer science. It is based on a formal model of the clinical process that provides the physicians with support for assessing diagnostic procedures in a co-operative surgical environment [7]. Diagnostic, grading, and staging procedures are integrated in a single model that can be formally expressed and analyzed. The approach is exemplified by presenting an elementary model of the diagnosis, grading and staging procedures for the colo-rectal cancer and ulcerative colitis.", "num_citations": "2\n", "authors": ["264"]}
{"title": "Parallel Execution of Real-time Petri Nets\n", "abstract": " We discuss how a special class of high-level Petri nets (called Real-Time nets\u2014RT nets) can be executed in a parallel/distributed environment. This will allow direct execution of a specification given in terms of RT nets on a target parallel architecture. We discuss different decomposition schemes to parallelize execution and we provide an experimental evaluation of each on a transputer-based architecture. We then outline how real-time scheduling requirements may be taken into account in an implementation based on one such scheme.", "num_citations": "2\n", "authors": ["264"]}
{"title": "The rationale of an environment for real-time software\n", "abstract": " Some core ideas that lead to the definition of an environment for real-time systems are discussed. It is explained how integration, flexibility, and validation support can be achieved by using a suitable formalism as a hidden underlying kernel for the environment. The kernel formal notation proposed (called environment relationship nets, or ER nets) is an extension of Petri nets where tokens are not anonymous. Rather, they are environments, i.e. mappings between variables and values. The use of the kernel model for supporting the development cycle is described, and the flexibility of the model which can support a wide set of views at different levels of abstraction and for different kinds of users at the same level is shown. The problems connected with the quality assurances of the system being developed are also discussed.< >", "num_citations": "2\n", "authors": ["264"]}
{"title": "On introducing automatic test case generation in practice: A success story and lessons learned\n", "abstract": " The level and quality of automation dramatically affects software testing activities, determines costs and effectiveness of the testing process, and largely impacts on the quality of the final product. While costs and benefits of automating many testing activities in industrial practice (including managing the quality process, executing large test suites, and managing regression test suites) are well understood and documented, the benefits and obstacles of automatically generating system test suites in industrial practice are not well reported yet, despite the recent progresses of automated test case generation tools. Proprietary tools for automatically generating test cases are becoming common practice in large software organizations, and commercial tools are becoming available for some application domains and testing levels. However, generating system test cases in small and medium-size software companies is still\u00a0\u2026", "num_citations": "1\n", "authors": ["264"]}
{"title": "An Evolutionary Approach to Adapt Tests Across Mobile Apps\n", "abstract": " Automatic generators of GUI tests often fail to generate semantically relevant test cases, and thus miss important test scenarios. To address this issue, test adaptation techniques can be used to automatically generate semantically meaningful GUI tests from test cases of applications with similar functionalities. In this paper, we present ADAPTDROID, a technique that approaches the test adaptation problem as a search-problem, and uses evolutionary testing to adapt GUI tests (including oracles) across similar Android apps. In our evaluation with 32 popular Android apps, ADAPTDROID successfully adapted semantically relevant test cases in 11 out of 20 cross-app adaptation scenarios.", "num_citations": "1\n", "authors": ["264"]}
{"title": "Introduction to the special issue on ISSTA 2013\n", "abstract": " This special issue contains five articles extended from the 2013 International Symposium on Software Testing and Analysis (ISSTA 2013), held in Lugano Switzerland, and chaired by Mauro Pezze (General Chair) and Mark Harman (Program Chair). ISSTA brings together academics, industrial researchers, and practitioners to exchange new ideas, problems, and experience on how to analyse and test software systems. Software analysis and testing is a rich and varied research area that encompasses theoretical, empirical, and practical analysis and techniques that assist software engineers with the ever-present tasks of finding and fixing faults in software systems, thereby improving confidence in the correct operation of these systems. Testing involves automated tool support to help generate test inputs, typically guided by criteria that assess test suite adequacy. Testing is also naturally targeted at critical faults that\u00a0\u2026", "num_citations": "1\n", "authors": ["264"]}
{"title": "Towards cost-effective oracles\n", "abstract": " Oracles are the key of the success and, at the same time, the bottleneck of automated software testing. Today we can generate enormous amount of test cases and reach impressive code coverage, but the effectiveness of test cases is limited by the availability of test oracles that can distinguish failing executions. Simple implicit oracles come at negligible cost, but can reveal only few categories of often uninteresting failures. Semantically relevant test oracles can be extremely effective in revealing subtle failures, but come at high cost. In this paper we outline the dimensions of the problem, highlight the open research areas, and suggest how to produce a new generation of cost-effective oracles.", "num_citations": "1\n", "authors": ["264"]}
{"title": "Introduction to the Special Issue International Conference on Software Engineering (ICSE 2012)\n", "abstract": " The rapid evolution of software systems, the dynamic nature of modern software applications, the growing importance of distributed environments, the emerging role of the cloud technology, the pervasiveness of software solutions, and the increasing importance of software reliability are challenging the software engineering discipline with new problems and questions.", "num_citations": "1\n", "authors": ["264"]}
{"title": "From off-Line to continuous on-line maintenance\n", "abstract": " Summary form only given. Software is the cornerstone of the modern society. Many human activities rely on software systems that shall operate seamlessly 24/7, and failures in such systems may cause severe problems and considerable economic loss. To efficiently address a growing variety of increasingly complex activities, software systems rely on sophisticated technologies. Most software systems are assembled from modules and subsystems that are often developed by third party organization, and sometime are not even available at the system build time. This is the case for example of many Web applications that link Web services built and changed independently by third party organizations while the Web applications are running. The progresses of software engineering in the last decades have increased the productivity, reduced the costs and improved the reliability of software products, but have not\u00a0\u2026", "num_citations": "1\n", "authors": ["264"]}
{"title": "Automatic diagnosis of software functional faults by means of inferred behavioral models\n", "abstract": " Software failures have a relevant impact on today economy. The US National Institute of Standards and Technology (NIST), estimated that software failures cost US economy $59.5 billion annually [95]. Different experiments [82, 48] and catastrophic events [69] indicate that functional and integration faults remain one of the main issues software engineers have to focus on.The adoption of extensive validation and verification activities during development can improve the quality of the developed software but it cannot guarantee the removal of all the faults in a system. Moreover, validation and verification techniques often require source code or specifications to be applied and thus cannot be used with many software systems, like those that integrate Off The Shelf components, which are usually provided without source code or with incomplete specifications. Since many faults still remain undetected till the software is used in the field, the adoption of techniques that facilitate and reduce the time necessary to diagnose faults can reduce the costs caused by system failures and downtime. In addition, since fault diagnosis is one of the activities with the greatest impact on software development and maintenance costs [128], the adoption of automated fault diagnosis techniques can reduce also the costs of the whole software development process. Existing automated and semi-automated fault diagnosis techniques present many limitations. Interactive debugging techniques require a lot of developers effort, because the whole debugging process is completely manual [21, 6, 81]. Slicing [127] and spectra based techniques [61, 7] pinpoint the faulty code but\u00a0\u2026", "num_citations": "1\n", "authors": ["264"]}
{"title": "kBehavior: Algorithms and Complexity\n", "abstract": " Different software testing and analysis techniques use finite state automata (FSA) inferred from execution traces to model the behavior of a software system and help software engineers to re-engineer [2], debug [3, 4], or identify failures and anomalies in the software [5\u20137]. FSA inference algorithms generate FSA that accept and generalize strings of symbols received as input [8\u201317]. We described the kBehavior inference engine in [1]. kBehavior has been successfully adopted to infer FSAs that model the behavior of different software systems. In particular, it has been successfully applied to derive FSAs from both traces collected by monitoring the activity of software components [1, 6, 18] and standard log files produced by Enterprise systems [19\u201321]. This report analyzes kBehavior complexity and complements the algorithms description in [1]. The report proceeds as follow: Section 2 recalls some background on Finite State Automata, Section 3 overviews the kBehavior algorithm, Section 4 presents the analysis of kBehavior complexity, Section 4 concludes the report.", "num_citations": "1\n", "authors": ["264"]}
{"title": "Introduction to the special section from the ACM international symposium on software testing and analysis (ISSTA 2006)\n", "abstract": " Advances in software design and development fight the increasing complexity of software applications, and address stringent productivity and quality requirements. New abstraction approaches that foster modularity and separation of concerns reduce and sometimes even eliminate the incidence of classic faults; but they often expose latent problems, which in turn calls for new test and analysis approaches. Thus, the software testing and analysis discipline continues to occupy a central role in software engineering, and attracts much research interest as witnessed by the growing number of conferences, annual events, and papers presented at the major software engineering conferences. The ACM International Symposium of Software Testing and Analysis (ISSTA) is continuing a tradition that was initiated in 1978 in Fort Lauderdale as the Symposium on Testing, Analysis and Verification (TAV); in 1993 it assumed the\u00a0\u2026", "num_citations": "1\n", "authors": ["264"]}
{"title": "Generation of Self-Testing Components\n", "abstract": " Internet software tightly integrates classic computation with communication software. Heterogeneity and complexity can be tackled with a component-based approach, where components are developed by application experts and integrated by domain experts. Component-based systems cannot be tested with classic approaches but present new problems. Current techniques for integration testing are based upon the component developer providing test specifications or suites with their components. However, components are increasingly being used in ways not envisioned by their developer, thus making their test specification and suites invalid. In this paper, we propose an approach for implementing self-testing components, which allow integration test specifications and suites to be developed by observing both the behavior of the component and of the entire system.", "num_citations": "1\n", "authors": ["264"]}
{"title": "Automatic validation of component-based systems\n", "abstract": " Component-based systems presents new testing challenges. This paper copes with the problem of testing evolving systems, ie, systems obtained by modifying and/or substituting some of the system components, and proposes a technique to automatically identify behavioral differences between new and old system versions. The approach is based on the automatic distilling of invariants from in-filed executions Distilled invariants can be used to monitor the behavior of new components and highlight unexpected interactions.", "num_citations": "1\n", "authors": ["264"]}
{"title": "A formal definition of Structured Analysis with programmable graph grammars\n", "abstract": " Structured Analysis has been one of the most widely used specification notations of the last decades. Friendliness and flexibility promoted its use, but informality hampered its precision and efficacy. The many proposals that tried to overcome the problem improve precision, but constrain flexibility. They propose formal and specific interpretations of Structured Analysis that, even if meritorious, do not impact on day-to-day practice. To meet the goal, formalization attempts should not try to impose particular interpretations, but they should allow users to tailor the interpretation to their current needs.               In this paper, we present a solution that merges precision and flexibility to provide a customizable and formal definition of Structured Analysis. Formalization consists of a set of customization rules and a consistency framework. Customization rules, based on graph grammars, formalize the different behaviors of\u00a0\u2026", "num_citations": "1\n", "authors": ["264"]}
{"title": "A software architecture approach for designing CASE systems\n", "abstract": " This paper presents a software architecture approach that supports facilities for customizing the set of functionalities offered by CASE systems, without affecting the complexity of the system. The proposed software architecture approach supports on-line composition of elementary and composite functionalities to implement new complex functionalities required for specific applications. The proposal pushes the interesting results obtained by other research groups beyond the goals reached so far by allowing new functionalities to be defined at run-time, without recompiling the system, but relying on dynamic linking mechanisms. The proposal is illustrated with a case-study, successfully used in some industrial pilot projects.", "num_citations": "1\n", "authors": ["264"]}