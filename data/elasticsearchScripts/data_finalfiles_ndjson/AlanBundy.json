{"title": "The use of explicit plans to guide inductive proofs\n", "abstract": " We propose the use of explicit proof plans to guide the search for a proof in automatic theorem proving. By representing proof plans as the specifications of LCF-like tactics, [Gordon et al 79], and by recording these specifications in a sorted meta-logic, we are able to reason about the conjectures to be proved and the methods available to prove them. In this way we can build proof plans of wide generality, formally account for and predict their successes and failures, apply them flexibly, recover from their failures, and learn them from example proofs.             We illustrate this technique by building a proof plan based on a simple subset of the implicit proof plan embedded in the Boyer-Moore theorem prover, [Boyer & Moore 79].             Space restrictions have forced us to omit many of the details of our work. These are included in a longer version of this paper which is available from: The Documentation Secretary\u00a0\u2026", "num_citations": "558\n", "authors": ["1609"]}
{"title": "The computer modelling of mathematical reasoning\n", "abstract": " This book started as notes for a postgraduate course in Mathematical Reasoning given in the Department of Artificial Intelligence at Edinburgh from 1979 onwards. Students on the course are drawn from a wide range of backgrounds: Psychology, Computer Science, Mathematics, Education, etc. The first draft of the notes was written during a terms sabbatical leave in 1980. Later they were used for a similar course at undergraduate level.While there are now several textbooks on Artificial Intelligence techniques and, more particularly, on Problem Solving and Theorem Proving, I felt the need for a book concentrating on applications of these techniques to Mathematics. There was certainly enough material, but it was scattered in research journals, conference proceedings and theses. If it were collected together I hoped it might prove of interest to a wider audience than the usual artificial intelligentsia; I hoped that mathematicians and educationalists might find it a eye opener to how computational ideas could shed light on the process of doing Mathematics.", "num_citations": "476\n", "authors": ["1609"]}
{"title": "Rippling: A heuristic for guiding inductive proofs\n", "abstract": " We describe rippling: a tactic for the heuristic control of the key part of proofs by mathematical induction. This tactic significantly reduces the search for a proof of a wide variety of inductive theorems. We first present a basic version of rippling, followed by various extensions which are necessary to capture larger classes of inductive proofs. Finally, we present a generalised form of rippling which embodies these extensions as special cases. We prove that generalised rippling always terminates, and we discuss the implementation of the tactic and its relation with other inductive proof search heuristics.", "num_citations": "373\n", "authors": ["1609"]}
{"title": "The OYSTER-CLAM system\n", "abstract": " OYSTER [Horn 88] is an interactive proof editor closely based on the Cornell NuPRL system, but implemented in Prolog. The object-level logic is a version of Martin-LSf type theory (a higher order constructive logic including induction) in a sequent-calculus formulation. Proofs are constructed in a top-down fashion by application of the rules of inference. Notational definitions and libraries of theorems are supported. The tactic language for the system is Prolog. Predicates describing properties of a proof under construction are available to the user, who may also include arbitrary Prolog in tactics. Soundness of the system is ensured by the use of an abstract data type of proofs: partial proofs can only be altered by application of the primitive proof rules. Tactics can be combined using system defined tacticats. Prolog pattern-matching and backtracking in tactics have proved useful in the automation of proof search. Since\u00a0\u2026", "num_citations": "338\n", "authors": ["1609"]}
{"title": "Computational thinking is pervasive\n", "abstract": " A remarkable intellectual revolution is happening all around us, but few people are remarking on it. Computational thinking is influencing research in nearly all disciplines, both in the sciences and the humanities. Researchers are using computational metaphors to enrich theories as diverse as protoeomics and the mind-body problem. Computing has enabled researchers to ask new kinds of questions and to accept new kinds of answers, for instance, questions that require the processing of huge amounts of data. Of course, we all have computers on our desks nowadays. We all use them for email, web browsing, word processing, game playing, etc. But the computational thinking revolution goes much deeper than that; it is changing the way we think. Computational concepts provide a new language for describing hypotheses and theories. Computers provide an extension to our cognitive faculties. If you want to understand the 21st Century then you must first understand computation.At the University of Edinburgh, we have been exploring these themes in a series of seminars (http://www. inf. ed. ac. uk/research/programmes/comp-think/). At each seminar, one or more experts have discussed the influence of computational thinking on their discipline. We have enjoyed a stellar cast of speakers from a wide range of disciplines. Speakers have included the Principal of the University and three Vice Principals. The disciplines have ranged from physics, biology, and medicine to philosophy, architecture, and education. Often the influences described have been quite subtle. A common story is that the most direct approach does not work for some interesting\u00a0\u2026", "num_citations": "315\n", "authors": ["1609"]}
{"title": "Productive use of failure in inductive proof\n", "abstract": " Proof by mathematical induction gives rise to various kinds of eureka steps, e.g., missing lemmata and generalization. Most inductive theorem provers rely upon user intervention in supplying the required eureka steps. In contrast, we present a novel theorem-proving architecture for supporting the automatic discovery of eureka steps. We build upon rippling, a search control heuristic designed for inductive reasoning. We show how the failure if rippling can be used in bridging gaps in the search for inductive proofs.", "num_citations": "244\n", "authors": ["1609"]}
{"title": "Experiments with proof plans for induction\n", "abstract": " The technique of proof plans is explained. This technique is used to guide automatic inference in order to avoid a combinatorial explosion. Empirical research is described to test this technique in the domain of theorem proving by mathematical induction. Heuristics, adapted from the work of Boyer and Moore, have been implemented as Prolog programs, called tactics, and used to guide an inductive proof checker, Oyster. These tactics have been partially specified in a meta-logic, and the plan formation program, CLAM, has been used to reason with these specifications and form plans. These plans are then executed by running their associated tactics and, hence, performing an Oyster proof. Results are presented of the use of this technique on a number of standard theorems from the literature. Searching in the planning space is shown to be considerably cheaper than searching directly in Oyster's search\u00a0\u2026", "num_citations": "208\n", "authors": ["1609"]}
{"title": "The automation of proof by mathematical induction\n", "abstract": " This paper is a chapter of the Handbook of Automated Reasoning edited by Voronkov and Robinson. It describes techniques for automated reasoning in theories containing rules of mathematical induction. Firstly, inductive reasoning is defined and its importance fore reasoning about any form of repitition is stressed. Then the special search problems that arise in inductive theories are explained followed by descriptions of the heuristic methods that have been devised to solve these problems.", "num_citations": "196\n", "authors": ["1609"]}
{"title": "A science of reasoning\n", "abstract": " How can we understand reasoning in general and mathematical proofs in particular? It is argued that a high-level understanding of proofs is needed to complement the low-level understanding provided by Logic. A role for computation is proposed to provide this high-level understanding, namely by the association of proof plans with proofs. Criteria are given for assessing the association of a proof plan with a proof.", "num_citations": "188\n", "authors": ["1609"]}
{"title": "Incidence calculus: a mechanism for probabilistic reasoning\n", "abstract": " Mechanisms for the automation of uncertainty are required for expert systems. Sometimes these mechanisms need to obey the properties of probabilistic reasoning. We argue that a purely numeric mechanism, like those proposed so far, cannot provide a probabilistic logic with truth functional connectives. We propose an alternative mechanism, Incidence Calculus, which is based on a representation of uncertainty using sets of points, which might represent situations models or possible worlds. Incidence Calculus does provide a probabilistic logic with truth functional connectives.", "num_citations": "182\n", "authors": ["1609"]}
{"title": "Using meta-level inference for selective application of multiple rewrite rule sets in algebraic manipulation\n", "abstract": " In this paper we describe a technique for controlling inference, called meta-level inference, and a program for algebraic manipulation, PRESS, which embodies this technique. In PRESS, algebraic expressions are manipulated by a series of methods. The appropriate method is chosen by meta-level inference and itself uses meta-level reasoning to select and apply rewrite rules to the current expression.The use of meta-level inference is shown to drastically cut down on search, lead to clear and modular programs, aid the proving of properties of the program and enable the automatic learning of both new algebraic facts and new control information.", "num_citations": "177\n", "authors": ["1609"]}
{"title": "Solving mechanics problems using meta-level inference\n", "abstract": " In this paper we shall describe a program (MECHO), written in Prolog[14], which solves a wide range of mechanics problems from statements in both predicate calculus and English. Mecho uses the technique of meta-level inference to control search in natural language understanding, common sense inference, model formation and algebraic manipulation. We argue that this is a powerful technique for controlling search while retaining the modularity of declarative knowledge representations.", "num_citations": "172\n", "authors": ["1609"]}
{"title": "Explanation-based generalisation= partial evaluation\n", "abstract": " We argue that explanation-based generalisation as recently proposed in the machine learning literature is essentially equivalent to partial evaluation, a well-known technique in the functional and logic programming literature. We show this equivalence by analysing the definitions and underlying algorithms of both techniques, and by giving a PROLOG program which can be interpreted as doing either explanation-based generalisation or partial evaluation.", "num_citations": "171\n", "authors": ["1609"]}
{"title": "An analytical comparison of some rule-learning programs\n", "abstract": " To become a mature science, Artificial Intelligence needs more theoretical work. One form this should take is the analytic comparison of existing programs to extract precise techniques from the code, compare similar techniques, expose faults, and extend successful techniques.In this spirit, we compare the rule-learning programs of Brazdil [2], Langley [7], Mitchell et al. [14, 15], Shapiro [18], and Waterman [22]. Each of these programs has two main parts: a critic for identifying faulty rules and a modifier for correcting them. To aid comparison we describe the techniques of the various authors using a uniform notation. We find several similarities in the techniques used by the various authors and uncover the relations between them.We compare the rule-learning programs with the concept-learning programs of Quinlan [17], and Young et al. [25]. The two types of program have much in common, and many of the rule\u00a0\u2026", "num_citations": "171\n", "authors": ["1609"]}
{"title": "Catalogue of artificial intelligence tools\n", "abstract": " A viewer-centred representation making explicit the depths, local orientations and discontinuities of visible surfaces, created and maintained from a number of cues eg. tereop. l.< 261> and optical flow< 180>. It was thought by Marr to be at the limit of pure perception, ie subsequent processes are no longer completely data-driven, and for him it provides a representation of objective physical reality that precedes the decomposition of the scene into objects.", "num_citations": "164\n", "authors": ["1609"]}
{"title": "On the notion of interestingness in automated mathematical discovery\n", "abstract": " We survey five mathematical discovery programs by looking in detail at the discovery processes they illustrate and the success they had. We focus on how they estimate the interestingness of concepts and conjectures and extract some common notions about interestingness in automated mathematical discovery. We detail how empirical evidence is used to give plausibility to conjectures, and the different ways in which a result can be thought of as novel. We also look at the ways in which the programs assess how surprising and complex a conjecture statement is, and the different ways in which the applicability of a concept or conjecture is used. Finally, we note how a user can set tasks for the program to achieve and how this affects the calculation of interestingness. We conclude with some hints on the use of interestingness measures for future developers of discovery programs in mathematics.", "num_citations": "124\n", "authors": ["1609"]}
{"title": "Preparing for the future of artificial intelligence\n", "abstract": " In October 2016, the US National Science and Technology Council published a report on Artificial Intelligence (AI)(United States 2016) that summarised evidence from a wide variety of sources on how they expect AI to develop, what impact it would have and what actions it recommended the US Government to take. It built on several previous US Government reports, eg United States (2014, 2016), and consulted widely among AI experts in the USA, eg Horvitz and Selman (2009) and five workshops. A companion document has also been published:\u2018\u2018The National Artificial Intelligence Research and Development Strategic Plan\u2019\u2019, which lays out a strategic plan for federally funded research and development in AI. Overall, this is a comprehensive report, which I recommend to anyone wanting a balanced review of the state of the AI art, its potential impact and what ethical, economic and societal issues it presents. It\u00a0\u2026", "num_citations": "106\n", "authors": ["1609"]}
{"title": "Extensions to the rippling-out tactic for guiding inductive proofs\n", "abstract": " In earlier papers we described a technique for automatically constructing inductive proofs, using a heuristic search control tactic called rippling-out. Further testing on harder examples has shown that the rippling-out tactic significantly reduces the search for a proof of a wide variety of theorems, with relatively few cases in which all proofs were pruned. However, it also proved necessary to generalise and extend rippling-out in various ways. Each of the various extensions are described with examples to illustrate why they are needed, but it is shown that the spirit of the original rippling-out tactic has been retained.", "num_citations": "97\n", "authors": ["1609"]}
{"title": "Conjecture synthesis for inductive theories\n", "abstract": " We have developed a program for inductive theory formation, called IsaCoSy, which synthesises conjectures \u2018bottom-up\u2019 from the available constants and free variables. The synthesis process is made tractable by only generating irreducible terms, which are then filtered through counter-example checking and passed to the automatic inductive prover IsaPlanner. The main technical contribution is the presentation of a constraint mechanism for synthesis. As theorems are discovered, this generates additional constraints on the synthesis process. We evaluate IsaCoSy as a tool for automatically generating the background theories one would expect in a mature proof assistant, such as the Isabelle system. The results show that IsaCoSy produces most, and sometimes all, of the theorems in the Isabelle libraries. The number of additional un-interesting theorems are small enough to be easily pruned by hand.", "num_citations": "92\n", "authors": ["1609"]}
{"title": "Eco-Logic: logic-based approaches to ecological modelling\n", "abstract": " Eco-logic | Guide books ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksEco-logic: logic-based approaches to ecological modelling ABSTRACT No abstract available. Index Terms 1.Eco-logic 1.Applied computing 1.Physical sciences and engineering 1.Earth and atmospheric sciences 2.Computing methodologies 1.Modeling and simulation 3.Theory of computation 1.Logic 1.Constraint and logic programming Comments Login options Check if you have access through your login credentials or your institution to get full access on this article. Sign in Full Access Get this Publication Information Contributors Published in Guide books \u2026", "num_citations": "91\n", "authors": ["1609"]}
{"title": "Automatic verification of design patterns in Java\n", "abstract": " Design patterns are widely used by designers and developers for building complex systems in object-oriented programming languages such as Java. However, systems evolve over time, increasing the chance that the pattern in its original form will be broken. To verify that a design pattern has not been broken requires specifying the original intent of the design pattern. Whilst informal descriptions of design patterns exist, no formal specifications are available due to differences in implementations between programming languages. We present a pattern specification language, Spine, that allows patterns to be defined in terms of constraints on their implementation in Java. We also present some examples of patterns defined in Spine and show how they are processed using a proof engine called Hedgehog. The conclusion discusses the type of patterns that are amenable to defining in Spine, and highlights some\u00a0\u2026", "num_citations": "85\n", "authors": ["1609"]}
{"title": "Automatic concept formation in pure mathematics\n", "abstract": " The HR program forms concepts and makes conjectures in domains of pure mathematics and uses theorem prover OTTER and model generator MACE to prove or disprove the conjectures. HR measures properties of concepts and assesses the theorems and proofs involving them to estimate the interestingness of each  concept and employ a best first search. This approach has led HR to the discovery of interesting new mathematics and enables it to build theories from just the axioms of finite algebras.", "num_citations": "84\n", "authors": ["1609"]}
{"title": "Artificial intelligence: an introductory course\n", "abstract": " Artificial Intelligence: An introductory course \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Artificial Intelligence: An introductory course Alan Bundy, R. Burstall, S. Weir, R. Young School of Informatics Research output: Book/Report \u203a Book Overview Fingerprint Abstract Also translated into Japanese 1978. Original language English Publisher Edinburgh University Press Publication status Published - 1978 Access to Document Full textAccepted author manuscript, 18.4 MB Fingerprint Dive into the research topics of 'Artificial Intelligence: An introductory course'. Together they form a unique fingerprint. Artificial intelligence Engineering & Materials Science \u2026", "num_citations": "79\n", "authors": ["1609"]}
{"title": "Turning eureka steps into calculations in automatic program synthesis\n", "abstract": " A description is given of a technique called middle-out reasoning for the control of search in automatic theorem proving. The authors illustrate it use in the domain of automatic program synthesis. Programs can be synthesised from proofs that their logical specifications are satisfiable. Each proof step is also a program construction step. Unfortunately, a naive use of this technique requires a human or computer to produce proof steps which provide the essential structure of the desired program. It is hard to see the justification for these steps at the time that they are made; the reason for them emerges only later in the proof. Such proof steps are often call eureka steps. Middle-out reasoning enables these eureka steps to be produced, automatically, as a side effect of non-eureka steps.< >", "num_citations": "78\n", "authors": ["1609"]}
{"title": "AI's greatest trends and controversies\n", "abstract": " The transition to the next millennium gives us an opportunity to reflect on the past and project the future. In this spirit, we have asked a set of distinguished scholars and practitioners who were involved in AI's formative stages to describe the most notable trend or controversy (or nontrend or noncontroversy) during AI's development. The responses provide an interesting characterization of AI-and, in many ways, of the people of AI. We gave our contributors a great deal of flexibility in the nature of their responses. Some provided grand summaries of the history of the field as a whole. Others commented insightfully on more focused topics. Some observed changes and changed along with them. Others are still making advances on research agendas articulated presciently long ago. Some are optimistic. Others are pessimistic. Despite the range, both individually and collectively they provide insights into where we have\u00a0\u2026", "num_citations": "73\n", "authors": ["1609"]}
{"title": "The synthesis of logic programs from inductive proofs\n", "abstract": " We describe a technique for synthesising logic (Prolog) programs from non-executable specifications. This technique is adapted from one for synthesising functional programs as total functions. Logic programs, on the other hand, define predicates. They can be run in different input modes, they sometimes produce multiple outputs and sometimes none. They may not terminate. The key idea of the adaptation is that a predicate is a total function in the all-ground mode, i.e. when all its arguments are inputs (pred(+,...,+) in Prolog notation). The program is synthesised as a function in this mode and then run in other modes. To make the technique work it is necessary to synthesise pure logic programs, without the closed world assumption, and then compile these into Prolog programs. The technique has been tested on the OYSTER (functional) program development system.", "num_citations": "72\n", "authors": ["1609"]}
{"title": "A rational reconstruction and extension of recursion analysis\n", "abstract": " The focus of this paper is the technique of recursion analysis. Recursion analysis is used by the Boyer-Moore Theorem Prover to choose an appropriate induction schema and variable to prove theorems by mathematical induction. A rational reconstruction of recursion analysis is outlined, using the technique of proof plans. This rational reconstruction suggests an extension of recursion analysis which frees the induction suggestion from the forms of recursion found in the conjecture. Preliminary results are reported of the automation of this rational reconstruction and extension using the clam-Oyster system.", "num_citations": "72\n", "authors": ["1609"]}
{"title": "Doing arithmetic with diagrams\n", "abstract": " ABSTRACT\u2022A theorem prover for part of arithmetic in described which proves theorems by representing them in the form of a diagram or network. The nodes of this network represent'ideal integers', ie objects which have all the properties of integers, without being any particular intoger. The links in the network represent relationships between'ideal integers'. The procedures which draw these diagrams make elementary deductions based on their built-in knowledge of the functions and predicates of arithmetic. This theorem prover is intended as a model of some kinds of human problemsolving behaviour.", "num_citations": "69\n", "authors": ["1609"]}
{"title": "Dynamic, automatic, first-order ontology repair by diagnosis of failed plan execution\n", "abstract": " We describe ORS, an ontology repair system. In contrast to most ontology matching systems, ORS is designed to repair an ontology that does not accurately model its domain. ORS\u2019s ontology repairs include belief revisions, but more often makes signature repairs. It does not require full access to the ontologies of other agents and works entirely automatically and dynamically. ORS is the first example of a new breed of dynamic, automatic ontology-repair mechanisms, which we believe will be essential to realise the vision of autonomous, interacting agents, such as envisaged in the emantic Web. Full access to another (potentially rival) agent\u2019s ontology is unrealistic; static and interactive matching mechanisms are unrealistic in the context of huge, dynamic populations of agents and full ontological agreement is pragmatically unrealistic. We present encouraging experimental results, plus an analysis of current\u00a0\u2026", "num_citations": "63\n", "authors": ["1609"]}
{"title": "On automating diagrammatic proofs of arithmetic arguments\n", "abstract": " Theorems in automated theorem proving are usually proved by formal logical proofs. However, there is a subset of problems which humans can prove by the use of geometric operations on diagrams, so called diagrammatic proofs. Insight is often more clearly perceived in these proofs than in the corresponding algebraic proofs; they capture an intuitive notion of truthfulness that humans find easy to see and understand. We are investigating and automating such diagrammatic reasoning about mathematical theorems. Concrete, rather than general diagrams are used to prove particular concrete instances of the universally quantified theorem. The diagrammatic proof is captured by the use of geometric operations on the diagram. These operations are the \u201cinference steps\u201d of the proof. An abstracted schematic proof of the universally quantified theorem is induced from these proof instances. The constructive \u03c9\u00a0\u2026", "num_citations": "61\n", "authors": ["1609"]}
{"title": "What stories should we tell novice\n", "abstract": " To understand the execution of a program, a model of the programming language is required. For Prolog it is necessary to explain such features as backtracking, recursion and unification. In order to do so, the model should include information about the search space, the flow of control through the search space and the Prolog clauses. This model is equivalent to du Boulay and O\u2019Shea\u2019s \u2018virtual machine\u2019in Logo (du Boulay and O\u2019Shea, 1978), and to Lawrence Byrd\u2019s \u2018notional machine\u2019(Byrd, 1980). There are a number of different ways of representing the model. The way we choose will depend upon the context and the audience. Each of the different ways of representing the model will be referred to as a Prolog story.", "num_citations": "56\n", "authors": ["1609"]}
{"title": "The use of proof plans to sum series\n", "abstract": " We describe a program for finding closed form solutions to finite sums. The program was built to test the applicability of the proof planning search control technique in a domain of mathematics outwith induction. This experiment was successful. The series summing program extends previous work in this area and was built in a short time just by providing new series summing methods to our existing inductive theorem proving system CLAM.             One surprising discovery was the usefulness of the ripple tactic in summing series. Rippling is the key tactic for controlling inductive proofs, and was previously thought to be specialised to such proofs. However, it turns out to be the key sub-tactic used by all the main tactics for summing series. The only change required was that it had to be supplemented by a difference matching algorithm to set up some initial meta-level annotations to guide the rippling process. In\u00a0\u2026", "num_citations": "53\n", "authors": ["1609"]}
{"title": "Analyzing mathematical proofs (or reading between the lines)\n", "abstract": " He study equation solving and analyse the solutions of experienced mathematicians. We find that traditional theorem proving methods are inadequate to explain the directness of these solutions, and that the well known algorithms for polynomials etc. are inadequate to explain the wide variety of equations solved. Our analysis reveals a system of high-level descriptions, strategies and goals, which can be used to guide the search through an explosively large search space. A few of these strategies will be investigated in detail. It is hoped that this analysis will eventually be incorporated into a computer program that solves equations.", "num_citations": "53\n", "authors": ["1609"]}
{"title": "Automatic identification of mathematical concepts\n", "abstract": " The HR program by Colton et al.(1999) performs theory formation in mathematics by exploring a space of mathematical concepts. By enabling HR to determine when it has found a particular concept, and by adding a forward looking mechanism, we have applied HR to the problem of identifying mathematical concepts. We illustrate this by using HR to identify and extrapolate integer sequences and by performing a qualitative comparison with the machine learning program Progol.", "num_citations": "52\n", "authors": ["1609"]}
{"title": "A survey of automated deduction\n", "abstract": " We survey research in the automation of deductive inference, from its beginnings in the early history of computing to the present day. We identify and describe the major areas of research interest and their applications. The area is characterised by its wide variety of proof methods, forms of automated deduction and applications.", "num_citations": "52\n", "authors": ["1609"]}
{"title": "Automatic invention of integer sequences\n", "abstract": " We report on the application of the HR program (Colton, Bundy, & Walsh 1999) to the problem of automatically inventing integer sequences. Seventeen sequences invented by HR are interesting enough to have been accepted into the Encyclopedia of Integer Sequences (Sloane 2000) and all were supplied with interesting conjectures about their nature, also discovered by HR. By extending HR, we have enabled it to perform a two stage process of invention and investigation. This involves generating both the definition and terms of a new sequence, relating it to sequences already in the Encyclopedia and pruning the output to help identify the most surprising and interesting results.", "num_citations": "51\n", "authors": ["1609"]}
{"title": "An interface between CLAM and HOL\n", "abstract": " This paper describes an interface between the CLAM proof planner and the HOL interactive theorem prover. The interface sends HOL goals to CLAM for planning, and translates plans back into HOL tactics that solve the initial goals. The combined system is able to automatically prove a number of theorems involving recursively defined functions.", "num_citations": "49\n", "authors": ["1609"]}
{"title": "Intelligent front ends\n", "abstract": " An intelligent front end is a user-friendly interface to a software package, which uses Artificial Intelligence techniques to enable the user to interact with the computer using his/her own terminology rather than that demanded by the package. Several such systems exist and provide interfaces for finite element. statistical and simulation packages, and the area is an important area of growth for expert systems. In this paper we discuss the techniques required in an intelligent front end and whether general tools can be provided for their construction.", "num_citations": "48\n", "authors": ["1609"]}
{"title": "Will it reach the top? Prediction in the mechanics world\n", "abstract": " We describe an extension of a mechanics problem solving program to the set of \u201croller coaster\u201d problems, i.e. problems about the motion of a particle on a complex path. The reasoning strategy adopted by the program is described and compared to earlier work in this domain. Conclusions are drawn about the representation of motion and prediction. Questions are raised about Frames and Multiple Representations.", "num_citations": "47\n", "authors": ["1609"]}
{"title": "Correctness criteria of some algorithms for uncertain reasoning using incidence calculus\n", "abstract": " Incidence Calculus is a technique for associating uncertainty values with logical sentences. These uncertainty values are called incidences and they are sets of points, which may be thought of as representing equivalence classes of situations, Tarskian models, or possible worlds. Incidence Calculus was originally introduced in [1].               Incidence Calculus was designed to overcome various inherent problems with purely numeric mechanisms for uncertain reasoning [2]. In particular, incidences can represent the dependence between sentences, which numbers cannot, and hence Incidence Calculus can provide genuine, probabilistic reasoning.               In this paper we prove soundness and completeness results for some algorithms introduced in [1] and hence satisfy some of the correctness criteria for Incidence Calculus. These algorithms can be used for probabilistic reasoning and to check the\u00a0\u2026", "num_citations": "45\n", "authors": ["1609"]}
{"title": "Automatic verification of Java design patterns\n", "abstract": " Design patterns are widely used by object oriented designers and developers for building complex systems in object oriented programming languages such as Java. However, systems evolve over time, increasing the chance that the pattern in its original form will be broken. We attempt to show that many design patterns (implemented in Java) can be verified automatically. Patterns are defined in terms of variants, mini-patterns, and artifacts in a pattern description language called SPINE. These specifications are then processed by Hedgehog, an automated proof tool that attempts to prove that Java source code meets these specifications.", "num_citations": "44\n", "authors": ["1609"]}
{"title": "What is a proof?\n", "abstract": " To those brought up in a logic-based tradition there seems to be a simple and clear definition of proof. But this is largely a twentieth century invention; many earlier proofs had a different nature. We will look particularly at the faulty proof of Euler's Theorem and Lakatos' rational reconstruction of the history of this proof. We will ask: how is it possible for the errors in a faulty proof to remain undetected for several years\u2014even when counter-examples to it are known? How is it possible to have a proof about concepts that are only partially defined? And can we give a logic-based account of such phenomena? We introduce the concept of schematic proofs and argue that they offer a possible cognitive model for the human construction of proofs in mathematics. In particular, we show how they can account for persistent errors in proofs.", "num_citations": "43\n", "authors": ["1609"]}
{"title": "Scheme-based theorem discovery and concept invention\n", "abstract": " We describe an approach to automatically invent/explore new mathematical theories, with the goal of producing results comparable to those produced by humans, as represented, for example, in the libraries of the Isabelle proof assistant. Our approach is based on \u2018schemes\u2019, which are formulae in higher-order logic. We show that it is possible to automate the instantiation process of schemes to generate conjectures and definitions. We also show how the new definitions and the lemmata discovered during the exploration of a theory can be used, not only to help with the proof obligations during the exploration, but also to reduce redundancies inherent in most theory-formation systems. We exploit associative-commutative (AC) operators using ordered rewriting to avoid AC variations of the same instantiation. We implemented our ideas in an automated tool, called IsaScheme, which employs Knuth\u2013Bendix completion\u00a0\u2026", "num_citations": "42\n", "authors": ["1609"]}
{"title": "MECHO: A program to solve mechanics problems\n", "abstract": " Bundy, A., Byrd, L., Luger, G., Mellish, C., Milne, R., & Palmer, M.(1979). MECHO: A program to solve mechanics problems. Department of Artificial Intelligence, University of Edinburgh. http://www. worldcat. org/title/mecho-a-program-to-solve-mechanics-problems/oclc/475999217", "num_citations": "41\n", "authors": ["1609"]}
{"title": "Constructing induction rules for deductive synthesis proofs\n", "abstract": " We describe novel computational techniques for constructing induction rules for deductive synthesis proofs. Deductive synthesis holds out the promise of automated construction of correct computer programs from specifications of their desired behaviour. Synthesis of programs with iteration or recursion requires inductive proof, but standard techniques for the construction of appropriate induction rules are restricted to recycling the recursive structure of the specifications. What is needed is induction rule construction techniques that can introduce novel recursive structures. We show that a combination of rippling and the use of meta-variables as a least-commitment device can provide such novelty.", "num_citations": "40\n", "authors": ["1609"]}
{"title": "How to improve the reliability of expert systems\n", "abstract": " Reliability is likely to become an increasingly important issue for applied AI, as it already is for other branches of computer science. Without assurances of reliability, AI researchers will be restricted in the scale and scope of the expert systems they can build and will not be able to capitalize on their current success. Similar remarks hold for other application areas of AI. The key to greater reliability is a sound theoretical foundation for current and new AI techniques\u2014making basic AI a proper engineering science. Logic has a key role in providing the mathematical foundation for basic AI. We illustrate the kind of theoretical work that is required with a few case studies, in the areas of search control, fault diagnosis, and learning.* I am grateful for conversations on the topics of this chapter with, and/or feedback on the paper itself from, Richard O'Keefe, Aaron Sloman, Robert Inder, Roberto Desimone, Bill Sharpe, Alison Kidd, Josie Bundy, Mitch Harris, Liam Lynch, and Mike Uschold. I would also like to thank Bill Sharpe and Hewlett Packard for allowing me to write it in the ivory tower of their Bristol Research Laboratory, away from the hubbub of university life. Some of the work described here was funded by SERC grant GR/D/44874. An earlier version of this chapter was given as an invited talk at the Expert Systems 87 conference and appeared as (Bundy, 1987b). Additional material has also been added from my acceptance speech for the 1986 SPL Insight Award, which appeared as (Bundy, 1987a). I would like to thank Steve Torrance for suggesting this rearrangement of the texts.", "num_citations": "40\n", "authors": ["1609"]}
{"title": "The use of data-mining for the automatic formation of tactics\n", "abstract": " This paper discusses the usse of data-mining for the automatic formation of tactics. It was presented at the Workshop on Computer-Supported Mathematical Theory Development held at IJCAR in 2004. The aim of this project is to evaluate the applicability of data-mining techniques to the automatic formation of tactics from large corpuses of proofs. We data-mine information from large proof corpuses to find commonly occurring patterns. These patterns are then evolved into tactics using genetic programming techniques.", "num_citations": "38\n", "authors": ["1609"]}
{"title": "What is the difference between real creativity and mere novelty?\n", "abstract": " //static.cambridge.org/content/id/urn%3Acambridge.org%3Aid%3Aarticle%3AS0140525X0003572X/resource/name/firstPage-S0140525X0003572Xa.jpg", "num_citations": "38\n", "authors": ["1609"]}
{"title": "Solving symbolic equations with PRESS\n", "abstract": " We outline a program, PRESS (PRolog Equation Solving System) for solving symbolic, transcendental, non-differential equations. The methods used for solving equations are described, together with the service facilities. The principal technique, meta-level inference, appears to have applications in the broader field of symbolic and algebraic manipulation.", "num_citations": "35\n", "authors": ["1609"]}
{"title": "Automated discovery of inductive theorems\n", "abstract": " Inductive mathematical theorems have, as a rule, historically been quite difficult to prove\u2013both for mathematics students and for automated theorem provers. That said, there has been considerable progress over the past several years, within the automated reasoning community, towards proving some of these theorems. However, little work has been done thus far towards automatically discovering them. In this paper we present our methods of discovering (as well as proving) inductive theorems, within an automated system. These methods have been tested over the natural numbers, with regards to addition and multiplication, as well as to exponents of group elements.", "num_citations": "34\n", "authors": ["1609"]}
{"title": "Dynamic ontology refinement\n", "abstract": " One of the main reasons why plan execution meets with failure is because the environment in which the plan is being executed does not conform to an agent\u2019s expectations of how it will behave. If an agent is forming and attempting to execute plans in a given domain, these plans will be based on the agent\u2019s understanding of the domain, described in its ontology. Errors in this ontology are likely to lead to plans that are not executable. We propose to address this problem by dynamically refining the ontology as execution failure occurs and replanning using this updated ontology, thus creating more robust plans that are more likely to be executable.", "num_citations": "34\n", "authors": ["1609"]}
{"title": "Incidence calculus\n", "abstract": " We describe incidence calculus, a logic for probabilistic reasoning. In incidence calculus, probabilities are not directly associated with formulae. Rather sets of possible worlds are directly associated with formulae and probabilities are calculated from these. This enables incidence calculus to be truth functional, which a logic based on a purely numeric uncertainty measure cannot be. This, in turn, enables tighter probablity intervals to be calculated for theorems of an incidence calculus theory than is possible in a purely numeric uncertainty theory.", "num_citations": "34\n", "authors": ["1609"]}
{"title": "Proof plans for the correction of false conjectures\n", "abstract": " Theorem proving is the systematic derivation of a mathematical proof from a set of axioms by the use of rules of inference. We are interested in a related but far less explored problem: the analysis and correction of false conjectures, especially where that correction involves finding a collection of antecedents that, together with a set of axioms, transform non-theorems into theorems. Most failed search trees are huge, and special care is to be taken in order to tackle the combinatorial explosion phenomenon. Fortunately, the planning search space generated by proof plans, see [1], are moderately small. We have explored the possibility of using this technique in the implementation of an abduction mechanism to correct non-theorems.", "num_citations": "33\n", "authors": ["1609"]}
{"title": "Towards ontology evolution in physics\n", "abstract": " We investigate the problem of automatically repairing inconsistent ontologies. A repair is triggered when a contradiction is detected between the current theory and new experimental evidence. We are working in the domain of physics because it has good historical records of such contradictions and how they were resolved. We use these records to both develop and evaluate our techniques. To deal with problems of inferential search control and ambiguity in the atomic repair operations, we have developed ontology repair plans, which represent common patterns of repair. They first diagnose the inconsistency and then direct the resulting repair. Two such plans have been developed to repair ontologies that disagree over the value and the dependence of a function, respectively. We have implemented the repair plans in the galileo system and successfully evaluated galileo on a diverse range of examples\u00a0\u2026", "num_citations": "32\n", "authors": ["1609"]}
{"title": "A critique of proof planning\n", "abstract": " Proof planning is an approach to the automation of theorem proving in which search is conducted, not at the object-level, but among a set of proof methods. This approach dramatically reduces the amount of search but at the cost of completeness. We critically examine proof planning, identifying both its strengths and weaknesses. We use this analysis to explore ways of enhancing proof planning to overcome its current weaknesses.", "num_citations": "32\n", "authors": ["1609"]}
{"title": "Extensions to a generalization critic for inductive proof\n", "abstract": " In earlier papers a critic for automatically generalizing conjectures in the context of failed inductive proofs was presented. The critic exploits the partial success of the search control heuristic known as rippling. Through empirical testing a natural generalization and extension of the basic critic emerged. Here we describe our extended generalization critic together with some promising experimental results.", "num_citations": "32\n", "authors": ["1609"]}
{"title": "Solving symbolic equations with PRESS\n", "abstract": " We describe a program, PRESS, (PRolog Equation Solving System) for solving symbolic, transcendental, non-differential equations in one or more variables. PRESS solves autonomously, i.e. without guidance from the user. The methods used for solving equations are described, together with the service facilities. The principal technique, metal-level inference, appears to be relevant to the broader field of symbolic and algebraic manipulation.", "num_citations": "32\n", "authors": ["1609"]}
{"title": "Case-analysis for rippling and inductive proof\n", "abstract": " Rippling is a heuristic used to guide rewriting and is typically used for inductive theorem proving. We introduce a method to support case-analysis within rippling. Like earlier work, this allows goals containing if-statements to be proved automatically. The new contribution is that our method also supports case-analysis on datatypes. By locating the case-analysis as a step within rippling we also maintain the termination. The work has been implemented in IsaPlanner and used to extend the existing inductive proof method. We evaluate this extended prover on a large set of examples from Isabelle\u2019s theory library and from the inductive theorem proving literature. We find that this leads to a significant improvement in the coverage of inductive theorem proving. The main limitations of the extended prover are identified, highlight the need for advances in the treatment of assumptions during rippling and when\u00a0\u2026", "num_citations": "31\n", "authors": ["1609"]}
{"title": "Ascertaining mathematical theorems\n", "abstract": " Whereas to most logicians, the word \u201ctheorem\u201d refers to any statement which has been shown to be true, to mathematicians, the word \u201cTheorem\u201d is, relatively speaking, rarely applied, and denotes something far more special. In this paper, we examine some of the underlying reasons behind this difference in terminology, and we show how this discrepancy might be exploited, in order to build a computer system which automatically selects the latter type of \u201cTheorems\u201d from amongst the former. Indeed, we have begun building the automated discovery system MATHsAiD, the design of which is based upon our research. We provide some preliminary results produced by this system, and compare these results to Theorems appearing in various mathematics textbooks.", "num_citations": "31\n", "authors": ["1609"]}
{"title": "A recursive techniques editor for Prolog\n", "abstract": " We describe an editor geared to recursive Prolog procedures. It is similar to the structure editors built for many programming languages, except that instead of just ensuring the correctness of the syntax of the procedures built by the editor, the editor also ensures the correct use of recursion. By correct here we mean that the recursive procedure is guaranteed to terminate and to be well-defined. Within these constraints we have tried to ensure that the range of procedures that can be built is as complete as possible.", "num_citations": "31\n", "authors": ["1609"]}
{"title": "A critical survey of rule learning programs\n", "abstract": " We survey the rule learning programs of [Brazdil 81, Mitchell et al 81, Langley 81, Shapiro 81]. Each of these programs has two main parts: a critic for identifying faulty rules and a modifier for correcting them. To aid comparison we describe the techniques of the various authors using a uniformnotation. We find several similarities in the techniques used by the various authors and uncover the relations between them. In particular, the concept learning technique of [Young et al 77] is shown to subsume most of the rule modifying techniques. We also uncover some funnies in some of the research.", "num_citations": "31\n", "authors": ["1609"]}
{"title": "On some equivalence relations between incidence calculus and Dempster-Shafer theory of evidence\n", "abstract": " Incidence Calculus and Dempster-Shafer Theory of Evidence are both theories to describe agents' degrees of belief in propositions, thus being appropriate to represent uncertainty in reasoning systems. This paper presents a straightforward equivalence proof between some special cases of these theories.", "num_citations": "30\n", "authors": ["1609"]}
{"title": "Representation as a fluent: An AI challenge for the next half century\n", "abstract": " The authors argued that AI systems must be able to manipulate their own internal representations automatically to deal with an infinitely complex and ever-changing world and to scale up to rich, complex applications. Such manipulation must go beyond changing beliefs and learning new concepts in terms of old concepts; it must be able to change ontology's underlying syntax and semantics. Initial progress has been made, but further progress is urgently needed owing to the demands of autonomous multiagent systems. Understanding and implementing this ability must be a major focus of AI for the next 50 years", "num_citations": "30\n", "authors": ["1609"]}
{"title": "Automatic verification of functions with accumulating parameters\n", "abstract": " Proof by mathematical induction plays a crucial role in reasoning about functional programs. A generalization step often holds the key to discovering an inductive proof. We present a generalization technique which is particularly applicable when reasoning about functional programs involving accumulating parameters. We provide empirical evidence for the success of our technique and show how it is contributing to the ongoing development of a parallelizing compiler for Standard ML.", "num_citations": "30\n", "authors": ["1609"]}
{"title": "Attacking group protocols by refuting incorrect inductive conjectures\n", "abstract": " Automated tools for finding attacks on flawed security protocols often fail to deal adequately with group protocols. The reason is that the abstractions made to improve performance on fixed two- or three-party protocols either preclude the modeling of group protocols altogether or permit modeling only in a fixed scenario, which can prevent attacks from being discovered. This paper describes Coral, a tool for finding counterexamples to incorrect inductive conjectures, which we have used to model protocols for both group key agreement and group key management, without any restrictions on the scenario. We show how we used Coral to discover six previously unknown attacks on three group protocols.", "num_citations": "28\n", "authors": ["1609"]}
{"title": "Using a generalisation critic to find bisimulations for coinductive proofs\n", "abstract": " Coinduction is a method of growing importance in reasoning about functional languages, due to the increasing prominence of lazy data structures. Through the use of bisimulations and proofs that bisimilarity is a congruence in various domains it can be used to prove the congruence of two processes.             A coinductive proof requires a relation to be chosen which can be proved to be a bisimulation. We use proof planning to develop a heuristic method which automatically constructs a candidate relation. If this relation does not allow the proof to go through a proof critic analyses the reasons why it failed and modifies the relation accordingly.             Several proof tools have been developed to aid coinductive proofs but all require user interaction. Crucially they require the user to supply an appropriate relation which the system can then prove to be a bisimulation.", "num_citations": "28\n", "authors": ["1609"]}
{"title": "The use of proof plans for normalization\n", "abstract": " We propose using proof plans to implement expression normalizers in automatic theorem proving. We outline some general-purpose proof plans and show how these can be combined in various ways to yield some standard normalizers. We claim that using proof plans facilitates the flexible application of these normalizers so that they can interact with the theorem prover in which they are embedded. We intend to extend this technique to decision procedures.", "num_citations": "28\n", "authors": ["1609"]}
{"title": "A proposed Prolog story\n", "abstract": " A proposed prolog story \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation A proposed prolog story Alan Bundy, Helen Pain, P. Brna, L. Lynch School of Informatics Research output: Working paper Overview Original language English Publication status Unpublished - 1986 Publication series Name DAI Research Paper No. 283 Access to Document Full textAccepted author manuscript, 449 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A., Pain, H., Brna, P., & Lynch, L. (1986). A proposed prolog story. (DAI Research Paper No. 283). Bundy, Alan ; Pain, Helen ; Brna, P. ; Lynch, L. / A proposed prolog story. 1986. (DAI Research \u2026", "num_citations": "28\n", "authors": ["1609"]}
{"title": "Automated theorem provers: a practical tool for the working mathematician?\n", "abstract": " In contrast to the widespread use of computer algebra systems in mathematics automated theorem provers have largely met with indifference. There are signs that this is at last beginning to change. We argue that it is inevitable that automated provers will be adopted as a practical tool for the working mathematician. Mathematical applications of automated provers raises profound challenges for their developers.", "num_citations": "27\n", "authors": ["1609"]}
{"title": "Finding counterexamples to inductive conjectures and discovering security protocol attacks\n", "abstract": " We present an implementation of a method for finding counterexamples to universally quantified conjectures in first-order logic. Our method uses the proof by consistency strategy to guide a search for a counterexample and a standard first-order theorem prover to perform a concurrent check for inconsistency. We explain briefly the theory behind the method, describe our implementation, and evaluate results achieved on a variety of incorrect conjectures from various sources.Some work in progress is also presented: we are applying the method to the verification of cryptographic security protocols. In this context, a counterexample to a security property can indicate an attack on the protocol, and our method extracts the trace of messages exchanged in order to effect this attack. This application demonstrates the advantages of the method, in that quite complex side conditions decide whether a particular sequence of messages is possible. Using a theorem prover provides a natural way of dealing with this. Some early results are presented and we discuss future work.", "num_citations": "27\n", "authors": ["1609"]}
{"title": "Using middle-out reasoning to control the synthesis of tail-recursive programs\n", "abstract": " We describe a novel technique for the automatic synthesis of tail-recursive programs. The technique is to specify the required program using the standard equations and then synthesise the tail-recursive program using the proofs as programs technique. This requires the specification to be proved realisable in a constructive logic. Restrictions on the form of the proof ensure that the synthesised program is tail-recursive.             The automatic search for a synthesis proof is controlled by proof plans, which are descriptions of the high-level structure of proofs of this kind. We have extended the known proof plans for inductive proofs by adding a new form of generalisation and by making greater use of middle-out reasoning. In middle-out reasoning we postpone decisions in the early part of the proof by the use of meta-variables which are instantiated, by unification, during later parts of the proof. Higher order\u00a0\u2026", "num_citations": "27\n", "authors": ["1609"]}
{"title": "Models of interaction as a grounding for peer to peer knowledge sharing\n", "abstract": " Most current attempts to achieve reliable knowledge sharing on a large scale have relied on pre-engineering of content and supply services. This, like traditional knowledge engineering, does not by itself scale to large, open, peer to peer systems because the cost of being precise about the absolute semantics of services and their knowledge rises rapidly as more services participate. We describe how to break out of this deadlock by focusing on semantics related to interaction and using this to avoid dependency on a priori semantic agreement; instead making semantic commitments incrementally at run time. Our method is based on interaction models that are mobile in the sense that they may be transferred to other components, this being a mechanism for service composition and for coalition formation. By shifting the emphasis to interaction (the details of which may be hidden from users) we can obtain\u00a0\u2026", "num_citations": "26\n", "authors": ["1609"]}
{"title": "MATHsAiD: a mathematical theorem discovery tool\n", "abstract": " In the field of automated reasoning, one of the most challenging (even if perhaps, somewhat overlooked) problems thus far has been to develop a means of discerning, from amongst all the truths that can be discovered and proved, those which are either useful or interesting enough to be worth recording. As for human reasoning, mathematicians are well known for their predilection towards designating certain discoveries as theorems, lemmas, corollaries, etc., whilst relegating all others as relatively unimportant. However, precisely how mathematicians determine which results to keep, and which to discard, is perhaps not so well known. Nevertheless, this practice is an essential part of the mathematical process, as it allows mathematicians to manage what would otherwise be an overwhelming amount of knowledge. MATHsAiD is a system intended for use by research mathematicians, and is designed to produce\u00a0\u2026", "num_citations": "26\n", "authors": ["1609"]}
{"title": "Applying adversarial planning techniques to Go\n", "abstract": " Approaches to computer game playing based on \u03b1\u2013\u03b2 search of the tree of possible move sequences combined with a position evaluation function have been successful for many games, notably Chess. Such approaches are less successful for games with large search spaces and complex positions, such as Go, and we are led to seek alternatives. One such alternative is to model the goals of the players, and their strategies for achieving these goals. This approach means searching the space of possible goal expansions, typically much smaller than the space of move sequences. Previous attempts to apply these techniques to Go have been unable to provide results for anything other than a high strategic level or very open game positions. In this paper we describe how adversarial hierarchical task network planning can provide a framework for goal-directed game playing in Go which is also applicable both strategic\u00a0\u2026", "num_citations": "26\n", "authors": ["1609"]}
{"title": "Agent based cooperative theory formation in pure mathematics\n", "abstract": " The HR program, Colton et al.(1999), performs theory formation in domains of pure mathematics. Given only minimal information about a domain, it invents concepts, make conjectures, proves theorems and finds counterexamples to false conjectures. We present here a multi-agent version of HR which may provide a model for how individual mathematicians perform separate investigations but communicate their results to the mathematical community, learning from others as they do. We detail the exhaustive categorisation problem to which we have applied a multi-agent approach.", "num_citations": "26\n", "authors": ["1609"]}
{"title": "Artificial intelligence and musical cognition\n", "abstract": " There has been much interest, in recent years, in the possibility of representing our musical faculties in computational terms. A necessary first step is to develop a formally precise theory of musical structure, and to this end, useful analogies may be drawn between music and natural language. Metrical rhythms resemble syntactic structures in being generated by phrase-structure grammars; as for the pitch relations between notes, the tonal intervals of Western music form a mathematical group generated by the octave, the fifth and the third. On this theoretical foundation one can construct AI programs for the transcription, editing and performance of classical keyboard music. A high degree of complexity and precision is required for the faithful representation of a sophisticated pianoforte composition, and to achieve a satisfactory level of performance it is essential to respect the minute variations of loudness and timing by\u00a0\u2026", "num_citations": "26\n", "authors": ["1609"]}
{"title": "Automated Deduction, Cade-12.: 12th International Conference on Automated Deduction, Nancy, France, June 26-July 1, 1994. Proceedings.\n", "abstract": " This volume contains the reviewed papers presented at the 12th International Conference on Automated Deduction (CADE-12) held at Nancy, France in June/July 1994. The 67 papers presented were selected from 177 submissions and document many of the most important research results in automated deduction since CADE-11 was held in June 1992. The volume is organized in chapters on heuristics, resolution systems, induction, controlling resolutions, ATP problems, unification, LP applications, special-purpose provers, rewrite rule termination, ATP efficiency, AC unification, higher-order theorem proving, natural systems, problem sets, and system descriptions.", "num_citations": "26\n", "authors": ["1609"]}
{"title": "What kind of field is AI?\n", "abstract": " A. Bundy, What kind of field is AI? - PhilPapers Sign in | Create an account PhilPapers PhilPeople PhilArchive PhilEvents PhilJobs PhilPapers home Syntax Advanced Search Syntax Advanced Search Syntax Advanced Search What kind of field is AI? A. Bundy In Derek Partridge & Y. Wilks (eds.), The Foundations of Artificial Intelligence: A Sourcebook. Cambridge University Press (1990) Abstract This article has no associated abstract. (fix it) Keywords No keywords specified (fix it) Categories Philosophy of Artificial Intelligence, Miscellaneous in Philosophy of Cognitive Science The Nature of Artificial Intelligence in Philosophy of Cognitive Science (categorize this paper) Buy the book Find it on Amazon.com Options Edit this record Mark as duplicate Export citation Find it on Scholar Request removal from index Revision history Download options PhilArchive copy Upload a copy of this paper Check publisher's policy \u2026", "num_citations": "25\n", "authors": ["1609"]}
{"title": "A generalized interval package and its use for semantic checking\n", "abstract": " In this paper we describe a general interval package, that is, a computer program, called INT, in which arithmetic functions are extended so that they apply, not just to numbers, but to open and closed intervals of numbers. If f is an n-ary function, then it can be extended to sets of numbers (and hence, intervals) with the following definition:", "num_citations": "25\n", "authors": ["1609"]}
{"title": "A general setting for flexibly combining and augmenting decision procedures\n", "abstract": " The efficient combining and augmenting of decision procedures are often very important for a successful use of theorem provers. There are several schemes for combining and augmenting decision procedures; some of them support handling uninterpreted functions, use of available lemmas, and the like. In this paper we introduce a general setting for describing different schemes for both combining and augmenting decision procedures. This setting is based on the macro inference rules used in different approaches. Some of these rules are abstraction, entailment, congruence closure, and lemma invoking. The general setting gives a simple description and the key ideas of one scheme and makes different schemes comparable. Also, it makes easier combining ideas from different schemes. In this paper we describe several schemes via introduced macro inference rules and report on our prototype\u00a0\u2026", "num_citations": "24\n", "authors": ["1609"]}
{"title": "Special purpose, but domain independent, inference mechanisms\n", "abstract": " vie  describe  a  number  of  special  purpose,  but  domain  independent,  inference mechanisms.  While  these  rl1echanisms  are  limited  to  certain  kinds  of  inference  and illference  rules,  they  do  not  rely  on  special  properties  of  the  domain,  but  on logical  properties  of  predicates  and  rules,  ~Ihich  make  them  equally  applicable  to other  domains.  These  logical  properties  include:  transitivity,  functionality  and unarit.y.  The  union  of  the~e  mechanisms  handles  nearly  all  the  inference  required  in the  Mecho  project  for  solving mechanics  problems  stated  in  English.", "num_citations": "24\n", "authors": ["1609"]}
{"title": "MECHO: Year one\n", "abstract": " This is a progress report on the MECHO project originally announced in DAI Working Paper No. 8, Bundy, Luger and Stone, 1975. The project is to write a computer program which can solve mechanics problems stated in English. This is motivated by a desire to understand how it is possible to form a mathematical model of a real world situation. A problem, typical of those solved by our program, is given and the natural language analysis, equation extraction and the solution of the problem discussed.", "num_citations": "24\n", "authors": ["1609"]}
{"title": "Smart machines are not a threat to humanity\n", "abstract": " Worrying about machines that are too smart distracts us from the real and present threat from machines that are too dumb.", "num_citations": "23\n", "authors": ["1609"]}
{"title": "Proofs about lists using ellipsis\n", "abstract": " In this paper we explore the use of ellipsis in proofs about lists. We present a higher-order formulation of elliptic formulae, and describe its implementation in the \u03bbClam proof planner. We use an unambiguous higher-order formulation of lists which is amenable to formal proofs without using induction, and to display using the familiar ... notation.", "num_citations": "23\n", "authors": ["1609"]}
{"title": "An adversarial planning approach to Go\n", "abstract": " Approaches to computer game playing based on (typically \u03b1- \u03b2) search of the tree of possible move sequences combined with an evaluation function have been successful for many games, notably Chess. For games with large search spaces and complex positions, such as Go, these approaches are less successful and we are led to seek alternative approaches.               One such alternative is to model the goals of the players, and their strategies for achieving these goals. This approach means searching the space of possible goal expansions, typically much smaller than the space of move sequences.               In this paper we describe how adversarial hierarchical task network planning can provide a framework for goal-directed game playing, and its application to the game of Go.", "num_citations": "23\n", "authors": ["1609"]}
{"title": "Dr. Doodle: A diagrammatic theorem prover\n", "abstract": " This paper presents the Dr.Doodle system, an interactive theorem prover that uses diagrammatic representations. The assumption underlying this project is that, for some domains (principally geometry), diagrammatic reasoning is easier to understand than conventional algebraic approaches \u2013 at least for a significant number of people. The Dr.Doodle system was developed for the domain of metric-space analysis (a geometric domain, but traditionally taught using a dry algebraic formalism). Pilot experiments were conducted to evaluate its potential as the basis of an educational tool, with encouraging results.", "num_citations": "22\n", "authors": ["1609"]}
{"title": "The nature of mathematical proof\n", "abstract": " Mathematical proof is one of the highest intellectual achievements of humankind. It contains the deepest, most complex and most rigorous arguments of which we are capable.", "num_citations": "21\n", "authors": ["1609"]}
{"title": "A science of reasoning: Extended abstract\n", "abstract": " How can we understand reasoning in general and mathematical proofs in particular? It is argued that a high-level understanding of proofs is needed to complement the low-level understanding provided by Logic. A role for computation is proposed to provide this high-level understanding, namely by the association of proof plans with proofs. Criteria are given for assessing the association of a proof plan with a proof.", "num_citations": "20\n", "authors": ["1609"]}
{"title": "An ML editor based on proofs-as-programs\n", "abstract": " C/sup Y/NTHIA is a novel editor for the functional programming language ML in which each function definition is represented as the proof of a simple specification. Users of C/sup Y/NTHIA edit programs by applying sequences of high-level editing commands to existing programs. These commands make changes to the proof representation from which a new program is then extracted. The use of proofs is a sound framework for analysing ML programs and giving useful feedback about errors. Amongst the properties analysed within C/sup Y/NTHIA at present is termination. C/sup Y/NTHIA has been successfully used in the teaching of ML in two courses at Napier University, Scotland. C/sup Y/NTHIA is a convincing, real-world application of the proofs-as-programs idea.", "num_citations": "20\n", "authors": ["1609"]}
{"title": "A broader interpretation of logic in logic programming\n", "abstract": " A broader interpretation of logic in logic programming \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation A broader interpretation of logic in logic programming Alan Bundy School of Informatics Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution Overview Original language English Title of host publication Proceedings of the Fifth International Logic Programming Conference/ Fifth Symposium on Logic Programming Publication status Published - 1988 Access to Document Full textAccepted author manuscript, 563 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (1988). A broader \u2026", "num_citations": "20\n", "authors": ["1609"]}
{"title": "Programming tools for Prolog environments\n", "abstract": " The Prolog programmer programs in an environment which provides a number of debugging tools. There is often a mismatch between the way a programmer describes some perceived error and the way in which a debugging tool needs to be used. Worse, there are some problems which existing tools cannot tackle easily-if at all The main aim of the work described is to construct a coherent framework on which to base the design of programming tools. This paper describes a particular classification of programming errors. Error classification is then used to provide a natural description of the tools that can, or could, assist the programmer. Examples are given of useful tools which are not part of well known current Prolog implementations and suggestions are made as to how current tools can be improved to increase their utility.", "num_citations": "20\n", "authors": ["1609"]}
{"title": "Attacking group multicast key management protocols using CORAL\n", "abstract": " This paper describes the modelling of a two multicast group key management protocols in a firstorder inductive model, and the discovery of previously unknown attacks on them by the automated inductive counterexample finder Coral. These kinds of protocols had not been analysed in a scenario with an active intruder before. Coral proved to be a suitable tool for a job because, unlike most automated tools for discovering attacks, it deals directly with an open-ended model where the number of agents and the roles they play are unbounded. Additionally, Coral's model allows us to reason explicitly about lists of terms in a message, which proved to be essential for modelling the second protocol. In the course of the case studies, we also discuss other issues surrounding multicast protocol analysis, including identifying the goals of the protocol with respect to the intended trust model, modelling of the control conditions\u00a0\u2026", "num_citations": "19\n", "authors": ["1609"]}
{"title": "Increasing the versatility of heuristic based theorem provers\n", "abstract": " Heuristic based theorem proving systems typically impose a fixed ordering on the strategies which they embody. The ordering reflects the general experience of the system designer. As a consequence, there will exist a variety of specific instances where the fixed ordering breaks down. We present an approach which liberates such systems by introducing a more versatile framework for organising proof strategies.", "num_citations": "19\n", "authors": ["1609"]}
{"title": "The researchers' bible\n", "abstract": " Getting a Ph. D. or M. Phil is hard work. This document gives advice about various aspects of the task. Section 1 describes the problem {what is a thesis? Section 2 sets out the formal requirements of gaining a thesis. Sections 3 and 4 describe some of the pitfalls and hurdles which students have encountered. Sections 5 and 6 advise about choosing and then executing a research project. Sections 7, 8 and 9 deal with two of the three R's: reading and writing. Section 10 describes the examination process for a research degree, and how to cope with it. Finally, section 11 lists journals which accept AI material.", "num_citations": "19\n", "authors": ["1609"]}
{"title": "Discovery and reasoning in mathematics\n", "abstract": " We discuss the automation of mathematical reasoning, surveying the abilities displayed by human mathematicians and the computational techniques available for automating these abilities. We argue the importance of the simultaneous study of these techniques, because problems inherent in one technique can often be solved if it is able to interact with others.", "num_citations": "18\n", "authors": ["1609"]}
{"title": "The interaction of representation and reasoning\n", "abstract": " Automated reasoning is an enabling technology for many applications of informatics. These applications include verifying that a computer program meets its specification; enabling a robot to form a plan to achieve a task and answering questions by combining information from diverse sources, e.g. on the Internet, etc. How is automated reasoning possible? Firstly, knowledge of a domain must be stored in a computer, usually in the form of logical formulae. This knowledge might, for instance, have been entered manually, retrieved from the Internet or perceived in the environment via sensors, such as cameras. Secondly, rules of inference are applied to old knowledge to derive new knowledge. Automated reasoning techniques have been adapted from logic, a branch of mathematics that was originally designed to formalize the reasoning of humans, especially mathematicians. My special interest is in the way that\u00a0\u2026", "num_citations": "17\n", "authors": ["1609"]}
{"title": "Scheme-based synthesis of inductive theories\n", "abstract": " We describe an approach to automatically invent/explore new mathematical theories, with the goal of producing results comparable to those produced by humans, as represented, for example, in the libraries of the Isabelle proof assistant. Our approach is based on \u2018schemes\u2019, which are terms in higher-order logic. We show that it is possible to automate the instantiation process of schemes to generate conjectures and definitions. We also show how the new definitions and the lemmata discovered during the exploration of the theory can be used not only to help with the proof obligations during the exploration, but also to reduce redundancies inherent in most theory formation systems. We implemented our ideas in an automated tool, called IsaScheme, which employs Knuth-Bendix completion and recent automatic inductive proof tools. We have evaluated our system in a theory of natural numbers and a theory\u00a0\u2026", "num_citations": "17\n", "authors": ["1609"]}
{"title": "Attacking a protocol for group key agreement by refuting incorrect inductive conjectures\n", "abstract": " Automated tools for finding attacks on flawed security protocols often struggle to deal with protocols for group key agreement. Systems designed for fixed 2 or 3 party protocols may not be able to model a group protocol, or its intended security properties. Frequently, such tools require an abstraction to a group of fixed size to be made before the automated analysis takes place. This can prejudice chances of finding attacks on the protocol. In this paper, we describe Coral, our system for finding security protocol attacks by refuting incorrect inductive conjectures. We have used Coral to model a group key protocol in a general way. By posing inductive conjectures about the trace of messages exchanged, we can investigate novel properties of the protocol, such as tolerance to disruption, and whether it results in agreement on a single key. This has allowed us to find three distinct novel attacks on groups of size two\u00a0\u2026", "num_citations": "17\n", "authors": ["1609"]}
{"title": "Adversarial planning in complex domains\n", "abstract": " Most current planning research relies on the assumption of a benign domain containing only co-operative agents. There has been little work on applying modern planning techniques to adversarial domains where this assumption does not hold. This paper discusses the development of an HTN Adversarial Planning architecture and the issues involved in successfully applying such a goal driven approach in complex domains. The architecture was tested by using it to build a reasoning system name gobi able to plan for Go problems of the types commonly found in teaching books. gobi only has limited knowledge but still performs well on a signi cant subset of Go problems. The Results also opened up interesting new approaches to tackling the problem of programming for Go but this paper concentrates on the planning aspect.", "num_citations": "17\n", "authors": ["1609"]}
{"title": "An overview of Prolog debugging tools\n", "abstract": " In this paper we present an overview of the advances in debugging standard Prolog programs. The analysis offered is in terms of a classification of tools that provide different degrees of activity in the debugging process. Other possible dimensions of analysis are also outlined.", "num_citations": "17\n", "authors": ["1609"]}
{"title": "Attacking the asokan-ginzboorg protocol for key distribution in an ad-hoc bluetooth network using coral\n", "abstract": " We describe Coral, a counterexample finder for incorrect inductive conjectures. By devising a first-order version of Paulson's formalism for cryptographic protocol analysis, we are able to use Coral to attack protocols which may have an unbounded number of principals involved in a single run. We show two attacks we have found on the Asokan--Ginzboorg protocol for establishing a group key in an ad-hoc network of Bluetooth devices.", "num_citations": "16\n", "authors": ["1609"]}
{"title": "Proof planning\n", "abstract": " We describe proof planning, a technique for the global control of search in automatic theorem proving. A proof plan captures the common patterns of reasoning in a family of similar proofs and is used to guide the search for new proofs in this family. Proof plans are very similar to the plans constructed by plan formation techniques. Some differences are the nonpersistence of objects in the mathematical domain, the absence of goal interaction in mathematics, the high degree of generality of proof plans, the use of a metalogic to describe preconditions in proof planning and the use of annotations in formulae to guide search.", "num_citations": "16\n", "authors": ["1609"]}
{"title": "A comprehensive comparison between generalized incidence calculus and the Dempster-Shafer theory of evidence\n", "abstract": " Dealing with uncertainty problems in intelligent systems has attracted a lot of attention in the AI community. Quite a few techniques have been proposed. Among them, the Dempster-Shafer theory of evidence (DS theory) has been widely appreciated. In DS theory, Dempster's combination rule plays a major role. However, it has been pointed out that the application domains of the rule are rather limited and the application of the theory sometimes gives unexpected results. We have previously explored the problem with Dempster's combination rule and proposed an alternative combination mechanism in generalized incidence calculus. In this paper we give a comprehensive comparison between generalized incidence calculus and the Dempster-Shafer theory of evidence. We first prove that these two theories have the same ability in representing evidence and combining DS-independent evidence. We then show that\u00a0\u2026", "num_citations": "16\n", "authors": ["1609"]}
{"title": "Using matching in algebraic equation solving\n", "abstract": " This paper describes the use of powerful algebraic matching techniques for applying rewrite rules in equation solving. A matcher is presented that knows about the commutativlty and associativity of addition and multiplication, will provide defaults for missing summands and factors, and if necessary will solve algebraically for the value of pattern variables.", "num_citations": "16\n", "authors": ["1609"]}
{"title": "Cooperating reasoning processes: More than just the sum of their parts\n", "abstract": " Using the achievements of my research group over the last 30+ years, I provide evidence to support the following hypothesis:", "num_citations": "15\n", "authors": ["1609"]}
{"title": "2, 4, 6-Trinitrotoluene inhibits endothelial nitric oxide synthase activity and elevates blood pressure in rats\n", "abstract": " 2,4,6-Trinitrotoluene (TNT), which is widely used in explosives, is an important occupational and environmental pollutant. Human exposure to TNT has been reported to be associated with cardiovascular dysfunction, but the mechanism is not well understood. In this study, we examine the endothelial nitric oxide synthase (eNOS) activity and blood pressure value following TNT exposure. With a crude enzyme preparation, we found that TNT inhibited the enzyme activity of eNOS in a concentration-dependent manner (IC50 value=49.4\u00a0\u03bcM). With an intraperitoneal administration of TNT (10 and 30\u00a0mg/kg) to rats, systolic blood pressure was significantly elevated 1\u00a0h after TNT exposure (1.2- and 1.3-fold of that of the control, respectively). Under the conditions, however, experiments with the inducible NOS inhibitor aminoguanidine revealed that an adaptive response against hypertension caused by TNT occurs\u00a0\u2026", "num_citations": "15\n", "authors": ["1609"]}
{"title": "A framework for the flexible integration of a class of decision procedures into theorem provers\n", "abstract": " The role of decision procedures is often essential in theorem proving. Decision procedures can reduce the search space of heuristic components of a prover and increase its abilities. However, in some applications only a small number of conjectures fall within the scope of the available decision procedures. Some of these conjectures could in an informal sense fall \u2018just outside\u2019 that scope. In these situations a problem arises because lemmas have to be invoked or the decision procedure has to communicate with the heuristic component of a theorem prover. This problem is also related to the general problem of how to flexibly integrate decision procedures into heuristic theorem provers. In this paper we address such problems and describe a framework for the flexible integration of decision procedures into other proof methods. The proposed framework can be used in different theorem provers, for different\u00a0\u2026", "num_citations": "15\n", "authors": ["1609"]}
{"title": "System description: An interface between CLAM and HOL\n", "abstract": " The CLAM proof planner has been interfaced to the HOL interactive theorem prover to provide the power of proof planning to people using HOL for formal verification, etc. The interface sends HOL goals to CLAM for planning and translates plans back into HOL tactics that solve the initial goals. The project homepage can be found at http://www.cl.cam.ac.uk/Research/HVG/Clam.HOL/intro.html.", "num_citations": "15\n", "authors": ["1609"]}
{"title": "Lightweight formalisation in support of requirements engineering\n", "abstract": " Formal design supported by automated reasoning can help keep track of requirements\u2014a particular problem for large, detailed systems. Designers of system specifications are often constrained by codes of practice and must show not only that these have been observed but also demonstrate how that has been achieved. This is especially important in safety-critical systems where sections of the requirements will be regulations or guidelines. Using a \u201clightweight\u201d approach, where formal proofs are used to support rather than guarantee adherence to requirements, we have developed an interactive system for formalising and managing information in codes of practice from the offshore oil industry. As a design proceeds, relevant requirements are found automatically and checked before being notified to the designer with an accompanying explanation of whether or not they are currently satisfied. Progress in\u00a0\u2026", "num_citations": "15\n", "authors": ["1609"]}
{"title": "A comparison of decision procedures in Presburger arithmetic\n", "abstract": " It is part of the tradition and folklore of automated reasoning that the intractability of Cooper\u2019s decision procedure for Presburger integer arithmetic makes is too expensive for practical use. More than 25 years of work has resulted in numerous approximate procedures via rational arithmetic, all of which are incomplete and restricted to the quantifier-free fragment. In this paper we report on an experiment which strongly questions this tradition. We measured the performance of procedures due to Hodes, Cooper (and heuristic variants thereof which detect counterexamples), across a corpus of 10000 randomly generated quantifier-free Presburger formulae. The results are startling: a variant of Cooper\u2019s procedure outperforms Hodes\u2019 procedure, and is fast enough for practical use. These results contradict much perceived wisdom that decision procedures for integer arithmetic are too expensive to use in practice.", "num_citations": "15\n", "authors": ["1609"]}
{"title": "Relational rippling: A general approach\n", "abstract": " We propose a new version of rippling, called relational rippling. Rippling is a heuristic for guiding proof search, especially in the step cases of inductive proofs. Relational rippling is designed for representations in which value passing is by shared existential variables, as opposed to function nesting. Thus relational rippling can be used to guide reasoning about logic programs or circuits represented as relations. We give an informal motivation and introduction to relational rippling. More details, including formal de nitions and termination proofs can be found in the longer version of this paper, Bundy and Lombart, 1995].", "num_citations": "15\n", "authors": ["1609"]}
{"title": "Synthesis and transformation of logic programs from constructive, inductive proof\n", "abstract": " We discuss a technique which allows synthesis of logic programs in the \u201cproofs-as-programs\u201d paradigm [Constable 82]. Constructive, inductive proof is used to show that the specification of a program is realisable; elaboration of a proof gives rise to the synthesis of a program which realises it. We present an update on earlier ideas, and give examples of and justification for them. The work is presented as foundation for further work in proof planning, where we aim to synthesise not only programs, but good programs.", "num_citations": "15\n", "authors": ["1609"]}
{"title": "Using failure to guide inductive proof\n", "abstract": " Lemma discovery and generalization are two of the major hurdles in automating inductive proof. This paper addresses aspects of these related problems. We build upon rippling, a heuristic which plays a pivotal role in guiding inductive proof. Rippling provides a high-level explanation of how to control the search for a proof. We demonstrate how this high-level explanation can be exploited productively when a proof attempt fails. In particular we show how failure can be used to focus the search for lemmas and generalizations.", "num_citations": "15\n", "authors": ["1609"]}
{"title": "A higher order approach to ontology evolution in physics\n", "abstract": " A higher-order logical approach to ontology evolution is applied to examples in Physics. Based on this approach, a framework is proposed which includes a methodology for the formalisation of ontology evolution in higher-order logic and an implementation of this methodology in the theorem prover Isabelle. The proposed basic mechanisms for evolution are called ontology repair plans. These operate on ontologies formalised as contexts, i.e., as multiple logical theories. In such a setting, ontologies may contradict one another or introduce redundancies with respect to one another, without any of them containing logical contradictions or redundancies. When, though, an inconsistency or a redundancy between two or more ontologies becomes explicit, it may be resolved by the application of an ontology repair plan, as each plan compiles together a pattern for diagnosis and transformation rules for effecting a\u00a0\u2026", "num_citations": "14\n", "authors": ["1609"]}
{"title": "Using animation in diagrammatic theorem proving\n", "abstract": " Diagrams have many uses in mathematics, one of the most ambitious of which is as a form of proof. The domain we consider is real analysis, where quantification issues are subtle but crucial. Computers offer new possibilities in diagrammatic reasoning, one of which is animation. Here we develop animated rules as a solution to problems of quantification. We show a simple application of this to constraint diagrams, and also how it can deal with the more complex questions of quantification and generalisation in diagrams that use more specific representations. This allows us to tackle difficult theorems that previously could only be proved algebraically.", "num_citations": "14\n", "authors": ["1609"]}
{"title": "Proof planning for maintainable configuration systems\n", "abstract": " Configuration is a complex task generally involving varying measures of constraint satisfaction, optimization, and the management of soft constraints. Although many successful systems have been developed, these are often difficult to maintain and to generalize in rapidly changing domains. In this paper, we consider building intelligent knowledge-based systems with maintainability well to the fore in our requirements for such systems. We introduce two case studies: the initial proof of concept, which was in the domain of computer configuration, and a further field-tested study, the configuration of compressors. Central to our approach is the use of the proof planning technique, and the clean separation of different kinds of knowledge: factual, heuristic, and strategic.", "num_citations": "14\n", "authors": ["1609"]}
{"title": "Tutorial notes: reasoning about logic programs\n", "abstract": " We will see that reasoning about programs is an important software engineering tool. It can be used to improve the efficiency and reliability of computer programs. Such reasoning is particularly well suited to logic programs. In fact, the ease of reasoning with logic programs is one of their main advantages over programs in other languages.", "num_citations": "14\n", "authors": ["1609"]}
{"title": "Meta-level inference and program verification\n", "abstract": " In [Bundy and Sterling 81] we described how meta-level inference was useful for controlling search and deriving control information in the domain of algebra. Similar techniques are applicable to the verification of logic programs. A developing meta-language is described, and an explicit proof plan using this language is given. A program, IMPRESS, is outlined which executes this plan.", "num_citations": "14\n", "authors": ["1609"]}
{"title": "A study in the use of Event-B for system development from a software engineering viewpoint\n", "abstract": " In this chapter we state the motivation for using formal methods and we state our reasons about why formal methods should become more widely accepted. Our aim is to show the importance of formal methods and the difference that they can make to improve software development. Hence justify our purpose for finding patterns.", "num_citations": "13\n", "authors": ["1609"]}
{"title": "An alternative interpretation of the seven-pointed star on CBS 1766\n", "abstract": " An Alternative Interpretation of the Seven-Pointed Star on CBS 1766 (2007) | www.narcis.nl KNAW KNAW Narcis Back to search results VU University Amsterdam Publication An Alternative Interpretation of the Seven-Pointed Star on CBS 1766 (2007) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title An Alternative Interpretation of the Seven-Pointed Star on CBS 1766 Published in Nouvelles Assyriologiques Br\u00e8ves et Utilitaires, 2007(40). ISSN 0989-5671. Author Waerzeggers, CAH; Siebes, RM Publisher Antiquity and Archeology; Business Web and Media; Art and Culture, History, Antiquity; Business Web and Media Date issued 2007 Access Restricted Access Language English Type Article Publication https://research.vu.nl/en/publications/e21dd8eb-e374-492b-99... OpenURL Search this publication in (your) library Persistent Identifiers NBN urn:\u2026", "num_citations": "13\n", "authors": ["1609"]}
{"title": "The ECO browser\n", "abstract": " * Department of Artificial Intelligence, University of Edinburgh.** Department of Forestry and Natural Resources, University of Edinburgh.", "num_citations": "13\n", "authors": ["1609"]}
{"title": "Meta-level inference in algebra\n", "abstract": " We describe two uses of meta-level inference: to control the search for aproof; and to derive new control information, and illustrate them in the domain of algebraic equation solving. The derivation of control information is the main focus of the paper. It involves the proving of theorems in the Meta-Theory of Algebra. These proofs are guided by meta-meta-level inference. We are developing a meta-meta-language to descrj. be formulae, and proof plans, and have built a program, IMPRESS, which uses these plans to build a proof. IMPRESS will form part of a self improving algebra system.", "num_citations": "13\n", "authors": ["1609"]}
{"title": "Planning as deductive synthesis in intuitionistic linear logic\n", "abstract": " We describe a new formalisation in Isabelle/HOL of Intuitionistic Linear Logic and consider the support this provides for constructing plans using deductive synthesis of the proof terms. This representation of plans in linear logic provides a concise account of planning with sensing actions, allows the creation and deletion of objects, and solves the frame problem in an elegant way. Within this setting, we show how planning algorithms are implemented as search strategies within the proof assistant. This allows us to provide a flexible methodology for developing search strategies that is independent of soundness issues. This feature is illustrated in two ways. Firstly, following ideas from logic programming, we show how a significant symmetry in search, caused by context splitting, can be pruned by using a derived inference rule. Secondly, we show how domain specific constraints on synthesis are supported and how they can be used to find contingent or conformant plans.", "num_citations": "12\n", "authors": ["1609"]}
{"title": "Planning from rich ontologies through translation between representations\n", "abstract": " The richness and expressivity of standard ontology representations and the limitations on expressivity required by modern planners have resulted in a situation where it is hard for an agent both to have a rich ontology and be capable of efficient planning. We discuss how translation between different kinds of representation can allow an agent to have different versions of the same ontology, so that it can simultaneously meet different demands of expressivity. We introduce our ontology refinement system (ORS), in which these ideas are implemented.", "num_citations": "12\n", "authors": ["1609"]}
{"title": "Proof planning methods as schemas\n", "abstract": " A major problem in automated theorem proving is search control. Fully expanded proofs are generally built from a large number of relatively low-level inference steps, with the result that searching the space of possible proofs at this level is very expensive. Proof planning is a technique by which common proof techniques are encoded as schemas, which we call methods. Proofs built using methods tend to be short, because the methods encode relatively long sequences of inference steps, and to be understandable, because the user can recognise the mathematical techniques being applied. Proof critics exploit the high-level nature of proof plans to patch failed proof attempts. A mapping from proof planning methods and proof construction tactics provides a link between the proof planning meta-level and fully expansive (object-level) proofs. Extensive experiments with proof planning reveal that a knowledge-based approach to automating proof construction works, and has useful properties.", "num_citations": "12\n", "authors": ["1609"]}
{"title": "An editor for helping novices to learn Standard ML\n", "abstract": " This paper describes a novel editor intended as an aid in the learning of the functional programming language Standard ML. A common technique used by novices is programming by analogy whereby students refer to similar programs that they have written before or have seen in the course literature and use these programs as a basis to write a new program. We present a novel editor for ML which supports programming by analogy by providing a collection of editing commands that transform old programs into new ones. Each command makes changes to an isolated part of the program. These changes are propagated to the rest of the program using analogical techniques. We observed a group of novice ML students to determine the most common programming errors in learning ML and restrict our editor such that it is impossible to commit these errors. In this way, students encounter fewer bugs and so their\u00a0\u2026", "num_citations": "12\n", "authors": ["1609"]}
{"title": "The nature of AI principles\n", "abstract": " The nature of AI principles | The foundation of artificial intelligence---a sourcebook ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksThe foundation of artificial intelligence---a sourcebookThe nature of AI principles chapter The nature of AI principles Share on Authors: Alan Bundy View Profile , Stellan Ohlsson View Profile Authors Info & Affiliations Publication: The foundation of artificial intelligence---a sourcebookApril 1990 Pages 135\u2013154 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert has been successfully added and will be \u2026", "num_citations": "12\n", "authors": ["1609"]}
{"title": "Meta-level inference: Two applications\n", "abstract": " We describe two uses of meta-level inference: to control the search for a proof; and to derive new control information, and illustrate them in the domain of algebraic equation solving. The derivation of control information is the main focus of the paper. It involves the proving of theorems in the Meta-Theory of Algebra. These proofs are guided by meta-meta-level inference. We are developing a meta-meta-language to describe formulae, and proof plans, and have built a program, IMPRESS, which uses these plans to build a proof. We describe one such proof plan in detail. IMPRESS will form part of a self-improving algebra system.", "num_citations": "12\n", "authors": ["1609"]}
{"title": "Context-free grammar\n", "abstract": " A context-free grammar is a collection of context-free phrase structure rules. Each such rule names a constituent type and specifies a possible expansion thereof. The standard notation is:  where lhs names the constituent, and rhs1 through rhsn the expansion. Such rules are context-free rules because the expansion is unconditional \u2014 the environment of the constituent to be expanded is irrelevant.", "num_citations": "12\n", "authors": ["1609"]}
{"title": "Homogenization: Preparing Equations for Change of Unknown.\n", "abstract": " PRESS is a computer program for solving symbolic, transcendental, non-differential equations,[3]. We describe a new equation solving method, called Homogenization, which we have implemented in PRESS. Homogenization prepares equations, so that they can be solved by the Change of Unknown method. It does this by oausing different occurrences of the unknown to occur within identical subterms. The method has application, outside equation solving, to the problem of generalizing expressions.", "num_citations": "12\n", "authors": ["1609"]}
{"title": "MATHsAiD: Automated mathematical theory exploration\n", "abstract": " The aim of the MATHsAiD project is to build a tool for automated theorem-discovery; to design and build a tool to automatically conjecture and prove theorems (lemmas, corollaries, etc.) from a set of user-supplied axioms and definitions. No other input is required. This tool would, for instance, allow a mathematician to try several versions of a particular definition, and in a relatively small amount of time, be able to see some of the consequences, in terms of the resulting theorems, of each version. Moreover, the automatically discovered theorems could perhaps help the users to discover and prove further theorems for themselves. The tool could also easily be used by educators (to generate exercise sets, for instance) and by students as well. In a similar fashion, it might also prove useful in enabling automated theorem provers to dispatch many of the more difficult proof obligations arising in software verification\u00a0\u2026", "num_citations": "11\n", "authors": ["1609"]}
{"title": "Getting to know your card: reverse-engineering the smart-card application protocol data unit\n", "abstract": " Smart-cards are considered to be one of the most secure, tamper-resistant, and trusted devices for implementing confidential operations, such as authentication, key management, encryption and decryption for financial, communication, security and data management purposes. The commonly used RSA PKCS# 11 standard defines the Application Programming Interface for cryptographic devices such as smart-cards. Though there has been work on formally verifying the correctness of the implementation of PKCS# 11 in the API level, little attention has been paid to the low-level cryptographic protocols that implement it.", "num_citations": "11\n", "authors": ["1609"]}
{"title": "English summaries of mathematical proofs\n", "abstract": " Automated theorem proving is becoming more important as the volume of applications in industrial and practical research areas increases. Due to the formalism of theorem provers and the massive amount of information included in machineoriented proofs, formal proofs are difficult to understand without specific training. A verbalisation system, ClamNL, was developed to generate English text from formal representations of inductive proofs, as produced by the Clam proof planner. The aim was to generate natural language proofs that resemble the presentation of proofs found in mathematical textbooks and that contain only the mathematically interesting parts of the proof.", "num_citations": "11\n", "authors": ["1609"]}
{"title": "Diagnosing and repairing ontological mismatches\n", "abstract": " The development of the semantic web ensures that the facilitation of agent communication is an issue of increasing importance. It is usually assumed that agents are using the same ontology and hence can understand one another, but the dynamic and distributed nature of the semantic web mean that this is not always a valid assumption. In this paper, we describe a system under construction that can dynamically discover ontological mismatches between agents during communication and then refine them, so that communication between these agents is facilitated.", "num_citations": "11\n", "authors": ["1609"]}
{"title": "Automation of diagrammatic reasoning\n", "abstract": " Theorems in automated theorem proving are usually proved by logical formal proofs. However, there is a subset of problems which humans can prove in a different way by the use of geometric operations on diagrams, so called diagrammatic proofs. Insight is more clearly perceived in these than in the corresponding algebraic proofs: they capture an intuitive notion of truthfulness that humans find easy to see and understand. We are identifying and automating this diagrammatic reasoning on mathematical theorems. The user gives the system, called DIAMOND, a theorem and then interactively proves it by the use of geometric manipulations on the diagram. These operations are the\" inference steps\" of the proof. DIAMOND then automatically derives from these example proofs a generaJised proof. The constructive w-rule is used as a mathematical basis to capture the generality of inductive diagrammatic proofs. In this way, we explore the relation between diagrammatic and algebraic proofs.", "num_citations": "11\n", "authors": ["1609"]}
{"title": "Raising the standard of AI products\n", "abstract": " We propose a mechanism for the promotion of  high-standards in commercial Artificial Intelligence products, namely an association of companies which would regulate their own membership using a code  of practice and the precedents set by previous  cases. Membership would provide some assurance of  quality. We argue the benefits of such a  mechanism, and discuss some of the details including  the proposal of a code of practice. This paper is  intended as a vehicle for discussion rather than as  the presentation of a definitive solution.", "num_citations": "11\n", "authors": ["1609"]}
{"title": "Meta-level inference and consciousness\n", "abstract": " Of all the mental phenanena, exhibited by the human mjnd, none seems to offer mo~ of a challenge to AI modelljng than conscjousness. So much so that it is ra~ ly d jscussed (I would say unconsc jously avoided) jn AI. Th is pape r add~ sses the question\" Can AI shed ljght on the phenanena of conscjousness?\". The question can be fruitfully djvjded into two subquestions. a) Can AI modelling help us to understand the role of consciousness cognition? in (b) Could we build a canputer program which experienced the sensation of consciousness?", "num_citations": "11\n", "authors": ["1609"]}
{"title": "Typed meta-interpretive learning for proof strategies.\n", "abstract": " Formal verification of computer programs is increasingly used in industry. A popular technique is interactive theorem proving, used for instance by Intel in HOL light. The ability to learn and re-apply proof strategies from a small set of proofs would significantly increase the productivity of these systems, and make them more cost-effective to use. Previous learning attempts have had limited success, which we believe is a result of missing key goal properties in the strategies. Capturing such properties requires predicate invention, and the only state-of-the-art ILP technique which supports this is meta-interpretive learning (MIL). We show that MIL is applicable to this problem, but that without type information it offers limited improvements in quality over previous work. We then extend MIL with types and give preliminary results indicating that this extension learns better-quality strategies with suitable goal properties. We also show that the quality of the learned strategies can be further enhanced through the use of dependent learning.", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Harnessing the power of folksonomies for formal ontology matching on-the-fly\n", "abstract": " This paper is a short introduction to our work on building and using folksonomies to facilitate communication between Semantic Web agents with disparate ontological representations. We briefly present the Semantic Matcher, a system that measures the semantic proximity between terms in interacting agents' ontologies at run-time, fully automatically and minimally: that is, only for semantic mismatches that impede communication. The system is designed to allow agents to\" understand\" the meanings of terms to be matched by comparing their folksonomy-based\" mental representations\".", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Dynamic rippling, middle-out reasoning and lemma discovery\n", "abstract": " We present a succinct account of dynamic rippling, a technique used to guide the automation of inductive proofs. This simplifies termination proofs for rippling and hence facilitates extending the technique in ways that preserve termination. We illustrate this by extending rippling with a terminating version of middle-out reasoning for lemma speculation. This supports automatic speculation of schematic lemmas which are incrementally instantiated by unification as the rippling proof progresses. Middle-out reasoning and lemma speculation have been implemented in higher-order logic and evaluated on typical libraries of formalised mathematics. This reveals that, when applied, the technique often finds the needed lemmas to complete the proof, but it is not as frequently applicable as initially expected. In comparison, we show that theory formation methods, combined with simpler proof methods, offer an effective\u00a0\u2026", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Planning and patching proof\n", "abstract": " We describe proof planning: a technique for both describing the hierarchical structure of proofs and then using this structure to guide proof attempts. When such a proof attempt fails, these failures can be analyzed and a patch formulated and applied. We also describe rippling: a powerful proof method used in proof planning. We pose and answer a number of common questions about proof planning and rippling.", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Automatic guidance of program synthesis proofs\n", "abstract": " Automatic guidance of program synthesis proofs \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Automatic guidance of program synthesis proofs Alan Bundy School of Informatics Research output: Working paper \u203a Discussion paper Overview Original language English Publication status Unpublished - 2000 Access to Document Full textSubmitted manuscript, 168 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (2000). Automatic guidance of program synthesis proofs. Bundy, Alan. / Automatic guidance of program synthesis proofs. 2000. Bundy, A 2000 'Automatic guidance of program synthesis proofs'. Automatic guidance of \u2026", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Cross domain mathematical concept formation\n", "abstract": " Many interesting concepts in mathematics are essentially \"cross-domain\" in nature, relating objects from more than one area of mathematics, e.g. prime order groups. These concepts are often vital to the formation of a mathematical theory. Often, the introduction of cross-domain concepts to an investigation seems to exercise a mathematician's creative ability. The HR program, (Colton, 1999), proposes new concepts in mathematics. Its original implementation was limited to working in one mathematical domain at a time, so it was unable to create cross-domain concepts. Here, we describe an extension of HR to multiple domains. Cross-domain concept formation is facilitated by generalisation of the data structures and heuristic measures employed by the program, and the implementation of a new production rule. Results achieved include generation of the concepts of prime order groups, graph nodes of maximal degree and an interesting class of graph.", "num_citations": "10\n", "authors": ["1609"]}
{"title": "The use of proof planning for co-operative theorem proving\n", "abstract": " We describe barnacle: a co-operative interface to the clam inductive theorem proving system. For the foreseeable future, there will be theorems which cannot be proved completely automatically, so the ability to allow human intervention is desirable; for this intervention to be productive the problem of orienting the user in the proof attempt must be overcome. There are many semi-automatic theorem provers: we call our style of theorem provingco-operative, in that the skills of both human and automaton are used each to their best advantage, and used together may find a proof where other methods fail. The co-operative nature of the barnacle interface is made possible by the proof planning technique underpinning clam. Our claim is that proof planning makes new kinds of user interaction possible. Proof planning is a technique for guiding the search for a proof in automatic theorem proving. Common patterns of\u00a0\u2026", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Altering the description space for focussing\n", "abstract": " This paper touches on the issue of how to program the acquisition of a concept. Although it is a technical essay in search heuristics, contributions are made, at least indirectly, to logic, psychology, and the theory of automated learning. In learning a concept, the learning program positions markers on a tree structure, which maps the space of categorical relations, in such a way that the markers converge, resulting in the appropriate terminal position. The process of \u201cfocusing\u201d(as it is termed) has the program raise or lower the markers in response to examples and counter-examples. But this heuristic of focusing is limited and sometimes produces contradictions as the program fails to fit an example, which it ought to be able to classify, or classifies a counter-example, which ought not to fit. The description tree needs to be reorganized or, in the authors' term,\u201chacked.\u201d The paper is a contribution to providing a step-by-step\u00a0\u2026", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Linear predictive coding\n", "abstract": " If one approximates the vocal tract as a series of fixed length tubes (which is equivalent to representing it as an all-pole digital filter) it becomes possible to predict successive samples of the speech wave as linear combinations of previous samples. The coefficients in the linear combination characterize the shape of the vocal tract. A sequence of sets of coefficients can be used to characterize the changing shape of the vocal tract over time. This representation is widely used because of the particularly efficient algorithms associated with it.", "num_citations": "10\n", "authors": ["1609"]}
{"title": "Automating change of representation for proofs in discrete mathematics (extended version)\n", "abstract": " Representation determines how we can reason about a specific problem. Sometimes one representation helps us to find a proof more easily than others. Most current automated reasoning tools focus on reasoning within one representation. There is, therefore, a need for the development of better tools to mechanise and automate formal and logically sound changes of representation. In this paper we look at examples of representational transformations in discrete mathematics, and show how we have used tools from Isabelle\u2019s Transfer package to automate the use of these transformations in proofs. We give an overview of a general theory of transformations that we consider appropriate for thinking about the matter, and we explain how it relates to the Transfer package. We show a few reasoning tactics we developed in Isabelle to improve the use of transformations, including the automation of search in the\u00a0\u2026", "num_citations": "9\n", "authors": ["1609"]}
{"title": "A very mathematical dilemma\n", "abstract": " The Annual Boole Lecture was established and is sponsored by the Boole Centre for Research in Informatics, the Cork Constraint Computation Centre, the Department of Computer Science, and the School of Mathematics, Applied Mathematics and Statistics, at University College Cork. The series in named in honour of George Boole, the first professor of Mathematics at UCC, whose seminal work on logic in the mid-1800s is central to modern digital computing. To mark this great contribution, leaders in the field of computing and mathematics are invited to talk to the general public on directions in science, on past achievements and on visions for the future.", "num_citations": "9\n", "authors": ["1609"]}
{"title": "Merging stories with shallow semantics\n", "abstract": " We demonstrate a proof-of-concept system that uses a shallow chunking-based technique for knowledge extraction from natural language text, in particular looking at the task of story understanding. This technique is extended with a reasoning engine that borrows techniques from dynamic ontology refinement to discover the semantic similarity of stories and to merge them together.", "num_citations": "9\n", "authors": ["1609"]}
{"title": "Planning proofs of equations in CCS\n", "abstract": " Most efforts to automate formal verification of communicating systems have centred around finite-state systems (FSSs). However, FSSs are incapable of modelling many practical communicating systems, including a novel class of problems, which we call VIPS. VIPSs are value-passing, infinite-state, parameterised systems. Existing approaches using model checking over FSSs are insufficient for VIPSs. This is due to their inability both to reason with and about domain-specific theories, and to cope with systems having an unbounded or arbitrary state space.               We use the Calculus of Communicating Systems (CCS) (Communication and Concurrency. London: Prentice Hall, 1989) to express and specify VIPSs. We take program verification to be proving the program and its intended specification equivalent. We use the laws of CCS to conduct the verification task. This approach allows us to study\u00a0\u2026", "num_citations": "9\n", "authors": ["1609"]}
{"title": "The method of assigning incidences\n", "abstract": " Incidence calculus is a probabilistic logic in which incidences, standing for the situations in which formulae may be true, are assigned to some formulae, and probabilities are assigned to incidences. However, numerical values may be assigned to formulae directly without specifying the incidences. In this paper, we propose a method of discovering incidences under these circumstances which produces a unique output comparing with the large number of outputs from other approaches. Some theoretical aspects of this method are thoroughly studied and the completeness of the result generated from it is proved. The result can be used to calculate mass functions from belief functions in the Dempster-Shafer theory of evidence (DS theory) and define probability spaces from inner measures (or lower bounds) of probabilities on the relevant propositional language set.", "num_citations": "9\n", "authors": ["1609"]}
{"title": "The combination of different pieces of evidence using incidence calculus\n", "abstract": " Combining multiple sources of information is a major and difficult task in the management of uncertainty. Dempster's combination rule is one of the attractive approaches. However, many researchers have pointed out that the application domains of the rule are rather limited and it sometimes gives unexpected results. In this paper, we have further explored the nature of combination and achieved the following main results. 1). The condition of combination in Dempster's original combination framework is more strict than that required by Dempster's combination rule in Dempster-Shafer theory of evidence. 2). Some counterintuitive results of using Dempster's combination rule shown in some papers are caused by the overlooking (or ignorance) of different independence conditions required by Dempster's original combination framework and Dempster's combination rule. 3). In Dempster's combination rule, combinations are performed at the target information level. This rule itself does not provide a c...", "num_citations": "9\n", "authors": ["1609"]}
{"title": "Building abstractions\n", "abstract": " The use of abstraction has been largely informal. As a consequence, it has often been difficult to see how or why a particular abstraction works. This paper attempts to help correct this trend by presenting a formal theory of abstraction. We use this theory to characterise the different types of abstraction that can be built; the different classes of abstractions we identify capture the majority of abstractions of which we are aware. We end by proposing a method for automatically building one very common type of abstraction, that used in Abstrips; our proposal is motivated by consideration of the various formal properties that such a method should possess.", "num_citations": "9\n", "authors": ["1609"]}
{"title": "The nature of AI: a reply to Schank\n", "abstract": " In this article in the AI magazine, vol. IV, no. 1, The Current State of AI: One Man's Opinion,\" Roger Schank puts forward various views on the nature of AI. In fact, there are enough opinions for four men. That is, the views advanced are contradictory. I agree with one of the Roger Schanks, and disagree with the other three. Schank hoped that his article would start a debate on the issues he raised.", "num_citations": "9\n", "authors": ["1609"]}
{"title": "Automated reasoning in the age of the internet\n", "abstract": " The internet hosts a vast store of information that we cannot and should not ignore. It\u2019s not enough just to retrieve facts. To make full use of the internet we must also infer new information from old. This is an exciting new opportunity for automated reasoning, but it also presents new kinds of research challenge.                                                                                There are a huge number of potential axioms from which to infer new theorems. Methods of choosing appropriate axioms are needed.                                                                 Information is stored on the Internet in diverse forms, e.g., graph and relational databases, JSON (JavaScript Object Notation), CSV (Comma-Separated Values) files, and many others. Some contain errors and others are incomplete: lacking vital contextual details such as time and units of measurements.                                                                 Information retrieved from the Internet must be\u00a0\u2026", "num_citations": "8\n", "authors": ["1609"]}
{"title": "Improving Dynamic Information Exchange in Emergency Response Scenarios.\n", "abstract": " Emergency response scenarios are characterized by the participation of multiple agencies, which cooperate to control the situation and restore normality. These agencies can come from diverse areas of expertise which entails that they represent knowledge differently, using their own vocabularies and terminologies. This fact complicates the automation of the information-sharing process, creating problems such as ambiguity or specialisation. In this paper we present an approach to tackle these problems by domain-aware semantic matching. This method requires the formalisation of domain-specific terminologies which will be added to an existing system oriented to emergency response. Concretely, we have formalised terms from the UK Civil and Protection Terminology lexicon, which gathers some of the most common terms that UK agencies use in these scenarios.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "Functional inferences over heterogeneous data\n", "abstract": " The increasing availability of knowledge bases (KBs) on the web has opened up the possibility of improved inference in automated query answering (QyA) systems. We have developed a rich inference framework (RIF) that responds to queries where no suitable answer is readily contained in any available data source, by applying functional inferences over heterogeneous data from the web. Our technique combines heuristics, logic and statistical methods to infer novel answers to queries. It also determines what facts are needed for inference, searches for them, and then integrates these diverse facts and their formalisms into a local query-specific inference tree. We explain the internal representation of RIF, the grammar and inference methods for expressing queries and the algorithm for inference. We also show how RIF estimates confidence in its answers, given the various forms of uncertainty faced by the\u00a0\u2026", "num_citations": "8\n", "authors": ["1609"]}
{"title": "A statistical relational learning challenge\u2013extracting proof strategies from exemplar proofs\n", "abstract": " Proof automation is becoming a bottleneck in both mechanised mathematics and particularly industrial use of formal development methods. In this challenge paper, we argue that statistical relational learning can be used to discover and extract proof strategies, which can aid with this automation. We outline what we believe are key problems and possible approaches to solve these questions.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "Verified planning by deductive synthesis in intuitionistic linear logic\n", "abstract": " We describe a new formalisation in Isabelle/HOL of Intuitionistic Linear Logic and consider the support this provides for constructing plans by proving the achievability of given planning goals. The plans so found are provably correct, by construction. This representation of plans in linear logic provides a concise account of planning with sensing actions, allows the creation and deletion of objects, and solves the frame problem in an elegant way. Within this setting, we show how planning algorithms are implemented as search strategies within a theorem proving system. This allows us to provide a flexible methodology for developing search strategies that is independent of soundness issues. This feature is illustrated in two ways. Firstly, following ideas from logic programming, we show how a significant symmetry in search, caused by context splitting, can be pruned by using a derived inference rule. Secondly, we show how domain specific constraints on synthesis are supported and how they can be used to find contingent or conformant plans. We illustrate the approach with example planning scenarios.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "On Repairing Reasoning Reversals via Representational Refinements.\n", "abstract": " Representation is a fluent. A mismatch between the real world and an agent\u2019s representation of it can be signalled by unexpected failures (or successes) of the agent\u2019s reasoning. The \u2018real world\u2019may include the ontologies of other agents. Such mismatches can be repaired by refining or abstracting an agent\u2019s ontology. These refinements or abstractions may not be limited to changes of belief, but may also change the signature of the agent\u2019s ontology. We describe the implementation and successful evaluation of these ideas in the ORS system. ORS diagnoses failures in plan execution and then repairs the faulty ontologies. Our automated approach to dynamic ontology repair has been designed specifically to address real issues in multi-agent systems, for instance, as envisaged in the Semantic Web.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "Making a productive use of failure to generate witnesses for coinduction from divergent proof attempts\n", "abstract": " Coinduction is a proof rule. It is the dual of induction. It allows reasoning about non-well-founded structures such as lazy lists or streams and is of particular use for reasoning about equivalences. A central difficulty in the automation of coinductive proof is the choice of a relation (called a bisimulation).               We present an automation of coinductive theorem proving. This automation is based on the idea of proof planning [7]. Proof planning constructs the higher level steps in a proof, using knowledge of the general structure of a family of proofs and exploiting this knowledge to control the proof search. Part of proof planning involves the use of failure information to modify the plan by the use of a proof critic [23] which exploits the information gained from the failed proof attempt.               Our approach to the problem was to develop a strategy that makes an initial simple guess at a bisimulation and then uses\u00a0\u2026", "num_citations": "8\n", "authors": ["1609"]}
{"title": "Automating the synthesis of decision procedures in a constructive metatheory\n", "abstract": " We present an approach to the automatic construction of decision procedures, via a detailed example in propositional logic. The approach adapts the methods of proof\u2010planning and the heuristics for induction to a new domain, that of metatheoretic procedures. This approach starts by providing an alternative characterisation of validity; the proofs of the correctness and completeness of this characterisation, and the existence of a decision procedure, are then amenable to automation in the way we describe. In this paper we identify a set of principled extensions to the heuristics for induction needed to tackle the proof obligations arising in the new problem domain and discuss their integration within the clam\u2010Oyster system.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "Planning and proof planning\n", "abstract": " The paper adresses proof planning as a specific AI planning. It describes some peculiarities of proof planning and discusses some possible cross-fertilization of planning and proof planning.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "A subsumption architecture for theorem proving?\n", "abstract": " Brooks has criticized traditional approaches to artificial intelligence as too inefficient. In particular, he has singled out techniques involving search as inadequate to achieve the fast reaction times required by robots and other AI products that need to work in the real world. Instead he proposes the subsumption architecture as an overall organizing principle. This consists of layers of behavioural modules, each of which is capable of carrying out a complete (usually simple) task. He has employed this architecture to build a series of simple mobile robots, but he claims that it is appropriate for all AI products. Brooks\u2019s proposal is usually seen as an example of nouvelle AI, in contrast to good old-fashioned AI (GOFAl). Automatic theorem proving is the archetypal example of GOFAl. The resolution theorem proving technique once served as the engine of AI. Of all areas of AI it seems the most difficult to implement using Brooks\u00a0\u2026", "num_citations": "8\n", "authors": ["1609"]}
{"title": "CHI\n", "abstract": " CHI is an integrated environment of knowledge-based tools <201> that assist with various aspects of the process of building computing systems. These tools include: a very-high-level, wide spectrum language named V used to express not only software specifications but also programming knowledge; a program transformation component and a set of rules (in V) for program synthesis, including data structure generation; a constraint maintenance system; and a program/knowledge editor and debugger. Designed but not yet fully implemented for the CHI system are: support for project management and integrated communication facilities for documentation, bug reports, message sending, etc.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "Expert Systems in the Micro-electronic Age\n", "abstract": " In this paper we shail describe a program (MECHO), written in Prolog [14], which solves a wide range of mechanics problems from statements in both predicate calculus and English. MECHO uses the technique of meta-level inference to Control search in natural language understanding, common sense inference, model formation and algebraic manipulation. We argue that this is a powerful technique for controlling search while retaining the modularity of a declarative knowledge representation.", "num_citations": "8\n", "authors": ["1609"]}
{"title": "ABC Repair System for Datalog-like Theories.\n", "abstract": " This paper aims to develop a domain-independent system for repairing faulty Datalog-like theories by combining three existing techniques: abduction, belief revision and conceptual change. Accordingly, the proposed system is named the ABC repair system. Given an observed assertion and a current theory, abduction adds axioms which represent the simplest and most likely explanation. Belief revision incorporates a new piece of information which conflicts with the input theory by deleting axioms. Conceptual change uses the reformation algorithm for blocking unwanted proofs or unblocking wanted proofs. The former two techniques change an axiom as a whole, while reformation changes the language in which the theory is written. These three techniques are complementary: abduction adds new axioms, belief revision deletes conflicting axioms, while reformation changes the language of the theory. But they have not previously been combined into one system. We are working on aligning these three techniques in the ABC repair system, which is capable of repairing logical theories with better quality than individual techniques. Datalog is used as the underlying logic of theories in this paper, but the proposed system has the potential to be adapted to theories in other logics.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Reformation: A domain-independent algorithm for theory repair\n", "abstract": " We describe reformation, a new algorithm for the automated repair of faulty logical theories. A fault is revealed by a reasoning failure: either the proof of a false conjecture or the failure to prove a true conjecture. Repair suggestions are systematically extracted via analysis of (un) successful unifications of formulae in (broken) proofs. These suggestions will either unblock a wanted but unsuccessful unification attempt or block an unwanted but successful unification. In contrast to traditional abduction and belief revision mechanisms, the repairs are to the language of the theory as well as to the axioms. The intention is that the language repairs suggested by reformation complement these axiom deleting and adding repairs, adding to the overall capability of theory repair and evolution. Reformation is self-inverse in that any blocking repair can be undone by an unblocking one, and vice versa. This self-inverse property provides some assurance that reformation repairs are minimal.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Dynamic data sharing for facilitating communication during emergency responses.\n", "abstract": " This paper describes the CHAIn system, which is designed to facilitate data sharing between disparate organisations during emergency response situations. It uses structured data matching to reformulate failed queries in cases where these queries failed because of incompatibilities between the query and the schema of the queried datasource. This reformulation is done by developing matches between the schema according to which the query is written and the schema of the queried datasource. These matches are then used to reformulate the query and retrieve responses relevant to those expected by the original query. Despite the growing interest in intelligent query answering, we believe that integration of data matching into query answering is novel, and allows users to successfully query datasources even if they do not know how the data in that source is organized, which is often necessary during emergency responses. We describe the proof-of-concept system we have developed and the encouraging evaluation we have so far carried out.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Using the method of fibres in Mecho to calculate radii of gyration\n", "abstract": " This chapter describes recent developments in the Mecho project. The aim of the project is to build a computer program, Mecho, which solves Mechanics prob lems stated in English, in order to understand how it is possible for experienced mathematicians to form a formal specification of a problem (eg equations) from an informal specification (eg English sentences). We hope that building this computer program will serve as a vehicle for gaining a detailed understanding of the processes involved, and that this enhanced understanding will suggest im provements in the teaching of Mechanics. A summary of the project can be found in (Bundy, Byrd, Luger, Mellish, Milne, & Palmer, 1979).ABSTRACT", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Why Ontology Evolution is Essential in Modeling Scientific Discovery.\n", "abstract": " We can model scientific discovery as automated reasoning and learning, eg, using a logic-based representation of knowledge, which we will here call an ontology. Much of what Kuhn calls \u201cnormal science\u201d may be modelled as problem solving within the shared ontology of a scientific community (Kuhn 1970). However, to model what Kuhn calls a \u201cparadigm shift\u201d, we need mechanisms for changing this ontology. This is what W3C call ontology evolution1. As we will see, ontology evolution can also happen during \u201cnormal science\u201d. Moreover, ontology evolution requires not just belief revision, but changes to the underlying signature of the ontology. For instance, functions might be split or merged; new arguments might be added to or removed from them.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Inconstancy: An Ontology Repair Plan for Adding Hidden Variables.\n", "abstract": " We describe mechanisms for automated evolution of ontologies to adapt to new circumstances and to make them better suited to the given task. If a conflict is detected between the original theory and new experimental evidence, a repair is required to resolve the inconsistency and to recover from failure. The rules for conflict diagnosis and transformation of the ontologies are composed together into ontology repair plans. The repair plans have been implemented in the GALILEO system and successfully evaluated on a diverse range of examples from the history of physics. By applying the described repair plans, the initially incorrect physical theories in our examples are repaired to become consistent with experimental results.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Where\u2019s my stuff? An ontology repair plan\n", "abstract": " Appropriate representation is the key to successful reasoning. Hence, if intelligent agents are to cope with changing goals in a changing environment, they must be able to adapt their representations, ie, to detect that a current representation is inadequate, to diagnose its shortcomings and to repair it. In this paper we address the most basic kind of representational shortcoming: inconsistency. We focus on how certain kinds of inconsistency can be repaired using a repair plan that we entitle Where\u2019s My Stuff?. We apply this repair plan manually to four examples from the domain of Physics. In each case an inconsistent ontology is repaired into a consistent one. This extends the interest of the Disproving workshop beyond the \u201creparation of non-theorems\u201d to the reparation of inconsistent ontologies. The Physics domain has the advantage that many faulty ontologies have been recorded by historians of science, together with the evidence that identified their faults and the ontological repairs that were proposed to mend them. These records provide plenty of data for developing and evaluating ontology repair plans.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Constructing probabilistic ATMSs using extended incidence calculus\n", "abstract": " This paper discusses the relations between extended incidence calculus and assumption-based truth maintenance systems (ATMSs). We first prove that managing labels for statements (nodes) in an ATMS is equivalent to producing incidence sets of these statements in extended incidence calculus. We then demonstrate that the justification set for a node is functionally equivalent to the implication relation set for the same node in extended incidence calculus. As a consequence, extended incidence calculus can provide justifications for an ATMS, because implication relation sets are discovered by the system automatically. We also show that extended incidence calculus provides a theoretical basis for constructing a probabilistic ATMS by associating proper probability distributions on assumptions. In this way, we can not only produce labels for all nodes in the system, but also calculate the probability of any of such\u00a0\u2026", "num_citations": "7\n", "authors": ["1609"]}
{"title": "The use of proof planning for cooperative theorem proving\n", "abstract": " We describe BARNACLE: a cooperative interface to an inductive theorem prover. The cooperative nature of the BARNACLE interface is made possible by proof planning. Proof planning is a technique for guiding the search for a proof in automatic theorem proving. Common patterns of reasoning in proofs are identified and represented computationa. lly as proof plans. These proof plans are then used to guide the search for proofs of new conjectures. Where a proof requires more than common patterns of reasoning, proof planning needs to be supplemented by human interaction. Proof planning makes new kinds of user interaction possible. Proof plans structure proofs hierarchica. lly. This can be used to present partial proofs to users without overwhelming them with detail. Proof plans use a meta-logic to relate each chunk of a proof to its parents and daughters in the hierarchy and to its subparts. Proof plans sometimes annotate the proof steps to display the rationale behind them. The relations between proof chunks and the annotations can both be used to help users understand the state of proof attempts. This improved understanding can help them find patches to failed proofs and the metalogic provides a high-level language for specifying the patch.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Assignment methods for incidence calculus\n", "abstract": " Incidence calculus is a mechanism for probabilistic reasoning in which sets of possible worlds, called incidences, are associated with axioms, and probabilities are then associated with these sets. Inference rules are used to deduce bounds on the incidence of formulae which are not axioms, and bounds for the probability of such a formula can then be obtained. In practice an assignment of probabilities directly to axioms may be given, and it is then necessary to find an assignment of incidence which will reproduce these probabilities. We show that this task of assigning incidences can be viewed as a tree searching problem, and two techniques for performing this research are discussed. One of these is a new proposal involving a depth first search, while the other incorporates a random element. A Prolog implementation of these methods has been developed. The two approaches are compared for efficiency and the\u00a0\u2026", "num_citations": "7\n", "authors": ["1609"]}
{"title": "The use of typed lambda calculus for requirements capture in the domain of ecological modelling\n", "abstract": " We will describe the use of order-sorted, typed lambda calculus to represent ecological simulation models. It will be used to represent both the qualitative specifications of ecological situations and the differential equations which describe these situations quantitatively. The numerical solution of these equations constitutes a simulation model of the situation. This logic-based formalism extends previous formalisms for ecological modelling. Particular emphasis will be given to the automatic help given to users in formulating their goals, describing the ecological situation to be modelled and forming the differential equations which model it. The typed lambda calculus representation provides a simple grammar for constraining and guiding this process of requirements capture. Users develop their goals by a process of refinement of logical expressions. Each refinement step can also be understood in ecological terms. The refinement steps are necessarily not truth preserving because we are engaged in requirements capture rather than specification transformation.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Branch-and-bound algorithms\n", "abstract": " A solution technique for discrete optimisation problems which is widely used outside Al and is closely related to the A* algorithm <2>. The task is to find the optimally valued tip of a walkable search tree. A subtree of the search tree need not be searched if a computation at its root yields a bound for its set of tip values which implies that none of them can be optimal.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "What is the well-dressed AI educator wearing now?\n", "abstract": " A funny thing happened to me at IJCAI-81. I went to a panel on\" Education in AI\" and stepped back into an argument that I had thought settled several years ago. The debate was between the\" scruffies,\" led by Roger Schank and Ed Feignbaum, and the\" neats,\" led by Nils Nilsson. The neats argued that no education in AI was complete without a strong theoretical component, containing, for instance, courses on predicate logic and automata theory. The scruffies maintained that such a theoretical component was not only unnecessary, but harmful.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "A treatise on elementary equation solving\n", "abstract": " This memo was originally started in April 1972, but progress was interrupted. It describes our ideas on equation solving prior to Bundy 1975 and the writing of the PRESS equation solving program. Following a revival of interest in equation solving we have decided to finish the memo and produce it as a working paper. It deals with some areas not covered in Bundy 1975 and is complementary to that paper. Our ideas on some of these areas have never been implemented, especially those on polynomial, rational and trigonometric equations.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Exploiting the properties of functions to control search\n", "abstract": " In the first half (sections 1-5) of the paper we examine the tradeoff between representaing knowledge using functions and using relations. The peroperties of functions turn out to be indispensable, but to be a major contribution to the combinatorial explosion. In the second half (sections 6-12) an examination of these properties suggests incorporating them in the inference mechanism, where they can be used to great advantage in controlling search and reducing the combinatorial explosion. This incorporation is seen as the first step in the design of a computational logic attuned to the demands of automatic reasoning. The purpose of the first half is introductory and may be omitted by those steeped in the traditions of Resolution theorem proving.", "num_citations": "7\n", "authors": ["1609"]}
{"title": "Automating Event-B invariant proofs by rippling and proof patching\n", "abstract": " The use of formal method techniques can contribute to the production of more reliable and dependable systems. However, a common bottleneck for industrial adoption of such techniques is the needs for interactive proofs. We use a popular formal method, called Event-B, as our working domain, and set invariant preservation (INV) proofs as targets, because INV proofs can account for a significant proportion of the proofs requiring human interactions. We apply an inductive theorem proving technique, called rippling, for Event-B INV proofs. Rippling automates proofs using meta-level guidance. The guidance is in particular useful to develop proof patches to recover failed proof attempts. We are interested in the case when a missing lemma is required. We combine a scheme-based theory-exploration system, called IsaScheme [MRMDB10], with rippling to develop a proof patch via lemma discovery. We also\u00a0\u2026", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Creating a new generation of computational thinkers\n", "abstract": " Experiences with a successful school program in Scotland.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "The use of rippling to automate Event-B invariant preservation proofs\n", "abstract": " Proof automation is a common bottleneck for industrial adoption of formal methods. In Event-B, a significant proportion of proof obligations which require human interaction fall into a family called invariant preservation. In this paper we show that a rewriting technique called rippling can increase the automation of proofs in this family, and extend this technique by combining two existing approaches.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "GALILEO: A system for automating ontology evolution\n", "abstract": " We describe the GALILEO system, which is designed for automating the evolution of higher-order logic ontologies by incorporating user interaction for diagnosing and repairing faults. In particular, we present our approach to ontological conflict diagnosis, which circumvents problems posed by HOL\u2019s undecidability by means of: formalising modular ontologies as Isabelle locales and preparing the system by user interaction; applying ontology repair plans and automatically identifying logically valid terms that are responsible for the conflict; and, automatically eliminating physically meaningless terms.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "OpenKnowledge Deliverable 3.1.: Dynamic ontology matching: a survey\n", "abstract": " Matching has been recognized as a plausible solution for the semantic heterogeneity problem in many traditional applications, such as schema integration, ontology integration, data warehouses, data integration, and so on. Recently, there have emerged a line of new applications characterized by their dynamics, such as peer-to-peer systems, agents, web-services. In this deliverable we extend the notion of ontology matching, as it has been understood in traditional applications, to dynamic ontology matching. In particular, we examine real-world scenarios and collect the requirements they pose towards a plausible solution. We consider five general matching directions which we believe can appropriately address those requirements. These are: (i) approximate and partial ontology matching, (ii) interactive ontology matching, (iii continuous \u201ddesign-time\u201d ontology matching, (iv) community-driven ontology matching and (v) multi-ontology matching. We give an overview of state of the art matching systems as well as their evaluation principles from the dynamic ontology matching perspective. Finally, the key open issues and challenges towards a plausible dynamic ontology matching solution are discussed, thereby providing a vision for future activities.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Facilitating agent communication through detecting, diagnosing and refining ontological mismatch\n", "abstract": " The development of the semantic web makes the facilitation of agent communication an issue of increasing importance. It is often assumed that agents are using the same ontology and hence can understand one another, but the dynamic and distributed nature of the semantic web can mean that this is not always a valid assumption. We describe a system that can dynamically discover some ontological mismatches between agents during communication and then refine them, to enable communication to proceed successfully between these agents.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "On the correction of faulty formulae\n", "abstract": " We present an abduction mechanism capable of correct-ing faulty formulae. A formula, G, is said to be faulty if it is not deriuable from a theory, T, uritten T \u00ba G, but une intended it to be. Giuen a theory, I', and i faulty formula, G, the mechanism aims to build a corrective condition, P, that transforms G into a theorem, THP\u2013G. The method imposes restrictions upon the quality of a correctiue condition. The mechanismi is fully automatic. It is given as a collection of heuris-ties, Each heuristic contral captures fhe restricted ty in thich the search for a proof of a faulty formula can fail, and provides knowledge to recover from such a fail-", "num_citations": "6\n", "authors": ["1609"]}
{"title": "A proposal for automating diagrammatic reasoning in continuous domains\n", "abstract": " This paper presents one approach to the formalisation of diagrammatic proofs as an alternative to algebraic logic. An idea of \u2018generic diagrams\u2019 is developed whereby one diagram (or rather, one sequence of diagrams) can be used to prove many instances of a theorem. This allows the extension of Jamnik\u2019s ideas in the Diamond system to continuous domains. The domain is restricted to non-recursive proofs in real analysis whose statement and proof have a strong geometric component. The aim is to develop a system of diagrams and redraw rules to allow a mechanised construction of sequences of diagrams constituting a proof. This approach involves creating a diagrammatic theory. The method is justified formally by (a) a diagrammatic axiomatisation, and (b) an appeal to analysis, viewing the diagram as an object in\u211d2. The idea is to then establish an isomorphism between diagrams acted on by redraw\u00a0\u2026", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Extensions to the estimation calculus\n", "abstract": " Walther\u2019s estimation calculus was designed to prove the termination of functional programs, and can also be used to solve the similar problem of proving the well-foundedness of induction rules. However, there are certain features of the goal formulae which are more common to the problem of induction rule well-foundedness than the problem of termination, and which the calculus cannot handle. We present a sound extension of the calculus that is capable of dealing with these features. The extension develops Walther\u2019s concept of an argument bounded function in two ways: firstly, so that the function may be bounded below by its argument, and secondly, so that a bound may exist between two arguments of a predicate. Our calculus enables automatic proofs of the well-foundedness of a large class of induction rules not captured by the original calculus.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Planning equational verification in ccs\n", "abstract": " Most efforts to automate the formal verification of communicating systems have centred around finite-state systems (FSSs). However, FSSs are incapable of modelling many practical communicating systems, and hence there is interest in a novel class of problems, which we call VIPSs (Value-passing Infinite-state Parameterised Systems). Existing approaches using model checking over FSSs are insufficient for VIPSs, due to their inability both to reason with and about domain-specific theories, and to cope with systems having an unbounded or arbitrary state space. We use the Calculus of Communicating Systems (CCS) with parameterised constants to express and specify VIPSs. We use the laws of CCS to conduct the verification task. This approach allows us to study communicating systems, regardless of their state space, and the data such systems communicate. Automating theorem proving in this system is an\u00a0\u2026", "num_citations": "6\n", "authors": ["1609"]}
{"title": "HR-a system for machine discovery in finite algebras\n", "abstract": " We describe the HR concept formation program which invents mathematical definitions and conjectures in finite algebras such as group theory and ring theory. We give the methods behind and the reasons for the concept formation in HR, an evaluation of its performance in its training domain, group theory, and a look at HR in domains other than group theory.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Proof planning and industrial configuration\n", "abstract": " I introduce Proof Planning, declarative programming paradigm, in order to address main bottlenecks of Knowledge Based Systems design. Commercially successful application needs to consider the problems of enhancability and maintainability as well as clear separation of knowledge and inference engine-weak solver. The proposed methodology is supposed to offer possibility of fast prototyping and reduce overall development times then. The selection of an appropriate methodology shall correspond clearly to the complexity of a problem. In my judgement Proof Planning is very suitable methodology for medium sized Knowledge Based Systems.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Automation of diagrammatic proofs in mathematics\n", "abstract": " Theorems in automated theorem proving are usually proved by logical formal proofs. However, there is a subset of problems which can also be proved in a more informal way by the use of geometric operations on diagrams, so called diagrammatic proofs. Insight is more clearly perceived in these than in the corresponding logical proofs: they capture an intuitive notion of truthfulness that humans find easy to see and understand. The proposed research project is to identify and ultimately automate this diagrammatic reasoning on mathematical theorems. The system that we are in the process of implementing will be given a theorem and will (initially) interactively prove it by the use of geometric manipulations on the diagram that the user chooses to be the appropriate ones. These operations will be the inference steps of the proof. The constructive-rule will be used as a tool to capture the generality of diagrammatic\u00a0\u2026", "num_citations": "6\n", "authors": ["1609"]}
{"title": "An incompleteness theorem via abstraction\n", "abstract": " ion Alan Bundy 1, Fausto Giunchiglia 2; 3, Adolfo Villafiorita 4; 5 and Toby Walsh 2; 5 1. Mathematical Reasoning Group, Dept of AI, University of Edinburgh 2. Mechanized Reasoning Group, IRST 3. DISA, University of Trento 4. Istituto di Informatica, University of Ancona 5. Mechanized Reasoning Group, DIST, University of Genoa April 13, 1996 Abstract We demonstrate the use of abstraction in aiding the construction of an interesting and difficult example in a proof checking system. This experiment demonstrates that abstraction can make proofs easier to comprehend and to verify mechanically. To support such proof checking, we have developed a formal theory of abstraction and added facilities for using abstraction to the GETFOL proof checking system. 1 Introduction This paper describes an experiment in which we use abstraction to aid the construction of a simplified proof of Godel's first incompleteness theorem. We show that this use of abstraction makes the proof more ac...", "num_citations": "6\n", "authors": ["1609"]}
{"title": "A rational reconstruction of incidence calculus\n", "abstract": " Incidence Calculus was introduced in [Bundy 85] as a mechanism for the automation of qualified reasoning. It is based on probabilities of sets of possible worlds, and presents the feature of being truth functional with respect to logical connectives. In this paper we present a rational reconstruction of Incidence Calculus, providing a more formal basis for this mechanism.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Artificial Intelligence: Art or Science?\n", "abstract": " THE CHAIRMAN: Alan Bundy read mathematics at Leicester University and obtained a PhD in 1971. He then moved to the meta-mathematics unit, going on to the Department of Computational Logic at Edinburgh University, which evolved to become the Department of Artificial Intelligence in 1974. He has been there ever since. Computation and logic have been a main theme of his work for many years now. He has long had a particular interest in problem-solving programs, programs that themselves produce solutions to problems in various branches of mathematics, ranging from number theory through to applied fields such as ecological modelling and automatic programming. He has a wide range of interests in the general field of artificial intelligence and in encouraging and fostering a responsible AI community, especially through his many activities in organizing conferences, editing journals and books, and\u00a0\u2026", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Advances in Artificial Intelligence\n", "abstract": " Brings together papers on fundamental AI research and its potential in creating new applications. Each explores an aspect of the classical problem for researchers in this field: how to produce intelligent behavior by constructing systems capable of representing and reasoning about the world. Topics include representing and reasoning about: time, physical objects, belief, human problem solving, and linguistic structure. The papers in this work build on previous AI research, adding a new seriousness and rigor associated with current, ever-increasing use of formal techniques.", "num_citations": "6\n", "authors": ["1609"]}
{"title": "Calculating error bars on inferences from web data\n", "abstract": " In this work, we explore uncertainty in automated question answering over real-valued data from knowledge bases on the Internet. We argue that the coefficient of variation (cov) is an intuitive and general form in which to express this uncertainty, with the added advantage, it can be calculated exactly and efficiently. The large amounts of data on the Internet presents a good opportunity to answer queries that go beyond simply looking up facts and returning them. However, such data is often vague and noisy. For discrete results, e.g. stating that a particular city is the capital of a particular country, probabilities are a natural way to assign uncertainty to answers. For continuous variables or quantities that are typically treated as continuous (such as populations of countries), probabilities are uninformative, being infinitesimal. For instance, the probability that the population of India is exactly equal to last census count\u00a0\u2026", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Solving guesstimation problems using the Semantic Web: Four lessons from an application.\n", "abstract": " We draw on our experience of implementing a semi-automated guesstimation application of the Semantic Web, gort, to draw four lessons, which we claim are of general applicability. These are:", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Using linked data for semi-automatic guesstimation\n", "abstract": " GORT is a system that combines Linked Data from across several Semantic Web data sources to solve guesstimation problems, with user assistance. The system uses customised inference rules over the relationships in the OpenCyc ontology, combined with data from DBPedia, to reason and perform its calculations. The system is extensible with new Linked Data, as it becomes available, and is capable of answering a small range of guesstimation questions.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Automatic synthesis of decision procedures: a case study of ground and linear arithmetic\n", "abstract": " We address the problem of automatic synthesis of decision procedures. Our synthesis mechanism consists of several stages and sub-mechanisms and is well-suited to the proof-planning paradigm. The system (adeptus), that we present in this paper, synthesised a decision procedure for ground arithmetic completely automatically and it used some specific method generators in generating a decision procedure for linear arithmetic, in only a few seconds of cpu time. We believe that this approach can lead to automated assistance in constructing decision procedures and to more reliable implementations of decision procedures.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Best-first rippling\n", "abstract": " Rippling is a form of rewriting that guides search by only performing steps that reduce the differences between formulae. Termination is normally ensured by a defined measure that is required to decrease with each step. Because of these restrictions, rippling will fail to prove theorems about, for example, mutual recursion where steps that temporarily increase the differences are necessary. Best-first rippling is an extension to rippling where the restrictions have been recast as heuristic scores for use in best-first search. If nothing better is available, previously illegal steps can be considered, making best-first rippling more flexible than ordinary rippling. We have implemented best-first rippling in the IsaPlanner system together with a mechanism for caching proof-states that helps remove symmetries in the search space, and machinery to ensure termination based on term embeddings. Our experiments show that\u00a0\u2026", "num_citations": "5\n", "authors": ["1609"]}
{"title": "An automatic translator from KIF to PDDL\n", "abstract": " In this paper, we present a translation process that we have developed to convert KIF ontologies into PDDL. This allows us to define KIF-based agents that  can plan efficiently. We discuss the difficulties inherent in such a translation process, and the steps we have taken to overcome them. This process is translates from only a subset of KIF to a corresponding subset of PDDL.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Supporting programming by analogy in the learning of functional programming languages\n", "abstract": " This paper examines the learning of the functional programming language Standard ML. A common technique used by novices is programming by analogy whereby students refer to similar programs that they have written before or have seen in the course literature and use these programs as a basis to write a new program. We present a novel editor for ML which supports programming by analogy by providing a collection of editing commands that transform old programs into new ones. Each command makes changes to an isolated part of the program. These changes are propagated to the rest of the program using analogical techniques. Many commands are at a level high enough to provide guidance to the novice during program development. We observed a group of novice ML students to determine the most common programming errors in learning ML and restrict our editor such that it is impossible to commit these errors. In this way, students encounter fewer bugs and so their rate of learning increases.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "The new software copyright law\n", "abstract": " On January 1st 1993 a new UK law slipped onto the statute books, with remarkably little public comment. Since this law could have a significant impact on the market for computer products in the UK, it was especially surprising that there was little debate about it in the computing community. The new law is called \u201cThe Copyright (Computer Programs) Regulations 1992\". To understand this law, and the changes to software copyright that it will bring, it is necessary to understand some of the events which led up to its introduction.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "\u201cSemantic procedure\u201d is an oxymoron\n", "abstract": " //static.cambridge.org/content/id/urn%3Acambridge.org%3Aid%3Aarticle%3AS0140525X00030338/resource/name/firstPage-S0140525X00030338a.jpg", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Clear Thinking about Artificial Intelligence\n", "abstract": " In recent years artificial intelligence (AI) has become more mathematical. The'empirical programming'methodology is no longer the norm. AI researchers have been extracting'neat'techniques from their'scruffy'programs and formalising them. Mathematics is often found to provide an appropriate language for this formalisation. New uses are being found for mathematics in every area of AI. Where existing mathematics is not up to the task, new kinds of mathematics are being invented. These developments are generally to be welcomed as a sign of increasing maturity in the field and as a way of improving the reliability of AI products. However, it creates some problems which we must try to solve by (1) improved human/computer interfaces to make the new mathematical techniques more accessible,(2) improved teaching to produce AI researchers who are comfortable with these new techniques and (3) more testing of\u00a0\u2026", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Using Middle-out Reasoning to Transform Na\u00efve Programs Into Tail-recursive Ones\n", "abstract": " We describe a novel technique for the automatic transformation of naive computer programs into tail-recursive ones, with a consequent gain in efficiency. The technique is to specify the required program using the naive definition and then synthesise the tail-recursive program using the proofs as programs technique. This requires the specification to be proved realisable in a constructive logic. Restrictions on the form of the proof ensure that the synthesised program is tail-recursive. The automatic search for a synthesis proof is controlled by proof plans, which are descriptions of the high-level structure of proofs of this kind. We have extended the known proof plans for inductive proofs by adding a new form of generalisation and by making greater use of middle-out reasoning. In middle-out reasoning we postpone decisions in the early part of the proof by the use of meta-variables which are instantiated, by unification\u00a0\u2026", "num_citations": "5\n", "authors": ["1609"]}
{"title": "A framework for the principled debugging of Prolog programs: How to debug non-terminating programs\n", "abstract": " The search for better Prolog debugging environments has taken a number of different paths of which three are particularly important: improvements to monitoring tools (notably the Transparent Prolog Machine (Eisenstadt & Brayshaw, 1987)), providing for greater user control over the debugging process (notably as in Opium+(Ducasse, 1988)), and partially automating the debugging process (notably in;(Pereira, 1986; l. loyd, 1986; Pereira & Calejo, 1988; Naish, 19, 88)). A serious problem associated with this activity lies in pro~ ding a principled con-!-ceptual framework within which the programmer can work with a number of different'debugging tools. Here, we outline a framework that we have'developed for the debugging of Prolog programs. We point out the relationship that holds between this framework and each of these three advances in debugging..'In order to demonstrate how the framework can be used, we explore an issue that has received relatively little attention recently: the run-time detection of programs that do not appear to terminate.Our analysis of (apparent) non-termination is based on a four level Bug Description Framework that we have developed. This analysis goes further than the consideration of programs that would normally be regarded as' looping'. We describe a debugging strategy in conjunction with a range of monitoring tools that provide greater assistance than currently found, We indicate the increased efficiency that would be gained through a close-coupling of the program construction and execution phases.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Commentary on: solving symbolic equations with PRESS\n", "abstract": " The paper \"Solving Symbolic Equations with PRESS\", by Sterling, Bundy, Byrd, O'Keefe and Silver [Sterling et al 82], describes a program PRESS (PROLOG Equation Solving System) which solves some symbolic, transcendental, non-differential equations. The problem of solving such equations is divided into two levels, a meta-level which is intended as a research vehicle for exploring search strategies in mathematical reasoning and a set of equation-solving modules used initially for the MECHO [Bundy et al 79] system (a project which aims to solve high-school mechanics problems stated in English.)", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Proving properties of logic programs: a progress report\n", "abstract": " We outline the progress we have made in connection with the Alvey Grant\" Proving Properties of Logic Programs\"(SERC GRjDj44270 and Alvey IKBS 137). This grant runs for three years from 1st November 1985. The grant holders are Professor Alan Bundy and Dr Don Sannella and it employs or has employed Dr Fausto Giunchiglia, Frank van Harmelen, Jane Hesketh, Dr Alan Smaill and Dr Lincoln Wallen as Research Associates. Pete Madden and Andrew Stevens are attached Ph. D. students.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Computer-aided construction of ecological simulation models\n", "abstract": " Computer-Aided Construction of Ecological Simulation Models \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Computer-Aided Construction of Ecological Simulation Models R. Muetzelfeldt, D. Robertson, M. Uschold, Alan Bundy School of Informatics School of GeoSciences Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution Overview Original language English Title of host publication International Symposium on AI, Expert Systems and Languages in Modelling and Simulation Publication status Published - 1987 Access to Document 19_Computer_aided_construction_of_ecological_simulation_models\u2026", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Dempster-Shafer Theory\n", "abstract": " This is a theory of evidence potentially suitable for knowledge-based systems. The system is based on \u201cbasic probabilities\u201d which can be visualized as probability masses that are constrained to stay within the subset with which they are associated, but are free to move over every point in the subset. From these basic probabilities we can derive upper and lower probabilities (Dempster) or belief functions and plausibilities (Shafer). The means of combining basic probabilities is using Dempster\u2019s Rule which is valid given independent evidences. A position of complete ignorance about an hypothesis is represented by having an upper probability of one and a lower probability of zero. Complete certainty about the probability of an hypothesis is represented when the upper and lower probabilities are equal. The approach can suffer from high computation times, although this can be reduced when each piece of\u00a0\u2026", "num_citations": "5\n", "authors": ["1609"]}
{"title": "What has learning got to do with expert systems?\n", "abstract": " What has learning got to do with expert systems? \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation What has learning got to do with expert systems? Alan Bundy School of Informatics Research output: Working paper Overview Original language English Publication status Unpublished - 1984 Publication series Name DAI Research Paper No. 214 Access to Document Full textAccepted author manuscript, 302 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (1984). What has learning got to do with expert systems? (DAI Research Paper No. 214). Bundy, Alan. / What has learning got to do with expert systems?. 1984. (DAI \u2026", "num_citations": "5\n", "authors": ["1609"]}
{"title": "Knowledge about knowledge: Making decisions in mechanics problem solving\n", "abstract": " In October 1975 we started a three year SRC supported project on\" A Program to Solve Mechanics Problems Stated in English\". This project is now beginning its third year and many of its initial objectives have been achieved. In this paper we briefly review the progress that has been made and discuss some of the more promising areas of research that we have uncovered.", "num_citations": "5\n", "authors": ["1609"]}
{"title": "New voices: multicultural short stories\n", "abstract": " New voices: multicultural short stories Skip to main content Nottingham Repository Home Research Outputs People Faculties, Schools & Groups Research Areas Arts & Humanities - Area Studies Arts & Humanities - Art & Design Arts & Humanities - Classics Arts & Humanities - Communication, Cultural & Media Studies Arts & Humanities - English Language and Literature Arts & Humanities - History Arts & Humanities - Modern Languages & Linguistics Arts & Humanities - Music, Drama, Dance & Performing Arts Arts & Humanities - Other/Miscellaneous Arts & Humanities - Philosophy Arts & Humanities - Theology & Religious Studies Bio/Medical/Health - Agriculture Bio/Medical/Health - Allied Health Professions, Dentistry, Nursing & Pharmacy Bio/Medical/Health - Biochemistry & Molecular Biology Bio/Medical/Health - Biotechnology Bio/Medical/Health - Botany & Plant Sciences Bio/Medical/Health - Cell & \u2026", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Anchoring knowledge in interaction: Towards a harmonic subsymbolic/symbolic framework and architecture of computational cognition\n", "abstract": " We outline a proposal for a research program leading to a new paradigm, architectural framework, and prototypical implementation, for the cognitively inspired anchoring of an agent\u2019s learning, knowledge formation, and higher reasoning abilities in real-world interactions: Learning through interaction in real-time in a real environment triggers the incremental accumulation and repair of knowledge that leads to the formation of theories at a higher level of abstraction. The transformations at this higher level filter down and inform the learning process as part of a permanent cycle of learning through experience, higher-order deliberation, theory formation and revision.                 The envisioned framework will provide a precise computational theory, algorithmic descriptions, and an implementation in cyber-physical systems, addressing the lifting of action patterns from the subsymbolic to the symbolic knowledge\u00a0\u2026", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Catalogue of artificial intelligence techniques\n", "abstract": " The purpose of the Catalogue of Artificial Intelligence Techniques is to promote interaction between members of the AI community. It does this by announcing the existence of AI techniques, and acting as a pointer into the literature. Thus the AI community will have access to a common, extensional definition of the field, which will promote a common terminology, discourage the reinvention of wheels, and act as a clearing house for ideas and algorithms. The catalogue is a reference work providing a quick guide to the AI techniques available for different jobs. It is not intended to be a textbook like the Artificial Intelligence Handbook. Intentionally, it only provides a brief description of each technique, with no extended discussion of its historical origin or how it has been used in particular AI programs. The original version of the catalogue was hastily built in 1983 as part of the UK SERC-DoI, IKBS Architecture Study. It was adopted by the UK Alvey Programme and, during the life of the programme, was both circulated to Alvey grant holders in hard copy form and maintained as an on-line document. A version designed for the international community was published as a paperback by Springer-Verlag. All these versions have undergone constant revision and refinement. Springer-Verlag has agreed to reprint the catalogue at frequent intervals in order to keep it up to date and this is the third edition of their paperback version.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Towards the automatic detection and correction of errors in automatically constructed ontologies\n", "abstract": " The Open Information Extraction Project is one of the most ambitious attempts in the area of automatically constructing ontologies by harvesting information from the web. What we will call their Know-It-All Ontology contains about 6 billion items, consisting of triples and rules. The downside of such automatically constructed ontologies is that they contain a vast number of errors: some arising from errors in the original web data and some from errors in extracting the data. In this project we explore whether techniques we have developed in the domain of ontology repair can be used to detect and correct some of these errors. In particular, we explore whether the errors in their ontology can be automatically detected by using a theorem prover. We also present a manual classification of the errors as a preliminary feasibility exploration, and discuss our future work towards automatically correcting the ontology based on the\u00a0\u2026", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Higher-order Representation and Reasoning for Automated Ontology Evolution.\n", "abstract": " The GALILEO system aims at realising automated ontology evolution. This is necessary to enable intelligent agents to manipulate their own knowledge autonomously and thus reason and communicate effectively in open, dynamic digital environments characterised by the heterogeneity of data and of representation languages. Our approach is based on patterns of diagnosis of faults detected across multiple ontologies. Such patterns allow to identify the type of repair required when conflicting ontologies yield erroneous inferences. We assume that each ontology is locally consistent, ie inconsistency arises only across ontologies when they are merged together. Local consistency avoids the derivation of uninteresting theorems, so the formula for diagnosis can essentially be seen as an open theorem over the ontologies. The system\u2019s application domain is physics; we have adopted a modular formalisation of physics, structured by means of locales in Isabelle, to perform modular higher-order reasoning, and visualised by means of development graphs.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Lemma discovery and middle-out reasoning for automated inductive proof\n", "abstract": " Lemma speculation has long been considered a promising technique to automate the discovery of missing lemmas for inductive proofs. This technique involves speculating a schematic lemma that becomes incrementally instantiated by unification as the proof continues. This synthesis process is known as middle-out reasoning. We have extended lemma speculation, and more generally middle-out reasoning, to dynamic rippling for higher-order domains, implemented it in the Isa-Planner system and improved the technique to ensure termination. This provides a practical basis for exploring the applications of middle-out reasoning. We demonstrate such an application by performing a critical and comparative evaluation of lemma speculation. This shows that when lemma speculation is applied it often finds the needed lemmas to complete the proof, but it is not applicable as often as initially expected. In comparison, we show that simpler proof methods combined with theory formation methods offer an effective alternative.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Deductive synthesis of workflows for e-Science\n", "abstract": " In this paper we show that the automated reasoning technique of deductive synthesis can be applied to address the problem of machine-assisted composition of e-Science workflows according to users' specifications. We encode formal specifications of e-Science data, services and workflows, constructed from their descriptions, in the generic theorem prover Isabelle. Workflows meeting this specification are then synthesised as a side-effect of proving that these specifications can be met.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "On differences between the real and physical plane\n", "abstract": " When formalising diagrammatic systems, it is quite common to situate diagrams in the real plane, . However this is not necessarily sound unless the link between formal and physical diagrams is examined. We explore some issues relating to this, and potential mistakes that can arise. This demonstrates that the effects of drawing resolution and the limits of perception can change the meaning of a\u00a0diagram in surprising ways. These effects should therefore be taken into account when giving formalisations based on .", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Inferring quality of service properties for grid applications\n", "abstract": " The project will evaluate the applicability of compositional calculi for estimating quality of service (QoS) properties arising in the development of an e-Science infrastructure such as the Grid.A compositional calculus is presented as a set of rules of inference or recursion equations, where each way of constructing new expressions from old corresponds to a rule or equation. And it is a method for forming logic combinations of QoS properties inherited by compound systems from their components.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Proofs-as-Programs as a Framework for the Design of an Analogy-Based ML Editor\n", "abstract": " C                                    Y                                  NTHIA is a transformation-based editor for a functional subset of ML that lies somewhere between a structure editor and a framework for formal program development. Users construct programs from existing code by applying editing commands that make a semantic analysis of the program's behaviour, e.g., whether it is terminating. All analysis is done using the Oyster system, which is an implementation of proofs-as-programs. We concentrate on identifying analyses that can be done fully automatically (e.g., using a decision procedure) and hence can be hidden from the user. As a result, C                                    Y                                  NTHIA represents progress towards a goal of program editors that make an intelligent analysis of their code, but in a way that requires no extra input from the programmer.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Recursive program optimization through inductive synthesis proof transformation\n", "abstract": " The research described in this paper involved developing transformation techniques that increase the efficiency of the original program, the source, by transforming its synthesis proof into one, the target, which yields a computationally more efficient algorithm. We describe a working proof transformation system that, by exploiting the duality between mathematical induction and recursion, employs the novel strategy of optimizing recursive programs by transforming inductive proofs. We compare and contrast this approach with the more traditional approaches to program transformation and highlight the benefits of proof transformation with regards to search, correctness, automatability, and generality.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "The Termination of Rippling and Unblocking\n", "abstract": " Rippling is a heuristic technique for guiding rewriting of a goal with respect to one or more givens. Rewriting is restricted so that the similarities between goal and given are preserved and the movement of di erences is directed and terminating. Unblocking is a technique for changing the di erences to enable a subsequent ripple.The standard de nition of rippling currently prevents certain, otherwise desirable, unblocking steps. In particular, some desirable unblocking steps increase the wellfounded measure on which the termination proof of rippling is based. We propose an alternative family of well-founded measures under which the combination of rippling and unblocking is terminating. These new measures extend the power of rippling.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "The use of classification in automated mathematical concept formation\n", "abstract": " Concept formation programs aim to produce a high yield of concepts which are considered interesting. One intelligent way to do this is to base a new concept on one or more concepts which are already known to be interesting. This requires a concrete notion of the \u2018interestingness\u2019 of a particular concept. Restricting the concepts formed to mathematical definitions in finite group theory, we derive three measures of the interestingness of a concept. These measures are based on how much the concept improves a classification of finite groups.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Proof planning and configuration\n", "abstract": " This paper presents two configuration problems: that of configuring computer hardware to meet a given specification, and an \u201cengineer-and-made-to-order\u201d problem in the domain of breathing air compressor production. We demonstrate how the different kinds of knowledge needed to solve problems in each domain (which we label factual, heuristic, and strategic) was captured for a proof planning system. The systems perform favourably when compared with human experts. The ideas developed for the first domain (computer hardware) transferred remarkably well to the new domain (compressors), the time taken from knowledge acquisition to prototype being less than three months. We show that such systems are also easy to maintain, and to adapt to similar but distinct problems in the same domain. We end with suggestions of further domains and problems which we believe are amenable to the proof planning approach.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "A prototype interface between Clam and HOL\n", "abstract": " This paper describes a prototype interface between the CLaM proof planner and the HOL interactive theorem prover. CLaM uses arti cial intelligence planning techniques to nd proofs at a high-level, especially where inductive proofs are required. HOL is a theorem prover for a classical higher-order logic and has been widely used in both academia and industry for veri cation of hardware and software. The interface sends HOL goals to CLaM for planning, and translates plans back into HOL tactics that solve the initial goals.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "An experimental comparison of rippling and exhaustive rewriting\n", "abstract": " We compare rippling and exhaustive rewriting using recursive path ordering, on a range of inductive proofs. We present statistics on success rates, branching rates and CPU times. We use these statistics to argue that rippling succeeds more often. However, these statistics also show that rippling and reduction are roughly the same in terms of average branching rate and that rippling often takes longer in terms of CPU time.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Philosophical transactions\n", "abstract": " Submission. Authors may send their papers direct to the Editorial Office, whose address is given below, or to the Editor or a member of the Editorial Board. Three copies of the typescript and figures are required, and originals of any figures should also be submitted. Papers should be prepared in accordance with the \u2018Instructions to authors\u2019 printed at the end of every volume of Philosophical Transactions A and available from the Editorial Office.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "A rational reconstruction of incidence calculus\n", "abstract": " Incidence Calcu/w was introduced in [Bundy 85] as a mechanism for the automation of qualified reasoning. It is based on probabilities of sets of possible worlds, and presents the feature of being truth functional with respect to logical connectives. In this paper we present a rational reconstruction of Incidence Calculus, providing a more formal basis for this mechanism.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "AI bridges and dreams\n", "abstract": " This paper is a modified version of my acceptance lecture for the 1986 SPL-Insight Award. It turned into something of a personal credo\u2014describing my view of the nature of AI the potential social benefit of applied AI the importance of basic AI research the role of logic and the methodology of rational construction the interplay of applied and basic AI research, and the importance of funding basic AI. These points are knitted together by an analogy between AI and structural engineering: in particular, between building expert systems and building bridges.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "'Poof'Analysis--A technique for concept formation\n", "abstract": " We report the discovery of an unexpected connection between the invention of the concept of uniform convergence and the occurs check in the unification algorithm. This discovery suggests the invention of further interesting concepts in analysis and a technique for automated concept formation. Part of this technique has been implemented. The discovery arose as part of an attempt to understand the role of proof analysis in mathematical reasoning, so as to incorporate it into a computer program. Silver (1986) and Mitchell (1983) have investigated the automatic analysis of model proofs in order to extract and learn knowledge about controlling search, including the knowledge of new concepts. We focus on the analysis and correction of faulty proofs or'poofs'. especially where that correction involves the invention of new mathematical concepts. A classic example of where the analysis of a poof leads to a new concept is the invention, by Weierstrass, Seidel, Cauchy and others, of uniform convergence as a result of an analysis of Cauchy's poof that the limit of a* A'poor, according to one of my mathematics lecturers, is a proof with something vital missing. Artificial Intelligence and its Applications, edited by AG Cohn and JR Thomas@ A. Bundy 1986, Published by John Wiley & Sons Ltd. 52 ARTIFICIAL INTELLIGENCE AND ITS APPLICATIONS convergent series of continuous functions is itself continuous. The correction consists of substituting in the theorem the new concept of'uniformly convergent'for'convergent'. We will investigate this example. The bug in Cauchy's poof is a violation of the occurs check. This observation suggests a technique for\u00a0\u2026", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Department of Artificial Intelligence\n", "abstract": " He study equation solving and analyse the solutions of experienced mathematicians. We find that traditional theorem proving methods are inadequate to explain the directness of these solutions, and that the well known algorithms for polynomials etc. are inadequate to explain the wide variety of equations solved. Our analysis reveals a system of high-level descriptions, strategies and goals, which can be used to guide the search through an explosively large search space. A few of these strategies will be investigated in detail. It is hoped that this analysis will eventually be incorporated into a computer program that solves equations.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Dynamic time warping\n", "abstract": " Two programs are provided, one that generates Ipc and autocorrelation coefficients from the speech utterances and the other that, using dynamic programming, compares the test utterance with the reference utterances and finds the best match. The method used is Constrained Endpoint with 2-to-l range of slope.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Rewrite rules\n", "abstract": " Rewrite rules are sets of ordered pairs of expressions (lhs,rhs) usually depicted as (lns \u21d2 rhs). There is usually a similarity relation between the \u201clhs\u201d and the \u201crhs\u201d such as equality, inequality or double implication. Rewrite rules, as the pairs are called, together with the rewriting rule of inference allow one expression to be \u201crewritten\u201d into another. A subexpression of the initial expression is matched with the \u201clhs\u201d of the rewrite rule yielding a substitution. The resulting expression is the expression obtained by replacing the distinguished subexpression with the \u201crhs\u201d of the rewrite rule after applying the substitution.", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Topics for circumscription\n", "abstract": " The power of ability to reason conclusions' and given. human reasoning can, to a great extent. from incomplete information. People make plausible conjectures which fill out be attributed to people's seem able to'jump to the information they are", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Similarity classes\n", "abstract": " Similarity classes \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Similarity classes Alan Bundy School of Informatics Research output: Working paper Overview Original language English Publication status Unpublished - Jan 1978 Publication series Name DAI Working Paper No. 25 Access to Document Full textAccepted author manuscript, 492 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (1978). Similarity classes. ( DAI Working Paper No. 25). Bundy, Alan. / Similarity classes. 1978. ( DAI Working Paper No. 25). Bundy, A 1978 'Similarity classes' DAI Working Paper No. 25. Similarity classes. / Bundy, Alan. 1978. ( DAI \u2026", "num_citations": "4\n", "authors": ["1609"]}
{"title": "Using domain lexicon and grammar for ontology matching.\n", "abstract": " There are multiple ontology matching approaches that use domain-specific background knowledge to match labels in domain ontologies or classifications. However, they tend to rely on lexical knowledge and do not consider the specificities of domain grammar. In this paper, we demonstrate the usefulness of both lexical and grammatical linguistic domain knowledge for ontology matching through examples from multiple domains. We also provide an evaluation of the impact of such knowledge on a real-world problem of matching classifications of mental illnesses from the health domain. Our experimentation with two matcher tools that use very different matching mechanisms\u2014LogMap and SMATCH\u2014shows that both lexical and grammatical knowledge improve matching results.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Dynamic data sharing from large data sources\n", "abstract": " This paper outlines our work on facilitating fast, effective, on-demand data sharing during emergency response situations. A lazy approach to matching is taken, matching only when and what is necessary for the immediate situation. This allows us to deal with data from large sources represented in different formats. This is a position paper, describing how we are adapting well-developed matching techniques to be appropriate for a different domain, explaining the key challenges and differences of this domain, and describing what has been revealed by the data analysis we have performed thus far.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Writing a good grant proposal\n", "abstract": " Writing a good research grant proposal is not easy. This document is an attempt to collect together a number of suggestions about what makes a good proposal. It is inevitably a personal view on the part of the authors; we would welcome feedback and suggestions from others.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Reasoning with context in the semantic web\n", "abstract": " Editorial: Reasoning with Context in the Semantic Web: Web Semantics: Science, Services and Agents on the World Wide Web: Vol 12-13, No null ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Web Semantics: Science, Services and Agents on the World Wide Web Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsWeb Semantics: Science, Services and Agents on the World Wide WebVol. -13Editorial: Reasoning with Context in the Semantic Web article Editorial: Reasoning with Context in the Semantic Web Share on Authors: Jos Lehmann profile image Jos Lehmann School of Informatics, University of Edinburgh, Informatics Forum, 10 \u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Reasoning about representations in autonomous systems: What P\u00f3lya and Lakatos have to say\n", "abstract": " Autonomous reasoning systems combine an often logic-based representation of some aspect of the world with rules for manipulating that representation. These representations are usually inherited from the literature or are built manually for a particular reasoning task. They are then regarded as fixed. We have argued that representations should instead be regarded as fluid, that is, their choice, construction and evolution should be under the control of the autonomous agent rather than predetermined and fixed (Bundy and McNeill, 2006).                                     Appropriate representation is the key to successful problem-solving. It follows that a successful problem-solver must be able to choose, construct or evolve whatever representation is best suited to solving the current problem.                                                     Autonomous agents use representations called ontologies. For different agents to communicate they\u00a0\u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Isacosy: Synthesis of inductive theorems\n", "abstract": " We have implemented a program for inductive theory formation, called IsaCoSy, which synthesises conjectures about recursively defined datatypes and functions. Only irreducible terms are generated, which keeps the search space tractably small. The synthesised terms are filtered through counter-example checking and then passed on to the automatic inductive prover IsaPlanner. Experiments have given promising results, with high recall of 83% for natural numbers and 100% for lists when compared to libraries for the Isabelle theorem prover. However, precision is somewhat lower, 38-63%.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Benchmarking methodology for good enough answers\n", "abstract": " This document discusses (i) a methodology for benchmarking good enough answers as well as (ii) the state of the art in the related areas that address the issues of evaluating quality of query answering in peer-to-peer systems.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "PG Tips: A Recommender System for an Interactive Theorem Prover\n", "abstract": " Interactive theorem provers require input from users to guide the proof process. Some theorems can be complicated, making it difficult to decide which direction to take at a specific point within a proof. PG Tips is a recommender system that has been incorporated into the theorem prover Isabelle\u2019s graphical user interface, Proof General, in order to assist users.Recommender systems are used, in a variety of situations, to provide predictions based on information supplied by a user. In this case, PG Tips is used to suggest possible proof steps based on the analysis of previous proofs. It is hoped that the creation of such a system will help users in finding proofs and accelerate the proof authoring process.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Psychological validity of schematic proofs\n", "abstract": " Schematic proofs are functions which can produce a proof of a proposition for each value of their parameters. A schematic proof can be constructed by abstracting a general pattern of proof from several examples of a family of proofs. In this paper we examine several interesting aspects of the use of schematic proofs in mathematics. Furthermore, we pose several conjectures about the psychological validity of the use of schematic proofs in mathematics. These conjectures need testing, hence we propose an empirical study which would either support or refute our conjectures. Ultimately, we suggest that schematic proofs are worthy of a closer and more detailed study and investigation.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "An experimental comparison of diagrammatic and algebraic logics\n", "abstract": " We have developed a\u00a0diagrammatic logic for theorem proving, focusing on the domain of metric-space analysis (a geometric domain, but traditionally taught using a\u00a0dry algebraic formalism). To evaluate its pragmatic value, pilot experiments were conducted using this logic \u2013 implemented in an interactive theorem prover \u2013 to teach undergraduate students (and comparing performance against an equivalent algebraic logic). Our results show significantly better performance for students using diagrammatic reasoning. We conclude that diagrams are a\u00a0useful tool for reasoning in such domains.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Using the CORAL system to discover attacks on security protocols\n", "abstract": " Conclusions             We have presented CORAL, our system for refuting incorrect inductive conjectures, and have shown how it can be applied to the problem of finding attacks on faulty security protocols. Our formalism is similar to Paulson\u2019s, which allows us to deal directly with protocols involving an arbitrary number of participants and nonces, and with principals playing multiple roles. CORAL has discovered a number of known attacks, and some new attacks on a group-key protocol. In the longer term, we hope to apply the system to other, related security problems and exploit its ability to do equational reasoning in order to analyse some crytpoanalytic properties of protocols. (This paper is a shortened and updated version of [21]. )", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Plan execution failure analysis using plan deconstruction\n", "abstract": " We consider the challenges that arise when plans are based on an incorrect representation of the domain in which they are executed. We describe how information about plan formation, and how the way in which each plan step is related to the domain representation, can help identify problems in the underlying ontology when plan failure is encountered. We introduce the notion of a plan deconstructor, used to extract this information from the domain representation so that is it available when plan failure occurs. Thus a more accurate ontology can be developed and more robust plans can be formed.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "An intelligent tutoring system for induction proofs\n", "abstract": " An Intelligent Tutoring System for Induction Proofs \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation An Intelligent Tutoring System for Induction Proofs Alan Bundy, Johanna Moore, Claus Zinn School of Informatics Institute of Language, Cognition and Computation Research output: Chapter in Book/Report/Conference proceeding \u203a Conference contribution Overview Original language English Title of host publication CADE-17 Workshop on Automated Deduction in Education Pages 4-13 Number of pages 10 Publication status Published - Jun 2000 Access to Document Bundy Moore ET AL 2000 An Intelligent Tutoring System for Induction ProofsAccepted \u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "The design of the CADE-16 inductive theorem prover contest\n", "abstract": " It has become a tradition at CADE to run a competition for first-order automated theorem provers based on the TPTP problem library. This competition (CASC) [SuSu96] aims at fully automatic ATP systems and provides various categories dedicated to diffirent problem classes of rst-order logic. The number of problems solved and the runtime is used to assess the winners in the individual categories of the competition.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Annotated term rewriting for deciding observation congruence\n", "abstract": " The use of term-rewriting systems (TRSs) is at the heart of automated theorem proving (ATP). There are a number of methods whereby we can build terminating TRSs with full deduction power, with respect to some theory. Yet, there exist theories these techniques cannot handle, giving rise to a trade-off between deduction power and termination. One such theory is the theory of observation congruence in CCS [9]. One approach to overcome the problem is to use heuristic to control search; it, however, introduces the problem of finding and capturing such meta-level control. This paper advocates the use of annotated TRSs (ATRSs) to tackle such problems. Annotations are used to capture meta-level information that helps direct proof search; they give an intuitive account of why and how rewriting is expected to work. We introduce an ATRS called Observant and show how it can be used to build a decision procedure for\u00a0\u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Abstract proof checking: An example motivated by an incompleteness theorem\n", "abstract": " We demonstrate the use of abstraction in aiding the construction of aninteresting and difficult example in a proof-checking system. Thisexperiment demonstrates that abstraction can make proofs easier tocomprehend and to verify mechanically. To support such proof checking, wehave developed a formal theory of abstraction and added facilities for usingabstraction to the GETFOL proof-checking system.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Automatic guidance of mechanically generated proofs\n", "abstract": " Formal methods of system development provide a high degree of reliability. For instance, they enable us to show that a system meets its speci cation. Both the system and its speci cation are represented as logical expressions and then mathematical reasoning can be used to prove that the system meets the speci cation. The proofs required are not deep, but they are often very long and complicated. Mechanical assistance is usually desirable to reduce both the tedium and the chance of error.Such research is identi ed as one of the core software and system areas in the IT and Electronics Technology Foresight Panel Report under x4. 1.2. 1\\Speci cation, modelling, analysis, veri cation and validation of large, complex and dynamic IT systems and networks\". Research on rigorous jusi cation of software safety cases is also identi ed in x4. 1.2. 3 as a speci c technology opportunity.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "General techniques for automatic program optimization and synthesis through theorem proving\n", "abstract": " We report on program optimization research within the proofs as programs paradigm. Firstly, we describe program optimization by the transformation of program synthesis proofs (ie constructive existence proofs). Synthesis proofs which yield inefficient programs are transformed into analogous proofs which yield more efficient programs. The key to program optimization lies in the transformation of the various induction schemas employed in synthesis proofs. This belief stems from the extensive work of Boyer and Moore concerning the dualities between induction and recursion. A system has been implemented which optimizes simple recursive behaviour by automatically transforming the associated synthesis proof structures (notably, the forms of mathematical induction used to synthesize recursive program constucts). A second approach to program optimization does not concern how target synthesis proofs can be\" mapped\" from source proofs. Rather, the question is can we devise a means by which these proofs can be automatically constructed. without the use of a source proof (but only the source equational definitions). A promising strategy is to use the\" proof planning\" approach [Bundy et aI, 199J. b] to theorem proving and to employ meta-variables at the meta-level planning phase which allow the planning to proceed even though certain object-level objects are (partially) unknown (such a strategy being known as middle-out reasoning [Bundy et aI, 1990a]). Subsequent planning provides the necessary information which, together with the original definitional equations. will allow us to instantiate such meta-variables through higher-order\u00a0\u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "The EcoLogic System\n", "abstract": " The EcoLogic System \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation The EcoLogic System Dave Robertson, A Bundy, M Uschold, R Muetzelfeldt School of Informatics School of GeoSciences Research output: Book/Report \u203a Book Overview Original language English Publisher Department of Artificial Intelligence, University of Edinburgh Publication status Published - 1988 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Robertson, D., Bundy, A., Uschold, M., & Muetzelfeldt, R. (1988). The EcoLogic System. Department of Artificial Intelligence, University of Edinburgh. Robertson, Dave ; Bundy, A ; Uschold, M ; Muetzelfeldt, R. / The \u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Helping inexperienced users to construct simulation programs: an overview of the ECO project\n", "abstract": " We provide an overview of the development of EGO, a program which enables ecologists with minimal mathematical or computing skills to build simulation models. The first version of this system used a System Dynamics formalism to represent users' models and relied on simple interface techniques. Subsequent trials revealed that the formalism was insufficiently expressive to represent the sophisticated models which users sometimes required. The system was also over-reliant upon users to drive dialogue during model construction and provided insufficient guidance for inexperienced users. We discuss techniques for solving these problems. Finally, we note the key contributions of this research in the context of related work.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "An expert system for medical diagnosis\n", "abstract": " An expert system for medical diagnosis \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation An expert system for medical diagnosis Alan Bundy School of Informatics Research output: Chapter in Book/Report/Conference proceeding \u203a Chapter Overview Original language English Title of host publication Intelligent knowledge based systems Editors T. O\u2019Shea, JS Self, G. Thomas Publisher Harper & Row ISBN (Print) 0000000000 Publication status Published - 1987 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (1987). An expert system for medical diagnosis. In T. O\u2019Shea, JS Self, & G. Thomas (Eds.), Intelligent knowledge based \u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Synthesis of simulation models from high level specifications\n", "abstract": " Synthesis of Simulation Models from High Level Specifications \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Synthesis of Simulation Models from High Level Specifications D. Robertson, A. Bundy, M. Uschold, R. Muetzelfeldt School of Informatics School of GeoSciences Research output: Working paper Overview Original language English Publication status Published - 1987 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Robertson, D., Bundy, A., Uschold, M., & Muetzelfeldt, R. (1987). Synthesis of Simulation Models from High Level Specifications. Robertson, D. ; Bundy, A. ; Uschold, M. ; Muetzelfeldt, R. / Synthesis of Simulation \u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Non-Monotonic Reasoning\n", "abstract": " Non-monotonic logics deal with non-monotonic reasoning, that involves adopting assumptions that may have to be abandoned in the light of new information. This reasoning is called non-monotonic in contrast with the monotonicity of deductive logic, in which the addition of new axioms to a set of axioms can never decrease the set of theorems or facts; quite often the new axioms give rise to new theorems so that the set of theorems grows monotonically with the set of axioms. In nonmonotonic logics, the set of theorems may lose members as well as gain members when new axioms are added.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "SAIL\n", "abstract": " Unlike many AI languages SAIL Is compiler (rather than Interpreter) based. Developed on top of ALGOL-60 it is also block structured. SAIL was originally designed with vision and speech understanding systems in mind, and incorporates a fast associative retrieval facility called LEAP.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Partitioned semantic net\n", "abstract": " Means of enhancing the organisational and expressive power of semantic nets<227> through the grouping of nodes and links, associated with Hendrix. Nodes and links may figure in one or more \u2018spaces\u2019, which may themselves be bundled into higherlevel \u2018vistas\u2019, which can be exploited autonomously and structured hierarchically. The effective encoding of logical statements involving connectives and quantifiers was an important motivation for partitioning, but the partitioning mechanisms involved are sufficiently well-founded, general and powerful to support the dynamic representation of a wide range of language and world knowledge; and partitioned nets have been extensively used for a range of such purposes at SRI.", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Computational models for problem solving\n", "abstract": " Computational Models for Problem Solving \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Computational Models for Problem Solving Alan Bundy School of Informatics Research output: Book/Report \u203a Book Overview Original language English Publisher The Open University Press Publication status Published - 1978 Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (1978). Computational Models for Problem Solving. The Open University Press. Bundy, Alan. / Computational Models for Problem Solving. The Open University Press, 1978. Bundy, A 1978, Computational Models for Problem Solving. The Open University Press. \u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Representing semantic information in pulley problems\n", "abstract": " Several recent research reports have focussed on the representation and solution of problems in semantically rich domains (1),(2),(3),(4). These include analysis of algebra word problems as well as problems in mechanics and thermodynamics. The term \" semantically rich \" characterizes these domains well in that successful problem solvers possess large amounts of problem specific information and task related knowledge. Compare this with the knowledge and information needed to solve tasks such as the Tower of Hanoi or cryptarithmetic. The MECHO group (2) has employed the familiar protocol analysis technique in an attempt to determine what information the person attempting to solve pulley problems has available, and further,", "num_citations": "3\n", "authors": ["1609"]}
{"title": "There is no best proof procedure\n", "abstract": " Intelligent question-answering programs do more than retrieve\" raw\" data; they make deductive inferences in order to return all valid responses. They report logical inconsistencies, possibly at the data input phase. Similarly, more information is requested from the user if a question asked proved to be ambiguous.A question-answering system of the above type has been designed and implemented. Besides retrieving explicit and implicit temporal relations, the system discovers potentially causal relationships which also satisfy different time restrictions. Questions concerning a generalized concept of co-existence can also be answered. It is hoped that programs of a similar nature will become of much pragmatic use to researchers in physics, chemistry, biology, and so on, in evaluating complex, interrelated experimental data. Several additional applications for this type of program are mentioned, ranging from problems\u00a0\u2026", "num_citations": "3\n", "authors": ["1609"]}
{"title": "Modelling Virtual Bargaining using Logical Representation Change\n", "abstract": " A recently developing body of empirical work on joint problem-solving explores limit cases of human coordination, where signalling conventions can still be efficiently formed, and flexibly revised, even without sufficient information bandwidth to coordinate them. In a typical example, pairs of human participants are presented with tasks where (1) the information required to complete each task, and (2) the capacity to act on that information, are divided between them. One participant\u2014a sender-holds key information but cannot act on it. The other participant\u2014a receiver\u2014can take the actions needed but needs additional information to select these actions among possible alternatives (Misyak et al., 2016). Neither participant can use language, or another medium of sufficient bandwidth to express all of the information required. Yet human participants comfortably succeed in these'impossible'coordination games. Humans select optimal moves (Misyak and Chater, 2014), create appropriate conven-tions (Misyak et al., 2016), and develop these initial conventions appropriately as the task complexity grows (Misyak and Chater, 2017) in order to maximize their joint profit\u2014all this despite a greatly restricted communication channel. This success in the face of insufficient explicit communication motivates the theory of virtual bargaining. Virtual bargaining rests on the need for additional inference to bridge the gap between the information available, and the information required to interpret players\u2019 signals. According to virtual bargaining, this added inference takes the form of a\u2018what if'scenario played out privately by both players, each adopting the most beneficial\u00a0\u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Explainable Inference in the FRANK Query Answering System\n", "abstract": " The demand for insights into how artificial intelligent systems work is rapidly growing. This has arisen as AI systems are being integrated into almost every aspect of our lives from finance to health, security and our social lives. Current techniques for generating explanations focus on explaining opaque algorithms such as neural network models. However, considering the fact that these models do not work in isolation, but are combined, either manually or automatically, with other inference operations, local explanations of individual components are simply not enough to give the user adequate insights into how an intelligent system works. It is not unusual for a system made up of fairly intuitive components to become opaque when it is combined with others to build an intelligent agent.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "European collaboration on automated reasoning\n", "abstract": " I provide a personal perspective of a European collaboration in the area of automated reasoning. I describe the birth and growth of this collaboration and give a snapshot of its current state. My involvement started with a two-way collaboration on inductive theorem proving between my group in Edinburgh and Siekmann's group in Saarbr\u00fccken, but this grew to broaden both in research area and geographical area. It focused on several series of workshops/conferences, research grants and bilateral lab visits, and established both new applications and collaborations with researchers outwith automated reasoning.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Reasoning with context in the semantic web\n", "abstract": " Reasoning with Context in the Semantic Web \u2013 Projects \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Reasoning with Context in the Semantic Web Jos Lehmann (Editor), IJ Varzinczak (Editor), Alan Bundy (Editor) School of Informatics Research output: Contribution to journal \u203a Special issue \u203a peer-review Overview Projects (1) Projects Projects per year 2008 2011 1 Finished 1 results Status, start date (descending) Title Start date End date Type Status, start date(ascending) Filter Finished Search results Finished Ontology Evolution in Physics Bundy, A. EPSRC 1/07/08 \u2192 31/12/11 Project: Research University of Edinburgh Research Explorer Logo \u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "A small experiement in event-b rippling\n", "abstract": " [width=0.15]../../resources/ai4fm-logo/ai4fm-logo-full 2em A (very) small experiment in Event-B rippling Page 1 AI4FM Rippling Experiment Help! Summary A (very) small experiment in Event-B rippling Gudmund Grov, Alan Bundy & Lucas Dixon Rodin Workshop, Dusseldorf 2010 Page 2 AI4FM Rippling Experiment Help! Summary Talk outline \u25b6 \u201cAbrial\u2019s MMPE rule\u201d : n \u2217 x/100 \u2217 f \u2217 p \u2217 20 \u25ba 100, 000 loc 3-12 Man Months of Proof Effort \u25b6 BUT, \u2203 families of Event-B UPOs based on proof strategy \u25ba AI4FM tries to explore such families to increase automation. \u25b6 This requires high-level proof strategies \u25ba rippling is an example of a high level proof strategy \u25ba implemented in Isaplanner. \u25b6 In this talk we will: \u25ba give an overview of the AI4FM project \u25ba describe a simple Event-B experiments with rippling/Isaplanner \u25ba beg for help! Page 3 AI4FM Rippling Experiment Help! Summary AI4FM overview \u25b6 The user \u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Productive use of failure in top-down formal methods\n", "abstract": " Proof obligations (POs) arise when applying formal methods (FMs). The undischarged POs can become a bottleneck in the use of FMs in industry. The work we proposed here, as a part of AI4FM project, aims at increasing the proportion of discharged POs by analysing failed proofs and related patches to classify POs as families and construct proof strategies, which can be used to guide proof search for the POs in the same families.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "A single-significant-digit calculus for semi-automated guesstimation\n", "abstract": " We describe a single-significant-digit calculus for estimating approximate solutions to guesstimation problems. The calculus is formalised as a collection of proof methods, which are combined into proof plans. These proof methods have been implemented as rewrite rules and successfully evaluated in an interactive system, gort, which forms a customised proof plan for each problem and then executes the plan to obtain a solution.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Formalising term synthesis for IsaCoSy\n", "abstract": " IsaCoSy is a theory formation system for inductive theories. It synthesises conjectures and uses the ones that can be proved to produce a background theory for a new formalisation within a proof assistant. We present a formal account of the algorithms implemented in the system, and prove their correctness. In particular, we show that IsaCoSy only produces irreducible terms, using constraints generated from the left-hand sides of a set of rewrite rules.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Concurrent-distributed programming techniques for SAT using DPLL-st\u00e5lmarck\n", "abstract": " This paper reports our work on application of concurrent/distributed techniques to SAT. These were investigated using each of the following methods: the DPLL algorithm, the dilemma rule of the Stalmarck's algorithm and a concurrent/distributed hybrid SAT solver: DPLL-Stalmarck, using a combination of the DPLL and the dilemma rule based algorithm. The prototypes have been implemented using Alice, an SML based language with support for distribution and concurrency. Our prototype framework allows for rapid-prototyping of and experimentation with application of various concurrent/distributed programming techniques to SAT. The emphasis is not on building an industry-standard SAT solver, but rather an investigation of the efficacy of use of these techniques for SAT at various levels of granularity.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "On process equivalence= equation solving in ccs\n", "abstract": " Unique Fixpoint Induction (UFI) is the chief inference rule to prove the equivalence of recursive processes in the Calculus of Communicating Systems (CCS)\u00a0(Milner\u00a01989). It plays a major role in the equational approach to verification. Equational verification is of special interest as it offers theoretical advantages in the analysis of systems that communicate values, have infinite state space or show parameterised behaviour. We call these kinds of systems VIPSs. VIPSs is the acronym of Value-passing, Infinite-State and Parameterised Systems. Automating the application of UFI in the context of VIPSs has been neglected. This is both because many VIPSs are given in terms of recursive function symbols, making it necessary to carefully apply induction rules other than UFI, and because proving that one VIPS process constitutes a fixpoint of another involves computing a process substitution, mapping states of\u00a0\u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Fast, but approximate, workflow-runtime estimation using the bell-curve calculus\n", "abstract": " \u2022 We evaluate both the accuracy and efficiency of this Bell-Curve Calculus approach compared to alternative approaches. In particular, we show that it is much quicker than piecewise approximation approaches, but trades this off against a loss of accuracy, which nevertheless is sufficient for some kinds of application.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "A comparison of two proof critics: Power vs. robustness\n", "abstract": " Proof critics are a technology from the proof planning paradigm. They examine failed proof attempts in order to extract information which can be used to generate a patch which will allow the proof to go through.               We consider the proof of the \u201cwhisky problem\u201d, a challenge problem from the domain of temporal logic. The proof requires a generalisation of the original conjecture and we examine two proof critics which can be used to create this generalisation. Using these critics we believe we have produced the first automatic proofs of this challenge problem.               We use this example to motivate a comparison of the two critics and propose that there is a place for specialist critics as well as powerful general critics. In particular we advocate the development of critics that do not use meta-variables.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Automated Theory Formation for Tutoring Tasks in Pure Mathematics\n", "abstract": " The HR program forms mathematical theories from as little information as the axioms of a domain. The theories include concepts with examples and de nitions, conjectures, theorems and proofs. Moreover, HR uses third party mathematical software including automated theorem provers and model generators. We suggest that a potential role for theory formation systems such as HR is as an aid to mathematics lecturers. We discuss an application of HR to the generation of a set of group theory exercises. This forms part of a project using HR to make discoveries in Zariski spaces, which is also detailed.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Searching for a solution to program verification= equation solving in CCS\n", "abstract": " Unique Fixpoint Induction, UFI, is a chief inference rule to prove the equivalence of recursive processes in CCS [7]. It plays a major role in the equational approach to verification. This approach is of special interest as it offers theoretical advantages in the analysis of systems that communicate values, have infinite state space or show parameterised behaviour.               The use of UFI, however, has been neglected, because automating theorem proving in this context is an extremely difficult task. The key problem with guiding the use of this rule is that we need to know fully the state space of the processes under consideration. Unfortunately, this is not always possible, because these processes may contain recursive symbols, parameters, and so on.               We introduce a method to automate the use of UFI. The method uses middle-out reasoning and, so, is able to apply the rule even without elaborating the\u00a0\u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Verification of diagrammatic proofs\n", "abstract": " Human mathematicians often\\prove\" theorems by the use of diagrams and manipulations on them. We call these diagrammatic proofs. In (Jamnik, Bundy, & Green 1997) we presented how\\informal\" reasoning with diagrams can be automated. Three stages of proof extraction procedure were identi ed. First, concrete rather than general diagrams are used to prove particular instances of the universally quanti ed theorem. The diagrammatic proof is captured by the use of geometric operations on the diagram. Second, an abstracted schematic proof of the universally quantied theorem is automatically induced from these proof instances. Third, the nal step is to con rm that the abstraction of the schematic proof from the proof instances is sound. Our main focus in this paper is on the third stage, the veri cation of schematic proofs. We de ne a theory of diagrams where we prove the correctness of a schematic proof. We give an example of an extraction of a schematic proof for a theorem about the sum of odd naturals, and prove its correctness in the theory of diagrams.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Automating reasoning support for design\n", "abstract": " Formalised design supported by automated reasoning can assist in the management of requirements-a particular problem for large, detailed systems. Designers developing an initial requirements into more detail and then producing a system specification must show not only that all the requirements have been met but also demonstrate how that has been achieved. This is especially important in safety-critical systems where sections of the requirements will be regulations or guidelines. Using real life examples from emergency shutdown systems for drilling rigs1, we show how lightweight (and therefore less time-consuming) formalisation supports validation in an engineering approach to requirements management. We have developed a requirements assistant-an interactive system for formalising and managing information about requirements including guideline requirements. As a design proceeds, relevant requirements are found automatically and checked before being notified to the designer with an accompanying explanation of whether or not they are currently satisfied. Progress in satisfying requirements is monitored automatically and contributing choices are recorded. Such evidence of adherence to guidelines is an assurance of the validity of the design. During any subsequent system modification, reference to this evidence can aid designers by drawing attention to the implications changes will have on maintaining guideline satisfaction. This paper describes how this automated reasoning support works using the demonstrator we have built.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "A general technique for automatically generating efficient programs through the use of proof planning\n", "abstract": " A general framework for synthesizing efficient programs, using tools such as higher-order unification (henceforth HOU), has been developed and holds promise for encapsulating an otherwise diverse, and often ad hoc, range of transformation techniques. A prototype system has been implemented. Proof plans \u2014 formal outlines of constructive proofs \u2014 are used to control the (automatic) synthesis of the efficient programs from standard definitional equations [1,6,5]. Programs are specified in the standard equational form within the logic of the OYSTER proof refinement system. The construction of the improved functions is automatically controlled using the CLAM proof planner [2].", "num_citations": "2\n", "authors": ["1609"]}
{"title": "On Dempster's combination rule\n", "abstract": " Critical analysis about using Dempster's combination rule has been carried out by many researchers. In this paper, continuing the discussions in [8],[32], we further explore the nature of combination and achieve the following results. 1). The condition of combination in Dempster's original combination framework is stricter than that required by Dempster's combination rule in Dempster-Shafer theory of evidence. 2). Some counterintuitive results of using Dempster's combination rule shown in some papers are caused by overlooking (or ignorance of) different independence conditions required by Dempster's original combination framework and Dempster's combination rule. 3). An alternative approach to the combination of different pieces of evidence by using incidence calculus is proposed. 4). In this approach, we can combine not only independent pieces of evidence but also dependent pieces of evidence. Thus incidence calculus combination extends Dempster's combination rule.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "A general technique for automatically optimizing programs through the use of proof plans\n", "abstract": " The use of proof plans -formal patterns of reasoning for theorem proving -to control the {automatic) synthesis of efficient programs from standard definitional equations is described. A general framework for synthesizing efficient programs, using tools such as higher-order unification, has been developed and holds promise for encapsulating an otherwise diverse, and often ad hoc, range of transformation techniques. A prototype system has been implemented. Proof plans are used to control the (automatic) synthesis of functional programs, specified in a standard equational form, t', by using the proofs as programs principle. The goal is that the program extracted from a constructive proof of the specification is an optimization of that defined solely by \u00a3. Thus the theorem proving process is a form of program optimization allowing for the construction of an efficient, target, program from the definition of an inefficient, source, program. The general technique for controlling the syntheses of efficient programs involves using t' to specify the target program and then introducing a new sub-goal into the proof of that specification. Different optimizations are achieved by placing different characterizing restrictions on the form of this new sub-goal and hence on the subsequent proof. Meta-variables and higher-order unification are used in a technique called middle-out reasoning to circumvent eureka steps concerning, amongst other things, the identification of recursive data-types, and unknown constraint functions. Such problems typically require user intervention.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "The nature of AI principles: a debate in the AISB Quarterly\n", "abstract": " The nature of AI principles: a debate in the AISB Quarterly \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation The nature of AI principles: a debate in the AISB Quarterly S. Ohlsson, Alan Bundy School of Informatics Research output: Chapter in Book/Report/Conference proceeding \u203a Chapter Overview Original language English Title of host publication The Foundations of Arti\ufb01cial Intelligence Editors D. Partridge, Y. Wilks Publisher Cambridge University Press ISBN (Print) 0521359449 Publication status Published - 1990 Access to Document http://rsta.royalsocietypublishing.org/content/363/1835.toc Cite this APA Author BIBTEX Harvard \u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "IJCAI Policy on Multiple Publication of Papers Revisited\n", "abstract": " In 1988 the IJCAI Trustees proposed a policy on the multiple submission and multiple publication of research papers. This proposal was made in an article, written by me on behalf of the Trustees, which was published in various AI magazines and newsletters during late 1988 and early 1989. We invited comments on our proposal-and we certainly provoked some. Having considered these comments we have now revised our proposal to bring them into line with what we perceive as the emerging consensus.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "High-emphasis filtering\n", "abstract": " A method for sharpening images. The differentiation of an image is grossly interpreted in the frequency domain as filtering that emphasizes higher frequency components. As any linear operator in the spatial domain can be converted into an equivalent transfer function in the frequency domain, a linear operator designed to emphasize abrupt changes in intensity can be implemented by a transfer function designed to emphasize areas of high frequency.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Skolemization\n", "abstract": " A technique borrowed from mathematical logic (and named after the mathematician Skolem). but much used in automatic theorem proving, for removing quantifiers from predicate calculus <189> formulae. If A(y) is a formula with free variables y, x1, ..., xn then \u2200y A(y) is replaced by A(y), and \u2203y A(y) is replaced by A(f(x1,....,xn)), where f is a new Skolem function. The technique is usually applied to formulae which have all their quantifiers at the front (Prenex normal form), but can be adapted to any formula. It produces a formula which has a model if and only If the original formula does.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Markgraf Karl Refutation Procedure\n", "abstract": " Developed at the University of Karlsruhe, West Germany, the Markgraf Karl is one of the largest theorem proving projects undertaken. It Is implemented in INTERLISP <103> and advances the thesis that theorem provers must be guided by large amounts of domain specific knowledge in order to overcome the combinatorial explosions traditionally associated with ungulded theorem provers.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Depth-first search\n", "abstract": " An uninformed graph searching strategy which searches the graph by exploring each possible path through it until either the required solution or a previously encountered node is encountered. The nodes are expanded in order of depth; with the deepest node expanded first and nodes of equal depth expanded in an arbitrary order. To prevent searching of an infinite path, a depth-bound is usually fixed and nodes below this depth are never generated, thus the strategy is neither guaranteed to produce the shortest path to the solution if one exists, nor to find a solution even if one exists.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Meta-level inference\n", "abstract": " The prefix meta- (contrast object- ) denotes a language whose subject matter is the representation of some theory, as distinct from the theory itself. Thus                 contains two occurences of the unknown \u2032x\u2032  are statements in respectively the theory and the meta-theory of algebra.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Alpha/beta pruning\n", "abstract": " A refinement of minimax <145> to determine the optimal move in a game. Nodes that are not needed to evaluate the possible moves of the top node are \u2018pruned\u2019. Suppose that MAX is to move at parent node P. and that it is known from previous calculations that daughter D1 guarantees a minimum gain of say +20 for MAX. Now we start exploring D2 and discover that the opponent can force a maximal gain of +10 by reacting to D2 with D2. 1. In this case there is no need to explore other daughters of D2, because MAX can never gain more than +10 and therefore will always prefer D1. Following this line of reasoning, both from the point of view of MAX and of MIN, large parts of the tree need not be explored and an optimal solution will still be found.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Homogenization: Preparing Equations for Change of Unknown\n", "abstract": " Homogenization Preparing equations for Change of Unknown - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/647942 Title : Homogenization Preparing equations for Change of Unknown Authors : Silver, B. ; Bundy, A. ; Corporate author : Edinburgh Univ. (UK). Dept. of Artificial Intelligence ; Publication year : 1981 Language : English ; Pagination/Size : 15 p. ; SIGLE classification : 12A - Pure mathematics ; Document type : R - Report ; Report number : DAI-RP--159 ; Other identifier : GB ; GB_ 1982:5928 ; handle : http://hdl.handle.net/10068/647942 Provenance : SIGLE ; Get a copy : BLDSC - British Library Document Supply Centre Availability : LD:3511.638(DAI-RP--159). Country : United Kingdom ; CNRS INIST Creative Commons Help Search Paper order \u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Artificial mathematicians: the computational modelling of mathematical reasoning\n", "abstract": " Artificial mathematicians The computational modelling of mathematical reasoning - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/625729 Title : Artificial mathematicians The computational modelling of mathematical reasoning Author : Bundy, A. ; Corporate author : Edinburgh Univ. (UK). Dept. of Artificial Intelligence ; Publication year : 1981 Language : English ; Pagination/Size : 197 p. ; SIGLE classification : GB_ 1981:3747 ; handle : http://hdl.handle.net/10068/625729 Provenance : SIGLE ; Get a copy : BLDSC - British Library Document Supply Centre Availability : LD:3511.636(DAI-OP--24). Country : United Kingdom ; CNRS INIST Creative Commons Help Search \u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "Can domain specific knowledge be generalized?\n", "abstract": " The MECHO project (see Cl]) consists of writ ing a computer program which can colve a wide var iety of simple mechanics problems stated in English. This program is being used as a vehicle for study ing methods for guiding search in a semantically rich domain. Our methodology is to find general, justifiable, inference rules which can be combined to carry out the reasoning necessary to solve the mechanics problems. As is well known, when rules like these are run on a general inference machine the result is often a combinatorial explosion. Rules are combined in unexpected ways and the search for a solution is developed along unreason able paths. These failures are used to debug the rules by adding to them local, domain specific control information. Finally these techniques are generalized and incorporated in the inference mechanism. We hope that this methodology will lead us to the design of a computational logic for natural reasoning.In this paper one such transition from domain specific to general inference technique will be described. We will use this example to emphasize the importance of this generalization stage. With out it one might be led to superficial and false conclusions about the nature of natural reasoning. Suppose we have available the following re lations: Vel (Object, v, time)-(v is the veloc ity of object during time); At (Object, place, moment)-(object is at place at moment); Final (period, moment)-(moment is the final moment of time interval period). We may have discovered the following domain specific in formation enabling us to guide the search for problem solutions along successful lines,(i) If the program is desperate to\u00a0\u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "A program to solve mechanics problems stated in English\n", "abstract": " A program to solve mechanics problems stated in English \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation A program to solve mechanics problems stated in English Alan Bundy, George Luger, Martha Stone School of Informatics Research output: Working paper Overview Original language English Publication status Unpublished - May 1975 Publication series Name DAI Working paper No. 8 Access to Document Full textAccepted author manuscript, 626 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A., Luger, G., & Stone, M. (1975). A program to solve mechanics problems stated in English. (DAI Working paper No. 8). Bundy, \u2026", "num_citations": "2\n", "authors": ["1609"]}
{"title": "The metatheory of the elementary equation calculus.\n", "abstract": " 1.1 E qua lity la (R eflex iv e law) x= x(u se A. 3 tw ice, th en T) 1b (Sym m etric Law)^\u2122(la, S bi, T) 1c (The a lte rna tiv e form s of T, th e T ra ns itiv e Law) e. g.A= G (lb, T) Remark A ll th ea lte rn a tiv e form sof T w ill be den o ted T. In view of th etransitivityofequa lity we may w rite Ao= Ai=...= An-i and deduce Ao= An~ i\u2022 This w ill a ls o be d enoted T.", "num_citations": "2\n", "authors": ["1609"]}
{"title": "GPy-ABCD: A Configurable Automatic Bayesian Covariance Discovery Implementation\n", "abstract": " Gaussian Processes (GPs) are a very flexible class of nonparametric models frequently used in supervised learning tasks because of their ability to fit data with very few assumptions, namely just the type of correlation (kernel) the data is expected to display. Automatic Bayesian Covariance Discovery (ABCD) is an iterative GP regression framework aimed at removing the requirement for even this initial correlation form assumption. An original ABCD implementation exists and is a complex stand-alone system designed to produce long-form text analyses of provided data. This paper presents a lighter, more functional and configurable implementation of the ABCD idea, outputting only fit models and short descriptions: the Python package GPy-ABCD, which was developed as part of an adaptive modelling component for the FRANK query-answering system. It uses a revised model-space search algorithm and removes a search bias which was required in order to retain model explainability in the original system.Ethics Statement: This paper presents a library implementing an improved version of an already existing framework for automatically selecting an interpretable-shape Gaussian Process model for input data. Given the very basic nature of the output (ie a list of statistical models with short descriptions) the areas of applications are the same as, say, linear regression or time series analysis, but the real utility lies in extracting the functional shape features of the fit models. In constructing larger systems relying on these identified data features (and their text description), the typical risks of introducing model explainability apply, such as overreliance on\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The Use of Max-Sat for Optimal Choice of Automated Theory Repairs\n", "abstract": " The ABC system repairs faulty Datalog theories using a combination of abduction, belief revision and conceptual change via reformation. Abduction and Belief Revision add/delete axioms or delete/add preconditions to rules, respectively. Reformation repairs them by changing the language of the faulty theory. Unfortunately, the ABC system overproduces repair suggestions. Our aim is to prune these suggestions to leave only a Pareto front of the optimal ones. We apply an algorithm for solving Max-Sat problems, which we call the Partial Max-Sat algorithm, to form this Pareto front.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "A Common Type of Rigorous Proof that Resists Hilbert\u2019s Programme\n", "abstract": " Following Hilbert, there seems to be a simple and clear definition of mathematical proof: it is a sequence of formulae each of which is either an axiom or follows from earlier formulae by a rule of inference. Automated theorem provers are based on this Hilbertian concept\u00a0of proof, in which the formulae and rules of inference are represented in a formal logic. These logic-based proofs are typically an order of magnitude longer than the rigorous proofs\u00a0produced by human mathematicians. There is a consensus, however, that rigorous proofs\u00a0could, in principle, be unpacked into logical proofs, but this programme is rarely carried out because it would be tedious and uninformative. We argue that, for at least one class of rigorous proofs, which we will call schematic proofs, such\u00a0a simple unpacking is not available. We will illustrate schematic proofs\u00a0by analysing Cauchy\u2019s faulty proof of Euler\u2019s Theorem V \u2212 E + F = 2\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Towards a Theory of Diagnosis of Faulty Ontologies.\n", "abstract": " We initiate research into a generic theory of diagnosis of faulty ontologies. The proposals are based on, but generalise, our experience with the GALILEO and ORS systems. We make some initial simplifying assumptions, which we hope will not restrict the application of the proposals to new areas. In particular, we look at repairing faulty ontologies where the fault is revealed by an inference failure and the repair is implemented by a signature and/or theory morphism. More concretely, we focus on situations where a false conjecture has been proved. Diagnosis consists of constructing a morphism by analysis of failed inference. It is assumed that an oracle is available that can answer (and sometimes ask) questions, but whose own ontology is otherwise inscrutable.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Evolution of inconsistent ontologies in physics\n", "abstract": " Inconsistency robustness in autonomous software can be seen as a problem of automated reasoning about ontology evolution. Formal ontologies specify the knowledge that software systems use when reasoning about the entities in their domain. Such knowledge is bound to evolve in the face of new information. Robust software should therefore be able to maintain the consistency between its own ontologies and any incoming information that contradicts them. This can be achieved either by isolating the inconsistency or by evolving the ontologies. We propose a higher-order logical approach to ontology evolution and apply it to examples in physics, as advances in this field are naturally modelled as cases of ontology evolution. GALILEO, a system based on this approach, is being implemented and tested. Its basic mechanisms for evolution are ontology repair plans. These operate on ontologies formalised and implemented as contexts, which are logical theories that use their own local concepts to describe the domain, thus preventing potential contradictions with other theories to arise. When, though, ontologies are mapped or aligned, they share axioms. This may allow the proof of contradictory facts that affect the robustness of the system. At this stage, the application of an ontology repair plan may resolve the inconsistency, as each plan compiles together a pattern for diagnosis of conflicts between ontologies and transformation rules for effecting a repair. The repair can combine the retraction of axioms, the change of beliefs as well as the deeper modification of the language in which the ontology is represented.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "A contextual approach to detection of conflicting ontologies\n", "abstract": " The knowledge represented in an ontology can be regarded as merely a perception from a particular perspective\u2013whether it be that of the modeler or of an autonomous agent. Such interpretation is inline with the representation of ontological knowledge in a context, which is often regarded as a subtheory about the world for a particular situation. Our primary interest is to automatically evolve ontologies by diagnosing and repairing ontological faults, so we will describe a preliminary investigation into the detection of conflicts between ontologies by means of analysing relations between contexts. The presentation of the analysis is based on an example of a physics paradox, caused by a contradiction between the predictive theory and sensory data. Suppose a bouncy ball B is suspended at a height above ground and a student, who believes that the total energy (TE) of an object is always defined as the summation of only the kinetic energy (KE) and potential energy (PE), is to predict TE of the ball when it impacts with the ground. Sensory data shows that both the velocity and the height can be deduced to be zero at that moment. This causes a contradiction because the TE at the end of the drop, TE (B, End (Drop)), calculated using the sensory data is zero, whereas the predicted value is positive by the law of energy conservation. The paradox arises from the (wrong) idealisation of the ball as a particle without extent, so the contribution of elastic energy to TE is neglected. This is the bouncing-ball paradox. A natural representation of the paradox is to encode the predictive theory in one ontology (Ot) and the sensory data in another (Os), letting each be a\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The theory behind theorymine\n", "abstract": " We describe the technology behind the TheoryMine novelty gift company. A tower of four computer systems is used to generate recursive theories, then to speculate conjectures in those theories and then to prove these conjectures. All stages of the process are entirely automatic. The process guarantees large numbers of sound, novel theorems of some intrinsic merit.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Properties of IsaCoSy\u2019s constraint generation algorithm\n", "abstract": " This report states and sketches the proofs for some important properties of IsaCoSy\u2019s constraint generation algorithm. We claim that for any rewrite rule, the constraint generation algorithm produces constraints that will prohibit the synthesis of exactly the terms containing a redex which can be rewritten by that rule (and no others).", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Notes of the ECAI-10 Workshop on Automated Reasoning about Context and Ontology Evolution\n", "abstract": " Notes of the ECAI-10 Workshop on Automated Reasoning about Context and Ontology Evolution \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Notes of the ECAI-10 Workshop on Automated Reasoning about Context and Ontology Evolution Alan Bundy, J. Lehmann, G. Qi, I. Varzinczak School of Informatics Research output: Contribution to journal \u203a Article \u203a peer-review Overview Projects (1) Original language English Journal ECAI-10 Workshop on Automated Reasoning about Context and Ontology Evolution Publication status Published - 2010 Access to Document Full textAccepted author manuscript, 1.75 MB Projects Projects per year 2007 2011 1 \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Facilitating virtual interaction through flexible representation\n", "abstract": " Fluent, effortless and diverse e-business transactions depend on the ability of systems to interact automatically. The difficulties of tailoring representation and information to be consistent and therefore interoperable needs to fall not on human users but on automated systems. In this paper, we present our system, ORS, which is designed to be a tool for automated agents acting on behalf of people or systems which need to interact, to enable them to understand one another, despite the fact that they are not centrally or consistently designed.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Harnessing the power of folksonomies for formal ontology matching on-the-y\n", "abstract": " This paper is a short introduction to our work on building and using folksonomies to facilitate communication between Semantic Web agents with disparate ontological representations. We briefly present the Semantic Matcher, a system that measures the semantic proximity between terms in interacting agents' ontologies at run-time, fully automatically and minimally: that is, only for semantic mismatches that impede communication. The system is designed to allow agents to\" understand\" the meanings of terms to be matched by comparing their folksonomy-based\" mental representations\".", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Unite: A new plan for automated ontology evolution in physics\n", "abstract": " We are developing a novel technique for ontology evolution, which we call ontology repair plans. Our development case studies are drawn from instances in the history of physics where experimental observation contradicted current physical theories, which then had to be evolved. In particular, it was often necessary to evolve the representation language of these theories, and not just the physical laws. To date, we have implemented one ontology repair plan for splitting a function into three and another for adding an additional argument to a function. In this paper, we describe a new ontology repair plan Unite, for equating two previously distinct functions.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "An architecture of galileo: A system for automated ontology evolution in physics\n", "abstract": " These mechanisms, called ontology repair plans, resolve logical conflicts between several modular ontologies. To demonstrate that ontology evolution can be automated using ontology repair plans, we propose a flexible architecture for the implementation. The support for inference of formulae that trigger repair plans and modularisation of ontologies is central to the design. Huet\u2019s zipper data structure is to be used to combine the benefits of shallow and deep embeddings. For a high degree of modularity, the management of a collection of ontologies is handled by development graphs.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Automating signature evolution in logical theories\n", "abstract": " The automation of reasoning as deduction in logical theories is well established. Such logical theories are usually inherited from the literature or are built manually for a particular reasoning task. They are then regarded as fixed. We will argue that they should be regarded as fluid.                                                                                1                                                 As P\u00f3lya and others have argued, appropriate representation is the key to successful problem solving [P\u00f3lya, 1945]. It follows that a successful problem solver must be able to choose or construct the representation best suited to solving the current problem. Some of the most seminal episodes in human problem solving required radical representational change.                                                                                        1                                                 Automated agents use logical theories called ontologies. For different agents to communicate they must align their\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Towards a bell-curve calculus for e-science\n", "abstract": " Workflows are playing a crucial role in e-Science systems. In many cases, e-Scientists need to do average case estimates of the performance of workflows. Quality of Service (QoS) properties are used to do the evaluation. We defined the Bell-Curve Calculus (BCC) to describe and calculate the selected QoS properties. The paper presents our motivation of using the BCC and the methodology used during the developing procedure. It also gives the analysis and discussions of the experimental results from the ongoing development.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The Boole Lecture A Very Mathematical Dilemma\n", "abstract": " Mathematics is facing a dilemma at its heart: the nature of mathematical proof. We have known since Church and Turing independently showed that mathematical provability was undecidable that there are theorems whose shortest proofs are enormous. Within the last half-century we have discovered practical examples of such theorems: the classification of all finite simple groups, the Four Colour Theorem and Kepler\u2019s Conjecture. These theorems were only proved with the aid of a computer. But computer proof is very controversial, with many mathematicians refusing to accept a proof that has not been thoroughly checked and understood by a human. The choice seems to be between either abandoning the investigation of theorems whose only proofs are enormous or changing traditional mathematical practice to include computer-aided proofs. Or is there a way to make large computer proofs more accessible to human mathematicians?", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Constructing, selecting and repairing representations of knowledge\n", "abstract": " Using the achievements of my research group over the last 30+ years, I provide evidence to support two hypotheses:", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Towards a bell-curve calculus and its application to e-science\n", "abstract": " Grid computing has an almost ten-year history since it was derived, from an analogy to the power Grid, to denote a proposed distributed computing infrastructure for advanced science and engineering collaborations [1]. It is strongly required by consumers, scientists, engineers, enterprises, countries, and even the whole world to share resources, services and knowledge [2]. This sharing is supported and implemented by web services, software systems designed to support interoperable machine-to-machine interaction over a network. These services can be composed to form workflows in all kinds of structures. It is very helpful to measure the performance of the services because their quality affects the quality of the Grid directly. In scientific workflows, experimental data is generated and propagated from one service to another. It would be useful to get rough estimates of various QoS properties, eg reliability, accuracy, run time, etc. so that e-Scientists could do analysis and evaluations on either services or data produced. We have previously thought about the use of interval arithmetic to calculate error bounds on such estimates. The idea is to extend a numeric value to a number interval, eg we use the interval [41, 43] to represent the possible value of 42. Extended numeric analysis is used as the way of interval propagation in workflows. The simplest example is for a unary and monotonically increasing function f (x), the extended function f*([a, b])=[f (a), f (b)] 3. Using interval arithmetic and propagating error bounds will get the biggest accumulative error during workflow executions, so it is a good method for doing a worst-case analysis.However, in\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Rob Milne: A Tribute to a Pioneering AI Scientist, Entrepreneur, and Mountaineer\n", "abstract": " \" Rob Milne was a remarkable man. He died of a heart attack on the 5th of June 2005 while climbing Mount Everest in Nepal. Milne (48) lived an active life: combining his three'careers' seemingly effortlessly. He was a hi-tech entrepreneur, an AI researcher and a passionate mountaineer. Mount Everest was last on his list of the highest summits on each continent. He was only 400 meters from the top when he died. This publication commemorates and celebrates the life of Rob Milne. It covers all facets of Rob Milne's life and contains contributions by the people who have known him well and pay tribute to his life and his legacy. Rob Milne is survived by his wife Val and his two children Alex and Rosemary. After he died, his wife said in a radio interview:\u201cRob died at the top, doing what he loved.\u201d\"", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The Paradox of the Case Study\n", "abstract": " In order to demonstrate the scalability of automated reasoning techniques, it is important to embark on large-scale case studies. As a field, we need to present a reward system for the completion of such case studies. However, there is a paradox at the heart of the case study. It is this paradox, and its resolution, that I intend to explore in this note.The classic example of the theorem proving case study is Shankar\u2019s proof of Godel\u2019s incompleteness proof using the Nqthm theorem prover [Shankar, 1994]. Shankar\u2019s reward for this case study was that he received a PhD. Note that the theorem prover was developed by third parties: Shankar\u2019s supervisors Bob Boyer and J Moore.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The application of deductive synthesis techniques to the rapid assembly and re-assembly of grid applications\n", "abstract": " 2.1 BackgroundThe recent epsrc report \u201cComputer Science Challenges to Emerge from eScience\u201d identifies the \u201crapid customised assembly of services\u201d and \u201cautonomic computing\u201d as two of the major challenges that must be solved if the dream of the Grid is to become a reality. We propose an exploratory investigation to discover whether techniques of deductive synthesis can address these two challenges.2.1. 1 eScience and the GridWe take eScience to be the application of high-performance computing and high-bandwidth communications to the automatic processing and interpretation of the massive data sets arising from data-intensive scientific experiments, such as the Large Hadron Collider, the Human Genome Project and Earth-monitoring satellites. Many sciences are becoming data-intensive, so progress in eScience is an essential ingredient in further scientific advance. We take the Grid to be a globally distributed, hetereogenous collection of data-sources and data-processors, which can be combined in a wide variety of ways to provide customised knowledge retrieval and processing for scientists. We will call these customised systems, Grid applications, and their atomic components, Grid services.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Towards the Integration of Difference Removal and Difference Moving\n", "abstract": " We describe work in progress to combine techniques of difference moving, ie rippling, with techniques of difference removal, such as E-resolution, RUE resolution and equality graphs. This will allow these two different difference reduction techniques to be interleaved, greatly adding to the power of each. The proposal is to adapt the rippling measure to encompass difference removal. This rippling measure will provide much-needed heuristic control to difference removal.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Using Implicit Induction to Guide a Parallel Search for Inconsistency\n", "abstract": " We describe the first full implementation of the Comon-Nieuwenhuis method for implicit induction, including a consistency checker, in a novel system where the proof and refutation programs communicate via sockets. This allows the system to attempt to prove and disprove a conjecture at the same time, using parallel theorem proving processes. As well as refuting several non-theorems, this system has accomplished what is, to the best of our knowledge, the first fully automated proof by implicit induction of the commutativity of gcd. This had been posed as a challenge problem to the technique in the past.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Solving integrals at the method level\n", "abstract": " Solving integrals at the method level (poster session) | Symbolic computation and automated reasoning ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleBooksSymbolic computation and automated reasoningSolving integrals at the method level (poster session) chapter Solving integrals at the method level (poster session) Share on Authors: Alex Heneveld profile image Alex Heneveld View Profile , Ewen Maclean profile image Ewen Maclean View Profile , Alan R Bundy profile image Alan Bundy View Profile , Jacques D Fleuriot profile image Jacques Fleuriot View Profile , Alan Smaill profile image Alan Smaill View Profile Authors Info \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Strict general setting for building decision procedures into theorem provers\n", "abstract": " The efficient and flexible incorporating of decision procedures into theorem provers is very important for their successful use. There are several approaches for combining and augmenting of decision procedures; some of them support handling uninterpreted functions, congruence closure, lemma invoking etc. In this paper we present a variant of one general setting for building decision procedures into theorem provers (gs framework [18]). That setting is based on macro inference rules motivated by techniques used in different approaches. The general setting enables a simple describing of different combination/augmentation schemes. In this paper, we further develop and extend this setting by an imposed ordering on the macro inference rules. That ordering leads to a\u201d strict setting\u201d. It makes implementing and using variants of well-known or new schemes within this framework a very easy task even for a non-expert user. Also, this setting enables easy comparison of different combination/augmentation schemes and combination of their ideas.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "A critique of proof planning\n", "abstract": " Proof planning is an approach to the automation of theorem proving in which search is conducted, not at the object level, but among a set of proof tactics. This approach dramatically reduces the amount of search but at the cost of completeness. We critically examine proof planning, identifying both its strengths and weaknesses. We use this analysis to explore ways of enhancing proof planning to overcome its current weaknesses.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Leaky virtual machines and the best of both worlds\n", "abstract": " The concept of virtual machine allows us to combine the dynamical and computational hypotheses in an investigation of cognition. Van Gelder explicitly rejects this approach, but not only does it allow us to use the modelling technique most appropriate to the task, it also opens up a new range of phenomena where these techniques interact.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Rippling: greatest hits\n", "abstract": " Rippling: greatest hits \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation Rippling: greatest hits Alan Bundy School of Informatics Research output: Working paper \u203a Discussion paper Overview Original language English Publication status Unpublished - 4 Apr 1996 Access to Document Full textAccepted author manuscript, 8.85 MB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (1996). Rippling: greatest hits. Bundy, Alan. / Rippling: greatest hits. 1996. Bundy, A 1996 'Rippling: greatest hits'. Rippling: greatest hits. / Bundy, Alan. 1996. Research output: Working paper \u203a Discussion paper Bundy A. Rippling: greatest hits. 1996 Apr 4. \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Artificial mathematicians\n", "abstract": " CiNii \u8ad6\u6587 - Artificial mathematicians CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 Artificial mathematicians BUNDY Alan \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BUNDY Alan \u53ce\u9332\u520a\u884c\u7269 http://www.dai.ed.ac.uk/daidb/staff/personal_pages/bundy http://www.dai.ed.ac.uk/daidb/staff/personal_pages/bundy, 1996 \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u77e5\u7684\u6559\u6388\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u554f\u984c\u89e3\u6c7a\u904e\u7a0b\u306e\u5b9a\u5f0f\u5316 \u677e\u7530 \u6607 , \u5ca1\u672c \u654f\u96c4 \u96fb\u5b50\u60c5\u5831\u901a\u4fe1\u5b66\u4f1a \u6280\u8853\u7814\u7a76\u5831\u544a. KBSE, \u77e5\u80fd\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u5de5\u5b66 96(595), 81-88, 1997-03-18 \u53c2\u8003\u6587\u732e9\u4ef6 Tweet \u5404\u7a2e\u30b3\u30fc\u30c9 NII\u8ad6\u6587ID(NAID) 10022373015 \u8cc7\u6599\u7a2e\u5225 \u305d\u306e\u4ed6 \u30c7\u30fc\u30bf\u63d0\u4f9b\u5143 CJP\u5f15\u7528 \u66f8\u304d\u51fa\u3057 RefWorks\u306b\u66f8\u304d\u51fa\u3057 EndNote\u306b\u66f8\u304d\u51fa\u3057 Mendeley\u306b\u66f8\u304d\u51fa\u3057 Refer/BiblX\u3067\u8868\u793a RIS\u3067\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Proof Planning the Verification of CCS Programs\n", "abstract": " The veri cation of CCS programs has often been characterised as an expensive, time-consuming, and error-prone task, where computer assistance is thought to be essential. Yet, existing theorem proving based frameworks, for the veri cation of programs using CCS, are no much use because their poor level of automation. In this paper, we propose the use of proof plans as the chief mechanism for the automation of CCS program veri cation. We present a collection of proof plan methods that guide a proof plan formation for the veri cation of deterministic, and divergence-free CCS programs. The meta-level reasoning embedded in these methods takes full advantage of the operational interpretation of processes while outputting plan steps to the appropriate object-level representation.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Automation of diagrammatic proofs in mathematics\n", "abstract": " Theorems in automated theorem proving are usually proved by logical formal proofs. However, there is a subset of problems which can also be proved in a more informal way by the use of geometric operations on diagrams, so called diagrammatic proofs. Insight is more clearly perceived in these than in the corresponding logical proofs: they capture an intuitive notion of truthfulness that humans find easy to see and understand. The proposed research project is to identify and ultimately automate this diagrammatic reasoning on mathematical theorems. The system that we are in the process of implementing will be given a theorem and will (initially) interactively prove it by the use of geometric manipulations on the diagram that the user chooses to be the appropriate ones. These operations will be the inference steps of the proof. The constructive!-rule will be used as a tool to capture the generality of diagrammatic proofs. In this way, we hope to verify and to show that the diagra...", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Relational rippling: A general approach\n", "abstract": " We propose a new version of rippling, called relational rippling. Rippling is a heuristic for guiding proof search, especially in the step cases of inductive proofs. Relational rippling is designed for representations in which value passing is by shared existential variables, as opposed to function nesting. Thus relational rippling can be used to guide reasoning about logic programs or circuits represented as relations. Relational rippling is often part of a multiple phase process: typical proofs consist of mixed functional and relational rippling phases. The purely relational phase is called rippling-past.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The Use of Proof Plans to Sum Series\n", "abstract": " [DAI Research Paper No. 563] -------------------------------------------------------------------------------- Title: THE USE OF PROOF PLANS TO SUM SERIES Authors: BUNDY, A.; NUNES, A.; WALSH, T. Abstract: We describe a program for finding closed form solutions to finite sums. The program was built to test the applicability of the proof planning search control technique in a domain of mathematics outwith induction. This experiment was successful. The series summing program extends previous work in this area and was built in a short time just by providing new series summing methods to our existing inductive theorem proving system CLAM. One surprising discovery was the usefulness of the ripple tactic in summing series. Rippling is the key tactic for controlling inductive proofs, and was previously thought to be specialised to such proofs. However, it turns out to be the key sub-tactic used by all the main tactics for summing \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The use of proof plans in formal methods\n", "abstract": " Proof plans is a new AI technique for controlling the search that arises in automatic theorem proving. We have applied it to the satisfaction of the proof obligations arising from the use of formal methods in software engineering. We first describe the particular formal methods technique we have adopted as a vehicle and then describe how proof plans are used within this techniqtle.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Benefits and risks of knowledge-based systems\n", "abstract": " This concise yet highly informative title provides a unique treatment of the many benefits, as well as the potential dangers, of the new advice-giving computer systems used in many major institutions. The volume gives readers authoritative coverage of the applications and social implications of such knowledge-based systems, and includes a useful account that details how these powerful systems function. Recommendations for the development and marketing of advanced information technology are set forth, with complete analyses that will be of interest to everyone who works with, or is concerned about, this new technology.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Probability, truth and logic: reply to cheeseman\n", "abstract": " This short paper will appear in the journal\" Computational Intelligence\" as one of a set of peer commentaries on a paper by Peter Cheeseman entitled\" An inquiry into computer understanding\", in which he advocates the use of Bayesian Inference to represent plausible inference. Cheeseman's paper is itself a reaction to Drew Mc-Dermott's paper\" A critique of pure reason\", which also appeared in\" Computational Intelligence\" with peer commentary. McDermott was criticising the use of logic for representing plausible inference. Historical note. For the benefit of people to whom the titles of the above papers sound vaguely familiar, but they cannot quite put their finger on the references: the McDermott title was first used by Kant; the Cheeseman title is adapted from a paper by Hume; and mine is adapted from a book by Ayer. I am broadly in sympathy with Cheeseman's attempt to promote the use of Bayesian probability in artificial intelligence; it may well have a role to play in", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Computer-aided construction of ecological simulation programs\n", "abstract": " Abstract''. We describe the development of ECO, a program which enables ecologists with minimal mathematical or computing skills to build simulation models. Two important problems are found in this system: the specification formalism (System Dynamics) is not sufficiently expressive; and the system requires that users adapt to its specification language, rather than using their own terminology. We describe solutions to both these problems. The Submodels system permits users to construct models using more complex computational structures than those found in System Dynamics. Use of a\" high level\" specification language, based on a typed logic, is proposed as a means of allowing the users to express their modelling requirements using ecological terminology. This approach is compact, expressive and provides a framework for guiding the user during the process of model construction. 1", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The Eco Browser\n", "abstract": " The ECO Browser is the prototype of a Prolog browser for a large knowledge base of loosely structured observational information. Its purpose is to provide a way for users quickly to access the information they require without necessarily having predetermined ideas about the things that are of interest to them. The browsing mechanism is based on simple principles. We adopt an approach which relies on a gradual narrowing of the area of interest, until the desired information is found. Recent small scale tests suggest that this method is natural and easy to use, although a short introductory session is often required. Several extensions to the current system are suggested, including the incorporation of intelligent guidancemethods.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Correctness criteria of some algorithms for uncertain reasoning using Incidence Calculus\n", "abstract": " Correctness criteria of some algorithms for uncertain reasoning using incidence calculus - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/648574 Title : Correctness criteria of some algorithms for uncertain reasoning using incidence calculus Author : Bundy, A ; Corporate author : Edinburgh Univ. (United Kingdom). Dept. of Artificial Intelligence ; Publication year : 1985 Language : English ; Pagination/Size : 23 p. ; SIGLE classification : 12B - Statistics, operations research ; 09I - Control systems, control theory ; Document type : R - Report ; Report number : DAI-RP--259 ; Other identifier : GB ; GB_ 1986:30883 ; handle : http://hdl.handle.net/10068/648574 Provenance : SIGLE ; Get a copy : BLDSC - British Library Document Supply Centre Availability : Available from \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "How to get a PhD in AI\n", "abstract": " How to get a PhD in AI \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation How to get a PhD in AI Alan Bundy, du Boulay Ben, Howe Jim, Plotkin Gordon School of Informatics Research output: Chapter in Book/Report/Conference proceeding \u203a Chapter Overview Original language English Title of host publication Artificial Intelligence Skills Editors T. O\u2019Shea, M. Eisenstadt Publisher Harper and Row ISBN (Print) NA Publication status Published - 1984 Access to Document Full textAccepted author manuscript, 261 KB SLICESAccepted author manuscript, 77.3 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A., Ben, DB, Jim, H., & \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Hook\n", "abstract": " A hook is a piece of code or data-structure in an implementation which is made available to simplify later augmentation of the system. A good example is an evaluation algorithm that leaves sufficient information on the run-time stack for a good debugging package to be possible.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Zero-crossings\n", "abstract": " A point at which a mathematical function changes its sign, i.e. passes through zero. Technically a zero-crossing is defined as the intersection of the zero plane (z=0) with a surface (z=f(x,y)). Convolving a grey-level image <89> with a difference of gaussians operator and then finding zero crossings in the output is one way of looking for edge points. See edge detection <65>.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Minimax\n", "abstract": " This is a technique for searching game trees in order to determine the best move in a given position. The limited lookahead gametree is used and the values of the leaves are given. Minimax starts from the leaves and works up to the root of the tree. If MAX is to play at a node, the minimum possible value at that node is taken to be the the nodes value. In this way MAX maximizes his/her gain, assuming optimal play by the opponent. Working up from the leaves the values of the possible moves in the initial state are calculated.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Emycin\n", "abstract": " An expert system shell implemented in INTERLISP <103>. EMYCIN is a domainin-dependent version of MYCIN, a production rule system <192> designed for medical consultations. Problem-specific knowledge is represented as production rules where the antecedent is effectively a boolean function of predicates of attribute-object-value triples and both the condition and action have a certainty value associated with them. Uses a backward chaining control strategy. Incorporates a sophisticated front-end to handle user interactions and facilities for explaining how conclusions were reached and answering questions.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Bidirectional Search\n", "abstract": " In bidirectional search of a state space <240>, search proceeds both backwards and forwards. The program terminates when a common state is reached, since this means that a path has been found from the initial state to the goal state.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Formant Synthesis\n", "abstract": " Basic sounds of a language can be characterized by their formant <73> frequencies. The formant pattern for a synthetic utterance can be created by interpolating between the formant values of the sounds composing the desired utterance. A speech wave can be computed from such a formant pattern, to be played through a digital-to-analogue converter, or the formant pattern can be used directly to excite a set of resonators, acting as an electrical analogue of the vocal tract.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Primal sketch\n", "abstract": " A term used by Marr for a representation making explicit the properties of intensity changes in the retinal image(s). The Raw Primal Sketch makes explicit only very localised properties, such as size, position and orientation; the Full Primal Sketch, resulting from grouping elements of the Raw Primal Sketch, makes explicit more global properties such as alignment. Marr claimed that higher level processes interact only with the primal sketch and its derivatives, not with the data from which the primal sketch Is derived. The primal sketch is computed by dedicated processors which are independent of higher level processes.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Augmented transition network\n", "abstract": " Representation for grammars developed from simple finite state transition networks by allowing (a) recursion and (b) augmentation, i.e. the use of arbitrary tests and actions on arcs, giving full Turing machine power. The use of registers for storing constituents, and the use of tests and actions on register contents allow great flexibility in parsing, and in particular permit the construction of sentence representations quite distinct from the surface text e.g. deep as opposed to surface syntactic structures. The form of grammar representation is procedurally oriented, but the grammar itself is separated from the interpretive parser, which is top-down <248> and usually depth-first <55>. ATNs are a popular formalism and can be adapted e.g. to guide parsing by explicit arc ordering. Problems arise with e.g. passing information between subnets, and the treatment of conjunctions.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Convolution\n", "abstract": " The application of a mathematical operation to each neighbourhood in an image is called convolution. The operation is defined by a \u201cmask\u201d specifying for each neighbourhood, how many points it contains and how the corresponding image point affects the computations. Each location in the operator mask contains a weighting value, these are multiplied by the value of the corresponding image location and the results summed to give the convolution value for that neighbourhood. Doing this for all neighbourhoods produces a new array of values. Mathematically, the convolution integral is the integrated cross product of a weighting function with an image. See local grey-level operations <127>.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Constructive solid geometry\n", "abstract": " A method of constructing solid models in which a set of primitive solids is provided (e. g, cuboids, cylinders), and further solids are created by either moving a solid in space, or by set combinations of solids (i. e, set union, intersection, or difference). The technique has been put on a firm theoretical basis [Requicha and Voelker 80].", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Semantic grammar\n", "abstract": " Semantic grammar is contrasted with conventional grammars, by relying predominantly on semantic rather than syntactic categories, e. g.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Smalltalk\n", "abstract": " SmallTalk is a programming environment developed by members of the Learning Research Group at the Xerox Palo Alto Research Center. A number of editions of SmallTalk have emerged over the past few years, starting with SmallTalk-72. which appeared around 1976 [Goldberg and Kay 76]. and culminating with SmallTalk-80. announced in Byte magazine [Byte. 1981]. The fundamental philosophy of SmallTalk is developed from Simula [Birtwistle et al 73], and although the different editions differ in syntax, all share the view of a programming system as a collection of (active) objects, communicating by passing messages. In addition. SmallTalk adopted Simula\u2019s \u201cclass\u201d concept, and has extended and refined it considerably. The power of the SmallTalk system arises mainly from the modularity enforced by the packaging of declarative and procedural knowledge into individual objects. If the services of an\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "KL-One/KL-Two\n", "abstract": " KL-ONE and KL-TWO are programming systems for representing and manipulating knowledge. They have been in development at BBN during the late 1970s. KL-ONE is an outgrowth of the semantic net <227> school of representation, and includes some of the features of Minsky\u2019s frames <77>. Classes are represented by \u201cConcepts\u201d (similar to frames). and their properties are represented by \u201croles\u201d (similar to slots of frames). Concepts and roles are organized into separate taxonomies based on the relation of subsumption (similar to ISA) (see ISA hierarchy <108>). giving a notion of sub- and super-Concepts, as well as sub- and super- Roles. There is a system of inheritance by which Concepts (and Roles) acquire attributes of their super-Concepts (and super-Roles). and there is an algorithm, called classification, that discovers appropriate subsumption relationships that are not explicitly stated. A formal\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Procedural attachment\n", "abstract": " It is often necessary in knowledge representation for an executable procedure to be directly associated with one or more data structures in order to indicate when it should be used. Attached procedures lie dormant until certain conditions are satisfied, at which point they are executed. In KRL <115> two types of attached procedures were identified (but these hold for most systems):                                     (I)                                                                servants: these are executed when a procedure is required to apply some operation to a data object (or set of data objects). a selection mechanism may be required to select the correct procedure if more than one servant is available:                                                                        (ii)                                                                demons <51>: these are invoked when something has been done or is about to be done. (note that all demons whose conditions are met are activated\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "A* algorithm\n", "abstract": " A form of heuristic search <92> that tries to find the cheapest path from the initial state to the goal. Its characteristic feature is the evaluation function. This is the sum of two components: the estimated minimum cost of a path from the initial state to the current state, and the estimated cost from the current state to the goal. The first component can be calculated if the search space is a tree, or it can be approximated by the cheapest known path if the search space is a graph. The second component must be defined, like any evaluation function, with respect to the domain. The heuristic power of this method depends on the properties of the evaluation function.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "AI 1 problem solving notes\n", "abstract": " AI 1 problem solving notes - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/626299 Title : AI 1 problem solving notes Author : Bundy, A. ; Corporate author : Edinburgh Univ. (UK). Dept. of Artificial Intelligence ; Publication year : 1983 Language : English ; Pagination/Size : 119 p. ; SIGLE classification : 09H - Computer software, programming ; 06D - Bionics ; Document type : R - Report ; Report number : DAI-OP--30 ; Other identifier : GB ; GB_ 1983:4986 ; handle : http://hdl.handle.net/10068/626299 Provenance : SIGLE ; Get a copy : BLDSC - British Library Document Supply Centre Availability : LD:3511.636(DAI-OP--30) Country : United Kingdom ; CNRS INIST Creative Commons Help Search Paper order Contact Legals Terms of Use Credits About OpenGrey \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Using Meta-level Descriptions for Selective Applications of Multiple Rewrite Rules in Algebraic Manipulation\n", "abstract": " In this paper we describe a technique for controlling inference, called meta-level inference, and a prograrn for algebraic manipulation, PRESS, which embodies this technique. In PRESS. algebraic expressions are manipulated by a series of methods. The appropriate method is chosen by meta-level inference and itself uses meta-level reasoning to select and apply rewrite rules to the current expression. The use of meta-level inference is shown to drastically cut down on search, lead to clear and modular programs. aid the proving of properties of the prograrn and enable the automatic learning of both new algebraic facts and new control infomlation.", "num_citations": "1\n", "authors": ["1609"]}
{"title": "The role of inference in the solving of mechanics problems\n", "abstract": " The role of inference in the solving of mechanics problems \u2014 University of Edinburgh Research Explorer Skip to main navigation Skip to search Skip to main content University of Edinburgh Research Explorer Logo Help & FAQ Home Research output Profiles Research Units Projects Datasets Prizes Activities Press / Media Equipment Search by expertise, name or affiliation The role of inference in the solving of mechanics problems Alan Bundy School of Informatics Research output: Working paper \u203a Discussion paper Overview Original language English Publication status Unpublished - 1976 Publication series Name DAI Working Paper No. 16 Access to Document Full textAccepted author manuscript, 322 KB Cite this APA Author BIBTEX Harvard Standard RIS Vancouver Bundy, A. (1976). The role of inference in the solving of mechanics problems. (DAI Working Paper No. 16). Bundy, Alan. / The role of inference in \u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Symbolic Computation\n", "abstract": " Making a diagnosis when something goes wrong with a natural or manmade system can be difficult. In many fields, such as medicine or electronics, a long training period and apprenticeship are required to become a skilled diagnostician. During this time a novice diagnostician is asked to assimilate a large amount of knowledge about the class of systems to be diagnosed. In contrast, the novice is not really taught how to reason with this knowledge in arriving at a conclusion or a diagnosis, except perhaps implicitly through ease examples. This would seem to indicate that many of the essential aspects of diagnostic reasoning are a type of intuitionbased, common sense reasoning.More precisely, diagnostic reasoning can be classified as a type of inference known as abductive reasoning or abduction. Abduction is defined to be a process of generating a plausible explanation for a given set of observations or facts\u00a0\u2026", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Automatic Conjecture Making in Mathematics\n", "abstract": " The primary output of the HR program is mathematical concepts. Conjectures are of secondary interest, used purely to help assess the concepts. Elsewhere in the literature, the question of how to produce mathematical conjectures by computer has only been addressed in isolated domains and with specialised techniques. We propose here to study and implement an important aspect of automatic conjecture making in mathematics:", "num_citations": "1\n", "authors": ["1609"]}
{"title": "Analysing IBMs Common Cryptographic Architecture API with a Protocol Analysis Tool\n", "abstract": " Security APIs are Application Programming Interfaces provided by devices called hardware security modules (HSMs) which are designed to carry out a series of cryptographic operations involving sensitive data. The goal of a security API is to ensure that any sensitive data does not appear in an unencrypted form outside of the HSM itself, and that the data is manipulated in a precisely controlled manner. An attack on a security API is defined as a series of legal command calls which result in sensitive information being released. A number of attacks on IBM\u2019s CCA API were discovered by Bond, and are presented together in [Bond 2001, \u00a7 5]. TheAuthors\u2019 address: School of Informatics, The University of Edinburgh, Edinburgh, EH8 9LE. E-mail addresses: g. keighren@ sms. ed. ac. uk, graham. steel@ ed. ac. uk, a. bundy@ ed. ac. uk. 1 Supported by an EPSRC-funded Collaborative Training Account from The University of Edin-", "num_citations": "1\n", "authors": ["1609"]}