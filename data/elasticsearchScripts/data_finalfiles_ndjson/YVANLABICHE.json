{"title": "Using mutation analysis for assessing and comparing testing coverage criteria\n", "abstract": " The empirical assessment of test techniques plays an important role in software testing research. One common practice is to seed faults in subject software, either manually or by using a program that generates all possible mutants based on a set of mutation operators. The latter allows the systematic, repeatable seeding of large numbers of faults, thus facilitating the statistical analysis of fault detection effectiveness of test suites; however, we do not know whether empirical results obtained this way lead to valid, representative conclusions. Focusing on four common control and data flow criteria (block, decision, C-use, and P-use), this paper investigates this important issue based on a middle size industrial program with a comprehensive pool of test cases and known faults. Based on the data available thus far, the results are very consistent across the investigated criteria as they show that the use of mutation operators\u00a0\u2026", "num_citations": "547\n", "authors": ["318"]}
{"title": "Impact analysis and change management of UML models\n", "abstract": " The use of Unified Modeling Language (UML) analysis/design models on large projects leads to a large number of interdependent UML diagrams. As software systems evolve, those diagrams undergo changes to, for instance, correct errors or address changes in the requirements. Those changes can in turn lead to subsequent changes to other elements in the UML diagrams. Impact analysis is then defined as the process of identifying the potential consequences (side-effects) of a change, and estimating what needs to be modified to accomplish a change. In this article, we propose a UML model-based approach to impact analysis that can be applied before any implementation of the changes, thus allowing an early decision-making and change planning process. We first verify that the UML diagrams are consistent (consistency check). Then changes between two different versions of a UML model are identified\u00a0\u2026", "num_citations": "281\n", "authors": ["318"]}
{"title": "A UML-based approach to system testing\n", "abstract": " System testing is concerned with testing an entire system based on its specifications. In the context of object-oriented, UML development, this means that system test requirements are derived from UML analysis artifacts such as use cases, their corresponding sequence and collaboration diagrams, class diagrams, and possibly the use of the Object Constraint Language across all these artifacts. Our goal is to support the derivation of test requirements, which will be transformed into test cases, test oracles, and test drivers once we have detailed design information.               Another important issue we address is the one of testability. Testability requirements (or rules) need to be imposed on UML artifacts so as to be able to support system testing efficiently. Those testability requirements result from a trade-off between analysis and design overhead and improved testability. The potential for automation is also an\u00a0\u2026", "num_citations": "281\n", "authors": ["318"]}
{"title": "The impact of UML documentation on software maintenance: An experimental evaluation\n", "abstract": " The Unified Modeling Language (UML) is becoming the de facto standard for software analysis and design modeling. However, there is still significant resistance to model-driven development in many software organizations because it is perceived to be expensive and not necessarily cost-effective. Hence, it is important to investigate the benefits obtained from modeling. As a first step in this direction, this paper reports on controlled experiments, spanning two locations, that investigate the impact of UML documentation on software maintenance. Results show that, for complex tasks and past a certain learning curve, the availability of UML documentation may result in significant improvements in the functional correctness of changes as well as the quality of their design. However, there does not seem to be any saving of time. For simpler tasks, the time needed to update the UML documentation may be substantial\u00a0\u2026", "num_citations": "231\n", "authors": ["318"]}
{"title": "A systematic review of transformation approaches between user requirements and analysis models\n", "abstract": " Model transformation is one of the basic principles of Model Driven Architecture. To build a software system, a sequence of transformations is performed, starting from requirements and ending with implementation. However, requirements are mostly in the form of text, but not a model that can be easily understood by computers; therefore, automated transformations from requirements to analysis models are not easy to achieve. The overall objective of this systematic review is to examine existing literature works that transform textual requirements into analysis models, highlight open issues, and provide suggestions on potential directions of future research. The systematic review led to the analysis of 20 primary studies (16 approaches) obtained after a carefully designed procedure for selecting papers published in journals and conferences from 1996 to 2008 and Software Engineering textbooks. A conceptual\u00a0\u2026", "num_citations": "191\n", "authors": ["318"]}
{"title": "An investigation of graph-based class integration test order strategies\n", "abstract": " The issue of ordering class integration in the context of integration testing has been discussed by a number of researchers. More specifically, strategies have been proposed to generate a test order while minimizing stubbing. Recent papers have addressed the problem of deriving an integration order in the presence of dependency cycles in the class diagram. Such dependencies represent a practical problem as they make any topological ordering of classes impossible. Three main approaches, aimed at \"breaking\" cycles, have been proposed. The first one was proposed by Tai and Daniels (1999) and is based on assigning a higher-level order according to aggregation and inheritance relationships and a lower-level order according to associations. The second one was proposed by Le Traon et al. (2000) and is based on identifying strongly connected components in the dependency graph. The third one was\u00a0\u2026", "num_citations": "168\n", "authors": ["318"]}
{"title": "Automated impact analysis of UML models\n", "abstract": " The use of Unified Modeling Language (UML) analysis/design models on large projects leads to a large number of interdependent UML diagrams. As software systems evolve, UML diagrams undergo changes that address error corrections and changed requirements. Those changes can in turn lead to subsequent changes to other elements in the UML diagrams. Impact analysis is defined as the process of identifying the potential consequences (side-effects) of a change, and estimating what needs to be modified to accomplish that change. In this article, we propose a UML model-based approach to impact analysis that can be applied before implementation of changes, thus allowing early decision-making and change planning. We first verify that the UML diagrams in a design model are consistent. Then the changes between two different versions of UML models are automatically identified according to a change\u00a0\u2026", "num_citations": "160\n", "authors": ["318"]}
{"title": "A systematic review of state-based test tools\n", "abstract": " Model-based testing (MBT) is about testing a software system using a model of its behaviour. To benefit fully from MBT, automation support is required. The goal of this systematic review is determining the current state of the art of prominent MBT tool support where we focus on tools that rely on state-based models. We automatically searched different source of information including digital libraries and mailing lists dedicated to the topic. Precisely defined criteria are used to compare selected tools and comprise support for test adequacy and coverage criteria, level of automation for various testing activities and support for the construction of test scaffolding. Simple adequacy criteria are supported but not advanced ones; data(-flow) criteria are seldom supported; support for creating test scaffolding varies a great deal. The results of this review should be of interest to a wide range of stakeholders: software\u00a0\u2026", "num_citations": "159\n", "authors": ["318"]}
{"title": "Facilitating the transition from use case models to analysis models: Approach and experiments\n", "abstract": " Use case modeling, including use case diagrams and use case specifications (UCSs), is commonly applied to structure and document requirements. UCSs are usually structured but unrestricted textual documents complying with a certain use case template. However, because Use Case Models (UCMods) remain essentially textual, ambiguity is inevitably introduced. In this article, we propose a use case modeling approach, called Restricted Use Case Modeling (RUCM), which is composed of a set of well-defined restriction rules and a modified use case template. The goal is two-fold: (1) restrict the way users can document UCSs in order to reduce ambiguity and (2) facilitate the manual derivation of initial analysis models which, when using the Unified Modeling Language (UML), are typically composed of class diagrams, sequence diagrams, and possibly other types of diagrams. Though the proposed restriction\u00a0\u2026", "num_citations": "135\n", "authors": ["318"]}
{"title": "Solving the class responsibility assignment problem in object-oriented analysis with multi-objective genetic algorithms\n", "abstract": " In the context of object-oriented analysis and design (OOAD), class responsibility assignment is not an easy skill to acquire. Though there are many methodologies for assigning responsibilities to classes, they all rely on human judgment and decision making. Our objective is to provide decision-making support to reassign methods and attributes to classes in a class diagram. Our solution is based on a multi-objective genetic algorithm (MOGA) and uses class coupling and cohesion measurement for defining fitness functions. Our MOGA takes as input a class diagram to be optimized and suggests possible improvements to it. The choice of a MOGA stems from the fact that there are typically many evaluation criteria that cannot be easily combined into one objective, and several alternative solutions are acceptable for a given OO domain model. Using a carefully selected case study, this paper investigates the\u00a0\u2026", "num_citations": "135\n", "authors": ["318"]}
{"title": "Using machine learning to support debugging with tarantula\n", "abstract": " Using a specific machine learning technique, this paper proposes a way to identify suspicious statements during debugging. The technique is based on principles similar to Tarantula but addresses its main flaw: its difficulty to deal with the presence of multiple faults as it assumes that failing test cases execute the same fault(s). The improvement we present in this paper results from the use of C4.5 decision trees to identify various failure conditions based on information regarding the test cases' inputs and outputs. Failing test cases executing under similar conditions are then assumed to fail due to the same fault(s). Statements are then considered suspicious if they are covered by a large proportion of failing test cases that execute under similar conditions. We report on a case study that demonstrates improvement over the original Tarantula technique in terms of statement ranking. Another contribution of this paper is to\u00a0\u2026", "num_citations": "128\n", "authors": ["318"]}
{"title": "A measurement framework for object-oriented software testability\n", "abstract": " Testing is an expensive activity in the development process of any software system. Measuring and assessing the testability of software would help in planning testing activities and allocating required resources. More importantly, measuring software testability early in the development process, during analysis or design stages, can yield the highest payoff as design refactoring can be used to improve testability before the implementation starts.This paper presents a generic and extensible measurement framework for object-oriented software testability, which is based on a theory expressed as a set of operational hypotheses. We identify design attributes that have an impact on testability directly or indirectly, by having an impact on testing activities and sub-activities. We also describe the cause-effect relationships between these attributes and software testability based on thorough review of the literature and our own\u00a0\u2026", "num_citations": "102\n", "authors": ["318"]}
{"title": "Investigating the use of analysis contracts to improve the testability of object\u2010oriented code\n", "abstract": " A number of activities involved in testing software are known to be difficult and time consuming. Among them is the definition and coding of test oracles and the isolation of faults once failures have been detected. Through a thorough and rigorous empirical study, we investigate how the instrumentation of contracts could address both issues. Contracts are known to be a useful technique in specifying the precondition and postcondition of operations and class invariants, thus making the definition of object\u2010oriented analysis or design elements more precise. It is one of the reasons the Object Constraint Language (OCL) was made part of the Unified Modeling Language. Our aim in this paper is to reuse and instrument contracts to ease testing. A thorough case study is run where we define OCL contracts, instrument them using a commercial tool and assess the benefits and limitations of doing so to support the\u00a0\u2026", "num_citations": "95\n", "authors": ["318"]}
{"title": "Testing levels for object-oriented software\n", "abstract": " One of the characteristics of object-oriented software is the complex dependency that may exist between classes due to inheritance, association and aggregation relationships. Hence, where to start testing and how to define an integration strategy are issues that require further investigation. This paper presents an approach to define a test order by exploiting a model produced during design stages (eg, using OMT, UML), namely the class diagram. Our goal is to minimize the number of stubs to be constructed in order to decrease the cost of testing. This is done by testing a class after the classes it depends on. The novelty of the test order lies in the fact that it takes account of:(i) dynamic (polymorphism) dependencies;(ii) abstract classes that cannot be instantiated, making some testing levels infeasible. The test order is represented by a graph showing which testing levels must be done in sequence and which ones\u00a0\u2026", "num_citations": "93\n", "authors": ["318"]}
{"title": "A use case modeling approach to facilitate the transition towards analysis models: Concepts and empirical evaluation\n", "abstract": " Use case modeling (UCM) is commonly applied to document requirements. Use case specifications (UCSs) are usually structured, unrestricted textual documents complying with a certain template. However, because they remain essentially textual, ambiguities are inevitable. In this paper, we propose a new UCM approach, which is composed of a set of well-defined restriction rules and a new template. The goal is to reduce ambiguity and facilitate automated analysis, though the later point is not addressed in this paper. We also report on a controlled experiment which evaluates our approach in terms of its ease of application and the quality of the analysis models derived by trained individuals. Results show that the restriction rules are overall easy to apply and that our approach results in significant improvements over UCM using a standard template and no restrictions in UCSs, in terms of the correctness of\u00a0\u2026", "num_citations": "91\n", "authors": ["318"]}
{"title": "Modeling safety and airworthiness (RTCA DO-178B) information: conceptual model and UML profile\n", "abstract": " Several safety-related standards exist for developing and certifying safety-critical systems. System safety assessments are common practice and system certification according to a standard requires submitting relevant system safety information to appropriate authorities. The RTCA DO-178B standard is a software quality assurance, safety-related standard for the development of software aspects of aerospace systems. This research introduces an approach to improve communication and collaboration among safety engineers, software engineers, and certification authorities in the context of RTCA DO-178B. This is achieved by utilizing a Unified Modeling Language (UML) profile that allows software engineers to model safety-related concepts and properties in UML, the de facto software modeling standard. A conceptual meta-model is defined based on RTCA DO-178B, and then a corresponding UML profile\u00a0\u2026", "num_citations": "89\n", "authors": ["318"]}
{"title": "Using genetic algorithms for early schedulability analysis and stress testing in real-time systems\n", "abstract": " Reactive real-time systems have to react to external events within time constraints: Triggered tasks must execute within deadlines. It is therefore important for the designers of such systems to analyze the schedulability of tasks during the design process, as well as to test the system's response time to events in an effective manner once it is implemented. This article explores the use of genetic algorithms to provide automated support for both tasks. Our main objective is then to automate, based on the system task architecture, the derivation of test cases that maximize the chances of critical deadline misses within the system; we refer to this testing activity as stress testing. A second objective is to enable an early but realistic analysis of tasks' schedulability at design time. We have developed a specific solution based on genetic algorithms and implemented it in a tool. Case studies were run and results show that\u00a0\u2026", "num_citations": "86\n", "authors": ["318"]}
{"title": "Coverage\u2010based regression test case selection, minimization and prioritization: A case study on an industrial system\n", "abstract": " This paper presents a case study of coverage\u2010based regression testing techniques on a real world industrial system with real regression faults. The study evaluates four common prioritization techniques, a test selection technique, a test suite minimization technique and a hybrid approach that combines selection and minimization. The study also examines the effects of using various coverage criteria on the effectiveness of the studied approaches. The results show that prioritization techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information in prioritization techniques does not significantly enhance fault detection rates. The results show that test selection does not provide significant savings in execution cost (<2%), which might be attributed to the nature of the changes made to the\u00a0\u2026", "num_citations": "85\n", "authors": ["318"]}
{"title": "An automated approach to transform use cases into activity diagrams\n", "abstract": " Use cases are commonly used to structure and document requirements while UML activity diagrams are often used to visualize and formalize use cases, for example to support automated test case generation. Therefore the automated support for the transition from use cases to activity diagrams would provide significant, practical help. Additionally, traceability could be established through automated transformation, which could then be used for instance to relate requirements to design decisions and test cases. In this paper, we propose an approach to automatically generate activity diagrams from use cases while establishing traceability links. Data flow information can also be generated and added to these activity diagrams. Our approach is implemented in a tool, which we used to perform five case studies. The results show that high quality activity diagrams can be generated. Our analysis also shows that\u00a0\u2026", "num_citations": "81\n", "authors": ["318"]}
{"title": "aToucan: an automated framework to derive UML analysis models from use case models\n", "abstract": " The transition from an informal requirements specification in natural language to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA), where a key step is to transition from a use case model to an analysis model. However, providing automated support for this transition is challenging, mostly because, in practice, requirements are expressed in natural language and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this article, we propose a method and a tool called aToucan, building on existing work, to automatically generate a UML analysis model\u00a0\u2026", "num_citations": "79\n", "authors": ["318"]}
{"title": "Revisiting strategies for ordering class integration testing in the presence of dependency cycles\n", "abstract": " The issue of ordering class integration in the context of integration testing of object-oriented software has been discussed by a number of researchers. More specifically, strategies have been proposed to generate a test order while minimizing stubbing. Recent papers have addressed the problem of deriving an integration order in the presence of dependency cycles in the class diagram. Such dependencies represent a practical problem as they make any topological ordering of classes impossible. The paper proposes a strategy that integrates two existing methods aimed at \"breaking\" cycles so as to allow a topological order of classes. The first one was proposed by K.-C. Tai and F.J. Daniels (1999) and is based on assigning a higher-level order according to aggregation and inheritance relationships and a lower-level order according to associations. The second one was proposed by Y. Le Traon et al. (2000) and is\u00a0\u2026", "num_citations": "73\n", "authors": ["318"]}
{"title": "A UML profile for developing airworthiness-compliant (RTCA DO-178B), safety-critical software\n", "abstract": " Many safety-related, certification standards exist for developing safety-critical systems. System safety assessments are common practice and system certification according to a standard requires submitting relevant software safety information to appropriate authorities. The airworthiness standard, RTCA DO-178B, is the de-facto standard for certifying aerospace systems containing software. This research introduces an approach to improve communication and collaboration among safety engineers and software engineers by proposing a Unified Modeling Language (UML) profile that allows software engineers to model safety-related concepts and properties in UML, the de-facto software modeling language. Key safety-related concepts are extracted from RTCA DO-178B, and then a UML profile is defined to enable their precise modeling. We show that the profile improves the line of communication between\u00a0\u2026", "num_citations": "69\n", "authors": ["318"]}
{"title": "Automated traceability analysis for UML model refinements\n", "abstract": " During iterative, UML-based software development, various UML diagrams, modeling the same system at different levels of abstraction are developed. These models must remain consistent when changes are performed. In this context, we refine the notion of impact analysis and distinguish horizontal impact analysis\u2013that focuses on changes and impacts at one level of abstraction\u2013from vertical impact analysis\u2013that focuses on changes at one level of abstraction and their impacts on another level. Vertical impact analysis requires that some traceability links be established between model elements at the two levels of abstraction. We propose a traceability analysis approach for UML 2.0 class diagrams which is based on a careful formalization of changes to those models, refinements which are composed of those changes, and traceability links corresponding to refinements. We show how actual refinements and\u00a0\u2026", "num_citations": "63\n", "authors": ["318"]}
{"title": "An orchestrated survey of available algorithms and tools for combinatorial testing\n", "abstract": " For functional testing based on the input domain of a functionality, parameters and their values are identified and a test suite is generated using a criterion exercising combinations of those parameters and values. Since software systems are large, resulting in large numbers of parameters and values, a technique based on combinatorics called Combinatorial Testing (CT) is used to automate the process of creating those combinations. CT is typically performed with the help of combinatorial objects called Covering Arrays. The goal of the present work is to determine available algorithms/tools for generating a combinatorial test suite. We tried to be as complete as possible by using a precise protocol for selecting papers describing those algorithms/tools. The 75 algorithms/tools we identified are then categorized on the basis of different comparison criteria, including: the test suite generation technique, the support for\u00a0\u2026", "num_citations": "60\n", "authors": ["318"]}
{"title": "UML consistency rules: a systematic mapping study\n", "abstract": " Context: The Unified Modeling Language (UML), with its 14 different diagram types, is the de-facto standard modeling language for object-oriented modeling and documentation. Since the various UML diagrams describe different aspects of one, and only one, software under development, they are not independent but strongly depend on each other in many ways. In other words, the UML diagrams describing a software product must be consistent. Inconsistencies between these diagrams may be a source of faults in software systems. It is therefore paramount that these inconsistencies be detected, analyzed and hopefully fixed.Objective: The aim of this article is to deliver a comprehensive summary of UML consistency rules as they are described in the literature to date to obtain an extensive and detailed overview of the current research in this area.Method: We performed a Systematic Mapping Study by following well\u00a0\u2026", "num_citations": "58\n", "authors": ["318"]}
{"title": "Automated support for deriving test requirements from UML statecharts\n", "abstract": " Many statechart-based testing strategies result in specifying a set of paths to be executed through a (flattened) statechart. These techniques can usually be easily automated so that the tester does not have to go through the tedious procedure of deriving paths manually to comply with a coverage criterion. The next step is then to take each test path individually and derive test requirements leading to fully specified test cases. This requires that we determine the system state required for each event/transition that is part of the path to be tested and the input parameter values for all events and actions associated with the transitions. We propose here a methodology towards the automation of this procedure, which is based on a careful normalization and analysis of operation contracts and transition guards written with the Object Constraint Language (OCL). It is illustrated by one case study that exemplifies the\u00a0\u2026", "num_citations": "58\n", "authors": ["318"]}
{"title": "Towards automated support for deriving test data from UML statecharts\n", "abstract": " Many statechart-based testing strategies result in specifying a set of paths to be executed through a (flattened) statechart. These techniques can usually be easily automated so that the tester does not have to go through the tedious procedure of deriving paths manually to comply with a coverage criterion. The next step is then to take each test path individually and derive test data, i.e., fully specified test cases. This requires that we determine the system state required for each event/transition that is part of the path to be tested and the input parameter values for all events and actions on the transitions. We propose here a methodology to automate this procedure, which is based on a careful normalization and analysis of event/action contracts and transition guards written with the Object Constraint Language (OCL). It is illustrated by a case study that exemplifies the steps and provides an initial validation\u00a0\u2026", "num_citations": "58\n", "authors": ["318"]}
{"title": "A metamodeling approach to pattern specification\n", "abstract": " This paper presents the Pattern Modeling Framework (PMF), a new metamodeling approach to pattern specification for MOF-compliant modeling frameworks and languages. Patterns need to be precisely specified before a tool can manipulate them, and though several approaches to pattern specification have been proposed, they do not provide the scalability and flexibility required in practice. PMF provides a pattern specification language called Epattern, which is capable of precisely specifying patterns in MOF-compliant metamodels. The language is defined as an extension to MOF by adding semantics inspired from the UML composite structure diagram. The language also comes with a graphical notation and a recommended iterative specification process. It also contains features to manage the complexity of specifying patterns and simplify their application and detection in user models. Most importantly\u00a0\u2026", "num_citations": "57\n", "authors": ["318"]}
{"title": "Design Synthesis from Interaction and State Based Specifications\n", "abstract": " Interaction-based and state-based modeling are two complementary approaches of behavior modeling. The former focuses on global interactions between system components. The latter concentrates on the internal states of individual components. Both approaches have been proven useful in practice. One challenging and important research objective is to combine the modeling power of both effectively and then use the combination as the basis for automatic design synthesis. We present a combination of interaction-based and state-based modeling, namely, live sequence charts and Z, for system specification. We then propose a way of generating distributed design from the combinations. Our approach handles systems with intensive interactive behaviors as well as complex state structures", "num_citations": "57\n", "authors": ["318"]}
{"title": "Automated, contract-based user testing of commercial-off-the-shelf components\n", "abstract": " Commercial-off-the-Shelf (COTS) components provide a means to construct software (component-based) systems in reduced time and cost. In a COTS component software market there exist component vendors (original developers of the component) and component users (developers of the component-based systems). The former provide the component to the user without source code or design documentation, and as a result it is difficult for the latter to adequately test the component when deployed in their system. In this article we propose a framework that clarifies the roles and responsibilities of both parties so that the user can adequately test the component in a deployment environment and the vendor does not need to release proprietary details. Then, based on this framework we combine and adapt two specification-based testing techniques and describe (and implement) a method for the automated\u00a0\u2026", "num_citations": "56\n", "authors": ["318"]}
{"title": "Lessons learned from developing a dynamic OCL constraint enforcement tool for Java\n", "abstract": " Analysis and design by contract allows the definition of a formal agreement between a class and its clients, expressing each party\u2019s rights and obligations. Contracts written in the Object Constraint Language (OCL) are known to be a useful technique to specify the precondition and postcondition of operations and class invariants in a UML context, making the definition of object-oriented analysis or design elements more precise while also helping in testing and debugging. In this article, we report on the experiences with the development of ocl2j, a tool that automatically instruments OCL constraints in Java programs using aspect-oriented programming (AOP). The approach strives for automatic and efficient generation of contract code, and a non-intrusive instrumentation technique. A summary of our approach is given along with the results of an initial case study, the discussion of encountered problems, and\u00a0\u2026", "num_citations": "43\n", "authors": ["318"]}
{"title": "A uml/marte model analysis method for uncovering scenarios leading to starvation and deadlocks in concurrent systems\n", "abstract": " Concurrency problems such as starvation and deadlocks should be identified early in the design process. As larger, more complex concurrent systems are being developed, this is made increasingly difficult. We propose here a general approach based on the analysis of specialized design models expressed in the Unified Modeling Language (UML) that uses a specifically designed genetic algorithm to detect concurrency problems. Though the current paper addresses deadlocks and starvation, we will show how the approach can be easily tailored to other concurrency issues. Our main motivations are 1) to devise solutions that are applicable in the context of the UML design of concurrent systems without requiring additional modeling and 2) to use a search technique to achieve scalable automation in terms of concurrency problem detection. To achieve the first objective, we show how all relevant concurrency\u00a0\u2026", "num_citations": "40\n", "authors": ["318"]}
{"title": "Instrumenting Contracts with Aspect-Oriented Programming to Increase Observability and Support Debugging.\n", "abstract": " In this paper we report on how aspect-oriented programming (AOP), using AspectJ, can be employed to automatically and efficiently instrument contracts and invariants in Java. The paper focuses on the templates to instrument preconditions, postconditions, and class invariants, and the necessary instrumentation for compliance-checking to the Liskov substitution principle.", "num_citations": "40\n", "authors": ["318"]}
{"title": "Multi-objective genetic algorithm to support class responsibility assignment\n", "abstract": " Class responsibility assignment is not an easy skill to acquire. Though there are many methodologies for assigning responsibilities to classes, they all rely on human judgment and decision making. Our objective is to provide decision-making help to re-assign methods and attributes to classes in a class diagram. Our solution is based on a multi-objective genetic algorithm (MOGA) and uses class coupling and cohesion measurement. Our MOGA takes as input a class diagram to be optimized and suggests possible improvements to it. The choice of a MOGA stems from the fact that there are typically many evaluation criteria that cannot be easily combined into one objective, and several alternative solutions are acceptable for a given OO domain model. This article presents our approach in detail, our decisions regarding the multi-objective genetic algorithm, and reports on a case study. Our results suggest that the\u00a0\u2026", "num_citations": "39\n", "authors": ["318"]}
{"title": "Automatically deriving a UML analysis model from a use case model\n", "abstract": " The transition from requirements expressed in Natural Language (NL) to a structured, precise specification is an important challenge in practice. It is particularly so for object-oriented methods, defined in the context of the OMG's Model Driven Architecture (MDA). However, its automation has received little attention, mostly because requirements are in practice expressed in NL and are much less structured than other kinds of development artifacts. Such an automated transformation would enable at least the generation of an initial, likely incomplete, analysis model and enable automated traceability from requirements to code, through various intermediate models. In this thesis, we propose a method and a tool, building on existing work, to automatically generate analysis models from requirements. Such models include class, sequence and activity diagrams to describe the structure and behavior of a system. It also includes automatically establishing traceability links between the requirements and the automatically generated analysis model. Requirements are assumed to be modeled with our use case modeling approach (RUCM), which we showed to have enough expressive power, to be easy to apply, to help improve the quality of manually derived analysis models.", "num_citations": "38\n", "authors": ["318"]}
{"title": "Automating image segmentation verification and validation by learning test oracles\n", "abstract": " An image segmentation algorithm delineates (an) object(s) of interest in an image. Its output is referred to as a segmentation. Developing these algorithms is a manual, iterative process involving repetitive verification and validation tasks. This process is time-consuming and depends on the availability of experts, who may be a scarce resource (e.g., medical experts). We propose a framework referred to as Image Segmentation Automated Oracle (ISAO) that uses machine learning to construct an oracle, which can then be used to automatically verify the correctness of image segmentations, thus saving substantial resources and making the image segmentation verification and validation task significantly more efficient. The framework also gives informative feedback to the developer as the segmentation algorithm evolves and provides a systematic means of testing different parametric configurations of the algorithm\u00a0\u2026", "num_citations": "37\n", "authors": ["318"]}
{"title": "A systematic identification of consistency rules for UML diagrams\n", "abstract": " UML diagrams describe different views of one software. These diagrams strongly depend on each other and must therefore be consistent with one another, since inconsistencies between diagrams may be a source of faults during software development activities that rely on these diagrams. It is therefore paramount that consistency rules be defined and that inconsistencies be detected, analyzed and fixed. The relevant literature shows that authors typically define their own consistency rules, sometimes defining the same rules and sometimes defining rules that are already in the UML standard. The reason might be that no consolidated set of rules that are relevant by authors can be found to date. The aim of our research is to provide an up to date, consolidated set of UML consistency rules and obtain a detailed overview of the current research in this area. We therefore followed a systematic procedure in order to\u00a0\u2026", "num_citations": "34\n", "authors": ["318"]}
{"title": "Improving statechart testing criteria using data flow information\n", "abstract": " Empirical studies have shown there is wide variation in cost (e.g., of devising and executing test cases) and effectiveness (at finding faults) across existing state-based coverage criteria. As these criteria can be considered as executing the control flow structure of the statechart, we are attempting to investigate how data flow information can be used to improve their cost-effectiveness. This article presents a comprehensive methodology to perform data flow analysis of UML statecharts, applies it to the round-trip path (transition tree) coverage criterion and reports on two case studies. The results of the case studies show that dataflow information can be used to select the best cost-effective transition tree when more than one satisfies the transition tree criterion. We further propose a more optimal strategy for the transition tree criterion, in terms of cost and effectiveness. The improved tree strategy is evaluated through the\u00a0\u2026", "num_citations": "34\n", "authors": ["318"]}
{"title": "Improving the coverage criteria of UML state machines using data flow analysis\n", "abstract": " A number of coverage criteria have been proposed for testing classes and class clusters modeled with state machines. Previous research has revealed their limitations in terms of their capability to detect faults. As these criteria can be considered to execute the control flow structure of the state machine, we are investigating how data flow information can be used to improve them in the context of UML state machines. More specifically, we investigate how such data flow analysis can be used to further refine the selection of a cost\u2010effective test suite among alternative, adequate test suites for a given state machine criterion. This paper presents a comprehensive methodology to perform data flow analysis of UML state machines\u2014with a specific focus on identifying the data flow from OCL guard conditions and operation contracts\u2014and applies it to a widely referenced coverage criterion, the round\u2010trip path (transition tree\u00a0\u2026", "num_citations": "31\n", "authors": ["318"]}
{"title": "Towards the verification and validation of DEVS models\n", "abstract": " The creation of a simulation model, like the creation of any software product, is guided by principles and procedures that have been reasonably well established within the software engineering community. In the context of a simulation, we need to be able to characterize the dynamic behavior of the system, and such characterization should be expressed in a format that is as clear and unambiguous as possible. Wherever feasible, formal approaches should be used. One of these formal techniques, the DEVS formalism, has gained popularity in recent years. Although some efforts have been dedicated to the Validation and Verification (V&V) of DEVS models, this is an open research area with interesting opportunities for application of advanced software engineering techniques. Indeed, it appears that thanks to the characteristics of DEVS models and the fact that DEVS models can be executed (eg, the CD++ toolkit allows the use of DEVS and Cell-DEVS formalisms) well-known software testing techniques are worth investigating for the V&V of DEVS models. In this article, we show these similarities and discuss open research paths in the field of DEVS modeling Verification and Validation by means of testing.", "num_citations": "31\n", "authors": ["318"]}
{"title": "Combining static and dynamic analyses to reverse-engineer scenario diagrams\n", "abstract": " This paper discusses a step towards reverse engineering source code to produce UML sequence diagrams, with the aim to aid program comprehension and other activities (e.g., verification). Specifically, our objective being to obtain a lightweight instrumentation and therefore disturb the software behaviour as little as possible in order to eventually produce accurate sequence diagrams. To achieve this, we combine static and dynamic analyses of a Java software, reducing information we collect at runtime (lightweight instrumentation) and compensating for the reduced runtime information with information obtained statically from source code. Static and dynamic information are represented as models and UML diagram generation becomes a model transformation problem. Our validation against a previous, correct approach shows that we indeed reduce the execution overhead inherent to dynamic analysis, while still\u00a0\u2026", "num_citations": "29\n", "authors": ["318"]}
{"title": "Domain-specific model verification with QVT\n", "abstract": " Model verification is the process of checking models for known problems (or anti-patterns). We propose a new approach to declaratively specify and automatically detect problems in domain-specific models using QVT (Query/View/Transformation). Problems are specified with QVT-Relations transformations from models where elements involved in problems are identified, to result models where problem occurrences are reported in a structured and concise manner. The approach uses a standard formalism, applies generically to any MOF-based modeling language and has well-defined detection semantics. We apply the approach by defining a catalog of problems for a particular but important kind of models, namely metamodels. We report on a case study where we used the catalog to verify recent revisions of the UML metamodel. We detected many problem occurrences that we analyzed and helped\u00a0\u2026", "num_citations": "29\n", "authors": ["318"]}
{"title": "A UML/SPT model analysis methodology for concurrent systems based on genetic algorithms\n", "abstract": " Concurrency problems, such as deadlocks, should be identified early in the design process. This is made increasingly difficult as larger and more complex concurrent systems are being developed. We propose here an approach, based on the analysis of specific models expressed in the Unified Modeling Language (UML) that uses a specifically designed genetic algorithm to detect deadlocks. Our main motivations are (1) to devise practical solutions that are applicable in the context of UML design without requiring additional modeling and (2) to achieve scalable automation. All relevant concurrency information is extracted from systems\u2019 UML models that comply with the UML Schedulability, Performance and Time profile, a standardized specialization of UML for real-time, concurrent systems. Our genetic algorithm is then used to search for execution sequences exhibiting deadlocks. Results on three case\u00a0\u2026", "num_citations": "22\n", "authors": ["318"]}
{"title": "On the round trip path testing strategy\n", "abstract": " A number of techniques have been proposed for state-based testing. One well-known technique (criterion) is to traverse the graph representing the state machine and generate a so-called transition tree, in an attempt to exercise round trip paths, i.e., paths that start and end in the same state without any other repeating state. Several hypotheses are made when one uses this criterion: exercising paths in the tree, which do not always trigger complete round trip paths, is equivalent to covering round-trip paths; different traversal algorithms are equivalent. In this paper we investigate whether these assumptions hold in practice, and if they do not, we investigate their consequences. Results also lead us to propose a new transition tree construction algorithm that results in higher efficiency and lower cost.", "num_citations": "21\n", "authors": ["318"]}
{"title": "Assessing, comparing, and combining statechart-based testing and structural testing: An experiment\n", "abstract": " Although models have been proven to be helpful in a number of software engineering activities there is still significant resistance to model-driven development. This paper investigates one specific aspect of this larger problem. It addresses the impact of using statecharts for testing class clusters that exhibit a state-dependent behavior. More precisely, it reports on a controlled experiment that investigates their impact on testing fault-detection effectiveness. Code-based, structural testing is compared to statechart-based testing and their combination is investigated to determine whether they are complementary. Results show that there is no significant difference between the fault detection effectiveness of the two test strategies but that they are significantly more effective when combined. This implies that a cost-effective strategy would specify statechart-based test cases early on, execute them once the source code is\u00a0\u2026", "num_citations": "21\n", "authors": ["318"]}
{"title": "The Need for Traceability in Heterogeneous Systems: A systematic literature review\n", "abstract": " Traceability provides a mean for Software Engineers to track system artifacts at different levels of abstraction to verify and validate system requirements. This paper provides a systematic literature review about modeling traceability in computer systems, particularly, systems that involve artifacts that come from different domains of expertise (i.e., heterogeneous artifacts). Our findings show that there is a lack of research that focus on modeling traceability among heterogeneous artifacts, which reflects in inadequate traceability tools, and that precise semantics for trace links among artifacts is needed. Our findings lead us to highlight the key areas that can enhance research on those directions.", "num_citations": "20\n", "authors": ["318"]}
{"title": "Multi-objective construction of an entire adequate test suite for an EFSM\n", "abstract": " In this paper we propose a method and a tool to generate test suites from extended finite state machines, accounting for multiple (potentially conflicting) objectives. We aim at maximizing coverage and feasibility of a test suite while minimizing similarity between its test cases and minimizing overall cost. Therefore, we define a multi-objective genetic algorithm that searches for optimal test suites based on four objective functions. In doing so, we create an entire test suite at once as opposed to test cases one at a time. Our approach is evaluated on two different case studies, showing interesting initial results.", "num_citations": "18\n", "authors": ["318"]}
{"title": "Automatically deriving UML sequence diagrams from use cases\n", "abstract": " Use cases are commonly used to structure and document requirements during requirement elicitation while sequence diagrams are often used during the analysis phase to document use cases as objects\u2019 interactions. Since creating such sequence diagrams is mostly manual, automated support would provide significant, practical help. Additionally, traceability could be easily established through automated transformation, which could then be used for instance to relate requirements to design. In this paper, we propose an approach and a tool to automatically generate sequence diagrams from use cases while establishing traceability links. We validate our approach with six case studies, where we compare sequence diagrams generated by our tool to the ones devised by experts and trained 4th year undergraduate students. Results show that sequence diagrams automatically generated by our tool are highly consistent with the ones devised by experts and are also very complete. Results also show that the automatically generated diagrams are far more complete than the ones manually created by students. These encouraging results suggest that our approach and tool would indeed provide significant, practical help to engineers creating (initial) sequence diagrams from use case descriptions.", "num_citations": "16\n", "authors": ["318"]}
{"title": "Toward automatic generation of intrusion detection verification rules\n", "abstract": " An Intrusion Detection System (IDS) is a crucial element of a network security posture. One class of IDS, called signature-based network IDSs, monitors network traffic, looking for evidence of malicious behavior as specified in attack descriptions (referred to as signatures). Many studies have reported that IDSs can generate thousands of alarms a day, many of which are false alarms. The problem often lies in the low accuracy of IDS signatures. It is therefore important to have more accurate signatures in order to reduce the number of false alarms. One part of the false alarm problem is the inability of IDSs to verify attacks (i.e. distinguish between successful and failed attacks). If IDSs were able to accurately verify attacks, this would reduce the number of false alarms a network administrator has to investigate. In this paper, we demonstrate the feasibility of using a data mining algorithm to automatically generate IDS\u00a0\u2026", "num_citations": "16\n", "authors": ["318"]}
{"title": "UML diagram synthesis techniques: a systematic mapping study\n", "abstract": " Context: UML software development relies on different types of UML diagrams, which must be consistent with one another. UML Synthesis techniques suggest to generate diagram (s) from other diagram (s), thereby implicitly suggesting that input and output diagrams of the synthesis process be consistent with one another.Objective: Our aim is to provide a comprehensive summary of UML synthesis techniques as they have been described in the literature to date to then collect UML consistency rules, which can then be used to verify UML models. Method: We performed a Systematic Mapping Study by following well-known guidelines. We selected 14 studies by means of a search with seven search engines executed until January, 2018. Results: Researchers have not frequently published papers concerning UML synthesis techniques since 2004. We present a set of 47 UML consistency rules collected from the\u00a0\u2026", "num_citations": "15\n", "authors": ["318"]}
{"title": "On the effectiveness of contracts as test oracles in the detection and diagnosis of functional faults in concurrent object-oriented software\n", "abstract": " Design by contract (DbC) is a software development methodology that focuses on clearly defining the interfaces between components to produce better quality object-oriented software. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. Using Java as the target programming language, we tackle such challenges by augmenting the Java Modelling Language (JML) and modifying the JML compiler (jmlc) to generate runtime assertion checking code to support DbC in concurrent programs. We applied our solution in a carefully designed case study on a highly concurrent industrial software system from the telecommunications domain to assess the effectiveness of contracts as test oracles in detecting and diagnosing functional faults in concurrent software. Based on these results, clear and objective requirements are defined for\u00a0\u2026", "num_citations": "15\n", "authors": ["318"]}
{"title": "The UML is more than boxes and lines\n", "abstract": " The Unified Modeling Language (UML) is now the de-facto standard for the analysis and design of object-oriented software systems. There is a general consensus among researchers and practitioners that the UML could have a stronger semantic content. However, even the semantics of the UML, as described for example as well-formedness rules in the UML standard documentation, is not very well-known to many practitioners. As a result, practitioners often perceive the UML merely as a graphic tool. This paper discusses the apprenticeship of the UML semantics and presents a pedagogical method to help students overcome their limited view of the UML language as merely a set of annotated boxes and lines and to allow them to discover UML semantics.", "num_citations": "15\n", "authors": ["318"]}
{"title": "Combining UML sequence and state machine diagrams for data-flow based integration testing\n", "abstract": " UML interaction diagrams are used during integration testing. However, this will typically not find all integration faults as some incorrect behaviors are only exhibited in certain states of the collaborating classes during interactions. State machine diagrams are typically used to model the behavior of state-dependent objects. This paper presents a technique to enhance interaction testing by accounting for state-based behavior as well as data-flow information. UML sequence and state machine diagrams are combined into a control-flow graph to then generate integration test cases, adapting well-known coupling-based, data-flow testing criteria. In order to assess our technique, we developed a prototype tool and applied it on a small case study. The results suggest that the proposed technique is more cost-effective than the most closely related approach reported in the literature, which only relies on control flow\u00a0\u2026", "num_citations": "14\n", "authors": ["318"]}
{"title": "VPML: an approach to detect design patterns of MOF-based modeling languages\n", "abstract": " A design pattern is a recurring and well-understood design fragment. In a model-driven engineering methodology, detecting occurrences of design patterns supports the activities of model comprehension and maintenance. With the recent explosion of domain-specific modeling languages, each with its own syntax and semantics, there has been a corresponding explosion in approaches to detecting design patterns that are so much tailored to those many languages that they are difficult to reuse. This makes developing generic analysis tools extremely hard. Such a generic tool is however desirable to reduce the learning curve for pattern designers as they specify patterns for different languages used to model different aspects of a system. In this paper, we propose a unified approach to detecting design patterns of MOF-based modeling languages. MOF is increasingly used to define modeling languages\u00a0\u2026", "num_citations": "13\n", "authors": ["318"]}
{"title": "An Analysis of Complex Industrial Test Code using Clone analysis\n", "abstract": " Many companies, including Ericsson, experience increased software verification costs. Agile cross-functional teams find it easy to make new additions of test cases for every change and fix. The consequence of this phenomenon is duplications of test code. In this paper, we perform an industrial case study that aims at better understanding such duplicated test fragments or as we call them, clones. In our study, 49% (LOC) of the entire test code are clones. The reported results include figures about clone frequencies, types, similarity, fragments, and size distributions, and the number of line differences in cloned test cases. It is challenging to keep clones consistent and remove unnecessary clones during the entire testing process of large-scale commercial software.", "num_citations": "12\n", "authors": ["318"]}
{"title": "State-based tests suites automatic generation tool (Stage-1)\n", "abstract": " State diagrams are widely used to model software artifacts, making state-based testing an interesting research topic. When conducting research on state-based testing for evaluating different testing criteria, often there is a need to devise numerous test suites in a systematic way according to selection criteria such as all-edges, all-transition-pairs, or the transition tree (W-method). Moreover, one also needs to satisfy each criterion in as many ways as possible to account for possible stochastic phenomena within each criterion. The main issue is then: how to automate the generation of as many, or even all, the different test suites for each criterion? This paper presents the first part of a framework, an automation tool chain that generates test trees from a state machine diagram, extracts test cases from the generated trees, and composes a test suite from each generated tree. This tool is the first to generate all possible\u00a0\u2026", "num_citations": "11\n", "authors": ["318"]}
{"title": "Diagram definition: a case study with the UML class diagram\n", "abstract": " The abstract syntax of a graphical modeling language is typically defined with a metamodel while its concrete syntax (diagram) is informally defined with text and figures. Recently, the Object Management Group (OMG) released a beta specification, called Diagram Definition (DD), to formally define both the interchange syntax and the graphical syntax of diagrams. In this paper, we validate DD by using it to define a subset of the UML class diagram. Specifically, we define the interchange syntax with a MOF-based metamodel and the graphical syntax with a QVT mapping to a graphics metamodel. We then run an experiment where we interchange and render an example diagram. We highlight various design decisions and discuss challenges of using DD in practice. Finally, we conclude that DD is a sound approach for formally defining diagrams that is expected to facilitate the interchange and the consistent\u00a0\u2026", "num_citations": "11\n", "authors": ["318"]}
{"title": "An experimental evaluation of the impact of system sequence diagrams and system operation contracts on the quality of the domain model\n", "abstract": " The Unified Modeling Language (UML) is an object-oriented analysis and design language widely used to created artifacts during the software system lifecycle. UML being a standard notation, without specific guidelines as to how to use it, it must be applied in the context of a specific software development process. The Unified Process (UP) is one such process, extensively used by the object-oriented community, which delivers software best practices via guidelines for all software lifecycle activities. The UP suggests many artifacts to be produced during the software lifecycle. But many practitioners are reluctant to use those artifacts as they question their benefits. System Sequence Diagrams and System Operation Contracts are artifacts, suggested by Larman in his well-known methodology, to complement standard UP artifacts with the intent of better understanding the input and output events related to the system\u00a0\u2026", "num_citations": "11\n", "authors": ["318"]}
{"title": "On the effectiveness of contracts as test oracles in the detection and diagnosis of race conditions and deadlocks in concurrent object-oriented software\n", "abstract": " The idea behind Design by Contract (DbC) is that a method defines a contract stating the requirements a client needs to fulfill to use it, the precondition, and the properties it ensures after its execution, the post condition. Though there exists ample support for DbC for sequential programs, applying DbC to concurrent programs presents several challenges. We have proposed a solution to these challenges in the context of Java as programming language and the Java Modeling language as specification language. This paper presents our findings when applying our DbC technique on an industrial case study to evaluate the ability of contract-based, runtime assertion checking code at detecting and diagnosing race conditions and deadlocks during system testing. The case study is a highly concurrent industrial system from the telecommunications domain, with actual faults. It is the first work to systematically investigate\u00a0\u2026", "num_citations": "11\n", "authors": ["318"]}
{"title": "Specification-based testing of intrusion detection engines using logical expression testing criteria\n", "abstract": " An Intrusion Detection System (IDS) protects computer networks against attacks and intrusions. One class of IDS is called signature-based network IDSs as they monitor network traffic, looking for evidence of malicious behaviour as specified in attack descriptions (referred to as signatures). Many studies report that IDSs have problems accurately identifying attacks. Therefore, it is important to precisely understand under which conditions IDSs accurately identify attacks or fail to do so. However, no systematic approach has so far been defined and used to study this problem. Recognizing that signatures in essence provide the specification of an IDS engine, studying the accuracy of an IDS engine becomes a black-box testing problem. We therefore precisely and systematically evaluate which mature testing techniques can be used (and adapted) to derive tests from IDS signatures. We experiment with those criteria on\u00a0\u2026", "num_citations": "11\n", "authors": ["318"]}
{"title": "1st International Workshop on UML Consistency Rules (WUCOR 2015) Post workshop report\n", "abstract": " The Unified Modeling Language (UML), with its 14 different diagram types, is the de-facto standard modeling language for object-oriented software modeling and documentation. Since the various UML diagrams describe different views of one, and only one, software system under development, they strongly depend on each other in many ways. In other words, the UML diagrams describing a software system must be consistent. Inconsistencies among these diagrams may be a source of faults during software development and analysis. It is therefore paramount that these inconsistencies be detected, analyzed and -- hopefully -- fixed. The goal of this workshop was to gather input and feedbacks on UML consistency rules from the community. This workshop provided an opportunity for researchers who have been working in the area of UML consistency to interact with each other at a highly interactive venue, improve\u00a0\u2026", "num_citations": "10\n", "authors": ["318"]}
{"title": "A multi-objective genetic algorithm for generating test suites from extended finite state machines\n", "abstract": " We propose a test suite generation technique from extended finite state machines based on a genetic algorithm that fulfills multiple (conflicting) objectives. We aim at maximizing coverage and feasibility of a set of test cases while minimizing similarity between these cases and minimizing overall cost.", "num_citations": "10\n", "authors": ["318"]}
{"title": "A UML/MARTE model analysis method for detection of data races in concurrent systems\n", "abstract": " The earlier concurrency problems are identified, the less costly they are to fix. As larger, more complex concurrent systems are developed, early detection of problems is made increasingly difficult. We have developed a general approach meant to be used in the context of Model Driven Development. Our approach is based on the analysis of design models expressed in the Unified Modeling Language (UML) and uses specifically designed genetic algorithms to detect concurrency problems. Our main motivation is to devise practical solutions that are applicable in the context of UML design of concurrent systems without requiring additional modeling. All relevant concurrency information is extracted from UML models that comply with the UML Modeling and Analysis of Real-Time and Embedded Systems (MARTE) profile. Our approach was shown to work for both deadlocks and starvation. The current paper\u00a0\u2026", "num_citations": "10\n", "authors": ["318"]}
{"title": "Experimenting with Category Partition's 1-way and 2-way test selection criteria\n", "abstract": " The Category Partition (CP) black-box testing method has shown to be effective in a number of situations. There is however little support for automating its use and little is known about the cost effectiveness of its associated selection criteria. In this paper, we report on a tool to automatically create test frames, i.e., test case specifications, for three wellknown criteria associated with the CP method, including the application of 1-way and 2-way interactions. We then report on the cost, in terms of number of test cases, and the effectiveness, in terms of mutation score, of adequate test suites for these criteria. The main lesson learnt is that, in addition to the intuition that the CP specification does impact effectiveness, a significant part of the effectiveness is also due to the test input selection procedure for test frames.", "num_citations": "9\n", "authors": ["318"]}
{"title": "On the verification and validation of signature-based, network intrusion detection systems\n", "abstract": " An Intrusion Detection System (IDS) protects computer networks against attacks and intrusions in combination with firewalls and anti-virus systems. One class of IDS is called signature-based network IDSs as they monitor network traffic, looking for evidence of malicious behaviour as specified in attack descriptions (referred to as signatures). It is common knowledge in the research community that IDSs have problems accurately identifying attacks. In this paper we discuss this accuracy problem and decompose it into a detection problem and a confirmation problem. We then map the evaluation of this accuracy problem to the traditional software verification and validation problem, which allows us to analyze the techniques academics have been using to evaluate their IDS technologies. As a result, we are able to identify areas where research is needed to improve the assessment of the IDS accuracy problem through\u00a0\u2026", "num_citations": "9\n", "authors": ["318"]}
{"title": "Model-driven, network-context sensitive intrusion detection\n", "abstract": " Intrusion Detection Systems (IDSs) have the reputation of generating many false positives. Recent approaches, known as stateful IDSs, take the state of communication sessions into account to address this issue. A substantial reduction of false positives, however, requires some correlation between the state of the session, known vulnerabilities, and the gathering of more network context information by the IDS than what is currently done (e.g., configuration of a node, its operating system, running applications). In this paper we present an IDS approach that attempts to decrease the number of false positives by collecting more network context and combining this information with known vulnerabilities. The approach is model-driven as it relies on the modeling of packet and network information as UML class diagrams, and the definition of intrusion detection rules as OCL expressions constraining these diagrams\u00a0\u2026", "num_citations": "9\n", "authors": ["318"]}
{"title": "Inferring a Distributed Application Behavior Model for Anomaly Based Intrusion Detection\n", "abstract": " As distributed computations become more and more common in highly distributed environments like the cloud, intrusion detection systems have to follow these paradigms. Anomaly based intrusion detection systems in distributed systems usually rely on a total order of the observed events. However, such hypothesis is often too strong, as in a highly distributed environment the order of the observed events is partially unknown. This paper demonstrates it is possible to infer a distributed application behavior model for intrusion detection, relying only on a partial ordering of events. The originality of the proposed approach is to tackle the problem by combining two types of models that are usually used separately: an automaton modeling the distributed computation, and a list of temporal properties that the computation must comply with. Finally, we apply the approach on two examples, and assess the method on a real\u00a0\u2026", "num_citations": "8\n", "authors": ["318"]}
{"title": "Modeling traceability for heterogeneous systems\n", "abstract": " In System Engineering, many systems encompass widely different domains of expertise; there are several challenges in relating these domains due to their heterogeneity and complexity. Although, literature provides many techniques to model traceability among heterogeneous domains, existing solutions are either tailored to specific domains (e.g., Ecore modeling languages), or not complete enough (e.g., lack support to specify traceability link semantics). This paper proposes a generic traceability model that is not domain specific; it provides a solution for modeling traceability links among heterogeneous models, that is, systems for which traceability links need to be established between artifacts in widely different modeling languages (e.g., UML, block diagrams, informal documents). Our solution tackles the drawbacks of existing solutions, and incorporates some of their ideas in an attempt to be as complete as\u00a0\u2026", "num_citations": "8\n", "authors": ["318"]}
{"title": "An analysis of signature overlaps in Intrusion Detection Systems\n", "abstract": " An Intrusion Detection System (IDS) protects computer networks against attacks and intrusions, in combination with firewalls and anti-virus systems. One class of IDS is called signature-based network IDSs, as they monitor network traffic, looking for evidence of malicious behaviour as specified in attack descriptions (referred to as signatures).Many studies report that IDSs, including signature-based network IDSs, have problems to accurately identify attacks. One possible reason that we observed in our past work, and that is worth investigating further, is that several signatures (i.e., several alarms) can be triggered on the same group of packets, a situation we coined overlapping signatures. This paper presents a technique to precisely and systemat ically quantify the signature overlapping problem of an IDS signature database. The solution we describe is based on set theory and finite state automaton theory, and we\u00a0\u2026", "num_citations": "8\n", "authors": ["318"]}
{"title": "Automated state-based online testing real-time embedded software with RTEdge\n", "abstract": " Verifying a real time embedded application is challenging since one has to consider timing requirements in addition to functional ones. During online state-based testing the generation and execution of test cases happen concurrently: test case generation uses information from a state-based test model in combination with observed execution behaviour. This paper describes a practical online testing algorithm that is implemented in the state-based modeling tool RTEdge. Two case studies show that our online testing algorithm produces a test suite that achieves high model coverage, thus facilitating the automated verification of real-time embedded software.", "num_citations": "7\n", "authors": ["318"]}
{"title": "Incremental class testing from a class test order\n", "abstract": " Many approaches exist to decide the order in which classes should be integrated during (integration) testing. Most of them, based on an analysis of class dependencies (for instance described in a UML class diagram) aim at producing a partial order indicating which classes should be tested in sequence and which ones can be tested in parallel. We argue in this article that, thanks to the specifics of such a class test order, it is possible to define an incremental strategy for testing classes that promotes reuse during testing, not only along class inheritance hierarchies.", "num_citations": "7\n", "authors": ["318"]}
{"title": "On FSM-Based Testing: An Empirical Study: Complete Round-Trip Versus Transition Trees\n", "abstract": " Finite state machines being intuitively understandable and suitable for modeling in many domains, they are adopted by many software designers. Therefore, testing systems that are modeled with state machines has received genuine attention. Among the studied testing strategies are complete round-trip paths and transition trees that cover round-trip paths in a piece wise manner. We present an empirical study that aims at comparing the effectiveness of the complete round-trip paths test suites to the transition trees test suites in one hand, and comparing the effectiveness of the different techniques used to generate transition trees (breadth first traversal, depth first traversal, and random traversal) on the other hand. We also compare the effectiveness of all the testing trees generated using each single traversal criterion. This is done through conducting an empirical evaluation using four case studies from different\u00a0\u2026", "num_citations": "6\n", "authors": ["318"]}
{"title": "An extension of category partition testing for highly constrained systems\n", "abstract": " To ensure software is performing as intended it can be black-box or white-box tested. Category partition is a black box, specification based testing technique which begins by identifying the parameters, categories (characteristics of parameters) and choices (acceptable values for categories). These choices are then combined to form test frames on the basis of various criteria such as base choice and each choice. To ensure that the combinations of choices are feasible, constraints are introduced. While combining choices to form an each choice adequate test set it is feasible (e.g., using constrained covering arrays from combinatorial testing), the base choice criterion has not been defined to specifically account for constraints on choices. In this paper, we introduce two extensions of the base choice criterion to specifically account for complex constraints among choices. Adequate test suites of the different criteria are\u00a0\u2026", "num_citations": "6\n", "authors": ["318"]}
{"title": "Towards Automating Interface Control Documents Elaboration and Management.\n", "abstract": " Avionic systems have been migrating from the legacy federated architecture towards an integrated modular architecture (IMA). The IMA architecture replaces the equipment principle by a set of interoperable components (hardware and software). The interoperability between the integrated components requires a detailed specification and description of their interfaces, which, in the avionic domain, is usually written in Interface Control Documents (ICD). However, ICD creation and usage during the integration process is challenging. In fact, the two main problems with the usage of ICDs are the lack of a commonly accepted language to define and use them on the one hand, and the lack of tool support in their production and consumption. In this paper, we present our approach and methodology to overcome these limitations.", "num_citations": "6\n", "authors": ["318"]}
{"title": "Mitigating threats to validity in empirical software engineering: A traceability case study\n", "abstract": " The issue of validity threats in empirical software engineering research is important. However, some authors overlook this, focusing on validating their work through application of fundamental testing techniques, instead. However, testing is different to empirical validation, with the latter being more concerned about how experimental conclusions are justified. An important factor that can render an experimental conclusion incorrect is researcher's bias, which can be especially relevant when setting the experimental parameters. Therefore, consideration of validity threats is essential to enable confidence in research results and assure the research quality. This paper provides a practical approach for mitigating threats to validity in empirical software engineering using a sequence of software activities. The paper is based on a real-world traceability case study for illustration purposes.", "num_citations": "5\n", "authors": ["318"]}
{"title": "Finding All Breadth First Full Spanning Trees in a Directed Graph\n", "abstract": " This paper proposes an algorithm that is particularly concerned with generating all possible distinct spanning trees that are based on breadth-first-search directed graph traversal. The generated trees span all edges and vertices of the original directed graph. The algorithm starts by generating an initial tree, and then generates the rest of the trees using elementary transformations. It runs in O(E+T) time where E is the number of edges and T is the number of generated trees. In the worst-case scenario, this is equivalent to O (E+En/Nn) time complexity where N is the number of nodes in the original graph. The algorithm requires O(T) space. However, possible modifications to improve the algorithm space complexity are suggested. Furthermore, experiments are conducted to evaluate the algorithm performance and the results are listed.", "num_citations": "5\n", "authors": ["318"]}
{"title": "The power of single and error annotations in category partition testing: an experimental evaluation\n", "abstract": " Category Partition (CP) is a black box testing technique that formalizes the specification of the input domain in a CP specification for the system under test. A CP specification is driven by tester's expertise and bundles parameters, categories (characteristics of parameters) and choices (acceptable values for categories) required for extensively testing the system. For completeness the choices correspond to permitted input values as well as some values to account for boundaries or robustness. These choices are then combined to form test frames on the basis of various criteria such as each choice or pairwise. To ensure that the combinations of choices are feasible and account for valid sets of user requirements, constraints are introduced to specify permitted combinations among choices, and to specify single or error choices. In a typical development environment where testing is driven by stringent deadlines a tester\u00a0\u2026", "num_citations": "5\n", "authors": ["318"]}
{"title": "A Data Extraction Process for Avionics Systems\u2019 Interface Specifications\n", "abstract": " Avionics systems, along with their internal hardware and software components interfaces, must be well defined and specified (e.g., unambiguous, complete, verifiable, consistent, and traceable specification). Such a specification is usually written in the form of an Interface Control Document (ICD), and represents the cornerstone of the avionics system integration activities. However, there is no commonly accepted language to define and use these ICDs and no common definition of what an ICD is or should contain. Indeed, avionics companies define their own, proprietary ICDs and processes. In this paper, we first identify the pieces of information that an ICD should contain for both federated and IMA open systems. Then, we propose a data extraction process that enables better understanding and more efficient extraction of open avionics systems interface specifications, and provides a clearer vision on the\u00a0\u2026", "num_citations": "5\n", "authors": ["318"]}
{"title": "Integration testing object-oriented software systems: An experiment-driven research approach\n", "abstract": " Any incremental class testing approach has to answer the two following questions: What integration process, indicating in which order classes are (integration) tested, should be selected? Which test design techniques should be applied to unit and integration test classes? Although there is a fairly large number of reported works on both questions, much remains to be done. On the one hand, class unit/integration testing techniques have mostly been described without any consideration for any integration process. On the other hand, integration processes have been suggested without much consideration for how they could be used in practice, along with class unit/integration testing techniques. There is a lack of research and practical results on how a class integration process can be used to conduct class unit/integration testing. This paper summarizes the problem and suggests an experiment-driven research\u00a0\u2026", "num_citations": "5\n", "authors": ["318"]}
{"title": "How consistency is handled in Model Driven Software Engineering and UML: a survey of experts in academia and industry\n", "abstract": " Model Driven Software Engineering (MDSE) is an established approach for developing complex software systems. The", "num_citations": "4\n", "authors": ["318"]}
{"title": "Employing linked data in building a trace links taxonomy\n", "abstract": " Software traceability provides a means for capturing the relationship between artifacts at all phases of software and systems development. The relationships between the artifacts that are generated during systems development can provide valuable information for software and systems Engineers. It can be used for change impact analysis, systems verification and validation, among other things. However, there is no consensus among researchers about the syntax or semantics of trace links across multiple domains. Moreover, existing trace links classifications do not consider a unified method for combining all trace links types in one taxonomy that can be utilized in Requirement Engineering, Model Driven Engineering and Systems Engineering. This paper is one step towards solving this issue. We first present requirements that a trace links taxonomy should satisfy. Second, we present a technique to build a trace links taxonomy that has well-defined semantics. We implemented the taxonomy by employing the Link data and the Resource Description Framework (RDF). The taxonomy can be configured with traceability models using Open Service for Lifecycle Collaboration (OSLC) in order to capture traceability information among different artifacts and at different levels of granularity. In addition, the taxonomy offers reasoning and quantitative and qualitative analysis about trace links. We presented validation criteria for validating the taxonomy requirements and validate the solution through an example.", "num_citations": "4\n", "authors": ["318"]}
{"title": "UML consistency rules in technical books\n", "abstract": " This paper highlights the need of identifying a set of UML consistency rules that is as complete as possible. For that purpose, we propose to complement our previous research works, by searching for such rules directly from technical books on UML-based object-oriented software development.", "num_citations": "4\n", "authors": ["318"]}
{"title": "On the effect of counters in guard conditions when state-based multi-objective testing\n", "abstract": " During test case generation from an extended finite state machine (EFSM), the counter problem is caused by the presence of guard conditions that refer to counter variables. Because such variables are initialized and updated by transitions in the EFSM, every traversal of the state machine graph is not necessarily feasible, i.e., executable. The problem manifests itself by the fact that a transition, a sequence of transitions, or a more complex behavior in the state machine, has to be repeatedly triggered to eventually trigger a specific behavior (another transition). In this paper we define different manifestations of the counter problem and experiment with a new search based solution for that problem. We also investigate how the counter problem affects a multi-objective genetic algorithm that generates test suites from an EFSM. We evaluate our solution and compare it with an existing one, using three different case studies.", "num_citations": "4\n", "authors": ["318"]}
{"title": "Using semantic web to establish traceability links between heterogeneous artifacts\n", "abstract": " Semantic Web enables the users of the World Wide Web (WWW) to create non-traditional data repositories. The data can be linked in a flat hierarchy structure that allows the extensibility of data without the need for changing the structure itself. The linked data along with other rules can be used to infer or extract other data. We propose a semantic web technique that employs the Resource Description Framework (RDF) for building a trace links taxonomy. The taxonomy can be utilized to link heterogeneous artifacts coming from different domains of expertise. This technique allows users to refer to any trace link type in the taxonomy using a unique Uniform Resource Identifier (URI). The taxonomy can also be integrated to a traceability framework using the Open Service for Lifecycle Collaboration (OSLC) in order to accommodate the traceability of heterogeneous artifacts. We present validation criteria for\u00a0\u2026", "num_citations": "3\n", "authors": ["318"]}
{"title": "UML Consistency Rules: a Case Study with Open-Source UML Models\n", "abstract": " UML models are standard artifacts used by software engineers for designing software. As software is designed, different UML diagram types (eg, class diagrams and sequence diagrams) are produced by software designers. Since the various UML diagram types describe different aspects of a software system, they are not independent but strongly depend on each other, hence they must be consistent. Inconsistencies cause faults in thefi nal software systems. It is, therefore, paramount that they get detected, analyzed, andfi xed. Consistency rules are a useful tool proposed in the literature to detect inconsistencies. They categorize constraints that help in identifying inconsistencies when violated. This case study aims at collecting and analyzing UML models with OCL consistency rules proposed in the literature and at promoting the development of a reference benchmark that can be reused by the (FM-) research\u00a0\u2026", "num_citations": "2\n", "authors": ["318"]}
{"title": "Traceability in Systems Engineering: An Avionics Case Study\n", "abstract": " In Systems Engineering (SE), development of complex systems involves a collaboration of expertise from different domains. Heterogeneous artifacts are generated using different modeling tools. Capturing the traceability information among these artifacts helps serve many purposes, including change impact analysis; validation and verification; and requirements tracking. However, creating trace links among these heterogeneous artifacts is problematic. No precise semantics exist for the trace links that relate them. This paper shows how to capture traceability information in a system with heterogeneous artifacts, illustrated here using an avionics case study that uses a traceability model and a trace links taxonomy that we constructed and published previously.", "num_citations": "2\n", "authors": ["318"]}
{"title": "How does GUI testing exercise application logic functionality?\n", "abstract": " The practitioner interested in reducing software verification effort may found herself lost in the many alternative definitions of Graphical User Interface (GUI) testing that exist and their relation to the notion of system testing . One result of these many definitions is that one may end up testing the same parts of the Software Under Test (SUT), specifically the application logic, twice. To clarify two important testing activities and avoid duplicate testing effort, this paper empirically evaluates to what extent GUI tests exercise the application logic of the software under test (and not only the GUI code). Experimental results show that GUI tests do not necessarily entirely exercise application logic functionality, at least not as much as system tests directly interacting with application logic code.", "num_citations": "2\n", "authors": ["318"]}
{"title": "Joint Proceedings of the 8th International Workshop on Model-based Architecting of Cyber-physical and Embedded Systems and 1st International Workshop on UML Consistency Rules\u00a0\u2026\n", "abstract": " Joint Proceedings of the 8th International Workshop on Model-based Architecting of Cyber-physical and Embedded Systems and 1st International Workshop on UML Consistency Rules (ACES-MB 2015 & WUCOR 201 \u2014 Aalto University's research portal Skip to main navigation Skip to search Skip to main content Aalto University's research portal Logo Accessibility statement English Suomi Home Profiles Research output Datasets Projects Prizes Activities Press / Media Infrastructure Research Units Impacts Search by expertise, name or affiliation Joint Proceedings of the 8th International Workshop on Model-based Architecting of Cyber-physical and Embedded Systems and 1st International Workshop on UML Consistency Rules (ACES-MB 2015 & WUCOR 201 Iulia Dragomir, Susanne Graf, Gabor Karsai, Florian Noyrit, Iulian Ober, Damiano Torre, Yvan Labiche, Marcela Genero, Maged Elaasar Department of output\u2026", "num_citations": "2\n", "authors": ["318"]}
{"title": "Model interchange testing: a process and a case study\n", "abstract": " Modeling standards by the Object Management Group (OMG) enable the interchange of models between tools. In practice, the success of such interchange has been severely limited due to ambiguities and inconsistencies in the standards and lack of rigorous testing of tools\u2019 interchange capabilities. This motivated a number of OMG members, including tool vendors and users, to form a Model Interchange Working Group (MIWG) to test and improve model interchange between tools. In this paper, we report on the activities of the MIWG, presenting its testing process and highlighting its design decisions and challenges. We also report on a case study where the MIWG has used its process to test the interchange of UML and SysML models. We make observations, present statistics and discuss lessons learned. We conclude that the MIWG has indeed defined a rigorous, effective and semi-automated process for\u00a0\u2026", "num_citations": "2\n", "authors": ["318"]}
{"title": "Requirement-based software testing with the UML: a systematic mapping study\n", "abstract": " Our goal is to determine the current state of the art in requirement based testing in a UML context. We combined an automated search in digital libraries with a manual search in related journal and conference venues. The search resulted in about 1,300 papers. After applying inclusion/exclusion criteria, we selected 100 papers as our final set of primary studies. Classification results based on several criteria lead us to interesting observations such as: A small proportion of the primary studies evaluate techniques through experiments and one-third of the techniques are simply illustrated with an example; More advanced selection criteria exist in literature than those used in primary studies.", "num_citations": "2\n", "authors": ["318"]}
{"title": "On testing object-oriented programs\n", "abstract": " The object-oriented paradigm is a new technology for producing software. This new technology has many benefits for parts of the entire software development cycle (analysis, design and implementation phases): the object-oriented development process is iterative, the object-oriented paradigm emphasize reuse, the items of interest are always the objects,... Thus, engineers and managers want to use this technology in their own field. But, for critical systems, which need a certification, the testing process is an important task. Then, the testing techniques for object-oriented programs should be studied even though some people can think the object-oriented paradigm seams to be the panacea. By applying usual testing techniques (ie those for procedural programs) for object-oriented programs one found two major problems. First, procedural testing techniques are not well-suited for object-oriented ones: there is an intrinsic difference between the procedural and the object-oriented approach (structure vs. behaviour). Second, the new useful features introduced (such as encapsulation, inheritance, or polymorphism) imply new problems for the testing process: problems of observability, for example.Then we are interested in studying these new mechanisms with the tester viewpoint in mind through three major questions. What are the new problems raised? How usual solutions for the testing process can be applied (or extended) or do we have to find new ones? and, How objectorientedness (the new mechanisms) can help us for the testing process?", "num_citations": "2\n", "authors": ["318"]}
{"title": "Test automation-Automation of what?\n", "abstract": " Taking a birds-eye view at the different activities that take place when someone engages in software testing, we discuss automation problems and some deployed solutions to the broad notion of software test automation. In doing so, we discover engineering/deployment problems as well as more fundamental scientific/research issues.", "num_citations": "1\n", "authors": ["318"]}
{"title": "Towards GUI functional verification using abstract interpretation\n", "abstract": " Abstract interpretation is a static analysis technique used mostly for non-functional verification of software. In this paper, we show the status of the technology that implements abstract interpretation which can help in GUI-based software verification. Specifically, we investigate the use of the Julia tool for the functional verification of a Graphical User Interface (GUI).", "num_citations": "1\n", "authors": ["318"]}
{"title": "Life Sciences-Inspired Test Case Similarity Measures for Search-Based, FSM-Based Software Testing\n", "abstract": " Researchers and practitioners alike have the intuition that test cases diversity is positively correlated to fault detection. Empirical results already show that some measurement of diversity within a pre-existing state-based test suite (i.e., a test suite not necessarily created to have diverse tests in the first place) indeed relates to fault detection. In this paper we show how our procedure, based on a genetic algorithm, to construct an entire (all-transition) adequate test suite with as diverse tests as possible fares in terms of fault detection. We experimentally compare on a case study nine different ways of computing test suite diversity, including measures already used by others in software testing as well as measures inspired by the notion of diversity in the life sciences. Although our results confirm a positive correlation between diversity and fault detection, we believe our results raise more questions than they\u00a0\u2026", "num_citations": "1\n", "authors": ["318"]}
{"title": "Extending Category Partition\u2019s Base Choice criterion to better support constraints\n", "abstract": " To ensure software is performing as intended, it can be black\u2010box or white\u2010box tested. Category partition is a black\u2010box, specification\u2010based testing technique that begins by identifying the parameters, categories (characteristics of parameters), and choices (acceptable values for categories). These choices are then combined to form test frames on the basis of various criteria such as Base Choice and Each Choice. To ensure that the combinations of choices are feasible, constraints on choices are introduced. Combining choices, while accounting for constraints, to form an each choice adequate test set is feasible (eg, using constrained covering arrays from combinatorial testing). However, the Base Choice criterion has not been defined to specifically account for constraints on choices, resulting in adverse consequences. In this paper, we introduce two extensions to the Base Choice criterion, namely, Constrained\u00a0\u2026", "num_citations": "1\n", "authors": ["318"]}
{"title": "An analysis and extension of Category partition testing for constrained systems\n", "abstract": " Software systems are overly complex and testing them is a challenging task. Testing such systems in the absence of systematic techniques results either in incomplete testing or testing with unmanageable (time, cost) number of test cases.", "num_citations": "1\n", "authors": ["318"]}
{"title": "Getting more in less: The power of single/error annotations in category partition\n", "abstract": " Category Partition (CP) [1] is a black box testing technique which is used to formalize the specification of the input domain. The process which relies on the expertise of the tester begins by identifying the parameters and environment variables on the basis of function's behaviour. The characteristics/categories of these parameters/environment variables are identified and partitioned into choices. The choices of a category are mutually exclusive and can be based on input partitioning and boundary value analysis. Thereafter, the choices are combined on the basis of various combination criteria (each choice, pairwise, all combination, base choice) to form test frames.", "num_citations": "1\n", "authors": ["318"]}
{"title": "A comparative study of invariants generated by Daikon and user-defined design contracts\n", "abstract": " A lot of progress has been made towards reverse-engineering program specification under the form of contracts. Ensuring the quality of such reverse-engineered contracts, referred to as likely invariants when using Daikon, is paramount since those contracts are used in several other contexts. One aspect that can influence the \"quality\" of the reverse-engineered contracts is the configuration being used when executing Daikon. In this paper we evaluate the impact of two such configuration parameters which help the user control in two different ways how many variables of the program are considered by Daikon when inferring likely invariants. We perform a case study with a program equipped with test cases and high-level design contracts (i.e., design contracts produced before implementation) and systematically compare likely invariants reverse-engineered by Daikon to those contracts, thanks to a comparison\u00a0\u2026", "num_citations": "1\n", "authors": ["318"]}
{"title": "Planning and scheduling from a class test order\n", "abstract": " One of the characteristics of object-oriented software is the complex dependencies that may exist between classes due to generalization and client-server relationships. Hence, where to start testing and how to define an integration strategy are issues that require investigation. A number of techniques exist to order the test of classes with the aim of reducing costs. They usually generate a class test order that is a directed graph indicating in which order classes have to be tested, or which classes can be tested in parallel. This paper shows how such a class test order can be beneficial to testers and designers when conducting various activities, such as planning and scheduling class testing. This is illustrated by means of a case study", "num_citations": "1\n", "authors": ["318"]}