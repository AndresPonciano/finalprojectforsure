{"title": "Beautiful interpolants\n", "abstract": " We describe a compositional approach to Craig interpolation based on the heuristic that simpler proofs of special cases are more likely to generalize. The method produces simple interpolants because it is able to summarize a large set of cases using one relatively simple fact. In particular, we present a method for finding such simple facts in the theory of linear rational arithmetic. This makes it possible to use interpolation to discover inductive invariants for numerical programs that are challenging for existing techniques. We show that in some cases, the compositional approach can also be more efficient than traditional lazy SMT as a decision procedure.", "num_citations": "101\n", "authors": ["705"]}
{"title": "MapReduce program synthesis\n", "abstract": " By abstracting away the complexity of distributed systems, large-scale data processing platforms\u2014MapReduce, Hadoop, Spark, Dryad, etc.\u2014have provided developers with simple means for harnessing the power of the cloud. In this paper, we ask whether we can automatically synthesize MapReduce-style distributed programs from input\u2013output examples. Our ultimate goal is to enable end users to specify large-scale data analyses through the simple interface of examples. We thus present a new algorithm and tool for synthesizing programs composed of efficient data-parallel operations that can execute on cloud computing infrastructure. We evaluate our tool on a range of real-world big-data analysis tasks and general computations. Our results demonstrate the efficiency of our approach and the small number of examples it requires to synthesize correct, scalable programs.", "num_citations": "90\n", "authors": ["705"]}
{"title": "Synthesizing coupling proofs of differential privacy\n", "abstract": " Differential privacy has emerged as a promising probabilistic formulation of privacy, generating intense interest within academia and industry. We present a push-button, automated technique for verifying \u03b5-differential privacy of sophisticated randomized algorithms. We make several conceptual, algorithmic, and practical contributions: (i) Inspired by the recent advances on approximate couplings and randomness alignment, we present a new proof technique called coupling strategies, which casts differential privacy proofs as a winning strategy in a game where we have finite privacy resources to expend. (ii) To discover a winning strategy, we present a constraint-based formulation of the problem as a set of Horn modulo couplings (HMC) constraints, a novel combination of first-order Horn clauses and probabilistic constraints. (iii) We present a technique for solving HMC constraints by transforming probabilistic\u00a0\u2026", "num_citations": "51\n", "authors": ["705"]}
{"title": "Constraint-based synthesis of datalog programs\n", "abstract": " We study the problem of synthesizing recursive Datalog programs from examples. We propose a constraint-based synthesis approach that uses an smt solver to efficiently navigate the space of Datalog programs and their corresponding derivation trees. We demonstrate our technique\u2019s ability to synthesize a range of graph-manipulating recursive programs from a small number of examples. In addition, we demonstrate our technique\u2019s potential for use in automatic construction of program analyses from example programs and desired analysis output.", "num_citations": "38\n", "authors": ["705"]}
{"title": "Syntax-guided synthesis of datalog programs\n", "abstract": " Datalog has witnessed promising applications in a variety of domains. We propose a programming-by-example system, ALPS, to synthesize Datalog programs from input-output examples. Scaling synthesis to realistic programs in this manner is challenging due to the rich expressivity of Datalog. We present a syntax-guided synthesis approach that prunes the search space by exploiting the observation that in practice Datalog programs comprise rules that have similar latent syntactic structure. We evaluate ALPS on a suite of 34 benchmarks from three domains\u2014knowledge discovery, program analysis, and database queries. The evaluation shows that ALPS can synthesize 33 of these benchmarks, and outperforms the state-of-the-art tools Metagol and Zaatar, which can synthesize only up to 10 of the benchmarks.", "num_citations": "34\n", "authors": ["705"]}
{"title": "Authoring and verifying human-robot interactions\n", "abstract": " As social agents, robots designed for human interaction must adhere to human social norms. How can we enable designers, engineers, and roboticists to design robot behaviors that adhere to human social norms and do not result in interaction breakdowns? In this paper, we use automated formal-verification methods to facilitate the encoding of appropriate social norms into the interaction design of social robots and the detection of breakdowns and norm violations in order to prevent them. We have developed an authoring environment that utilizes these methods to provide developers of social-robot applications with feedback at design time and evaluated the benefits of their use in reducing such breakdowns and violations in human-robot interactions. Our evaluation with application developers (N= 9) shows that the use of formal-verification methods increases designers' ability to identify and contextualize social\u00a0\u2026", "num_citations": "27\n", "authors": ["705"]}
{"title": "Fairness-aware programming\n", "abstract": " Increasingly, programming tasks involve automating and deploying sensitive decision-making processes that may have adverse impacts on individuals or groups of people. The issue of fairness in automated decision-making has thus become a major problem, attracting interdisciplinary attention. In this work, we aim to make fairness a first-class concern in programming. Specifically, we propose fairness-aware programming, where programmers can state fairness expectations natively in their code, and have a runtime system monitor decision-making and report violations of fairness.", "num_citations": "22\n", "authors": ["705"]}
{"title": "On the use of planning technology for verification\n", "abstract": " Formal verification of hardware and software systems in-volves proving or disproving the correctness of the intended behaviour of the system with respect to a formal specifica-tion of the system. An effective means of automating this process is by representing properties to be verified in a tem-poral logic and exploiting model checking or theorem prov-ing. In this paper we examine the use of planning techniques to perform automated verification. In particular, we exploit recent advances in heuristic search-based planning for tem-porally extended goals. We translate a specification of the behaviour of our system into planning domains and spec-ify safety, liveness, and fairness properties and constraints in linear temporal logic (LTL). Verification of these properties and constraints can then be achieved by some form of plan-ning where LTL formulae are treated as temporally extended goals. To illustrate our approach, we translate models from SMV to planning domains and problems in PDDL. Our ini-tial experimental results comparing our planning approach to NuSMV and SPIN show that planners can provide significant time improvements when checking safety and liveness prop-erties compared to state-of-the-art model checkers.", "num_citations": "21\n", "authors": ["705"]}
{"title": "Beyond storage APIs: Provable semantics for storage stacks\n", "abstract": " Applications are deployed upon deep, diverse storage stacks that are constructed on-demand. Although many storage stacks share a common API to allow portability, application behavior differs in subtle ways depending upon unspecified properties of the underlying storage stack. Currently, there is no way to test whether an application will behave correctly on a given storage stack: corruption or data loss could occur at any point in the application lifetime.", "num_citations": "20\n", "authors": ["705"]}
{"title": "Protocol-aware recovery for consensus-based storage\n", "abstract": " We introduce protocol-aware recovery (PAR), a new approach that exploits protocol-specific knowledge to correctly recover from storage faults in distributed systems. We demonstrate the efficacy of PAR through the design and implementation of corruption-tolerant replication (CTRL), a PAR mechanism specific to replicated state machine (RSM) systems. We experimentally show that the CTRL versions of two systems, LogCabin and ZooKeeper, safely recover from storage faults and provide high availability, while the unmodified versions can lose data or become unavailable. We also show that the CTRL versions have little performance overhead.", "num_citations": "17\n", "authors": ["705"]}
{"title": "Bodystorming human-robot interactions\n", "abstract": " Designing and implementing human-robot interactions requires numerous skills, from having a rich understanding of social interactions and the capacity to articulate their subtle requirements, to the ability to then program a social robot with the many facets of such a complex interaction. Although designers are best suited to develop and implement these interactions due to their inherent understanding of the context and its requirements, these skills are a barrier to enabling designers to rapidly explore and prototype ideas: it is impractical for designers to also be experts on social interaction behaviors, and the technical challenges associated with programming a social robot are prohibitive. In this work, we introduce Synth\u00e9, which allows designers to act out, or bodystorm, multiple demonstrations of an interaction. These demonstrations are automatically captured and translated into prototypes for the design team using\u00a0\u2026", "num_citations": "16\n", "authors": ["705"]}
{"title": "Discovering relational specifications\n", "abstract": " Formal specifications of library functions play a critical role in a number of program analysis and development tasks. We present Bach, a technique for discovering likely relational specifications from data describing input-output behavior of a set of functions comprising a library or a program. Relational specifications correlate different executions of different functions; for instance, commutativity, transitivity, equivalence of two functions, etc. Bach combines novel insights from program synthesis and databases to discover a rich array of specifications. We apply Bach to learn specifications from data generated for a number of standard libraries. Our experimental evaluation demonstrates Bach's ability to learn useful and deep specifications in a small amount of time.", "num_citations": "14\n", "authors": ["705"]}
{"title": "Program synthesis with equivalence reduction\n", "abstract": " We introduce program synthesis with equivalence reduction, a synthesis methodology that utilizes relational specifications over components of a given synthesis domain to reduce the search space. Leveraging a blend of classic and modern techniques from term rewriting, we use relational specifications to discover a canonical representative per equivalence class of programs. We show how to design synthesis procedures that only consider programs in normal form, thus pruning the search space. We discuss how to implement equivalence reduction using efficient data structures, and demonstrate the significant reductions it can achieve in synthesis time.", "num_citations": "13\n", "authors": ["705"]}
{"title": "Distribution policies for datalog\n", "abstract": " Modern data management systems extensively use parallelism to speed up query processing over massive volumes of data. This trend has inspired a rich line of research on how to formally reason about the parallel complexity of join computation. In this paper, we go beyond joins and study the parallel evaluation of recursive queries. We introduce a novel framework to reason about multi-round evaluation of Datalog programs, which combines implicit predicate restriction with distribution policies to allow expressing a combination of data-parallel and query-parallel evaluation strategies. Using our framework, we reason about key properties of distributed Datalog evaluation, including parallel-correctness of the evaluation strategy, disjointness of the computation effort, and bounds on the number of communication rounds.", "num_citations": "9\n", "authors": ["705"]}
{"title": "Synthesizing action sequences for modifying model decisions\n", "abstract": " When a model makes a consequential decision, eg, denying someone a loan, it needs to additionally generate actionable, realistic feedback on what the person can do to favorably change the decision. We cast this problem through the lens of program synthesis, in which our goal is to synthesize an optimal (realistically cheapest or simplest) sequence of actions that if a person executes successfully can change their classification. We present a novel and general approach that combines search-based program synthesis and test-time adversarial attacks to construct action sequences over a domain-specific set of actions. We demonstrate the effectiveness of our approach on a number of deep neural networks.", "num_citations": "9\n", "authors": ["705"]}
{"title": "Effectively propositional interpolants\n", "abstract": " We present a novel interpolation algorithm for effectively propositional logic (epr), a decidable fragment of first-order logic that enjoys a small-model property. epr is a powerful fragment of quantified formulas that has been used to model and verify a range of programs, including heap-manipulating programs and distributed protocols. Our interpolation technique samples finite models from two sides of the interpolation problem and generalizes them to learn a quantified interpolant. Our results demonstrate our technique\u2019s ability to compute universally-quantified, existentially-quantified, as well as alternation-free interpolants and inductive invariants, thus improving the state of the art.", "num_citations": "9\n", "authors": ["705"]}
{"title": "Synthesizing differentially private programs\n", "abstract": " Inspired by the proliferation of data-analysis tasks, recent research in program synthesis has had a strong focus on enabling users to specify data-analysis programs through intuitive specifications, like examples and natural language. However, with the ever-increasing threat to privacy through data analysis, we believe it is imperative to reimagine program synthesis technology in the presence of formal privacy constraints.   In this paper, we study the problem of automatically synthesizing randomized, differentially private programs, where the user can provide the synthesizer with a constraint on the privacy of the desired algorithm. We base our technique on a linear dependent type system that can track the resources consumed by a program, and hence its privacy cost. We develop a novel type-directed synthesis algorithm that constructs randomized differentially private programs. We apply our technique to the\u00a0\u2026", "num_citations": "8\n", "authors": ["705"]}
{"title": "Trace abstraction modulo probability\n", "abstract": " We propose trace abstraction modulo probability, a proof technique for verifying high-probability accuracy guarantees of probabilistic programs. Our proofs overapproximate the set of program traces using failure automata, finite-state automata that upper bound the probability of failing to satisfy a target specification. We automate proof construction by reducing probabilistic reasoning to logical reasoning: we use program synthesis methods to select axioms for sampling instructions, and then apply Craig interpolation to prove that traces fail the target specification with only a small probability. Our method handles programs with unknown inputs, parameterized distributions, infinite state spaces, and parameterized specifications. We evaluate our technique on a range of randomized algorithms drawn from the differential privacy literature and beyond. To our knowledge, our approach is the first to automatically establish\u00a0\u2026", "num_citations": "8\n", "authors": ["705"]}
{"title": "Scaling-up in-memory datalog processing: Observations and techniques\n", "abstract": " Recursive query processing has experienced a recent resurgence, as a result of its use in many modern application domains, including data integration, graph analytics, security, program analysis, networking and decision making. Due to the large volumes of data being processed, several research efforts, across multiple communities, have explored how to scale up recursive queries, typically expressed in Datalog. Our experience with these tools indicated that their performance does not translate across domains (e.g., a tool design for large-scale graph analytics does not exhibit the same performance on program-analysis tasks, and vice versa). As a result, we designed and implemented a general-purpose Datalog engine, called RecStep, on top of a parallel single-node relational system. In this paper, we outline the different techniques we use in RecStep, and the contribution of each technique to overall performance. We also present results from a detailed set of experiments comparing RecStep with a number of other Datalog systems using both graph analytics and program-analysis tasks, summarizing pros and cons of existing techniques based on the analysis of our observations. We show that RecStep generally outperforms the state-of-the-art parallel Datalog engines on complex and large-scale Datalog program evaluation, by a 4-6X margin. An additional insight from our work is that we show that it is possible to build a high-performance Datalog system on top of a relational engine, an idea that has been dismissed in past work in this area.", "num_citations": "7\n", "authors": ["705"]}
{"title": "Backdoors in neural models of source code\n", "abstract": " Deep neural networks are vulnerable to a range of adversaries. A particularly pernicious class of vulnerabilities are backdoors, where model predictions diverge in the presence of subtle triggers in inputs. An attacker can implant a backdoor by poisoning the training data to yield a desired target prediction on triggered inputs. We study backdoors in the context of deep-learning for source code. (1) We define a range of backdoor classes for source-code tasks and show how to poison a dataset to install such backdoors. (2) We adapt and improve recent algorithms from robust statistics for our setting, showing that backdoors leave a spectral signature in the learned representation of source code, thus enabling detection of poisoned data. (3) We conduct a thorough evaluation on different architectures and languages, showing the ease of injecting backdoors and our ability to eliminate them.", "num_citations": "5\n", "authors": ["705"]}
{"title": "A static analysis-based cross-architecture performance prediction using machine learning\n", "abstract": " Porting code from CPU to GPU is costly and time-consuming; Unless much time is invested in development and optimization, it is not obvious, a priori, how much speed-up is achievable or how much room is left for improvement. Knowing the potential speed-up a priori can be very useful: It can save hundreds of engineering hours, help programmers with prioritization and algorithm selection. We aim to address this problem using machine learning in a supervised setting, using solely the single-threaded source code of the program, without having to run or profile the code. We propose a static analysis-based cross-architecture performance prediction framework (Static XAPP) which relies solely on program properties collected using static analysis of the CPU source code and predicts whether the potential speed-up is above or below a given threshold. We offer preliminary results that show we can achieve 94% accuracy in binary classification, in average, across different thresholds", "num_citations": "5\n", "authors": ["705"]}
{"title": "Computational tools for human-robot interaction design\n", "abstract": " Robots must exercise socially appropriate behavior when interacting with humans. How can we assist interaction designers to embed socially appropriate and avoid socially inappropriate behavior within human-robot interactions? We propose a multi-faceted interaction-design approach that intersects human-robot interaction and formal methods to help us achieve this goal. At the lowest level, designers create interactions from scratch and receive feedback from formal verification, while higher levels involve automated synthesis and repair of designs. In this extended abstract, we discuss past, present, and future work within each level of our design approach.", "num_citations": "5\n", "authors": ["705"]}
{"title": "Automated tuning of query degree of parallelism via machine learning\n", "abstract": " Determining the degree of parallelism (DOP) for query execution is of great importance to both performance and resource provisioning. However, recent work that applies machine learning (ML) to query optimization and query performance prediction in relational database management systems (RDBMSs) has ignored the effect of intra-query parallelism. In this work, we argue that determining the optimal or near-optimal DOP for query execution is a fundamental and challenging task that benefits both query performance and cost-benefit tradeoffs. We then present promising preliminary results on how ML techniques can be applied to automate DOP tuning. We conclude with a list of challenges we encountered, as well as future directions for our work.", "num_citations": "4\n", "authors": ["705"]}
{"title": "Transforming robot programs based on social context\n", "abstract": " Social robots have varied effectiveness when interacting with humans in different interaction contexts. A robot programmed to escort individuals to a different location, for instance, may behave more appropriately in a crowded airport than a quiet library, or vice versa. To address these issues, we exploit ideas from program synthesis and propose an approach to transforming the structure of hand-crafted interaction programs that uses user-scored execution traces as input, in which end users score their paths through the interaction based on their experience. Additionally, our approach guarantees that transformations to a program will not violate task and social expectations that must be maintained across contexts. We evaluated our approach by adapting a robot program to both real-world and simulated contexts and found evidence that making informed edits to the robot's program improves user experience.", "num_citations": "4\n", "authors": ["705"]}
{"title": "Interval Universal Approximation for Neural Networks\n", "abstract": " To verify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using the interval abstract domain. In this paper, we study the theoretical power and limits of the interval domain for neural-network verification. First, we introduce the interval universal approximation (IUA) theorem. IUA shows that neural networks not only can approximate any continuous function (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrarily close approximation of the set semantics of (the result of applying  to a set of inputs). We call this notion of approximation interval approximation. Our theorem generalizes the recent result of Baader et al.(2020) from ReLUs to a rich class of activation functions that we call squashable functions. Additionally, the IUA theorem implies that we\u00a0\u2026", "num_citations": "3\n", "authors": ["705"]}
{"title": "Probabilistic Horn Clause Verification\n", "abstract": " Constrained Horn clauses have proven to be a natural intermediate language for logically characterizing program semantics and reasoning about program behavior. In this paper, we present probabilistically constrained Horn clauses (pchc), which incorporate probabilistic variables inside otherwise traditional constrained Horn clauses. pchc enable reasoning about probabilistic programs by encoding them as Horn clauses. Encoding probabilistic program semantics as pchc allows us to seamlessly handle procedure calls and recursion, as well as angelic and demonic forms of nondeterminism. We formalize pchc semantics and present a verification algorithm that can prove probabilistic safety properties of programs. We present an implementation and evaluation of our approach on a number of probabilistic programs and properties.", "num_citations": "3\n", "authors": ["705"]}
{"title": "Introduction to neural network verification\n", "abstract": " Deep learning has transformed the way we think of software and what it can do. But deep neural networks are fragile and their behaviors are often surprising. In many settings, we need to provide formal guarantees on the safety, security, correctness, or robustness of neural networks. This book covers foundational ideas from formal verification and their adaptation to reasoning about neural networks and deep learning.", "num_citations": "2\n", "authors": ["705"]}
{"title": "Constraint-Based Synthesis of Coupling Proofs\n", "abstract": " Proof by coupling is a classical technique for proving properties about pairs of randomized algorithms by carefully relating (or coupling) two probabilistic executions. In this paper, we show how to automatically construct such proofs for probabilistic programs. First, we present f-coupled postconditions, an abstraction describing two correlated program executions. Second, we show how properties of f-coupled postconditions can imply various probabilistic properties of the original programs. Third, we demonstrate how to reduce the proof-search problem to a purely logical synthesis problem of the form, making probabilistic reasoning unnecessary. We develop a prototype implementation to automatically build coupling proofs for probabilistic properties, including uniformity and independence of program expressions.", "num_citations": "2\n", "authors": ["705"]}
{"title": "Software verification with program-graph interpolation and abstraction\n", "abstract": " Picture a world where you can ask questions about a piece of code and have tools that automatically and efficiently answer them for you. Can a division by zero ever occur? Are all elements in this list always greater than ten? Does this program always terminate? Are there any race conditions? In such a world, software is considerably more reliable and secure than what we currently have; software has fewer bugs and is easily certifiable like other engineering artifacts; software-induced disasters are effortlessly avoided; and the billions of dollars that are normally spent on testing and maintenance activities are instead poured into more productive endeavours. Alas, such a world is an imaginary utopia, as the majority of these verification questions translate into undecidable problems (due to their relation to the halting problem). Nevertheless, we can still try to design algorithms that answer some questions about\u00a0\u2026", "num_citations": "2\n", "authors": ["705"]}
{"title": "Learning Differentially Private Mechanisms\n", "abstract": " Differential privacy is a formal, mathematical definition of data privacy that has gained traction in academia, industry, and government. The task of correctly constructing differentially private algorithms is non-trivial, and mistakes have been made in foundational algorithms. Currently, there is no automated support for converting an existing, non-private program into a differentially private version. In this paper, we propose a technique for automatically learning an accurate and differentially private version of a given non-private program. We show how to solve this difficult program synthesis problem via a combination of techniques: carefully picking representative example inputs, reducing the problem to continuous optimization, and mapping the results back to symbolic expressions. We demonstrate that our approach is able to learn foundational algorithms from the differential privacy literature and significantly outperforms natural program synthesis baselines.", "num_citations": "1\n", "authors": ["705"]}
{"title": "Liveness Verification of Stateful Networks\n", "abstract": " Network verification tools focus almost exclusively on various \u201creachability\u201d properties, eg, is there a path from host A to host B? Thus, they are inapplicable to providing strong correctness guarantees for modern programmable networks that increasingly rely on stateful network functions. Correct operations of such networks depend on the validity of a larger set of properties, in particular liveness properties. For instance, a stateful firewall that only allows solicited external traffic works correctly if it eventually detects and blocks malicious connections, eg, if it eventually blocks an external host B that tries to reach the internal host A before receiving a request from A.Alas, verifying liveness properties is computationally expensive and in some cases undecidable. Existing verification techniques do not scale to verify such properties. In this work, we provide a compositional programming abstraction that decouples\u00a0\u2026", "num_citations": "1\n", "authors": ["705"]}