{"title": "Efficient path profiling\n", "abstract": " A path profile determines how many times each acyclic path in a routine executes. This type of profiling subsumes the more common basic block and edge profiling, which only approximate path frequencies. Path profiles have many potential uses in program performance tuning, profile-directed compilation, and software test coverage. This paper describes a new algorithm for path profiling. This simple, fast algorithm selects and places profile instrumentation to minimize run-time overhead. Instrumented programs run with overhead comparable to the best previous profiling techniques. On the SPEC95 benchmarks, path profiling overhead averaged 31%, as compared to 16% for efficient edge profiling. Path profiling also identifies longer paths than a previous technique, which predicted paths from edge profiles (average of 88, versus 34 instructions). Moreover, profiling shows that the SPEC95 train input datasets\u00a0\u2026", "num_citations": "922\n", "authors": ["253"]}
{"title": "Optimally profiling and tracing programs\n", "abstract": " This paper describes algorithms for inserting monitoring code to profile and trace programs. These algorithms greatly reduce the cost of measuring programs with respect to the commonly used technique of placing code in each basic block. Program profiling counts the number of times each basic block in a program executes. Instruction tracing records the sequence of basic blocks traversed in a program execution. The algorithms optimize the placement of counting/tracing code with respect to the expected or measured frequency of each block or edge in a program's control-flow graph. We have implemented the algorithms in a profiling/tracing tool, and they substantially reduce the overhead of profiling and tracing.We also define and study the hierarchy of profiling problems. These  problems have two dimensions: what is profiled (i.e., vertices (basic blocks) or edges in a control-flow graph) and where the\u00a0\u2026", "num_citations": "763\n", "authors": ["253"]}
{"title": "Exploiting hardware performance counters with flow and context sensitive profiling\n", "abstract": " A program profile attributes run-time costs to portions of a program's execution. Most profiling systems suffer from two major deficiencies: first, they only apportion simple metrics, such as execution frequency or elapsed time to static, syntactic units, such as procedures or statements; second, they aggressively reduce the volume of information collected and reported, although aggregation can hide striking differences in program behavior.This paper addresses both concerns by exploiting the hardware counters available in most modern processors and by incorporating two concepts from data flow analysis--flow and context sensitivity--to report more context for measurements. This paper extends our previous work on efficient path profiling to flow sensitive profiling, which associates hardware performance metrics with a path through a procedure. In addition, it describes a data structure, the calling context tree, that\u00a0\u2026", "num_citations": "645\n", "authors": ["253"]}
{"title": "Software visualization in the large\n", "abstract": " The invisible nature of software hides system complexity, particularly for large team-oriented projects. The authors have evolved four innovative visual representations of code to help solve this problem: line representation; pixel representation; file summary representation; and hierarchical representation. We first describe these four visual code representations and then discuss the interaction techniques for manipulating them. We illustrate our software visualization techniques through five case studies. The first three focus on software history and static software characteristics; the last two discuss execution behavior. The software library and its implementation are then described. Finally, we briefly review some related work and compare and contrast our different techniques for visualizing software.", "num_citations": "615\n", "authors": ["253"]}
{"title": "Method and apparatus for tracking and viewing changes on the web\n", "abstract": " A system for accessing documents contained in a remote repository, which change in content from version-to-version. The system allows users to specify lists of documents of interest. Based on the lists, the system maintains an archive, which contains a copy of one version of each listed document, and material from which the other versions can be reconstructed. The system periodically compares the archive with current versions of the documents located in the repository, and updates the archive, thereby maintaining the ability to reconstruct current versions. The system also monitors access to the versions by each user. When a user calls for a current version, the system presents the current version, and indicates what parts of the current version have not been previously accessed by the user.", "num_citations": "464\n", "authors": ["253"]}
{"title": "Branch prediction for free\n", "abstract": " Many compilers rely on branch prediction to improve program performance by identifying frequently executed regions and by aiding in scheduling instructions.Profile-based predictors require a time-consuming and inconvenient compile-profile-compile cycle in order to make predictions. We present a program-based branch predictor that performs well for a large and diverse set of programs written in C and Fortran. In addition to using natural loop analysis to predict branches that control the iteration of loops, we focus on heuristics for predicting non-loop branches, which dominate the dynamic branch count of many programs. The heuristics are simple and require little program analysis, yet they are effective in terms of coverage and miss rate. Although program-based prediction does not equal the accuracy of profile-based prediction, we believe it reaches a sufficiently high level to be useful. Additional type and\u00a0\u2026", "num_citations": "437\n", "authors": ["253"]}
{"title": "The concept of dynamic analysis\n", "abstract": " Dynamic analysis is the analysis of the properties of a running program. In this paper, we explore two new dynamic analyses based on program profiling:  \u2014 Frequency Spectrum Analysis. We show how analyzing the frequencies of program entities in a single execution can help programmers to decompose a program, identify related computations, and find computations related to specific input and output characteristics of a program.   \u2014 Coverage Concept Analysis. Concept analysis of test coverage data computes dynamic analogs to static control flow relationships such as domination, postdomination, and regions. Comparison of these dynamically computed relationships to their static counterparts can point to areas of code requiring more testing and can aid programmers in understanding how a program and its test sets relate to one another.", "num_citations": "411\n", "authors": ["253"]}
{"title": "User interface for translating natural language inquiries into database queries and data presentations\n", "abstract": " A natural language-based interface data presentation system interfaces, for example, information visualization system interfaces, is realized by employing so-called open-ended natural language inquiries to the interface that translates them into database queries and a set of information to be provided to a user. More specifically, a natural language inquiry is translated to database queries by determining if any complete database queries can be formulated based on the natural language inquiry and, if so, specifying which complete database queries are to be made. In accordance with one aspect of the invention, knowledge of the information visualization presentation is advantageously employed in the interface to guide a user in response to the user's natural language inquiries. In accordance with another aspect of the invention, knowledge of the database and knowledge of the information visualization\u00a0\u2026", "num_citations": "348\n", "authors": ["253"]}
{"title": "Method of providing transfer capability on web-based interactive voice response services\n", "abstract": " Interactive voice response (IVR) services are provided to an end user at a telephone terminal (201) connected to the PSTN (202) through a telephone/IP server (205) that serves as an interface between the PSTN and an IP network (204) such as the Internet. A first IVR service is provided by a web server (203) running a service logic (207) for that service, which produces pages formatted in a phone markup language (PML) in response to an HTTP request sent over the IP network by the telephone/IP server to the web server at the URL address associated with the service. Hyperlinks to a second IVR service offered on a web server (208) at a different URL address are embedded and associated with a specific question or statement in a PML-formatted page produced by the first service. When the end user affirmatively responds to that statement or question through a verbal or touch-tone input, the telephone/IP server\u00a0\u2026", "num_citations": "332\n", "authors": ["253"]}
{"title": "Rewriting executable files to measure program behavior\n", "abstract": " Inserting instrumentation code in a program is an effective technique for detecting, recording, and measuring many aspects of a program's performance. Instrumentation code can be added at any stage of the compilation process by specially\u2010modified system tools such as a compiler or linker or by new tools from a measurement system. For several reasons, adding instrumentation code after the compilation process\u2014by rewriting the executable file\u2014presents fewer complications and leads to more complete measurements. This paper describes the difficulties in adding code to executable files that arose in developing the profiling and tracing tools qp and qpt. The techniques used by these tools to instrument programs on MIPS and SPARC processors are applicable in other instrumentation systems running on many processors and operating systems. In addition, many difficulties could have been avoided with minor\u00a0\u2026", "num_citations": "278\n", "authors": ["253"]}
{"title": "Predicate-based test coverage and generation\n", "abstract": " Techniques and tools for achieving improved test coverage in a finite program state space are described, such as a technique for selecting a set of predicates, calculating a set of possible predicate values, calculating a subset of the set of possible predicate values, and generating a test for the computer program based at least in part on the subset. The subset comprises an approximation (eg, an under-approximation) of reachable states in the program. A superset of the set of possible predicate values also can be calculated; the superset comprises an over-approximation of the reachable states in the program. In another aspect, a Boolean abstraction of a program is generated, reachability analysis is performed based at least in part on the Boolean abstraction, and symbolic execution is performed to generate test data. The reachability analysis can include computing lower and/or upper bounds of reachable\u00a0\u2026", "num_citations": "277\n", "authors": ["253"]}
{"title": "Visualizing interactions in program executions\n", "abstract": " Implementing, validating, modifying, or reengineering an object-oriented system requires an understanding of the object and class interactions which occur as a program executes. This work seeks to identify, visualize, and analyze interactions in object-oriented program executions as a means for examining and understanding dynamic behavior. We have discovered recurring interaction scenarios in program executions that can be used as abstractions in the understanding process, and have developed a means for identifying these interaction patterns. Our visualizations focus on supporting design recovery, validation, and reengineering tasks, and can be applied to both object-oriented and procedural programs.", "num_citations": "263\n", "authors": ["253"]}
{"title": "The AT&T Internet Difference Engine: Tracking and viewing changes on the web\n", "abstract": " The AT&T Internet Difference Engine (AIDE) is a system that finds and displays changes to pages on the World Wide Web. The system consists of several components, including a web\u2010crawler that detects changes, an archive of past versions of pages, a tool called HtmlDiff to highlight changes between versions of a page, and a graphical interface to view the relationship between pages over time. This paper describes AIDE, with an emphasis on the evolution of the system and experiences with it. It also raises some sociological and legal issues.", "num_citations": "225\n", "authors": ["253"]}
{"title": "Slicing programs with arbitrary control-flow\n", "abstract": " Program slicing is a program transformation that is useful in program debugging, program maintenance, and other applications that involve understanding program behavior. Given a program point p and a set of variables V, the goal of slicing is to create a projection of the program (by eliminating some statements), such that the projection and the original program compute the same values for all variables in V at point p.             This paper addresses the problem of slicing programs with arbitrary controlflow. Previous slicing algorithms do not always form semantically correct program projections when applied to such programs. We give the first algorithm for slicing programs with arbitrary control-flow and a proof of its correctness. Our algorithm works for programs with completely arbitrary control-flow, including irreducible control-flow.", "num_citations": "214\n", "authors": ["253"]}
{"title": "Identifying changes in on-line data repositories\n", "abstract": " A system for accessing documents contained in a remote repository, which change in content from version-to-version. The system allows users to specify lists of documents of interest. Based on the lists, the system maintains an archive, which contains a copy of one version of each listed document, and material from which the other versions can be reconstructed. The system periodically compares the archive with current versions of the documents located in the repository, and updates the archive, thereby maintaining the ability to reconstruct current versions. The system also monitors access to the versions by each user. When a user calls for a current version, the system presents the current version, and indicates what parts of the current version have not been previously accessed by the user.", "num_citations": "212\n", "authors": ["253"]}
{"title": "Boolean programs: A model and process for software analysis\n", "abstract": " CiNii \u8ad6\u6587 - Boolean programs : A model and process for software analysis CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f \u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e \u518d\u958b\u306b\u3064\u3044\u3066 Boolean programs : A model and process for software analysis BALL T. \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 BALL T. \u53ce\u9332\u520a\u884c\u7269 http://research.microsoft.com/slam http://research.microsoft.com/slam Microsoft Research \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 Synchronization Verification in System-Level Design with ILP Solvers SAKUNKONCHAK Thanyapat , KOMATSU Satoshi , FUJITA Masahiro IEICE transactions on fundamentals of electronics, communications and computer sciences 89(12), 3387-3396, 2006-12-01 \u53c2\u8003\u6587\u732e30\u4ef6 \u88ab\u5f15\u75281(\u2026", "num_citations": "174\n", "authors": ["253"]}
{"title": "Refined criteria for gradual typing\n", "abstract": " Siek and Taha [2006] coined the term gradual typing to describe a theory for integrating static and dynamic typing within a single language that 1) puts the programmer in control of which regions of code are statically or dynamically typed and 2) enables the gradual evolution of code between the two typing disciplines. Since 2006, the term gradual typing has become quite popular but its meaning has become diluted to encompass anything related to the integration of static and dynamic typing. This dilution is partly the fault of the original paper, which provided an incomplete formal characterization of what it means to be gradually typed. In this paper we draw a crisp line in the sand that includes a new formal property, named the gradual guarantee, that relates the behavior of programs that differ only with respect to their type annotations. We argue that the gradual guarantee provides important guidance for designers of gradually typed languages. We survey the gradual typing literature, critiquing designs in light of the gradual guarantee. We also report on a mechanized proof that the gradual guarantee holds for the Gradually Typed Lambda Calculus.", "num_citations": "163\n", "authors": ["253"]}
{"title": "On the limit of control flow analysis for regression test selection\n", "abstract": " Automated analyses for regression test selection (RTS) attempt to determine if a modified program, when run on a test t, will have the same behavior as an old version of the program run on t, but without running the new program on t. RTS analyses must confront a price/performance tradeoff: a more precise analysis might be able to eliminate more tests, but could take much longer to run. We focus on the application of control flow analysis and control flow coverage, relatively inexpensive analyses, to the RTS problem, considering how the precision of RTS algorithms can be affected by the type of coverage information collected. We define a strong optimality condition (edge-optimality) for RTS algorithms based on edge coverage that precisely captures when such an algorithm will report that re-testing is needed, when, in actuality, it is not. We reformulate Rothermel and Harrold's RTS algorithm and present three new\u00a0\u2026", "num_citations": "153\n", "authors": ["253"]}
{"title": "A theory of predicate-complete test coverage and generation\n", "abstract": " Consider a program with m statements and n predicates, where the predicates are derived from the conditional statements and assertions in a program. An observable state is an evaluation of the n predicates under some state at a program statement. The goal of predicate-complete testing (PCT) is to evaluate all the predicates at every program state. That is, we wish to cover every reachable observable state (at most m \u00d7 2                 n                of them) in a program. PCT coverage subsumes many existing control-flow coverage criteria and is incomparable to path coverage. To support the generation of tests to achieve high PCT coverage, we show how to define an upper bound U and lower bound L to the (unknown) set of reachable observable states R. These bounds are constructed automatically using Boolean (predicate) abstraction over modal transition systems and can be used to guide test generation\u00a0\u2026", "num_citations": "148\n", "authors": ["253"]}
{"title": "SLAM2: Static driver verification with under 4% false alarms\n", "abstract": " In theory, counterexample-guided abstraction refinement (CEGAR) uses spurious counterexamples to refine overapproximations so as to eliminate provably false alarms. In practice, CEGAR can report false alarms because: (1) the underlying problem CEGAR is trying to solve is undecidable; (2) approximations introduced for optimization purposes may cause CEGAR to be unable to eliminate a false alarm; (3) CEGAR has no termination guarantee - if it runs out of time or memory then the last counterexample generated is provably a false alarm. We report on advances in the SLAM analysis engine, which implements CEGAR for C programs using predicate abstraction, that greatly reduce the false alarm rate. SLAM is used by the Static Driver Verifier (SDV) tool. Compared to the first version of SLAM (SLAM1, shipped in SDV 1.6), the improved version (SLAM2, shipped in SDV 2.0) reduces the percentage of false\u00a0\u2026", "num_citations": "114\n", "authors": ["253"]}
{"title": "Modular and verified automatic program repair\n", "abstract": " We study the problem of suggesting code repairs at design time, based on the warnings issued by modular program verifiers. We introduce the concept of a verified repair, a change to a program's source that removes bad execution traces while increasing the number of good traces, where the bad/good traces form a partition of all the traces of a program. Repairs are property-specific. We demonstrate our framework in the context of warnings produced by the modular cccheck (a.k.a. Clousot) abstract interpreter, and generate repairs for missing contracts, incorrect locals and objects initialization, wrong conditionals, buffer overruns, arithmetic overflow and incorrect floating point comparisons. We report our experience with automatically generating repairs for the .NET framework libraries, generating verified repairs for over 80% of the warnings generated by cccheck.", "num_citations": "105\n", "authors": ["253"]}
{"title": "Using version control data to evaluate the impact of software tools: A case study of the version editor\n", "abstract": " Software tools can improve the quality and maintainability of software, but are expensive to acquire, deploy, and maintain, especially in large organizations. We explore how to quantify the effects of a software tool once it has been deployed in a development environment. We present an effort-analysis method that derives tool usage statistics and developer actions from a project's change history (version control system) and uses a novel effort estimation algorithm to quantify the effort savings attributable to tool usage. We apply this method to assess the impact of a software tool called VE, a version-sensitive editor used in Bell Labs. VE aids software developers in coping with the rampant use of certain preprocessor directives (similar to #if/#endif in C source files). Our analysis found that developers were approximately 40 percent more productive when using VE than when using standard text editors.", "num_citations": "103\n", "authors": ["253"]}
{"title": "Querying and navigating changes in web repositories\n", "abstract": " A system and method is provided for identifying if a document linked from a base document has changed over time. A first time and a later second time are identified. A link in a base document is recursively identified, where the link refers to a linked document. A version repository is searched to determine if the repository stores versions of the linked document associated with the first and second times. An indication is provided to the user if it is determined that a version associated with the first time and a version associated with the second time are stored in the repository.", "num_citations": "91\n", "authors": ["253"]}
{"title": "What's in a region? or computing control dependence regions in near-linear time for reducible control flow\n", "abstract": " Regions of control dependence identify the instructions in a program that execute under the same control conditions. They have a variety of applications in parallelizing and optimizing compilers. Two vertices in a control-flow graph (which may represent instructions or basic blocks in a program) are in the same region if they have the same set of control dependence predecessors. The common algorithm for computing regions examines each control dependence at least once. As there may be O(V x E) control dependences in the worst case, where V and E are the number of vertices and edges in the control-flow graph, this algorithm has a worst-case running time of O(V x D). We present algorithms for finding regions in reducible control-flow graphs in near-linear time, without using control dependence. These algorithms are based on alternative definitions of regions, which are easier to reason with than the definitions\u00a0\u2026", "num_citations": "91\n", "authors": ["253"]}
{"title": "Visualizing program slices\n", "abstract": " Program slicing is an automatic technique for determining which code in a program is relevant to a particular computation. Slicing has been applied in many areas, including program understanding, debugging, and maintenance. However, little attention has been paid to suitable interfaces for exploring program slices. We present an interface for program slicing that allows slicing at the statement procedure, or file level, and provides fast visual feedback on slice structure, integral to the interface is a global visualization of the program that shows the extent of a slice as it crosses procedure and file boundaries, and facilitates quick browsing of numerous slices.< >", "num_citations": "88\n", "authors": ["253"]}
{"title": "Tracking and Viewing Changes on the Web.\n", "abstract": " We describe a set of tools that detect when World-Wide-Web pages have been modified and present the modifications visually to the user through markedup HTML. The tools consist of three components: w3newer, which detects changes to pages; snapshot, which permits a user to store a copy of an arbitrary Web page and to compare any subsequent version of a page with the saved version; and HtmlDiff, which marks up HTML text to indicate how it has changed from a previous version. We refer to the tools collectively as the", "num_citations": "87\n", "authors": ["253"]}
{"title": "The static driver verifier research platform\n", "abstract": " The Sdv Research Platform (Sdvrp) is a new academic release of Static Driver Verifier (Sdv) and the Slam software model checker that contains: (1) a parameterized version of Sdv that allows one to write custom API rules for APIs independent of device drivers; (2) thousands of Boolean programs generated by Sdv in the course of verifying Windows device drivers, including the functional and performance results (of the Bebop model checker) and test scripts to allow comparison against other Boolean program model checkers; (3) a new version of the Slam analysis engine, called Slam2, that is much more robust and performant.", "num_citations": "75\n", "authors": ["253"]}
{"title": "WebGUIDE: Querying and navigating changes in web repositories\n", "abstract": " WebGUIDE is a system for exploring changes to World Wide Web pages and Web structure that supports recursive document comparison: users may explore the differences between pages with respect to two dates. Differences between pages are computed automatically and summarized in a new HTML page, and differences in link structure are shown via graphical representations. WebGUIDE is the combination of two tools that complement one another: the AT&T Internet Difference Engine (AIDE)[9] is a tool for tracking and viewing modifications to World-Wide-Web pages, which has been extended to support recursive tracking of pages; Ciao[6] is a graphical navigator that allows users to query and browse structural connections embedded in a document repository. The union of these tools let users get information on the evolution of pages of interest (both textually and graphically), browse the differences\u00a0\u2026", "num_citations": "73\n", "authors": ["253"]}
{"title": "The racket manifesto\n", "abstract": " The creation of a programming language calls for guiding principles that point the developers to goals. This article spells out the three basic principles behind the 20-year development of Racket. First, programming is about stating and solving problems, and this activity normally takes place in a context with its own language of discourse; good programmers ought to formulate this language as a programming language. Hence, Racket is a programming language for creating new programming languages. Second, by following this language-oriented approach to programming, systems become multi-lingual collections of interconnected components. Each language and component must be able to protect its specific invariants. In support, Racket offers protection mechanisms to implement a full language spectrum, from C-level bit manipulation to soundly typed extensions. Third, because Racket considers programming as problem solving in the correct language, Racket also turns extra-linguistic mechanisms into linguistic constructs, especially mechanisms for managing resources and projects. The paper explains these principles and how Racket lives up to them, presents the evaluation framework behind the design process, and concludes with a sketch of Racket's imperfections and opportunities for future improvements.", "num_citations": "71\n", "authors": ["253"]}
{"title": "Sisl: Several interfaces, single logic\n", "abstract": " Modern interactive services such as information and e-commerce services are becoming increasingly more flexible in the types of user interfaces they support. These interfaces incorporate automatic speech recognition and natural language understanding and include graphical user interfaces on the desktop and web-based interfaces using applets and HTML forms. To what extent can the user interface software be decoupled from the service logic software (the code that defines the essential function of a service)? Decoupling of user interface from service logic directly impacts the flexibility of services, or how easy they are to modify and extend.               To explore these issues, we have developed Sisl, an architecture and domain-specific language for designing and implementing interactive services with multiple user interfaces. A key principle underlying Sisl is that all user interfaces to a service share the\u00a0\u2026", "num_citations": "70\n", "authors": ["253"]}
{"title": "Two for the price of one: A model for parallel and incremental computation\n", "abstract": " Parallel or incremental versions of an algorithm can significantly outperform their counterparts, but are often difficult to develop. Programming models that provide appropriate abstractions to decompose data and tasks can simplify parallelization. We show in this work that the same abstractions can enable both parallel and incremental execution. We present a novel algorithm for parallel self-adjusting computation. This algorithm extends a deterministic parallel programming model (concurrent revisions) with support for recording and repeating computations. On record, we construct a dynamic dependence graph of the parallel computation. On repeat, we reexecute only parts whose dependencies have changed. We implement and evaluate our idea by studying five example programs, including a realistic multi-pass CSS layout algorithm. We describe programming techniques that proved particularly useful to improve\u00a0\u2026", "num_citations": "63\n", "authors": ["253"]}
{"title": "Using version control data to evaluate the impact of software tools\n", "abstract": " Software tools can improve the quality and maintainability of software, but are expensive to acquire, deploy and maintain, especially in large organizations. We explore how to quantify the effects of a software tool once it has been deployed in a development environment. We present a simple methodology for tool evaluation that correlates tool usage statistics with estimates of developer effort, as derived from a project\u2019s change history (version control system).Our work complements controlled experiments on software tools, which usually take place outside the industrial setting, and tool assessment studies that predict the impact of software tools before deployment. Our analysis is inexpensive, non-intrusive and can be applied to an entire software project in its actual setting. A key part of our analysis is how to control confounding variables such as developer work-style and experience in order accurately to quantify\u00a0\u2026", "num_citations": "61\n", "authors": ["253"]}
{"title": "Using paths to measure, explain, and enhance program behavior\n", "abstract": " What happens when a computer program runs? The answer can be frustratingly elusive, as anyone who has debugged or tuned a program knows. As it runs, a program overwrites its previous state, which might have provided a clue as to how the program got to the point at which it computed the wrong answer or otherwise failed. This all-too-common experience is symptomatic of a more general problem: the difficulty of accurately and efficiently capturing and analyzing the sequence of events that occur when a program executes. Program paths offer an insight into a program's dynamic behavior that is difficult to achieve any other way. Unlike simpler measures such as program profiles, which aggregate information to reduce the cost of collecting or storing data, paths capture some of the usually invisible dynamic sequencing of statements. The article exploits the insight that program statements do not execute in\u00a0\u2026", "num_citations": "60\n", "authors": ["253"]}
{"title": "Everything you want to know about pointer-based checking\n", "abstract": " Lack of memory safety in C/C++ has resulted in numerous security vulnerabilities and serious bugs in large software systems. This paper highlights the challenges in enforcing memory safety for C/C++ programs and progress made as part of the SoftBoundCETS project. We have been exploring memory safety enforcement at various levels-in hardware, in the compiler, and as a hardware-compiler hybrid-in this project. Our research has identified that maintaining metadata with pointers in a disjoint metadata space and performing bounds and use-after-free checking can provide comprehensive memory safety. We describe the rationale behind the design decisions and its ramifications on various dimensions, our experience with the various variants that we explored in this project, and the lessons learned in the process. We also describe and analyze the forthcoming Intel Memory Protection Extensions (MPX) that provides hardware acceleration for disjoint metadata and pointer checking in mainstream hardware, which is expected to be available later this year.", "num_citations": "59\n", "authors": ["253"]}
{"title": "State generation and automated class testing\n", "abstract": " The maturity of object\u2010oriented methods has led to the wide availability of container classes: classes that encapsulate classical data structures and algorithms. Container classes are included in the C++ and Java standard libraries, and in many proprietary libraries. The wide availability and use of these classes makes reliability important, and testing plays a central role in achieving that reliability. The large number of cases necessary for thorough testing of container classes makes automated testing essential. This paper presents a novel approach for automated testing of container classes based on combinatorial algorithms for state generation. The approach is illustrated with black\u2010box and white\u2010box test drivers for a class implemented with the red\u2013black tree data structure, used widely in industry and, in particular, in the C++ Standard Template Library. The white\u2010box driver is based on a new algorithm for red\u00a0\u2026", "num_citations": "58\n", "authors": ["253"]}
{"title": "Microsoft touch develop and the bbc micro: bit\n", "abstract": " The chance to influence the lives of a million children does not come often. Through a partnership between the BBC and several technology companies, a small instructional computing device called the BBC micro:bit will be given to a million children in the UK in 2016. Moreover, using the micro:bit will be part of the CS curriculum. We describe how Microsoft's Touch Develop programming platform works with the BBC micro:bit. We describe the design and architecture of the micro:bit and the software engineering hurdles that had to be overcome to ensure it was as accessible as possible to children and teachers. The combined hardware/software platform is evaluated and early anecdotal evidence is presented.", "num_citations": "41\n", "authors": ["253"]}
{"title": "Verified compilers for a multi-language world\n", "abstract": " Though there has been remarkable progress on formally verified compilers in recent years, most of these compilers suffer from a serious limitation: they are proved correct under the assumption that they will only be used to compile whole programs. This is an unrealistic assumption since most software systems today are comprised of components written in different languages-both typed and untyped-compiled by different compilers to a common target, as well as low-level libraries that may be handwritten in the target language. We are pursuing a new methodology for building verified compilers for today's world of multi-language software. The project has two central themes, both of which stem from a view of compiler correctness as a language interoperability problem. First, to specify correctness of component compilation, we require that if a source component s compiles to target component t, then t linked with some arbitrary target code t'should behave the same as s interoperating with t'. The latter demands a formal semantics of interoperability between the source and target languages. Second, to enable safe interoperability between components compiled from languages as different as ML, Rust, Python, and C, we plan to design a gradually type-safe target language based on LLVM that supports safe interoperability between more precisely typed, less precisely typed, and type-unsafe components. Our approach opens up a new avenue for exploring sensible language interoperability while also tackling compiler correctness.", "num_citations": "41\n", "authors": ["253"]}
{"title": "An internet difference engine and its applications\n", "abstract": " Finding interesting information on the the Internet is difficult. Keeping up-to-date on this information requires a mechanism to determine when it has changed and how it has changed. The AT&T Internet Difference Engine (AIDE) is a system that detects and displays changes to pages on the World Wide Web. A web tracking tool finds when pages have changed. A new tool called HtmlDiff displays those changes by determining the differences between two HTML pages and creating a new HTML page summarizing them. We describe the major components of the Internet Difference Engine, and then show how it has been applied to a variety of Web applications, including collaborative editing environments and automated \"What's New?\" reports.", "num_citations": "37\n", "authors": ["253"]}
{"title": "Visualizing message patterns in object-oriented program executions\n", "abstract": " The dynamic behavior of object-oriented programs is difficult to design, implement, and modify. Understanding the interactions between classes and objects is necessary to create efficient designs and make safe modifications. This work seeks to identify, visualize, and analyze recurring message patterns in object-oriented program executions as a means for understanding and examining dynamic behavior. Our visualizations focus on supporting design recovery, validation, and reengineering tasks.", "num_citations": "36\n", "authors": ["253"]}
{"title": "Efficiently counting program events with support for on-line queries\n", "abstract": " The ability to count events in a program's execution is required by many program analysis applications. We represent an instrumentation method for efficiently counting events in a program's execution, with support for on-line queries of the event count. Event counting differs from basic block profiling in that an aggregate count of events is kept rather than a set of counters. Due to this difference, solutions to basic block profiling are not well suited to event counting. Our algorithm finds a subset of points in a program to instrument, while guaranteeing that accurate event counts can be obtained efficiently at every point in the execution.", "num_citations": "36\n", "authors": ["253"]}
{"title": "Abstraction-guided test generation: A case study\n", "abstract": " We define an automated behavioral approach to unit test generation for C code based on three steps:(1) predicate abstraction of the C code generates a boolean abstraction based on a set of observations (predicates);(2) reachability analysis of the boolean abstraction computes an overapproximation to the set of observable states and generates a small set of paths to cover these states;(3) SAT-based symbolic execution of the C code generates tests to cover the paths. Our approach generalizes a variety of test generation approaches based on covering code and deals naturally with the difficult issue of infeasible program paths that plagues many code-based test generation strategies. We explain our approach via a case study of generating test data for a small program and discuss its capabilities and limitations.", "num_citations": "35\n", "authors": ["253"]}
{"title": "Deconstructing dynamic symbolic execution\n", "abstract": " Dynamic symbolic execution (DSE) is a well-known technique for automatically generating tests to achieve higher levels of coverage in a program. Two keys ideas of DSE are to:(1) seed symbolic execution by executing a program on an initial input;(2) use concrete values from the program execution in place of symbolic expressions whenever symbolic reasoning is hard or not desired. We describe DSE for a simple core language and then present a minimalist implementation of DSE for Python (in Python) that follows this basic recipe. The code is available at https://www. github. com/thomasjball/PyExZ3/(tagged \u201cv1. 0\u201d) and has been designed to make it easy to experiment with and extend.", "num_citations": "34\n", "authors": ["253"]}
{"title": "Program analysis through predicate abstraction and refinement\n", "abstract": " An analysis engine is described for performing static analysis using CEGAR loop functionality, using a combination of forward and backward validation-phase trace analyses. The analysis engine includes a number of features. For example:(1) the analysis engine can operate on blocks of program statements of different adjustable sizes;(2) the analysis engine can identify a subtrace of the trace and perform analysis on that subtrace (rather than the full trace);(3) the analysis engine can form a pyramid of state conditions and extract predicates based on the pyramid and/or from auxiliary source (s);(4) the analysis engine can generate predicates using an increasingly-aggressive series of available discovery techniques;(5) the analysis engine can selectively concretize procedure calls associated with the trace on an as-needed basis and perform other refinements; and (6) the analysis engine can add additional\u00a0\u2026", "num_citations": "32\n", "authors": ["253"]}
{"title": "Method and apparatus for providing interactive services with multiple interfaces\n", "abstract": " Interactive services are provided by employing a modular approach to implementing interactive services with multiple interfaces. Such an approach facilitates supporting natural language understanding interaction with users through use of interfaces that allow at least different ordering of inputs, and/or incomplete information, and/or correction of information, and/or the return of control to prior points in the service. This is realized, in an embodiment of the invention, by employing a single interactive service logic that uses \u201creactive constraint graphs\u201d, ie, a form of event-driven graph, in which nodes contain a set of constraints on events. Specifically, control progresses from a node to a derivative node, ie,\u201cchild\u201d, only when all the constraints in the set on the node have been satisfied. A single set of constraints implicitly supports a significant number of the possible different orderings of inputs. Incomplete information is\u00a0\u2026", "num_citations": "32\n", "authors": ["253"]}
{"title": "Apparatus for visualizing program slices\n", "abstract": " Apparatus for visualizing slices of transitive closures of entities having dependence relationships with one another. A preferred embodiment visualizes slices of programs. A display in a computer system includes reduced representations of the files, procedures, and lines making up a program. The user employs a pointing device to interactively select a reduced representation as the slice point and the apparatus computes the slice and changes the color of the reduced representations in the slice. The color of the reduced representation indicates the distance of the entity represented by the reduced representation from the slice point. The display may be rearranged so that files and procedures are ordered by distance from the slice point. Other aspects of the display include scaling the size of the reduced representation of a procedure to indicate the number of lines in the procedure and filling the reduced\u00a0\u2026", "num_citations": "30\n", "authors": ["253"]}
{"title": "Programs follow paths\n", "abstract": " Program paths\u2014sequences of executed basic blocks\u2014have proven to be an effective way to capture a program\u2019s elusive dynamic behavior. This paper shows how paths and path spectra compactly and precisely record many aspects of programs\u2019 execution-time control flow behavior and explores applications of these paths in computer architecture, compilers, debugging, program testing, and software maintenance.", "num_citations": "28\n", "authors": ["253"]}
{"title": "The use of control flow and control dependence in software tools\n", "abstract": " Program development, debugging, and maintenance can be greatly improved by the use of software tools that provide information about program behavior. This thesis focuses on a number of useful software tools and shows how their ef\ufb01ciency, generality, and precision can be increased through the use of control\u2014\ufb02ow and control dependence analysis. We consider two classes of tools: execution measurement tools, which collect information about a particular pro-gram execution; and program analysis tools, which provide information about potential program behavior by statically analyzing the program.We consider three tools that measure aspects of a program\u2019s execution: pro\ufb01ling, tracing, and event counting tools. We describe algorithms for pro\ufb01ling and tracing programs that use a com-bination of control\u2014\ufb02ow analysis and program instrumentation to produce exact pro\ufb01les and traces with low run-time overhead\u00a0\u2026", "num_citations": "27\n", "authors": ["253"]}
{"title": "ARcadia: A rapid prototyping platform for real-time tangible interfaces\n", "abstract": " Paper-based fabrication techniques offer powerful opportunities to prototype new technological interfaces. Typically, paper-based interfaces are either static mockups or require integration with sensors to provide real-time interactivity. The latter can be challenging and expensive, requiring knowledge of electronics, programming, and sensing. But what if computer vision could be combined with prototyping domain-aware programming tools to support the rapid construction of interactive, paper-based tangible interfaces? We designed a toolkit called ARcadia that allows for rapid, low-cost prototyping of TUIs that only requires access to a webcam, a web browser, and paper. ARcadia brings paper prototypes to life through the use of marker based augmented reality (AR). Users create mappings between real-world tangible objects and different UI elements. After a crafting and programming phase, all subsequent\u00a0\u2026", "num_citations": "24\n", "authors": ["253"]}
{"title": "Formalizing counterexample-driven refinement with weakest preconditions\n", "abstract": " To check a safety property of a program, it is sufficient to check the property on an abstraction that has more behaviors than the original program. If the safety property holds of the abstraction then it also holds of the original program.               However, if the property does not hold of the abstraction along some trace t (a counterexample), it may or may not hold of the original program on trace t. If it can be proved that the property does not hold in the original program on trace t then it makes sense to refine the abstraction to eliminate the \u201cspurious counterexample\u201d t (rather than a report a known false negative to the user).               The SLAM tool developed at Microsoft Research implements such an automated abstraction-refinement process. In this paper, we reformulate this process for a tiny while language using the concepts of weakest preconditions, bounded model checking and Craig interpolants. This\u00a0\u2026", "num_citations": "24\n", "authors": ["253"]}
{"title": "Method and apparatus for providing interactive services with multiple interfaces\n", "abstract": " Interactive services with multiple interfaces are realized by employing a modular approach to their implementation. Such an approach facilitates supporting natural language understanding interaction with users through use of interfaces that at least allow the user to provide information beyond what is currently being requested by the service, and/or different ordering of inputs, and/or incomplete information, and/or correction of information, and/or the return of control to prior points in the service. This is realized, in an embodiment of the invention, by employing an interactive service logic that uses \u201creactive constraint graphs\u201d, ie, a form of event-driven graph in which nodes contain constraints on events, in conjunction with a service monitor. The service monitor manages the communication between the service logic and the multiple user interfaces. As such it provides a communication mechanism in the form of a so\u00a0\u2026", "num_citations": "24\n", "authors": ["253"]}
{"title": "Storm Watch: A Tool for Visualizing Memory System Protocols\n", "abstract": " Recent research has offered programmers increased options for programming parallel computers by exposing system policies (e.g., memory coherence protocols) or by providing several programming paradigms (e.g. message passing and shared memory) on the same platform. Increased flexibility can lead to higher performance, but it is also a double-edged sword that demands a programmer understand his or her application and system at a more fundamental level. Our system, Tempest, allows a programmer to select or implement communication and memory coherence policies that fit an application's communication patterns. With it, we have achieved substantial performance gains without making major changes in programs. However, the process of selecting, designing, and implementing coherence protocols is difficult and time consuming, without tools to supply detailed information about an application's\u00a0\u2026", "num_citations": "24\n", "authors": ["253"]}
{"title": "Efficient modular SAT solving for IC3\n", "abstract": " We describe an efficient way to compose SAT solvers into chains, while still allowing unit propagation between those solvers. We show how such a \u201cSAT Modulo SAT\u201d solver naturally produces sequence interpolants as a side effect - there is no need to generate a resolution proof and post-process it to extract an interpolant. We have implemented a version of IC3 using this SAT Modulo SAT solver, which solves both more SAT instances and more UNSAT instances than PDR and IC3 on each of the 2008, 2010, and 2012 Hardware Model Checking Competition benchmarks.", "num_citations": "22\n", "authors": ["253"]}
{"title": "Hardware-software co-design: not just a clich\u00e9\n", "abstract": " The age of the air-tight hardware abstraction is over. As the computing ecosystem moves beyond the predictable yearly advances of Moore's Law, appeals to familiarity and backwards compatibility will become less convincing: fundamental shifts in abstraction and design will look more enticing. It is time to embrace hardware-software co-design in earnest, to cooperate between programming languages and architecture to upend legacy constraints on computing. We describe our work on approximate computing, a new avenue spanning the system stack from applications and languages to microarchitectures. We reflect on the challenges and successes of approximation research and, with these lessons in mind, distill opportunities for future hardware-software co-design efforts.", "num_citations": "21\n", "authors": ["253"]}
{"title": "An automata-theoretic approach to hardware/software co-verification\n", "abstract": " In this paper, we present an automata-theoretic approach to Hardware/ Software (HW/SW) co-verification. We designed a co-specification framework describing HW/SW systems; synthesized a hybrid B\u00fcchi Automaton Pushdown System model for co-verification, namely B\u00fcchi Pushdown System (BPDS), from the co-specification; and built a software tool for deciding reachability of BPDS models. Using our approach, we succeeded in co-verifying the Windows driver and the hardware model of the PIO-24 digital I/O card, finding a previously undiscovered software bug. In addition, our experiments have shown that our co-verification approach performs well in terms of time and memory usages.", "num_citations": "21\n", "authors": ["253"]}
{"title": "Teach foundational language principles\n", "abstract": " Industry is ready and waiting for more graduates educated in the principles of programming languages.", "num_citations": "20\n", "authors": ["253"]}
{"title": "The BBC micro: bit: from the UK to the world\n", "abstract": " A codable computer half the size of a credit card is inspiring students worldwide to develop core computing skills in fun and creative ways.", "num_citations": "18\n", "authors": ["253"]}
{"title": "Programming with\" big code\": Lessons, techniques and applications\n", "abstract": " Programming tools based on probabilistic models of massive codebases (aka\" Big Code\") promise to solve important programming tasks that were difficult or practically infeasible to address before. However, building such tools requires solving a number of hard problems at the intersection of programming languages, program analysis and machine learning. In this paper we summarize some of our experiences and insights obtained by developing several such probabilistic systems over the last few years (some of these systems are regularly used by thousands of developers worldwide). We hope these observations can provide a guideline for others attempting to create such systems. We also present a prediction approach we find suitable as a starting point for building probabilistic tools, and discuss a practical framework implementing this approach, called Nice2Predict. We release the Nice2Predict framework publicly-the framework can be immediately used as a basis for developing new probabilistic tools. Finally, we present programming applications that we believe will benefit from probabilistic models and should be investigated further.", "num_citations": "18\n", "authors": ["253"]}
{"title": "Automatic and systematic detection of race conditions and atomicity violations\n", "abstract": " A library or application is selected comprising one or more functions or methods. An interesting subset of the functions or methods is created. A plurality of multi-threaded test cases are generated from the subset of interesting functions or methods, with each test case comprising a unique pair or triple of functions or methods from the subset. The resulting set of test cases may then be filtered of thread safe test cases using static analysis techniques. The filtered set of test cases is then used as an input to a specialized application that executes each of the multi-threaded test cases to detect atomicity violations and race conditions. The results of the execution of each of the test cases by the specialized application are then aggregated and presented to a user or administrator in a report, for example.", "num_citations": "18\n", "authors": ["253"]}
{"title": "Formalizing hardware/software interface specifications\n", "abstract": " Software drivers are usually developed after hardware devices become available. This dependency can induce a long product cycle. Although co-simulation and co-verification techniques have been utilized to facilitate the driver development, Hardware/Software (HW/SW) interface models, as the test harnesses, are often challenging to specify. Such interface models should have formal semantics, be efficient for testing, and cover all HW/SW behaviors described by HW/SW interface protocols. We present an approach to formalizing HW/SW interface specifications, where we propose a semantic model, relative atomicity, to capture the concurrency model in HW/SW interfaces; demonstrate our approach via a realistic example; elaborate on how we have utilized this approach in device/driver development process; and discuss criteria for evaluating our formal specifications. We have detected fifteen issues in four\u00a0\u2026", "num_citations": "18\n", "authors": ["253"]}
{"title": "Predicate abstraction via symbolic decision procedures\n", "abstract": " Predicate abstraction techniques and tools. Using symbolic decision procedures, predicate abstractions for computer programs are generated based on a set of predicates representing observations of expected behavior of the program. The set of predicates may be generated by an automatic program analysis tool or may be provided a user based on the user's observations. The predicate abstraction process may employ binary decision diagrams. Two or more symbolic decision procedures (eg, for different kinds of program logic) can be combined to form a combined symbolic decision procedure to be used for predicate abstraction. A data structure can be used to track derived predicates during predicate abstraction.", "num_citations": "18\n", "authors": ["253"]}
{"title": "Automatic creation of environment models via training\n", "abstract": " Model checking suffers not only from the state-space explosion problem, but also from the environment modeling problem: how can one create an accurate enough model of the environment to enable precise yet efficient model checking? We present a novel approach to the automatic creation of environment models via training. The idea of training is to take several programs that use a common API and apply model checking to create abstractions of the API procedures. These abstractions then are reused on subsequent verification runs to model-check different programs (which utilize the same API). This approach has been realized in SLAM, a software model checker for C programs, and applied to the domain of Windows device drivers that utilize the Windows Driver Model API (a set of entry points into the Windows kernel). We show how the boolean abstractions of the kernel routines accessed from a\u00a0\u2026", "num_citations": "18\n", "authors": ["253"]}
{"title": "Physical computing: A key element of modern computer science education\n", "abstract": " A recent growth area in computer science education is physical computing, which involves combining software and hardware to build interactive physical systems that sense and respond to the real world. This article provides an overview of physical computing and its value in the classroom, using the BBC micro:bit as an example.", "num_citations": "17\n", "authors": ["253"]}
{"title": "MakeCode and CODAL: intuitive and efficient embedded systems programming for education\n", "abstract": " Across the globe, it is now commonplace for educators to engage in the making (design and development) of embedded systems in the classroom to motivate and excite their students. This new domain brings its own set of unique requirements. Historically, embedded systems development requires knowledge of low-level programming languages, local installation of compilation toolchains, device drivers, and applications. For students and educators, these requirements can introduce insurmountable barriers.     We present the motivation, requirements, implementation, and evaluation of a new programming platform that enables novice users to create software for embedded systems. The platform has two major components: 1) Microsoft MakeCode (www.makecode.com), a web app that encapsulates an entire beginner IDE for microcontrollers; and 2) CODAL, an efficient component-oriented C++ runtime for\u00a0\u2026", "num_citations": "17\n", "authors": ["253"]}
{"title": "The verified software challenge: A call for a holistic approach to reliability\n", "abstract": " The software analysis community has made a lot of progress in creating software tools for detecting defects and performing proofs of shallow properties of programs. We are witnessing the birth of a virtuous cycle between software tools and their consumers and I, for one, am very excited about this. We understand much better how to engineer program analyses to scale to large code bases and deal with the difficult problem of false errors and reducing their number. We understand better the tradeoffs in sound vs. unsound analyses. The software tools developed and applied over the last eight years have had impact. This list of tools includes Blast [HJMS02], CCured [NMW02], CQual [FTA02], ESC/Java [FLL\u2009+\u200902], ESP [DLS02], Feaver [Hol00], MAGIC [CCG\u2009+\u200904], MC [HCXE02], MOPS [CDW04], Prefast [LBD+04], Prefix [BPS00], SLAM [BR01], Splint [EL02] and Verisoft [God97], to name a few.               This\u00a0\u2026", "num_citations": "16\n", "authors": ["253"]}
{"title": "Speech\u2010enabled services using TelePortal\u2122 software and VoiceXML*\n", "abstract": " TelePortal\u2122 software, which resides on a speech\u2010enabled telephony platform, brings the advantages of the World Wide Web to advanced speech recognition telephone services. In response to an incoming call, this software retrieves a dialogue specification document from a Web server, interprets it to collect input from a caller, and submits the input to a (possibly different) Web server, which processes the input and may continue the call by returning another dialogue specification document. The TelePortal architecture includes a browser (to retrieve and cache Web content), a set of interpreters (to process documents), and a set of platform interfaces (to allow the interpreters to control the speech and telephony resources of the host platform). Using the Web to retrieve dialogue documents and to process the input they collect creates a new business opportunity for network operators and third\u2010party application\u00a0\u2026", "num_citations": "16\n", "authors": ["253"]}
{"title": "Constructing control flow from control dependence\n", "abstract": " Control dependences were introduced in [8, 10] to characterize how the predicates in a program govern the execution of other statements and predicates. Control dependences are defined in terms of a program's control-flow graph; given a control-flow graph G, a corresponding control dependence graph, CDG (G), can be constructed using the methods of [6, 7, 10]. This paper addresses the inverse problem: we define an algorithm that, given a control dependence graph C, finds a corresponding control-flow graph G.(ie, a graph G such that CDG (G) is isomorphic to C), or determines that no such control-flow graph exists. We call this process CDG-reconstitution.CDG-reconstitution is a sub-problem of program dependence graph reconstitution: given a program dependence graph (a graph that combines a program's control dependence and data dependence graphs), find a corresponding program or determine that\u00a0\u2026", "num_citations": "15\n", "authors": ["253"]}
{"title": "MakeCode and CODAL: Intuitive and efficient embedded systems programming for education\n", "abstract": " Historically, embedded systems development has been a specialist skill, requiring knowledge of low-level programming languages, complex compilation toolchains, and specialist hardware, firmware, device drivers and applications. However, it has now become commonplace for a broader range of non-specialists to engage in the making (design and development) of embedded systems - including educators to motivate and excite their students in the classroom. This diversity brings its own set of unique requirements, and the complexities of existing embedded systems development platforms introduce insurmountable barriers to entry.In this paper we present the motivation, requirements, implementation, and evaluation of a new programming platform that enables novice users to create effective and efficient software for embedded systems. The platform has two major components: (1) Microsoft MakeCode (www\u00a0\u2026", "num_citations": "14\n", "authors": ["253"]}
{"title": "Efficient evaluation of pointer predicates with Z3 SMT Solver in SLAM2\n", "abstract": " Static Driver Verifier (SDV) is a verification tool included in the Windows 7 Driver Kit (WDK). SDV uses SLAM as the program analysis engine. SDV 2.0 released with Windows 7 uses a re-designed SLAM2 engine. SLAM2 improves the precision and performance of predicate evaluation by using Z3 SMT solver. To handle predicates with pointers in SLAM2, we propose a novel set of axioms that defines a logical memory model, which is one of the underlying concepts and limitations of SLAM. We also designed an algorithm of encoding predicates passed to Z3 with uninterpreted functions over integers. In this paper, we present the axioms and the encoding. We also show how the axioms can be modified to achieve a better precision by refining the memory model. Our profiling of SDV runs on real device drivers confirms that the axioms and the encoding allowed SLAM2 to achieve a good balance between the precision required by the logical memory model, and Z3 performance on complex predicates. Our presentation of the axioms and the encoding in this paper is decoupled from SLAM2, such that they could be utilized by other static analysis tools when dealing with pointer predicates-often a bottleneck in such tools.", "num_citations": "13\n", "authors": ["253"]}
{"title": "Computer Aided Verification\n", "abstract": " The open access two-volume set LNCS 12224 and 12225 constitutes the refereed proceedings of the 32st International Conference on Computer Aided Verification, CAV 2020, held in Los Angeles, CA, USA, in July 2020.* The 43 full papers presented together with 18 tool papers and 4 case studies, were carefully reviewed and selected from 240 submissions. The papers were organized in the following topical sections: Part I: AI verification; blockchain and Security; Concurrency; hardware verification and decision procedures; and hybrid and dynamic systems. Part II: model checking; software verification; stochastic systems; and synthesis.* The conference was held virtually due to the COVID-19 pandemic.", "num_citations": "12\n", "authors": ["253"]}
{"title": "What's in a region?-or-computing control dependence regions in linear time and space\n", "abstract": " Regions of control dependence identify the instructions in a program that execute under the same control conditions. They have a variety of applications in parallelizing and optimizing compilers. Two vertices in a control flow graph (which may represent instructions or basic blocks in a program) are in the same region if they have the same set of control dependence predecessors. The best known algorithm for computing regions takes O (VxE) time, where V and E are the number of vertices and edges in the control flow graph, respectively. We present algorithms for finding regions in O (V+ E) time and O (V+ E) space, without using control dependence. These algorithms are based on alternative definitions of regions, which are easier to reason with than the definitions based on control dependence,", "num_citations": "9\n", "authors": ["253"]}
{"title": "The BBC micro: bit coded by Microsoft Touch Develop\n", "abstract": " The chance to influence the lives of a million children comes once in a generation. With the partnership between the BBC and several technology companies, a small device, the BBC micro: bit will be given to a million children in the UK in 2016. Moreover, using the micro: bit will be part of the curriculum. This demo describes the BBC micro: bit together with its software platform, based on Microsoft's Touch Develop. The demo will illustrate the architecture of the micro: bit and the software engineering hurdles that had to be overcome to enable it to be used by children. Evaluation of studies of the software platform are available and early anecdotal evidence of the hardware. A video about the micro: bit is available at aka. ms/bbcmicrobit.", "num_citations": "8\n", "authors": ["253"]}
{"title": "MakerArcade: Using Gaming and Physical Computing for Playful Making, Learning, and Creativity\n", "abstract": " The growing maker movement has created a number of hardware and construction toolkits that lower the barriers of entry into programming for youth and others, using a variety of approaches, such as gaming or robotics. For constructionist-like kits that use gaming, many are focused on designing and programming games that are single player, and few explore using physical and craft-like approaches that move beyond the screen and single player experiences. Moving beyond the screen to incorporate physical sensors into the creation of gaming experiences provides new opportunities for learning about concepts in a variety of areas in computer science and making. In this early work, we elucidate our design goals and prototype for a mini-arcade system that builds upon principles in constructionist gaming-making games to learn programming-as well as physical computing.", "num_citations": "6\n", "authors": ["253"]}
{"title": "CloudSDV enabling static driver verifier using Microsoft azure\n", "abstract": " In this paper we describe our experience of enabling Static Driver Verifier to use the Microsoft Azure cloud computing platform. We first describe in detail our architecture and methodology for enabling SDV to operate in the Microsoft Azure cloud. We then present our results of using CloudSDV on single drivers and driver suites using various configurations of the cloud relative to a local machine. Our experiments show that using the cloud, we are able to achieve speedups in excess of 20x, which has enabled us to perform mass scale verification in a matter of hours as opposed to days. Finally, we present a brief discussion about our results and experiences.", "num_citations": "6\n", "authors": ["253"]}
{"title": "Bridging the gap between general-purpose and domain-specific compilers with synthesis\n", "abstract": " This paper describes a new approach to program optimization that allows general purpose code to benefit from the optimization power of domain-specific compilers. The key to this approach is a synthesis-based technique to raise the level of abstraction of general-purpose code to enable aggressive domain-specific optimizations. We have been implementing this approach in an extensible system called Herd. The system is designed around a collection of parameterized kernel translators. Each kernel translator is associated with a domain-specific compiler, and the role of each kernel translator is to scan the input code in search of code fragments that can be optimized by the domain-specific compiler embedded within each kernel translator. By leveraging general synthesis technology, it is possible to have a generic kernel translator that can be specialized by compiler developers for each domain-specific compiler, making it easy to build new domain knowledge into the overall system. We illustrate this new approach to build optimizing compilers in two different domains, and highlight research challenges that need to be addressed in order to achieve the ultimate vision.", "num_citations": "6\n", "authors": ["253"]}
{"title": "Coupling memory and computation for locality management\n", "abstract": " We articulate the need for managing (data) locality automatically rather than leaving it to the programmer, especially in parallel programming systems. To this end, we propose techniques for coupling tightly the computation (including the thread scheduler) and the memory manager so that data and computation can be positioned closely in hardware. Such tight coupling of computation and memory management is in sharp contrast with the prevailing practice of considering each in isolation. For example, memory-management techniques usually abstract the computation as an unknown\" mutator\", which is treated as a\" black box\". As an example of the approach, in this paper we consider a specific class of parallel computations, nested-parallel computations. Such computations dynamically create a nesting of parallel tasks. We propose a method for organizing memory as a tree of heaps reflecting the structure of the nesting. More specifically, our approach creates a heap for a task if it is separately scheduled on a processor. This allows us to couple garbage collection with the structure of the computation and the way in which it is dynamically scheduled on the processors. This coupling enables taking advantage of locality in the program by mapping it to the locality of the hardware. For example for improved locality a heap can be garbage collected immediately after its task finishes when the heap contents is likely in cache.", "num_citations": "6\n", "authors": ["253"]}
{"title": "Online creation of object states for testing\n", "abstract": " An application is tested by using the public interface to determine the possible class types, choosing a class type at random, and then finding a constructor which creates an object of that class type. A method that takes the object is selected, and input values for the method are selected. The object is then created using the constructor and any other required methods. The selected method is called on the object using the selected inputs. If calling the method with the object changes the state of the object or an associated object, then the object and method are extended, sometimes by adding another method call to the existing method call (s), creating a new plan, and the new plan is then tested. If the state is not changed, then the object and the method are not further tested.", "num_citations": "6\n", "authors": ["253"]}
{"title": "Paths between imperative and functional programming\n", "abstract": " This article explores relationships between imperative and functional programming by viewing a program as a set of paths. We argue, through a small case study, that the presence of infeasible (or unexecutable) paths makes programs harder to understand. We identify two main causes of infeasible paths,\" unnecessary\" sequencing and destructive update, hallmarks of an imperative programming style. Functional programming eschews sequencing and destructive update, which can result in programs with fewer infeasible paths that are easier to understand. No proofs are included. We intend to provoke discussion regarding imperative and fimctional programruing styles.", "num_citations": "6\n", "authors": ["253"]}
{"title": "Microsoft MakeCode: embedded programming for education, in blocks and TypeScript\n", "abstract": " Microsoft MakeCode (https://www. makecode. com) is a platform and accompanying web app for simplifying the programming of microcontroller-based devices in the classroom. For each device, MakeCode provides a customized end-to-end experience in the web browser consisting of code editors, device simulator, debugger, compiler to machine code, and linker to a pre-compiled C++ runtime, as well as a documentation and tutorial system. We present an overview of MakeCode and detail the major design decisions behind the platform.", "num_citations": "5\n", "authors": ["253"]}
{"title": "Static TypeScript: an implementation of a static compiler for the TypeScript language\n", "abstract": " While the programming of microcontroller-based embeddable devices typically is the realm of the C language, such devices are now finding their way into the classroom for CS education, even at the level of middle school. As a result, the use of scripting languages (such as JavaScript and Python) for microcontrollers is on the rise.", "num_citations": "5\n", "authors": ["253"]}
{"title": "Tracking the flow of ideas through the programming languages literature\n", "abstract": " How have conferences like ICFP, OOPSLA, PLDI, and POPL evolved over the last 20 years? Did generalizing the Call for Papers for OOPSLA in 2007 or changing the name of the umbrella conference to SPLASH in 2010 have any effect on the kinds of papers published there? How do POPL and PLDI papers compare, topic-wise? Is there related work that I am missing? Have the ideas in O'Hearn's classic paper on separation logic shifted the kinds of papers that appear in POPL? Does a proposed program committee cover the range of submissions expected for the conference? If we had better tools for analyzing the programming language literature, we might be able to answer these questions and others like them in a data-driven way. In this paper, we explore how topic modeling, a branch of machine learning, might help the programming language community better understand our literature.", "num_citations": "5\n", "authors": ["253"]}
{"title": "Program repair\n", "abstract": " A method and system for repairing a program are provided herein. The method includes statically analyzing a code of a program via a modular program verifier and determining semantic errors within the code of the program based on the static analysis. The method also includes inferring verified repairs to the code of the program based on the semantic errors.", "num_citations": "5\n", "authors": ["253"]}
{"title": "Efficient reachability analysis of B\u00fcchi pushdown systems for hardware/software co-verification\n", "abstract": " We present an efficient approach to reachability analysis of B\u00fcchi Pushdown System (BPDS) models for Hardware/Software (HW/SW) co-verificat-ion. This approach utilizes the asynchronous nature of the HW/SW interactions to reduce unnecessary HW/SW state transition orders being explored in co-verificat-ion. The reduction is applied when the verification model is constructed. We have realized this approach in our co-verification tool, CoVer, and applied it to the co-verification of two fully functional Windows device drivers with their device models respectively. Both of the drivers are open source and their original C code has been used. CoVer has proven seven safety properties and detected seven previously undiscovered software bugs. Evaluation shows that the reduction can significantly scale co-verification.", "num_citations": "5\n", "authors": ["253"]}
{"title": "Web-based analysis of large-scale software systems\n", "abstract": " The software development process is complex and multifaceted, especially in large-scale projects, leaving a trace of many different documents. A plethora of hand-written and automatically generated documents define the software process, requirements for a system, its architecture, status and details of its implementation, testing, etc. Often these documents are kept in disparate databases and data formats. In large-scale projects, the source code of a system and changes to the code are recorded automatically in a version control system, a form of documentation that is quite voluminous and rich in detail. While all the documents are related in well-defined ways, it is hard to explore these relationships because of the different interfaces to each of the data sources.To help describe and understand different aspects of software evolution simultaneously, we have developed a number of tools for examining changes to documents and visualizing system artifacts such as source code and source version history. To integrate the tools and to provide unified simple access, we implemented them using standard web infrastructure, such as common gateway interface (CGI) scripts to process and retrieve the data, HTML and", "num_citations": "5\n", "authors": ["253"]}
{"title": "Increasing human-tool interaction via the web\n", "abstract": " Software tools researchers can accelerate their ability to learn by exposing tools to users via web technologies, allowing them to observe and test the interactions between humans and tools. At Microsoft Research, we have developed a web service (http://www. rise4fun. com/) for such a purpose that is available for community use.", "num_citations": "3\n", "authors": ["253"]}
{"title": "The Essence of Dynamic Analysis\n", "abstract": " The Essence of Dynamic Analysis Page 1 The Essence of Dynamic Analysis Thomas Ball Microsoft Research (modified by Zhang) Page 2 A \u201cPresent\u201d Challenge for Dynamic Analysis #include <stdio.h> main(t,_,a) char *a; { return!0<t?t<3?main(-79,-13,a+main(-87,1-_,main(-86,0,a+1)+a)): 1,t<_?main(t+1,_,a):3,main(-94,-27+t,a)&&t==2?_<13? main(2,_+1,\"%s %d %d\\n\"):9:16:t<0?t<-72?main(_,t, \"@n'+,#'/*{}w+/w#cdnr/+,{}r/*de}+,/*{*+,/w{%+,/w#q#n+,/#{l+,/n{n+,/+#n+,/#\\ ;#q#n+,/+k#;*+,/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \\ q#'+d'K#!/+k#;q#'r}eKK#}w'r}eKK{nl]'/#;#q#n'){)#}w'){){nl]'/+#n';d}rw' i;# \\ ){nl]!/n{n#'; r{#w'r nc{nl]'/#{l,+'K {rw' iK{;[{nl]'/w#q#n'wk nw' \\ iwk{KK{nl]!/w{%'l##w#' i; :{nl]'/*{q#'ld;r'}{nlwb!/*de}'c \\ ;;{nl'-{}rw]'/+,}##'*}#nc,',#nw]'/+kd'+e}+;#'rdq#w! nr'/ ') }+}{rl#'{n' ')# \\ }'+}##(!!/\") :t<-50?_==*a?putchar(31[a]):main(-65,_,a+1):main((*a=='/')+t,_,a+1) :0<t?main(2,2,\"%s\"):*a=='/'||main(0,main(-61,*a, \"!ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\\-;.\u2026", "num_citations": "3\n", "authors": ["253"]}
{"title": "Multi-platform computing for physical devices via MakeCode and CODAL\n", "abstract": " As the Internet of Things becomes commonplace, modern software must encompass the sensors, actuators and controllers that make up these physical computers. But can non-experts program such systems? Can such software development be undertaken by anyone, especially programmers who are learning or who are not aiming to be technical experts? We describe the motivation and principles behind Microsoft MakeCode and CODAL, two symbiotic frameworks which have many innovative engineering features for physical computing. Together, these two technologies highlight a new approach to software development for embedded computing devices which provides accessible programming languages and environments that reduce the complexity of programming embedded devices without compromising the flexibility or performance of the resulting software.", "num_citations": "2\n", "authors": ["253"]}
{"title": "The Design of Terra: Harnessing the best features of high-level and low-level languages\n", "abstract": " Applications are often written using a combination of high-level and low-level languages since it allows performance critical parts to be carefully optimized, while other parts can be written more productively. This approach is used in web development, game programming, and in build systems for applications themselves. However, most languages were not designed with interoperability in mind, resulting in glue code and duplicated features that add complexity. We propose a two-language system where both languages were designed to interoperate. Lua is used for our high-level language since it was originally designed with interoperability in mind. We create a new low-level language, Terra, that we designed to interoperate with Lua. It is embedded in Lua, and meta-programmed from it, but has a low level of abstraction suited for writing high-performance code. We discuss important design decisions-compartmentalized runtimes, glue-free interoperation, and meta-programming features-that enable Lua and Terra to be more powerful than the sum of their parts.", "num_citations": "2\n", "authors": ["253"]}
{"title": "Computer Aided Verification: 18th International Conference, CAV 2006, Seattle, WA, USA, August 17-20, 2006, Proceedings\n", "abstract": " This book constitutes the refereed proceedings of the 18th International Conference on Computer Aided Verification, CAV 2006, held as part of the 4th Federated Logic Conference, FLoC 2006. Presents 35 revised full papers together with 10 tool papers and 4 invited papers adressing all current issues in computer aided verification and model checking-from foundational and methodological issues ranging to the evaluation of major tools and systems", "num_citations": "2\n", "authors": ["253"]}
{"title": "Optimally Profiling and Tracing Programs\n", "abstract": " This paper presents two algorithms for inserting monitoring code to profile and trace programs. These algorithms greatly reduce the cost of measuring programs. Profiling, which counts the number of times each basic block in a program executes, is widely used to measure instruction set utilization of computers, identify program bottlenecks, and estimate program execution times for code optimization. Instruction traces are the basis for trace-driven simulation and analysis and are used also in trace-driven debugging. The profiling algorithm instruments a program for profiling by choosing a placement of counters that is optimized\u2014and frequently optimal\u2014with respect to the expected or measured execution frequency of each basic block and branch in the program. The tracing algorithm instruments a program to obtain a subsequence of the basic block trace\u2014whose length is optimized with respect to the program's\u00a0\u2026", "num_citations": "2\n", "authors": ["253"]}
{"title": "Rethinking the Runway: Using Avant-Garde Fashion To Design a System for Wearables\n", "abstract": " Technology has become increasingly pervasive in the creative and experimental environment of the avant-garde fashion runway, particularly in relation to its garments. However, several disciplines are often necessary when exploring technologies for the construction of expressive garments (eg garments that respond to their environment), creating a barrier for fashion designers that has limited their ability to leverage new technologies. To help overcome this barrier, we designed and deployed Brookdale, a prototyping system for wearable technology consisting of new plug-and-play hardware that can be programmed using drag-and-drop software. Brookdale was created using a 24-week participatory design process with 17 novice fashion-tech designers. At the end of the 24 week process, designers showcased their Brookdale-enhanced garment collections at an avant-garde fashion-tech runway show in New York\u00a0\u2026", "num_citations": "1\n", "authors": ["253"]}
{"title": "TileCode: Creation of Video Games on Gaming Handhelds\n", "abstract": " We present TileCode, a video game creation environment that runs on battery-powered microcontroller-based gaming handhelds. Our work is motivated by the popularity of retro video games, the availability of low-cost gaming handhelds loaded with many such games, and the concomitant lack of a means to create games on the same handhelds. With TileCode, we seek to close the gap between the consumers and creators of video games and to motivate more individuals to participate in the design and creation of their own games. The TileCode programming model is based on tile maps and provides a visual means for specifying the context around a sprite, how a sprite should move based on that context, and what should happen upon sprite collisions. We demonstrate that a variety of popular video games can be programmed with TileCode using 10-15 visual rules and compare/contrast with block-based\u00a0\u2026", "num_citations": "1\n", "authors": ["253"]}
{"title": "The Micro: bit: Hands-on Computing for the New Generation\n", "abstract": " The micro: bit (http://www. microbit. org) is a pocket-sized, programmable computing device, designed to engage people with computing technology. The micro: bit is visually appealing, fun, easy to code and inexpensive. It is widely available at schools in the United Kingdom and is now being rolled out world-wide. Key features of the micro: bit that make it a great device for physical computing include a display of 25 LEDs, two programmable input buttons, a USB connector, an edge connector, built-in sensors (eg accelerometer, compass and temperature sensor), Bluetooth and a battery pack connector. With these physical attributes, the micro: bit can be used to interact with the world in engaging ways such as a watch, a guitar or a moisture sensor. Multi-person games and apps are possible since micro: bits can communicate with each other. Programming the micro: bit can take place on almost any device (laptop\u00a0\u2026", "num_citations": "1\n", "authors": ["253"]}
{"title": "Toward Full Elasticity in Distributed Static Analysis\n", "abstract": " In this paper we present the design and implementation of a distributed static analysis framework that is designed to scale with the size of the input. Our approach is based on the actor programming model and is deployed in the cloud. Our reliance on a cloud cluster provides a degree of elasticity for CPU, memory, and storage resources. To demonstrate the potential of our technique, we show how a typical call graph analysis can be implemented in a distributed setting. The vision that motivates this work is that every large-scale software repository such as GitHub, BitBucket, or Visual Studio Online will be able to perform static analysis on a very large scale.We experimentally validate our distributed analysis approach using a combination of both synthetic and real benchmarks. To show scalability, we demonstrate how the analysis presented in this paper is able to handle inputs that are almost 10 million LOC in size, without running out of memory. Our results show that the analysis scales well in terms of memory pressure independently of the input size, as we add more VMs. As the number of analysis VMs increases, we observe that the analysis time generally improves as well. Lastly, we demonstrate that querying the results can be performed with a median latency of 15 ms.", "num_citations": "1\n", "authors": ["253"]}
{"title": "Correctness via compilation to logic: A decade of verification at Microsoft Research\n", "abstract": " Advances in automated theorem provers over the last decade have led to a renaissance in software tools that compile problems of correctness to problems over logic formula. In this talk, I will review progress in automated theorem provers, such as Z3 from Microsoft Research, and consider a variety of program correctness tools that build upon Z3, such as automated test generators, automated safety/termination checkers, as well as interactive functional verifiers. I'll then describe a number of new projects that make use of the\" correctness via compilation to logic\" approach, including the design of new programming languages, ensuring the security of data centers, and safely programming gesture recognizers such as Kinect.", "num_citations": "1\n", "authors": ["253"]}
{"title": "Beyond first-order satisfaction: Fixed points, interpolants, automata and polynomials\n", "abstract": " In the last decade, advances in satisfiability-modulo-theories (SMT) solvers have powered a new generation of software tools for verification and testing. These tools transform various program analysis problems into the problem of satisfiability of formulas in propositional or first-order logic, where they are discharged by SMT solvers, such as Z3 from Microsoft Research. This paper briefly summarizes four initiatives from Microsoft Research that build upon Z3 and move beyond first-order satisfaction: Fixed points\u2014\u03bcZ is a scalable, efficient engine for discharging fixed point queries over recursive predicates with logical constraints, integrated in Z3; Interpolants\u2014Interpolating Z3 uses Z3\u2019s proof generation capability to generate Craig interpolants in the first-order theory of uninterpreted functions, arrays and linear arithmetic; Automata\u2014The symbolic automata toolkit lifts classical automata analyses to work\u00a0\u2026", "num_citations": "1\n", "authors": ["253"]}
{"title": "Model checking b\u00fcchi pushdown systems\n", "abstract": " We develop an approach to model checking Linear Temporal Logic (LTL) properties of B\u00fcchi Pushdown Systems (BPDS). Such BPDS models are suitable for Hardware/Software (HW/SW) co-verification. Since a BPDS represents the asynchronous transitions between hardware and software, some transition orders are unnecessary to be explored in verification. We design an algorithm to reduce BPDS transition rules, so that these transition orders will not be explored by model checkers. Our reduction algorithm is applied at compile time; therefore, it is also suitable to runtime techniques such as co-simulation. As a proof of concept, we have implemented our approach in our co-verification tool, CoVer. CoVer not only verifies LTL properties on the BPDS models represented by Boolean programs, but also accepts assumptions in LTL formulae. The evaluation demonstrates that our reduction algorithm can\u00a0\u2026", "num_citations": "1\n", "authors": ["253"]}
{"title": "A brief history of software\u2014from Bell Labs to Microsoft Research\n", "abstract": " In the mid 1990s, I was (tangentially) part of an effort in Bell Labs called the \"Code Decay\" project. The hypothesis of this project was that over time code becomes fragile (more difficult to change without introducing problems), and that this process of decay could be empirically validated. This effort awakened me to the power of combining statistical expertise with software engineering expertise to address pressing problems of software production in a statistically valid manner. I will revisit some of the work we did in the Code Decay project at Bell Labs and then turn to what has been happening in this area in Microsoft in the last five years. In particular, I will trace how we have progressed from studying the data produced by product teams to validate hypotheses, to being actively involved with the product groups in creating and evaluating new tools and techniques for empirically-based software production.", "num_citations": "1\n", "authors": ["253"]}
{"title": "Automated abstraction of software\n", "abstract": " Automatically proving that a program has some property requires the discovery of appropriate abstractions. Such abstractions simplify the proof task and make it tractable. One approach is for a human to identify an appropriate abstraction. Another approach is to use the computer to search for an appropriate abstraction, based on the program and property under consideration. I will explain how the techniques of predicate abstraction and analysis of spurious error paths can guide the search for appropriate abstractions. These techniques are embedded in the SLAM analysis engine, which forms the core of a recently released Microsoft tool for checking Windows device drivers, called Static Driver Verifier.", "num_citations": "1\n", "authors": ["253"]}
{"title": "AE introduction\n", "abstract": " Editorial: AE introduction Page 1 Thomas Ball receiving the PhD degree in computer science in 1993 from the University of Wisconsin\u2014 Madison. He is a senior researcher at Microsoft Research (MSR) where he leads the Testing, Verification, and Measurement group. His research interests are in how combinations of static and dynamic program analysis, model checking, and theorem proving techniques can help improve the correctness and reliability of programs. For the last four years, he has been working on the SLAM project with Sriram Rajamani, the main product of which is an analysis engine for checking temporal safety properties of C programs. This engine forms the core of a new tool called Static Driver Verifier, in development in the Windows division, for checking that Windows device drivers are good clients of the Windows kernel API. Previous to working at MSR, he was a researcher at Bell Labs (1993-). , /\u2026", "num_citations": "1\n", "authors": ["253"]}
{"title": "Analyzing Path Pro les with the Hot Path Browser\n", "abstract": " Measuring program behavior is easy; understanding program behavior is hard. It is not di cult to construct program pro lers using instrumentation libraries such as ATOM SE94], and EEL LS95]. The resultant pro ling tools can accurately and e ciently record many aspects of programs' execution. Such tools generate reams of data but o er little support to the end user in analyzing and understanding this data. Producing useful information that provides insight into a program's behavior remains a di cult task. Without this understanding, consumers of measurement data {programmers, computer architects, compiler writers, etc.{can be distracted by minor moguls and miss important mountains. We have built a tool, called the Hot Path Browser (HPB) for graphically displaying path pro les BL96, ABL97]. Paths provide a concise record of a program's dynamic control ow. Even large complex programs, such as gcc and Microsoft Word, only execute a few tens of thousands of paths in an execution. Moreover, the vast majority of these paths contribute little to the overall execution, which is dominated by a small subset of hot paths. HPB helps the user analyze the path pro le data from one or more runs of a program. Its browser-like windowing interface provides mechanisms for isolating and displaying hot paths at the source level. With a tool such as HPB, a programmer can examine the small number of heavily executed paths, to better understand his or her program's behavior and to nd redundant computation. Similarly, a computer architect or compiler writer can use the tool to relate dynamic metrics, such as cache misses or instructions stalls along a path to\u00a0\u2026", "num_citations": "1\n", "authors": ["253"]}