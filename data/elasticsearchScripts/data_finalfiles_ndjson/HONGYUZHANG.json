{"title": "Log Clustering based Problem Identification for Online Service Systems\n", "abstract": " Logs play an important role in the maintenance of large-scale online service systems. When an online service fails, engineers need to examine recorded logs to gain insights into the failure and identify the potential problems. Traditionally, engineers perform simple keyword search (such as \u201cerror\u201d and \u201cexception\u201d) of logs that may be associated with the failures. Such an approach is often time consuming and error prone. Through our collaboration with Microsoft service product teams, we propose LogCluster, an approach that clusters the logs to ease log-based problem identification. LogCluster also utilizes a knowledge base to check if the log sequences occurred before. Engineers only need to examine a small number of previously unseen, representative log sequences extracted from the clusters to identify a problem, thus significantly reducing the number of logs that should be examined, meanwhile improving the\u00a0\u2026", "num_citations": "204\n", "authors": ["190"]}
{"title": "Formal semantics and verification for feature modeling\n", "abstract": " Research on features has received much attention in the domain engineering community. Feature modeling plays an important role in the design and implementation of complex software systems. However, the presentation and analysis of feature models are still largely informal. There is also an increasing need for methods and tools that can support automated feature model analysis. This paper presents a formal engineering approach to the specification and verification of feature models. A formal semantics for the feature modeling language is defined using first-order logic. It provides a precise and rigorous formal interpretation for the graphical notation. In addition, further validation of the semantics using the Z/EVES theorem prover is presented. Finally, we demonstrate that the consistency of a feature model and its configurations can be automatically verified by encoding the semantics into the Alloy Analyzer. A\u00a0\u2026", "num_citations": "197\n", "authors": ["190"]}
{"title": "Sample-based software defect prediction with active and semi-supervised learning\n", "abstract": " Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised\u00a0\u2026", "num_citations": "177\n", "authors": ["190"]}
{"title": "Predicting bug-fixing time: an empirical study of commercial software projects\n", "abstract": " For a large and evolving software system, the project team could receive many bug reports over a long period of time. It is important to achieve a quantitative understanding of bug-fixing time. The ability to predict bug-fixing time can help a project team better estimate software maintenance efforts and better manage software projects. In this paper, we perform an empirical study of bug-fixing time for three CA Technologies projects. We propose a Markov-based method for predicting the number of bugs that will be fixed in future. For a given number of defects, we propose a method for estimating the total amount of time required to fix them based on the empirical distribution of bug-fixing time derived from historical data. For a given bug report, we can also construct a classification model to predict slow or quick fix (e.g., below or above a time threshold). We evaluate our methods using real maintenance data from three\u00a0\u2026", "num_citations": "174\n", "authors": ["190"]}
{"title": "Verifying feature models using OWL\n", "abstract": " Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a formal semantics and reasoning support of feature models has hindered the development of this area. Industrial experiences also show that methods and tools that can support feature model analysis are badly appreciated. Such reasoning tool should be fully automated and efficient. At the same time, the reasoning tool should scale up well since it may need to handle hundreds or even thousands of features a that modern software systems may have. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web OWL ontologies. We use OWL DL ontologies to precisely capture the inter-relationships among the features in a feature diagram. OWL reasoning engines such as FaCT++ are deployed to check for the\u00a0\u2026", "num_citations": "169\n", "authors": ["190"]}
{"title": "A Novel Neural Source Code Representation based on Abstract Syntax Tree\n", "abstract": " Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements\u00a0\u2026", "num_citations": "159\n", "authors": ["190"]}
{"title": "An investigation of the relationships between lines of code and defects\n", "abstract": " It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.", "num_citations": "154\n", "authors": ["190"]}
{"title": "CodeHow: Effective Code Search based on API Understanding and Extended Boolean Model\n", "abstract": " Over the years of software development, a vast amount of source code has been accumulated. Many code search tools were proposed to help programmers reuse previously-written code by performing free-text queries over a large-scale codebase. Our experience shows that the accuracy of these code search tools are often unsatisfactory. One major reason is that existing tools lack of query understanding ability. In this paper, we propose CodeHow, a code search technique that can recognize potential APIs a user query refers to. Having understood the potentially relevant APIs, CodeHow expands the query with the APIs and performs code retrieval by applying the Extended Boolean model, which considers the impact of both text similarity and potential APIs on code search. We deploy the backend of CodeHow as a Microsoft Azure service and implement the front-end as a Visual Studio extension. We evaluate\u00a0\u2026", "num_citations": "152\n", "authors": ["190"]}
{"title": "Comments on \"Data Mining Static Code Attributes to Learn Defect Predictors\"\n", "abstract": " In this correspondence, we point out a discrepancy in a recent paper, \"data mining static code attributes to learn defect predictors,\" that was published in this journal. Because of the small percentage of defective modules, using probability of detection (pd) and probability of false alarm (pf) as accuracy measures may lead to impractical prediction models.", "num_citations": "138\n", "authors": ["190"]}
{"title": "Measuring design complexity of semantic web ontologies\n", "abstract": " Ontology languages such as OWL are being widely used as the Semantic Web movement gains momentum. With the proliferation of the Semantic Web, more and more large-scale ontologies are being developed in real-world applications to represent and integrate knowledge and data. There is an increasing need for measuring the complexity of these ontologies in order for people to better understand, maintain, reuse and integrate them. In this paper, inspired by the concept of software metrics, we propose a suite of ontology metrics, at both the ontology-level and class-level, to measure the design complexity of ontologies. The proposed metrics are analytically evaluated against Weyuker\u2019s criteria. We have also performed empirical analysis on public domain ontologies to show the characteristics and usefulness of the metrics. We point out possible applications of the proposed metrics to ontology quality control\u00a0\u2026", "num_citations": "132\n", "authors": ["190"]}
{"title": "A semantic web approach to feature modeling and verification\n", "abstract": " Feature models are widely used in domain engineering to capture common and variant concepts among systems in a particular domain. However, the lack of a formal semantics of feature models has hindered the development of this area. This paper presents a Semantic Web environment for modeling and verifying feature diagrams using ontologies. We use OWL DL (a decidable dialect of OWL) to precisely capture the relationships among features in feature diagrams and configurations. OWL reasoning engines such as RACER are deployed to check for the inconsistencies of feature configurations fully automatically. As part of the environment, we also develop a CASE tool to facilitate the visual development, interchange and reasoning of feature diagrams represented as ontologies.", "num_citations": "125\n", "authors": ["190"]}
{"title": "Balancing privacy and utility in cross-company defect prediction\n", "abstract": " Background: Cross-company defect prediction (CCDP) is a field of study where an organization lacking enough local data can use data from other organizations for building defect predictors. To support CCDP, data must be shared. Such shared data must be privatized, but that privatization could severely damage the utility of the data. Aim: To enable effective defect prediction from shared data while preserving privacy. Method: We explore privatization algorithms that maintain class boundaries in a dataset. CLIFF is an instance pruner that deletes irrelevant examples. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. CLIFF+MORPH are tested in a CCDP study among 10 defect datasets from the PROMISE data repository. Results: We find: 1) The CLIFFed+MORPHed algorithms provide more privacy than the state-of-the-art privacy algorithms; 2) in terms of\u00a0\u2026", "num_citations": "119\n", "authors": ["190"]}
{"title": "On the distribution of software faults\n", "abstract": " The Pareto principle is often used to describe how faults in large software systems are distributed over modules. A recent paper by Andersson and Runeson again confirmed the Pareto principle of fault distribution. In this paper, we show that the distribution of software faults can be more precisely described as the Weibull distribution.", "num_citations": "101\n", "authors": ["190"]}
{"title": "Predicting defective software components from code complexity measures\n", "abstract": " The ability to predict defective modules can help us allocate limited quality assurance resources effectively and efficiently. In this paper, we propose a complexity- based method for predicting defect-prone components. Our method takes three code-level complexity measures as input, namely Lines of Code, McCabe's Cyclomatic Complexity and Halstead's Volume, and classifies components as either defective or non-defective. We perform an extensive study of twelve classification models using the public NASA datasets. Cross-validation results show that our method can achieve good prediction accuracy. This study confirms that static code complexity measures can be useful indicators of component quality.", "num_citations": "85\n", "authors": ["190"]}
{"title": "Identifying impactful service system problems via log analysis\n", "abstract": " Logs are often used for troubleshooting in large-scale software systems. For a cloud-based online system that provides 24/7 service, a huge number of logs could be generated every day. However, these logs are highly imbalanced in general, because most logs indicate normal system operations, and only a small percentage of logs reveal impactful problems. Problems that lead to the decline of system KPIs (Key Performance Indicators) are impactful and should be fixed by engineers with a high priority. Furthermore, there are various types of system problems, which are hard to be distinguished manually. In this paper, we propose Log3C, a novel clustering-based approach to promptly and precisely identify impactful system problems, by utilizing both log sequences (a sequence of log events) and system KPIs. More specifically, we design a novel cascading clustering algorithm, which can greatly save the clustering\u00a0\u2026", "num_citations": "66\n", "authors": ["190"]}
{"title": "Conceptual data model-based software size estimation for information systems\n", "abstract": " Size estimation plays a key role in effort estimation that has a crucial impact on software projects in the software industry. Some information required by existing software sizing methods is difficult to predict in the early stage of software development. A conceptual data model is widely used in the early stage of requirements analysis for information systems. Lines of code (LOC) is a commonly used software size measure. This article proposes a novel LOC estimation method for information systems from their conceptual data models through using a multiple linear regression model. We have validated the proposed method using samples from both the software industry and open-source systems.", "num_citations": "53\n", "authors": ["190"]}
{"title": "Interactive fault localization leveraging simple user feedback\n", "abstract": " Many fault localization methods have been proposed in the literature. These methods take in a set of program execution profiles and output a list of suspicious program elements. The list of program elements ranked by their suspiciousness is then presented to developers for manual inspection. Currently, the suspicious elements are ranked in a batch process where developers' inspection efforts are rarely utilized for ranking. The inaccuracy and static nature of existing fault localization methods prompt us to incorporate user feedback to improve the accuracy of the existing methods. In this paper, we propose an interactive fault localization framework that leverages simple user feedback. Our framework only needs users to label the statements examined as faulty or clean, which does not require additional effort than conventional non-interactive methods. After users label suspicious program elements as faulty or clean\u00a0\u2026", "num_citations": "46\n", "authors": ["190"]}
{"title": "An Empirical Study on Quality Issues of Production Big Data Platform\n", "abstract": " Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. There is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-defined incident management process, which helps customers report and mitigate service quality issues on 24/7 basis. This paper explores the common symptom, causes and mitigation of service quality issues in Big Data computing. We conduct an empirical study on 210 real service quality issues in ProductA. Our major findings include (1) 21.0% of escalations are caused by\u00a0\u2026", "num_citations": "42\n", "authors": ["190"]}
{"title": "DeepPerf: Performance Prediction for Configurable Software with Deep Sparse Neural Network\n", "abstract": " Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method\u00a0\u2026", "num_citations": "41\n", "authors": ["190"]}
{"title": "Predicting node failure in cloud service systems\n", "abstract": " In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure\u00a0\u2026", "num_citations": "40\n", "authors": ["190"]}
{"title": "Learning to rank duplicate bug reports\n", "abstract": " For a large and complex software system, the project team could receive a large number of bug reports. Some bug reports could be duplicates as they essentially report the same problem. It is often tedious and costly to manually check if a newly reported bug is a duplicate of an already reported bug. In this paper, we propose BugSim, a method that can automatically retrieve duplicate bug reports given a new bug report. BugSim is based on learning to rank concepts. We identify textual and statistical features of bug reports and propose a similarity function for bug reports based on the features. We then construct a training set by assembling pairs of duplicate and non-duplicate bug reports. We train the weights of features by applying the stochastic gradient descent algorithm over the training set. For a new bug report, we retrieve candidate duplicate reports using the trained model. We evaluate BugSim using more\u00a0\u2026", "num_citations": "40\n", "authors": ["190"]}
{"title": "Retrieval-based Neural Source Code Summarization\n", "abstract": " Source code summarization aims to automatically generate concise summaries of source code in natural language texts, in order to help developers better understand and maintain source code. Traditional work generates a source code summary by utilizing information retrieval techniques, which select terms from original source code or adapt summaries of similar code snippets. Recent studies adopt Neural Machine Translation techniques and generate summaries from code snippets using encoder-decoder neural networks. The neural-based approaches prefer the high-frequency words in the corpus and have trouble with the low-frequency ones. In this paper, we propose a retrieval-based neural source code summarization approach where we enhance the neural model with the most similar code snippets retrieved from the training set. Our approach can take advantages of both neural and retrieval-based\u00a0\u2026", "num_citations": "37\n", "authors": ["190"]}
{"title": "Estimating LOC for information systems from their conceptual data models\n", "abstract": " Effort and cost estimation is crucial in software management. Estimation of software size plays a key role in the estimation. Line of Code (LOC) is still a commonly used software size measure. Despite the fact that software sizing is well recognized as an important problem for more than two decades, there is still much problem in existing methods. Conceptual data model is widely used in the requirements analysis for information systems. It is also not difficult to construct conceptual data models in the early stage of developing information systems. Much characteristic of an information system is actually reflected from its conceptual data model. We explore into the use of conceptual data model for estimating LOC. This paper proposes a novel method for estimating LOC for an information system from its conceptual data model through the use of multiple linear regression model. We have validated the method through\u00a0\u2026", "num_citations": "37\n", "authors": ["190"]}
{"title": "Bing developer assistant: improving developer productivity by recommending sample code\n", "abstract": " In programming practice, developers often need sample code in order to learn how to solve a programming-related problem. For example, how to reuse an Application Programming Interface (API) of a large-scale software library and how to implement a certain functionality. We believe that previously written code can help developers understand how others addressed the similar problems and can help them write new programs. We develop a tool called Bing Developer Assistant (BDA), which improves developer productivity by recommending sample code mined from public software repositories (such as GitHub) and web pages (such as Stack Overflow). BDA can automatically mine code snippets that implement an API or answer a code search query. It has been implemented as a free-downloadable extension of Microsoft Visual Studio and has received more than 670K downloads since its initial release in\u00a0\u2026", "num_citations": "36\n", "authors": ["190"]}
{"title": "Diversity maximization speedup for fault localization\n", "abstract": " Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than\u00a0\u2026", "num_citations": "34\n", "authors": ["190"]}
{"title": "An empirical investigation of incident triage for online service systems\n", "abstract": " Online service systems have become increasingly popular. During operation of an online service system, incidents (unplanned interruptions or outages of the service) are inevitable. As an initial step of incident management, it is important to be able to automatically assign an incident report to a suitable team. We call this step incident triage, which can significantly affect the efficiency and accuracy of overall incident management. To better understand the incident-triage practice in industry, we perform an empirical study of incident triage on 20 large-scale online service systems in Microsoft. We find that incorrect assignment of incident reports occurs frequently and incurs unnecessary cost, especially for the incidents with high severity. For example, about 4.11% to 91.58% of incident reports are reassigned at least once and the average increment in incident-triage time caused by the reassignments is up to 10.16X\u00a0\u2026", "num_citations": "33\n", "authors": ["190"]}
{"title": "Has this bug been reported?\n", "abstract": " Bug reporting is essentially an uncoordinated process. The same bugs could be repeatedly reported because users or testers are unaware of previously reported bugs. As a result, extra time could be spent on bug triaging and fixing. In order to reduce redundant effort, it is important to provide bug reporters with the ability to search for previously reported bugs. The search functions provided by the existing bug tracking systems are using relatively simple ranking functions, which often produce unsatisfactory results. In this paper, we adopt Ranking SVM, a Learning to Rank technique to construct a ranking model for effective bug report search. We also propose to use the knowledge of Wikipedia to discover the semantic relations among words and documents. Given a user query, the constructed ranking model can search for relevant bug reports in a bug tracking system. Unlike related works on duplicate bug report\u00a0\u2026", "num_citations": "33\n", "authors": ["190"]}
{"title": "Heterogeneous defect prediction through multiple kernel learning and ensemble learning\n", "abstract": " Heterogeneous defect prediction (HDP) aims to predict defect-prone software modules in one project using heterogeneous data collected from other projects. Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect prediction data: (1) data could be linearly inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Ensemble Multiple Kernel Correlation Alignment (EMKCA) based approach to HDP, which takes into consideration the two characteristics of the defect prediction data. Specifically, we first map the source and target project data into high dimensional kernel space through multiple kernel leaning, where the defective and non-defective modules can be better separated. Then, we design a kernel correlation\u00a0\u2026", "num_citations": "32\n", "authors": ["190"]}
{"title": "Symcrash: Selective recording for reproducing crashes\n", "abstract": " Software often crashes despite tremendous effort on software quality assurance. Once developers receive a crash report, they need to reproduce the crash in order to understand the problem and locate the fault. However, limited information from crash reports often makes crash reproduction difficult. Many\" capture-and-replay\" techniques have been proposed to automatically capture program execution data from the failing code, and help developers replay the crash scenarios based on the captured data. However, such techniques often suffer from heavy overhead and introduce privacy concerns. Recently, methods such as BugRedux were proposed to generate test input that leads to crash through symbolic execution. However, such methods have inherent limitations because they rely on conventional symbolic execution techniques. In this paper, we propose a dynamic symbolic execution method called SymCon\u00a0\u2026", "num_citations": "31\n", "authors": ["190"]}
{"title": "Continuous incident triage for large-scale online service systems\n", "abstract": " In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve\u00a0\u2026", "num_citations": "30\n", "authors": ["190"]}
{"title": "The scale-free nature of semantic web ontology\n", "abstract": " Semantic web ontology languages, such as OWL, have been widely used for knowledge representation. Through empirical analysis of real-world ontologies we discover that, like many natural and social phenomenon, the semantic web ontology is also\" scale-free\".", "num_citations": "29\n", "authors": ["190"]}
{"title": "Quality evaluation of volunteered geographic information: The case of OpenStreetMap\n", "abstract": " A large amount of crowd-sourced geospatial data have been created in recent years due to the interactivity of Web 2.0 and the availability of Global Positioning System (GPS). This geo-information is typically referred to as volunteered geographic information (VGI). OpenStreetMap (OSM) is a popular VGI platform that allows users to create or edit maps using GPS-enabled devices or aerial imageries. The issue of quality of geo-information generated by OSM has become a trending research topic because of the large size of the dataset and the inapplicability of Linus' Law in a geospatial context. This chapter systematically reviews the quality evaluation process of OSM, and demonstrates a case study of London, Canada for the assessment of completeness, positional accuracy and attribute accuracy. The findings of the quality evaluation can potentially serve as a guide of cartographic product selection and provide a\u00a0\u2026", "num_citations": "28\n", "authors": ["190"]}
{"title": "Discovering power laws in computer programs\n", "abstract": " The power-law regularities have been discovered behind many complex natural and social phenomenons. We discover that the power-law regularities, especially the Zipf\u2019s and Heaps\u2019 laws, also exist in large-scale software systems. We find that the distribution of lexical tokens in modern Java, C++ and C programs follows Zipf\u2013Mandelbrot law, and the growth of program vocabulary follows Heaps\u2019 law. The results are obtained through empirical analysis of real-world software systems. We believe our discovery reveals the statistical regularities behind computer programming.", "num_citations": "27\n", "authors": ["190"]}
{"title": "An empirical study of class sizes for large java systems\n", "abstract": " We perform an empirical study of class sizes (in terms of Lines of Code) on a number of large Java software systems, and discover an interesting pattern - that many classes have only small sizes whereas a few classes have large size. We call this phenomenon the small class phenomenon. Further analysis shows that the class sizes follow the lognormal distribution. Having understood the distribution of class sizes, we then derive a general size estimation model, which reveals the relationship between the size of a large Java system and the number of classes the system has. In this paper, we also show that the adoption of object- orientation is a possible cause of the small class phenomenon. We believe our study reveals the regularity that emerges from large-scale object-oriented software construction, and hope our research can contribute to a deep understanding of computer programming.", "num_citations": "27\n", "authors": ["190"]}
{"title": "Identifying recurrent and unknown performance issues\n", "abstract": " For a large-scale software system, especially an online service system, when a performance issue occurs, it is desirable to check whether this issue has occurred before. If there are past similar issues, a known remedy could be applied. Otherwise, a new troubleshooting process may have to be initiated. The symptom of a performance issue can be characterized by a set of metrics. Due to the sophisticated nature of software systems, manual diagnosis of performance issues based on metric data is typically expensive and laborious. In this paper, we propose a Hidden Markov Random Field (HMRF) based approach to automatic identification of recurrent and unknown performance issues. We formulate the problem of issue identification as a HMRF-based clustering problem. Our approach incorporates the learning of metric discretization thresholds and the optimization of issue clustering. Based on the learned\u00a0\u2026", "num_citations": "25\n", "authors": ["190"]}
{"title": "On the value of learning from defect dense components for software defect prediction\n", "abstract": " BACKGROUND: Defect predictors learned from static code measures can isolate code modules with a higher than usual probability of defects.AIMS: To improve those learners by focusing on the defect-rich portions of the training sets.METHOD: Defect data CM1, KC1, MC1, PC1, PC3 was separated into components. A subset of the projects (selected at random) were set aside for testing. Training sets were generated for a NaiveBayes classifier in two ways. In sample the dense treatment, the components with higher than the median number of defective modules were used for training. In the standard treatment, modules from any component were used for training. Both samples were run against the test set and evaluated using recall, probability of false alarm, and precision. In addition, under sampling and over sampling was performed on the defect data. Each method was repeated in a 10-by-10 cross-validation\u00a0\u2026", "num_citations": "25\n", "authors": ["190"]}
{"title": "A 3-D printed redundant six-component force sensor with eight parallel limbs\n", "abstract": " In this paper, the design, manufacture and calibration test of a 3-D printed parallel six-component force sensor are presented. Aiming at improving the measuring performances and downsizing the outline of six-component force sensor, a novel redundant parallel six-component force sensor with spoke structure combining parallel mechanisms with flexible mechanisms is proposed. The mathematical model of the proposed sensor is established by screw theory which reveals the force mapping relationship. Considering the structure particularity, three-dimensional printing technology is innovatively applied to manufacture six-component force sensor prototype and main details regarding manufacture are described. Calibration tests for the sensor prototype are also carried out. Based on the data recorded in experiments, the calibration matrix is obtained and its performances such as non-linearity, repeatability and\u00a0\u2026", "num_citations": "23\n", "authors": ["190"]}
{"title": "Heterogeneous defect prediction with two-stage ensemble learning\n", "abstract": " Heterogeneous defect prediction (HDP) refers to predicting defect-prone software modules in one project (target) using heterogeneous data collected from other projects (source). Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect data: (1) data could be linear inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Two-Stage Ensemble Learning (TSEL) approach to HDP, which contains two stages: ensemble multi-kernel domain adaptation (EMDA) stage and ensemble data sampling (EDS) stage. In the EMDA stage, we develop an Ensemble Multiple Kernel Correlation Alignment (EMKCA) predictor, which combines the advantage of multiple kernel learning and domain adaptation techniques. In\u00a0\u2026", "num_citations": "22\n", "authors": ["190"]}
{"title": "Outage prediction and diagnosis for cloud service systems\n", "abstract": " With the rapid growth of cloud service systems and their increasing complexity, service failures become unavoidable. Outages, which are critical service failures, could dramatically degrade system availability and impact user experience. To minimize service downtime and ensure high system availability, we develop an intelligent outage management approach, called AirAlert, which can forecast the occurrence of outages before they actually happen and diagnose the root cause after they indeed occur. AirAlert works as a global watcher for the entire cloud system, which collects all alerting signals, detects dependency among signals and proactively predicts outages that may happen anywhere in the whole cloud system. We analyze the relationships between outages and alerting signals by leveraging Bayesian network and predict outages using a robust gradient boosting tree based classification method. The\u00a0\u2026", "num_citations": "22\n", "authors": ["190"]}
{"title": "iDice: Problem Identification for Emerging Issues\n", "abstract": " One challenge for maintaining a large-scale software system, especially an online service system, is to quickly respond to customer issues. The issue reports typically have many categorical attributes that reflect the characteristics of the issues. For a commercial system, most of the time the volume of reported issues is relatively constant. Sometimes, there are emerging issues that lead to significant volume increase. It is important for support engineers to efficiently and effectively identify and resolve such emerging issues, since they have impacted a large number of customers. Currently, problem identification for an emerging issue is a tedious and error-prone process, because it requires support engineers to manually identify a particular attribute combination that characterizes the emerging issue among a large number of attribute combinations. We call such an attribute combination effective combination, which is\u00a0\u2026", "num_citations": "22\n", "authors": ["190"]}
{"title": "Exploring regularity in source code: Software science and Zipf's law\n", "abstract": " Are there statistical regularities behind computer programming? In 1970s, Halstead proposed the software science theory which attempted to describe some of the regularities based on the direct measurement of lexical tokens in programs. The famous software science length equation models the relationship between program length and vocabulary. By analyzing the source code of twelve Java software systems collected from public software repositories, we find that Halstead's length equation does not hold for large-scale modern software systems. We discover that the distribution of lexical tokens in studied systems follows the Zipf's law (or more generally, Zipf-Mandelbrot law), which is an empirical law in statistical natural language processing. Based on the discovery of Zipf's law, we propose a revised software science length equation for describing the vocabulary-length relationship. Our new equation fits the real\u00a0\u2026", "num_citations": "22\n", "authors": ["190"]}
{"title": "Neural Programming by Example\n", "abstract": " Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-to-end to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.", "num_citations": "19\n", "authors": ["190"]}
{"title": "Accuracy evaluation of the Canadian OpenStreetMap road networks\n", "abstract": " Volunteered geographic information (VGI) has been applied in many fields such as participatory planning, humanitarian relief and crisis management. One of the reasons for popularity of VGI is its cost-effectiveness. However, the coverage and accuracy of VGI cannot be guaranteed. The issue of geospatial data quality in the OpenStreetMap (OSM) project has become a trending research topic because of the large size of the dataset and the multiple channels of data access. This paper provides details on a national study of the Canadian OSM street network data for the assessment ofcompleteness, positional accuracy, attribute accuracy, semantic accuracy and lineage. The findings of the map quality can potentially guide cartographic product selection for interested parties and offer a better understanding of future improvement of OSM quality. In addition, the study presents the complex processes behind OSM contributions possibly influenced by data import and remote mapping.", "num_citations": "19\n", "authors": ["190"]}
{"title": "Predicting defect numbers based on defect state transition models\n", "abstract": " During software maintenance, a large number of defects could be discovered and reported. A defect can enter many states during its lifecycle, such as NEW, ASSIGNED, and RESOLVED. The ability to predict the number of defects at each state can help project teams better evaluate and plan maintenance activities. In this paper, we present BugStates, a method for predicting defect numbers at each state based on defect state transition models. In our method, we first construct defect state transition models using historical data. We then derive a stability metric from the transition models to measure a project's defect-fixing performance. For projects with stable defect-fixing performance, we show that we can apply Markovian method to predict the number of defects at each state in future based on the state transition model. We evaluate the effectiveness of BugStates using six open source projects and the results are\u00a0\u2026", "num_citations": "19\n", "authors": ["190"]}
{"title": "Preference model driven services selection\n", "abstract": " Service, as a computing and business paradigm, is gaining daily growing attention, which is being recognized and adopted by more and more people. For all involved players, it is inevitable to face service selection situations where multiple qualities of services criteria needs to be taken into account, and complex interrelationships between different impact factors and actors need to be understood and traded off. In this paper, we propose using goal and agent-based preference models, represented with annotated NFR/i* framework to drive these decision making activities. Particularly, we present how we enhance the modeling language with quantitative preference information based on input from domain experts and end users, how softgoals interrelationships graph can be used to group impact factors with common focus, and how actor dependency models can be used to represent and evaluate alternative\u00a0\u2026", "num_citations": "18\n", "authors": ["190"]}
{"title": "An initial study of the growth of eclipse defects\n", "abstract": " We analyze the Eclipse defect data from June 2004 to November 2007, and find that the growth of the number of defects can be well modeled by polynomial functions. Furthermore, we can predict the number of future Eclipse defects based on the nature of defect growth.", "num_citations": "18\n", "authors": ["190"]}
{"title": "An Empirical Study on Program Failures of Deep Learning Jobs\n", "abstract": " Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O. This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major\u00a0\u2026", "num_citations": "17\n", "authors": ["190"]}
{"title": "Isotropy analysis of redundant parallel six-axis force sensor\n", "abstract": " The structural model of generalized redundant parallel six-axis force sensor is proposed. Based on the modified Stewart platform, its mathematical model is established with screw theory. The structural models of four typical redundant six-axis force sensors are proposed and their mathematical models are established for the corresponding structures. The isotropy of the four structural models is analyzed. The parameters relation leading to spatially isotropic configuration is conduced and demonstrated by a numerical example. Analytic solutions to the isotropy of the four models are conducted and their valid range of the analytic results leading to isotropic configuration is discussed. The conclusion is drawn that under the condition of force isotropy, two structures with the relatively least thickness are selected when all the legs of different structures are the same.", "num_citations": "17\n", "authors": ["190"]}
{"title": "Understanding chinese characteristics of requirements engineering\n", "abstract": " Rapid changes in the social and technical environment bring about many new challenges to system requirements engineering, amongst which out-sourcing or off-shoring of certain design tasks to countries with more human resources and broader markets becomes promising business leverage. This paper reports the results from a survey of requirements practices in China and points out their implications. The survey aims to understand the current state of RE practice in China, and investigate the impacts that Chinese culture has on requirements engineering activities. We collected data from 149 participants in 97 Chinese companies and 15 research institutes. We also analyzed the impact of Chinese culture on requirements engineering practices. We hope our results are useful for industrial practitioners and academic researchers wishing to improve current practices, and for foreign software companies wishing to\u00a0\u2026", "num_citations": "17\n", "authors": ["190"]}
{"title": "Cross-project and within-project semi-supervised software defect prediction problems study using a unified solution\n", "abstract": " When there exists not enough historical defect data for building accurate prediction model, semi-supervised defect prediction (SSDP) and cross-project defect prediction (CPDP) are two feasible solutions. Existing CPDP methods assume that the available source data is well labeled. However, due to expensive human efforts for labeling a large amount of defect data, usually, we can only make use of the suitable unlabeled source data to help build the prediction model. We call CPDP in this scenario as cross-project semi-supervised defect prediction (CSDP). As to within-project semi-supervised defect prediction (WSDP), although some WSDP methods have been developed in recent years, there still exists much room for improvement. In this paper, we aim to provide an effective solution for both CSDP and WSDP problems. We introduce the semi-supervised dictionary learning technique, an effective machine\u00a0\u2026", "num_citations": "15\n", "authors": ["190"]}
{"title": "Casper: An Efficient Approach to Call Trace Collection\n", "abstract": " Call traces, i.e., sequences of function calls and returns, are fundamental to a wide range of program analyses such as bug reproduction, fault diagnosis, performance analysis, and many others. The conventional approach to collect call traces that instruments each function call and return site incurs large space and time overhead. Our approach aims at reducing the recording overheads by instrumenting only a small amount of call sites while keeping the capability of recovering the full trace. We propose a call trace model and a logged call trace model based on an LL(1) grammar, which enables us to define the criteria of a feasible solution to call trace collection. Based on the two models, we prove that to collect call traces with minimal instrumentation is an NP-hard problem. We then propose an efficient approach to obtaining a suboptimal solution. We implemented our approach as a tool Casper and evaluated it\u00a0\u2026", "num_citations": "15\n", "authors": ["190"]}
{"title": "BugMap: a topographic map of bugs\n", "abstract": " A large and complex software system could contain a large number of bugs. It is desirable for developers to understand how these bugs are distributed across the system, so they could have a better overview of software quality. In this paper, we describe BugMap, a tool we developed for visualizing large-scale bug location information. Taken source code and bug data as the input, BugMap can display bug localizations on a topographic map. By examining the topographic map, developers can understand how the components and files are affected by bugs. We apply this tool to visualize the distribution of Eclipse bugs across components/files. The results show that our tool is effective for understanding the overall quality status of a large-scale system and for identifying the problematic areas of the system.", "num_citations": "14\n", "authors": ["190"]}
{"title": "Estimating gpu memory consumption of deep learning models\n", "abstract": " Deep learning (DL) has been increasingly adopted by a variety of software-intensive systems. Developers mainly use GPUs to accelerate the training, testing, and deployment of DL models. However, the GPU memory consumed by a DL model is often unknown to them before the DL job executes. Therefore, an improper choice of neural architecture or hyperparameters can cause such a job to run out of the limited GPU memory and fail. Our recent empirical study has found that many DL job failures are due to the exhaustion of GPU memory. This leads to a horrendous waste of computing resources and a significant reduction in development productivity. In this paper, we propose DNNMem, an accurate estimation tool for GPU memory consumption of DL models. DNNMem employs an analytic estimation approach to systematically calculate the memory consumption of both the computation graph and the DL\u00a0\u2026", "num_citations": "13\n", "authors": ["190"]}
{"title": "Local Search with Efficient Automatic Configuration for Minimum Vertex Cover.\n", "abstract": " Minimum vertex cover (MinVC) is a prominent NP-hard problem in artificial intelligence, with considerable importance in applications. Local search solvers define the state of the art in solving MinVC. However, there is no single MinVC solver that works best across all types of MinVC instances, and finding the most suitable solver for a given application poses considerable challenges. In this work, we present a new local search framework for MinVC called MetaVC, which is highly parametric and incorporates many effective local search techniques. Using an automatic algorithm configurator, the performance of MetaVC can be optimized for particular types of MinVC instances. Through extensive experiments, we demonstrate that MetaVC significantly outperforms previous solvers on medium-size hard MinVC instances, and shows competitive performance on large MinVC instances. We further introduce a neural-networkbased approach for enhancing the automatic configuration process, by identifying and terminating unpromising configuration runs. Our results demonstrate that MetaVC, when automatically configured using this method, can achieve improvements in the best known solutions for 16 large MinVC instances.", "num_citations": "13\n", "authors": ["190"]}
{"title": "A cost-effectiveness criterion for applying software defect prediction models\n", "abstract": " Ideally, software defect prediction models should help organize software quality assurance (SQA) resources and reduce cost of finding defects by allowing the modules most likely to contain defects to be inspected first. In this paper, we study the cost-effectiveness of applying defect prediction models in SQA and propose a basic cost-effectiveness criterion. The criterion implies that defect prediction models should be applied with caution. We also propose a new metric FN/(FN+ TN) to measure the cost-effectiveness of a defect prediction model.", "num_citations": "13\n", "authors": ["190"]}
{"title": "How incidental are the incidents? characterizing and prioritizing incidents for large-scale online service systems\n", "abstract": " Although tremendous efforts have been devoted to the quality assurance of online service systems, in reality, these systems still come across many incidents (ie, unplanned interruptions and outages), which can decrease user satisfaction or cause economic loss. To better understand the characteristics of incidents and improve the incident management process, we perform the first large-scale empirical analysis of incidents collected from 18 real-world online service systems in Microsoft. Surprisingly, we find that although a large number of incidents could occur over a short period of time, many of them actually do not matter, ie, engineers will not fix them with a high priority after manually identifying their root cause. We call these incidents incidental incidents. Our qualitative and quantitative analyses show that incidental incidents are significant in terms of both number and cost. Therefore, it is important to prioritize\u00a0\u2026", "num_citations": "12\n", "authors": ["190"]}
{"title": "Querying sequential software engineering data\n", "abstract": " We propose a pattern-based approach to effectively and efficiently analyzing sequential software engineering (SE) data. Different from other types of SE data, sequential SE data preserves unique temporal properties, which cannot be easily analyzed without much programming effort. In order to facilitate the analysis of sequential SE data, we design a sequential pattern query language (SPQL), which specifies the temporal properties based on regular expressions, and is enhanced with variables and statements to store and manipulate matching states. We also propose a query engine to effectively process the SPQL queries. We have applied our approach to analyze two types of SE data, namely bug report history and source code change history. We experiment with 181,213 Eclipse bug reports and 323,989 code revisions of Android. SPQL enables us to explore interesting temporal properties underneath these\u00a0\u2026", "num_citations": "12\n", "authors": ["190"]}
{"title": "Sampling program quality\n", "abstract": " Many modern software systems are large, consisting of hundreds or even thousands of programs (source files). Understanding the overall quality of these programs is a resource and time-consuming activity. It is desirable to have a quick yet accurate estimation of the overall program quality in a cost-effective manner. In this paper, we propose a sampling based approach - for a large software project, we only sample a small percentage of source files, and then estimate the quality of the entire programs in the project based on the characteristics of the sample. Through experiments on public defect datasets, we show that we can successfully estimate the total number of defects, proportions of defective programs, defect distributions, and defect-proneness - all from a small sample of programs. Our experiments also show that small samples can achieve similar prediction accuracies as larger samples do.", "num_citations": "12\n", "authors": ["190"]}
{"title": "Codekernel: A graph kernel based approach to the selection of api usage examples\n", "abstract": " Developers often want to find out how to use a certain API (e.g., FileReader.read in JDK library). API usage examples are very helpful in this regard. Over the years, many automated methods have been proposed to generate code examples by clustering and summarizing relevant code snippets extracted from a code corpus. These approaches simplify source code as method invocation sequences or feature vectors. Such simplifications only model partial aspects of the code and tend to yield inaccurate examples. We propose CodeKernel, a graph kernel based approach to the selection of API usage examples. Instead of approximating source code as method invocation sequences or feature vectors, CodeKernel represents source code as object usage graphs. Then, it clusters graphs by embedding them into a continuous space using a graph kernel. Finally, it outputs code examples by selecting a representative\u00a0\u2026", "num_citations": "11\n", "authors": ["190"]}
{"title": "Performance-Influence Model for Highly Configurable Software with Fourier Learning and Lasso Regression\n", "abstract": " Many software systems are highly configurable, which provide a large number of configuration options for users to choose from. During the maintenance and operation of these configurable systems, it is important to estimate the system performance under any specific configurations and understand the performance-influencing configuration options. However, it is often not feasible to measure the system performance under all the possible configurations as the combination of configurations could be exponential. In this paper, we propose PerLasso, a performance modeling and prediction method based on Fourier Learning and Lasso (Least absolute shrinkage and selection operator) regression techniques. Using a small sample of measured performance values of a configurable system, PerLasso produces a performance-influence model, which can 1) predict system performance under a new configuration; 2\u00a0\u2026", "num_citations": "11\n", "authors": ["190"]}
{"title": "Task-oriented design method and research on force compliant experiment of six-axis wrist force sensor\n", "abstract": " This paper analyzes the task-oriented design method of six-axis force sensor and proposes the task model of the sensor. The task mathematical model of the sensor is established based on the idea of task ellipsoid. The models of force ellipsoid and moment ellipsoid are also established. The relational expression between the task model and ellipsoid model of sensor is obtained. Then, a fully pre-stressed dual-layer parallel six-axis wrist force sensor is proposed, whose static mathematical model is also established. The sensor task model for assembly work is proposed and the analytical expression between the sensor structure parameters and task model is deduced. According to the assembly work, the sensor structure is designed specifically, and the specific structure sizes of the sensor are obtained. Then the new sensor prototype manufactured for peg-in-hole assembly is processed. The calibration experiment\u00a0\u2026", "num_citations": "11\n", "authors": ["190"]}
{"title": "Identifying linked incidents in large-scale online service systems\n", "abstract": " In large-scale online service systems, incidents occur frequently due to a variety of causes, from updates of software and hardware to changes in operation environment. These incidents could significantly degrade system\u2019s availability and customers\u2019 satisfaction. Some incidents are linked because they are duplicate or inter-related. The linked incidents can greatly help on-call engineers find mitigation solutions and identify the root causes. In this work, we investigate the incidents and their links in a representative real-world incident management (IcM) system. Based on the identified indicators of linked incidents, we further propose LiDAR (Linked Incident identification with DAta-driven Representation), a deep learning based approach to incident linking. More specifically, we incorporate the textual description of incidents and structural information extracted from historical linked incidents to identify possible links\u00a0\u2026", "num_citations": "10\n", "authors": ["190"]}
{"title": "Automatic discovery and cleansing of numerical metamorphic relations\n", "abstract": " Metamorphic relations (MRs) describe the invariant relationships between program inputs and outputs. By checking for violations of MRs, faults in programs can be detected. Identifying MRs manually is a tedious and error-prone task. In this paper, we propose AutoMR, a novel method for systematically inferring and cleansing MRs. AutoMR can discover various types of equality and inequality MRs through a search method (particle swarm optimization). It also employs matrix singular-value decomposition and constraint solving techniques to remove the redundant MRs in the search results. Our experiments on 37 numerical programs from two popular open source packages show that AutoMR can effectively infer a set of accurate and succinct MRs and outperform the state-of-the-art method. Furthermore, we show that the discovered MRs have high fault detection ability in mutation testing and differential testing.", "num_citations": "9\n", "authors": ["190"]}
{"title": "Poster: Automatically answering api-related questions\n", "abstract": " Automatically recommending API-related tutorial fragments or Q&A pairs from Stack Overflow (SO) is very helpful for developers, especially when they need to use unfamiliar APIs to complete programming tasks. However, in practice developers are more likely to express the API-related questions using natural language when they do not know the exact name of an unfamiliar API. In this paper, we propose an approach, called SOTU, to automatically find answers for API-related natural language questions (NLQs) from tutorials and SO. We first identify relevant API-related tutorial fragments and extract API-related Q&A pairs from SO. We then construct an API-Answer corpus by combining these two sources of information. For an API-related NLQ given by the developer, we parse it into several potential APIs and then retrieve potential answers from the API-Answer corpus. Finally, we return a list of potential results\u00a0\u2026", "num_citations": "9\n", "authors": ["190"]}
{"title": "Detecting infeasible branches based on code patterns\n", "abstract": " Infeasible branches are program branches that can never be exercised regardless of the inputs of the program. Detecting infeasible branches is important to many software engineering tasks such as test case generation and test coverage measurement. Applying full-scale symbolic evaluation to infeasible branch detection could be very costly, especially for a large software system. In this work, we propose a code pattern based method for detecting infeasible branches. We first introduce two general patterns that can characterize the source code containing infeasible branches. We then develop a tool, called Pattern-based method for Infeasible branch Detection (PIND), to detect infeasible branches based on the discovered code patterns. PIND only performs symbolic evaluation for the branches that exhibit the identified code patterns, therefore significantly reduce the number of symbolic evaluations required. We\u00a0\u2026", "num_citations": "9\n", "authors": ["190"]}
{"title": "Integrating software engineering data using semantic web technologies\n", "abstract": " A plethora of software engineering data have been produced by different organizations and tools over time. These data may come from different sources, and are often disparate and distributed. The integration of these data may open up the possibility of conducting systemic, holistic study of software projects in ways previously unexplored. Semantic Web technologies have been used successfully in a wide array of domains such as health care and life sciences as a platform for information integration and knowledge management. The success is largely due to the open and extensible nature of ontology languages as well as growing tool support. We believe that Semantic Web technologies represent an ideal platform for the integration of software engineering data in a semantic repository. By querying and analyzing such a repository, researchers and practitioners can better understand and control software\u00a0\u2026", "num_citations": "9\n", "authors": ["190"]}
{"title": "Bigin4: Instant, interactive insight identification for multi-dimensional big data\n", "abstract": " The ability to identify insights from multi-dimensional big data is important for business intelligence. To enable interactive identification of insights, a large number of dimension combinations need to be searched and a series of aggregation queries need to be quickly answered. The existing approaches answer interactive queries on big data through data cubes or approximate query processing. However, these approaches can hardly satisfy the performance or accuracy requirements for ad-hoc queries demanded by interactive exploration. In this paper, we present BigIN4, a system for instant, interactive identification of insights from multi-dimensional big data. BigIN4 gives insight suggestions by enumerating subspaces and answers queries by combining data cube and approximate query processing techniques. If a query cannot be answered by the cubes, BigIN4 decomposes it into several low dimensional queries\u00a0\u2026", "num_citations": "8\n", "authors": ["190"]}
{"title": "Towards intelligent incident management: why we need it and how we make it\n", "abstract": " The management of cloud service incidents (unplanned interruptions or outages of a service/product) greatly affects customer satisfaction and business revenue. After years of efforts, cloud enterprises are able to solve most incidents automatically and timely. However, in practice, we still observe critical service incidents that occurred in an unexpected manner and orchestrated diagnosis workflow failed to mitigate them. In order to accelerate the understanding of unprecedented incidents and provide actionable recommendations, modern incident management system employs the strategy of AIOps (Artificial Intelligence for IT Operations). In this paper, to provide a broad view of industrial incident management and understand the modern incident management system, we conduct a comprehensive empirical study spanning over two years of incident management practices at Microsoft. Particularly, we identify two\u00a0\u2026", "num_citations": "7\n", "authors": ["190"]}
{"title": "Intelligent Virtual Machine Provisioning in Cloud Computing.\n", "abstract": " Virtual machine (VM) provisioning is a common and critical problem in cloud computing. In industrial cloud platforms, there are a huge number of VMs provisioned per day. Due to the complexity and resource constraints, it needs to be carefully optimized to make cloud platforms effectively utilize the resources. Moreover, in practice, provisioning a VM from scratch requires fairly long time, which would degrade the customer experience. Hence, it is advisable to provision VMs ahead for upcoming demands. In this work, we formulate the practical scenario as the predictive VM provisioning (PreVMP) problem, where upcoming demands are unknown and need to be predicted in advance, and then the VM provisioning plan is optimized based on the predicted demands. Further, we propose Uncertainty-Aware Heuristic Search (UAHS) for solving the PreVMP problem. UAHS first models the prediction uncertainty, and then utilizes the prediction uncertainty in optimization. Moreover, UAHS leverages Bayesian optimization to interact prediction and optimization to improve its practical performance. Extensive experiments show that UAHS performs much better than state-of-the-art competitors on two public datasets and an industrial dataset. UAHS has been successfully applied in Microsoft Azure and brought practical benefits in real-world applications.", "num_citations": "7\n", "authors": ["190"]}
{"title": "Developer recommendation for Topcoder through a meta-learning based policy model\n", "abstract": " Crowdsourcing Software Development (CSD) has emerged as a new software development paradigm. Topcoder is now the largest competition-based CSD platform. Many organizations use Topcoder to outsource their software tasks to crowd developers in the form of open challenges. To facilitate timely completion of the crowdsourced tasks, it is important to find right developers who are more likely to win a challenge. Recently, many developer recommendation methods for CSD platforms have been proposed. However, these methods often make unrealistic assumptions about developer status or application scenarios. For example, they consider only skillful developers or only developers registered with the challenges. In this paper, we propose a meta-learning based policy model, which firstly filters out those developers who are unlikely to participate in or submit to a given challenge and then recommend\u00a0\u2026", "num_citations": "7\n", "authors": ["190"]}
{"title": "Neural feature search: A neural architecture for automated feature engineering\n", "abstract": " Feature engineering is a crucial step for developing effective machine learning models. Traditionally, feature engineering is performed manually, which requires much domain knowledge and is time-consuming. In recent years, many automated feature engineering methods have been proposed. These methods improve the accuracy of a machine learning model by automatically transforming the original features into a set of new features. However, existing methods either lack ability to perform high-order transformations or suffer from the feature space explosion problem. In this paper, we present Neural Feature Search (NFS), a novel neural architecture for automated feature engineering. We utilize a recurrent neural network based controller to transform each raw feature through a series of transformation functions. The controller is trained through reinforcement learning to maximize the expected performance of the\u00a0\u2026", "num_citations": "7\n", "authors": ["190"]}
{"title": "Towards more efficient meta-heuristic algorithms for combinatorial test generation\n", "abstract": " Combinatorial interaction testing (CIT) is a popular approach to detecting faults in highly configurable software systems. The core task of CIT is to generate a small test suite called a t-way covering array (CA), where t is the covering strength. Many meta-heuristic algorithms have been proposed to solve the constrained covering array generating (CCAG) problem. A major drawback of existing algorithms is that they usually need considerable time to obtain a good-quality solution, which hinders the wider applications of such algorithms. We observe that the high time consumption of existing meta-heuristic algorithms for CCAG is mainly due to the procedure of score computation. In this work, we propose a much more efficient method for score computation. The score computation method is applied to a state-of-the-art algorithm TCA, showing significant improvements. The new score computation method opens a way to\u00a0\u2026", "num_citations": "7\n", "authors": ["190"]}
{"title": "Automated Extraction of Data Lifecycle Support from Database Applications.\n", "abstract": " Database application is one of the most common types of systems. Grounded on the simple concept of data lifecycle\u2014any data in database is created from insertion, used via selection and modification and terminated at deletion\u2014this paper proposes a novel approach to reverse engineer the data lifecycle automatically from the source code of database applications. The extracted information can be used for the selection of opensource database applications for adaptation. It can also be used for maintenance and verification of database applications. A tool has been developed to implement the proposed approach for PHP-based database applications. Case studies have also been conducted to evaluate the use of the proposed approach.", "num_citations": "7\n", "authors": ["190"]}
{"title": "Verify Feature Models using protegeOWL\n", "abstract": " Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a widely-adopted means of precisely representing and formally verifying feature models has hindered the development of this area. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web ontologies.", "num_citations": "7\n", "authors": ["190"]}
{"title": "Efficient incident identification from multi-dimensional issue reports via meta-heuristic search\n", "abstract": " In large-scale cloud systems, unplanned service interruptions and outages may cause severe degradation of service availability. Such incidents can occur in a bursty manner, which will deteriorate user satisfaction. Identifying incidents rapidly and accurately is critical to the operation and maintenance of a cloud system. In industrial practice, incidents are typically detected through analyzing the issue reports, which are generated over time by monitoring cloud services. Identifying incidents in a large number of issue reports is quite challenging. An issue report is typically multi-dimensional: it has many categorical attributes. It is difficult to identify a specific attribute combination that indicates an incident. Existing methods generally rely on pruning-based search, which is time-consuming given high-dimensional data, thus not practical to incident detection in large-scale cloud systems. In this paper, we propose MID (Multi\u00a0\u2026", "num_citations": "5\n", "authors": ["190"]}
{"title": "Fault-tolerant parallel six-component force sensor\n", "abstract": " During some key force measuring tasks in  the harsh environment such as aerospace, national defense, deep-sea exploration, higher working reliability is required. The design concept of fault-tolerance is proposed in the design of parallel six-component force sensor. Two kinds of fault-tolerant parallel structure force sensor without pre-stressing platform and two kinds of fault-tolerant and fully pre-stressed parallel structure force sensor using modified spherical pairs, together with another three kinds of wheel-spoke fault-tolerant force sensor are proposed. Compared with traditional Stewart platform-based sensor, the structural characteristics of the sensors are analyzed. The complete mathematic models including the models before and after signal fault are established by using theory of screw and the relations between stiffness and force distribution. The unified mathematic model is suitable for all the fault\u00a0\u2026", "num_citations": "5\n", "authors": ["190"]}
{"title": "Improving failure detection by automatically generating test cases near the boundaries\n", "abstract": " Boundary value analysis is a typical conventional testing technique. However, manually identifying input regions and writing test cases are labor-intensive and time-consuming. In this paper, we propose a search-based random testing approach, which automatically generates test data along the boundaries of semantic regions of the input domain. The experiments on mutated programs confirm the effectiveness and efficiency of the proposed approach. Furthermore, our approach significantly outperforms the conventional ART (Adaptive Random Testing) methods, which sample test cases evenly across the input regions. Our approach also outperforms EvoSuite, a state-of-the-art tool that generates test cases satisfying certain coverage criterion.", "num_citations": "5\n", "authors": ["190"]}
{"title": "Method and system for detecting infeasible paths\n", "abstract": " A method of testing a software program comprises obtaining path properties of an infeasible path, selecting a path from the software program and obtaining path properties of the selected path, wherein the method further comprises comparing path properties of the selected path to the path properties of the infeasible path to identify a target path and determine infeasibility of the target path.", "num_citations": "5\n", "authors": ["190"]}
{"title": "Anomaly detection via mining numerical workflow relations from logs\n", "abstract": " Complex software-intensive systems, especially distributed systems, generate logs for troubleshooting. The logs are text messages recording system events, which can help engineers determine the system\u2019s runtime status. This paper proposes a novel approach named ADR (stands for Anomaly Detection by workflow Relations), which employs matrix nullspace to mine numerical relations from log data. The mined relations can be used for both offline and online anomaly detection and facilitate fault diagnosis. We have evaluated ADR on log data collected from two distributed systems. ADR successfully mined 87 and 669 numerical relations from the logs and used them to detect anomalies with high precision and recall. For online anomaly detection, ADR employs PSO (Particle Swarm Optimization) to find the optimal sliding windows\u2019 size and achieves fast anomaly detection. The experimental results confirm that\u00a0\u2026", "num_citations": "4\n", "authors": ["190"]}
{"title": "Data\u2010driven approach to application programming interface documentation mining: A review\n", "abstract": " Application programming interface (API) is an important form of software reuse. API documentations, such as API specifications, tutorials, and online forums, are valuable learning resources for reusing the APIs. In recent years, many data\u2010driven API documentation mining (ADM) methods have been proposed. These methods mine API documentations and return API\u2010related information to help developers better understand and reuse APIs. These methods treat documentations as unstructured data and apply various data mining techniques to analyze the documentation data. Currently, there is no comprehensive review of the data\u2010driven approach to API documentation mining. This review aims to fill in this gap by analyzing and discussing the state of the art ADM papers. We survey 32 representative papers published in prominent software engineering journals and conferences in recent 5\u2009years (January 2014\u00a0\u2026", "num_citations": "4\n", "authors": ["190"]}
{"title": "Learning to handle exceptions\n", "abstract": " Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically\u00a0\u2026", "num_citations": "3\n", "authors": ["190"]}
{"title": "ABOR: an automatic framework for buffer overflow removal in c/c++ programs\n", "abstract": " Buffer overflow vulnerability is one of the commonly found significant security vulnerabilities. This vulnerability may occur if a program does not sufficiently prevent input from exceeding intended size and accessing unintended memory locations. Researchers have put effort in different directions to address this vulnerability. How, authorized reports and data showed that as more sophisticated attack vectors are being discovered, efforts on a single direction are not sufficient to resolve this critical issue well. In this paper, we characterize buffer overflow vulnerability in four patterns and propose ABOR, a framework to remove buffer overflow vulnerabilities from source code automatically. It only patches identified code segments, which means it is an optimized solution that eliminates buffer overflows at the maximum while adds runtime overhead at the minimum. We have implemented the proposed approach and\u00a0\u2026", "num_citations": "3\n", "authors": ["190"]}
{"title": "Detection of buffer overflow vulnerabilities in C/C++ with pattern based limited symbolic evaluation\n", "abstract": " Buffer overflow vulnerability is one of the major security threats for applications written in C/C++. Among the existing approaches for detecting buffer overflow vulnerability, though flow sensitive based approaches offer higher precision but they are limited by heavy overhead and the fact that many constraints are unsolvable. We propose a novel method to efficiently detect vulnerable buffer overflows in any given control flow graph through recognizing two patterns. The proposed approach first uses syntax analysis to filter away those branches that cannot possibly comply with any of the two patterns before applying a limited symbolic evaluation for a precise matching against the patterns. The proposed approach only needs to evaluate a limited set of selected branch predicates according to the patterns and avoids the need to deal with a large number of general branch predicates. This significantly improves the\u00a0\u2026", "num_citations": "3\n", "authors": ["190"]}
{"title": "CRaDLe: Deep code retrieval based on semantic Dependency Learning\n", "abstract": " Code retrieval is a common practice for programmers to reuse existing code snippets in the open-source repositories. Given a user query (i.e., a natural language description), code retrieval aims at searching the most relevant ones from a set of code snippets. The main challenge of effective code retrieval lies in mitigating the semantic gap between natural language descriptions and code snippets. With the ever-increasing amount of available open-source code, recent studies resort to neural networks to learn the semantic matching relationships between the two sources. The statement-level dependency information, which highlights the dependency relations among the program statements during the execution, reflects the structural importance of one statement in the code, which is favorable for accurately capturing the code semantics but has never been explored for the code retrieval task. In this paper, we\u00a0\u2026", "num_citations": "2\n", "authors": ["190"]}
{"title": "Fast Outage Analysis of Large-scale Production Clouds with Service Correlation Mining\n", "abstract": " Cloud-based services are surging into popularity in recent years. However, outages, i.e., severe incidents that always impact multiple services, can dramatically affect user experience and incur severe economic losses. Locating the root-cause service, i.e., the service that contains the root cause of the outage, is a crucial step to mitigate the impact of the outage. In current industrial practice, this is generally performed in a bootstrap manner and largely depends on human efforts: the service that directly causes the outage is identified first, and the suspected root cause is traced back manually from service to service during diagnosis until the actual root cause is found. Unfortunately, production cloud systems typically contain a large number of interdependent services. Such a manual root cause analysis is often time-consuming and labor-intensive. In this work, we propose COT, the first outage triage approach that\u00a0\u2026", "num_citations": "2\n", "authors": ["190"]}
{"title": "PULNS: Positive-Unlabeled Learning with Effective Negative Sample Selector\n", "abstract": " Positive-unlabeled learning (PU learning) is an important case of binary classification where the training data only contains positive and unlabeled samples. The current state-of-the-art approach for PU learning is the cost-sensitive approach, which casts PU learning as a cost-sensitive classification problem and relies on unbiased risk estimator for correcting the bias introduced by the unlabeled samples. However, this approach requires the knowledge of class prior and is subject to the potential label noise. In this paper, we propose a novel PU learning approach dubbed PULNS, equipped with an effective negative sample selector, which is optimized by reinforcement learning. Our PULNS approach employs an effective negative sample selector as the agent responsible for selecting negative samples from the unlabeled data. While the selected, likely negative samples can be used to improve the classifier, the performance of classifier is also used as the reward to improve the selector through the REINFORCE algorithm. By alternating the updates of the selector and the classifier, the performance of both is improved. Extensive experimental studies on 7 real-world application benchmarks demonstrate that PULNS consistently outperforms the current state-of-the-art methods in PU learning, and our experimental results also confirm the effectiveness of the negative sample selector underlying PULNS.", "num_citations": "2\n", "authors": ["190"]}
{"title": "Correlation-Aware Heuristic Search for Intelligent Virtual Machine Provisioning in Cloud Systems\n", "abstract": " The optimization of resource is crucial for the operation of public cloud systems such as Microsoft Azure, as well as servers dedicated to the workloads of large customers such as Microsoft 365. Those optimization tasks often need to take unknown parameters into consideration and can be formulated as Prediction+ Optimization problems. This paper proposes a new Prediction+ Optimization method named Correlation-Aware Heuristic Search (CAHS) that is capable of accounting for the uncertainty in unknown parameters and delivering effective solutions to difficult optimization problems. We apply this method to solving the predictive virtual machine (VM) provisioning (PreVMP) problem, where the VM provisioning plans are optimized based on the predicted demands of different VM types, to ensure rapid provisions upon customers' requests and to pursue high resource utilization. Unlike the current state-of-the-art PreVMP approaches that assume independence among the demands for different VM types, CAHS incorporates demand correlation when conducting prediction and optimization in a novel and effective way. Our experiments on two public benchmarks and one industrial benchmark demonstrate that CAHS can achieve better performance than its nine state-of-the-art competitors. CAHS has been successfully deployed in Microsoft Azure and significantly improved its performance. The main ideas of CAHS have also been leveraged to improve the efficiency and the reliability of the cloud services provided by Microsoft 365.", "num_citations": "2\n", "authors": ["190"]}
{"title": "POSTER: Live path control flow integrity\n", "abstract": " Per-Input Control Flow Integrity (PICFI) represents a recent advance in dynamic CFI techniques. PICFI starts with the empty CFG of a program and lazily adds edges to the CFG during execution according to concrete inputs. However, this CFG grows monotonically, i.e., invalid edges are never removed when corresponding control flow transfers (via indirect calls) become illegal (i.e., will never be executed again). This paper presents LPCFI, Live Path Control Flow Integrity, to more precisely enforce forward edge CFI using a dynamically computed CFG by both adding and removing edges for all indirect control flow transfers from function pointer calls, thereby raising the bar against control flow hijacking attacks.", "num_citations": "2\n", "authors": ["190"]}
{"title": "How to tame your online services\n", "abstract": " Online service systems have become increasingly popular and important. Service incidents can lead to huge economic loss. We designed a set of incident management techniques based on the analysis of a huge amount of data collected at service runtime. Our tool is called Service Analysis Studio (SAS), which has been successfully applied to large-scale online services provided by Microsoft.", "num_citations": "2\n", "authors": ["190"]}
{"title": "Feature Reasoning\u2013Scaling Up and Pinning Down\n", "abstract": " Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a formal semantics and reasoning support of feature models has hindered the development of this area. There is an increasing need for methods and tools that can support feature model analysis. Ideally, such reasoning tool should be fully automated, expressive and helpful in locating inconsistencies. At the same time, the reasoning tool should scale up well since it may need to handle hundreds or even thousands of features a that modern software systems may have. In this paper, we propose to use existing software engineering techniques and tools, ie, Z/EVES and Alloy Analyzer, complemented with novel Semantic Web ontology tools such as FaCT++ to check feature models. In this approach, FaCT++ is firstly used to identify any inconsistencies in a given feature model. Then the origins of the inconsistencies can be traced by Alloy Analyzer. Finally Z/EVES is used again to express complex non-standard feature relationship and to provide some advanced reasoning service. We have successfully applied this approach to a large and complicated feature model comprising 1000 features. A number of configurations have been checked and inconsistency have been detected and traced.", "num_citations": "2\n", "authors": ["190"]}
{"title": "Generating API tags for tutorial fragments from Stack Overflow\n", "abstract": " API tutorials are important learning resources as they explain how to use certain APIs in a given programming context. An API tutorial can be split into a number of units. Consecutive units that describe a same topic are often called a tutorial fragment. We consider the API explained by a tutorial fragment as an API tag. Generating API tags for a tutorial fragment can help understand, navigate, and retrieve the fragment. Existing approaches often do not perform well on API tag generation due to high manual effort and low accuracy. Like API tutorials, Stack Overflow (SO) is also an important learning resource that provides the explanations of APIs. Thus, SO posts also contain API tags. Besides, API tags of SO posts are abundant and can be extracted easily. In this paper, we propose a novel approach ATTACK (stands for A PI T ag for T utorial frA gments using C rowd K nowledge), which can automatically generate API\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "Efficient Compiler Autotuning via Bayesian Optimization\n", "abstract": " A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "AutoCCAG: An Automated Approach to Constrained Covering Array Generation\n", "abstract": " Combinatorial interaction testing (CIT) is an important technique for testing highly configurable software systems with demonstrated effectiveness in practice. The goal of CIT is to generate test cases covering the interactions of configuration options, under certain hard constraints. In this context, constrained covering arrays (CCAs) are frequently used as test cases in CIT. Constrained Covering Array Generation (CCAG) is an NP-hard combinatorial optimization problem, solving which requires an effective method for generating small CCAs. In particular, effectively solving t-way CCAG with  is even more challenging. Inspired by the success of automated algorithm configuration and automated algorithm selection in solving combinatorial optimization problems, in this paper, we investigate the efficacy of automated algorithm configuration and automated algorithm selection for the CCAG problem, and propose a\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "Leveraging Stack Overflow to Detect Relevant Tutorial Fragments of APIs\n", "abstract": " Developers often use learning resources such as API tutorials and Stack Overflow (SO) to learn how to use an unfamiliar API. An API tutorial can be divided into a number of consecutive units that describe the same topic, denoted as tutorial fragments. We consider a tutorial fragment explaining the API usage knowledge as a relevant fragment of the API. Discovering relevant tutorial fragments of APIs can facilitate API understanding and learning. However, existing approaches, based on supervised or unsupervised approaches, often suffer from either high manual efforts or lack of consideration of the relevance information. In this paper, we propose a novel approach, called SO2RT, to detect relevant tutorial fragments of APIs based on SO posts. SO2RT first automatically extracts relevant and irrelevant <API,QA> pairs based on heuristic rules of SO, and constructs <API, FRA> pairs (FRA stands out fragment) by using\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "Runtime Performance Prediction for Deep Learning Models with Graph Neural Network\n", "abstract": " Recently, deep learning (DL) has been widely adopted in many application domains. Predicting the runtime performance of DL models such as GPU memory consumption and training time is important to boost development productivity and reduce resource waste because improper configurations of hyperparameters and neural architectures can result in many failed training jobs or inappropriate models. However, general runtime performance prediction for DL models is challenging due to the hybrid DL programming paradigm, complicated hidden factors within the framework runtime, fairly huge model configuration space, and wide differences among models. In this paper, we propose DNNPerf, a novel and general machine learning approach to predict the runtime performance of DL models using Graph Neural Network. DNNPerf represents a DL model as a directed acyclic computation graph and designs a rich set of effective performance-related features based on the computational semantics of both nodes and edges. We also propose a new Attention-based Node-Edge Encoder to better encode the node and edge features. DNNPerf is extensively evaluated on thousands of configurations of real-world and synthetic DL models to predict their GPU memory consumption and training time. The experimental results demonstrate that DNNPerf achieves an overall error of 13.684% for the GPU memory consumption prediction and an overall error of 7.443% for the training time prediction, outperforming all the compared methods.", "num_citations": "1\n", "authors": ["190"]}
{"title": "Cost-Effective Testing of a Deep Learning Model through Input Reduction\n", "abstract": " With the increasing adoption of Deep Learning (DL) models in various applications, testing DL models is vitally important. However, testing DL models is costly and expensive, e.g., manual labelling is widely-recognized to be costly. To reduce testing cost, we propose to select only a subset of testing data, which is small but representative enough for a quick estimation of the performance of DL models. Our approach, DeepReduce, adopts a two-phase strategy. At first, our approach selects testing data for the purpose of satisfying testing adequacy. Then, it selects more testing data to approximate the distribution between the whole testing data and the selected data by leveraging relative entropy minimization. We evaluate DeepReduce on four widely-used datasets (with 15 models in total). We find that DeepReduce reduces the whole testing data to 7.5% on average and can reliably estimate the performance of DL\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "Ares: inferring error specifications through static analysis\n", "abstract": " Misuse of APIs happens frequently due to misunderstanding of API semantics and lack of documentation. An important category of API-related defects is the error handling defects, which may result in security and reliability flaws. These defects can be detected with the help of static program analysis, provided that error specifications are known. The error specification of an API function indicates how the function can fail. Writing error specifications manually is time-consuming and tedious. Therefore, automatic inferring the error specification from API usage code is preferred. In this paper, we present Ares, a tool for automatic inferring error specifications for C code through static analysis. We employ multiple heuristics to identify error handling blocks and infer error specifications by analyzing the corresponding condition logic. Ares is evaluated on 19 real world projects, and the results reveal that Ares outperforms the\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "Live path cfi against control flow hijacking attacks\n", "abstract": " Through memory vulnerabilities, control flow hijacking allows an attacker to force a running program to execute other than what the programmer has intended. Control Flow Integrity (CFI) aims to prevent the adversarial effects of these attacks. CFI attempts to enforce the programmer\u2019s intent by ensuring that a program only runs according to a control flow graph (CFG) of the program. The enforced CFG can be built statically or dynamically, and Per-Input Control Flow Integrity (PICFI) represents a recent advance in dynamic CFI techniques. PICFI begins execution with the empty CFG of a program and lazily adds edges to the CFG during execution according to concrete inputs. However, this CFG grows monotonically, i.e., edges are never removed when corresponding control flow transfers become illegal. This paper presents LPCFI, Live Path Control Flow Integrity, to more precisely enforce forward edge CFI using a\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "Visual analytics for software engineering data\n", "abstract": " Many data analysis techniques require substantial knowledge and skills and are typically performed by \u201cdata scientists\u201d. Ordinary users may find it difficult to apply these techniques to quickly explore the data by themselves. We propose MetroEyes, a visual analytics tool for interactive data exploration. We have successfully transferred the main concepts and experiences of MetroEyes to Microsoft Power BI.", "num_citations": "1\n", "authors": ["190"]}
{"title": "Proceedings of the 2nd International Workshop on Software Mining\n", "abstract": " \"Proceedings of the 2nd International Workshop on Software Mining\" by Ming Li, Hongyu Zhang et al. Home Search Browse Collections My Account About DC Network Digital Commons Network\u2122 Skip to main content Institutional Knowledge at Singapore Management University Singapore Management University Singapore Management University Libraries Home About FAQ My Account Home > Schools > SIS > SIS_RESEARCH > 1995 Research Collection School Of Information Systems Proceedings of the 2nd International Workshop on Software Mining Ming Li Hongyu Zhang David LO, Singapore Management University Co-located with the 28th IEEE/ACM International Conference on Automated Software Engineering (ASE) Abstract This paper has been withdrawn. Search Enter search terms: Advanced Search Notify me via email or RSS Links SMU Libraries Office of Research Singapore Management Browse | \u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "New generation of software metrics\n", "abstract": " Software measurement has always been an issue in software engineering. On one hand, engineering is about designing and building things and modeling, characterizing, monitoring, evaluating, defining, predicting,\u201cprescripting\u201d, controlling, and changing processes and their artifacts; additionally, measurement is essential to verify that the built artifacts comply with their requirements, to validate the product built versus its requirements and design and to keep the production process in control; the old management adage \u201cyou can\u2019t manage what you don\u2019t measure\u201d is valid also in software development. On the other hand, because software is nonmaterial, it is a very difficult beast to measure. The compliance with requirements, which can vary greatly from system to system even in the same domain, is something difficult to \u201cmeasure\u201d, and it is almost impossible to devise metrics able to support such a measure that is repeatable across different systems. The development effort is perhaps easier to estimate and define across different software technologies and domains\u2014after all it is always a matter of money\u2014but it is still very difficult to measure in the ever changing languages, environments, and technologies.The difficulty of measuring software is even more critical in present times due to the increasing size and criticality of software within all technological systems, and the frequent changes in architectural paradigms and process models. New development methods, such as agile and lean approaches, the large diffusion of Open Source Software (OSS) products, service oriented architectures, cloud computing and softwareas-a-service business\u00a0\u2026", "num_citations": "1\n", "authors": ["190"]}
{"title": "Evaluating Product Line Technologies: A Graph Product Line Case Study\n", "abstract": " In recent years the software product line approach has emerged as a promising way to improve software productivity and quality. Many technologies, such as GenVoca, XVCL and Template Metaprogramming, have been proposed to develop reusable product line assets. An extensive evaluation is required in order to understand the relative strengths and weaknesses among these technologies. Lopez-Herrejon and Batory proposed the Graph Product Line (GPL) as a standard problem for evaluating product line technologies [LHB01]. In this paper, we describe the development of the Graph Product Line using XVCL. We then compare our solution with the GenVoca solution presented by Lopez-Herrejon and Batory. We perform an evaluation between GenVoca and XVCL based on our experiments.", "num_citations": "1\n", "authors": ["190"]}