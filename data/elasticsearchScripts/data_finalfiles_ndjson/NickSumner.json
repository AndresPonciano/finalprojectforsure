{"title": "Needle: Leveraging program analysis to analyze and extract accelerators from whole programs\n", "abstract": " Technology constraints have increasingly led to the adoption of specialized coprocessors, i.e. hardware accelerators. The first challenge that computer architects encounter is identifying \u201cwhat to specialize in the program\u201d. We demonstrate that this requires precise enumeration of program paths based on dynamic program behavior. We hypothesize that path-based [4] accelerator offloading leads to good coverage of dynamic instructions and improve energy efficiency. Unfortunately, hot paths across programs demonstrate diverse control flow behavior. Accelerators (typically based on dataflow execution), often lack an energy-efficient, complexity effective, and high performance (eg. branch prediction) support for control flow. We have developed NEEDLE, an LLVM based compiler framework that leverages dynamic profile information to identify, merge, and offload acceleratable paths from whole applications\u00a0\u2026", "num_citations": "15\n", "authors": ["1844"]}
{"title": "Pardis: Priority Aware Test Case Reduction\n", "abstract": " Test cases play an important role in testing and debugging software. Smaller tests are easier to understand and use for these tasks. Given a test that demonstrates a bug, test case reduction finds a smaller variant of the test case that exhibits the same bug. Classically, one of the challenges for test case reduction is that the process is slow, often taking hours. For hierarchically structured inputs like source code, the state of the art is Perses, a recent grammar aware and queue driven approach for test case reduction. Perses traverses nodes in the abstract syntax tree (AST) of a program (test case) based on a priority order and tries to reduce them while preserving syntactic validity. In this paper, we show that Perses\u2019 reduction strategy suffers from priority inversion, where significant time may be spent trying to perform reduction operations on lower priority portions of the AST. We show that this adversely affects the\u00a0\u2026", "num_citations": "7\n", "authors": ["1844"]}
{"title": "Peruse and profit: Estimating the accelerability of loops\n", "abstract": " There exist a multitude of execution models available today for a developer to target. The choices vary from general purpose processors to fixed-function hardware accelerators with a large number of variations in-between. There is a growing demand to assess the potential benefits of porting or rewriting an application to a target architecture in order to fully exploit the benefits of performance and/or energy efficiency offered by such targets. However, as a first step of this process, it is necessary to determine whether the application has characteristics suitable for acceleration.", "num_citations": "5\n", "authors": ["1844"]}
{"title": "Nachos: Software-driven hardware-assisted memory disambiguation for accelerators\n", "abstract": " Hardware accelerators have relied on the compiler to extract instruction parallelism but may waste significant energy in enforcing memory ordering and discovering memory parallelism. Accelerators tend to either serialize memory operations [43] or reuse power hungry load-store queues (LSQs) [8], [27]. Recent works [11], [15] use the compiler for scheduling but continue to rely on LSQs for memory disambiguation. NACHOS is a hardware assisted software-driven approach to memory disambiguation for accelerators. In NACHOS, the compiler classifies pairs of memory operations as NO alias (i.e., independent memory operations), MUST alias (i.e., ordering required), or MAY alias (i.e., compiler uncertain). We developed a compiler-only approach called NACHOS-SW that serializes memory operations both when the compiler is certain (MUST alias) and uncertain (MAY alias). Our study analyzes multiple stages of\u00a0\u2026", "num_citations": "4\n", "authors": ["1844"]}
{"title": "Spec-ax and parsec-ax: Extracting accelerator benchmarks from microprocessor benchmarks\n", "abstract": " The end of Dennard Scaling has necessitated research into the adoption of specialized architectures for offloading specific code regions in applications. Recent works in accelerator architectures have chosen diverse workloads and even diverse code regions (within the same workload) to highlight the efficacy of specific accelerator architectures. However this makes it challenging to evaluate the power/performance benefits of each accelerator. It is unclear in the era of specialization whether it will be feasible to standardize a new set of kernels across different architectural ideas. We present an alternative vision where we identify and prepare \"acceleratable\" code regions from existing CPU-based benchmark suites that are widely used. We identify acceleratable paths by leveraging program analysis [1] to precisely identify directed acyclic paths that are frequently executed. We reconstruct the paths into a separate\u00a0\u2026", "num_citations": "4\n", "authors": ["1844"]}
{"title": "Avoiding the familiar to speed up test case reduction\n", "abstract": " Delta Debugging is a longstanding approach to automated test case reduction. It divides an input into chunks and attempts to remove them to produce a smaller input. When a chunk is successfully removed, all chunks are revisited, as they may become removable from the smaller input. When no chunk can be removed, the chunks are subdivided and the process continues recursively. In the worst case, this revisiting behavior has an O(n^2) running time. We explore the possibility that good test case reduction can be achieved without revisiting, yielding an O(n) algorithm. We identify three independent conditions that can make this reasonable in practice and validate the hypothesis on a suite of user-reported and fuzzer-generated test cases. Results show that on a suite of large fuzzer-generated test cases for compilers, our O(n) approach yields reduced test cases with similar size, while decreasing the reduction time\u00a0\u2026", "num_citations": "3\n", "authors": ["1844"]}
{"title": "Bitwise data parallelism with llvm: The icgrep case study\n", "abstract": " Bitwise data parallelism using short vector (SIMD) instructions has recently been shown to have considerable promise as the basis for a new, fundamentally parallel, style of regular expression processing. This paper examines the application of this approach to the development a full-featured Unicode-capable open-source grep implementation. Constructed using a layered architecture combining Parabix and LLVM compiler technologies, icGrep is the first instance of a potentially large class of text processing applications that achieve high performance text processing through the combination of dynamic compilation and bitwise data parallelism. In performance comparisons with several contemporary alternatives, 10 or better speedups are often observed.", "num_citations": "3\n", "authors": ["1844"]}
{"title": "Automated failure explanation through execution comparison\n", "abstract": " When fixing a bug in software, developers must build an understanding or explanation of the bug and how the bug flows through a program. The effort that developers must put into building this explanation is costly and laborious. Thus, developers need tools that can assist them in explaining the behavior of bugs. Dynamic slicing is one technique that can effectively show how a bug propagates through an execution up to the point where a program fails. However, dynamic slices are large because they do not just explain the bug itself; they include extra information that explains any observed behavior that might be connected to the bug. Thus, the explanation of the bug is hidden within this other tangentially related information. This dissertation addresses the problem and shows how a failing execution and a correct execution may be compared in order to construct explanations that include only information about\u00a0\u2026", "num_citations": "1\n", "authors": ["1844"]}