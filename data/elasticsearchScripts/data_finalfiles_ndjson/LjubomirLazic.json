{"title": "Cost effective software test metrics\n", "abstract": " This paper discusses software test metrics and their ability to show objective evidence necessary to make process improvements in a development organization. When used properly, test metrics assist in the improvement of the software development process by providing pragmatic, objective evidence of process change initiatives. This paper also describes several test metrics that can be implemented, a method for creating a practical approach to tracking & interpreting the metrics, and illustrates one organization\u2019s use of test metrics to prove the effectiveness of process changes. Also, this paper provides the Balanced Productivity Metrics (BPM) strategy and approach in order to design and produce useful project metrics from basic test planning and defect data. Software test metrics is a useful for test managers, which aids in precise estimation of project effort, addresses the interests of metric group, software managers of the software organization who are interested in estimating software test effort and improve both development and testing processes.", "num_citations": "107\n", "authors": ["1226"]}
{"title": "Applying simulation and design of experiments to the embedded software testing process\n", "abstract": " This paper presents some original solutions with regard to the deployment of the U.S. Department of Defense Simulation, Test and Evaluation Process (DoD STEP), using an automated target tracking radar system as a case study. Besides the integration of modelling and simulation, to form a model\u2010based approach to the software testing process, the number of experiments, i.e. test cases, have been dramatically reduced by applying an optimized design\u2010of\u2010experiment plan and an orthogonal array\u2010based robust testing methodology. Also, computer\u2010based simulation at various abstraction levels of the system/software under test can serve as a test oracle. Simulation\u2010based (stochastic) experiments, combined with optimized design\u2010of\u2010experiment plans, in the case study have shown a minimum productivity increase of 100 times in comparison to current practice without DoD STEP deployment. Copyright \u00a9 2004\u00a0\u2026", "num_citations": "64\n", "authors": ["1226"]}
{"title": "Orthogonal Array application for optimal combination of software defect detection techniques choices\n", "abstract": " In this paper, we consider a problem that arises in black box testing: generating small test suites (ie, sets of test cases) where the combinations that have to be covered are specified by input-output parameter relationships of a software system. That is, we only consider combinations of input parameters that affect an output parameter, and we do not assume that the input parameters have the same number of values. To solve this problem, we propose interaction testing, particularly an Orthogonal Array Testing Strategy (OATS) as a systematic, statistical way of testing pair-wise interactions. In software testing process (STP), it provides a natural mechanism for testing systems to be deployed on a variety of hardware and software configurations. The combinatorial approach to software testing uses models to generate a minimal number of test inputs so that selected combinations of input values are covered. The most common coverage criteria are two-way or pairwise coverage of value combinations, though for higher confidence three-way or higher coverage may be required. This paper presents some examples of software-system test requirements and corresponding models for applying the combinatorial approach to those test requirements. The method bridges contributions from mathematics, design of experiments, software test, and algorithms for application to usability testing. Also, this study presents a brief overview of the response surface methods (RSM) for computer experiments available in the literature. The Bayesian approach and orthogonal arrays constructed for computer experiments (OACE) were briefly discussed. An example, of a novel\u00a0\u2026", "num_citations": "42\n", "authors": ["1226"]}
{"title": "Experimental analysis of picture quality after compression by different methods\n", "abstract": " In this paper we present experimental results comparing the quality of still Black & White (B/W) images compressed using four methods: JPEG, JPEG2000, EZW and SPIHT. The compression was performed on three pictures with differing levels of detail and density (bit-rates-bpp) using VCDemo software. The quality of the compressed pictures is determined by values of MSE, SNR and PSNR. The values are presented in appropriate tables and diagrams. By comparing the values obtained, we have found the methods that give best results depending on the picture bitrate and level of detail.", "num_citations": "27\n", "authors": ["1226"]}
{"title": "Software testing optimization by advanced quantitative defect management\n", "abstract": " Software testing provides a means to reduce errors, cut maintenance and overall software costs. Numerous software development and testing methodologies, tools, and techniques have emerged over the last few decades promising to enhance software quality. While it can be argued that there has been some improvement it is apparent that many of the techniques and tools are isolated to a specific lifecycle phase or functional area. This paper presents a set of best practice models and techniques integrated in optimized and quantitatively managed software testing process (OptimalSQM), expanding testing throughout the SDLC. Further, we explained how can Quantitative Defect Management Model be enhanced to be practically useful for determining which activities need to be addressed to improve the degree of early and cost-effective software fault detection with assured confidence is proposed. To enable software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices, test plans improvement using Simulated Defect Removal Cost Savings model is offered in this paper.", "num_citations": "25\n", "authors": ["1226"]}
{"title": "Industrial PLC security issues\n", "abstract": " In this paper we have shown that PLC devices are complex embedded systems often relying on some operating system. They are plagued by the same sorts of vulnerabilities and exploits as general purpose operating systems. In fact, the number of latent vulnerabilities in the typical microprocessor-based device can be surprisingly high. However we don't need bugs or vulnerabilities in order to attack the PLC. We can exploit its normal operation provided we have some access to the device. It is suggested that the one of effective ways to avoid expensive business losses or production disruption due to misuse of the PLC is to start protecting the system with defence-in-depth measures.", "num_citations": "24\n", "authors": ["1226"]}
{"title": "The software quality economics model for software project optimization\n", "abstract": " There are many definitions of quality being given by experts that explains quality for manufacturing industry but still unable to define it with absolute clarity for software engineering. To enable software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices is given. In this paper we propose a model which traces design decisions and the possible alternatives. With this model it is possible to minimize the cost of switching between design alternatives, when the current choice cannot fulfill the quality constraints. With this model we do not aim to automate the software design process or the identification of design alternatives. Much rather we aim to define a method with which it is possible to assist the software engineer in evaluating design alternatives and adjusting design decisions in a systematic manner. As of today there is very little knowledge is available about the economics of software quality. The costs incurred and benefits of implementing different quality practices over the software development life cycle are not well understood. There are some prepositions, which are not being tested comprehensively, but some useful Economic Model of Software Quality Costs (CoSQ) and data from industry are described in this article. Significant research is needed to understand the economics of implementing quality practices and its behaviour. Such research must evaluate the cost benefit trade-offs in investing in quality practices where the returns are maximized over the software development life cycle. From a developer\u2019s perspective, there are two types of benefits that can accrue from the\u00a0\u2026", "num_citations": "22\n", "authors": ["1226"]}
{"title": "Choosing the right RTOS for IoT platform\n", "abstract": " To fully take advantage of the opportunity offered by the Internet of Things (IoT), manufacturers of embedded systems must meet multiple challenges, which cannot be addressed without a real time operating system. In this paper we propose some criteria that are important for the selection of such an operating system, which could be used for different classes of IoT platforms. We give a short survey of open-source solutions that are applicable in the IoT systems based both on ARM Cortex-M and TI MSP430 microcontrollers. Selected operating system was ported to the target test platform, but it turned out that porting was not as trivial as it seemed at first glance. The proposed platform was tested in a wireless environment with an intention to be applied in home automation.", "num_citations": "19\n", "authors": ["1226"]}
{"title": "RBOSTP: Risk-based optimization of software testing process Part 2\n", "abstract": " Testing represents a significant portion of the software applications development budget. Risk-Based Optimization of Software Testing Process ie RBOSTP is part of a proven and documented Integrated and Optimized Software Testing Process (IOSTP) designed to improve the efficiency and effectiveness of the testing effort assuring the low project risk of developing and maintaining high quality complex software systems within schedule and budget constraints. Basic considerations of RBOSTP are described in this, Part 1 article and some RBOSTP implementation issues, experience results are presented in Part 2. In this Part 1 article, we describe how RBOSTP combines Earned (Economic) Value Management (EVM) and Risk Management (RM) methodology through simulation-based software testing scenarios at various abstraction levels of the system/software under test activities to manage stable (predictable\u00a0\u2026", "num_citations": "18\n", "authors": ["1226"]}
{"title": "Software Quality Engineering versus Software Testing Process\n", "abstract": " Software Quality Engineering (SQE) is a comprehensive lifecycle approach concerned with every aspect of the software product development process (SDP). An SQE program includes a comprehensive set of quality objectives, measurable quality attributes (quality metrics) to assess progress towards those objectives, and quantitative certification targets for all component software development processes.", "num_citations": "14\n", "authors": ["1226"]}
{"title": "OptimalSQM: Integrated and optimized software quality management\n", "abstract": " Software testing provides a means to reduce errors, cut maintenance and overall software costs. Early in the history of software development, testing was confined to testing the finished code, but, testing is more of a quality control mechanism. However, as the practice of software development has evolved, there has been increasing interest in expanding the role of testing upwards in the SDLC stages, embedding testing throughout the systems development process. Numerous software development and testing methodologies, tools, and techniques have emerged over the last few decades promising to enhance software quality. While it can be argued that there has been some improvement it is apparent that many of the techniques and tools are isolated to a specific lifecycle phase or functional area. This paper presents a set of best practice models and techniques integrated in optimized and quantitatively managed software testing process (OptimalSQM), expanding testing throughout the SDLC. Further, we explained how can Quantitative Defect Management (QDM) Model be enhanced to be practically useful for determining which activities need to be addressed to improve the degree of early and cost-effective software fault detection with assured confidence, then optimality and stability criteria of very complex STP dynamics problem control is proposed.", "num_citations": "12\n", "authors": ["1226"]}
{"title": "Two novel effort estimation models based on quality metrics in web projects\n", "abstract": " Web development projects are certainly different from traditional software development projects and, hence, require differently tailored measures for accurate effort estimation. Effort estimation accuracy will affect the availability of resource allocation and task scheduling. In this paper, we investigate the suitability of a newly proposed quality metrics and models to estimate the effort and duration for small or medium-size Web development projects. It then describes a new size metrics, presents two novel methods (WEBMO+ and VPM+), based on WEB model (WEBMO) using Web objects instead of SLOC and Vector Prediction Model (VPM), to fast estimate the development effort of Web-based information systems. We also empirically validate the approach with a four projects study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life cycle to within+/-20 percent across a range of application types. In contrast with other existing methods, WEBMO+ and VPM+ uses raw historical information about development capability and high granularity information about the system to be developed, in order to carry out such estimations. This method is simple and specially suited for small or medium-size Web based information systems.", "num_citations": "11\n", "authors": ["1226"]}
{"title": "Some experiences in building IoT platform\n", "abstract": " In this paper we give a short survey of some existing solutions and describe our attempt to build an Internet of Things platform independent of underlying hardware. It is concluded that the best way to do that is to virtualize hardware by using common microkernel-based operating system. Such an operating systems exists as an open source, however porting it to the particular hardware board turned out to be a non-trivial task. The proposed platform was tested in setup with full radio coverage and with star topology by using the standard socket API.", "num_citations": "10\n", "authors": ["1226"]}
{"title": "A simultaneous application of combinatorial testing and virtualization as a method for software testing\n", "abstract": " We propose in this paper a general framework for an integrated End-to-End Testing of IT Architecture and Applications using the simultaneous application of combinatorial testing and virtualization. Combinatorial testing methods are often applied in cases of the configuration testing. The combinatorial approach to software testing uses models, particularly an Orthogonal Array Testing Strategy (OATS) is proposed as a systematic, statistical way of testing pair-wise interactions to generate a minimal number of test inputs so that selected combinations of input values are covered. Virtualization, in the process of testing, is based on setting the necessary environment to multiple virtual machines, which run on one or in smaller groups of physical computers, which are: reduce the cost of equipment and related resources, reduce the time required to manage the testing process, and favors raising removal of test infrastructure. Together, combinatorial testing and virtualization presents practical approach to improving process of testing, through the balancing quality, cost and time.", "num_citations": "10\n", "authors": ["1226"]}
{"title": "The COTECOMO: COnstractive test effort COst model\n", "abstract": " The primary purpose of Software Testing Process and Evaluation (STP&E) is to reduce risk. While there exists extensive literature on software cost estimation techniques, industry practice continues to rely upon standard regression-based algorithms. These software effort models are typically calibrated or tuned to local conditions using local data. This paper cautions that current approaches to model calibration often produce sub-optimal models because of the large variance problem which is inherent in cost data and by including far more effort multipliers than the data supports. Building optimal models requires that a wider range of models be considered while correctly calibrating these models requires rejection rules that prune variables and records and use multiple criteria for evaluating model performance. This article compares the approaches taken by three (COCOMO II, FP, UCP) widely used models\u00a0\u2026", "num_citations": "10\n", "authors": ["1226"]}
{"title": "Reducing software defects removal cost via design of experiments using Taguchi approach\n", "abstract": " There are a variety of models, methods and tools aimed at helping organizations in managing defects found in software development. Early estimation of project size, software quality (number of defects injected and fixed) and completion time is essential for successful project planning and tracking. In this paper, we suggest a strategy for the optimization of the action plans in the test process by applying the design of experiments to the testing of maturity model assessment procedure, which provides the step-by-step guidelines for improving organizational test processes. We are able to quantify process performance in terms of a defect containment matrix for each process stage. This is accomplished by the application of Taguchi\u2019s design of experiments and surface response methods in order to find parametric equations for the cost of quality, which is in turn related to defect removal efficacy.", "num_citations": "9\n", "authors": ["1226"]}
{"title": "The software testing challenges and methods\n", "abstract": " The Software Testing Process (STP) raised many challenging issues in past decades of software development practice, several of which remain open. The System/Software under test (SUT) continually increases complexity of applied technology, software application domain model and corresponding process knowledge and experience. Today's SUT have billions of possible inputs and outputs. How does one obtain adequate test coverage with reasonable or even optimal number of test events ie test cases? How does one measure test effectiveness, efficacy, benefits, risks (confidence) of project success, availability of resources, budget, time allocated to STP? How does one plan, estimate, predict, control, evaluate and choose\" the best\" test scenario among hundreds of possible (considered, available, feasible) number of test events (test cases)? How does one judge, decide if satisfied/not satisfied program\u00a0\u2026", "num_citations": "9\n", "authors": ["1226"]}
{"title": "Experimental Images Analysis with Linear Change Positive and Negative Degree of Brightness\n", "abstract": " In this paper we presented an analysis of the image that is applied to the linear rate of increasing and decreasing of brightness on image. The analysis of the results are presented in tabular and graphical form. The analysis used the parameters of the Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), Different Structural Similarity Index (DSSIM). It was found that the different degrees of linear brightness changes the parameters PSNR and MSE have approximately the same value, and the mathematical changes to the image can only be analyzed through SSIM and DSSIM parameters. Analysis of the impact of positive and negative brightness analyzed through histograms for the RGB channels.", "num_citations": "8\n", "authors": ["1226"]}
{"title": "Evaluation of some time-stamping authority software\n", "abstract": " This paper provides an overview of the main state-of-the-art algorithms for time stamping applications and briefly discusses pros and cons of each technique. In particular, it highlights the crucial issues raised by the practical implementation of time stamping algorithms. In addition, it shows the first attempt at time-stamp authority software evaluation regarding some fundamental issues, such as conformance to the Serbian legislation, operating environment, renewal feasibility and responsiveness. Evaluation was carried out both for some commercial and open source time-stamping authority software.", "num_citations": "8\n", "authors": ["1226"]}
{"title": "The oracles-based software testing: problems and solutions.\n", "abstract": " Software test (manual, automated or combined) is often a difficult and complex process. The most familiar aspects of test automation are organizing and running of test cases and capturing and verifying test results. When we design a test we identify what needs to be verified. A set of expected result values are needed for each test in order to check the actual results. Generation of expected results is often done using a mechanism called a test oracle. For the most part, however, the effective use of test oracles has neglected, even though they are a critical component of software testing process. All software-testing methods depend on the availability of an oracle, that is, some method for checking whether thesystem under test has behaved correctly on a particular execution. An ideal oracle would provide an unerring pass/fail judgment for any possible program execution, judged against a natural specification of\u00a0\u2026", "num_citations": "7\n", "authors": ["1226"]}
{"title": "A new approach to software effort estimation using different Artificial Neural Network architectures and Taguchi Orthogonal Arrays\n", "abstract": " In this article, two different architectures of Artificial Neural Networks (ANN) are proposed as an efficient tool for predicting and estimating software effort. Artificial Neural Networks, as a branch of machine learning, are used in estimation because they tend towards fast learning and giving better and more accurate results. The search/optimization embraced here is motivated by the Taguchi method based on Orthogonal Arrays (an extraordinary set of Latin Squares), which demonstrated to be an effective apparatus in a robust design. This article aims to minimize the magnitude relative error (MRE) in effort estimation by using Taguchi\u2019s Orthogonal Arrays, as well as to find the simplest possible architecture of an artificial Neural Network for optimized learning. A descending gradient (GA) criterion has also been introduced to know when to stop performing iterations. Given the importance of estimating software projects\u00a0\u2026", "num_citations": "6\n", "authors": ["1226"]}
{"title": "Recursive PLL based on the Measurement and Processing of Time\n", "abstract": " This paper describes one recursive model of the Phase Locked Loop (PLL) of the second order based on the measurement and processing of the time difference between the input and output periods. The stability and the other conditions, under which the described system can have the properties of a PLL, are investigated using the Z transform analyses. Computer simulation of PLL is introduced in order to give better insight into the PLL characteristics and to confirm the mathematical analyses too. Made analysis shows that PLL is suitable for the wide range of applications. For the corresponding system parameters, PLL possesses the power noise rejection ability. For some other system parameters this PLL possesses the power tracking ability. It can also be used for the measurement of the period of the input signal in the noise environment. The oscilloscope picture of the input and output signals recorded on the realized PLL is presented.", "num_citations": "6\n", "authors": ["1226"]}
{"title": "Software engineering framework for digital service-oriented ecosystem\n", "abstract": " This paper deals with a software development process framework intended to act as a testbed in a service-oriented digital ecoSystem. After short introduction to software ecosystems a concise overview of the framework and its role in such environment is given. After that, the main software engineering components of the framework are described with few examples of how it works.", "num_citations": "6\n", "authors": ["1226"]}
{"title": "Integrated intelligent modeling, simulation and design of experiments for Software Testing Process\n", "abstract": " This paper presents some original solutions with regard to deployment of the US Department of Defense Simulation, Test and Evaluation Process (DoD STEP), using an automated target tracking radar system as a case study of Integrated and Optimized Software Testing Process (IOSTP) framework deployment. This paper is a composite of what is in hand and within reasonable reach in the application of many science and engineering areas that can be successfully applied to software projects management to assure stable (observable and controllable) development process. Besides the integration of modeling and simulation, to form a modelbased approach to the software testing process, the number of experiments, ie test cases, have been dramatically reduced by applying an optimized design-of-experiment plan and an orthogonal array-based robust testing methodology. Intelligence is gathered throughout the project lifecycle, from early reviews to final acceptance testing. The intelligence gathered, enables project and stakeholder management to judge how and whether progress is being made. Computer-based simulation at various abstraction levels of the system/software under test can serve as a test oracle too. Simulation-based (stochastic) experiments, combined with optimized design-of-experiment plans, in the case study, have shown a minimum productivity increase of 100 times in comparison to current practice without IOSTP deployment.", "num_citations": "6\n", "authors": ["1226"]}
{"title": "The use of modeling & simulation-based analysis & optimization of software testing\n", "abstract": " The Software Testing Process (STP) raised many challenging issues in past decades of software development practice, several of which remain open. The System/Software under test (SUT) continually increases complexity of applied technology, software application domain model and corresponding process knowledge and experience. Today's SUT have billions of possible inputs and outputs. How does one obtain adequate test coverage with reasonable or even optimal number of test events ie test cases? How does one measure test effectiveness, efficacy, benefits, risks (confidence) of project success, availability of resources, budget, time allocated to STP? How does one plan, estimate, predict, control, evaluate and choose\" the best\" test scenario among hundreds of possible (considered, available, feasible) number of test events (test cases)? How does one judge, decide if satisfied/not satisfied program\u00a0\u2026", "num_citations": "6\n", "authors": ["1226"]}
{"title": "Convergence rate of Artificial Neural Networks for estimation in software development projects\n", "abstract": " Context:Nowadays, companies are investing in brand new software, given that fact they always need help with estimating software development, effort, costs, and the period of time needed for completing the software itself. In this paper, four different architectures of Artificial Neural Networks (ANN), as one of the most desired tools for predicting and estimating effort in software development, were used.Objective:This paper aims to determine the convergence rate of each of the proposed ANNs, when obtaining the minimum relative error, first depending on the cost effect function, then on the nature of the data on which the training, testing, and validation is performed.Method:Magnitude relative error (MRE) is calculated based on Taguchi\u2019s orthogonal plans for each of these four proposed ANN architectures. The fuzzification method, five different datasets, the clustering method for input values of each dataset, and\u00a0\u2026", "num_citations": "5\n", "authors": ["1226"]}
{"title": "BENEFIT FROM AI IN CYBERSECURITY\n", "abstract": " This paper explained the role of AI in cyber security and proposes recommendation how organizations are benefitting from AI in cybersecurity. Machine learning, a component of AI, applies existing data to constantly improve its functions and strategies over time. It learns and understands normal user behaviour and can identify even the slightest variation from that pattern. But besides gathering information to detect and identify threats, AI can use this data to improve its own functions and strategies as well. In this paper, we research existing obfuscation and de-obfuscation techniques which currently are applied to the android applications, then suggest the de-obfuscation platform based on LLVM (Low-Level Virtual Machine) to perform de-obfuscation process more efficiently. Also, AndrODet solution, an online learning system to detect three common types of obfuscation techniques in Android applications, known as identifier renaming, string encryption, and control flow obfuscation is investigated.", "num_citations": "5\n", "authors": ["1226"]}
{"title": "A contribution to acceleration of graphlet counting\n", "abstract": " Graphlets are small non-isomorphic connected subgraphs used for different kinds of analyses in social networks, bioinformatics and other areas described by large networks, where their number can provide a characterization of the network properties. Much of existing methods for counting the graphlets are based on direct enumeration. However, in case of large networks, this type of counting becomes computationally very demanding. Fortunately, for very sparse networks the computational cost is much less prohibitive than in dense networks, leading to the more efficient graphlet counting algorithms. This work is our first attempt to accelerate one of such algorithms by parallelization using graphic hardware.", "num_citations": "5\n", "authors": ["1226"]}
{"title": "The development of digital satellite television in countries of the former Yugoslavia\n", "abstract": " Subject review This paper presents an overview of the development of satellite television in the former Yugoslavia (Serbia, Montenegro, Bosnia and Herzegovina, Croatia, Slovenia and Macedonia) in the period 1991\u00f7 2012. As a precursor to digital satellite TV, a brief analysis of the development of analog satellite TV is given. A number of free and coded channels for the area since the first digital channel are given in charts and graphics. Also, the basic characteristics of direct to home platforms are given, and an overview of the most popular satellite positions used by providers. We have analysed the presence of foreign satellite channels that perform localization of specific languages. The graphs show the development of localization, as well as the representation of specialized genre of foreign satellite channels. Based on the presented analysis, it can be determined in which direction will go further development of satellite television in the observed region.", "num_citations": "5\n", "authors": ["1226"]}
{"title": "Use of orthogonal arrays and design of experiments via Taguchi methods in software testing\n", "abstract": " To solve the problem of great number of test cases, and to force the configuration testing to be effective, combinatorial testing is proposed, using an Orthogonal Array Testing Strategy (OATS) as a systematic, statistical way of testing pair-wise interactions. This combinatorial approach to software testing uses models to generate a minimal number of test inputs so that selected combinations of input values are covered. The OAT method can simultaneously reduce testing costs, product introduction delays, and faults going to the field by generating test cases that are more efficient and thorough in finding faults. Often the result is a 50% reduction in the number of tests and detection of more faults. An advantage of the Taguchi method application in Software Testing is that it emphasizes a mean performance characteristic (Defect fixing time and cost of software Quality) value close to the target value rather than a value within certain specification limits, thus improving the product quality. Additionally, Taguchi's method for experimental design is straightforward and easy to apply as we did for defect Cost to fix [$] and Total Resolution time [Days] minimisation versus controlled factors: Severity, Complexity and engineers Experience to many engineering situations, making it a powerful yet simple tool.", "num_citations": "5\n", "authors": ["1226"]}
{"title": "OptimalSQM: Optimal software quality management framework architecture\n", "abstract": " A Software companies are often challenged with providing QA and testing of their software in an effective and efficient manner. What a QA staff and project management must do to make software quality more certain? They must have adequate software project management and test management infrastructure such is Optimal Software Quality Management (OptimalSQM) Framework. OptimalSQM Framework consists of a Business Intelligent Simulation Architecture with integrated Software expert tools (Profit eXpert, Planner eXpert, Risk Management eXpert, Quality eXpert, Maintenance eXpert, People Performance eXpert and Process Dynamics Control eXpert) in order to find optimal development activity combination at the beginning for every SDLC model. OptimalSQM solution is an faster, better and cheaper solution which enable software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices, test plans improvement using Simulated Defect Removal Cost Savings model as we described in this paper. In this paper we described OptimalSQM Framework Architecture, proposed a set of components which enables to minimize the cost of switching between test plan alternatives, when the current choice cannot fulfill the quality constraints.", "num_citations": "5\n", "authors": ["1226"]}
{"title": "Applying modeling & simulation to the software testing process: one test oracle solution\n", "abstract": " This paper suggests that the software engineering community could exploit simulation to much greater advantage. There are several reasons for this. First, the Office of the Secretary of Defense has indicated that simulation will play a significant role in the acquisition of defense-related systems to cut costs, improve reliability and bring systems into operation more rapidly. Second, there many areas where simulation can be applied to support software development and acquisition. Such areas include requirements specification, process improvement, architecture trade-off analysis and software testing practices. Third, commercial simulation technology, capable of supporting software development needs is now mature, is easy to use, is of low cost and is readily available. Computer-based simulation at various abstraction levels of the system/software under test can serve as a efficient test oracle, as described in this\u00a0\u2026", "num_citations": "5\n", "authors": ["1226"]}
{"title": "Analysis of the impact of front and back light on image compression with SPIHT method during realization of the chroma key effect in virtual TV studio\n", "abstract": " In this paper analysis of the impact of front and back light on compression of static image necessary for realization of the chrome-key effect in virtual TV studio is presented. For image compression SPIHT method is used. The analysis is performed for the fixed value of front light and variable values of the back light. Compression is applied to different values of the bit per pixel (bpp). The quality of compressed images is rated based on the values MSE, SNR and PSNR. The obtained values are displayed in tables and graphics. Based on these graphics a comparison between compressed images with different levels of front and back lights was made and it is given a level of image brightness produces the best results. It was found how the quality of compression varies with the changing brightness of images at different bit per pixel.", "num_citations": "4\n", "authors": ["1226"]}
{"title": "Challenges in estimating software testing effort\n", "abstract": " Effort estimation is one of the critical challenges in Software Testing Life Cycle (STLC). It is the basis for the project\u2019s effort estimation, planning, scheduling and budget planning. This paper illustrates few models with an objective to depict the accuracy and bias variation of an organization\u2019s estimates of software testing effort using two historical datasets of 50 finished software projects. In this research, a statistical study is performed. A multiple regression analysis model has been built to predict the Testing Time Effort for software development projects based on several Independent or Predictor variables. The independent variables under consideration are: total number of test cases created per module, experience of the developer of the module in years, experience of the tester in years and the size of the module. Besides, the software documentation size (# Pages of requirements, design, project plans, test plans, requirements changes, training materials, HELP text, and user manuals) was selected and used for the software testing effort estimation. The proposed models\u2019 estimation figures are accurate enough to be appropriate techniques for estimating effort for software testing.", "num_citations": "4\n", "authors": ["1226"]}
{"title": "Estimating cost and defect removal effectiveness in SDLC testing activities\n", "abstract": " There are a variety of models, methods and tools to help organizations manage defects found in the development of software. Defect tracking and processing must be integrated in the project life cycle and the software testing process as we did in our Optimal Quantitatively Managing Testing (OptimalSQM) process. We describe phased defect injection and removal cost model for projects where test activities are performed sequentially. We can quantify process performance in terms of the\" defect containment matrix\" of each process stage for use in process improvement and estimating rework costs using Taguchi\u2019s Design of Experiments method and SRM (Surface Response Method) to find parametric equation for cost of quality (CoQ) related to DRE (Defect Removal Efficacy).", "num_citations": "4\n", "authors": ["1226"]}
{"title": "The relationship between changes of deflection and natural frequencies of damaged beams\n", "abstract": " The paper describes the relationship between changes of deflection of beams due damages and the natural frequency changes. Previous papers relate the influence of damage location and severity upon the natural frequency changes of weak-axis bending vibration modes, for which the authors have found a correlation. It base on the reduction of stiffness in a slice of the beam and consequently on the reduction of the potential stored energy in that slice. While the stiffness reduction affect also the deflection under the own mass of the beam, we concluded that it can be found a relationship between deflection and frequency changes. Researches performed this direction revealed that it is a clear dependency between deflection and frequency changes, what makes deflection an important feature of beam behavior, usable in damage assessment.", "num_citations": "4\n", "authors": ["1226"]}
{"title": "Software testing as a service (TaaS): The BISA approach\n", "abstract": " In this paper an approach for designing a service-oriented software testing framework is discussed. This framework relies on a set of intelligent expert tools developed earlier in the context of the OptimalSQM initiative. After an overview of OptimalSQM, the paper concentrates on the key challenges that rise out of adopting Test as a Service (TaaS) in compliance with the movement of Software as a Service (SaaS). TaaS is, also examined from a social perspective by helping out software engineers discover necessary test services.", "num_citations": "4\n", "authors": ["1226"]}
{"title": "Orthogonal Array and Virtualization as a Method for Configuration Testing Improvement\n", "abstract": " Combinatorial testing methods are often applied in cases of configuration testing. Good practice points to the simultaneous application of combinatorial testing and virtualization. Virtualization, in the process of testing, is based on setting the necessary environment to multiple virtual machines, which run on one computer or in smaller groups of computers, which are: reduce the cost of equipment and related resources, reduce the time required to manage the testing process, and favours raising removal of test infrastructure. Together, combinatorial testing and virtualization presents a practical approach to improving testing process, through the balancing of quality, cost and time.", "num_citations": "4\n", "authors": ["1226"]}
{"title": "Optimizing test process action plans by simulated defect removal cost savings\n", "abstract": " To enable software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices, test plans improvement using Simulated Defect Removal Cost Savings model is offered. In this paper we propose a model which enables to minimize the cost of switching between test plan alternatives, when the current choice cannot fulfill the quality constraints. The defect containment measure is traditionally used to provide insight into project success (or lack thereof) at capturing defects early in the project life cycle, ie, the time when defect repair costs are at their minimum. We offered a simulation method with which it is possible to assist the test manager in evaluating test plan alternatives and adjusting test process improvement decisions in a systematic manner. Using raw defect containment data and deriving Quantitative Defect Management (QDM) measures early in the development life cycle provides opportunities for a project to identify issues in defect capture before costs spiral out of control and schedule delays ensue. We simulated two scenarios (a standard quality assurance activities plan and comprehensive quality assurance activities plan) and calculated test process improvement potential according to derived QDM measures.", "num_citations": "4\n", "authors": ["1226"]}
{"title": "Techniques to reduce a set of test cases\n", "abstract": " The primary purpose of Software Testing Process and Evaluation (STP& E) is to reduce risk. All testing provides insight and helps identify\" unknown-unknowns\". This paper describes STP with Assured Confidence techniques. The uncertainty problem is an important issue in the computer industry today and testing is still the main technique for quality assurance. There is a need to ensure that the software is reasonably safe from severe faults after testing. When faced with financial or a schedule constraints, testing is usually cut horizontally attempting to cover as many different test requirements at the expense of depth. We have reached a point where we must test smarter and apply Statistical-Risk-Based Test with Assured Confidence (SRBTAC) management procedure. We need to pick the right assessment tools to make vertical cuts in our test strategies. Any given test has a low probability of detecting a problem, but\u00a0\u2026", "num_citations": "4\n", "authors": ["1226"]}
{"title": "OptimalSQM: Optimal Software Quality Management Repository is a Software Testing Center of Excellence\n", "abstract": " A Software companies are often challenged with providing QA and testing of their software in an effective and efficient manner. In this paper we will present our OptimalSQM Framework solution based on our Business Intelligent Simulation Architecture (BISA) design with integrated Software expert tools (Profit eXpert, Planer eXpert, Risk Management eXpert, Quality eXpert, Maintenance eXpert, People Performance eXpert and Process Dynamics Control eXpert) to find optimal development activity combination at the beginning for every SDLC model. The BISA realization with complete integrated expert tools offer to SMEs the OptimalSQM solution which is a faster, better and cheaper solution which enables software designers to achieve a higher quality for their design, a better insight into quality predictions for their design choices, test plans improvement using Simulated Defect Removal Cost Savings model as we described in this paper. OptimalSQM solution aim is to assist with advanced software testing technology adoption in organizations, especially focusing on small-to medium-sized enterprises (SMEs) ie OptimalSQM solution is a Software Testing Center of Excellence for SMEs.", "num_citations": "4\n", "authors": ["1226"]}
{"title": "Improved effort and cost estimation model using artificial neural networks and taguchi method with different activation functions\n", "abstract": " Software estimation involves meeting a huge number of different requirements, such as resource allocation, cost estimation, effort estimation, time estimation, and the changing demands of software product customers. Numerous estimation models try to solve these problems. In our experiment, a clustering method of input values to mitigate the heterogeneous nature of selected projects was used. Additionally, homogeneity of the data was achieved with the fuzzification method, and we proposed two different activation functions inside a hidden layer, during the construction of artificial neural networks (ANNs). In this research, we present an experiment that uses two different architectures of ANNs, based on Taguchi\u2019s orthogonal vector plans, to satisfy the set conditions, with additional methods and criteria for validation of the proposed model, in this approach. The aim of this paper is the comparative analysis of the obtained results of mean magnitude relative error (MMRE) values. At the same time, our goal is also to find a relatively simple architecture that minimizes the error value while covering a wide range of different software projects. For this purpose, six different datasets are divided into four chosen clusters. The obtained results show that the estimation of diverse projects by dividing them into clusters can contribute to an efficient, reliable, and accurate software product assessment. The contribution of this paper is in the discovered solution that enables the execution of a small number of iterations, which reduces the execution time and achieves the minimum error.", "num_citations": "3\n", "authors": ["1226"]}
{"title": "Some facts about industrial software security\n", "abstract": " In this paper we provide an analysis of some potential vulnerabilities, and security concerns including recommendations toward improving security for industrial control systems. We briefly describe the architecture of contemporary control devices. It appears to be a challenge to most serious vulnerability researchers-real time operating systems are often plagued by the same sorts of vulnerabilities and exploits as general purpose operating systems. In fact, the number of latent vulnerabilities in the typical microprocessor-based device can be surprisingly high. Then, we provide recommendations toward an enhanced security for control systems software. We have shown that it is necessary to address not only the individual vulnerabilities, but the breadth of risks that can interfere with critical operations. We describe some requirements and features needed to improve the security of the control system. The one of effective ways to avoid expensive business losses or production disruption is to start protecting the system with defence-in-depth measures.", "num_citations": "3\n", "authors": ["1226"]}
{"title": "Comparative Study on Applicability of Four Software Size Estimation Models Based on Lines of Code\n", "abstract": " Early estimation of project size and completion time is essential for successful project planning and tracking. Multiple methods have been proposed to estimate software size and cost parameters. Suitability of the estimation methods depends on many factors like software application domain, product complexity, availability of historical data, team expertise etc. We present an empirical validation of four software size estimation methods based on Lines of Code and object-oriented paradigm. Line of Code (LOC) is still a commonly used software size measure. Despite the fact that software sizing is well recognized as an important problem for more than two decades, there is still much problem in existing methods. This paper proposes a method for estimating LOC for an information system from its conceptual data model through the use of multiple linear regression models. We have validated the method through collecting samples from both the industry and opensource systems published in literature. In this paper we report some experimental evaluations of metrics for OO models. Such metrics were obtained by adapting for OO models some metrics that ware originally conceived for OO code. According to the reported results, we sketch a methodological approach to the measurement of model size and other quantitative characteristics. A cross validation approach was adopted to build and evaluate linear and polynomial models where the independent variable was a traditional OO entity: classes, methods, association, inheritance, or a combination of them.", "num_citations": "3\n", "authors": ["1226"]}
{"title": "Po\u0161ta Srbije kao izdavalac vremenskih \u017eigova\n", "abstract": " Javno preduze\u0107e PTT saobra\u0107aja\" Srbija\"(Po\u0161ta Srbije) je izgradilo sistem za izdavanje vremenskih \u017eigova i postalo je izdavalac vremenskih \u017eigova (Time-Stamping Authority-TSA) u Republici Srbiji, u skladu sa Zakonom o elektronskom dokumentu [1] i Pravilnikom o izdavanju vremenskog \u017eiga [2]. Vremenski \u017eigovi Po\u0161te namenjeni su svim u\u010desnicima elektronskog poslovanja u Republici Srbiji, i fizi\u010dkim i pravnim licima (dr\u017eavna uprava, lokalna samouprava, javne slu\u017ebe, preduze\u0107a, banke, osiguravaju\u0107a dru\u0161tva, organizacije, institucije,...).", "num_citations": "2\n", "authors": ["1226"]}
{"title": "Improvement in decision-making in emergency medicine by using PC-based system for electrocardiography\n", "abstract": " In situations where an urgent decision-making is necessary, it is important to rely on input from more than one source. By using concept of virtual instrumentation, ECG signal can be digitized and transferred to computer for further processing. Software in computer enables use of modern tools for digital signal processing. One of such tools is time-frequency analysis with continuous wavelet transform. The same (digitized) signal can be presented to the cardiologist in standard - time domain, but also simultaneously in time-frequency domain. By looking at both presentations of the same signal and by interpreting them, making a diagnosis becomes more reliable. The process of software development, with the goal mentioned, requires continuous monitoring and correction of faults that might appear in interpreting of two different presentations. We have decided to use standard DMAIC model in the process of software development and maintenance in order to make and keep quality of proposed decision making tool in electrocardiography.", "num_citations": "2\n", "authors": ["1226"]}
{"title": "Optimality and stability criteria for software testing process control model\n", "abstract": " The software development industry spends more than half of its budget on maintenance related activities. Software testing provides a means to reduce errors, cut maintenance and overall software costs. Software testing involves the process of detecting software discrepancies so that they can be corrected before they are installed into a live environment supporting operational business units. Early in the history of software development, testing was confined to testing the finished code, but, testing is more of a quality control mechanism. However, as the practice of software development has evolved, there has been increasing interest in expanding the role of testing upwards in the SDLC stages, embedding testing throughout the systems development process. Numerous software development and testing methodologies, tools, and techniques have emerged over the last few decades promising to enhance software quality. While it can be argued that there has been some improvement it is apparent that many of the techniques and tools are isolated to a specific lifecycle phase or functional area. This paper presents a set of best practice models and techniques integrated in optimized and quantitatively managed software testing process (OptimalSQM), expanding testing throughout the SDLC. Further, we explained how can Quantitative Defect Management (QDM) Model be enhanced to be practically useful for determining which activities need to be addressed to improve the degree of early and costeffective software fault detection with assured confidence, then optimality and stability criteria of very complex STP dynamics problem control is proposed.", "num_citations": "2\n", "authors": ["1226"]}
{"title": "Access Control For E-Business: Structure Or Architectural Security Principles\n", "abstract": " Security vulnerabilities are rampant throughout our information infrastructures. The fundamental security principles from more than four decades of research and development in information security technology were reviewed. The results of this analysis have been distilled into a review of the principles that underlie the design and implementation of trustworthy systems. In this article we described Structure or Architectural Design for Security principles: Economy and Elegance, Secure System Evolution, Trust, and Distributed Composition principles. Also, their adjustment for emerging smart card and real e-Business manufacturers is emphasized. Among them, MO (bile) PASS (port) is a consortium lead by Japanese major electric appliance manufacturers for the next generation smart card. Its scope includes the software middleware that bridges between the smart card and real e-Businesses. In fact, the specification of Ticket Authentication Protocol, which provides access control functionality applicable to a wide range of e-Businesses (eg digital ticketing, digital content distribution), is under consideration by MOPASS. The specification is open and direct since it is based on a new methodology characterized by user\u2019s access-rights being straightforwardly authenticated by PKI. In addition, it covers the functionality of revoking, duplicating and transferring of access-rights in a peer-to-peer manner. Strengths and weakness of others access control protocols are discussed.", "num_citations": "2\n", "authors": ["1226"]}
{"title": "KNOWLEDGE ASSESSMENT USING CAUSE-EFFECT GRAPHING METHODS\n", "abstract": " This paper presents a test of the learning process that aims to check students\u2019 knowledge. Through additional activities in the learning process it\u2019s possible to check knowledge of the student, and in this paper are used activity questions and answers and multiple choice activities. It has been created a learning process that consists of a series of questions with possible answers true and false and multiple choice questions that support marking multiple correct answers. Testing will be performed with cause-effect method and the result will be shown by Cause-effect graph with defining test cases based on the obtained results.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Simulation based life-cycle analysis of a vehicle fleet\n", "abstract": " Throughout the years the logistic experts have developed a variety of simulation models for specific applications. This paper addresses a discrete-event simulation model, which estimates the operational availability and maintenance cost of a vehicle fleet throughout complete life cycle, under a certain maintenance scenario. The model gives all necessary parameters to compute the total vehicle life cycle cost, linking reliability, operational tempo and maintenance scenario with vehicle acquisition and maintenance costs. Based on simulation results one can make cost-effective decision relative to buying adequate vehicles and organizing proper fleet maintenance, which gives required operational availability at lowest costs. In this work we have analyzed application of our model on an example\u2013a light tactical vehicle fleet.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Quantitative model for allocation of resources based on success rate of software projects using design of experiments\n", "abstract": " Early estimation of project size, software quality (number of defects injected and fixed) and completion time is essential for successful project planning and tracking. Despite the fact that software sizing, defects injection and fixing distribution that are expected in the project, which is used for planning and to track project progress is well recognized as an important problem for more than two decades, there is still much problem in existing methods for resources allocation by software project managers. We present a quantitative model which estimates Success Rate of Software Projects using Design of Experiments based on collected data for variation of: estimated number defects, number of developers and development effort in stuff hours. We have validated the method through collecting samples from both the industry and open-source systems published in literature. The investigation result shows a significant impact of aforementioned factors on the success of software project and on the company.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Optimization of Vehicle Maintenance Concept Using Simulation\n", "abstract": " This paper addresses a discrete-event simulation model, which estimates the operational availability and maintenance cost of a vehicle fleet throughout complete life cycle, under a variety of acquisition requirements, operational tempos, and maintenance scenarios. Based on simulation results one can make cost-effective decision relative to buying adequate vehicles and organizing proper fleet maintenance. In this work we have analyzed application of our model on an example-finding potential vehicles, capable of fulfilling requirements, under different maintenance conditions and working within a set of operational tempos.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Pristup proceni raspolo\u017eivosti slo\u017eenih komunikacionih mre\u017ea pomo\u0107u simulacije\n", "abstract": " Sadr\u017eaj\u2014U radu se prikazuje pristup proceni slo\u017eenih komunikacionih mre\u017ea koje se sastoje od popravljivih \u010dvorova i linkova primenom metode simulacije diskretnih doga\u0111aja. Date su potrebne definicije, elementi simulacionog modela, algoritam programa-simulatora za procenu dvoterminalne raspolo\u017eivosti komunikacione mre\u017ee realizovanog u simulacionom jeziku GPSS World i primer sa kratkom analizom rezultata prvih eksperimenta", "num_citations": "1\n", "authors": ["1226"]}
{"title": "One approach to the testing of security of proposed database application software\n", "abstract": " This paper presents the concept of database configuration and development considering security issues especially when connected to internet. Regardless of precautions on security voulnerabilities implemented on other levels of database environment, such as: network, operating system, client application, it is important to protect database itself by avoiding well known database security issues. In order to prove that proposed configuration has a high level of security protection, security testing has to be performed. The overall goal of security testing is to reduce vulnerabilities within a software system and we have proposed testing methodology including code review and vulnerability assessment that represent the most widespread of best practices for software security assurance.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Discrete Events Simulation at Union University School of Computing\n", "abstract": " Union University School of Computing approach to discrete events simulation teaching and research has been presented in the paper. The teaching is going on through basic course in modelling and simulation, and simulation also takes part in courses of computer architecture and organization and safety critical systems. The research is going on in several areas, the most important being defence systems, safety critical systems and flexible manufacturing systems.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Software economics: Quality-based return-on-investment model\n", "abstract": " Along with the ever more apparent importance and criticality of software systems for modern societies, arises the urgent need to deal efficiently with the quality assurance of these systems. Even though the necessity of investments into software quality should not be underestimated, it seems economically unwise to invest seemingly random amounts of money into quality assurance. The precise prediction of the costs and benefits of various software quality assurance techniques within a particular project allows for economically sound decision-making. This article explains the return on investment rate (ROI) of Software Process Improvement (SPI), and introduces practical metrics and models for the ROI of SPI. Furthermore an analytical idealized model of defect detection techniques is presented. It provides a range of metrics: the ROTI of software quality assurance for example. The method of ROTI calculation is exemplified in this paper. In conclusion, an overview on the debate when software is purchased, concerning quality and cost ascertaining in general will be given. Although today there are a number of techniques to verify the cost-effectiveness of quality assurance, the results are thus far often unsatisfactory. More importantly, this article helps sort through the seldom and often confusing literature by identifying a small set of practical metrics, models, and examples for the ROI of SPI.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Comparative study on applicability of two novel effort estimation models in web projects\n", "abstract": " In software engineering research area, cost/effort estimation is one of the most important issues. Effort estimation accuracy will affect the availability of resource allocation and task scheduling. This article discusses the need for new metrics and models to estimate the effort and duration for Web development projects. It then describes a new size metrics, presents two novel methods (WEBMO+ and VPM+), based on WEB model (WEBMO) using Web objects instead of SLOC and Vector Prediction Model (VPM), to fast estimate the development effort of Web-based information systems. We also empirically validate the approach with a four projects study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life cycle to within+/-20 percent across a range of application types. In contrast with other existing methods, WEBMO+ and VPM+ uses raw historical information about development capability and high granularity information about the system to be developed, in order to carry out such estimations. This method is simple and specially suited for small or medium-size Web based information systems.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Testiranje softvera kao mera za\u0161tite korisnika softvera\n", "abstract": " Testiranje softvera je ve\u0107 vi\u0161e godina, pa i decenija, ustanovljeno kao nau\u010dna, odnosno in\u017eenjerska disciplina. Testiranje softvera se sastoji od aktivnosti dinami\u010dke verifikacije pona\u0161anja programa na bazi kona\u010dnog skupa testova, odabranih na pogodan na\u010din iz beskona\u010dnog skupa mogu\u0107ih na\u010dina izvr\u0161avanja programa, a prema specificiranom o\u010dekivanom pona\u0161anju softvera u razvijanoj aplikaciji tj. zahtevanog kvaliteta softvera. Zbog toga u ovom radu se daje opis rezultata vi\u0161egodi\u0161njeg istra\u017eivanja integralnog i optimiziranog procesa testiranja (TS) softvera, ukazano je na prednosti i nedostatke primene raznih tehnika i strategija TS, predlo\u017eeni su postupci za dono\u0161enje odluke o odabiru tehnika testiranja u op\u0161tijem slu\u010daju da se spre\u010de defekti u softveru u toku eksploatacije od strane korisnika, \u010dime se izbegavaju sudski procesi usled nedovoljnog kvaliteta softvera.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Integralni i optimizirani process testiranja softvera\n", "abstract": " Ra\u010dunarski sistemi su odavno postali veoma slo\u017eeni, a softver koji se razvija ima stalnu tendenciju rasta kompleksnosti. Tr\u017ei\u0161te zahteva od kompanija koje razvijaju softver sve kra\u0107e vreme razvoja softvera koje zbog toga vode bespo\u0161tednu borbu u izvr\u0161avanju poznate mantre: br\u017ee, bolje jeftinije. \u0160ampioni su one kompanije koje uspeju da prona\u0111u kompromis izme\u0111u funkcionalnih zahteva, rokova, kvaliteta i raspolo\u017eivog bud\u017eeta. Sa rastom kompleksnosti realizovanih funkcija i primena, posebno je narastao zahtev za kvalitetom softvera u pogledu pouzdanosti (kod softvera sa kriti\u010dnom misijom), pogodnosti za testiranje i odr\u017eavanje, ponovne upotrebljivosti, otpornosti na gre\u0161ke i drugih faktora kvaliteta softvera. Nedostatak iskusnog i utreniranog razvojnog tima, ograni\u010deni resursi i sredstva za automatizovan razvoj softvera doveo je do niza sli\u010dnih roblema kako u velikim tako iu malim kompanijama koje razvijaju softverski proizvod. Ovi problemi uklju\u010duju problem nekompletnog razvoja, neefikasnog testiranja softvera, nizak nivo kvaliteta softvera, visoki tro\u0161kovi razvoja i odr\u017eavanja softvera, nezadovoljan kupac, ali se time lista roblema ne zavr\u0161ava [1-2]. U razvojnom ciklusu softvera sve je zna\u010dajniji zadatak Testiranja Softvera (TS) ili Verifikacije i Validacije (V&V) koji treba da obezbedi zahtevani nivo poverenja u ispravnost (korektnost) softvera kao i obezbe\u0111enja ostalih zahtevanih karakteristika softvera. Testiranje Softvera je skup proces, jer u proseku oko 50% ukupnog bud\u017eeta na razvoj softverskog proizvoda se tro\u0161i na testiranje softvera dok je u nekim oblastima primene \u010dak i preko 80%[3]. Veliki su gubici firmi usled gre\u0161aka u softveru koji\u00a0\u2026", "num_citations": "1\n", "authors": ["1226"]}
{"title": "E-MAIL FORENSICS: TECHNIQUES AND TOOLS FOR FORENSIC INVESTIGATION\n", "abstract": " E-mail has emerged as the most important application on Internet for communication of messages, delivery of documents and carrying out of transactions and is used not only from computers but many other electronic gadgets like mobile phones. This paper is an attempt to illustrate e-mail architecture from forensics perspective. Also, paper projects the need for e-mail forensic investigation and lists various methods and tools used for its realization. A detailed header analysis of a multiple tactic spoofed e-mail message is carried out in this paper. It also discusses various possibilities for detection of spoofed headers and identification of its originator. Further, difficulties that may be faced by investigators during forensic investigation of an e-mail message have been discussed along with their possible solutions. Our focus is on email header analysis phase offered by the tools. We examine the capability of a particular tools such as: EmailTrackerPro and aid4mail in action.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Access Control For E-Business: System Life Cycle And Functional Security Principles\n", "abstract": " Security vulnerabilities are rampant throughout our information infrastructures. The fundamental security principles from more than four decades of research and development in information security technology were reviewed. The results of this analysis have been distilled into a review of the principles that underlie the design and implementation of trustworthy systems. In this article we described Design for Security principles: Logic and Functional, as well as, System life cycle security principles. Also, their adjustment for emerging smart card and real e-Business manufacturers is emphasized. Among them, MO (bile) PASS (port) is a consortium lead by Japanese major electric appliance manufacturers for the next generation smart card. Its scope includes the software middleware that bridges between the smart card and real e-Businesses. In fact, the specification of Ticket Authentication Protocol, which provides access control functionality applicable to a wide range of e-Businesses (eg digital ticketing, digital content distribution), is under consideration by MOPASS. The specification is open and direct since it is based on a new methodology characterized by user\u2019s access-rights being straight forwardly authenticated by PKI. In addition, it covers the functionality of revoking, duplicating and transferring of access-rights in a peer-to-peer manner. Strengths and weakness of others access control protocols are discussed.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "A critical review of source code size estimation approaches for object-oriented programming languages: A comparative study\n", "abstract": " Size estimation plays a key role in effort estimation that has a crucial impact on software projects in the software industry. We investigate the accuracy of early lines of code (LOC) estimation approaches for an object-oriented system at the early software development phase. We have validated the proposed methods using samples from both the software industry and open-source systems. In this paper, authors perform a comparative study of several size estimation models on C++, C#, and Java programs by using object-oriented metrics to estimate LOC, which comprise measures for# classes, class size,# methods per class,# attributes per class etc. We applied Surface Response Modeling (SRM) method, a case of Design of Experiment (DOE) method, in order to determine the factors that contribute most to the estimation model accuracy.", "num_citations": "1\n", "authors": ["1226"]}
{"title": "Software Quality & Testing Metrics\n", "abstract": " Software Quality Assurance without measures is like a jet with no fuel. Development of high quality software is very complicated and unreliable task, but the management of software development and testing process is much harder without appropriate software quality metrics and measurement establishment that assure process testing quantitative management in order to increase efficiency of software bugs detection and removal. Process improvement enables the same amount of software to be built in less time with less effort and fewer defects. Progress control tracks defects found during development in order to avoid premature delivery and to ensure the reliability goals are achieved. In-progress control and reporting of development progress gives continuous visibility to both developers and purchasers. This paper focuses on software testing and the measurements which allow for the quantitative evaluation of this critical software development process. This paper describes the Software Quality & Testing Metrics Model, which is a framework for establishing and interpreting in-process metrics in software development. The model has been validated and used on large sample of software projects in a mature software development organization. The central issue for in-process metrics, the concept and definition of the model, and its use are discussed. Examples of metrics real-life projects are provided.", "num_citations": "1\n", "authors": ["1226"]}