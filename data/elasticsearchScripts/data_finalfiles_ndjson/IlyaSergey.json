{"title": "Finding the greedy, prodigal, and suicidal contracts at scale\n", "abstract": " Smart contracts---stateful executable objects hosted on blockchains like Ethereum---carry billions of dollars worth of coins and cannot be updated once deployed. We present a new systematic characterization of a class of trace vulnerabilities, which result from analyzing multiple invocations of a contract over its lifetime. We focus attention on three example properties of such trace vulnerabilities: finding contracts that either lock funds indefinitely, leak them carelessly to arbitrary users, or can be killed by anyone. We implemented Maian, the first tool for specifying and reasoning about trace properties, which employs interprocedural symbolic analysis and concrete validator for exhibiting real exploits. Our analysis of nearly one million contracts flags 34, 200 (2, 365 distinct) contracts vulnerable, in 10 seconds per contract. On a subset of 3, 759 contracts which we sampled for concrete validation and manual analysis, we\u00a0\u2026", "num_citations": "355\n", "authors": ["1714"]}
{"title": "Communicating State Transition Systems for Fine-Grained Concurrent Resources\n", "abstract": " We present a novel model of concurrent computations with shared memory and provide a simple, yet powerful, logical framework for uniform Hoarestyle reasoning about partial correctness of coarse- and fine-grained concurrent programs. The key idea is to specify arbitrary resource protocols as communicating state transition systems (STS) that describe valid states of a resource and the transitions the resource is allowed to make, including transfer of heap ownership.               We demonstrate how reasoning in terms of communicating STS makes it easy to crystallize behavioral invariants of a resource. We also provide entanglement operators to build large systems from an arbitrary number of STS components, by interconnecting their lines of communication. Furthermore, we show how the classical rules from the Concurrent Separation Logic (CSL), such as scoped resource allocation, can be generalized to\u00a0\u2026", "num_citations": "122\n", "authors": ["1714"]}
{"title": "Mechanized Verification of Fine-grained Concurrent Programs\n", "abstract": " Efficient concurrent programs and data structures rarely employ coarse-grained synchronization mechanisms (ie, locks); instead, they implement custom synchronization patterns via fine-grained primitives, such as compare-and-swap. Due to sophisticated interference scenarios between threads, reasoning about such programs is challenging and error-prone, and can benefit from mechanization. In this paper, we present the first completely formalized framework for mechanized verification of full functional correctness of fine-grained concurrent programs. Our tool is based on the recently proposed program logic FCSL. It is implemented as an embedded DSL in the dependently-typed language of the Coq proof assistant, and is powerful enough to reason about programming features such as higher-order functions and local thread spawning. By incorporating a uniform concurrency model, based on state-transition\u00a0\u2026", "num_citations": "117\n", "authors": ["1714"]}
{"title": "A concurrent perspective on smart contracts\n", "abstract": " In this paper, we explore remarkable similarities between multi-transactional behaviors of smart contracts in cryptocurrencies such as Ethereum and classical problems of shared-memory concurrency. We examine two real-world examples from the Ethereum blockchain and analyzing how they are vulnerable to bugs that are closely reminiscent to those that often occur in traditional concurrent programs. We then elaborate on the relation between observable contract behaviors and well-studied concurrency topics, such as atomicity, interference, synchronization, and resource ownership. The described contracts-as-concurrent-objects analogy provides deeper understanding of potential threats for smart contracts, indicate better engineering practices, and enable applications of existing state-of-the-art formal verification techniques.", "num_citations": "102\n", "authors": ["1714"]}
{"title": "Scilla: a smart contract intermediate-level language\n", "abstract": " This paper outlines key design principles of Scilla---an intermediate-level language for verified smart contracts. Scilla provides a clean separation between the communication aspect of smart contracts on a blockchain, allowing for the rich interaction patterns, and a programming component, which enjoys principled semantics and is amenable to formal verification. Scilla is not meant to be a high-level programming language, and we are going to use it as a translation target for high-level languages, such as Solidity, for performing program analysis and verification, before further compilation to an executable bytecode. We describe the automata-based model of Scilla, present its programming component and show how contract definitions in terms of automata streamline the process of mechanised verification of their safety and temporal properties.", "num_citations": "90\n", "authors": ["1714"]}
{"title": "Ownership types: A survey\n", "abstract": " Ownership types were devised nearly 15 years ago to provide a stronger notion of protection to object-oriented programming languages. Rather than simply protecting the fields of an object from external access, ownership types protect also the objects stored in the fields, thereby enabling an object to claim (exclusive) ownership of and access to other objects. Furthermore, this notion is statically enforced by now-standard type-checking techniques.               Originating as the formalisation of the core of Flexible Alias Protection, ownership types have since been extended and adapted in many ways, and the notion of protection provided has been refined into topological and encapsulation dimensions. This article surveys the various flavours of ownership types that have been developed over the years, along with the many applications and other developments. The chapter concludes by suggesting some\u00a0\u2026", "num_citations": "80\n", "authors": ["1714"]}
{"title": "EthIR: A Framework for High-Level Analysis of Ethereum Bytecode\n", "abstract": " Analyzing Ethereum bytecode, rather than the source code from which it was generated, is a necessity when: (1) the source code is not available (e.g., the blockchain only stores the bytecode), (2) the information to be gathered in the analysis is only visible at the level of bytecode (e.g., gas consumption is specified at the level of EVM instructions), (3) the analysis results may be affected by optimizations performed by the compiler (thus the analysis should be done ideally after compilation). This paper presents EthIR, a framework for analyzing Ethereum bytecode, which relies on (an extension of) Oyente, a tool that generates CFGs; EthIR produces from the CFGs, a rule-based representation (RBR) of the bytecode that enables the application of (existing) high-level analyses to infer properties of EVM code.", "num_citations": "67\n", "authors": ["1714"]}
{"title": "Introspective pushdown analysis of higher-order programs\n", "abstract": " In the static analysis of functional programs, pushdown flow analysis and abstract garbage collection skirt just inside the boundaries of soundness and decidability. Alone, each method reduces analysis times and boosts precision by orders of magnitude. This work illuminates and conquers the theoretical challenges that stand in the way of combining the power of these techniques. The challenge in marrying these techniques is not subtle: computing the reachable control states of a pushdown system relies on limiting access during transition to the top of the stack; abstract garbage collection, on the other hand, needs full access to the entire stack to compute a root set, just as concrete collection does. Introspective pushdown systems resolve this conflict. Introspective pushdown systems provide enough access to the stack to allow abstract garbage collection, but they remain restricted enough to compute control-state\u00a0\u2026", "num_citations": "67\n", "authors": ["1714"]}
{"title": "Mechanising Blockchain Consensus\n", "abstract": " We present the first formalisation of a blockchain-based distributed consensus protocol with a proof of its consistency mechanised in an interactive proof assistant.", "num_citations": "65\n", "authors": ["1714"]}
{"title": "Exploiting the laws of order in smart contracts\n", "abstract": " We investigate a family of bugs in blockchain-based smart contracts, which we dub event-ordering (or EO) bugs. These bugs are intimately related to the dynamic ordering of contract events, ie calls of its functions, and enable potential exploits of millions of USD worth of crypto-coins. Previous techniques to detect EO bugs have been restricted to those bugs that involve just one or two event orderings. Our work provides a new formulation of the general class of EO bugs arising in long permutations of such events by using techniques from concurrent program analysis. The technical challenge in detecting EO bugs in blockchain contracts is the inherent combinatorial blowup in path and state space analysis, even for simple contracts. We propose the first use of partial-order reduction techniques, using automatically extracted happens-before relations along with several dynamic symbolic execution optimizations. We\u00a0\u2026", "num_citations": "57\n", "authors": ["1714"]}
{"title": "Specifying and Verifying Concurrent Algorithms with Histories and Subjectivity.\n", "abstract": " We present a lightweight approach to Hoare-style specifications for fine-grained concurrency, based on a notion of time-stamped histories that abstractly capture atomic changes in the program state. Our key observation is that histories form a partial commutative monoid, a structure fundamental for representation of concurrent resources. This insight provides us with a unifying mechanism that allows us to treat histories just like heaps in separation logic. For example, both are subject to the same assertion logic and inference rules (e.g., the frame rule). Moreover, the notion of ownership transfer, which usually applies to heaps, has an equivalent in histories. It can be used to formally represent helping\u2014an important design pattern for concurrent algorithms whereby one thread can execute code on behalf of another. Specifications in terms of histories naturally abstract away the internal interference, so that\u00a0\u2026", "num_citations": "57\n", "authors": ["1714"]}
{"title": "Safer Smart Contract Programming with Scilla\n", "abstract": " The rise of programmable open distributed consensus platforms based on the blockchain technology has aroused a lot of interest in replicated stateful computations, aka smart contracts. As blockchains are used predominantly in financial applications, smart contracts frequently manage millions of dollars worth of virtual coins. Since smart contracts cannot be updated once deployed, the ability to reason about their correctness becomes a critical task. Yet, the de facto implementation standard, pioneered by the Ethereum platform, dictates smart contracts to be deployed in a low-level language, which renders independent audit and formal verification of deployed code infeasible in practice.   We report an ongoing experiment held with an industrial blockchain vendor on designing, evaluating, and deploying Scilla, a new programming language for safe smart contracts. Scilla is positioned as an intermediate-level\u00a0\u2026", "num_citations": "54\n", "authors": ["1714"]}
{"title": "Gradual Ownership Types\n", "abstract": " Gradual Ownership Types are a framework allowing programs to be partially annotated with ownership types, while providing the same encapsulation guarantees. The formalism provides a static guarantee of the desired encapsulation property for fully annotated programs, and dynamic guarantees for partially annotated programs via dynamic checks inserted by the compiler. This enables a smooth migration from ownership-unaware to ownership-typed code.             The paper provides a formal account of gradual ownership types. The theoretical novelty of this work is in adapting the notion of gradual type system with respect to program heap properties, which, unlike types in functional languages or object calculi, impose restrictions not only on data, but also on the environment the data is being processed in. From the practical side, we evaluate applicability of Gradual Ownership Types for Java 1.4 in the\u00a0\u2026", "num_citations": "52\n", "authors": ["1714"]}
{"title": "A semantics for context-oriented programming with layers\n", "abstract": " Context-oriented programming (COP) is a new programming approach whereby the context in which expressions evaluate can be adapted as a program runs. COP provides a degree of flexibility beyond object-oriented programming, while arguably retaining more modularity and structure than aspect-oriented programming. Although many languages exploring the context-oriented approach exist, to our knowledge no formal type-sound dynamic semantics of these languages exists. We address this shortcoming by providing a concise syntax-based formal semantics for context-oriented programming with layers, as witnessed by ContextL, ContextJ*, and other languages. Our language is based on Featherweight Java extended with layers and scoped layer activation and deactivation. As layers may introduce methods not appearing in classes, we also give a static type system that ensures that no program gets stuck\u00a0\u2026", "num_citations": "36\n", "authors": ["1714"]}
{"title": "Modular, higher order cardinality analysis in theory and practice\n", "abstract": " Since the mid '80s, compiler writers for functional languages (especially lazy ones) have been writing papers about identifying and exploiting thunks and lambdas that are used only once. However, it has proved difficult to achieve both power and simplicity in practice. In this paper, we describe a new, modular analysis for a higher order language, which is both simple and effective. We prove the analysis sound with respect to a standard call-by-need semantics, and present measurements of its use in a full-scale, state-of-the-art optimising compiler. The analysis finds many single-entry thunks and one-shot lambdas and enables a number of program optimisations. This paper extends our preceding conference publication (Sergey et al. 2014 Proceedings of the 41st Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL 2014). ACM, pp. 335\u2013348) with proofs, expanded report on\u00a0\u2026", "num_citations": "32\n", "authors": ["1714"]}
{"title": "Structuring the Synthesis of Heap-Manipulating Programs\n", "abstract": " This paper describes a deductive approach to synthesizing imperative programs with pointers from declarative specifications expressed in Separation Logic. Our synthesis algorithm takes as input a pair of assertions\u2014a pre- and a postcondition\u2014which describe two states of the symbolic heap, and derives a program that transforms one state into the other, guided by the shape of the heap. Our approach to program synthesis is grounded in proof theory: we introduce the novel framework of Synthetic Separation Logic (SSL), which generalises the classical notion of heap entailment P \u22a2 Q to incorporate a possibility of transforming a heap satisfying an assertion P into a heap satisfying an assertion Q. A synthesized program represents a proof term for a transforming entailment statement P \u219d Q, and the synthesis procedure corresponds to a proof search. The derived programs are, thus, correct by construction, in the\u00a0\u2026", "num_citations": "28\n", "authors": ["1714"]}
{"title": "Paxos Consensus, Deconstructed and Abstracted\n", "abstract": " Lamport\u2019s Paxos algorithm is a classic consensus protocol for state machine replication in environments that admit crash failures. Many versions of Paxos exploit the protocol\u2019s intrinsic properties for the sake of gaining better run-time performance, thus widening the gap between the original description of the algorithm, which was proven correct, and its real-world implementations. In this work, we address the challenge of specifying and verifying complex Paxos-based systems by (a) devising composable specifications for implementations of Paxos\u2019s singledecree version, and (b) engineering disciplines to reason about protocolaware, semantics-preserving optimisations to single-decree Paxos. In a nutshell, our approach elaborates on the deconstruction of single-decree Paxos by Boichat et al. We provide novel non-deterministic specifications for each module in the deconstruction and prove that the implementations refine the corresponding specifications, such that the proofs of the modules that remain unchanged can be reused across different implementations. We further reuse this result and show how to obtain a verified implementation of Multi-Paxos from a verified implementation of singledecree Paxos, by a series of novel protocol-aware transformations of the network semantics, which we prove to be behaviour-preserving.", "num_citations": "24\n", "authors": ["1714"]}
{"title": "Hoare-style specifications as correctness conditions for non-linearizable concurrent objects\n", "abstract": " Designing efficient concurrent objects often requires abandoning the standard specification technique of linearizability in favor of more relaxed correctness conditions. However, the variety of alternatives makes it difficult to choose which condition to employ, and how to compose them when using objects specified by different conditions.   In this work, we propose a uniform alternative in the form of Hoare logic, which can explicitly capture--in the auxiliary state--the interference of environment threads. We demonstrate the expressiveness of our method by verifying a number of concurrent objects and their clients, which have so far been specified only by non-standard conditions of concurrency-aware linearizability, quiescent, and quantitative quiescent consistency. We report on the implementation of the ideas in an existing Coq-based tool, providing the first mechanized proofs for all the examples in the paper.", "num_citations": "24\n", "authors": ["1714"]}
{"title": "Temporal Properties of Smart Contracts\n", "abstract": " Smart contracts\u2014shared stateful reactive objects stored on a blockchain\u2014are widely employed nowadays for mediating exchanges of crypto-currency between multiple untrusted parties. Despite a lot of attention given by the formal methods community to the notion of smart contract correctness, only a few efforts targeted their lifetime properties. In this paper, we focus on reasoning about execution traces of smart contracts. We report on our preliminary results of mechanically verifying some of such properties by embedding a smart contract language into the Coq proof assistant. We also discuss several common scenarios, all of which require multi-step blockchain-based arbitration and thus must be implemented via stateful contracts, and discuss possible temporal specifications of the corresponding smart contract implementations.", "num_citations": "23\n", "authors": ["1714"]}
{"title": "Concurrent Data Structures Linked in Time\n", "abstract": " Arguments about correctness of a concurrent data structure are typically carried out by using the notion of linearizability and specifying the linearization points of the data structure's procedures. Such arguments are often cumbersome as the linearization points' position in time can be dynamic (depend on the interference, run-time values and events from the past, or even future), non-local (appear in procedures other than the one considered), and whose position in the execution trace may only be determined after the considered procedure has already terminated. In this paper we propose a new method, based on a separation-style logic, for reasoning about concurrent objects with such linearization points. We embrace the dynamic nature of linearization points, and encode it as part of the data structure's auxiliary state, so that it can be dynamically modified in place by auxiliary code, as needed when some appropriate run-time event occurs. We name the idea linking-in-time, because it reduces temporal reasoning to spatial reasoning. For example, modifying a temporal position of a linearization point can be modeled similarly to a pointer update in separation logic. Furthermore, the auxiliary state provides a convenient way to concisely express the properties essential for reasoning about clients of such concurrent objects. We illustrate the method by verifying (mechanically in Coq) an intricate optimal snapshot algorithm due to Jayanti, as well as some clients.", "num_citations": "18\n", "authors": ["1714"]}
{"title": "Pushdown Flow Analysis with Abstract Garbage Collection\n", "abstract": " In the static analysis of functional programs, pushdown flow analysis and abstract garbage collection push the boundaries of what we can learn about programs statically. This work illuminates and poses solutions to theoretical and practical challenges that stand in the way of combining the power of these techniques. Pushdown flow analysis grants unbounded yet computable polyvariance to the analysis of return-flow in higher-order programs. Abstract garbage collection grants unbounded polyvariance to abstract addresses which become unreachable between invocations of the abstract contexts in which they were created. Pushdown analysis solves the problem of precisely analyzing recursion in higher-order languages; abstract garbage collection is essential in solving the \u201cstickiness\u201d problem. Alone, our benchmarks demonstrate that each method can reduce analysis times and boost precision by orders of\u00a0\u2026", "num_citations": "17\n", "authors": ["1714"]}
{"title": "Operational aspects of C/C++ concurrency\n", "abstract": " In this work, we present a family of operational semantics that gradually approximates the realistic program behaviors in the C/C++11 memory model. Each semantics in our framework is built by elaborating and combining two simple ingredients: viewfronts and operation buffers. Viewfronts allow us to express the spatial aspect of thread interaction, i.e., which values a thread can read, while operation buffers enable manipulation with the temporal execution aspect, i.e., determining the order in which the results of certain operations can be observed by concurrently running threads. Starting from a simple abstract state machine, through a series of gradual refinements of the abstract state, we capture such language aspects and synchronization primitives as release/acquire atomics, sequentially-consistent and non-atomic memory accesses, also providing a semantics for relaxed atomics, while avoiding the Out-of-Thin-Air problem. To the best of our knowledge, this is the first formal and executable operational semantics of C11 capable of expressing all essential concurrent aspects of the standard. We illustrate our approach via a number of characteristic examples, relating the observed behaviors to those of standard litmus test programs from the literature. We provide an executable implementation of the semantics in PLT Redex, along with a number of implemented litmus tests and examples, and showcase our prototype on a large case study: randomized testing and debugging of a realistic Read-Copy-Update data structure.", "num_citations": "14\n", "authors": ["1714"]}
{"title": "Calculating graph algorithms for dominance and shortest path\n", "abstract": " We calculate two iterative, polynomial-time graph algorithms from the literature: a dominance algorithm and an algorithm for the single-source shortest path problem. Both algorithms are calculated directly from the definition of the properties by fixed-point fusion of (1) a least fixed point expressing all finite paths through a directed graph and (2) Galois connections that capture dominance and path length.               The approach illustrates that reasoning in the style of fixed-point calculus extends gracefully to the domain of graph algorithms. We thereby bridge common practice from the school of program calculation with common practice from the school of static program analysis, and build a novel view on iterative graph algorithms as instances of abstract interpretation.", "num_citations": "9\n", "authors": ["1714"]}
{"title": "A correspondence between type checking via reduction and type checking via evaluation\n", "abstract": " We describe a derivational approach to proving the equivalence of different representations of a type system. Different ways of representing type assignments are convenient for particular applications such as reasoning or implementation, but some kind of correspondence between them should be proven. In this paper we address two such semantics for type checking: one, due to Kuan et al., in the form of a term rewriting system and the other in the form of a traditional set of derivation rules. By employing a set of techniques investigated by Danvy et al., we mechanically derive the correspondence between a reduction-based semantics for type checking and a traditional one in the form of derivation rules, implemented as a recursive descent. The correspondence is established through a series of semantics-preserving functional program transformations.", "num_citations": "9\n", "authors": ["1714"]}
{"title": "Cyclic Program Synthesis\n", "abstract": " We describe the first approach to automatically synthesizing heap-manipulating programs with auxiliary recursive procedures. Such procedures occur routinely in data structure transformations (eg, flattening a tree into a list) or traversals of composite structures (eg, n-ary trees). Our approach, dubbed cyclic program synthesis, enhances deductive program synthesis with a novel application of cyclic proofs. Specifically, we observe that the machinery used to form cycles in cyclic proofs can be reused to systematically and efficiently abduce recursive auxiliary procedures.", "num_citations": "7\n", "authors": ["1714"]}
{"title": "Towards Mechanising Probabilistic Properties of a Blockchain\n", "abstract": " We present our progress on the formalisation and mechanisation of a probabilistic model of a blockchain consensus protocol in Coq, taking steps towards the formal verification of its security properties, stated in terms of probabilities, in an adversarial environment.", "num_citations": "7\n", "authors": ["1714"]}
{"title": "Deriving Interpretations of the Gradually-Typed Lambda Calculus\n", "abstract": " Siek and Garcia (2012) have explored the dynamic semantics of the gradually-typed lambda calculus by means of definitional interpreters and abstract machines. The correspondence between the calculus's mathematically described small-step reduction semantics and the implemented big-step definitional interpreters was left as a conjecture. We prove and generalise Siek and Garcia's conjectures using program transformation. We establish the correspondence between the definitional interpreters and the reduction semantics of a closure-converted gradually-typed lambda calculus that unifies and amends various versions of the calculus. We use a layered approach and two-level continuation-passing style so that the correspondence is parametric on the subsidiary coercion calculus. We have implemented the whole derivation for the eager error-detection policy and the downcast blame-tracking strategy. The\u00a0\u2026", "num_citations": "7\n", "authors": ["1714"]}
{"title": "From type checking by recursive descent to type checking with an abstract machine\n", "abstract": " Modern type systems for programming languages usually incorporate additional information useful for program analysis, eg, effects, control flow, non-interference, strictness etc. When designing a typing predicate for such systems, a form of logical derivation rules is normally taken. Despite the expressivity of this approach, the straightforward implementation of an appropriate type checker is usually inefficient in terms of stack consumption and further optimisations. This leads to a significant gap between an analysis and program implementing the analysis.", "num_citations": "7\n", "authors": ["1714"]}
{"title": "Certifying Certainty and Uncertainty in Approximate Membership Query Structures\n", "abstract": " Approximate Membership Query structures (AMQs) rely on randomisation for time- and space-efficiency, while introducing a possibility of false positive and false negative answers. Correctness proofs of such structures involve subtle reasoning about bounds on probabilities of getting certain outcomes. Because of these subtleties, a number of unsound arguments in such proofs have been made over the years.                 In this work, we address the challenge of building rigorous and reusable computer-assisted proofs about probabilistic specifications of AMQs. We describe the framework for systematic decomposition of AMQs and their properties into a series of interfaces and reusable components. We implement our framework as a library in the Coq proof assistant and showcase it by encoding in it a number of non-trivial AMQs, such as Bloom filters, counting filters, quotient filters and blocked constructions\u00a0\u2026", "num_citations": "5\n", "authors": ["1714"]}
{"title": "Concise Read-Only Specifications for Better Synthesis of Programs with Pointers\n", "abstract": " In program synthesis there is a well-known trade-off between concise and strong specifications: if a specification is too verbose, it might be harder to write than the program; if it is too weak, the synthesised program might not match the user\u2019s intent. In this work we explore the use of annotations for restricting memory access permissions in program synthesis, and show that they can make specifications much stronger while remaining surprisingly concise. Specifically, we enhance Synthetic Separation Logic (SSL), a framework for synthesis of heap-manipulating programs, with the logical mechanism of read-only borrows. We observe that this minimalistic and conservative SSL extension benefits the synthesis in several ways, making it more (a) expressive (stronger correctness guarantees are achieved with a modest annotation overhead),(b) effective (it produces more concise and easier-to-read programs),(c) efficient (faster synthesis), and (d) robust (synthesis efficiency is less affected by the choice of the search heuristic). We explain the intuition and provide formal treatment for read-only borrows. We substantiate the claims (a)\u2013(d) by describing our quantitative evaluation of the borrowing-aware synthesis implementation on a series of standard benchmark specifications for various heap-manipulating programs.", "num_citations": "5\n", "authors": ["1714"]}
{"title": "Distributed protocol combinators\n", "abstract": " Distributed systems are hard to get right, model, test, debug, and teach. Their textbook definitions, typically given in a form of replicated state machines, are concise, yet prone to introducing programming errors if na\u00efvely translated into runnable implementations.               In this work, we present Distributed Protocol Combinators (DPC), a declarative programming framework that aims to bridge the gap between specifications and runnable implementations of distributed systems, and facilitate their modeling, testing, and execution. DPC builds on the ideas from the state-of-the art logics for compositional systems verification. The contribution of DPC is a novel family of program-level primitives, which facilitates construction of larger distributed systems from smaller components, streamlining the usage of the most common asynchronous message-passing communication patterns, and providing machinery for testing\u00a0\u2026", "num_citations": "5\n", "authors": ["1714"]}
{"title": "\u0420\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0433\u0438\u0431\u0440\u0438\u0434\u043d\u044b\u0445 \u0442\u0438\u043f\u043e\u0432 \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f \u0432 Java \u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u043d\u044b\u0445 \u0433\u0440\u0430\u043c\u043c\u0430\u0442\u0438\u043a\n", "abstract": " \u0412 \u0434\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435 \u043c\u044b \u0438\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0433\u0438\u0431\u0440\u0438\u0434\u043d\u043e\u0433\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043a \u0442\u0438\u043f\u0430\u043c \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f \u0438 \u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044e \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440\u0430 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043a\u043e\u0434\u0430, \u043b\u0438\u0448\u044c \u0447\u0430\u0441\u0442\u0438\u0447\u043d\u043e \u0430\u043d\u043d\u043e\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0442\u0438\u043f\u0430\u043c\u0438 \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f. \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0441\u0442 \u0434\u043e\u043b\u0436\u0435\u043d \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c \u0430\u043d\u043d\u043e\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043b\u0438\u0448\u044c \u0434\u043b\u044f \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b, \u0430 \u043f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440 \u0434\u043e\u0431\u0430\u0432\u0438\u0442 \u0432 \u043a\u043e\u0434 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u0442\u0435\u0445 \u0441\u043b\u0443\u0447\u0430\u0435\u0432, \u043a\u043e\u0433\u0434\u0430 \u0441\u0442\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0442\u0438\u043f\u043e\u0432 \u043d\u0435 \u0434\u0430\u0451\u0442 \u043f\u043e\u043b\u043d\u043e\u0439 \u0433\u0430\u0440\u0430\u043d\u0442\u0438\u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0438\u043d\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 \u0432\u043b\u0430\u0434\u0435\u043d\u0438\u044f. \u041f\u0440\u0435\u043f\u0440\u043e\u0446\u0435\u0441\u0441\u043e\u0440 \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d \u043f\u043e\u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c \u043f\u0430\u0440\u0430\u0434\u0438\u0433\u043c\u044b \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u043d\u044b\u0445 \u0433\u0440\u0430\u043c\u043c\u0430\u0442\u0438\u043a, \u0447\u0442\u043e \u0434\u0435\u043b\u0430\u0435\u0442 \u0435\u0433\u043e \u043b\u0435\u0433\u043a\u043e \u0430\u0434\u0430\u043f\u0442\u0438\u0440\u0443\u0435\u043c\u044b\u043c \u0434\u043b\u044f \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0445 \u0440\u0430\u0441\u0448\u0438\u0440\u0435\u043d\u0438\u0439 \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043d\u043e\u0432\u044b\u0445 \u0432\u0438\u0434\u043e\u0432 \u0441\u0442\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0430\u043d\u0430\u043b\u0438\u0437\u0430.", "num_citations": "5\n", "authors": ["1714"]}
{"title": "Practical Smart Contract Sharding with Ownership and Commutativity Analysis\n", "abstract": " Sharding is a popular way to achieve scalability in blockchain protocols, increasing their throughput by partitioning the set of transaction validators into a number of smaller committees, splitting the workload. Existing approaches for blockchain sharding, however, do not scale well when concurrent transactions alter the same replicated state component\u2014a common scenario in Ethereum-style smart contracts.", "num_citations": "3\n", "authors": ["1714"]}
{"title": "Automated Repair of Heap-Manipulating Programs using Deductive Synthesis\n", "abstract": " We propose a novel method to automatically repairing buggy heap-manipulating programs using constraint solving and deductive synthesis. Given an input program  and its formal specification in the form of a Hoare triple: , we use a separation-logic-based verifier to verify if program  is correct w.r.t. its specifications. If program  is found buggy, we then repair it in the following steps. First, we rely on the verification results to collect a list of suspicious statements of the buggy program. For each suspicious statement, we temporarily replace it with a template patch representing the desired statements. The template patch is also formally specified using a pair of unknown pre- and postcondition. Next, we use the verifier to analyze the temporarily patched program to collect constraints related to the pre- and postcondition of the template patch. Then, these constraints are solved by our constraint solving\u00a0\u2026", "num_citations": "3\n", "authors": ["1714"]}
{"title": "Experience Report: Growing and Shrinking Polygons for Random Testing of Computational Geometry Algorithms\n", "abstract": " This paper documents our experience of adapting and using the QuickCheck-style approach for extensive randomised property-based testing of computational geometry algorithms.   The need in rigorous evaluation of computational geometry procedures has naturally arisen in our quest of organising a medium-size programming contest for second year university students\u2014an experiment we conducted as an attempt to introduce them to computational geometry. The main effort in organising the event was implementation of a solid infrastructure for testing and ranking solutions. For this, we employed functional programming techniques. The choice of the language and the paradigm made it possible for us to engineer, from scratch and in a very short period of time, a series of robust geometric primitives and algorithms, as well as implement a scalable framework for their randomised testing.   We describe the main\u00a0\u2026", "num_citations": "3\n", "authors": ["1714"]}
{"title": "Dominance analysis via ownership types and abstract interpretation\n", "abstract": " Ownership types provide a declarative way to statically structure the topology of the heap and control aliasing in object-oriented programs. However, the relation between systematically derived static program analyses by abstract interpretation and semantic properties enforced by ownership types has not yet been investigated. In this work we build a framework to statically compute an abstract object dominance tree, based on the information provided by ownership types in the context of the owners-as-dominators policy. We develop a series of concrete and abstract domains that enable us to abstract a tree-like structure in a way that respects the ancestor relation between particular nodes of the tree and prove them to be Galois connections. We then plug the developed domains into a traditional abstract interpretation-based points-to analysis. The resulting abstract semantics is derived systematically from the concrete transition relation instrumented for tree computation and proven to be sound. The presented framework is tunable concerning polyvariance: the resulting abstract tree depends on the underlying context picking strategy.", "num_citations": "3\n", "authors": ["1714"]}
{"title": "Towards gradual ownership types\n", "abstract": " In this work we present a formal system to annotate programs with ownership types in a lightweight way, allowing only partial information about object owners. We adapt the system of Clarke and Drossopoulou to include gradual types of Siek and Taha. The resulting framework allows one to annotate programs incrementally with ownership types. For fully annotated programs the developed formalism provides a static guarantee of the desired encapsulation invariant, whereas for partially annotated programs necessary dynamic checks are inserted by the compiler.", "num_citations": "3\n", "authors": ["1714"]}
{"title": "Scripting an IDE for EDSL awareness\n", "abstract": " Modern dynamic programming languages provide various mechanisms to implement embedded domain-specific languages (EDSLs), usually based on the meta-object protocol or delegation. The main disadvantages of this approach are the difficulty of statically analyzing domain-specific constraints and providing reasonable code navigation in an existing integrated development environment (IDE), even when the IDE is aware of the host language\u2019s semantics. In this paper we present GroovyDSL, a flexible framework for describing semantics-based code assistance for custom EDSLs. GroovyDSL is based on the IntelliJ IDEA environment and allows a developer to add new rules to implement EDSL-aware references resolution and smart code completion. We present a fully implemented small language to describe such rules in a natural way for an EDSL, based on the Groovy programming language, abstracting from the IDE\u2019s internal language representation.", "num_citations": "2\n", "authors": ["1714"]}
{"title": "Automatic refactorings for Scala programs\n", "abstract": " Automatic refactorings for Scala programs Page 1 Automatic refactorings for Scala programs Ilya Sergey Dave Clarke Alexander Podkhalyuzin 15 April 2010 Scala Days 2010 Taming multi-paradigm code Page 2 Outline \u2022 Why implementing refactorings for Scala is challenging? \u2022 What refactorings have we implemented for now? \u2022 What kind of Scala-specific refactorings could be useful? Page 3 What are refactorings good for for \u2022 Cleaning up code \u2022 Changing internal code structure and design \u2022 Improving understandability \u2022 Providing better modularization Page 4 What are refactorings not used for \u2022 Adding new functionality \u2022 Fixing bugs \u2022 Changing overall program behaviour Page 5 Case study: extract method refactoring in Java void printBanner() { String name = getName(); final String banner = getBanner(); //begin System.out.println(\"name: \" + name); name = \"Mr. \" + name; System.out.println(\"banner \" + banner); // \u2026", "num_citations": "2\n", "authors": ["1714"]}
{"title": "Protocol Combinators for Modeling, Testing, and Execution of Distributed Systems\n", "abstract": " Distributed systems are hard to get right, model, test, debug, and teach. Their textbook definitions, typically given in a form of replicated state machines, are concise, yet prone to introducing programming errors if na\u00efvely translated into runnable implementations.       In this work, we present Distributed Protocol Combinators (DPC), a declarative programming framework that aims to bridge the gap between specifications and runnable implementations of distributed systems, and facilitate their modeling, testing, and execution. DPC builds on the ideas from the state-of-the art logics for compositional systems verification. The contribution of DPC is a novel family of program-level primitives, which facilitates construction of larger distributed systems from smaller components, streamlining the usage of the most common asynchronous message-passing communication patterns, and providing machinery for testing and user\u00a0\u2026", "num_citations": "1\n", "authors": ["1714"]}
{"title": "Engineering distributed systems that we can trust (and also run)\n", "abstract": " The interest in formal methods and verification of correctness-critical distributed systems is on the rise in the past few years. But what are the gains from proving statements about software in full mathematical rigour? Do they justify the high cost of verification? And how far can we extend our trust in formal methods when talking about realistic distributed systems and their client programs?", "num_citations": "1\n", "authors": ["1714"]}