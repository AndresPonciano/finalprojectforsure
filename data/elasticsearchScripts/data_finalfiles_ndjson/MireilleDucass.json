{"title": "M2D2: A formal data model for IDS alert correlation\n", "abstract": " At present, alert correlation techniques do not make full use of the information that is available. We propose a data model for IDS alert correlation called M2D2. It supplies four information types: information related to the characteristics of the monitored information system, information about the vulnerabilities, information about the security tools used for the monitoring, and information about the events observed. M2D2 is formally defined. As far as we know, no other formal model includes the vulnerability and alert parts of M2D2. Three examples of correlations are given. They are rigorously specified using the formal definition of M2D2. As opposed to already published correlation methods, these examples use more than the events generated by security tools; they make use of many concepts formalized in M2D2.", "num_citations": "360\n", "authors": ["1827"]}
{"title": "A serial combination of anomaly and misuse IDSes applied to HTTP traffic\n", "abstract": " Combining an \"anomaly\" and a \"misuse\" IDSes offers the advantage of separating the monitored events between normal, intrusive or unqualified classes (i.e. not known as an attack, but not recognize as safe either). In this article, we provide a framework to systematically reason about the combination of anomaly and misuse components. This framework applied to Web servers lead us to propose a serial architecture, using a drastic anomaly component with a sensitive misuse component. This architecture provides the operator with better qualification of the detection results, raises lower amount of false alarms and unqualified events.", "num_citations": "170\n", "authors": ["1827"]}
{"title": "A logic-based model to support alert correlation in intrusion detection\n", "abstract": " Managing and supervising security in large networks has become a challenging task, as new threats and flaws are being discovered on a daily basis. This requires an in depth and up-to-date knowledge of the context in which security-related events occur. Several tools have been proposed to support security operators in this task, each of which focuses on some specific aspects of the monitoring. Many alarm fusion and correlation approaches have also been investigated. However, most of these approaches suffer from two major drawbacks. First, they only take advantage of the information found in alerts, which is not sufficient to achieve the goals of alert correlation, that is to say to reduce the overall amount of alerts, while enhancing their semantics. Second, these techniques have been designed on an ad hoc basis and lack a shared data model that would allow them to reason about events in a cooperative way\u00a0\u2026", "num_citations": "130\n", "authors": ["1827"]}
{"title": "Coca: An automated debugger for C\n", "abstract": " Presents Coca, an automated debugger for C, where the breakpoint mechanism is based on events related to language constructs. Events have semantics, whereas the source lines used by most debuggers do not have any. A trace is a sequence of events. It can be seen as an ordered relation in a database. Users can specify precisely which events they want to see by specifying values for event attributes. At each event, visible variables can be queried. The trace query language is Prolog with a handful of primitives. The trace query mechanism searches through the execution traces using both control flow and data, whereas debuggers usually search according to either control flow or data. As opposed to fully \"relational\" debuggers which use plain database querying mechanisms, the Coca trace querying mechanism does not require any storage. The analysis is done on-the-fly, synchronously with the traced\u00a0\u2026", "num_citations": "114\n", "authors": ["1827"]}
{"title": "Opium: An extendable trace analyzer for prolog\n", "abstract": " Traces of program executions are a helpful source of information for program debugging. They, however, give a picture of program executions at such a low level that users often have difficulties to interpret the information. Opium, our extendable trace analyzer, is connected to a \u201cstandard\u201d Prolog tracer. Opium is programmable and extendable. It provides a trace query language and abstract views of executions. Users can therefore examine program executions at the levels of abstraction which suit them. Opium has shown its capabilities to build abstract tracers and automated debugging facilities. This article describes in depth the trace query mechanism, from the model to its implementation. Characteristic examples are detailed. Extensions written so far on top of the trace query mechanism are listed. Two recent extensions are presented: the abstract tracers for the LO (Linear Objects) and the CHR (Constraint\u00a0\u2026", "num_citations": "101\n", "authors": ["1827"]}
{"title": "A review of automated debugging systems: Knowledge, strategies and techniques\n", "abstract": " The authors propose a classification of debugging knowledge, and a description of the corresponding knowledge representation in the systems. Then they propose a classification of global debugging strategies used in the systems, and a description of the corresponding techniques. They assess the identified strategies from a real-world program development point of view. The knowledge types identified are:(1) knowledge of the intended program;(2) knowledge of the actual program;(3) understanding of the programming language;(4) general programming expertise;(5) knowledge of the application domain;(6) knowledge of bugs; and (7) knowledge of debugging methods. The strategies identified are:(1) filtering;(2) checking computational equivalence of intended program and actual one;(3) checking the well-formedness of actual program; and (4) recognizing stereotyped errors.<>", "num_citations": "84\n", "authors": ["1827"]}
{"title": "A pragmatic survey of automated debugging\n", "abstract": " This article proposes a structuring view of the area of automated debugging. Nineteen automated debugging systems are analyzed. Thirteen existing automated debugging techniques are briefly evaluated from a pragmatic point of view. The three underlying strategies are identified, namely verification with respect to specification, checking with respect to language knowledge and filtering with respect to symptom.             The verification strategy compares the actual program with some formal specification of the intended program. The checking strategy looks for suspect places which do not comply with some explicit knowledge of the programming language. The filtering strategy assumes correct parts of the code which cannot be responsible for the error symptom.             Assertion evaluation and algorithmic debugging are the most promising verification techniques. Some intrinsic limitations of the checking\u00a0\u2026", "num_citations": "70\n", "authors": ["1827"]}
{"title": "Logic programming environments: Dynamic program analysis and debugging\n", "abstract": " Programming environments are essential for the acceptance of programming languages. This survey emphasizes that program analysis, both static and dynamic, is the central issue of programming environments. Because their clean semantics makes powerful analysis possible, logic programming languages have an indisputable asset in the long term.This survey is focused on logic program analysis and debugging. The large number of references provided show that the field, although maybe scattered, is active. A unifying framework is given which separates environment tools into extraction, analysis, and visualization. It facilitates the analysis of existing tools and should give some guidelines to develop new ones.Achievements in logic programming are listed; some techniques developed for other languages are pointed out, and some trends for further research are drawn. Among the main achievements are\u00a0\u2026", "num_citations": "68\n", "authors": ["1827"]}
{"title": "Data mining and cross-checking of execution traces: a re-interpretation of jones, harrold and stasko test information\n", "abstract": " The current trend in debugging and testing is to cross-check information collected during several executions. Jones et al., for example, propose to use the instruction coverage of passing and failing runs in order to visualize suspicious statements. This seems promising but lacks a formal justification. In this paper, we show that the method of Jones et al. can be re-interpreted as a data mining procedure. More particularly, they define an indicator which characterizes association rules between data. With this formal framework we are able to explain intrinsic limitations of the above indicator.", "num_citations": "62\n", "authors": ["1827"]}
{"title": "A backward slicing algorithm for Prolog\n", "abstract": " Slicing is a program analysis technique originally developed by Weiser for imperative languages. Weiser showed that slicing is a natural tool for debugging, but it has other numerous applications (program integration, program optimization, etc.)             In this article we describe a backward slicing algorithm for Prolog which produces executable slices. The proposed algorithm is applicable at least to pure Prolog extended by some simple built-in predicates that handle the explicit unification=/2 and arithmetic.             To our knowledge, this algorithm is the first one to be proposed for Prolog. Because of the indeterminism and lack of explicit control flow of Prolog, existing algorithms cannot be trivially adapted. The two main contributions of this paper are a general definition of slicing adapted to Prolog and a slicing algorithm that produces executable programs.", "num_citations": "50\n", "authors": ["1827"]}
{"title": "Using events to debug Java programs backwards in time\n", "abstract": " An\" Omniscient Debugger\" works by recording all state changes in the run of a program, and then allowing the programmer to explore the history of that program-effectively going\" backwards in time.\" Event analysis debuggers work by observing events as they occur, and allowing the programmer to write queries which will pause the program when matched-effectively highly sophisticated breakpoints. Recently we have integrated the two techniques to produce an omniscient debugger which can use event queries to search the history of a program interactively. The query mechanism is designed along the lines of an EMACS incremental search, where the query is typed into a\" minibuffer\" at the bottom of the debugger window, and the commands\" next match\" and\" previous match\" are single keystrokes. The result is instantaneous feedback with no danger of missing an interesting state by going too far.", "num_citations": "46\n", "authors": ["1827"]}
{"title": "From declarative signatures to misuse IDS\n", "abstract": " In many existing misuse intrusion detection systems, intrusion signatures are very close to the detection algorithms. As a consequence, they contain too many cumbersome details. Recent work have proposed declarative signature languages that raise the level of abstraction when writing signatures. However, these languages do not always come with operational support. In this article, we show how to transform such declarative signatures into operational ones. This process points out several technical details which must be considered with care when performing the translation by hand, but which can be systematically handled.               A signature specification language named Sutekh is proposed. Its declarative semantics is precisely described. To produce rules for existing rule-based IDS from Sutekh signatures, an algorithm, based on the construction of a state-transition diagram, is given.", "num_citations": "43\n", "authors": ["1827"]}
{"title": "Formal Specification of Intrusion Signatures and Detection Rules.\n", "abstract": " 1 Misuse intrusion detection systems detect signatures of attack scenarios. Existing systems are split into two categories: transition-based and declarative. In the transitionbased systems what are the significant traces of attacks is hidden behind how they should be detected. This means that writing a signature is a very heavy task. In the declarative systems the signatures only contain what are the significant traces of attacks and an algorithm addresses how they should be detected. Writing signatures is thus much easier. However, the algorithm is a black box, and the security officer has no control over it.In this article, we propose to refine the declarative approach. We formally specify the algorithm in two stages: firstly we classify the signature instances, secondly we give a detection rule set which detects in an audit trail a representative of each class. The rules are formally specified with \u201cparsing schemata\u201d, a high level formalism used to specify grammar parsers. The algorithm defined by the rules is proved sound and complete. With our approach, the what (signatures) and the how (detection algorithm) are still cleanly separated, but the security officer can possibly parameterize the detection by choosing a class for each signature.", "num_citations": "42\n", "authors": ["1827"]}
{"title": "Myrtle: A set-oriented meta-interpreter driven by a \u201crelational\u201d trace for deductive databases debugging\n", "abstract": " Deductive databases manage large quantities of data and, in general, in a set-oriented way. The existing explanation systems for deductive databases [6,4,1] give information in the shape of forests of proof trees. Although proof trees are often useful, this representation is not sufficient. We propose a tracing technique which consists of integrating a \u201drelational\u201d trace and an instrumented meta-interpreter using substitution sets. The relational trace efficiently gives precise information about data extraction from the relational database. The meta-interpreter manages substitution sets and gives explanation on the deduction. The expensive aspects of meta-interpretation are reduced by the use of the trace which avoids many calculations. The flexibility of meta-interpretation is preserved. It allows different profiles of trace to be easily produced.", "num_citations": "35\n", "authors": ["1827"]}
{"title": "An abstract interpretation based combinator for modelling while loops in constraint programming\n", "abstract": " We present the w constraint combinator that models while loops in Constraint Programming. Embedded in a finite domain constraint solver, it allows programmers to develop non-trivial arithmetical relations using loops, exactly as in an imperative language style. The deduction capabilities of this combinator come from abstract interpretation over the polyhedra abstract domain. This combinator has already demonstrated its utility in constraint-based verification and we argue that it also facilitates the rapid prototyping of arithmetic constraints (e.g. power, gcd or sum).", "num_citations": "30\n", "authors": ["1827"]}
{"title": "Improving constraint-based testing with dynamic linear relaxations\n", "abstract": " Constraint-Based Testing (CBT) is the process of generating test cases against a testing objective by using constraint solving techniques. In CBT, testing objectives are given under the form of properties to be satisfied by program's input/output. Whenever the program or the properties contain disjunctions or multiplications between variables, CBT faces the problem of solving non-linear constraint systems. Currently, existing CBT tools tackle this problem by exploiting a finite-domains constraint solver. But, solving a non-linear constraint system over finite domains is NP hard and CBT tools fail to handle properly most properties to be tested. In this paper, we present a CBT approach where a finite domain constraint solver is enhanced by Dynamic Linear Relaxations (DLRs). DLRs are based on linear abstractions derived during the constraint solving process. They dramatically increase the solving capabilities of the\u00a0\u2026", "num_citations": "29\n", "authors": ["1827"]}
{"title": "An interactive guidance process supporting consistent updates of RDFS graphs\n", "abstract": " With existing tools, when creating a new object in the Semantic Web, users benefit neither from existing objects and their properties, nor from the already known properties of the new object. We propose UTILIS, an interactive process to help users add new objects. While creating a new object, relaxation rules are applied to its current description to find similar objects, whose properties serve as suggestions to expand the description. A user study conducted on a group of master students shows that students, even the ones disconcerted by the unconventional interface, used UTILIS suggestions. In most cases, they could find the searched element in the first three sets of properties of similar objects. Moreover, with UTILIS users did not create any duplicate whereas with the other tool used in the study more than half of them did.", "num_citations": "27\n", "authors": ["1827"]}
{"title": "A propagation tracer for gnu-prolog: from formal definition to efficient implementation\n", "abstract": " Tracers give some insight of program executions: with an execution trace a programmer can debug and tune programs. Traces can also be used by analysis tools, for example to produce statistics or build graphical views of program behaviors. Constraint propagation tracers are especially needed because constraint propagation problems are particularly hard to debug. Yet, there is no satisfactory tracer for CLP(FD) systems. Some do not provide enough information, others are very inefficient. The tracer formally described in this article provides more complete information than existing propagation tracers. Benchmarks show that its implementation is efficient. Its formal specification is useful both to implement the tracer and to understand the produced trace. It is designed to cover many debugging needs.", "num_citations": "24\n", "authors": ["1827"]}
{"title": "A generic trace schema for the portability of cp (fd) debugging tools\n", "abstract": " Debugging tools are essential to help tune constraint solving programs and each platform has its environment tools. However, at present, these tools are specific and have to be redesigned and re-implemented for each constraint solver whereas much could be factorized. This article sets the foundations to enable debugging tools to be defined almost independently from finite domain solvers, and conversely, tracers to be built independently from these tools. We propose a generic trace schema based on a generic observational semantics which formalizes relevant aspects of constraint programming and solving. We illustrate the genericity of the schema on three representative families of finite domain solvers: CLP (Gnu-Prolog), CSP (Choco) and explanation based CP (PaLM). Debugging tools can use the generic trace and do not have to take into account all the details of the solvers. We experimented\u00a0\u2026", "num_citations": "24\n", "authors": ["1827"]}
{"title": "Abstract views of Prolog executions in Opium\n", "abstract": " Opium is a system for analysing and debugging Prolog programs. Its kernel comprises an execution tracer and a programming language with a set of primitives for trace and source analysis. In this report we show the power of Opium for supporting abstract views of Prolog executions. Abstract views give high-level points of view about executions. They filter out irrelevant details; they restructure the remaining information; and they compact it so that the information given at each step has a reasonable size. The examples of abstract views given in the following are a goal execution profile, some data abstractions, an instantiation profile, a failure analysis, a loop analysis, and a kind of explanation for an expert system written in Prolog.", "num_citations": "24\n", "authors": ["1827"]}
{"title": "Tracing Prolog programs by source instrumentation is efficient enough\n", "abstract": " Tracing by automatic program source instrumentation has major advantages over compiled code instrumentation: it is more portable from one Prolog system to another, it produces traces in terms of the original program, and it can be tailored to specific debugging needs. The main argument usually put forward in favor of compiled code instrumentation is its supposed efficiency. We have compared the performances of two operational low-level Prolog tracers with source instrumentation. We have executed classical Prolog benchmark programs, collecting trace information without displaying it. On average, collecting trace information by program instrumentation is about as fast as using a low-level tracer in one case, and only twice slower in the other. This is a minor penalty to pay, compared to the advantages of the approach. To our knowledge, this is the first time that a quantitative comparison of both approaches is\u00a0\u2026", "num_citations": "19\n", "authors": ["1827"]}
{"title": "Generating Deductive Database Explanations.\n", "abstract": " Existing explanation systems for deductive databases show forests of proof trees. Although proof trees are often useful, they are only one possible interesting representation. We argue that an explanation system for deductive databases must be able to generate explanations at several levels of abstraction. One possible and well known technique to achieve this flexibility is to instrument meta-interpreters. It is, however, not often used because of its inefficiency. On the other hand, deductive databases often generate intermediate information stored in the physical database. This information can be considered as a low-level trace giving a faithful picture of what has happened at the relational level. The deductive reasoning is lost but can be very easily recovered by a meta-interpreter. In this article we describe a technique to generate explanations by integrating a relational trace and an instrumented meta-interpreter. The expensive aspects of meta-interpretation are reduced by the use of the trace which avoids many costly calculations. The flexibility of meta-interpretation is preserved, as illustrated by the generation of three different kinds of explanations: a box-oriented trace, a multi-SLD-AL tree and abstract AND trees. This technique enables powerful explanation systems to be implemented with very few modifications of the deductive database mechanism itself.", "num_citations": "16\n", "authors": ["1827"]}
{"title": "Opium+, a meta-debugger for Prolog\n", "abstract": " Opium+, a meta-debugger for prolog | Proceedings of the 8th European Conference on Artificial Intelligence ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsECAI'88Opium+, a meta-debugger for prolog ARTICLE Opium+, a meta-debugger for prolog Share on Author: Mireille Ducass\u00e9 profile image Mireille Ducass\u00e9 View Profile Authors Info & Affiliations Publication: ECAI'88: Proceedings of the 8th European Conference on Artificial IntelligenceAugust 1988 Pages 272\u2013277 0citation 0 Downloads Metrics Total Citations0 Total Downloads0 Last 12 Months0 Last 6 weeks0 Get Citation Alerts New Citation Alert added! This alert \u2026", "num_citations": "16\n", "authors": ["1827"]}
{"title": "Coca: A debugger for C based on fine grained control flow and data events\n", "abstract": " We present Coca, an automated debugger for C, where the breakpoint mechanism is based on events related to language constructs. Events have semantics whereas source lines used by most debuggers do not have any. A trace is a sequence of events. It can be seen as an ordered relation in a database. Users can specify precisely which events they want to see by specifying values for event attributes. At each event, visible variables can be queried. The trace query language is Prolog with a handful of primitives. The trace query mechanism searches through the execution traces using both control flow and data whereas debuggers usually search according to either control flow or data. As opposed to fully \u00abrelational\u00bb debuggers which use plain database querying mechanisms, Coca trace querying mechanism does not require any storage. The analysis is done on the fly, synchronously with the traced execution. Coca is therefore more powerful than \u00absource line\u00bb debuggers and more efficient than relational debuggers.", "num_citations": "15\n", "authors": ["1827"]}
{"title": "Automated analysis of CLP (FD) program execution traces\n", "abstract": " CLP(FD) programs can solve complex problems but they are difficult to develop and maintain. In particular, their operational behavior is not easy to understand. Execution tracers can give some insight of executions, but they are mapped onto the operational semantics of the language. This, in general, is a too low-level picture of the execution. In particular, application developers and end-users do not need to know all the details of the execution steps. They need abstract views of program behaviors.", "num_citations": "14\n", "authors": ["1827"]}
{"title": "A general trace query mechanism based on Prolog\n", "abstract": " We present a general trace query language which is a solution to the ever growing command sets of other tracers. It provides all the required generality while being very simple and efficient. We model a program execution into a trace which is a stream of events. Execution events have a uniform representation, and can be analysed by Prolog programs. With this approach and thanks to the expressive power of Prolog, two high-level primitives plus Prolog are enough to provide a general trace query language. With a few optimizations this language can work on large executions without any loss of performance, if compared to traditional tracers. This paper describes the trace query mechanism from its high level specification down to some implementation details. The proposed model of trace query depends only on the sequentiality of the execution, and the principles behind the design of the optimizations do\u00a0\u2026", "num_citations": "14\n", "authors": ["1827"]}
{"title": "An extendable trace analyser to support automated debugging\n", "abstract": " An extendable trace analyser to support automated debugging - OpenGrey fra | eng OpenGrey Open System for Information on Grey literature in Europe Home Search Subjects Partners Export Help Search XML To cite or link to this reference: http://hdl.handle.net/10068/45109 Title : An extendable trace analyser to support automated debugging Un analyseur de trace extensible pour l'automatisation du debogag Author : Ducasse, Mireille ; University : Rennes-1 Univ., 35 (France) ; Publication year : 1992 Language : French ; Pagination/Size : 210 p. ; SIGLE classification : 09H - Computer software, programming ; Document type : U - Thesis ; Other identifier : FR_ 2001:1871 ; FR ; handle : http://hdl.handle.net/10068/45109 Provenance : SIGLE ; Get a copy : INIST-CNRS - Institut de l'Information Scientifique et Technique Availability : Available from INIST (FR), Document Supply Service, under shelf-number : T 84567 \u2026", "num_citations": "14\n", "authors": ["1827"]}
{"title": "Opium: a debugging environment for Prolog development and debugging research\n", "abstract": " Opium is an extensible debugging environment for PROLOG providing high-level debugging facilities for programmers and debugging experts.", "num_citations": "13\n", "authors": ["1827"]}
{"title": "A generic trace model for finite domain solvers\n", "abstract": " Debugging tools are essential to help tune constraint solving programs. However, at present, these tools have to be redesigned and re-implemented for each constraint solver whereas much could be factorized. This paper sets the foundations...", "num_citations": "12\n", "authors": ["1827"]}
{"title": "Semantic faceted search: Safe and expressive navigation in RDF graphs\n", "abstract": " Faceted search and querying are the two main paradigms to search the Semantic Web. Querying languages, such as SPARQL, oer expressive means for searching knowledge bases, but they are dicult to use. Query assistants help users to write well-formed queries, but they do not prevent empty results. Faceted search supports exploratory search, i.e., guided navigation that returns rich feedbacks to users, and prevents them to make navigation steps that lead to empty results (dead-ends). However, faceted search systems do not oer the same expressiveness as query languages. We introduce semantic faceted search, the combination of an expressive query language and faceted search to reconcile the two paradigms. The query language is basically SPARQL, but with a syntax that extends Turtle with disjunction and negation, and that better ts in a faceted search interface: LISQL. We formalize the navigation of faceted search as a navigation graph, where nodes are queries, and navigation links are query transformations. We prove that this navigation graph is safe (no dead-end), and complete (every query that is not a dead-end can be reached by navigation). That formalization itself is a contribution to faceted search. A prototype, Camelis 2, has been implemented, and a usability evaluation with graduate students demonstrated that semantic faceted search retains the ease-of-use of faceted search, and enables most users to build complex queries with little training.", "num_citations": "11\n", "authors": ["1827"]}
{"title": "D\u00e9tection d'intrusions: corr\u00e9lation d'alertes\n", "abstract": " Pascal 001 Exact sciences and technology/001D Applied sciences/001D02 Computer science; control theory; systems/001D02B Software/001D02B07 Memory organisation. Data processing/001D02B07C Memory and file management (including protection and security)", "num_citations": "11\n", "authors": ["1827"]}
{"title": "Proving or disproving likely invariants with constraint reasoning\n", "abstract": " A program invariant is a property that holds for every execution of the program. Recent work suggest to infer likely-only invariants, via dynamic analysis. A likely invariant is a property that holds for some executions but is not guaranteed to hold for all executions. In this paper, we present work in progress addressing the challenging problem of automatically verifying that likely invariants are actual invariants. We propose a constraint-based reasoning approach that is able, unlike other approaches, to both prove or disprove likely invariants. In the latter case, our approach provides counter-examples. We illustrate the approach on a motivating example where automatically generated likely invariants are verified.", "num_citations": "10\n", "authors": ["1827"]}
{"title": "Combining faceted search and query languages for the semantic web\n", "abstract": " Faceted search and querying are the two main paradigms to search the Semantic Web. Querying languages, such as SPARQL, offer expressive means for searching knowledge bases, but they are difficult to use. Query assistants help users to write well-formed queries, but they do not prevent empty results. Faceted search supports exploratory search, i.e., guided navigation that returns rich feedbacks to users, and prevents them to fall in dead-ends (empty results). However, faceted search systems do not offer the same expressiveness as query languages. We introduce semantic faceted search, the combination of an expressive query language and faceted search to reconcile the two paradigms. The query language is basically SPARQL, but with a syntax that better fits in a faceted search interface. A prototype, Camelis\u00a02, has been implemented, and a usability evaluation demonstrated that semantic faceted\u00a0\u2026", "num_citations": "8\n", "authors": ["1827"]}
{"title": "Fair (er) and (almost) serene committee meetings with Logical and Formal Concept Analysis\n", "abstract": " In academia, many decisions are taken in committee, for example to hire people or to allocate resources. Genuine people often leave such meetings quite frustrated. Indeed, it is intrinsically hard to make multi-criteria decisions, selection criteria are hard to express and the global picture is too large for participants to embrace it fully. In this article, we describe a recruiting process where logical concept analysis and formal concept analysis are used to address the above problems. We do not pretend to totally eliminate the arbitrary side of the decision. We claim, however, that, thanks to concept analysis, genuine people have the possibility to 1) be fair with the candidates, 2) make a decision adapted to the circumstances, 3) smoothly express the rationales of decisions, 4) be consistent in their judgements during the whole meeting, 5) vote (or be arbitrary) only when all possibilities for consensus have been\u00a0\u2026", "num_citations": "8\n", "authors": ["1827"]}
{"title": "A tracer driver for hybrid execution analyses\n", "abstract": " Tracers provide users with useful information about program executions. In this paper we propose a\" tracer driver\", from a single tracer, it provides a powerful front-end for multiple dynamic analysis tools while limiting the overhead of the trace generation. The tracer driver can be used both synchronously and asynchronously. The relevant execution events are specified by flexible event patterns and a large variety of trace data can be given either systematically or\" on demand\". The proposed tracer driver has been designed and experimented in the context of constraint logic programming, within GNU-Prolog. Its principles are, however, independent of the traced programming language. Experimental measures show that the flexibility and power of the described architecture are also the basis of reasonable performances.", "num_citations": "8\n", "authors": ["1827"]}
{"title": "A hybrid backward slicing algorithm producing executable slices for Prolog\n", "abstract": " CiteSeerX \u2014 A Hybrid Backward Slicing Algorithm Producing Executable Slices for Prolog Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA A Hybrid Backward Slicing Algorithm Producing Executable Slices for Prolog (1995) Cached Download as a PDF Download Links [www.cs.usask.ca] [www.cs.usask.ca] Save to List Add to Collection Correct Errors Monitor Changes by St\u00e9phane Schoenig , Mireille Ducass\u00e9 Citations: 4 - 0 self Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source Contact Us Developed at and hosted by The College of Information Sciences and \u2026", "num_citations": "8\n", "authors": ["1827"]}
{"title": "Analysis of failing Prolog executions\n", "abstract": " The result of a Prolog execution can simply be\" no\", when the programmer is expecting something else. This symptom is typical of Prolog, and especially requires the help of an execution tracer to get clues of what the problem can be. We present a solution which helps programmers to understand how unexpected failures have occurred. We first propose a hierarchy of failing goals. We argue that there is one kind of leaf failures which is interesting to track at the first place. Then we give the algorithm for our leaf failure tracking and two examples illustrating its use. published in Proceedings of Journ'ees Francophones sur la Programmation Logique, Mai 92, Lille y Author's current address: IRISA/INSA, Campus universitaire de Beaulieu, F-35042 Rennes, ducasse@ irisa. fr How many Prolog programmers does it take to change a light bulb? no. 1 Introduction Program analysis, such as type checking and abstract interpretation has many uses in program development and maintenance. The dat...", "num_citations": "8\n", "authors": ["1827"]}
{"title": "A parameterized algorithm for exploring concept lattices\n", "abstract": " Formal Concept Analysis (FCA) is a natural framework for learning from positive and negative examples. Indeed, learning from examples results in sets of frequent concepts whose extent contains only these examples. In terms of association rules, the above learning strategy can be seen as searching the premises of exact rules where the consequence is fixed. In its most classical setting, FCA considers attributes as a non-ordered set. When attributes of the context are ordered, Conceptual Scaling allows the related taxonomy to be taken into account by producing a context completed with all attributes deduced from the taxonomy. The drawback, however, is that concept intents contain redundant information. In this article, we propose a parameterized generalization of a previously proposed algorithm, in order to learn rules in the presence of a taxonomy. The taxonomy is taken into account during the\u00a0\u2026", "num_citations": "7\n", "authors": ["1827"]}
{"title": "Opium\u2014An advanced debugging system\n", "abstract": " The data used by program analysis in general is often restricted to the source code of the analysed programs. However, there is a complementary source of information, namely traces of program executions. Usual tracers, which extract this trace information, do not allow for general trace analysis. Opium, our debugger for Prolog, sets up a framework where program sources and traces of program executions can be jointly analysed.As the debugging process is heuristic and not all the debugging strategies have been identified so far, Opium is programmable. In particular, its trace query language gives more flexibility and more power than the hard coded command sets of usual tracers. This trace query language is based on Prolog. Opium is therefore both a helpful tool for Prolog and a nice application of Prolog. The most innovative extensions of Opium compute abstract views of Prolog executions to help users\u00a0\u2026", "num_citations": "7\n", "authors": ["1827"]}
{"title": "Scalable query-based faceted search on top of SPARQL endpoints for guided and expressive semantic search\n", "abstract": " Because the Web of Documents is composed of structured pages that are not meaningful to machines, search in the Web of Documents is generally processed by keywords. However, because the Web of Data provides structured information, search in the Web of Data can be more precise. SPARQL is the standard query language for querying this structured information. SPARQL is expressive and its syntax is similar to SQL. However, casual user can not write SPARQL queries. Sewelis is a search system for the Web of Data offering to explore data progressively and more user-friendly than SPARQL. Sewelis guides the search with a query built incrementally because users only have to select query elements in order to complete the query. However, Sewelis does not scale to large datasets such as DBpedia, which is composed of about 2 billion triples. In this report, we introduce Scalewelis. Scalewelis is a search system for the Web of Data that is similar to Sewelis but scalable. Moreover, Scalewelis is independent to data because it connects to SPARQL endpoints. We took part in a challenge on DBpedia with Scalewelis. We were able to answer to 70 questions out of 99 with acceptable response times.", "num_citations": "6\n", "authors": ["1827"]}
{"title": "Guided semantic annotation of comic panels with sewelis\n", "abstract": " UTILIS (Updating Through Interaction in Logical Information Systems), introduced in a research paper at EKAW\u201912, is an interactive process to help users create new objects in a RDF graph. While creating a new object, relaxation rules are applied to its current description to find similar objects, whose properties serve as suggestions to expand the description. UTILIS is implemented in Sewelis, a system that reconciles the expressiveness of querying languages (e.g., SPARQL), and the benefits of exploratory search found in faceted search. The same interaction principles are used for both exploration and creation of semantic data. We illustrate the UTILIS approach by applying Sewelis to the semantic annotation of comic panels, reusing the dataset that was used for a user evaluation.", "num_citations": "6\n", "authors": ["1827"]}
{"title": "Guided creation and update of objects in rdf (s) bases\n", "abstract": " Updating existing knowledge bases is crucial to take into account the information that are regularly discovered. However, this is quite tedious and in practice Semantic Web data are rarely updated by users. This paper presents UTILIS, an approach to help users create and update objects in RDF (S) bases. While creating a new object, o, UTILIS searches for similar objects, found by applying relaxation rules to the description of o, taken as a query. The resulting objects and their properties serve as suggestions to expand the description of o.", "num_citations": "5\n", "authors": ["1827"]}
{"title": "Design and implementation of a tracer driver: Easy and efficient dynamic analyses of constraint logic programs1\n", "abstract": " Tracers provide users with useful information about program executions. In this article, we propose a \u201ctracer driver\u201d. From a single tracer, it provides a powerful front-end enabling multiple dynamic analysis tools to be easily implemented, while limiting the overhead of the trace generation. The relevant execution events are specified by flexible event patterns and a large variety of trace data can be given either systematically or \u201con demand\u201d. The proposed tracer driver has been designed in the context of constraint logic programming (CLP); experiments have been made within GNU-Prolog. Execution views provided by existing tools have been easily emulated with a negligible overhead. Experimental measures show that the flexibility and power of the described architecture lead to good performance. The tracer driver overhead is inversely proportional to the average time between two traced events. Whereas the\u00a0\u2026", "num_citations": "5\n", "authors": ["1827"]}
{"title": "Une s\u00e9mantique observationnelle du mod\u00e8le des bo\u00eetes pour la r\u00e9solution de programmes logiques\n", "abstract": " Dans cet article on \u00e9tudie une pr\u00e9sentation originale du mod\u00e8le des bo\u00eetes de Byrd bas\u00e9e sur la notion de s\u00e9mantique observationnelle. Cette approche permet de rendre compte de la s\u00e9mantique des traceurs Prolog ind\u00e9pendamment d'une implantation particuli\u00e8re. Le sch\u00e9ma explicatif obtenu est une pr\u00e9sentation formelle \u00e9pur\u00e9e d'une trace consid\u00e9r\u00e9e en g\u00e9n\u00e9ral comme plut\u00f4t obscure et difficile \u00e0 utiliser. Ilpeut constituer une approche simple et p\u00e9dagogique tant pour l'enseignement (par sa forme \u00e9pur\u00e9e) que pour les implantations de traceurs Prolog dont ilconstitue une forme de sp\u00e9cification. Ceci, en fait, n'est qu'un exemple pour illustrer une probl\u00e9matique g\u00e9n\u00e9rale relative aux traceurs et aux processus observants qui ne connaissent duprocessus observ\u00e9 que sa trace. La question est alors de pouvoir reconstituer par l'analyse de la trace l'essentiel du processus observ\u00e9, et si possible,sans perte d'information. Notre approche met en \u00e9vidence les qualit\u00e9s du mod\u00e8le des bo\u00eetes qui en ont fait son succ\u00e8s, mais aussi ses inconv\u00e9nients et ses limites.", "num_citations": "5\n", "authors": ["1827"]}
{"title": "An algorithm to find frequent concepts of a formal context with taxonomy\n", "abstract": " Formal Concept Analysis (FCA) considers attributes as a non-ordered set. This is appropriate when the data set is not structured. When an attribute taxonomy exists, existing techniques produce a completed context with all attributes deduced from the taxonomy. Usual algorithms can then be applied on the completed context for finding frequent concepts, but the results systematically contain redundant information. This article describes an algorithm which allows the frequent concepts of a formal context with taxonomy to be computed. It works on a non-completed context and uses the taxonomy information when needed. The results avoid the redundancy problem with equivalent performance.", "num_citations": "5\n", "authors": ["1827"]}
{"title": "A tracer driver for versatile dynamic analyses of constraint logic programs\n", "abstract": " Programs with constraints are hard to debug. In this paper, we describe a general architecture to help develop new debugging tools for constraint programming. The possible tools are fed by a single general-purpose tracer. A tracer-driver is used to adapt the actual content of the trace, according to the needs of the tool. This enables the tools and the tracer to communicate in a client-server scheme. Each tool describes its needs of execution data thanks to event patterns. The tracer driver scrutinizes the execution according to these event patterns and sends only the data that are relevant to the connected tools. Experimental measures show that this approach leads to good performance in the context of constraint logic programming, where a large variety of tools exists and the trace is potentially huge.", "num_citations": "5\n", "authors": ["1827"]}
{"title": "Tracing Execution of CLP (FD) Programs: A Trace Model and an Experimental Validation Environment\n", "abstract": " Developing and maintaining Constraint Logic Programs (CLP) requires performanc- e debugging tools based on visualization and explanation. However, existing tools are built in an ad hoc way and porting them from one platform to another is very difficult and experimentation of new tools remains limited. It has been shown in previous work that, from a fine-grained execution trace, a number of interesting views about logic program executions could be generated by trace analysis. In this report, we propose a generic trace model for constraint resolution by narrowing and a methodology to study and improve it. The trace model is the first one proposed for and does not pretend to be the ultimate one. The methodology is based on the following steps: definition of a formal model of trace, extraction of relevant informations by a trace analyzer, utilization of the extracted informations in several debugging tools. We present the trace model and an implementation which includes a tracer, based on a meta-interpreter written in ISO-Prolog, and an opium-like analyzer. The efficiency of the tracer is tested and some elementary debugging tools based on trace analysis are experimented. This work sets the basis for generic analysis of behavior of programs. is a short version of this report.", "num_citations": "5\n", "authors": ["1827"]}
{"title": "Proof obligations of the b formal method: Local proofs ensure global consistency\n", "abstract": " The B formal method has been successfully used in large projects and is not reserved to experts. The main correctness criterion of B is that every piece of code must preserve invariant properties. In this article, we briefly introduce the basic notions of B. We then concentrate on the proof obligations. After introducing them, we show how the sum of local proofs makes a global consistency. We believe that this strong modularity is essential for the tractability of the proofs.", "num_citations": "5\n", "authors": ["1827"]}
{"title": "Nested Forms with Dynamic Suggestions for Quality RDF Authoring\n", "abstract": " Knowledge acquisition is a central issue of the Semantic Web. Knowledge cannot always be automatically extracted from existing data, thus domain experts are required to manually produce it. On the one hand, learning formal languages such as RDF represents an important obstacle to non-IT experts. On the other hand, well-known data input interfaces do not address well the relational nature and flexibility of RDF. Furthermore, it is difficult to maintain data quality through time, and across contributors. We propose FORMULIS, a form-based interface for guided RDF authoring. It hides RDF notations, addresses the relational aspects with nested forms, and guides users by computing intelligent filling suggestions. Two user experiments show that FORMULIS helps users maintain good data quality, and can be used by users without Semantic Web knowledge.", "num_citations": "3\n", "authors": ["1827"]}
{"title": "Building up shared knowledge with logical information systems\n", "abstract": " Logical Information Systems (LIS) are based on Logical Concept Analysis, an extension of Formal Concept Analysis. This paper describes an application of LIS to support group decision. A case study gathered a research team. The objective was to decide on a set of potential conferences on which to send submissions. People individually used Abilis, a LIS web server, to preselect a set of conferences. Starting from 1041 call for papers, the individual participants preselected 63 conferences. They met and collectively used Abilis to select a shared set of 42 target conferences. The team could then sketch a publication planning. The case study provides evidence that LIS cover at least three of the collaboration patterns identified by Kolfschoten, de Vreede and Briggs. Abilis helped the team to build a more complete and relevant set of information (Generate/Gathering pattern); to build a shared understanding of the relevant information (Clarify/Building Shared Understanding); and to quickly reduce the number of target conferences (Reduce/Filtering pattern).", "num_citations": "3\n", "authors": ["1827"]}
{"title": "Aide \u00e0 la d\u00e9cision multicrit\u00e8re: coh\u00e9rence et \u00e9quit\u00e9 gr\u00e2ce \u00e0 l\u2019analyse de concepts\n", "abstract": " De nombreuses d\u00e9cisions sont prises en commission, par exemple pour affecter des ressources. Les crit\u00e8res de d\u00e9cision sont difficiles \u00e0 exprimer et la situation globale est en g\u00e9n\u00e9ral trop complexe pour que les participants puissent l\u2019appr\u00e9hender pleinement. Dans cet article, nous d\u00e9crivons un processus de d\u00e9cision pour la s\u00e9lection de candidats \u00e0 un emploi. L\u2019analyse de concepts y est utilis\u00e9e pour faire face aux probl\u00e8mes mentionn\u00e9s ci-dessus. Gr\u00e2ce \u00e0 l\u2019analyse formelle de concepts et aux syst\u00e8mes d\u2019information logiques, les personnes fair play ont la possibilit\u00e9 d\u2019\u00eatre \u00e9quitables envers les candidats et de faire preuve de coh\u00e9rence dans leurs jugements sur toute la dur\u00e9e du processus de d\u00e9cision.", "num_citations": "3\n", "authors": ["1827"]}
{"title": "Rigorous design of tracers: An experiment for constraint logic programming\n", "abstract": " In order to design and implement tracers, one must decide what exactly to trace and how to produce this trace. On the one hand, trace designs are too often guided by implementation concerns and are not as useful as they should be. On the other hand, an interesting trace which cannot be produced efficiently, is not very useful either. In this article we propose a methodology which helps to efficiently produce accurate traces. Firstly, design a formal specification of the trace model. Secondly, derive a prototype tracer from this specification. Thirdly, analyze the produced traces. Fourthly, implement an efficient tracer. Lastly, compare the traces of the two tracers. At each step, problems can be found. In that case one has to iterate the process. We have successfully applied the proposed methodology to the design and implementation of a real tracer for constraint logic programming which is able to efficiently generate information required to build interesting graphical views of executions.", "num_citations": "3\n", "authors": ["1827"]}
{"title": "Prototypage de traceurs CLP (FD)\n", "abstract": " Developing and maintaining CLP programs requires visualization and explanation tools. However, existing tools are built in an ad hoc way. Therefore porting tools from one platform to another is very difficult. We have shown in previous work that, from a fine-grained execution trace, a number of interesting views about logic program executions could be generated by trace analysis. In this article, we propose a trace model for constraint solving by narrowing. This trace model is the first one proposed for clp (fd) and does not pretend to be the ultimate one. We propose an instrumented meta-interpreter in order to experiment with the model. Furthermore, we show that the proposed trace model contains the necessary information to build known and useful execution views. This work sets the basis for generic execution analysis of clp (fd) programs.", "num_citations": "3\n", "authors": ["1827"]}
{"title": "Specifying Byrd's box model with a continuation semantics\n", "abstract": " We give a formal specification of Byrd's box model. This specification is based on a on a Prolog operational semantics with continuations. We also show how this specification can be executed by a direct translation into \u03bbProlog, leading to a Prolog interpreter that produces execution traces. This interpreter can be used both to experiment various trace models to validate them. We have hence a formal framework to specify and prototype trace models.", "num_citations": "3\n", "authors": ["1827"]}
{"title": "Benchmarking a distributed intrusion detection system based on ASAX: Preliminary results\n", "abstract": " The objective of the experimentation described in this abstract is to assess the feasibility of a sophisticated distributed intrusion detection system (DIDS). Indeed, informal discussions with system engineers show that they are reluctant to set up a DIDS. As a matter of fact, designing and maintaining a DIDS on a real life scale is a non trivial exercise. Most people fear that it would not be worth the effort, because the DIDS would make the performances of the audited network collapse. We believe that it is essential to have rational data about the costs of a DIDS.", "num_citations": "3\n", "authors": ["1827"]}
{"title": "Un analyseur de trace extensible pour l'automatisation du d\u00e9bogage\n", "abstract": " Le memoire decrit les aspects novateurs d'opium, un environnement de debogage pour prolog, construit au-dessus d'un traceur existant, et l'etendant a un analyseur de trace general. Premierement, opium offre un puissant langage de requetes portant sur des traces d'execution. Avec seulement deux primitives et prolog les utilisateurs peuvent specifier des requetes plus precises qu'avant les commandes des autres traceurs. Deuxiemement, opium est programmable et extensible. C'est, de ce fait, un environnement dans lequel des strategies de debogage peuvent etre facilement programmees et integrees. Quelques strategies ont deja ete implantees. Troisiemement, des vues abstraites d'executions sont proposees comme base d'un debogueur automatise. Ces vues aident les utilisateurs a comprendre le comportement des programmes en parcourant les executions a un niveau d'abstraction plus eleve qu'avec des traceurs pas-a-pas. Un prototype robuste d'opium a ete implante. Plus de 20 sites universitaires ont recemment installe ce prototype, et certains de ces sites implantent de nouvelles extensions", "num_citations": "3\n", "authors": ["1827"]}
{"title": "A High-level Debugging Environment for Prolog-OPIUM 3.1-User Manual\n", "abstract": " Opium is an extensible debugging environment for Prolog which offers support for high-level debugging strategies. A broad scope of Prolog programmers ranging from beginners to experienced Prolog users should find accurate debugging support in Opium.", "num_citations": "3\n", "authors": ["1827"]}
{"title": "FORMULIS: dynamic form-based interface for guided knowledge graph authoring\n", "abstract": " Knowledge acquisition is a central issue of the Semantic Web. Knowledge cannot always be automatically extracted from existing data, thus contributors have to make efforts to create new data. In this paper, we propose FORMULIS, a dynamic form-based interface designed to make RDF data authoring easier. FORMULIS guides contributors through the creation of RDF data by suggesting fields and values according to the previously filled fields and the previously created resources.", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Safe suggestions based on type convertibility to guide workflow composition\n", "abstract": " This paper proposes an interactive approach that guides users in the step-by-step composition of services by providing safe suggestions based on type convertibility. Users specify the points of the workflow (called the focus) they want to complete, and our approach suggests services and connections whose data types are compatible with the focus. We prove the safeness (every step produces a well-formed workflow) and the completeness (every well-formed workflow can be built) of our approach.", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Generating data converters to help compose services in bioinformatics workflows\n", "abstract": " Heterogeneity of data and data formats in bioinformatics often entail a mismatch between inputs and outputs of different services, making it difficult to compose them into workflows. To reduce those mismatches bioinformatics platforms propose ad\u2019hoc converters written by hand. This article proposes to systematically detect convertibility from output types to input types. Convertibility detection relies on abstract types, close to XML Schema, allowing to abstract data while precisely accounting for its composite structure. Detection is accompanied by an automatic generation of converters between input and output XML data. Our experiment on bioinformatics services and datatypes, performed with an implementation of our approach, shows that the detected convertibilities and produced converters are relevant from a biological point of view. Furthermore they automatically produce a graph of potentially compatible\u00a0\u2026", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Helping facilitators build on experience when preparing meetings with logical information systems\n", "abstract": " This paper reports work in progress about using Logical Information Systems to help facilitators build on experience when preparing meetings. Features of meetings similar to the one under construction are automatically suggested without having to ask for suggestions. Suggestions take into account the whole information about all the meetings already recorded in the system as well as facilitation knowledge, such as thinkLets. Usual techniques and processes that facilitators like to use are naturally suggested. An unusual technique is suggested for example if the facilitator enters a keyword that is a feature of that technique. Although a lot remains to be done, the proposed approach already shows contributions that make believe that it is worth investigating further. The main one is that it builds on the facilitator very practice. Other important features are flexibility and adaptability.", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Observational semantics of the Prolog Resolution Box Model\n", "abstract": " This paper specifies an observational semantics and gives an original presentation of the Byrd box model. The approach accounts for the semantics of Prolog tracers independently of a particular Prolog implementation. Prolog traces are, in general, considered as rather obscure and difficult to use. The proposed formal presentation of its trace constitutes a simple and pedagogical approach for teaching Prolog or for implementing Prolog tracers. It is a form of declarative specification for the tracers. The trace model introduced here is only one example to illustrate general problems relating to tracers and observing processes. Observing processes know, from observed processes, only their traces. The issue is then to be able to reconstitute, by the sole analysis of the trace, part of the behaviour of the observed process, and if possible, without any loss of information. As a matter of fact, our approach highlights qualities of the Prolog resolution box model which made its success, but also its insufficiencies.", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Visualization of internet flow records\n", "abstract": " Inria - Visualization of Internet Flow Records Acc\u00e9der directement au contenu Acc\u00e9der directement \u00e0 la navigation Toggle navigation CCSD HAL HAL HALSHS TEL M\u00e9diHAL Liste des portails AUR\u00e9HAL API Data Documentation Episciences.org Episciences.org Revues Documentation Sciencesconf.org Support HAL-Inria Les publications, logiciels... des scientifiques Inria Accueil D\u00e9poser Consulter tout HAL par date de publication/r\u00e9daction par domaine par type de publication par collection arXiv les derniers d\u00e9p\u00f4ts Publications Inria Recherche Services HalTools : cr\u00e9er sa page web Haltools : export RAWEB X2Hal : import par lot Consulter les structures de recherche connues de HAL Documentation Aide en ligne de HAL V3 Derni\u00e8res \u00e9volutions de HAL V3 Documentation API HAL Ajouter des vignettes Aide en ligne Haltools Aide en ligne de X2hal OpenAccess Inria soutient la science ouverte inria-00463801, \u2026", "num_citations": "2\n", "authors": ["1827"]}
{"title": "A tracer driver to enable concurrent dynamic analyses\n", "abstract": " Tracers provide users with useful information about program executions. In this report, we propose a ``tracer driver'', from a single tracer, it provides a powerful front-end for multiple dynamic analysis tools, while limiting the overhead of the trace generation. The tracer driver can be used both synchronously and asynchronously. The relevant execution events are specified by flexible event patterns and a large variety of trace data can be given either systematically or ``on demand''. The proposed tracer driver has been designed and experimented in the context of constraint logic programming, within GNU-Prolog. Its principles are, however, independent of the traced programming language. Experimental measures show that the flexibility and power of the described architecture are also the basis of reasonable performances.", "num_citations": "2\n", "authors": ["1827"]}
{"title": "A tracer driver to enable debugging, monitoring and visualization of CLP executions from a single tracer\n", "abstract": " Tracers provide users with useful information about program executions. A tracer can be used for many purposes. Embedded in an interactive console, it enables users to investigate program executions. It can also be the front-end of visualization tools, or it can be used by an automatic process to monitor execution behaviors. For each purpose, the requested information is similar but slightly different. It is therefore important to be able to interact with the tracer in order to tune the sent information.               We propose a so-called \u201ctracer driver\u201d. From a single tracer, it provides a powerful front-end for multiple dynamic analysis tools while limiting the overhead of the trace generation. The tracer driver can be used both synchronously and asynchronously. The relevant execution information is specified by flexible event patterns. A large variety of trace data can be given either systematically or \u201con demand\u201d. The\u00a0\u2026", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Analyse automatis\u00e9e de traces d'ex\u00e9cution de programmes CLP (FD)\n", "abstract": " Sauf mention contraire ci-dessus, le contenu de cette notice bibliographique peut \u00eatre utilis\u00e9 dans le cadre d\u2019une licence CC BY 4.0 Inist-CNRS/Unless otherwise stated above, the content of this bibliographic record may be used under a CC BY 4.0 licence by Inist-CNRS/A menos que se haya se\u00f1alado antes, el contenido de este registro bibliogr\u00e1fico puede ser utilizado al amparo de una licencia CC BY 4.0 Inist-CNRS", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Handling generic intrusion signatures is not trivial\n", "abstract": " This article presents work in progress in the context of misuse scenario detection, where the scenarios are combinations of several actions. An example of a masquerading scenario is:# users manage to usurp the identity of someone, say x, then they copy the# le# bin# sh and change its access rights such that later executions of the copy will give the privileges of x", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Automated Debugging Extensions of the Opium Trace Analyser\n", "abstract": " . Traces of program executions tell how programs behave in given cases. They are a helpful source of information for automated debugging. Opium is an automated trace analyser for Prolog programs. It is programmable and extendable. It provides a trace query language and abstract views of executions as a basis for automated debugging. Opium has shown its capabilities to build abstract tracers and automated debugging facilities. This paper lists the extensions written so far, and describes two recent extensions: the abstract tracers for the LO (Linear Objects) language and for the CHR (Constraint Handling Rules) language. 1 Introduction Debugging is a costly process, and automating it would significantly reduce the costs of software production and maintenance. However, it is unrealistic to aim at fully automating the task. In particular, programmers have to understand rapidly changing situations, examining large amounts of data. In the current state of the art taking the place of program...", "num_citations": "2\n", "authors": ["1827"]}
{"title": "Using Bids, Arguments and Preferences in Sensitive Multi-unit Assignments: A p-Equitable Process and a Course Allocation Case Study\n", "abstract": " Bonus distribution in enterprises or course allocation at universities are examples of sensitive multi-unit assignment problems, where a set of resources is to be allocated among a set of agents having multi-unit demands. Automatic processes exist, based on quantitative information, for example bids or preference ranking, or even on lotteries. In sensitive cases, however, decisions are taken by persons also using qualitative information. At present, no multi-unit assignment system supports both quantitative and qualitative information. In this paper, we propose MUAP-LIS, an interactive process for multi-assignment problems where, in addition to bids and preferences, agents can give arguments to motivate their choices. Bids are used to automatically make pre-assignments, qualitative arguments and preferences help decision makers break ties in a founded way. A group decision support system, based on\u00a0\u2026", "num_citations": "1\n", "authors": ["1827"]}
{"title": "Solving Data Mismatches in Bioinformatics Workflows by Generating Data Converters\n", "abstract": " Heterogeneity of data and data formats in bioinformatics entail mismatches between inputs and outputs of different services, making it difficult to compose them into workflows. To reduce those mismatches, bioinformatics platforms propose ad\u2019hoc converters, called shims. When shims are written by hand, they are time-consuming to develop, and cannot anticipate all needs. When shims are automatically generated, they miss transformations, for example data composition from multiple parts, or parallel conversion of list elements.               This article proposes to systematically detect convertibility from output types to input types. Convertibility detection relies on a rule system based on abstract types, close to XML Schema. Types allow to abstract data while precisely accounting for their composite structure. Detection is accompanied by an automatic generation of converters between input and output XML data\u00a0\u2026", "num_citations": "1\n", "authors": ["1827"]}
{"title": "Using biddings and motivations in multi-unit assignments\n", "abstract": " In this paper, we propose a process for small to medium scale multi-assignment problems. In addition to biddings, agents can give motivations to explain their choices in order to help decision makers break ties in a founded way. A group decision support system, based on Logical Information Systems, allows decision makers to easily face both biddings and motivations. Furthermore, it guaranties that all the agents are treated equally. A successful case study about a small course assignment problem at a technical university is reported.", "num_citations": "1\n", "authors": ["1827"]}
{"title": "The LogicalMulticriteriaSort ThinkLet: Logical Navigation for Fair and Fast Convergence in Multicriteria Group Decision Making\n", "abstract": " Information overload is a key issue in group decision. A heuristics, called ''take-the-best'', has been shown useful to face multicriteria decisions while reducing information overload: when making decisions people often take criteria in a predefined order, the first criterion which discriminates the alternatives at stake is used to make the decision. In order to rationalize group work, Briggs and de Vreede have proposed collaboration design patterns, called thinkLets. This article presents the LogicalMulticriteriaSort which can be seen as a generalization of the take-the-best heuristics. It also proposes to consider criteria one at the time but once a criterion has been found discriminating it is kept in a record, and the process is iterated. The thinkLet is supported by a GDSS, based on Logical Information Systems, which gives an instantaneous feedback of each micro decision and keeps tracks of all of the decisions taken so far. The LogicalMulticriteriaSort ThinkLet guarantees more fairness and speed than the ChauffeurSort thinkLet. It also avoids the need to give artificial values and weights to the criteria as opposed to the Multicriteria thinkLet. A successful test case is reported.", "num_citations": "1\n", "authors": ["1827"]}
{"title": "Cr\u00e9ation et mise \u00e0 jour guid\u00e9es d'objets dans une base RDF (S)\n", "abstract": " La mise \u00e0 jour des bases de connaissances existantes est cruciale pour tenir compte des nouvelles informations, r\u00e9guli\u00e8rement d\u00e9couvertes. Toutefois, en pratique, les donn\u00e9es actuelles du Web S\u00e9mantique sont rarement mises \u00e0 jour par les utilisateurs. Nous proposons UTILIS, une m\u00e9thode pour aider les utilisateurs \u00e0 ajouter de nouveaux objets. Un objet est une ressource de la base. Sa description correspond aux propri\u00e9t\u00e9s qu'il poss\u00e8de. Pendant la cr\u00e9ation d'un nouvel objet o, UTILIS recherche les objets similaires. Les propri\u00e9t\u00e9s des objets similaires sont utilis\u00e9es comme suggestions pour compl\u00e8ter la description de o. Les objets similaires sont trouv\u00e9s en appliquant des r\u00e8gles de relaxation \u00e0 la description de o, prise comme une requ\u00eate. Compar\u00e9 avec l'\u00e9tat de l'art, la contribution est qu'UTILIS est \u00e0 la fois incr\u00e9mental, chaque nouvelle propri\u00e9t\u00e9 est utilis\u00e9e pour la recherche, et interactif, l'utilisateur a un r\u00f4le actif dans le processus.", "num_citations": "1\n", "authors": ["1827"]}
{"title": "Formal concept analysis\n", "abstract": " Formal concept analysis (FCA) is a mathematical formalism based on order and lattice theory for data analysis. It has found applications in a broad range of neighboring fields including Semantic Web, data mining, knowledge representation, data visualization, and software engineering. ICFCA is a series of annual international conferences that started in 2003 in Darmstadt and has been held in several continents: Europe, Australia, America, and Africa. ICFCA has evolved to be the main forum for researchers working on theoretical or applied aspects of formal concept analysis worldwide. This volume contains the papers presented at the 11th conference within the series. The conference returned to Dresden in 2013 where it was previously held in 2006. Out of 46 submitted papers, 15 were accepted for publication in this volume, amounting to an acceptance rate of 33%. Less mature works, which were still\u00a0\u2026", "num_citations": "1\n", "authors": ["1827"]}
{"title": "Agr\u00e9gation d\u2019alarmes faiblement structur\u00e9es\n", "abstract": " La contribution principale de ce document1 est une approche pla\u00e7ant l\u2019op\u00e9rateur au coeur de l\u2019analyse de journaux d\u2019alarmes faiblement structur\u00e9es en lui permettant d\u2019utiliser ce qu\u2019il sait, m\u00eame si ses connaissances sont partielles, et sans le submerger d\u2019informations. Des motifs temporels structur\u00e9s sont extraits par agr\u00e9gation d\u2019alarmes g\u00e9n\u00e9ralis\u00e9es et corr\u00e9lation se basant sur la date des alarmes et sur la similarit\u00e9 d\u2019attributs autres que la date. L\u2019approche est appliqu\u00e9e aux alarmes produites par un concentrateur VPN (Virtual Private Network). Une \u00e9tude de cas montre comment 5000 d\u2019alarmes peuvent \u00eatre regroup\u00e9es en 50 motifs.", "num_citations": "1\n", "authors": ["1827"]}
{"title": "Building efficient tools to query execution traces\n", "abstract": " Understanding how a program execution proceeds often helps debug the program. An execution can be seen as a succession of computation steps. Tracers give information about these steps and what occurs at each of them. The traceable steps are traditionally called breakpoints. An execution frequently produces several millions of breakpoint occurrences. Programmers can therefore not analyze by hand all the computation steps. Existing tracers often have conditional breakpoints to inspect only steps which satisfy certain conditions. Unfortunately, the conditions that can be specified do not meet all the needs. To palliate this problem, an execution trace can be seen as a relational database. Each breakpoint information is represented by a tuple and queries simply use the language of the database management system. The problem is then that the time to create the database is much too long. We have, therefore, proposed an improved framework for trace querying. The interrogation is processed in two distinct steps: firstly, the trace is filtered on the fly with respect to the basic conditions of the queries; secondly, the remaining part of the query is then processed. Our approach has two main advantages. Firstly, filtering is very efficient. Done on the fly, it creates very few new data structures, the trace does not even have to be stored at all. In this report, we describe how to implement trace query tools based on the above framework. We propose an architecture where a tracer driver, containing a very efficient trace filtering algorithm, is integrated in the tracer process. The implementation guidelines are based on our experience building four prototypes\u00a0\u2026", "num_citations": "1\n", "authors": ["1827"]}
{"title": "Revisiting the\" traffic Lights\" B Case Study\n", "abstract": " In the class room, the two important B notions of re nement and implementation are best illustrated with a whole B system, with machine importation. However, whole systems are often quite di cult to grasp and students are lost in details. The tra c light case study presents a whole application easy to understand. The presented version revises in depth the case study of Chauvet which was hard-coded for the French tra c lights. It could not be easily generalized to model the German or the English tra c lights. As a consequence, it did not give su cient credit to the re nement process. In this report, intrinsic properties of crossroads and tra c lights are speci ed separately. Data re nement and reduction of non determinism are used stepwise. Thanks to this methodology, only the last re nement of the tra c light abstract machine needs a single minor adaptation to address either the German, the French, or the English lights. In 6 supervised hours students can grasp the main features of re nement invariants, data re nement, reduction of non-determinism and importation.", "num_citations": "1\n", "authors": ["1827"]}
{"title": "DDB trees: a basis for deductive database explanations\n", "abstract": " The power of deductive systems in general is that programs express what should be done and not how it should be done. Nevertheless; deductive systems need debugging and explanation facilities. Indeed; their operational semantics is less abstract than the declarative semantics of the programs. If users have to understand all the low level details of the operational semantics much of the benefits of using a deductive system is lost.", "num_citations": "1\n", "authors": ["1827"]}