{"title": "Analysis and testing of web applications\n", "abstract": " The economic relevance of Web applications increases the importance of controlling and improving their quality. Moreover, the newly available technologies for their development allow the insertion of sophisticated functions, but often leave the developers responsible for their organization and evolution. As a consequence, a high demand is emerging for methodologies and tools for the quality assurance of Web-based systems. In this paper, a UML model of Web applications is proposed for their high-level representation. Such a model is the starting point for several analyses, which can help in the assessment of the static site structure. Moreover, it drives Web application testing, in that it can be exploited to define white-box testing criteria and to semi-automatically generate the associated test cases. The proposed techniques were applied to several real-world Web applications. The results suggest that automatic\u00a0\u2026", "num_citations": "742\n", "authors": ["311"]}
{"title": "Evolutionary testing of classes\n", "abstract": " Object oriented programming promotes reuse of classes in multiple contexts. Thus, a class is designed and implemented with several usage scenarios in mind, some of which possibly open and generic. Correspondingly, the unit testing of classes cannot make too strict assumptions on the actual method invocation sequences, since these vary from application to application.In this paper, a genetic algorithm is exploited to automatically produce test cases for the unit testing of classes in a generic usage scenario. Test cases are described by chromosomes, which include information on which objects to create, which methods to invoke and which values to use as inputs. The proposed algorithm mutates them with the aim of maximizing a given coverage measure. The implementation of the algorithm and its application to classes from the Java standard library are described.", "num_citations": "531\n", "authors": ["311"]}
{"title": "State-based testing of Ajax web applications\n", "abstract": " Ajax supports the development of rich-client Web applications, by providing primitives for the execution of asynchronous requests and for the dynamic update of the page structure and content. Often, Ajax Web applications consist of a single page whose elements are updated in response to callbacks activated asynchronously by the user or by a server message. These features give rise to new kinds of faults that are hardly revealed by existing Web testing approaches. In this paper, we propose a novel state-based testing approach, specifically designed to exercise Ajax Web applications. The document object model (DOM) of the page manipulated by the Ajax code is abstracted into a state model. Callback executions triggered by asynchronous messages received from the Web server are associated with state transitions. Test cases are derived from the state model based on the notion of semantically interacting\u00a0\u2026", "num_citations": "280\n", "authors": ["311"]}
{"title": "Aspect mining through the formal concept analysis of execution traces\n", "abstract": " The presence of crosscutting concerns, i.e., functionalities that are not assigned to a single modular unit in the implementation, is one of the major problems in software understanding and evolution. In fact, they are hard to locate (scattering) and may give rise to multiple ripple effects (tangling). Aspect oriented programming offers mechanisms to factor them out into a modular unit, called an aspect. Aspect identification in existing code is supported by means of dynamic code analysis. Execution traces are generated for the use cases that exercise the main functionalities of the given application. The relationship between execution traces and executed computational units (class methods) is subjected to concept analysis. In the resulting lattice, potential aspects are detected by determining the use-case specific concepts and examining their specific computational units. When these come from multiple modules (classes\u00a0\u2026", "num_citations": "260\n", "authors": ["311"]}
{"title": "EEG data compression techniques\n", "abstract": " Electroencephalograph (EEG) and Holter EEG data compression techniques which allow perfect reconstruction of the recorded waveform from the compressed one are presented and discussed. Data compression permits one to achieve significant reduction in the space required to store signals and in transmission time. The Huffman coding technique in conjunction with derivative computation reaches high compression ratios (on average 49% on Holter and 58% on EEG signals) with low computational complexity. By exploiting this result a simple and fast encoder/decoder scheme capable of real-time performance on a PC was implemented. This simple technique is compared with other predictive transformations, vector quantization, discrete cosine transform (DCT), and repetition count compression methods. Finally, it is shown that the adoption of a collapsed Huffman tree for the encoding/decoding operations\u00a0\u2026", "num_citations": "230\n", "authors": ["311"]}
{"title": "Using a concept lattice of decomposition slices for program understanding and impact analysis\n", "abstract": " The decomposition slice graph and concept lattice are two program representations used to abstract the details of code into a higher-level view of the program. The decomposition slice graph partitions the program into computations performed on different variables and shows the dependence relation between computations, holding when a computation needs another computation as a building block. The concept lattice groups program entities which share common attributes and organizes such groupings into a hierarchy of concepts, which are related through generalizations/specializations. This paper investigates the relationship existing between these two program representations. The main result of this paper is a novel program representation, called concept lattice of decomposition slices, which is shown to be an extension of the decomposition slice graph, and is obtained by means of concept analysis, with\u00a0\u2026", "num_citations": "216\n", "authors": ["311"]}
{"title": "Reverse engineering of object oriented code\n", "abstract": " During software evolution, programmers devote most of their effort to the understanding of the structure and behavior of the system. For object oriented code, this might be particularly hard, when multiple, scattered objects contribute to the same function. Design views offer an invaluable help, but they are often not aligned with the code, when they are not missing at all. This tutorial describes some of the most advanced techniques that can be employed to reverse engineer several design views from the source code. The recovered diagrams, represented in UML (Unified Modeling Language), include class, object, interaction (collaboration and sequence), state and package diagrams. A unifying static code analysis framework used by most of the involved algorithms is presented at the beginning of the tutorial. A single running example is referred all over the presentation. Trade-offs (e.g., static vs. dynamic analysis\u00a0\u2026", "num_citations": "194\n", "authors": ["311"]}
{"title": "Concept analysis for module restructuring\n", "abstract": " Low coupling between modules and high cohesion inside each module are the key features of good software design. This paper proposes a new approach to using concept analysis for module restructuring, based on the computation of extended concept subpartitions. Alternative modularizations, characterized by high cohesion around the internal structures that are being manipulated, can be determined by such a method. To assess the quality of the restructured modules, the trade-off between encapsulation violations and decomposition is considered, and proper measures for both factors are defined. Furthermore, the cost of restructuring is evaluated through a measure of distance between the original and the new modularizations. Concept subpartitions were determined for a test suite of 20 programs of variable size: 10 public-domain and 10 industrial applications. The trade-off between encapsulation and\u00a0\u2026", "num_citations": "189\n", "authors": ["311"]}
{"title": "Measuring the effects of software aspectization\n", "abstract": " The aim of Aspect Oriented Programming (AOP) is the production of code that is easier to understand and evolve, thanks to the separation of the crosscutting concerns from the principal decomposition. However, AOP languages introduce an implicit coupling between the aspects and the modules in the principal decomposition, in that the latter may be unaware of the presence of aspects that intercept their execution and/or modify their structure. These invisible connections represent the main drawback of AOP. A measuring method is proposed to investigate the trade-off between advantages and disadvantages obtained by using the AOP approach. The method that we are currently studying is based on a metrics suite that extends the metrics traditionally used with the OO paradigm.", "num_citations": "186\n", "authors": ["311"]}
{"title": "Restructuring Program Identifier Names.\n", "abstract": " The identifiers chosen by programmers as entity names contain valuable information. They are often the starting point for the program understanding activities, especially when high level views, like the call graph, are available. An approach for the restructuring of program identifier names is proposed, aimed at improving their meaningfulness. It considers two forms of standardization, associated respectively to the lexicon of the composing terms and to the syntax of their arrangement. Automatic and semiautomatic techniques are described which can help the restructuring intervention. Their application to a real world case study is also presented.", "num_citations": "174\n", "authors": ["311"]}
{"title": "Nomen est omen: Analyzing the language of function identifiers\n", "abstract": " The identifiers chosen by programmers as function names contain valuable information. They are often the starting point for program understanding activities, especially when high-level views, like the call graph, are available. In this paper, the lexical, syntactic and semantic structure of function identifiers is analyzed by means of a segmentation technique, a regular language and a conceptual classification. The application of these analyses to a database of procedural programs suggests some potential uses of the results, ranging from support for program understanding to the evolution towards standard and more maintainable forms of programs.", "num_citations": "172\n", "authors": ["311"]}
{"title": "Object oriented design pattern inference\n", "abstract": " When designing a new application, experienced software engineers usually try to employ solutions that proved successful in previous projects. Such reuse of code organizations is seldom made explicit. Nevertheless it represents important information about the system, that can be extremely valuable in the maintenance phase by documenting the design choices underlying the implementation. In addition, having it available, it can be reused whenever a similar problem is encountered. In this paper an approach is proposed to the inference of recurrent design patterns directly from the code or the design. No assumption is made on the availability of any pattern library and the concept analysis algorithm, adapted for this purpose, is able to infer the presence of class groups which instantiate a common, repeated pattern. In fact, concept analysis provides sets of objects sharing attributes, which, in the case of object\u00a0\u2026", "num_citations": "157\n", "authors": ["311"]}
{"title": "Empirical studies in reverse engineering: state of the art and future trends\n", "abstract": " Starting with the aim of modernizing legacy systems, often written in old programming languages, reverse engineering has extended its applicability to virtually every kind of software system. Moreover, the methods originally designed to recover a diagrammatic, high-level view of the target system have been extended to address several other problems faced by programmers when they need to understand and modify existing software. The authors\u2019 position is that the next stage of development for this discipline will necessarily be based on empirical evaluation of methods. In fact, this evaluation is required to gain knowledge about the actual effects of applying a given approach, as well as to convince the end users of the positive cost\u2013benefit trade offs. The contribution of this paper to the state of the art is a roadmap for the future research in the field, which includes: clarifying the scope of investigation, defining\u00a0\u2026", "num_citations": "153\n", "authors": ["311"]}
{"title": "Understanding and restructuring Web sites with ReWeb\n", "abstract": " The authors investigated automatic and semiautomatic Web site analysis with our tool ReWeb, focusing on a site's architecture and evolution. A case study demonstrates how ReWeb addresses the need to support Web site maintenance and evolution while retaining and possibly improving quality. Although not definitive, the case study described confirms that an automatic or semiautomatic tool can help developers understand and maintain Web sites. High-level views, describing the overall site architecture, are very useful, and detailed analyses can help with a site's enclosed subparts. Specifically, we based restructuring on the reaching frames, dominators, and shortest path analyses. In fact, they highlight structural and navigation problems before restructuring and their absence after the intervention. We devote future work to improving the ReWeb's robustness, widening the spectrum of analyzable sites, and\u00a0\u2026", "num_citations": "153\n", "authors": ["311"]}
{"title": "Web Site Analysis: Structure and Evolution.\n", "abstract": " Web sites are becoming important assets for several companies, which need to incorporate sophisticated technologies into complex and large Web based systems. As a consequence, methodologies and tools are required for their design, implementation and maintenance. In particular the possibility for a site to evolve so as to provide updated and accessible information is a fundamental need. Web sites are considered the object of several analyses, focused on their structure and their history, with the purpose of supporting maintenance activities. Structural information may help understanding the organization of the pages in the site, while history analysis provides indications on modifications that do not correspond to the original design or that produce undesirable effects. A tool was developed to implement the analysis of Web site structure and evolution. Its application to some examples downloaded from the Web\u00a0\u2026", "num_citations": "152\n", "authors": ["311"]}
{"title": "Capture-replay vs. programmable web testing: An empirical assessment during test case evolution\n", "abstract": " There are several approaches for automated functional web testing and the choice among them depends on a number of factors, including the tools used for web testing and the costs associated with their adoption. In this paper, we present an empirical cost/benefit analysis of two different categories of automated functional web testing approaches: (1) capture-replay web testing (in particular, using Selenium IDE); and, (2) programmable web testing (using Selenium WebDriver). On a set of six web applications, we evaluated the costs of applying these testing approaches both when developing the initial test suites from scratch and when the test suites are maintained, upon the release of a new software version. Results indicate that, on the one hand, the development of the test suites is more expensive in terms of time required (between 32% and 112%) when the programmable web testing approach is adopted, but\u00a0\u2026", "num_citations": "124\n", "authors": ["311"]}
{"title": "Natural language parsing of program element names for concept extraction\n", "abstract": " To support programmers during program maintenance we present an approach which extracts concepts and relations from the source code. Our approach applies natural language parsing to sentences constructed from the terms that appear in program element identifiers. The result of parsing can be represented as a dependency tree. Then, we automatically extract an ontology by mapping linguistic entities (nodes and relations between nodes in the dependency tree) to concepts and relations among concepts. We applied our approach to a case study and assessed the result in terms of the support it can give to concept location, executed in the context of bug fixing.", "num_citations": "112\n", "authors": ["311"]}
{"title": "Reverse engineering of the interaction diagrams from C++ code\n", "abstract": " In object oriented programming, the functionalities of a system result from the interactions (message exchanges) among the objects allocated by the system. While designing object interactions is far more complex than designing the object structure in forward engineering, the problem of understanding object interactions during code evolution is even harder, because the related information is spread across the code. In this paper, a technique for the automatic extraction of UML interaction diagrams from C++ code is proposed. The algorithm is based on a static, conservative flow analysis that approximates the behavior of the system in any execution and for any possible input. Applicability of the approach to large software is achieved by means of two mechanisms: partial analysis and focusing. Usage of our method on a real world, large C++ system confirmed its viability.", "num_citations": "110\n", "authors": ["311"]}
{"title": "Combining model-based and combinatorial testing for effective test case generation\n", "abstract": " Model-based testing relies on the assumption that effective adequacy criteria can be defined in terms of model coverage achieved by a set of test paths. However, such test paths are only abstract test cases and input test data must be specified to make them concrete. We propose a novel approach that combines model-based and combinatorial testing in order to generate executable and effective test cases from a model. Our approach starts from a finite state model and applies model-based testing to generate test paths that represent sequences of events to be executed against the system under test. Such paths are transformed to classification trees, enriched with domain input specifications such as data types and partitions. Finally, executable test cases are generated from those trees using t-way combinatorial criteria.", "num_citations": "105\n", "authors": ["311"]}
{"title": "Using clustering to support the migration from static to dynamic web pages\n", "abstract": " Web sites of the first generation consist typically of a set of purely static Web pages. Content and presentation are not separated, and a same page structure is replicated every time a similar organization of the information is devised. Such a practice poses several problems to the evolution of these sites. It is not easy to update the content, and each time the HTML structure is modified, the same changes have to be propagated to all replications. In this paper, an approach is proposed for the identification of the Web pages that are more amenable to be migrated into a dynamic version, in that they share a similar structure, filled in with a content organized according to a common scheme. Clustering is used for this purpose: a common template is extracted from the pages in the same cluster and the variable information of the pages matching the template is migrated to a database. A server side program extracts the\u00a0\u2026", "num_citations": "103\n", "authors": ["311"]}
{"title": "Design\u2010code traceability for object\u2010oriented systems\n", "abstract": " Traceability is a key issue to ensure consistency among software artifacts of subsequent phases of the development cycle. However, few works have so far addressed the theme of tracing object oriented (OO) design into its implementation and evolving it. This paper presents an approach to checking the compliance of OO design with respect to source code and support its evolution. The process works on design artifacts expressed in the OMT (Object Modeling Technique) notation and accepts C++ source code. It recovers an \u201cas is\u201d design from the code, compares the recovered design with the actual design and helps the user to deal with inconsistencies. The recovery process exploits the edit distance computation and the maximum match algorithm to determine traceability links between design and code. The output is a similarity measure associated to design\u2010code class pairs, which can be classified as\u00a0\u2026", "num_citations": "94\n", "authors": ["311"]}
{"title": "Using keyword extraction for web site clustering\n", "abstract": " Reverse engineering techniques have the potential to support Web site understanding, by providing views that show the organization of a site and its navigational structure. However, representing each Web page as a node in the diagrams that are recovered from the source code of a Web site leads often to huge and unreadable graphs. Moreover, since the level of connectivity is typically high, the edges in such graphs make the overall result still less usable. Clustering can be used to produce cohesive groups of pages that are displayed as a single node in reverse engineered diagrams. In this paper, we propose a clustering method based on the automatic extraction of the keywords of a Web page. The presence of common keywords is exploited to decide when it is appropriate to group pages together. A second usage of the keywords is in the automatic labeling of the recovered clusters of pages.", "num_citations": "93\n", "authors": ["311"]}
{"title": "Dynamic model extraction and statistical analysis of web applications\n", "abstract": " The World Wide Web, initially intended as a way to publish static hypertexts on the Internet, is moving toward complex applications. Static Web sites are gradually being replaced by dynamic sites, where information is stored in databases and nontrivial computation is performed. Reverse engineering of a model from an existing Web application is useful for its understanding and evolution. However, static analysis of its source code may be extremely difficult (and, in general, infeasible) because of the presence of dynamic generation of the HTML code that is part of the application under analysis. Moreover, static analysis requires the ability to process multiple languages. In this paper, a dynamic analysis technique is proposed for the extraction of a Web application model through its execution. The HTML code produced during execution on proper input values is subject to static analysis. Availability of statistical data\u00a0\u2026", "num_citations": "91\n", "authors": ["311"]}
{"title": "Is AOP code easier or harder to test than OOP code\n", "abstract": " The adoption of traditional testing techniques with AOP systems is expected to be quite hard, because of the novel constructs offered by AOP. For example, testing should validate the pointcut designators, which define the execution points at which aspects apply. These may be difficult to test when they involve complex dynamic conditions, depending on the execution stack. Other sources of difficulties are associated with the aspect composition order, with the intertype declarations, and with the changes in normal and exceptional control flow, possibly introduced by aspects. On the other hand, a novel, aspect oriented approach to testing could be devised, which takes advantage of the separation of concerns implemented in AOP code, in order to extend the benefits of such separation to the testing phase. In this paper, an incremental testing process is considered, which allows testing the base code and the crosscutting functionalities, implemented as aspects, in separate, successive steps.", "num_citations": "84\n", "authors": ["311"]}
{"title": "A case study-based comparison of web testing techniques applied to AJAX web applications\n", "abstract": " Asynchronous Javascript And XML (AJAX) is a recent technology used to develop rich and dynamic Web applications. Different from traditional Web applications, AJAX applications consist of a single page whose elements are updated dynamically in response to callbacks activated asynchronously by the user or by a server message. On the one hand, AJAX improves the responsiveness and usability of a Web application, but on the other hand, it makes the testing phase more difficult. In this paper, our state-based testing technique, developed to test AJAX-based applications, is compared to existing Web testing techniques, such as white-box and black-box ones. To this aim, an experiment based on two case studies has been conducted to evaluate effectiveness and test effort involved in the compared Web testing techniques. In particular, the capability of each technique to reveal injected faults of different\u00a0\u2026", "num_citations": "82\n", "authors": ["311"]}
{"title": "A 2-layer model for the white-box testing of web applications\n", "abstract": " White-box testing exercises a software system by ensuring that a model of the internal structure is covered by the test cases. Extending this approach to Web applications is far from obvious, because at least two abstraction levels can be considered to represent the internal structure of a Web application: the navigation model and the control flow model. To further complicate the matter, dynamic code generation must be taken into account in both models. In this paper, the two alternative models are presented and white-box testing criteria are defined on them. Their usage for the white-box testing of a real-world Web application is described, highlighting the associated costs and benefits.", "num_citations": "80\n", "authors": ["311"]}
{"title": "Statistical testing of web applications\n", "abstract": " The World Wide Web, initially intended as a way to publish static hypertexts on the Internet, is moving toward complex applications. Static Web sites are being gradually replaced by dynamic sites, where information is stored in databases and non\u2010trivial computation is performed. In such a scenario, ensuring the quality of a Web application from the user's perspective is crucial. Techniques are being investigated for the analysis and testing of Web applications for such a purpose. However, a static analysis of the source code may be extremely difficult (and, in general, infeasible) because of the presence of dynamic generation of the HTML code that is part of the application under analysis. In this paper, a dynamic analysis technique is proposed for the extraction of a Web application model through its execution. Availability of statistical data about the accesses to the pages generated by the Web application is exploited\u00a0\u2026", "num_citations": "79\n", "authors": ["311"]}
{"title": "Visual vs. DOM-based web locators: An empirical study\n", "abstract": " Automation in Web testing has been successfully supported by DOM-based tools that allow testers to program the interactions of their test cases with the Web application under test. More recently a new generation of visual tools has been proposed where a test case interacts with the Web application by recognising the images of the widgets that can be actioned upon and by asserting the expected visual appearance of the result.               In this paper, we first discuss the inherent robustness of the locators created by following the visual and DOM-based approaches and we then compare empirically a visual and a DOM-based tool, taking into account both the cost for initial test suite development from scratch and the cost for test suite maintenance during code evolution. Since visual tools are known to be computationally demanding, we also measure the test suite execution time.               Results indicate that\u00a0\u2026", "num_citations": "77\n", "authors": ["311"]}
{"title": "Web application slicing\n", "abstract": " Program slicing revealed a useful way to limit the search of software defects during debugging and to better understand the decomposition of the application into computations. We propose to extend the extraction of slices to Web applications, in order to produce a reduced Web application which behaves as the original one with respect to some criterion, i.e., some displayed information of interest. After presenting the theoretical implications of applying slicing to Web applications, we demonstrate its usefulness with reference to an example, derived from a survey of a set of travel agency sites. Web application slicing helps to disclose relevant information and understand the internal system structure.", "num_citations": "75\n", "authors": ["311"]}
{"title": "Reasoning on semantically annotated processes\n", "abstract": " Enriching business process models with semantic tags taken from an ontology has become a crucial necessity in service provisioning, integration and composition. In this paper we propose to represent semantically labelled business processes as part of a knowledge base that formalises: business process structure, business domains, and a set of criteria describing correct semantic labelling. Our approach allows (1) to impose domain dependent constraints during the phase of process design, and (2) to automatically verify, via logical reasoning, if business processes fulfill a set of given constraints, and to formulate queries that involve both knowledge about the domain and the process structure. Feasibility and usefulness of our approach will be shown by means of two use cases. The first one on domain specific constraints, and the second one on mining and evolution of crosscutting concerns.", "num_citations": "72\n", "authors": ["311"]}
{"title": "Restructuring multilingual web sites\n", "abstract": " Current practice of Web site development does not address explicitly the problems related to multilingual sites. The same information, as well as the same navigation paths, page formatting and organization, are expected to be provided by the site independently from the chosen language. This is typically ensured by adopting personal conventions on the way pages are named and on their location in the file system. Updates are then performed manually and consistency depends on the ability of the programmers not to miss any impact of the change. In this paper an extension to XHTML, called MLHTML (MultiLingual XHTML), is proposed as the target representation of a restructuring process aimed at producing a maintainable and consistent multilingual Web site. MLHTML centralizes the language dependent variants of a page in a single representation, where shared parts are not duplicated Existing sites can be\u00a0\u2026", "num_citations": "67\n", "authors": ["311"]}
{"title": "Semantically-aided business process modeling\n", "abstract": " Enriching business process models with semantic annotations taken from an ontology has become a crucial necessity both in service provisioning, integration and composition, and in business processes management. In our work we represent semantically annotated business processes as part of an OWL knowledge base that formalises the business process structure, the business domain, and a set of criteria describing correct semantic annotations. In this paper we show how Semantic Web representation and reasoning techniques can be effectively applied to formalise, and automatically verify, sets of constraints on Business Process Diagrams that involve both knowledge about the domain and the process structure. We also present a tool for the automated transformation of an annotated Business Process Diagram into an OWL ontology. The use of the semantic web techniques and tool presented in the\u00a0\u2026", "num_citations": "65\n", "authors": ["311"]}
{"title": "Refactoring the aspectizable interfaces: An empirical assessment\n", "abstract": " Aspect oriented programming aims at addressing the problem of the crosscutting concerns, i.e., those functionalities that are scattered among several modules in a given system. Aspects can be defined to modularize such concerns. In this work, we focus on a specific kind of crosscutting concerns, the scattered implementation of methods declared by interfaces that do not belong to the principal decomposition. We call such interfaces aspectizable. All the aspectizable interfaces identified within a large number of classes from the Java Standard Library and from three Java applications have been automatically migrated to aspects. To assess the effects of the migration on the internal and external quality attributes of these systems, we collected a set of metrics and we conducted an empirical study, in which some maintenance tasks were executed on the two alternative versions (with and without aspects) of the same\u00a0\u2026", "num_citations": "62\n", "authors": ["311"]}
{"title": "Using multi-locators to increase the robustness of web test cases\n", "abstract": " The main reason for the fragility of web test cases is the inability of web element locators to work correctly when the web page DOM evolves. Web elements locators are used in web test cases to identify all the GUI objects to operate upon and eventually to retrieve web page content that is compared against some oracle in order to decide whether the test case has passed or not. Hence, web element locators play an extremely important role in web testing and when a web element locator gets broken developers have to spend substantial time and effort to repair it. While algorithms exist to produce robust web element locators to be used in web test scripts, no algorithm is perfect and different algorithms are exposed to different fragilities when the software evolves. Based on such observation, we propose a new type of locator, named multi-locator, which selects the best locator among a candidate set of locators\u00a0\u2026", "num_citations": "59\n", "authors": ["311"]}
{"title": "Testing processes of web applications\n", "abstract": " Current practice in Web application development is based on the skills of the individual programmers and often does not apply the principles of software engineering. The increasing economic relevance and internal complexity of the new generation of Web applications require that proper quality standards are reached and that development is kept under control. It is therefore likely that the formalization of the process followed while developing these applications will be one of the major research topics.               In this paper we focus on Web application testing, a crucial phase when quality and reliability are a goal. Testing is considered in the wider context of the whole development process, for which an incremental/iterative model is devised. The processes behind the testing activities are analyzed considering the specificity of Web applications, for which the availability of a reference model is shown to be\u00a0\u2026", "num_citations": "59\n", "authors": ["311"]}
{"title": "Construction of the system dependence graph for web application slicing\n", "abstract": " The computation of program slices on Web applications may be useful during debugging, when the amount of code to be inspected can be reduced, and during understanding, since the search for a given functionality can be better focused. The system dependence graph is an appropriate data structure for slice computation, in that it explicitly represents all dependences that have to be taken into account in slice determination. In this paper the main problems related to the construction of the system dependence graph are considered. With no loss of generality, solutions are presented with reference to the server side programming language PHP and to the client side language Javascript. Most of the difficulties concern event and hyperlink handling, dynamic generation of HTML code, and direct access to HTML elements by client code. An example of Web application is analyzed, supporting the feasibility of the\u00a0\u2026", "num_citations": "59\n", "authors": ["311"]}
{"title": "Design-code traceability recovery: selecting the basic linkage properties\n", "abstract": " Traceability ensures that software artifacts of subsequent phases of the development cycle are consistent. Few works have so far addressed the problem of automatically recovering traceability links between object-oriented (OO) design and code entities. Such a recovery process is required whenever there is no explicit support of traceability from the development process. The recovered information can drive the evolution of the available design so that it corresponds to the code, thus providing a still useful and updated high-level view of the system. Automatic recovery of traceability links can be achieved by determining the similarity of paired elements from design and code. The choice of the properties involved in the similarity computation is crucial for the success of the recovery process. In fact, design and code objects are complex artifacts with several properties attached. The basic anchors of the recovered\u00a0\u2026", "num_citations": "59\n", "authors": ["311"]}
{"title": "Building a tool for the analysis and testing of web applications: Problems and solutions\n", "abstract": " Web applications are becoming increasingly complex and important for companies. Their design, development, analysis and testing need therefore to be approached by means of support tools and methodologies. In this paper we consider the problems related to building tools for the analysis and testing of Web applications and we try to provide some indications on possible solutions, based upon our experience in the development of the tools ReWeb and TestWeb.               The definition of a proper reference model will be discussed, as well as the impact of dynamic pages during Web site downloading and subsequent model construction. Visualization techniques addressing the large amount of extracted data will be presented, while infeasibility problems will be considered with reference to the testing phase.", "num_citations": "57\n", "authors": ["311"]}
{"title": "Search-based testing of Ajax web applications\n", "abstract": " Ajax is an emerging Web engineering technology that supports advanced interaction features that go beyond Webpage navigation. The Ajax technology is based on asynchronous communication with the Web server and direct manipulation of the GUI, taking advantage of reflection.Correspondingly, new classes of Web faults are associated with Ajax applications.In previous work, we investigated a state-based testing approach, based on semantically interacting events. The main drawback of this approach is that exhaustive generation of semantically interacting event sequences limits quite severely the maximum achievable length, while longer sequences would have higher fault exposing capability. In this paper, we investigate a search-based algorithm for the exploration of the huge space of long interaction sequences, in order to select those that are most promising, based on a measure of test case diversity.", "num_citations": "56\n", "authors": ["311"]}
{"title": "Web testing: a roadmap for the empirical research\n", "abstract": " The quality delivered by existing Web applications is often poor. A consequence of this situation is a strong demand for techniques and tools that address the problem of the Web application quality. Lots of approaches are currently available, often coming with prototype or commercial tools implementing them. However, no attempt has been made so far to validate their effectiveness. In this paper, we consider the available techniques for Web testing and we propose a classification into three major groups. We deal with the problem of defining the Web-specific faults. Our approach is an empirical investigation of the reported faults, abstracted into a fault model. Then, we evaluate the available techniques against the fault model, in terms of the fault categories directly addressed by them. Finally, we sketch a roadmap for the future empirical research.", "num_citations": "56\n", "authors": ["311"]}
{"title": "Supporting ontology-based semantic annotation of business processes with automated suggestions\n", "abstract": " Annotation of Business Processes with semantic tags taken from a domain ontology is beneficial to several activities conducted on Business Processes, such as comprehension, documentation, analysis and evolution. On the other hand, the task of semantically annotating Business Processes is time-consuming and far from trivial. The authors support Business Process designers in the annotation of process elements by automatically suggesting candidate concepts. The annotation suggestions are computed on the basis of a similarity measure between the text information associated with process element labels and the ontology concepts. In turn, this requires support for the disambiguation of terms appearing in ontology concepts, which admit multiple linguistic senses, and for ontology extension, when the available concepts are insufficient.", "num_citations": "55\n", "authors": ["311"]}
{"title": "Using search-based algorithms for Ajax event sequence generation during testing\n", "abstract": " Modern Web applications offer a rich and unique user experience by taking advantages of the so called Web 2.0 technologies, among which Ajax. Ajax supports the intensive use of asynchronous communication between client-pages and the Web server and it allows on-the-fly manipulations of client-pages content and structure to realize a rich, dynamic and interactive user interface. Correspondingly, new types of faults that cannot be easily revealed by existing Web testing techniques are associated with modern Ajax-based applications. In our previous investigations, we used state-based testing for event sequence generation and it proved to be quite effective in exposing Ajax specific faults. However, the search space of the semantically interacting event sequences is huge, as it can grow exponentially with the event sequence length. In this paper, we apply search-based algorithms, namely hill climbing\u00a0\u2026", "num_citations": "54\n", "authors": ["311"]}
{"title": "Reverse engineering of business processes exposed as web applications\n", "abstract": " Business processes are often implemented by means of software systems which expose them to the user as an externally accessible Web application. This paper describes a technique for recovering business processes by dynamic analysis of the Web applications which ex-pose them. This approach does not require full access to internal software artifacts, such as source code or documentation. The business process is instead inferred through analysis of the GUI-forms exercised by the user during the navigation in the Web application which ex-poses the process. The recovered process is then abstracted by clustering its business tasks according to structural or logical criteria.A preliminary experiment has been conducted with the aim of evaluating understandability and readability of the reverse engineered business processes.", "num_citations": "54\n", "authors": ["311"]}
{"title": "Test case prioritization for audit testing of evolving web services using information retrieval techniques\n", "abstract": " Web services evolve frequently to meet new business demands and opportunities. However, service changes may affect service compositions that are currently consuming the services. Hence, audit testing (a form of regression testing in charge of checking for compatibility issues) is needed. As service compositions are often in continuous operation and the external services have limited (expensive) access when invoked for testing, audit testing has severe time and resources constraints, which make test prioritization a crucial technique (only the highest priority test cases will be executed).This paper presents a novel approach to the prioritization of audit test cases using information retrieval. This approach matches a service change description with the code portions exercised by the relevant test cases. So, test cases are prioritized based on their relevance to the service change. We evaluate the proposed approach\u00a0\u2026", "num_citations": "52\n", "authors": ["311"]}
{"title": "Approaches and tools for automated end-to-end web testing\n", "abstract": " The importance of test automation in web engineering comes from the widespread use of web applications and the associated demand for code quality. Test automation is considered crucial for delivering the quality levels expected by users, since it can save a lot of time in testing and it helps developers to release web applications with fewer defects. The main advantage of test automation comes from fast, unattended execution of a set of tests after some changes have been made to a web application. Moreover, modern web applications adopt a multitier architecture where the implementation is scattered across different layers and run on different machines. For this reason, end-to-end testing techniques are required to test the overall behavior of web applications.In the last years, several approaches have been proposed for automated end-to-end web testing and the choice among them depends on a number of\u00a0\u2026", "num_citations": "50\n", "authors": ["311"]}
{"title": "Crosscutting concern documentation by visual query of business processes\n", "abstract": " Business processes can be very large and may contain several different concerns, scattered across the process and tangled with other concerns. Crosscutting concerns are difficult to find and locate, thus making process design and evolution hard.               In this work, we propose a method to support business designers in documenting structural as well as business domain crosscutting concerns, thus facilitating concern understanding and evolution. We introduce a visual query language, which allows business designers to mine, explore, document and evolve crosscutting concerns, by means of visual queries performed on the business process. Such queries can be stored as additional design artefacts which document the existence and location of crosscutting design concerns.", "num_citations": "49\n", "authors": ["311"]}
{"title": "Augmenting pattern-based architectural recovery with flow analysis: Mosaic-a case study\n", "abstract": " Understanding the overall organization of a software system, i.e. its software architecture, is often required during software maintenance: tools can help maintainers in managing the evolution of legacy systems, by showing them architectural information. The analysis of a medium-sized application using a pattern based architectural recovery environment is presented. The results obtained give useful information about the system architecture but also show some limitations of a purely pattern based approach. To overcome such limitations, architectural analysis algorithms have been augmented with information about control and data flow and the case study application has been re-analyzed. Complementing pattern matching with flow information has also allowed detection of architectural constructs when they are spread over different procedures in source code and to extract useful additional information through the\u00a0\u2026", "num_citations": "48\n", "authors": ["311"]}
{"title": "Robula+: an algorithm for generating robust XPath locators for web testing\n", "abstract": " Automated test scripts are used with success in many web development projects, so as to automatically verify key functionalities of the web application under test, reveal possible regressions and run a large number of tests in short time. However, the adoption of automated web testing brings advantages but also novel problems, among which the test code fragility problem. During the evolution of the web application, existing test code may easily break and testers have to correct it. In the context of automated DOM\u2010based web testing, one of the major costs for evolving the test code is the manual effort necessary to repair broken web page element locators \u2013 lines of source code identifying the web elements (e.g. form fields and buttons) to interact with. In this work, we present Robula+, a novel algorithm able to generate robust XPath\u2010based locators \u2013 locators that are likely to work correctly on new releases of the web\u00a0\u2026", "num_citations": "47\n", "authors": ["311"]}
{"title": "Reajax: a reverse engineering tool for ajax web applications\n", "abstract": " In contrast to conventional multi-page Web applications, an Ajax application is often developed as a single-page application in which content and structure are changed at runtime according to user interactions, asynchronous messages received from the server and the current state of the application. These features make Ajax applications quite hard to understand for programmers. The authors propose to support Ajax comprehension through reverse engineering. In this study, the authors propose a reverse-engineering tool, ReAjax, to build GUI-based state models from Ajax applications. ReAjax applies dynamic analysis and uses execution traces to generate a finite state machine of the target application GUI. They show that GUI-based state models obtained semi-automatically are similar to those obtained manually and they can be used for program understanding purposes. Finally, the authors summarise a case\u00a0\u2026", "num_citations": "45\n", "authors": ["311"]}
{"title": "Inference of object\u2010oriented design patterns\n", "abstract": " When designing a new application, experienced software engineers usually adopt solutions that have proven successful in previous projects. Such reuse of code organizations is seldom made explicit. Nevertheless, it represents important information, which can be extremely valuable in the maintenance phase by documenting the design choices underlying the implementation. In addition it can be reused whenever a similar problem is encountered. In this paper an approach for the inference of recurrent design patterns directly from the code is proposed. No assumption is made on the availability of any pattern library, and the concept analysis algorithm\u2014adapted for this purpose\u2014is able to infer the presence of class groups which instantiate a common, repeated pattern. In fact, concept analysis provides sets of objects sharing attributes, which\u2014in the case of object\u2010oriented design patterns\u2014become class\u00a0\u2026", "num_citations": "45\n", "authors": ["311"]}
{"title": "Empirical validation of a web fault taxonomy and its usage for fault seeding\n", "abstract": " The increasing demand for reliable Web applications gives a central role to Web testing. Most of the existing works are focused on the definition of novel testing techniques, specifically tailored to the Web. However, no attempt was carried out so far to understand the specific nature of Web faults. This is of fundamental importance to assess the effectiveness of the proposed Web testing techniques. In this paper, we describe the process followed in the construction of a Web fault taxonomy. After the initial, top- down construction, the taxonomy was subjected to four iterations of empirical validation aimed at refining it and at understanding its effectiveness in bug classification. The final taxonomy is publicly available for consultation and editing on a Wiki page. Testers can use it in the definition of test cases that target specific classes of Web faults. Researchers can use it to build fault seeding tools that inject artificial faults\u00a0\u2026", "num_citations": "43\n", "authors": ["311"]}
{"title": "Reverse engineering of the UML class diagram from C++ code in presence of weakly typed containers\n", "abstract": " UML diagrams, and in particular the most frequently used one, the class diagram, represent a valuable source of information even after the delivery of the system, when it enters the maintenance phase. Several tools provide a reverse engineering engine to recover it from the code. In this paper an algorithm is proposed for the improvement of the accuracy of the UML class diagram extracted from the code. Specifically, important information about inter-class relations may be missed in a reverse engineered class diagram, when weakly typed containers, i.e., containers collecting objects whose type is the top of the inheritance hierarchy, are employed. In fact, the class of the contained objects is not directly known, and therefore no relation with it is apparent from the container declaration. The proposed approach was applied to several software components developed at CERN. Experimental results highlight that a\u00a0\u2026", "num_citations": "43\n", "authors": ["311"]}
{"title": "Why do record/replay tests of web applications break?\n", "abstract": " Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this manner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle, they can easily break as applications change. For this reason, researchers have begun to seek approaches for automatically repairing record/replay tests. To date, however, there have been no comprehensive attempts to characterize the causes of breakagesin record/replay tests for web applications. In this work, wepresent a taxonomy classifying the ways in which record/replay tests for web applications break, based on an analysis of 453 versions of popular web applications for which 1065 individual test breakages were recognized. The resulting taxonomy can help direct researchers in their attempts to repair\u00a0\u2026", "num_citations": "41\n", "authors": ["311"]}
{"title": "Do automatically generated test cases make debugging easier? an experimental assessment of debugging effectiveness and efficiency\n", "abstract": " Several techniques and tools have been proposed for the automatic generation of test cases. Usually, these tools are evaluated in terms of fault-revealing or coverage capability, but their impact on the manual debugging activity is not considered. The question is whether automatically generated test cases are equally effective in supporting debugging as manually written tests. We conducted a family of three experiments (five replications) with humans (in total, 55 subjects) to assess whether the features of automatically generated test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on the effectiveness and efficiency of debugging. The first two experiments compare different test case generation tools (Randoop vs. EvoSuite). The third experiment investigates the role of code identifiers in test cases (obfuscated vs. original identifiers\u00a0\u2026", "num_citations": "40\n", "authors": ["311"]}
{"title": "Reducing web test cases aging by means of robust XPath locators\n", "abstract": " In the context of web regression testing, the main aging factor for a test suite is related to the continuous evolution of the underlying web application that makes the test cases broken. This rapid decay forces the quality experts to evolve the test ware. One of the major costs of test case evolution is due to the manual effort necessary to repair broken web page element locators. Locators are lines of source code identifying the web elements the test cases interact with. Web test cases rely heavily on locators, for instance to identify and fill the input portions of a web page (e.g., The form fields), to execute some computations (e.g., By locating and clicking on buttons) and to verify the correctness of the output (by locating the web page elements showing the results). In this paper we present ROBULA (ROBUst Locator Algorithm), a novel algorithm able to partially prevent and thus reduce the aging of web test cases by\u00a0\u2026", "num_citations": "39\n", "authors": ["311"]}
{"title": "Interpolated n-grams for model based testing\n", "abstract": " Models-in particular finite state machine models-provide an invaluable source of information for the derivation of effective test cases. However, models usually approximate part of the program semantics and capture only some of the relevant dependencies and constraints. As a consequence, some of the test cases that are derived from models are infeasible.", "num_citations": "39\n", "authors": ["311"]}
{"title": "Web application slicing in presence of dynamic code generation\n", "abstract": " The computation of program slices on Web applications may be useful during debugging, when the amount of code to be inspected can be reduced, and during understanding, since the search for a given functionality can be better focused. The system dependence graph is an appropriate data structure for slice computation, in that it explicitly represents all dependences that have to be taken into account in slice determination.               Construction of the system dependence graph for Web applications is complicated by the presence of dynamically generated code. In fact, a Web application builds the HTML code to be transmitted to the browser at run time. Knowledge of such code is essential for slicing. In this paper an algorithm for the static approximation of the dynamically generated HTML code is proposed. The concatenations of constant strings and variables are propagated according to special purpose\u00a0\u2026", "num_citations": "39\n", "authors": ["311"]}
{"title": "Barrier slicing for remote software trusting\n", "abstract": " Remote trusting aims at verifying the \"healthy\" execution of a program running on an untrusted client that communicates with a trusted server via network connection. After giving a formal definition of the remote trusting problem and a test to determine whether an attack against a given remote trusting scheme is successful or not, we propose a protection against malicious modification of the client code, based on the replication of a portion of the client on the server. To minimize the size of the code that is replicated, we propose to use barrier slicing. We show the feasibility of our approach on a case study. Our results indicate that a barrier slice is significantly smaller than the corresponding backward slice while providing the same level of protection.", "num_citations": "38\n", "authors": ["311"]}
{"title": "Migrating interface implementations to aspects\n", "abstract": " Separation of concerns and modularization are the cornerstones of software engineering. However, when a system is decomposed into units, functionalities often emerge which cannot be assigned to a single element of the decomposition. The implementation of interfaces represents a typical instance of this problem. In fact, the code that defines the interface methods is often scattered across several classes in the system and tangled with the original code. Aspect oriented programming provides mechanisms for the dynamic and static composition of transversal functionalities, that can be used to factor out the implementation of interfaces. We describe a technique for the identification of those interface implementations that are most likely to represent crosscutting concerns. Moreover, the code transformation (refactoring) to migrate such interfaces to aspects is also presented. Experimental results validate the approach.", "num_citations": "37\n", "authors": ["311"]}
{"title": "Testing machine learning based systems: a systematic mapping\n", "abstract": " Context: A Machine Learning based System (MLS) is a software system including one or more components that learn how to perform a task from a given data set. The increasing adoption of MLSs in safety critical domains such as autonomous driving, healthcare, and finance has fostered much attention towards the quality assurance of such systems. Despite the advances in software testing, MLSs bring novel and unprecedented challenges, since their behaviour is defined jointly by the code that implements them and the data used for training them.   Objective: To identify the existing solutions for functional testing of MLSs, and classify them from three different perspectives: (1) the context of the problem they address, (2) their features, and (3) their empirical evaluation. To report demographic information about the ongoing research. To identify open challenges for future research.   Method: We conducted a systematic\u00a0\u2026", "num_citations": "36\n", "authors": ["311"]}
{"title": "Detecting anomaly and failure in web applications\n", "abstract": " Improving Web application quality will require automated evaluation tools. Many such tools are already available either as commercial products or research prototypes. The authors use their automated evaluation tools, ReWeb and TestWeb, for Web analysis and testing that improves Web pages and applications and to find some anomalies and failures in four case studies.", "num_citations": "36\n", "authors": ["311"]}
{"title": "Application and user interface migration from basic to visual C++\n", "abstract": " An approach to reengineer BASIC PC legacy code into modern graphical systems is proposed. BASIC has historically been one of the first languages available on PCs. Based on it, small or medium size companies have developed systems that represent valuable company assets to be preserved. Our goal is the automatic migration from the BASIC character oriented user interface to a graphical environment which includes a GUI builder, and compiles event driven C/C++ code. For this purpose a conceptual representation in terms of abstract graphical objects and call-backs has been inferred from the original code, and a translator from BASIC to C has been developed. Moreover the GUI builder internal representation has been generated, so that the user interface can be interactively fine-tuned by the programmer. We present and discuss BASIC peculiarities, with preliminary results on code translation. For the\u00a0\u2026", "num_citations": "36\n", "authors": ["311"]}
{"title": "Assessment of source code obfuscation techniques\n", "abstract": " Obfuscation techniques are a general category of software protections widely adopted to prevent malicious tampering of the code by making applications more difficult to understand and thus harder to modify. Obfuscation techniques are divided in code and data obfuscation, depending on the protected asset. While preliminary empirical studies have been conducted to determine the impact of code obfuscation, our work aims at assessing the effectiveness and efficiency in preventing attacks of a specific data obfuscation technique - VarMerge. We conducted an experiment with student participants performing two attack tasks on clear and obfuscated versions of two applications written in C. The experiment showed a significant effect of data obfuscation on both the time required to complete and the successful attack efficiency. An application with VarMerge reduces by six times the number of successful attacks per unit\u00a0\u2026", "num_citations": "35\n", "authors": ["311"]}
{"title": "APOGEN: automatic page object generator for web testing\n", "abstract": " Modern web applications are characterized by ultra-rapid development cycles, and web testers tend to pay scant attention to the quality of their automated end-to-end test suites. Indeed, these quickly become hard to maintain, as the application under test evolves. As a result, end-to-end automated test suites are abandoned, despite their great potential for catching regressions. The use of the Page Object pattern has proven to be very effective in end-to-end web testing. Page objects are fa\u00e7ade classes abstracting the internals of web pages into high-level business functions that can be invoked by the test cases. By decoupling test code from web page details, web test cases are more readable and maintainable. However, the manual development of such page objects requires substantial coding effort, which is paid off only later, during software evolution. In this paper, we describe a novel approach for the\u00a0\u2026", "num_citations": "34\n", "authors": ["311"]}
{"title": "Pesto: Automated migration of DOM\u2010based Web tests towards the visual approach\n", "abstract": " Test automation tools are widely adopted for testing complex Web applications. Three generations of tools exist: first, based on screen coordinates; second, based on DOM\u2013based commands; and third, based on visual image recognition. In our previous work, we proposed Pesto, a tool able to migrate second\u2010generation Selenium WebDriver test suites towards third\u2010generation Sikuli ones. In this work, we extend Pesto to manage Web elements having (1) complex visual interactions and (2) multiple visual appearances. Pesto relies on aspect\u2010oriented programming, computer vision, and code transformations. Our new improved tool has been evaluated on two Web test suites developed by an independent tester. Experimental results show that Pesto manages and transforms correctly test suites with Web elements having complex visual interactions and multistate elements. By using Pesto, the migration of existing\u00a0\u2026", "num_citations": "33\n", "authors": ["311"]}
{"title": "Automated oracles: An empirical study on cost and effectiveness\n", "abstract": " Software testing is an effective, yet expensive, method to improve software quality. Test automation, a potential way to reduce testing cost, has received enormous research attention recently, but the so-called \u201coracle problem\u201d(how to decide the PASS/FAIL outcome of a test execution) is still a major obstacle to such cost reduction. We have extensively investigated state-of-the-art works that contribute to address this problem, from areas such as specification mining and model inference. In this paper, we compare three types of automated oracles: Data invariants, Temporal invariants, and Finite State Automata. More specifically, we study the training cost and the false positive rate; we evaluate also their fault detection capability. Seven medium to large, industrial application subjects and real faults have been used in our empirical investigation.", "num_citations": "33\n", "authors": ["311"]}
{"title": "Misbehaviour prediction for autonomous driving systems\n", "abstract": " Deep Neural Networks (DNNs) are the core component of modern autonomous driving systems. To date, it is still unrealistic that a DNN will generalize correctly to all driving conditions. Current testing techniques consist of offline solutions that identify adversarial or corner cases for improving the training phase.", "num_citations": "32\n", "authors": ["311"]}
{"title": "Reproducing field failures for programs with complex grammar-based input\n", "abstract": " To isolate and fix failures that occur in the field, after deployment, developers must be able to reproduce and investigate such failures in-house. In practice, however, bug reports rarely provide enough information to recreate field failures, thus making in-house debugging an arduous task. This task becomes even more challenging for programs whose input must adhere to a formal specification, such as a grammar. To help developers address this issue, we propose an approach for automatically generating inputs that recreate field failures in-house. Given a faulty program and a field failure for this program, our approach exploits the potential of grammar-guided genetic programming to iteratively find legal inputs that can trigger the observed failure using a limited amount of runtime data collected in the field. When applied to 11 failures of 5 real-world programs, our approach was able to reproduce all but one of the\u00a0\u2026", "num_citations": "31\n", "authors": ["311"]}
{"title": "Semantics-based aspect-oriented management of exceptional flows in business processes\n", "abstract": " Enriching business process models with semantic annotations that are taken from an ontology has become a crucial need in service provisioning, integration and composition, and business processes management. We represent semantically annotated business processes as part of an Web ontology lanuage knowledge base that formalizes the business process structure, the business domain, a set of criteria that describe correct semantic annotations, and a set of constraints that describe requirements on the business process itself. In this paper, we show how the Semantic Web representation and reasoning techniques can be 1) exploited by our aspect-oriented approach to modularize exception-handling (as well as other crosscutting) mechanisms and 2) effectively applied to formalize and automatically verify constraints on the management of exceptional flows (as well as other relevant flows) in business\u00a0\u2026", "num_citations": "31\n", "authors": ["311"]}
{"title": "Automatic unit test data generation using mixed-integer linear programming and execution trees\n", "abstract": " This paper presents an approach to automatic unit test data generation for branch coverage using mixed-integer linear programming, execution trees, and symbolic execution. This approach can be useful to both general testing and regression testing after software maintenance and reengineering activities. Several strategies, including original algorithms, to move towards practical test data generation have been investigated in this paper. Methods include: the analysis of minimum path-length partial execution trees for unconstrained arcs, thus increasing the generation performance and reducing the difficulties originated by infeasible paths the reduction of the difficulties originated by nonlinear path conditions by considering alternative linear paths the reduction of the number of test cases, which are needed to achieve the desired coverage, based on the concept of unconstrained arcs in a control flow graph the\u00a0\u2026", "num_citations": "31\n", "authors": ["311"]}
{"title": "An empirical study on keyword-based web site clustering\n", "abstract": " Web site evolution is characterized by a limited support to the understanding activities offered to the developers. In fact, design diagrams are often missing or outdated. A potentially interesting option is to reverse engineer high level views of Web sites from the content of the Web pages. Clustering is a valuable technique that can be used in this respect. Web pages can be clustered together based on the similarity of summary information about their content, represented as a list of automatically extracted keywords. This work presents an empirical study that was conducted to determine the meaningfulness for Web developers of clusters automatically produced from the analysis of the Web page content. Natural language processing (NLP) plays a central role in content analysis and keyword extraction. Thus, a second objective of the study was to assess the contribution of some shallow NLP techniques to the clustering\u00a0\u2026", "num_citations": "30\n", "authors": ["311"]}
{"title": "Formal concept analysis in software engineering\n", "abstract": " Given a binary relationship between objects and attributes, concept analysis is a powerful technique to organize pairs of related sets of objects and attributes into a concept lattice, where higher level concepts represent general features shared by many objects, while lower level concepts represent the object-specific features. Concept analysis was recently applied to several software engineering problems, such as: restructuring the code into more cohesive components, identifying class candidates, locating features in the code by means of dynamic analysis, reengineering class hierarchies. This paper provides the background knowledge required by such applications. Moreover, the methodological issues involved in the different applications of this technique are considered by giving a detailed presentation of three of them: module restructuring, design pattern inference and impact analysis based on decomposition\u00a0\u2026", "num_citations": "30\n", "authors": ["311"]}
{"title": "Visualization of web site history\n", "abstract": " The economic relevance of web sites has increased for several companies, which incorporate sophisticated technologies into complex and large web based systems. As a consequence methodologies and tools are required for their design, implementation and maintenance.                    In this paper the evolution of web sites is analyzed, with the purpose of supporting maintenance activities. Colors are exploited to visualize modifications that take place on the site over time. Such analysis may reveal changes not corresponding to the original design or producing undesirable effects.                  A tool was developed to implement the analysis of web site evolution. Its application to some examples downlodaded from the Web highlights several areas where the extracted information can improve the control on the maintenance phase and provide valuable support", "num_citations": "30\n", "authors": ["311"]}
{"title": "Static and dynamic C++ code analysis for the recovery of the object diagram\n", "abstract": " When a software system enters the maintenance phase, the availability of accurate and consistent information about its organization can help alleviate the difficulties of program understanding. Reverse engineering methods aim at extracting such information directly from the code. While several tools support the recovery of the class diagram from object oriented code, so far no work has attacked the problem of statically characterizing the behavior of an object oriented system by means of diagrams which represent the class instances (objects) and their mutual relationships. In this paper a novel static analysis algorithm is proposed for extraction of the object diagram from the code, based on a program representation called the object flow graph. Partial object diagrams can be associated dynamically to the system by executing and tracing the program on a set of test cases. The complementary nature of these two\u00a0\u2026", "num_citations": "28\n", "authors": ["311"]}
{"title": "How professional hackers understand protected code while performing attack tasks\n", "abstract": " Code protections aim at blocking (or at least delaying) reverse engineering and tampering attacks to critical assets within programs. Knowing the way hackers understand protected code and perform attacks is important to achieve a stronger protection of the software assets, based on realistic assumptions about the hackers' behaviour. However, building such knowledge is difficult because hackers can hardly be involved in controlled experiments and empirical studies. The FP7 European project Aspire has given the authors of this paper the unique opportunity to have access to the professional penetration testers employed by the three industrial partners. In particular, we have been able to perform a qualitative analysis of three reports of professional penetration test performed on protected industrial code. Our qualitative analysis of the reports consists of open coding, carried out by 7 annotators and resulting in 459\u00a0\u2026", "num_citations": "27\n", "authors": ["311"]}
{"title": "Towards the extraction of domain concepts from the identifiers\n", "abstract": " Program identifiers represent an invaluable source of information for developers who are not familiar with the code to be evolved. Domain concepts and inter-concept relationships can be automatically extracted by means of natural language processing techniques applied to the program identifiers. However, the ontology produced by this approach tends to be very large and to include implementation details that reduce its usefulness for domain concept understanding. In this paper, we analyze the effectiveness of information retrieval based techniques used to filter domain concepts and relations from the implementation details, so as to obtain a smaller, more informative domain ontology. In particular, we show that fully automated techniques based on keywords or topics have quite poor performance, while a semi-automated approach, requiring limited user involvement, can highly improve the filtering of domain\u00a0\u2026", "num_citations": "27\n", "authors": ["311"]}
{"title": "Adding distribution to existing applications by means of aspect oriented programming\n", "abstract": " Aspect oriented programming (AOP) is a new programming paradigm that offers a novel modularization unit for the crosscutting concerns. Functionalities originally spread across several modules and tangled with each other can be factored out into a single, separate unit, called an aspect. The source code fragments introduced to port an existing application to a distributed environment (such as Java RAH) are typically scattered and tangled, thus representing an ideal candidate for the usage of aspects. In This work, we propose a distribution framework based on AOP and we describe the steps necessary to migrate an existing program to it. In our solution, the original application remains oblivious of the distribution concern and all required aspects are generated automatically. The approach was validated on a case study.", "num_citations": "27\n", "authors": ["311"]}
{"title": "Evaluation methods for web application clustering\n", "abstract": " Clustering of the entities composing a Web application (static and dynamic pages) can be used to support program understanding, However, several alternative options are available when a clustering technique is designed for Web applications. The entities to be clustered can be described in different ways (e.g., by their structure, by their connectivity, or by their content), different similarity measures are possible, and alternative procedures can be used to form the clusters. The problem is how to evaluate the competing clustering techniques in order to select the best for program understanding purposes. In this paper, two methods for clustering evaluation are considered, the gold standard and the task oriented approach. The advantages and disadvantages of both of them are analyzed in detail. Definition of a gold standard (reference clustering) is difficult and prone to subjectivity. On the other side, an evaluation\u00a0\u2026", "num_citations": "27\n", "authors": ["311"]}
{"title": "Improving Web site understanding with keyword\u2010based clustering\n", "abstract": " Web applications are becoming more and more complex and difficult to maintain. To satisfy the customer's demands, they need to be updated often and quickly. In the maintenance phase, Web site understanding is a central activity. In this phase, programmers spend a lot of time and effort in the comprehension of the internal Web site structure. Such activity is often required because the available documentation is not aligned with the implementation, if not missing at all. Reverse engineering techniques have the potential to support Web site understanding, by providing views that show the organization of a site and its navigational structure. However, representing each Web page as a node in a diagram recovered from the source code of the Web site often leads to huge and unreadable graphs. Moreover, since the level of connectivity is typically high, the edges in such graphs make the overall result even less\u00a0\u2026", "num_citations": "26\n", "authors": ["311"]}
{"title": "Evolving object oriented design to improve code traceability\n", "abstract": " Traceability is a key issue to ensure consistency among software artifacts of subsequent phases of the development cycle. However, few works have so far addressed the theme of tracing object oriented design into its implementation and evolving it. The paper presents an approach to checking the compliance of OO design with respect to source code and support its evolution. The process works on design artifacts expressed in the OMT notation and accepts C++ source code. It recovers an \"as is\" design from the code, compares recovered design with the actual design and helps the user to deal with inconsistencies. The recovery process exploits the edit distance computation and the maximum match algorithm to determine traceability links between design and code. The output is a similarity measure associated to each matched class, plus a set of unmatched classes. A graphic display of the design with different\u00a0\u2026", "num_citations": "26\n", "authors": ["311"]}
{"title": "Dynamic model for maintenance and testing effort\n", "abstract": " The dynamic evolution of ecological systems in which predators and prey compete for survival has been investigated by applying suitable mathematical models. Dynamic systems theory provides a useful way to model interspecies competition and thus the evolution of predator and prey populations. This kind of mathematical framework has been shown to be well suited to describe evolution of economical systems as well, where instead of predators and prey there are consumers and resources. Maintenance and testing activities absorb the most relevant part of the total life-cycle cost of software. Such economic relevance strongly suggests to investigate the maintenance and testing processes in order to find new models allowing software engineers to better estimate, plan and manage costs and activities. We show how dynamic systems theory could be usefully applied to maintenance and testing context, namely to\u00a0\u2026", "num_citations": "26\n", "authors": ["311"]}
{"title": "Why creating web page objects manually if it can be done automatically?\n", "abstract": " Page Object is a design pattern aimed at making web test scripts more readable, robust and maintainable. The effort to manually create the page objects needed for a web application may be substantial and unfortunately existing tools do not help web developers in such task. In this paper we present APOGEN, a tool for the automatic generation of page objects for web applications. Our tool automatically derives a testing model by reverse engineering the target web application and uses a combination of dynamic and static analysis to generate Java page objects for the popular Selenium WebDriver framework. Our preliminary evaluation shows that it is possible to use around 3/4 of the automatic page object methods as they are, while the remaining 1/4 need only minor modifications.", "num_citations": "25\n", "authors": ["311"]}
{"title": "A framework for the collaborative specification of semantically annotated business processes\n", "abstract": " Semantic annotations are a way to provide a precise meaning to business process elements, which supports reasoning on properties and constraints. Among the obstacles preventing widespread adoption of semantic annotations are the technical skills required to manage the formalization of the semantics and the difficulty of reconciling the different viewpoints of different analysts working on the same business process. In this paper, we support business analysts in the collaborative annotation of business processes by means of a tool inspired to the Wiki pages model. Using this tool, analysts can concurrently work on process elements, ontology concepts, process annotation or constraint specification. The underlying formalism is not exposed in the Wiki pages, where natural language templates are used. Copyright \u00a9 2011 John Wiley & Sons, Ltd.", "num_citations": "25\n", "authors": ["311"]}
{"title": "Web crawlers compared\n", "abstract": " Tools for the assessment of the quality and reliability of Web applications are based on the possibility of downloading the target of the analysis. This is achieved through Web crawlers, which can automatically navigate within a Web site and perform proper actions (such as download) during the visit. The most important performance indicators for a Web crawler are its completeness and robustness, measuring respectively the ability to visit the Web site entirely and without errors. The variety of implementation languages and technologies used for Web site development makes these two indicators hard to maximize. We conducted an evaluation study, in which we tested several of the available Web crawlers.", "num_citations": "25\n", "authors": ["311"]}
{"title": "Maintenance and testing effort modeled by linear and nonlinear dynamic systems\n", "abstract": " Maintenance and testing activities \u2014 conducted, respectively, on the release currently in use/to be delivered \u2014 absorb most of total lifetime cost of software development. Such economic relevance suggests investigating the maintenance and testing processes to find models allowing software engineers to better estimate, plan and manage costs and activities.Ecological systems in which predators and prey compete for surviving were investigated by applying suitable mathematical models. An analogy can be drawn between biological prey and software defects, and between predators and programmers. In fact, when programmers start trying to recognize and correct code defects, while the number of residual defects decreases, the effort spent to find any new defect has an initial increase, followed by a decline, when almost all defects are removed, similar to prey and predator populations.This paper proposes to\u00a0\u2026", "num_citations": "25\n", "authors": ["311"]}
{"title": "Model-based exploration of the frontier of behaviours for deep learning system testing\n", "abstract": " With the increasing adoption of Deep Learning (DL) for critical tasks, such as autonomous driving, the evaluation of the quality of systems that rely on DL has become crucial. Once trained, DL systems produce an output for any arbitrary numeric vector provided as input, regardless of whether it is within or outside the validity domain of the system under test. Hence, the quality of such systems is determined by the intersection between their validity domain and the regions where their outputs exhibit a misbehaviour.", "num_citations": "22\n", "authors": ["311"]}
{"title": "Understanding the behaviour of hackers while performing attack tasks in a professional setting and in a public challenge\n", "abstract": " When critical assets or functionalities are included in a piece of software accessible to the end users, code protections are used to hinder or delay the extraction or manipulation of such critical assets. The process and strategy followed by hackers to understand and tamper with protected software might differ from program understanding for benign purposes. Knowledge of the actual hacker behaviours while performing real attack tasks can inform better ways to protect the software and can provide more realistic assumptions to the developers, evaluators, and users of software protections. Within Aspire, a software protection research project funded by the EU under framework programme FP7, we have conducted three industrial case studies with the involvement of professional penetration testers and a public challenge consisting of eight attack tasks with open participation. We have applied a systematic\u00a0\u2026", "num_citations": "22\n", "authors": ["311"]}
{"title": "PESTO: A tool for migrating DOM-based to visual web tests\n", "abstract": " Automated testing of web applications reduces the effort needed in manual testing. Old 1st generation tools, based on screen coordinates, produce quite fragile test suites, tightly coupled with the specific screen resolution, window position and size experienced during test case recording. These tools have been replaced by a 2nd generation of tools, which offer easy selection and interaction with the web elements, based on DOM-oriented commands. Recently, a new 3rd generation of tools came up based on visual image recognition, bringing the promise of wider applicability and simplicity. A tester might ask if the migration towards such new technology is worthwhile, since the manual effort to rewrite a test suite might be overwhelming. In this paper, we propose PESTO, a tool facing the problem of the automated migration of 2nd generation test suites to the 3rd generation. PESTO determines automatically the\u00a0\u2026", "num_citations": "22\n", "authors": ["311"]}
{"title": "Recent advances in web testing\n", "abstract": " Web applications have become key assets of our society, which depends on web applications for sectors like business, health-care, and public administration. Testing is the most widely used and effective approach to ensure quality and dependability of the software, including web applications. However, web applications are special as compared to traditional software, because they involve dynamic code creation and interpretation and because they implement a specific interaction mode, based on the navigation structure of the web application.Researchers have investigated approaches and techniques to automate web testing, dealing with the special features of web applications. This chapter contains a comprehensive overview of the research carried out in the last 10\u00a0years to support web testing with automated tools. We categorize the works available in the literature according to the specific web testing phase\u00a0\u2026", "num_citations": "22\n", "authors": ["311"]}
{"title": "Cluster\u2010based modularization of processes recovered from web applications\n", "abstract": " Web applications are often used to expose business processes implemented as software systems. This paper describes a technique for recovering business processes based on a dynamic analysis of the applications behavior. The technique described here does not require any access to internal software artifacts of the application, such as source code or documentation. An initial process is inferred to by means of the analysis of execution traces, in which the execution of GUI elements such as forms and links is recorded. The recovered process is then abstracted by clustering its elements according to four different criteria: structural, page\u2010based, dependency\u2010based and semantical. A case study has been conducted with the aim of evaluating understandability and readability of the reverse engineered processes as well as the clustering techniques used in refining them. Copyright \u00a9 2010 John Wiley & Sons, Ltd.", "num_citations": "22\n", "authors": ["311"]}
{"title": "Crawlability metrics for automated web testing\n", "abstract": " Web applications are exposed to frequent changes both in requirements and involved technologies. At the same time, there is a continuously growing demand for quality and trust and such a fast evolution and quality constraints claim for mechanisms and techniques for automated testing. Web application automated testing often involves random crawlers to navigate the application under test and automatically explore its structure. However, owing to the specific challenges of the modern Web systems, automatic crawlers may leave large portions of the application unexplored. In this paper, we propose the use of structural metrics to predict whether an automatic crawler with given crawling capabilities will be sufficient or not to achieve high coverage of the application under test. In this work, we define a taxonomy of such capabilities and we determine which combination of them is expected to give the highest\u00a0\u2026", "num_citations": "22\n", "authors": ["311"]}
{"title": "Talking tests: an empirical assessment of the role of fit acceptance tests in clarifying requirements\n", "abstract": " The starting point for software evolution is usually a change request, expressing the new or updated requirements on the delivered system. The requirements specified in a change request document are often incomplete and inconsistent with the initial requirement document, as well as the implementation. Programmers working on the evolution of the software are often in trouble interpreting an under-specified change request document, resulting in code that does not meet the users' expectations and contains faults that can only be detected later through expensive testing activities.", "num_citations": "22\n", "authors": ["311"]}
{"title": "Variable precision reaching definitions analysis for software maintenance\n", "abstract": " A flow analyzer can be very helpful in the process of program understanding, by providing the programmer with different views of the code. As the documentation is often incomplete or inconsistent, it is extremely useful to extract the information a programmer may need directly from the code. Program understanding activities are interactive, thus program analysis tools may be asked for quick answers by the maintainer. Therefore the control on the trade-off between accuracy and efficiency should be given to the user. The paper presents an approach to interprocedural reaching definitions flow analysis based on three levels of precision depending on the sensitivity to the calling context and the control flow. A lower precision degree produces an overestimate of the data dependencies in a program. The result is anyhow conservative (all dependencies which hold are surely reported), and definitely faster than the more\u00a0\u2026", "num_citations": "22\n", "authors": ["311"]}
{"title": "Codebender: Remote software protection using orthogonal replacement\n", "abstract": " CodeBender implements a novel client replacement strategy to counter the malicious host problem and address the problem of guaranteeing client-code security. CodeBender is a tool that implements a novel client-replacement strategy to counter the malicious host problem. It works by limiting the client code's validity and, when the code expires, by having the server provide a new client that replaces the former one. The complexity of analyzing frequently changing, always different (orthogonal) program code deters an adversary's reverse engineering efforts. We've implemented CodeBender and tested its practicability in two case studies.", "num_citations": "21\n", "authors": ["311"]}
{"title": "Art: an architectural reverse engineering environment\n", "abstract": " When programmers perform maintenance tasks, program understanding is often required. One of the first activities in understanding a software system is identifying its subsystems and their relations, i.e., its software architecture. Since a large part of the effort is spent in creating a mental model of the system under study, tools can help maintainers in managing the evolution of legacy systems by showing them architectural information. This paper describes an environment for the architectural recovery of software systems called the architectural recovery tool (ART). The environment is based on a hierarchical architectural model that drives the application of a set of recognizers, each producing a different architectural view of a system or of some of its parts. Recognizers embody knowledge about architectural clich\u00e9s and use flow analysis techniques to make their output more accurate. To test the accuracy and\u00a0\u2026", "num_citations": "21\n", "authors": ["311"]}
{"title": "Diversity-based web test generation\n", "abstract": " Existing web test generators derive test paths from a navigational model of the web application, completed with either manually or randomly generated input values. However, manual test data selection is costly, while random generation often results in infeasible input sequences, which are rejected by the application under test. Random and search-based generation can achieve the desired level of model coverage only after a large number of test execution at-tempts, each slowed down by the need to interact with the browser during test execution. In this work, we present a novel web test generation algorithm that pre-selects the most promising candidate test cases based on their diversity from previously generated tests. As such, only the test cases that explore diverse behaviours of the application are considered for in-browser execution. We have implemented our approach in a tool called DIG. Our empirical\u00a0\u2026", "num_citations": "20\n", "authors": ["311"]}
{"title": "Generating valid grammar-based test inputs by means of genetic programming and annotated grammars\n", "abstract": " Automated generation of system level tests for grammar based systems requires the generation of complex and highly structured inputs, which must typically satisfy some formal grammar. In our previous work, we showed that genetic programming combined with probabilities learned from corpora gives significantly better results over the baseline (random) strategy. In this work, we extend our previous work by introducing grammar annotations as an alternative to learned probabilities, to be used when finding and preparing the corpus required for learning is not affordable. Experimental results carried out on six grammar based systems of varying levels of complexity show that grammar annotations produce a higher number of valid sentences and achieve similar levels of coverage and fault detection as learned probabilities.", "num_citations": "20\n", "authors": ["311"]}
{"title": "Automated identifier completion and replacement\n", "abstract": " Various studies indicate that having concise and consistent identifiers improves the quality of the source code and hence impacts positively source code understanding and maintenance. In order to write concise and consistent identifiers, however, developers need to have some knowledge about the concepts captured in the source code and how they are named. Acquiring such knowledge from the source code might be feasible only for small systems, while it is not viable for large systems. In this paper, we propose an automated approach which exploits concepts and relations automatically extracted from the source code to suggest identifiers. The suggestion is ranked based on the context in which a new identifier is introduced and it can be used either to complete the identifier being written or to replace it with a more appropriate one. To validate the proposed approach, we have conducted a case study by\u00a0\u2026", "num_citations": "20\n", "authors": ["311"]}
{"title": "Revolution: Automatic evolution of mined specifications\n", "abstract": " Specifications mined from execution traces are largely used to support testing and analysis of software applications with little runtime variability. However, when models are mined from applications that evolve at runtime, the resulting models become quickly obsolete, and thus of little support for any testing and analysis activity. To cope with such systems, mined specifications must be consistently updated every time the software changes. In principle, models can be periodically mined from scratch, but in many cases this solution is too expensive or even impossible. In this paper we describe Revolution, an approach for the automatic evolution of specifications mined by applying state abstraction techniques. Revolution produces models that are continuously updated and thus remain aligned with the actual implementation. Empirical results show that Revolution can suitably address run-time evolving applications.", "num_citations": "20\n", "authors": ["311"]}
{"title": "Towards testing future web applications\n", "abstract": " The current Web applications are in continuous evolution to provide new and more complex functionalities, which can improve the user experience by means of adaptivity and dynamic changes. Since testing is the most frequently used technique to evaluate the quality of software applications in industry, novel testing approaches will be necessary to evaluate the quality of future (and more complex) web applications. In this paper, we investigate the testing challenges of future web applications and propose a testing methodology that addresses these challenges by the integration of search-based testing, model-based testing, oracle learning, concurrency testing, combinatorial testing, regression testing, and coverage analysis. This paper also presents a testing metamodel that states testing concepts and their relationships, which are used as the theoretical basis of the proposed testing methodology.", "num_citations": "20\n", "authors": ["311"]}
{"title": "Goto elimination strategies in the migration of legacy code to Java\n", "abstract": " Legacy systems are often large and difficult to maintain, but rewriting them from scratch is usually not a viable option. Reenginering remains the only way to modernize them. We have been recently involved in a migration project aiming at porting an old, large (8 MLOC) legacy banking system to a modern architecture. The goal of the project is: (I) moving from an old, proprietary language to Java; (2) replacing ISAM indexed files with a relational database; (3) upgrading the character oriented interface to a modern GUI. One of the steps in the migration process deals with the elimination of unstructured code (unconditional jumps such as GOTO statements). In this paper we present four alternative strategies for GOTO elimination that we evaluated in the project. Each has pros and cons, but when used in a real case, it turned out that one produced completely unreadable code, hence it was discarded. The final choice\u00a0\u2026", "num_citations": "20\n", "authors": ["311"]}
{"title": "Designing and conducting an empirical study on test management automation\n", "abstract": " Test management aims at organizing, documenting and executing test cases, and at generating execution reports. The adoption of a support tool and of a standard process for such activities is expected to improve the current practice. ITALO is a European project devoted to the evaluation of the benefits coming from test management automation. In this paper the experiences collected and the lessons learned during ITALO are summarized. A formal methodology was adopted for the selection of a support tool among those available from the market. A survey of the current practice in component testing was conducted to adapt the existing process model so as to obtain the greatest benefits from automation. An empirical study was then designed to measure the effects that are expected to be produced by the new test process complemented with the introduction of the support tool. Three pilot projects were\u00a0\u2026", "num_citations": "20\n", "authors": ["311"]}
{"title": "Automated generation of visual web tests from DOM-based web tests\n", "abstract": " Functional test automation is increasingly adopted by web applications developers. In particular, 2nd generation tools overcome the limitations of 1st generation tools, based on screen coordinates, by providing APIs for easy selection and interaction with Document Object Model (DOM) elements. On the other hand, a new, 3rd generation of web testing tools, based on visual image recognition, brings the promise of wider applicability and simplicity. In this paper, we consider the problem of the automated creation of 3rd generation visual web tests from 2nd generation test suites. This transformation affects mostly the way in which test cases locate web page elements to interact with or to assert the expected test case outcome.", "num_citations": "19\n", "authors": ["311"]}
{"title": "An empirical study about the effectiveness of debugging when random test cases are used\n", "abstract": " Automatically generated test cases are usually evaluated in terms of their fault revealing or coverage capability. Beside these two aspects, test cases are also the major source of information for fault localization and fixing. The impact of automatically generated test cases on the debugging activity, compared to the use of manually written test cases, has never been studied before. In this paper we report the results obtained from two controlled experiments with human subjects performing debugging tasks using automatically generated or manually written test cases. We investigate whether the features of the former type of test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on accuracy and efficiency of debugging. The empirical study is aimed at investigating whether, despite the lack of readability in automatically generated test cases\u00a0\u2026", "num_citations": "19\n", "authors": ["311"]}
{"title": "An empirical validation of a web fault taxonomy and its usage for web testing\n", "abstract": " Web testing is assuming an increasingly important role in Web engineering, as a result of the quality demands put onto modern Web-based systems and of the complexity of the involved technologies. Most of the existing works in Web testing are focused on the definition of novel testing techniques, while only limited effort was devoted to understanding the specific nature of Web faults. However, the performance of a new Web testing technique is strictly dependent on the classes of Web faults it addresses. In this paper, we describe the process followed in the construction of aWeb fault taxonomy. We used an iterative, mixed top-down and bottom-up approach. An initial taxonomy was defined by analyzing the high level characteristics of Web applications. Then the taxonomy was subjected to several iterations of empirical validation. During each iteration the taxonomy was refined by analyzing real faults and mapping them onto the appropriate categories. Metrics collected during this process were used to ensure that in the final taxonomy bugs distribute quite evenly among fault categories; fault categories are not-too-big, not-too-small and not ambiguous. Testers can use our taxonomy to define test cases that target specific classes of Web faults, while researchers can use it to build fault seeding tools, to inject artificial Web faults into benchmark applications. The final taxonomy is publicly available for consultation: since it is organized as aWiki page, it is also open to external contributions. We conducted a case study in which test cases have been derived from the taxonomy for a sample Web application. The case study indicates that the proposed\u00a0\u2026", "num_citations": "19\n", "authors": ["311"]}
{"title": "Trading-off security and performance in barrier slicing for remote software entrusting\n", "abstract": " Network applications often require that a trust relationship is established between a trusted host (e.g., the server) and an untrusted host (e.g., the client). The remote entrusting problem is the problem of ensuring the trusted host that whenever a request from an untrusted host is served, the requester is in a genuine state, unaffected by malicious modifications or attacks.               Barrier slicing helps solve the remote entrusting problem. The computation of the sensitive client state is sliced and moved to the server, where it is not possible to tamper with it. However, this solution might involve unacceptable computation and communication costs for the server, especially when the slice to be moved is large. In this paper, we investigate the trade-off between security loss and performance overhead associated with moving only a portion of the barrier slice to the server and we show that this trade-off can be reduced to\u00a0\u2026", "num_citations": "19\n", "authors": ["311"]}
{"title": "Anomaly detection in web applications: A review of already conducted case studies\n", "abstract": " The quality of Web applications is everyday more important. Web applications are crucial vehicles for commerce, information exchange, and a host of social and educational activities. Since a bug in a Web application could interrupt an entire business and cost millions of dollars, there is a strong demand for methodologies, tools and models that can improve the Web quality and reliability. Aim of our ongoing-work has been to investigate, define and apply a variety of analysis and testing techniques able to support the quality of Web applications. Validity of our solutions was assessed by extensive empirical work. A critical review of this five year long work shows that only 40% of the randomly selected real-world Web applications exhibit no anomalies/failures. Some tables reported in this paper summarize the relations between type of anomalies found and analyses applied. We are in need of better methodologies\u00a0\u2026", "num_citations": "19\n", "authors": ["311"]}
{"title": "Recovering traceability links in multilingual web sites\n", "abstract": " The problem of verifying the consistency between Web site portions devoted to different languages is investigated. The purpose is to support the activity of the site maintainer, who is responsible for the alignment between different site versions. Anomalies that typically occur in such situations include the absence of pages in some languages, differences in the page structure in different languages, missing information and parts not translated. The approach which is proposed for recovering traceability links so as to simplify the update of the site to a consistent state, is based on a mix of structural and textual information extracted from the page. The syntax trees of the pages to be compared drive the page matching process. When structurally corresponding nodes are encountered during the tree visit, their text attributes are considered to see if they are each other's translation.", "num_citations": "19\n", "authors": ["311"]}
{"title": "Extraction of domain concepts from the source code\n", "abstract": " Program understanding involves mapping domain concepts to the code elements that implement them. Such mapping is often implicit and undocumented. However, identifier names contain relevant clues to rediscover the mapping and make it available to programmers.In this paper, we present two approaches that exploit structural and linguistic aspects of the source code to extract ontologies. The extracted ontologies are then compared in terms of the concepts they contain and the support they give to program understanding, specifically concept location. Such ontologies are composed of domain and implementation concepts as they come from the source code. To filter domain concepts, we have applied Information Retrieval (IR) based filtering techniques. We have assessed the resulting ontologies against a reference, manually defined, domain ontology.The experimentation was carried out using six real world\u00a0\u2026", "num_citations": "18\n", "authors": ["311"]}
{"title": "Future internet testing with fittest\n", "abstract": " The complexity of the technologies involved in the Future Internet makes testing extremely challenging and demands for novel approaches and major advancement in the field. The overall aim of the FITTEST project is to address these testing challenges, by developing an integrated environment for automated testing, which can monitor the Future Internet application under test and adapt to the dynamic changes observed. Future Internet applications do not remain fixed after their release, services and components can be dynamically added by customers. Consequently, FITTEST testing will be continuous and post-release such that maintenance and quality assurance can cope with the changes in the intended use of an application after release. The testing environment will integrate, adapt and automate various techniques for continuous Future Internet testing (dynamic model inference, model-based testing, log\u00a0\u2026", "num_citations": "18\n", "authors": ["311"]}
{"title": "An empirical evaluation of mutation operators for deep learning systems\n", "abstract": " Deep Learning (DL) is increasingly adopted to solve complex tasks such as image recognition or autonomous driving. Companies are considering the inclusion of DL components in production systems, but one of their main concerns is how to assess the quality of such systems. Mutation testing is a technique to inject artificial faults into a system, under the assumption that the capability to expose (kilt) such artificial faults translates into the capability to expose also real faults. Researchers have proposed approaches and tools (e.g., Deep-Mutation and MuNN) that make mutation testing applicable to deep learning systems. However, existing definitions of mutation killing, based on accuracy drop, do not take into account the stochastic nature of the training process (accuracy may drop even when re-training the un-mutated system). Moreover, the same mutation operator might be effective or might be trivial/impossible to\u00a0\u2026", "num_citations": "17\n", "authors": ["311"]}
{"title": "Search based path and input data generation for web application testing\n", "abstract": " Test case generation for web applications aims at ensuring full coverage of the navigation structure. Existing approaches resort to crawling and manual/random input generation, with or without a preliminary construction of the navigation model. However, crawlers might be unable to reach some parts of the web application and random input generation might not receive enough guidance to produce the inputs needed to cover a given path. In this paper, we take advantage of the navigation structure implicitly specified by developers when they write the page objects used for web testing and we define a novel set of genetic operators that support the joint generation of test inputs and feasible navigation paths. On a case study, our tool Subweb was able to achieve higher coverage of the navigation model than crawling based approaches, thanks to its intrinsic ability of generating inputs for feasible paths and of\u00a0\u2026", "num_citations": "17\n", "authors": ["311"]}
{"title": "Clustering-aided page object generation for web testing\n", "abstract": " To decouple test code from web page details, web testers adopt the Page Object design pattern. Page objects are facade classes abstracting the internals of web pages (e.g., form fields) into high-level business functions that can be invoked by test cases (e.g., user authentication). However, writing such page objects requires substantial effort, which is paid off only later, during software evolution. In this paper we propose a clustering-based approach for the identification of meaningful abstractions that are automatically turned into Java page objects. Our clustering approach to page object identification has been integrated into our tool for automated page object generation, Apogen. Experimental results indicate that the clustering approach provides clusters of web pages close to those manually produced by a human (with, on average, only three differences per web application). 75\u00a0% of the code generated\u00a0\u2026", "num_citations": "17\n", "authors": ["311"]}
{"title": "Change sensitivity based prioritization for audit testing of webservice compositions\n", "abstract": " Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget\u00a0\u2026", "num_citations": "17\n", "authors": ["311"]}
{"title": "Neural compression: an integrated application to EEG signals\n", "abstract": " In this paper a lossless and lossy Neural Compressor for electroencephalographic (EEG) signals is presented and compared with statistical and spectral techniques. The system is integrated on a board comprising a dedicated Neural Chip and is trained using a novel VLSI-friendly algorithm (RTS). The compression factors obtainable (50%-80%) make the on-line transmission of real EEG signals possible.", "num_citations": "17\n", "authors": ["311"]}
{"title": "Reverse engineering 4.7 million lines of code\n", "abstract": " The ITC\u2010Irst Reverse Engineering group was charged with analyzing a software application of approximately 4.7 million lines of C code. It was an old legacy system, maintained for a long time, on which several successive adaptive and corrective maintenance interventions had led to the degradation of the original structure. The company decided to re\u2010engineer the software instead of replacing it, because the complexity and costs of re\u2010implementing the application from scratch could not be afforded, and the associated risk could not be run. Several problems were encountered during re\u2010engineering, including identifying dependencies and detecting redundant functions that were not used anymore. To accomplish these goals, we adopted a conservative approach. Before performing any kind of analysis on the whole code, we carefully evaluated the expected costs. To this aim, a small but representative sample of\u00a0\u2026", "num_citations": "16\n", "authors": ["311"]}
{"title": "Points-to analysis for program understanding\n", "abstract": " Program understanding activities are more difficult for programs written in languages (such as C) that heavily make use of pointers for data structure manipulation, because the programmer needs to build a mental model of the memory use and of the pointers to its locations. Pointers also pose additional problems to the tools supporting program understanding, since they introduce additional dependences that have to be accounted for. This paper extends the flow insensitive context insensitive points-to analysis (PTA) algorithm proposed by Steensgaard, to cover arbitrary combinations of pointer dereferences, array subscripts and field selections. It exhibits interesting properties, among which scalability resulting from the low complexity and good performances is one. The results of the analysis are valuable by themselves, as their graphical display represents the points-to links between locations. They are also\u00a0\u2026", "num_citations": "16\n", "authors": ["311"]}
{"title": "Combining stochastic grammars and genetic programming for coverage testing at the system level\n", "abstract": " When tested at the system level, many programs require complex and highly structured inputs, which must typically satisfy some formal grammar. Existing techniques for grammar based testing make use of stochastic grammars that randomly derive test sentences from grammar productions, trying at the same time to avoid unbounded recursion. In this paper, we combine stochastic grammars with genetic programming, so as to take advantage of the guidance provided by a coverage oriented fitness function during the sentence derivation and evolution process. Experimental results show that the combination of stochastic grammars and genetic programming outperforms stochastic grammars alone.", "num_citations": "15\n", "authors": ["311"]}
{"title": "Optimizing the trade-off between complexity and conformance in process reduction\n", "abstract": " While models are recognized to be crucial for business process management, often no model is available at all or available models are not aligned with the actual process implementation. In these contexts, an appealing possibility is recovering the process model from the existing system. Several process recovery techniques have been proposed in the literature. However, the recovered processes are often complex, intricate and thus difficult to understand for business analysts.               In this paper, we propose a process reduction technique based on multi-objective optimization, which at the same time minimizes the process complexity and its non-conformances. This allows us to improve the process model understandability, while preserving its completeness with respect to the core business properties of the domain. We conducted a case study based on a real-life e-commerce system. Results indicate\u00a0\u2026", "num_citations": "15\n", "authors": ["311"]}
{"title": "Timbre clustering by self-organizing neural networks\n", "abstract": " Timbre is that attribute of auditory sensation which allows listeners to rate as different sounds presented in ways altogether similar with respect to intensity, duration, and pitch. The similarity between two sounds can be characterized in physical and mathematical tenus only with difficulty because it is a subjective attribute and it depends on a large number of parameters. 1. M. Grey, in his classic work [1], introduces the concept of\" timbre space\", a means with which he conveyed the vague notion of similarity between timbres into the precise notion of a metric rule in a three-dimensional space. This space was the result of a multidimensional scaling applied onto a large set of subjective similarity ratings obtained in experimental sessions. A physical interpretation of the reasons for such a spatial distribution was also provided. In this work we will try to follow the lines of Grey's experiment, but using a neural network as the means to rate timbre differences and to transform them into metric relations. Neural nets have been used already in this field of research [4]; the aim of our work is to simplify timbre multidimensionality, following the lines of Grey's experiment, and to obtain similarresuIts in terms of clusterization and of timbre space. The tools we use are Kohonen self-organizing neural networks (KNN): they show an ability to correctly classify items outside the training set, and they prove highly insensitive to noise. Another reason for their use comes from neurophysiology: the principles of self-organization Kohonen proposes were derived from a model of the cerebral cortex; it is therefore interesting to compare our results with those obtained by Grey\u00a0\u2026", "num_citations": "15\n", "authors": ["311"]}
{"title": "Automated generation of state abstraction functions using data invariant inference\n", "abstract": " Model based testing relies on the availability of models that can be defined manually or by means of model inference techniques. To generate models that include meaningful state abstractions, model inference requires a set of abstraction functions as input. However, their specification is difficult and involves substantial manual effort. In this paper, we investigate a technique to automatically infer both the abstraction functions necessary to perform state abstraction and the finite state models based on such abstractions. The proposed approach uses a combination of clustering, invariant inference and genetic algorithms to optimize the abstraction functions along three quality attributes that characterize the resulting models: size, determinism and infeasibility of the admitted behaviors. Preliminary results on a small e-commerce application are extremely encouraging because the automatically produced models include\u00a0\u2026", "num_citations": "13\n", "authors": ["311"]}
{"title": "Migrating legacy data structures based on variable overlay to Java\n", "abstract": " Legacy information systems, such as banking systems, are usually organized around their data model. Hence, when these systems are migrated to modern environments, translation of the data model involves the most critical decisions, having strong implications on the rest of the translation. In this paper, we report our experience and describe the approaches adopted in migrating a large banking system (ten million lines of code) to Java, starting from a proprietary data model which gives programmers explicit control of the variable overlay in memory. After presenting the basic translation scheme, we discuss the exceptions that may occur in practice. Then, we consider two heuristic approaches useful to reduce the number of cases where a behavior equivalent to that of unions must be reproduced in Java. Finally, we comment on the experimental results obtained so far. Copyright \u00a9 2009 John Wiley & Sons, Ltd.", "num_citations": "13\n", "authors": ["311"]}
{"title": "Remote software protection by orthogonal client replacement\n", "abstract": " In a typical client-server scenario, a trusted server provides valuable services to a client, which runs remotely on an untrusted platform. Of the many security vulnerabilities that may arise (such as authentication and authorization), guaranteeing the integrity of the client code is one of the most difficult to address. This security vulnerability is an instance of the malicious host problem, where an adversary in control of the client's host environment tries to tamper with the client code.", "num_citations": "13\n", "authors": ["311"]}
{"title": "Impact of function pointers on the call graph\n", "abstract": " Maintenance activities are made more difficult when pointers are heavily used in source code: the programmer needs to build a mental model of memory locations and of the way they are accessed by means of pointers, in order to comprehend the functionalities of the system. Although several points-to analysis algorithms have been proposed in literature to provide information about memory locations referenced by pointers, there are no quantitative evaluations of the impact of pointers on the overall program understanding activities. Program comprehension activities are usually supported by tools, providing suitable views of the source program. One of the most widely used code views is the call graph, a graph representing calls between functions in the given program. Unfortunately, when pointers, and especially function pointers, are heavily used in the code, the extracted call graph is highly inaccurate and thus\u00a0\u2026", "num_citations": "13\n", "authors": ["311"]}
{"title": "Crawlability metrics for web applications\n", "abstract": " Automated web crawlers can be used to explore and exercise portions of a web application under test. However, the possibility to achieve full exploration of a web application through automated crawling is severely limited by the choice of the input values submitted with forms. Depending on the crawler's capabilities, a larger or smaller portion of web application will be automatically explored. In this paper, we introduce web crawl ability metrics to quantify properties of application pages and forms that affect crawl ability. Moreover, we show that our metrics can be used to identify the boundaries between those parts of the application that can be successfully crawled automatically and those parts that will require manual intervention or other crawl ability support. We have validated our crawl ability metrics on real web applications, for which low crawl ability was indeed associated with the existence of pages never\u00a0\u2026", "num_citations": "12\n", "authors": ["311"]}
{"title": "Remote entrusting by run-time software authentication\n", "abstract": " The problem of software integrity is traditionally addressed as the static verification of the code before the execution, often by checking the code signature. However, there are no well-defined solutions to the run-time verification of code integrity when the code is executed remotely, which is refer to as run-time remote entrusting. In this paper we present the research challenges involved in run-time remote entrusting and how we intend to solve this problem. Specifically, we address the problem of ensuring that a given piece of code executes on an remote untrusted machine and that its functionalities have not been tampered with both before execution and during run-time.", "num_citations": "12\n", "authors": ["311"]}
{"title": "Self-organizing Neural Network and Grey's Timbre Space\n", "abstract": " The target of this research is an exploration of timbre multidimensionality, developed with the aid of self-organizing neural networks. Such networks show the interesting capability of extracting the main dimensions in a multidimensional input, and implement a learning algorithm derived from models of clustering in the human brain. The starting point is Grey's experiment, where a three-dimensional timbre space was determined by muldimensional scaling of subjective similarity judgement. Using Kohonen feature maps a timbre space was produced inside a three-dimensional neural network, allowing timbre mapping and clusterization. The input data derive directly from the Grey's music signals, after a preprocessing phase. The clusterization obtained from subjective judgement and by neural networks are compared. The results encourage the use of neural tools for timbre analysis, and suggest future developments in the fields of signal pre-processing and neural net fine tuiling.", "num_citations": "12\n", "authors": ["311"]}
{"title": "Code quality from the programmer\u2019s perspective\n", "abstract": " Quality in general and software quality in particular can be defined in several different ways, but any definition is relative to some person, who represents the target of the delivered quality. Quality is value for some person, rather than absolute value. This is especially true for software quality, where we can identify at least two categories of stakeholders for which quality is a central issue: users and developers. For users, software quality means ease of use, no crash at run-time, compliance with the requirements, correctness, etc. For developers, it means good design, encapsulation of functionalities, proper modularization, meaningful identifier names, documented interfaces, etc.These two views of the software quality are usually called external quality and internal quality of the software. Both are important and usually the assumption is made that the two are strongly related, so that it makes sense to invest in the improvement of both to eventually deliver a high quality product to the final user. For example, a good design may facilitate smooth adaptation of the software to a changed requirement or addition of new functionalities. A bad design may not prevent it, but may result in a system that contains more problems, due to the difficulty of evolving it, eventually delivering a lower quality (eg, more bugs) to the user. The external, user\u2019s, view on the quality is focused on \u201cwhat\u201d the software does, regardless of how it is implemented, but clearly the \u201chow\u201d is strictly connected with the (quality of) the \u201cwhat\u201d, in terms of correct and compliant implementation, ease of evolution and misbehaviors (eg, crashes) exhibited over time. While internal and external quality\u00a0\u2026", "num_citations": "11\n", "authors": ["311"]}
{"title": "Web site evolution\n", "abstract": " Web sites are becoming increasingly important for companies and organizations. Crucial information as well as economic transactions are managed by Web sites, so that their reliability, usability and overall quality are central issues. The advent of new technologies, such as Web services, makes the domain of Web applications a very dynamic one, where the evolution is rapid and unavoidable. Moreover, existing systems are also eligible for migration to the Web, thus requiring specific reengineering processes and methods. This special issue is focused on the recent achievements produced by software engineering research that has been conducted on Web sites. Reverse engineering, restructuring, testing and dynamic analysis are well\u2010established disciplines that have been investigated in the new domain of the Web. Moreover, migration of legacy code to the Web poses technical problems related to domain\u00a0\u2026", "num_citations": "11\n", "authors": ["311"]}
{"title": "Points to analysis for program understanding\n", "abstract": " Real world programs (in languages like C) heavily make use of pointers. Program understanding activities are thus made more difficult, since pointers affect the memory locations that are referenced in a statement, and also the functions called by a statement, when function pointers are used. The programmer needs to build a mental model of the memory use and of the pointers to its locations, in order to comprehend the functionalities of the system. This paper presents an efficient flow insensitive context insensitive points-to analysis algorithm capable of dealing with the features of the C code. It is extremely promising with regard to scalability, because of the low complexity. The results are valuable by themselves, as their graphical display represents the points to links between locations. They are also integrated with other program understanding techniques like, e.g., call graph construction, slicing, plan recognition\u00a0\u2026", "num_citations": "11\n", "authors": ["311"]}
{"title": "Supporting concept location through identifier parsing and ontology extraction\n", "abstract": " Identifier names play a key role in program understanding and in particular in concept location. Programmers can easily \u201cparse\u201d identifiers and understand the intended meaning. This, however, is not trivial for tools that try to exploit the information in the identifiers to support program understanding. To address this problem, we resort to natural language analyzers, which parse tokenized identifier names and provide the syntactic relationships (dependencies) among the terms composing the identifiers. Such relationships are then mapped to semantic relationships.In this study, we have evaluated the use of off-the-shelf and trained natural language analyzers to parse identifier names, extract an ontology and use it to support concept location. In the evaluation, we assessed whether the concepts taken from the ontology can be used to improve the efficiency of queries used in concept location. We have also\u00a0\u2026", "num_citations": "10\n", "authors": ["311"]}
{"title": "Web testware evolution\n", "abstract": " Web applications evolve at a very fast rate, to accommodate new functionalities, presentation styles and interaction modes. The test artefacts developed during web testing must be evolved accordingly. Among the other causes, one critical reason why test cases need maintenance during web evolution is that the locators used to uniquely identify the page elements under test may fail or may behave incorrectly. The robustness of web page locators used in test cases is thus critical to reduce the test maintenance effort. We present an algorithm that generates robust web page locators for the elements under test and we describe the design of an empirical study that we plan to execute to validate such robust locators.", "num_citations": "10\n", "authors": ["311"]}
{"title": "C++ code analysis: an open architecture for the verification of coding rules\n", "abstract": " The analysis of C++ code is the basic building block of the collaboration between ITC-irst and CERN, aimed at improving the quality of the software by exploiting the information that can be automatically gathered from the code. The first objective of the collaboration is the development of a coding rule check tool. Successive steps will include a reverse engineering module and an intelligent refactoring tool. Since all planned applications, and possibly also those not yet considered, share a common analysis bulk, particular attention was devoted to the development of an open architecture for the analysis of C++ code. In this paper the adopted architectural solutions are presented and discussed, highlighting their generality, the possibilities of extension that they offer, and the way details could be encapsulated within packages, so that a clear and sharp interface between the subsystems is defined. The peculiarities of the C++ language are also described, together with the way they were approached and the state of the current implementation.", "num_citations": "10\n", "authors": ["311"]}
{"title": "Modeling maintenance effort by means of dynamic systems\n", "abstract": " The dynamic evolution of ecological systems in which predators and prey compete for survival has been investigated by applying suitable mathematical models. Dynamic systems theory provides a useful way to model interspecies competition and thus the evolution of predators and prey populations. This kind of mathematical framework has been shown to be well suited to describe evolution of economical systems as well, where instead of predators and prey there are consumers and resources. This paper suggests how dynamic systems could be usefully applied to the maintenance context, namely to model the dynamic evolution of the maintenance effort. When maintainers start trying to recognize and correct code defects, while the number of residual defects decreases, the effort spent to find out any new defect has an initial increase, followed by a decline, in a similar way to prey and predator populations. The\u00a0\u2026", "num_citations": "10\n", "authors": ["311"]}
{"title": "Empirical comparison of graphical and annotation-based re-documentation approaches\n", "abstract": " Re-documentation is a complex activity that follows the comprehension of the code. Programmers record the knowledge they have gained in the form of text, views and diagrams that address specific aspects of the system under maintenance. Re-documentation of existing software can be achieved in several ways. The authors focus on two commonly used approaches: either using a drawing editor or annotating the source code. In the first case, diagrams are produced interactively, starting from the reverse engineered information. In the second case, design information is added in the form of code annotations. Diagrams may be produced, if needed, by an annotation-processing tool, which interprets the annotations previously inserted into the code and generates graphical views. The aim of this empirical work is the comparison of these two approaches, in order to understand which is easier to use and which the\u00a0\u2026", "num_citations": "9\n", "authors": ["311"]}
{"title": "Dynamic model extraction and statistical analysis of web applications: Follow-up after 6 years\n", "abstract": " In 2002 we proposed a method to reverse engineer a Web application model. The proposed method deals with dynamic Web applications, consisting of server components, typically interacting with a persistent layer, which build the Web pages displayed on the browser dynamically. Dynamic analysis and page merging heuristics were used for model extraction. The proposed model was successfully adopted in the Web analysis and testing research community. However, the features of future Web applications (involving rich client components and asynchronous communication with the server) challenge its future applicability. In this paper, we analyze the key properties of the 2002 model and identify those modeling decisions that remain valid and can be used to guide the extraction of models for future Web applications.", "num_citations": "9\n", "authors": ["311"]}
{"title": "Using the oa diagram to encapsulate dynamic memory access\n", "abstract": " Good software design is characterized by low coupling between modules and high cohesion inside each module. This is obtained by encapsulating the details about the internal structure of data and exporting only public functions with a clean interface. For programming languages such as C, which offer little support for encapsulation, code analysis tools may help in assessing and improving the access to data structures. In this paper a new representation of the accesses of functions to dynamic locations, called the O-A diagram, is proposed. By isolating meaningful groups of functions working on common dynamic data, such a diagram can be used to evaluate the encapsulation in a program and to drive possible interventions to improve it. Experimental results suggest that the aggregations identified by the O-A diagram are actually cohesive functions operating on a shared data structure. The results are useful in\u00a0\u2026", "num_citations": "9\n", "authors": ["311"]}
{"title": "Towards anomaly detectors that learn continuously\n", "abstract": " In this paper, we first discuss the challenges of adapting an already trained DNN-based anomaly detector with knowledge mined during the execution of the main system. Then, we present a framework for the continual learning of anomaly detectors, which records in-field behavioural data to determine what data are appropriate for adaptation. We evaluated our framework to improve an anomaly detector taken from the literature, in the context of misbehavior prediction for self-driving cars. Our results show that our solution can reduce the false positive rate by a large margin and adapt to nominal behaviour changes while maintaining the original anomaly detection capability.", "num_citations": "8\n", "authors": ["311"]}
{"title": "Meta-heuristic generation of robust XPath locators for web testing\n", "abstract": " Test scripts used for web testing rely on DOM locators, often expressed as XPaths, to identify the active web page elements and the web page data to be used in assertions. When the web application evolves, the major cost incurred for the evolution of the test scripts is due to broken locators, which fail to locate the target element in the new version of the software. We formulate the problem of automatically generating robust XPath locators as a graph exploration problem, for which we provide an optimal, greedy algorithm. Since such an algorithm has exponential time and space complexity, we present also a genetic algorithm.", "num_citations": "8\n", "authors": ["311"]}
{"title": "The FITTEST tool suite for testing future internet applications\n", "abstract": " Future Internet applications are expected to be much more complex and powerful, by exploiting various dynamic capabilities For testing, this is very challenging, as it means that the range of possible behavior to test is much larger, and moreover it may at the run time change quite frequently and significantly with respect to the assumed behavior tested prior to the release of such an application. The traditional way of testing will not be able to keep up with such dynamics. The Future Internet Testing (FITTEST) project (                   http://crest.cs.ucl.ac.uk/fittest/                                    ), a research project funded by the European Commission (grant agreement n. 257574) from 2010 till 2013, was set to explore new testing techniques that will improve our capacity to deal with the challenges of testing Future Internet applications. Such techniques should not be seen as replacement of the traditional testing, but rather as a\u00a0\u2026", "num_citations": "8\n", "authors": ["311"]}
{"title": "Testing of future internet applications running in the cloud\n", "abstract": " The cloud will be populated by software applications that consist of advanced, dynamic, and largely autonomic interactions among services, end-user applications, content, and media. The complexity of the technologies involved in the cloud makes testing extremely challenging and demands novel approaches and major advancements in the field. This chapter describes the main challenges associated with the testing of applications running in the cloud. The authors present a research agenda that has been defined in order to address the testing challenges. The goal of the agenda is to investigate the technologies for the development of an automated testing environment, which can monitor the applications under test and can react dynamically to the observed changes. Realization of this environment involves substantial research in areas such as search based testing, model inference, oracle learning, and anomaly\u00a0\u2026", "num_citations": "8\n", "authors": ["311"]}
{"title": "Crosscutting concern mining in business processes\n", "abstract": " One of the most complex tasks for business analysts is the consistent management of crosscutting concerns in large business processes. By crosscutting concerns the authors mean those process features that are not assigned to a single modular unit in the process and are thus scattered and tangled with other features. For example, the adaptation of the process to the user preferences involves several scattered activities that constitute a crosscutting concern. In this study, the authors propose a semi-automatic approach, based on formal concept analysis, for the identification of crosscutting concerns in business processes at design time. A preliminary evaluation of the effectiveness of the proposed technique in detecting crosscutting concerns has been provided by applying the approach to a case study.", "num_citations": "8\n", "authors": ["311"]}
{"title": "Under and over approximation of state models recovered for ajax applications\n", "abstract": " In contrast to conventional multi-page Web applications, an Ajax application is developed as a single-page application in which content and structure are changed at runtime according to user interactions, asynchronous messages received from the server and the current state of the application. These features make Ajax applications quite hard to understand for programmers. In this paper, we summarize an approach for supporting Ajax comprehension by recovering GUI-based state models of Ajax applications. Furthermore, we present a case study in which the model recovery approach has been assessed in terms of under and over approximation.", "num_citations": "8\n", "authors": ["311"]}
{"title": "Collaborative specification of semantically annotated business processes\n", "abstract": " Semantic annotations are a way to provide a precise meaning to business process elements, which supports reasoning on properties and constraints. The specification and annotation of business processes is a complex activity involving different analysts possibly working on the same business process.               In this paper we present a framework which aims at supporting business analysts in the collaborative specification and annotation of business processes. A shared workspace, theoretically grounded in a formal representation, allows to collaboratively manipulate processes, ontologies as well as constraints, while a dedicated tool enables to hide the complexity of the underlying formal representation to the users.", "num_citations": "8\n", "authors": ["311"]}
{"title": "Measuring the impact of different categories of software evolution\n", "abstract": " Software evolution involves different categories of interventions, having variable impact on the code. Knowledge about the expected impact of an intervention is fundamental for project planning and resource allocation. Moreover, deviations from the expected impact may hint for areas of the system having a poor design. In this paper, we investigate the relationship between evolution categories and impacted code by means of a set of metrics computed over time for a subject system.", "num_citations": "8\n", "authors": ["311"]}
{"title": "Data model reverse engineering in migrating a legacy system to Java\n", "abstract": " Central to any legacy migration project is the translation of the data model. Decisions made here will have strong implications to the rest of the translation. Some legacy languages lack a structured data model, relying instead on explicit programmer control of the overlay of variables. In this paper we present our experience inferring a structured data model in such a language as part of a migration of eight million lines of code to Java. We discuss the common idioms of coding that were observed and give an overview of our solution to this problem.", "num_citations": "8\n", "authors": ["311"]}
{"title": "Program transformation for web application restructuring\n", "abstract": " This chapter aims at providing a presentation of the principles and techniques involved in the (semi-) automatic transformation of Web applications, in several different restructuring contexts. The necessary background knowledge is provided to the reader in the sections about the syntax of the multiple languages involved in Web application development and about the role of restructuring in a highly dynamic and rapidly evolving development environment. Then, specific examples of Web restructuring are described in detail. In the presentation of the transformations required for restructuring, as well as in the description of the grammar for the involved languages, TXL (Cordy, Dean, Malton & Schneider, 2002) and its programming language is adopted as a unifying element. The chapter is organized into the following sections: in the section following the Introduction, the problems associated with the analysis of the\u00a0\u2026", "num_citations": "8\n", "authors": ["311"]}
{"title": "Effects of different flow insensitive points-to analyses on DEF/USE sets\n", "abstract": " Points-to analysis is required as a preliminary step for many code analyses used in program understanding and maintenance. Different flow insensitive points-to analyses have been proposed in literature. They are extremely appealing with respect to the flow sensitive counterparts because of their high efficiency. Their output consisting of a set of points-to pairs which hold for the given program, can be integrated with other analyses, such as reaching definitions or call graph computation. In any case, their role is to resolve dereference chains into the referenced locations. Two variants of flow insensitive points-to analysis are considered. The resulting points-to pairs are used to determine the locations that are defined or used by every program statement. When a definition or a use exploits a pointer to access the defined or used location, the preliminary computed points-to pairs allow one to find the accessed locations\u00a0\u2026", "num_citations": "8\n", "authors": ["311"]}
{"title": "Variable\u2010precision reaching definitions analysis\n", "abstract": " Ascertaining the reaching definitions from the source code can give views of the linkages in that source code. These views can aid source code analyses, such as impact analysis and program slicing, and can assist in the reverse engineering and re\u2010engineering of large legacy systems. Maintainers like to do such activities interactively and value fast responses from program analysis tools. Therefore the control of the trade\u2010off between accuracy and efficiency should be given to the maintainer. Since some real world programs, especially in languages like C, make much use of pointers, and efficient points\u2010to analysis should be integrated within the computation of the data dependencies during the process of ascertaining the reaching definitions. This paper proposes three different approaches to the analysis of the reaching definitions based on different levels of precision, reflecting differences in their sensitivity to the\u00a0\u2026", "num_citations": "8\n", "authors": ["311"]}
{"title": "Quality metrics and oracles for autonomous vehicles testing\n", "abstract": " The race for deploying AI-enabled autonomous vehicles (AVs) on public roads is based on the promise that such self-driving cars will be as safe as or safer than human drivers. Numerous techniques have been proposed to test AVs, which however lack oracle definitions that account for the quality of driving, due to the lack of a commonly used set of metrics.Towards filling this gap, we first performed a systematic analysis of the literature concerning the assessment of the quality of driving of human drivers and extracted 126 metrics. Then, we measured the correlation between such metrics and the human perception of driving quality when AVs are driving. Lastly, we performed a study based on mutation analysis to assess whether the 26 metrics that best capture the quality of AV driving according to the human study can be used as functional oracles. Our results, targeting the Udacity platform, indicate that our\u00a0\u2026", "num_citations": "7\n", "authors": ["311"]}
{"title": "An empirical validation of oracle improvement\n", "abstract": " We propose a human-in-the-loop approach for oracle improvement and analyse whether the proposed oracle improvement process is helping developers to create better oracles. For this, we conducted two human studies with 68 participants overall: an oracle assessment study and an oracle improvement study. Our results show that developers exhibit poor performance (29% accuracy) when manually assessing whether an assertion oracle contains a false positive, a false negative or none of the two. This shows that automated detection of these oracle deficiencies is beneficial for the users. Our tool OASIs (Oracle ASsessment and Improvement) helps developers produce assertions with higher quality. Participants who used OASIs in the improvement study were able to achieve 33% of full and 67% of partial correctness as opposed to participants without the tool who achieved only 21% of full and 43% of partial\u00a0\u2026", "num_citations": "7\n", "authors": ["311"]}
{"title": "Sbfr: A search based approach for reproducing failures of programs with grammar based input\n", "abstract": " Reproducing field failures in-house, a step developers must perform when assigned a bug report, is an arduous task. In most cases, developers must be able to reproduce a reported failure using only a stack trace and/or some informal description of the failure. The problem becomes even harder for the large class of programs whose input is highly structured and strictly specified by a grammar. To address this problem, we present SBFR, a search-based failure-reproduction technique for programs with structured input. SBFR formulates failure reproduction as a search problem. Starting from a reported failure and a limited amount of dynamic information about the failure, SBFR exploits the potential of genetic programming to iteratively find legal inputs that can trigger the failure.", "num_citations": "7\n", "authors": ["311"]}
{"title": "Dynamic aspect mining\n", "abstract": " Legacy systems often contain several crosscutting concerns that could potentially benefit from an aspect-oriented programming implementation. In this paper, we focus on the problem of aspect identification in existing code. The main idea is that use-cases can be defined in order to separate the base logics from the crosscutting concerns to be aspectised. The relationship between the execution traces associated with the use-cases and the executed computational units (class methods) is analysed through concept analysis. The results obtained on some case studies are discussed in the paper.", "num_citations": "7\n", "authors": ["311"]}
{"title": "Cooperative aspect oriented programming for executable business processes\n", "abstract": " AO4BPEL applied Aspect Oriented Programming to executable business processes. Although modularized, AO4BPEL aspects do not have an explicit interface and the implicit one, based on XPath, is often fragile, hence reusing aspects in different processes is quite hard. Cooperative Aspect Oriented Programming aims at making aspects reusable by means of cooperative work between base code and aspects, realized by increasing the explicit awareness of aspects at the expense of pure obliviousness. This work investigates the use of Cooperative Aspect Oriented Programming with BPEL processes.", "num_citations": "7\n", "authors": ["311"]}
{"title": "A comparative study on the re-documentation of existing software: Code annotations vs. drawing editors\n", "abstract": " During software evolution, programmers spend a lot of time and effort in the comprehension of the internal code structure. Such an activity is often required because the available documentation is not aligned with the implementation, if not missing at all. In order to avoid wasting the time devoted to this activity, programmers can record the knowledge they have gained in the form of multiple, structural views that address the specific aspects of the system that they have considered. Re-documentation of existing software through design views can be achieved either using a drawing editor or annotating the source code. In the first case, diagrams are produced interactively, starting from the reverse engineered information. In the second case, diagrams are produced by an annotation processing tool. Most of current reverse engineering tools fall into the first case but they have serious limitations in the information they can\u00a0\u2026", "num_citations": "7\n", "authors": ["311"]}
{"title": "Telephone transmission of 20-channel digital electroencephalogram using lossless data compression\n", "abstract": " Background The use of telecommunications for computer-assisted transmission of neurophysiological signals is a relatively new practice. With the development of digital technology, it is now possible to record electroencephalograms (EEGs) in digital form. Previous reports have demonstrated the possibility of real-time telephone transmission of a limited number of EEG channels.   Objectives To assess the effectiveness of specific data-compression software to improve the transmission of digital 20-channel EEG records over ordinary public telephone lines.   Methods A prototype system was built to transmit digital EEG signals from one computer to another using two 14.4-kbps modems and proprietary lossless data-compression software.   Results Forty compressed digital EEG records of 20 channels each were sent from different locations at variable distances using \"plain old telephone service\" (POTS). The mean\u00a0\u2026", "num_citations": "7\n", "authors": ["311"]}
{"title": "Poster: A measurement framework to quantify software protections\n", "abstract": " Programs often run under strict usage conditions (eg, license restrictions) that could be broken in case of code tampering. Possible attacks include malicious reverse engineering, tampering using static, dynamic and hybrid techniques, on standard devices as well as in labs with additional special purpose hardware equipment. ASPIRE (http://www. aspire-fp7. eu) is a European FP7 research project devoted to the elaboration of novel techniques to mitigate and prevent attacks to code integrity, to code/data confidentiality and to code lifting. This paper presents the ongoing activity to define a set of metrics aimed at quantifying the effect on code of the ASPIRE protections. The metrics have been conceived based on a measurement framework, which prescribes the identification of the relevant code features to consider and of their relationships with attacks and protections.", "num_citations": "6\n", "authors": ["311"]}
{"title": "Testing techniques applied to ajax web applications\n", "abstract": " New technologies for the development of Web applications, such as AJAX, support advanced, asynchronous interactions with the server, going beyond the submit/wait-for-response paradigm. AJAX improves the responsiveness and usability of a Web application but poses new challenges to the scientific community: one of them is testing. In this work, we try to apply existing Web testing techniques (eg, model based testing, code coverage testing, session based testing, etc.) to a small AJAX-based Web application with the purpose of understanding their real effectiveness. In particular, we try to answer the following questions:\u201cIs it possible to apply existing testing techniques to AJAX-based Web applications?\u201d;\u201cAre they adequate to test AJAX applications?\u201d; and,\u201cWhat are the problems and limitations they have with AJAX testing?\u201d. Our preliminary analysis suggests that these techniques, especially those based on white-box approaches, need to be changed or improved to be effectively used with AJAX-based Web applications.", "num_citations": "6\n", "authors": ["311"]}
{"title": "Fail-safe execution of deep learning based systems through uncertainty monitoring\n", "abstract": " Modern software systems rely on Deep Neural Networks (DNN) when processing complex, unstructured inputs, such as images, videos, natural language texts or audio signals. Provided the intractably large size of such input spaces, the intrinsic limitations of learning algorithms and the ambiguity about the expected predictions for some of the inputs, not only there is no guarantee that DNN\u2019s predictions are always correct, but rather developers must safely assume a low, though not negligible, error probability. A fail-safe Deep Learning based System (DLS) is one equipped to handle DNN faults by means of a supervisor, capable of recognizing predictions that should not be trusted and that should activate a healing procedure bringing the DLS to a safe state.In this paper, we propose an approach to use DNN uncertainty estimators to implement such supervisor. We first discuss advantages and disadvantages of\u00a0\u2026", "num_citations": "5\n", "authors": ["311"]}
{"title": "A federated society of bots for smart contract testing\n", "abstract": " Smart contracts are a new type of software that allows its users to perform irreversible transactions on a distributed persistent data storage called the blockchain. The nature of such contracts and the technical details of the blockchain architecture give raise to new kinds of faults, which require specific test behaviours to be exposed. In this paper we present SoCRATES, a generic and extensible framework to test smart contracts running in a blockchain. The key properties of SoCRATES are: (1) it comprises bots that interact with the blockchain according to a set of composable behaviours; (2) it can instantiate a society of bots, which can trigger faults due to multi-user interactions that are impossible to expose with a single bot. Our experimental results show that SoCRATES can expose known faults and detect previously unknown faults in contracts currently published in the Ethereum blockchain. They also show that a\u00a0\u2026", "num_citations": "5\n", "authors": ["311"]}
{"title": "Challenges in audit testing of web services\n", "abstract": " Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit.", "num_citations": "5\n", "authors": ["311"]}
{"title": "Experimental results on the alignment of multilingual web sites\n", "abstract": " Institutions and companies that are based in countries where the main language is not English typically publish Web sites that offer the same information at least in the local language and in English. However, the evolution of these Web sites may be troublesome, if the same pages are replicated for all supported languages. In fact, changes have to be propagated to all translations of a modified page. Algorithms that help ensure the consistency of multilingual Web pages exploit natural language processing (NLP) methods for the comparison of the content in the pages to be aligned. Since such methods are quite expensive from the point of view of the involved linguistic resources as well as of the computation time, a trade off should be considered between the benefits of more advanced techniques and the costs of their implementation. In this paper, an empirical evaluation is conducted to establish the proper NLP\u00a0\u2026", "num_citations": "5\n", "authors": ["311"]}
{"title": "Vector quantization with the reactive tabu search\n", "abstract": " A novel application of the Reactive Tabu Search to Vector Quantization (RTS-VQ) is presented. The results obtained on benchmark tasks demonstrate that a performance similar to that of traditional techniques can be obtained even if the code vectors are represented with small integers. The result is of interest for application-specific VLSI circuits.", "num_citations": "5\n", "authors": ["311"]}
{"title": "A family of experiments to assess the impact of page object pattern in web test suite development\n", "abstract": " Automated web testing is an appealing option, especially when continuous testing practices are adopted. However, web test cases are known to be fragile and to break easily when a web application evolves. The Page Object (PO) design pattern addresses such problem by providing a layer of indirection that decouples test cases from the internals of the web page, where web page elements are located and triggered by the web tests. However, PO development could potentially introduce an additional burden to the already strictly constrained testing activities. This paper reports an empirical investigation of costs and benefits due to the introduction of the PO pattern in web test suite development. In particular, we conducted a family of controlled experiments in which test cases were developed with and without the PO pattern. While the benefits of POs did not compensate for the extra development effort they require in\u00a0\u2026", "num_citations": "4\n", "authors": ["311"]}
{"title": "Empirical assessment of the effort needed to attack programs protected with client/server code splitting\n", "abstract": " Context                 Code hardening is meant to fight malicious tampering with sensitive code executed on client hosts. Code splitting is a hardening technique that moves selected chunks of code from client to server. Although widely adopted, the effective benefits of code splitting are not fully understood and thoroughly assessed.                                               Objective                 The objective of this work is to compare non protected code vs. code splitting protected code, considering two levels of the chunk size parameter, in order to assess the effectiveness of the protection - in terms of both attack time and success rate - and to understand the attack strategy and process used to overcome the protection.                                               Method                 We conducted an experiment with master students performing attack tasks on a small application hardened with different levels of protection. Students carried out\u00a0\u2026", "num_citations": "4\n", "authors": ["311"]}
{"title": "Business process concern documentation and evolution\n", "abstract": " Business processes can be very large and can contain many different concerns, scattered across the process and tangled with other concerns. Crosscutting concerns are difficult to find, locate and modify in a consistent way, thus making process maintenance and reuse hard, even for business experts. In this report, we propose a method to support business designers when they need to document existing crosscutting concerns and when they work on their evolution. We have adapted and applied available techniques for crosscutting concern browsing, mining and refactoring to business process models. More precisely, we propose to enrich BPMN process elements with semantic annotations taken from a domain ontology. We support ontology creation or enrichment in the given business domain. We introduce a visual query language, which allows business designers to quantify over processes and reason over the ontology. We use a mechanism for semi-automatic crosscutting concern mining and we support consistent evolution of crosscutting concerns, once these are modularized separately from the principal process flow.", "num_citations": "4\n", "authors": ["311"]}
{"title": "Model centered interoperability for source code analysis\n", "abstract": " Source code analysis would greatly benefit from interoperability. Existing tools reimplement functionalities available elsewhere, because it is not easy to import the results of a given source code processing from another tool. Ideally, if all source analysis modules were able to exchange information with each other, a new tool could be obtained by composition of existing functions (eg, a parsing module, a type inference module, etc.) and adding only the implementation of the new analysis method. While standards are emerging which define exchange formats for reverse engineering tools, it is not clear what source code models should be represented using these formats. It is the authors\u2019 position that the formalization of agreed and well understood models is a necessary precondition to achieve interoperability. Only when different analyses share a same view of the source code, in which the same information items are represented, it becomes possible to make them communicate with each other. This paper investigates the available models in source code analysis, their overlaps, and their dimensions of variability.", "num_citations": "4\n", "authors": ["311"]}
{"title": "Analysis and Testing of Web Applications\n", "abstract": " contents, computational facilities and services. Furthermore, they typically work in a distributed, asynchronous fashion. Correspondingly, the quality of Web applications is a complex, multidimensional attribute. The problem of improving the quality of Web applications involves several aspects, including the extraction of suitable models, testing, restructuring, assessment of multilingual alignment and accessibility. Figure 1: An example of Web application restructuring. After identifying menu candidates by means of concept analysis, the original navigation structure is replaced by a new, frame-based one. Motivation: The current situation in the development of Web applications is somewhat similar to the early development of software systems, when quality was totally dependent on individual skills and lucky choices. In fact, most Web applications have insofar been developed without following a formalized process model. Requirements are not captured and the architecture and detailed design of the system are not considered. Developers quickly move to the implementation phase and deliver the system without testing it. Finally, no documentation is usually produced about the internal organization of the application. While this kind of practice was motivated by the characteristics of the first generation of Web sites, things are now quickly changing and increasing demand exists for", "num_citations": "4\n", "authors": ["311"]}
{"title": "Deephyperion: exploring the feature space of deep learning-based systems through illumination search\n", "abstract": " Deep Learning (DL) has been successfully applied to a wide range of application domains, including safety-critical ones. Several DL testing approaches have been recently proposed in the literature but none of them aims to assess how different interpretable features of the generated inputs affect the system's behaviour.", "num_citations": "3\n", "authors": ["311"]}
{"title": "Uncertainty-wizard: Fast and user-friendly neural network uncertainty quantification\n", "abstract": " Uncertainty and confidence have been shown to be useful metrics in a wide variety of techniques proposed for deep learning testing, including test data selection and system supervision. We present Uncertainty-Wizard, a tool that allows to quantify such uncertainty and confidence in artificial neural networks. It is built on top of the industry-leading TF.KERAS deep learning API and it provides a near-transparent and easy to understand interface. At the same time, it includes major performance optimizations that we benchmarked on two different machines and different configurations.", "num_citations": "3\n", "authors": ["311"]}
{"title": "Deep reinforcement learning for black-box testing of android apps\n", "abstract": " The state space of Android apps is huge and its thorough exploration during testing remains a major challenge. In fact, the best exploration strategy is highly dependent on the features of the app under test. Reinforcement Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and error, guided by positive or negative reward, rather than by explicit supervision. Deep RL is a recent extension of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL suitable for complex exploration spaces such as the one of Android apps. However, state of the art, publicly available tools only support basic, tabular RL. We have developed ARES, a Deep RL approach for black-box testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than the baselines, which include state of the art RL based tools, such as TimeMachine and Q-Testing. We also investigated qualitatively the reasons behind such performance and we have identified the key features of Android apps that make Deep RL particularly effective on them to be the presence of chained and blocking activities.", "num_citations": "3\n", "authors": ["311"]}
{"title": "Dependency-aware web test generation\n", "abstract": " Web crawlers can perform long running in-depth explorations of a web application, achieving high coverage of the navigational structure. However, a crawling trace cannot be easily turned into a minimal test suite that achieves the same coverage. In fact, when the crawling trace is segmented into test cases, two problems arise: (1) test cases are dependent on each other, therefore they may raise errors when executed in isolation, and (2) test cases are redundant, since the same targets are covered multiple times by different test cases. In this paper, we propose DANTE, a novel web test generator that computes the test dependencies associated with the test cases obtained from a crawling session, and uses them to eliminate redundant tests and produce executable test schedules. DANTE can effectively turn a web crawler into a test case generator that produces minimal test suites, composed only of feasible tests\u00a0\u2026", "num_citations": "3\n", "authors": ["311"]}
{"title": "Automated Generation of Visual Web Tests from DOM-based Web Tests\n", "abstract": " Automated Generation of Visual Web Tests from DOM-based Web Tests IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS IRIS Fondazione Bruno Kessler Catalogo Ricerca FBK 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno Automated Generation of Visual Web Tests from DOM-based Web Tests Italiano Italiano Italiano Italiano English English Automated Generation of Visual Web Tests from DOM-based Web Tests / Maurizio Leotta; Andrea Stocco; Filippo Ricca; Paolo Tonella. - (2015). Scheda prodotto non validato Attenzione! I dati visualizzati non sono stati sottoposti a validazione da parte di FBK. Scheda breve Scheda completa Titolo: Automated Generation of Visual Web Tests from DOM-based Web Tests Autori: Tonella, Paolo mostra contributor esterni \u2026", "num_citations": "3\n", "authors": ["311"]}
{"title": "Visual vs. DOM-based web locators: An empirical study\n", "abstract": " Visual vs. DOM-based Web Locators: An Empirical Study IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS IRIS Fondazione Bruno Kessler Catalogo Ricerca FBK 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno Visual vs. DOM-based Web Locators: An Empirical Study Italiano Italiano Italiano Italiano English English Visual vs. DOM-based Web Locators: An Empirical Study / Maurizio Leotta; Diego Clerissi; Filippo Ricca; Paolo Tonella. - (2014). ((Intervento presentato al convegno ICWE 2014 tenutosi a Toulouse, France nel 1-4/7/2014. Scheda prodotto non validato Attenzione! I dati visualizzati non sono stati sottoposti a validazione da parte di FBK. Scheda breve Scheda completa Titolo: Visual vs. DOM-based Web Locators: An Empirical Study Autori: Tonella, Paolo \u2026", "num_citations": "3\n", "authors": ["311"]}
{"title": "Automated inference of classifications and dependencies for combinatorial testing\n", "abstract": " Even for small programs, the input space is huge - often unbounded. Partition testing divides the input space into disjoint equivalence classes and combinatorial testing selects a subset of all possible input class combinations, according to criteria such as pairwise coverage. The down side of this approach is that the partitioning of the input space into equivalence classes (input classification) is done manually. It is expensive and requires deep domain and implementation understanding. In this paper, we propose a novel approach to classify test inputs and their dependencies automatically. Firstly, random (or automatically generated) input vectors are sent to the system under test (SUT). For each input vector, an observed \u201chit vector\u201d is produced by monitoring the execution of the SUT. Secondly, hit vectors are grouped into clusters using machine learning. Each cluster contains similar hit vectors, i.e., similar behaviors\u00a0\u2026", "num_citations": "3\n", "authors": ["311"]}
{"title": "Automated detection of discontinuities in models inferred from execution traces\n", "abstract": " Modern applications (e.g., the so called Future Internet applications) exhibit properties that make them hard to model once for all. In fact, they dynamically adapt to the user's habits, to the context, to the environment, they dynamically discover new services and components to integrate, they modify themselves through reflection, automatically. Model inference techniques are based on the observation of the application behavior (trace collection) and on its generalization into a model. Model inference supports testing, understanding and evolution of the software. However, inferred models may become obsolete at run time, due to the evolution or the self-modifications of the software. We investigate an approach for the automated detection of model discontinuities, based on a trade off between delay of the detection and accuracy, measured in terms of few false negatives.", "num_citations": "3\n", "authors": ["311"]}
{"title": "Recovering structured data types from a legacy data model with overlays\n", "abstract": " Legacy systems are often written in programming languages that support arbitrary variable overlays. When migrating to modern languages, the data model must adhere to strict structuring rules, such as those associated with an object oriented data model, supporting classes, class attributes and inter-class relationships.In this paper, we deal with the problem of automatically transforming a data model which lacks structure and relies on the explicit layout of variables in memory as defined by programmers. We introduce an abstract syntax and a set of abstract rewrite rules to describe the proposed approach in a language neutral formalism. Then, we instantiate the approach for the proprietary programming language that was used to develop a large legacy system we are migrating to Java.", "num_citations": "3\n", "authors": ["311"]}
{"title": "Distributing Trust Verification to Increase Application Performance\n", "abstract": " The remote trust problem aims to address the issue of verifying the execution of a program running on an un-trusted host which communicates regularly with a trusted server. One proposed solution to this problem relies on a centralized scheme using assertions and replication to withhold usable services from a tampered client. We show how to extend such a scheme to a distributed trusted hardware such as tamper-resistant smartcards. We compared the performance and security of the proposed distributed system to the original centralized scheme on a case study. Our results indicate that, compared to a centralized scheme, our distributed trust scheme has dramatically lower network traffic, and smaller memory and computational requirements on the trusted server.", "num_citations": "3\n", "authors": ["311"]}
{"title": "Automatic support for the alignment of multilingual Web sites\n", "abstract": " Multilingual Web sites are expected to provide the same content expressed in various languages, presented according to a common style, with the same interaction facilities. To this extent, most Web developers start from a source language version of the site and produce the multilingual versions by providing translations in all supported languages. Translation pages are usually generated by replicating the HTML structure and the scripting language sections of the original pages and by translating the textual sections into the target languages. This practice exposes the site to several problems during its evolution. Updates may be not properly propagated to all translations, and unwanted divergences can be introduced over time in content, presentation and interaction. In this paper, we propose a prototype toolkit, limited to Western languages, that can help restructuring an existing static Web site, and migrating its\u00a0\u2026", "num_citations": "3\n", "authors": ["311"]}
{"title": "A comparative study on the re-documentation of existing software: Code annotations vs. drawing editors\n", "abstract": " A comparative study on the re-documentation of existing software: Code annotations vs. drawing editors IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS IRIS Fondazione Bruno Kessler Catalogo Ricerca FBK 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno A comparative study on the re-documentation of existing software: Code annotations vs. drawing editors Italiano Italiano Italiano Italiano English English A comparative study on the re-documentation of existing software: Code annotations vs. drawing editors / Marco Torchiano; Filippo Ricca; Paolo Tonella. - (2005), pp. 277-286. ((Intervento presentato al convegno 4th International Symposium on Empirical Software Engineering tenutosi a Noosa Heads, Australia nel da 17/11/2005 a 18/11/2005. Scheda prodotto non validato Attenzione! I -\u2026", "num_citations": "3\n", "authors": ["311"]}
{"title": "DeepCrime: mutation testing of deep learning systems based on real faults\n", "abstract": " Deep Learning (DL) solutions are increasingly adopted, but how to test them remains a major open research problem. Existing and new testing techniques have been proposed for and adapted to DL systems, including mutation testing. However, no approach has investigated the possibility to simulate the effects of real DL faults by means of mutation operators. We have defined 35 DL mutation operators relying on 3 empirical studies about real faults in DL systems. We followed a systematic process to extract the mutation operators from the existing fault taxonomies, with a formal phase of conflict resolution in case of disagreement. We have implemented 24 of these DL mutation operators into DeepCrime, the first source-level pre-training mutation tool based on real DL faults. We have assessed our mutation operators to understand their characteristics: whether they produce interesting, ie, killable but not trivial\u00a0\u2026", "num_citations": "2\n", "authors": ["311"]}
{"title": "Sidereal: Statistical adaptive generation of robust locators for web testing\n", "abstract": " By ensuring adequate functional coverage, End\u2010to\u2010End (E2E) testing is a key enabling factor of continuous integration. This is even more true for web applications, where automated E2E testing is the only way to exercise the full stack used to create a modern application. The test code used for web testing usually relies on DOM locators, often expressed as XPath expressions, to identify the web elements and to extract the data checked in assertions. When applications evolve, the most dominant cost for the evolution of test code is due to broken locators, which fail to locate the target element in the novel versions and must be repaired. In this paper, we formulate the robust XPath locator generation problem as a graph exploration problem, instead of relying on ad\u2010hoc heuristics as the one implemented by the state of the art tool robula+. Our approach is based on a statistical adaptive algorithm implemented by the\u00a0\u2026", "num_citations": "2\n", "authors": ["311"]}
{"title": "N-gram based test sequence generation from finite state models\n", "abstract": " Model based testing offers a powerful mechanism to test applications that change dynamically and continuously, for which only some limited black-box knowledge is available (this is typically the case of future internet applications). Models can be inferred from observations of real executions and test cases can be derived from models, according to various strategies (e.g., graph or random visits). The problem is that a relatively large proportion of the test cases obtained in this way might result to be non executable, because they involve infeasible paths.               In this paper, we propose a novel test case derivation strategy, based on the computation of the -gram statistics. Event sequences are generated for which the subsequences of size  respect the distribution of the -tuples observed in the execution traces. In this way, generated and observed sequences share the same context (up to length \u00a0\u2026", "num_citations": "2\n", "authors": ["311"]}
{"title": "Codebender: a tool for remote software protection using orthogonal replacement\n", "abstract": " In a typical client-server scenario, a server provides valuable services to client applications, which run remotely on untrusted client computers. Typical examples are video on demand, on line games, voice-over-ip communications and many others. However, the user on the client side often holds administrative privileges on his/her machine and could tamper with the client application to fulfill the service violating the service usage conditions or service agreements. For example, by tampering with the application, the user may gain personal benefit or unfair treatments when accessing the remote service.Of the many security vulnerabilities that may arise (such as authentication and authorization), guaranteeing the integrity of the client code is one of the most difficult to address. This security vulnerability is an instance of the malicious host problem, where an adversary in control of the client\u2019s host environment tries to tamper with the client code.", "num_citations": "2\n", "authors": ["311"]}
{"title": "Web site understanding and re-structuring with the reweb tool\n", "abstract": " The development of web sites is becoming part of the business of several companies. While an increasing number of implmentation technologies is offered to web developers, no widely accepted methodology is available for their design and few approaches have been proposed to support maintenance and evolution. Since the continuous update of web sites is often a key to success, the interventions for their comprehension and modification need special attention and support.        In this paper, web sites are considered the object of several analyses, focused on their structure and their history, with the purpose of supporting maintenance activities. Structural information may help understanding the organization of the pages in the site, while history analysis provides indications on modifications that do not correspond to the original design or that produce undesirable effects.        A tool was developed to implement the analysis of web site structure and evolution. It will be described with reference to a case study involving a web site understanding activity, followed by a re-structuring intervention. The information extracted by the tool was extremely valuable to analyze the site architecture both before and after its modification", "num_citations": "2\n", "authors": ["311"]}
{"title": "Test management automation: Lessons learned from a process improvement experiment\n", "abstract": " Test management aims at organizing, documenting and executing test cases, and at generating execution reports. The adoption of a support tool for such activities is expected to improve the current practice. ITALO is a European project devoted to the evaluation of the benefits coming from test management automation.             In this paper the experiences collected and the lessons learned during ITALO are summarized. An experiment was designed to measure the effects that are produced by the new test process complemented with the introduction of the support tool. Pilot projects were conducted to measure the benefits obtained from tool usage and process modification.", "num_citations": "2\n", "authors": ["311"]}
{"title": "DeepMetis: Augmenting a Deep Learning Test Set to Increase its Mutation Score\n", "abstract": " Deep Learning (DL) components are routinely integrated into software systems that need to perform complex tasks such as image or natural language processing. The adequacy of the test data used to test such systems can be assessed by their ability to expose artificially injected faults (mutations) that simulate real DL faults. In this paper, we describe an approach to automatically generate new test inputs that can be used to augment the existing test set so that its capability to detect DL mutations increases. Our tool DeepMetis implements a search based input generation strategy. To account for the non-determinism of the training and the mutation processes, our fitness function involves multiple instances of the DL model under test. Experimental results show that \\tool is effective at augmenting the given test set, increasing its capability to detect mutants by 63% on average. A leave-one-out experiment shows that the augmented test set is capable of exposing unseen mutants, which simulate the occurrence of yet undetected faults.", "num_citations": "1\n", "authors": ["311"]}
{"title": "COSMO: Code Coverage Made Easier for Android\n", "abstract": " The degree of code coverage reached by a test suite is an important indicator of the thoroughness of testing. Most coverage tools for Android apps work at the bytecode level and provide no information to developers about which source code lines have not yet been exercised by any test case. In this paper, we present COSMO, the first fully automated Android app instrumenter publicly available that operates at the source code level in a completely transparent way, making it fully compatible with existing system level testing technologies and Android test generators. The experiments that we have conducted on a large benchmark of Android apps show that COSMO can successfully instrument most apps without altering their execution traces, introducing a small, acceptable runtime overhead.", "num_citations": "1\n", "authors": ["311"]}
{"title": "A Review and Refinement of Surprise Adequacy\n", "abstract": " Surprise Adequacy (SA) is one of the emerging and most promising adequacy criteria for Deep Learning (DL) testing. As an adequacy criterion, it has been used to assess the strength of DL test suites. In addition, it has also been used to find inputs to a Deep Neural Network (DNN) which were not sufficiently represented in the training data, or to select samples for DNN retraining. However, computation of the SA metric for a test suite can be prohibitively expensive, as it involves a quadratic number of distance calculations. Hence, we developed and released a performance-optimized, but functionally equivalent, implementation of SA, reducing the evaluation time by up to 97\\%. We also propose refined variants of the SA omputation algorithm, aiming to further increase the evaluation speed. We then performed an empirical study on MNIST, focused on the out-of-distribution detection capabilities of SA, which allowed us to reproduce parts of the results presented when SA was first released. The experiments show that our refined variants are substantially faster than plain SA, while producing comparable outcomes. Our experimental results exposed also an overlooked issue of SA: it can be highly sensitive to the non-determinism associated with the DNN training procedure.", "num_citations": "1\n", "authors": ["311"]}
{"title": "Automatic Page Object Generation with APOGEN\n", "abstract": " Page objects are used in web test automation to decouple the test cases logic from their concrete implementation. Despite the undeniable advantages they bring, as decreasing the maintenance effort of a test suite, yet the burden of their manual development limits their wide adoption. In this demo paper, we give an overview of Apogen, a tool that leverages reverse engineering, clustering and static analysis, to automatically generate Java page objects for web applications.", "num_citations": "1\n", "authors": ["311"]}
{"title": "Weekly round trips from norms to requirements and tests: an industrial experience report\n", "abstract": " SEAC is a major software provider in Italy in the area of business management, with a focus on norms and human resources. SEAC is re-engineering their huge legacy system to C#/SQL Server. To minimise the risks associated with such reengineering project, SEAC has adopted an incremental and agile process model, which produces small and frequent releases of new, incremental modules that replace a portion of the legacy system at a time. Since the SEAC software handles business activities that are highly dependent on norms, such as the contracts of employees, the taxation of incomes and salaries, the pension contributions, one of the key challenges is to support a smooth transformation of norms into requirements, into code and eventually into test cases used to verify that norms have been implemented as prescribed by the law. The SE research unit at FBK has been involved to introduce a set of practices\u00a0\u2026", "num_citations": "1\n", "authors": ["311"]}
{"title": "Why creating web page objects manually if it can be done automatically?\n", "abstract": " Why Creating Web Page Objects Manually If It Can Be Done Automatically? IRIS nascondi/visualizza icone a destra nascondi/visualizza menu in alto Aiuto Sfoglia Scorri i prodotti per: Autore Titolo Riviste Serie Login IRIS IRIS Fondazione Bruno Kessler Catalogo Ricerca FBK 4 Contributo in Atti di Convegno (Proceeding) 4.1 Contributo in Atti di convegno Why Creating Web Page Objects Manually If It Can Be Done Automatically? Italiano Italiano Italiano Italiano English English Why Creating Web Page Objects Manually If It Can Be Done Automatically? / Andrea Stocco; Maurizio Leotta; Filippo Ricca; Paolo Tonella. - (2015). Scheda prodotto non validato Attenzione! I dati visualizzati non sono stati sottoposti a validazione da parte di FBK. Scheda breve Scheda completa Titolo: Why Creating Web Page Objects Manually If It Can Be Done Automatically? Autori: Tonella, Paolo mostra contributor esterni \u2026", "num_citations": "1\n", "authors": ["311"]}
{"title": "A Multi-objective Approach to Business Process Repair\n", "abstract": " Business process model repair aims at updating an existing model so as to accept deviant (e.g., new) behaviours, while remaining as close as possible to the initial model. In this paper, we present a multi-objective approach to process model repair, which maximizes the behaviours accepted by the repaired model while minimizing the cost associated with the repair operations. Given the repair operations for full process repair, we formulate the associated multi-objective problem in terms of a set of pseudo-Boolean constraints. In order to evaluate our approach, we have applied it to a case study from the Public Administration domain. Results indicate that it provides business analysts with a selection of good and tunable alternative solutions.", "num_citations": "1\n", "authors": ["311"]}
{"title": "Search-Based Test Case Generation\n", "abstract": " Hill climbing1. x_opt= x0//random 2. max= f (x_opt) 3. LOOP//with search budget m 4. improved= FALSE 5. FOR x IN Neighbors (x_opt) 6. IF f (x)> max 7. max= f (x) 8. x_opt= x 9. improved= TRUE 10. END IF 11. END FOR 12. IF NOT improved 13. RETURN x_opt 14. END IF 15. END LOOP 16. RETURN x_opt", "num_citations": "1\n", "authors": ["311"]}
{"title": "Static analysis for enforcing intra-thread consistent locks in the migration of a legacy system\n", "abstract": " Often, legacy data management systems provide no native support to transactions. Programmers protect data from concurrent access by adopting commonly agreed patterns, relying on low level concurrency primitives, such as semaphores. In such cases, consistent data access is granted only if all code components are compliant with the adopted mutual exclusion patterns. When migrating legacy systems to modern data management systems, the ad hoc mechanisms for data protection must be replaced with modern constructs for transaction management. In such cases, a literal translation may expose problems and bugs, which were originally masked by the specific implementation and patterns in use. In this paper, we propose a static flow analysis that determines the existence of potentially incompatible locks within the same thread, which require specific code re-engineering before migrating to a modern data\u00a0\u2026", "num_citations": "1\n", "authors": ["311"]}
{"title": "Migrazione di sistemi software legacy\n", "abstract": " Molti sistemi software legacy (tipicamente in ambito bancario) sono difficili e costosi da mantenere ed evolvere a causa delle tecnologie e dei linguaggi di programmazione usati. Spesso tali sistemi derivano da software scritto diverse decadi fa, quando le tecnologie disponibili non erano ancora fortemente orientate alla strutturazione del flusso di controllo e del modello dei dati. La riscrittura di tali sistemi da zero non \u00e8 di solito praticabile, in quanto si tratta di sistemi di grosse dimensioni (anche svariati milioni di linee di codice) che contengono in forma implicita tutte le regole di business relative al dominio su cui operano. Inoltre non \u00e8 di solito possibile fermare la manutenzione di questi sistemi per il tempo necessario a riscrivere il software. L\u2019alternativa che minimizza il rischio e consente un passaggio maggiormente controllato verso tecnologie pi\u00f9 moderne ricade nella categoria del reengineering o della migrazione [9]. I problemi principali che si incontrano in un progetto di migrazione da un linguaggio di programmazione e una tecnologia obsoleti verso una piattaforma moderna hanno a che vedere con il grado di strutturazione delle istruzioni e dei dati. La disponibilit\u00e0 di istruzioni di salto incondizionato (GOTO) consente di scrivere codice estremamente non-strutturato (codice a \u201cspaghetti\u201d), non consentito dai linguaggi di programmazione pi\u00f9 recenti. La possibilit\u00e0 di posizionare le variabili con un overlay arbitrario in memoria consente d\u2019altro lato di definire un modello dati non-strutturato, in cui la definizione (scrittura) di una variabile influenza il contenuto di altre variabili che si sovrappongono ad essa parzialmente o completamente. In\u00a0\u2026", "num_citations": "1\n", "authors": ["311"]}
{"title": "Using program transformations to add structure to a legacy data model\n", "abstract": " An appropriate translation of the data model is central to any language migration effort. Finding a mapping between original and target data models may be challenging for legacy languages (e.g., Assembly) which lack a structured data model and rely instead on explicit programmer control of the overlay of variables. Before legacy applications written in languages with an unstructured data model can be migrated to modern languages, a structured data model must be inferred. This paper describes a set of source transformations used to create such a model as part of a migration of eight million lines of code to Java. The original application is written in a proprietary language supporting variable layout by memory relocation.", "num_citations": "1\n", "authors": ["311"]}
{"title": "Inference of a structured data model in migrating a legacy system to Java\n", "abstract": " Central to any legacy migration project is the translation of the data model. Decisions made here will have strong implications to the rest of the translation. Some legacy languages lack a structured data model, relying instead on explicit programmer control of the overlay of variables. In this paper we present our experience inferring a structured data model in such a language as part of a migration of eight million lines of code to Java. We discuss the common idioms of coding that were observed and give an overview of our solution to this problem.", "num_citations": "1\n", "authors": ["311"]}
{"title": "Tools for anomaly and failure detection in web applications\n", "abstract": " Web applications have become crucial components of our life. However, research studies claim that the experienced quality and reliability of Web applications is often poor or not satisfactory. Evaluation tools are expected to give an important contribution to the improvement of Web applications. Unfortunately, the functionalities offered by available (commercial and research) tools are often limited. Aim of our previous work was to define and implement two tools (ReWeb and TestWeb) able to provide some novel, interesting analyses and testing techniques. In this paper, ReWeb and TestWeb are presented and used to find some anomalies and failures in four case studies. At the end of the paper a list of analysis and testing tools for Web applications is considered and confronted with ReWeb and TestWeb.", "num_citations": "1\n", "authors": ["311"]}
{"title": "Workshop on Empirical Studies in Reverse Engineering\n", "abstract": " The field of reverse engineering, originally tied to the analysis and restructuring of legacy systems, proved to be equally effective in supporting the evolution of modern software systems (e.g., OO code, Web applications, etc.). Correspondingly, a high number of techniques and tools have been developed to address the program comprehension needs of the programmers facing maintenance tasks on any kind of software. So far, the validation of the proposed approaches consisted mainly of proofs of concepts and limited case studies. The aim of this workshop was to assess the role of the empirical studies in the future developments of reverse engineering. Knowledge on the actual effectiveness of the available techniques and tools can be gained only through controlled experimentation. In this workshop, the scope of investigation of such studies was considered, and a (provisional) reference taxonomy of tools and\u00a0\u2026", "num_citations": "1\n", "authors": ["311"]}
{"title": "Package Diagram\n", "abstract": " The complexity involved in the management and description of large software systems can be faced by partitioning the overall collection of the composing entities into smaller, more manageable, units. Packages offer a general grouping mechanism that can be used to decompose a given system into sub-systems and to provide a separate description for each of them. Packages represented in the package diagram show the decomposition of a given system into cohesive units that are loosely coupled with each other. Each package can in turn be decomposed into sub-packages or it can contain the final, atomic entities, typically consisting of the classes and of their mutual relationships.The dependency relationships shown in a package diagram represent the usage of resources available from other packages. For example, if a method of a class contained in a package calls a method of a class that belongs to a\u00a0\u2026", "num_citations": "1\n", "authors": ["311"]}
{"title": "Web application quality: Supporting maintenance and testing\n", "abstract": " The World Wide Web has become an interesting opportunity for companies to deliver services and products at distance. Correspondingly, the quality of Web applications, responsible for the related transactions, has become a crucial factor. It can be improved by properly modeling the application during its design, but if the whole life cycle is considered, the availability of a consistent model of the application is fundamental also during maintenance and testing. In this chapter, the problem of recovering a model of a Web application from the implementation is faced. Algorithms are provided to obtain it even in presence of a highly dynamic structure. Based upon such a model, several static analysis techniques, among which reaching definitions and slicing, are considered, as well as some restructuring techniques. White box testing exploits the model in that the related coverage levels are based on it, while statistical\u00a0\u2026", "num_citations": "1\n", "authors": ["311"]}
{"title": "Cjj: a subset of C++ compliant with Java\n", "abstract": " The C++ programming language offers a wide range of coding alternatives to programmers. Some language characteristics inherited from C are potential sources of problems during the evolution of a system. Global variables, arbitrary branches, pointers, memory management instructions and macro directives are examples of them. The resulting code may be difficult to understand, maintain and test, if such language features are used without proper discipline. The Java language was designed with the explicit goal of excluding all troublesome features that can be found in C++, by disregarding some of them (e.g., globals), and disciplining other ones (e.g., pointers). For several companies the migration to Java is not strategically feasible. Nevertheless, the adoption of its philosophy within C++ code could help improve the quality of the software. In this paper a subset of C++, named Cjj, is defined which is compliant\u00a0\u2026", "num_citations": "1\n", "authors": ["311"]}