{"title": "One evaluation of model-based testing and its automation\n", "abstract": " Model-based testing relies on behavior models for the generation of model traces: input and expected output---test cases---for an implementation. We use the case study of an automotive network controller to assess different test suites in terms of error detection, model coverage, and implementation coverage. Some of these suites were generated automatically with and without models, purely at random, and with dedicated functional test selection criteria. Other suites were derived manually, with and without the model at hand. Both automatically and manually derived model-based test suites detected significantly more requirements errors than hand-crafted test suites that were directly derived from the requirements. The number of detected programming errors did not depend on the use of models. Automatically generated model-based test suites detected as many errors as hand-crafted model-based suites with the\u00a0\u2026", "num_citations": "293\n", "authors": ["176"]}
{"title": "Software product quality control\n", "abstract": " This book has been a much longer process than I would have ever anticipated. The original idea was to integrate and combine the research on software product quality control with my then colleagues Florian Deissenboeck and Elmar Juergens, which we have done in close collaboration with industry to help practitioners in implementing quality control in practice. As life goes on, however, Florian and Elmar decided to start their own company, and, over time, it became clear that they cannot spend enough time on this book project. Hence, in 2011, I bravely decided to write the book on my own.This led to some changes in the content, shifting away from areas I personally have not worked in so much, to other areas I could contribute more personal experience. In addition, I was the project leader for the consortium project Quamoco which had its focus on quality models and quality evaluation. The project and its result\u00a0\u2026", "num_citations": "189\n", "authors": ["176"]}
{"title": "Comparing bug finding tools with reviews and tests\n", "abstract": " Bug finding tools can find defects in software source code using an automated static analysis. This automation may be able to reduce the time spent for other testing and review activities. For this we need to have a clear understanding of how the defects found by bug finding tools relate to the defects found by other techniques. This paper describes a case study using several projects mainly from an industrial environment that were used to analyse the interrelationships. The main finding is that the bug finding tools predominantly find different defects than testing but a subset of defects found by reviews. However, the types that can be detected are analysed more thoroughly. Therefore, a combination is most advisable if the high number of false positives of the tools can be tolerated.", "num_citations": "168\n", "authors": ["176"]}
{"title": "Naming the pain in requirements engineering: A design for a global family of surveys and first results from Germany\n", "abstract": " ContextFor many years, we have observed industry struggling in defining a high quality requirements engineering (RE) and researchers trying to understand industrial expectations and problems. Although we are investigating the discipline with a plethora of empirical studies, they still do not allow for empirical generalisations.ObjectiveTo lay an empirical and externally valid foundation about the state of the practice in RE, we aim at a series of open and reproducible surveys that allow us to steer future research in a problem-driven manner.MethodWe designed a globally distributed family of surveys in joint collaborations with different researchers and completed the first run in Germany. The instrument is based on a theory in the form of a set of hypotheses inferred from our experiences and available studies. We test each hypothesis in our theory and identify further candidates to extend the theory by correlation and\u00a0\u2026", "num_citations": "122\n", "authors": ["176"]}
{"title": "Rapid quality assurance with requirements smells\n", "abstract": " Bad requirements quality can cause expensive consequences during the software development lifecycle, especially if iterations are long and feedback comes late. We aim at a light-weight static requirements analysis approach that allows for rapid checks immediately when requirements are written down. We transfer the concept of code smells to requirements engineering as Requirements Smells. To evaluate the benefits and limitations, we define Requirements Smells, realize our concepts for a smell detection in a prototype called Smella and apply Smella in a series of cases provided by three industrial and a university context. The automatic detection yields an average precision of 59% at an average recall of 82% with high variation. The evaluation in practical environments indicates benefits such as an increase of the awareness of quality defects. Yet, some smells were not clearly distinguishable. Lightweight\u00a0\u2026", "num_citations": "109\n", "authors": ["176"]}
{"title": "What do practitioners vary in using Scrum?\n", "abstract": " Background: Agile software development has become a popular way of developing software. Scrum is the most frequently used agile framework, but it is often reported to be adapted in practice. Objective: Thus, we aim to understand how Scrum is adapted in different contexts and what are the reasons for these changes. Method: Using a structured interview guideline, we interviewed ten German companies about their concrete usage of Scrum and analysed the results qualitatively. Results: All companies vary Scrum in some way. The least variations are in the Sprint length, events, team size and requirements engineering. Many users varied the roles, effort estimations and quality assurance. Conclusions: Many variations constitute a substantial deviation from Scrum as initially proposed. For some of these variations, there are good reasons. Sometimes, however, the variations are a result\u00a0\u2026", "num_citations": "96\n", "authors": ["176"]}
{"title": "Defect classification and defect types revisited\n", "abstract": " There have been various attempts to establish common defect classification systems in standards, academia and industry. The resulted classifications are partly similar but also contain variations and none of which is accepted as a basic tool in software projects. This position paper summarises the work on defect classifications so far, proposes a set of challenges and the direction to a solution.", "num_citations": "91\n", "authors": ["176"]}
{"title": "A Bayesian network approach to assess and predict software quality using activity-based quality models\n", "abstract": " ContextSoftware quality is a complex concept. Therefore, assessing and predicting it is still challenging in practice as well as in research. Activity-based quality models break down this complex concept into concrete definitions, more precisely facts about the system, process, and environment as well as their impact on activities performed on and with the system. However, these models lack an operationalisation that would allow them to be used in assessment and prediction of quality. Bayesian networks have been shown to be a viable means for this task incorporating variables with uncertainty.ObjectiveThe qualitative knowledge contained in activity-based quality models are an abundant basis for building Bayesian networks for quality assessment. This paper describes a four-step approach for deriving systematically a Bayesian network from an assessment goal and a quality model.MethodThe four steps of the\u00a0\u2026", "num_citations": "87\n", "authors": ["176"]}
{"title": "A literature survey of the quality economics of defect-detection techniques\n", "abstract": " Over the last decades, a considerable amount of empirical knowledge about the efficiency of defect-detection techniques has been accumulated. Also a few surveys have summarised those studies with different focuses, usually for a specific type of technique. This work reviews the results of empirical studies and associates them with a model of software quality economics. This allows a better comparison of the different techniques and supports the application of the model in practice as several parameters can be approximated with typical average values. The main contributions are the provision of average values of several interesting quantities wrt defect detection and the identification of areas that need further research because of the limited knowledge available.", "num_citations": "77\n", "authors": ["176"]}
{"title": "From monolith to microservices: A classification of refactoring approaches\n", "abstract": " While the recently emerged Microservices architectural style is widely discussed in literature, it is difficult to find clear guidance on the process of refactoring legacy applications. The importance of the topic is underpinned by high costs and effort of a refactoring process which has several other implications, e.g. overall processes (DevOps) and team structure. Software architects facing this challenge are in need of selecting an appropriate strategy and refactoring technique. One of the most discussed aspects in this context is finding the right service granularity to fully leverage the advantages of a Microservices architecture. This study first discusses the notion of architectural refactoring and subsequently compares 10 existing refactoring approaches recently proposed in academic literature. The approaches are classified by the underlying decomposition technique and visually presented in the form of a\u00a0\u2026", "num_citations": "73\n", "authors": ["176"]}
{"title": "A comprehensive safety engineering approach for software-intensive systems based on STPA\n", "abstract": " Formal verification and testing are complementary approaches which are used in the development process to verify the functional correctness of software. However, the correctness of software cannot ensure the safe operation of safety-critical software systems. The software must be verified against its safety requirements which are identified by safety analysis, to ensure that potential hazardous causes cannot occur. The complexity of software makes defining appropriate software safety requirements with traditional safety analysis techniques difficult. STPA (Systems-Theoretic Processes Analysis) is a unique safety analysis approach that has been developed to identify system hazards, including the software-related hazards. This paper presents a comprehensive safety engineering approach based on STPA, including software testing and model checking approaches for the purpose of developing safe software. The\u00a0\u2026", "num_citations": "72\n", "authors": ["176"]}
{"title": "A model and sensitivity analysis of the quality economics of defect-detection techniques\n", "abstract": " One of the main cost factors in software development is the detection and removal of defects. However, the relationships and influencing factors of the costs and revenues of defect-detection techniques are still not well understood. This paper proposes an analytical, stochastic model of the economics of defect detection and removal to improve this understanding. The model is able to incorporate dynamic as well as static techniques in contrast to most other models of that kind. We especially analyse the model with state-ofthe-art sensitivity analysis methods to (1) identify the most relevant factors for model simplification and (2) prioritise the factors to guide further research and measurements.", "num_citations": "56\n", "authors": ["176"]}
{"title": "Microservices in industry: insights into technologies, characteristics, and software quality\n", "abstract": " Microservices are a topic driven mainly by practitioners and academia is only starting to investigate them. Hence, there is no clear picture of the usage of Microservices in practice. In this paper, we contribute a qualitative study with insights into industry adoption and implementation of Microservices. Contrary to existing quantitative studies, we conducted interviews to gain a more in-depth understanding of the current state of practice. During 17 interviews with software professionals from 10 companies, we analyzed 14 service-based systems. The interviews focused on applied technologies, Microservices characteristics, and the perceived influence on software quality. We found that companies generally rely on well-established technologies for service implementation, communication, and deployment. Most systems, however, did not exhibit a high degree of technological diversity as commonly expected with\u00a0\u2026", "num_citations": "50\n", "authors": ["176"]}
{"title": "Field study on requirements engineering: Investigation of artefacts, project parameters, and execution strategies\n", "abstract": " ContextRequirements Engineering (RE) is a critical discipline mostly driven by uncertainty, since it is influenced by the customer domain or by the development process model used. Volatile project environments restrict the choice of methods and the decision about which artefacts to produce in RE.ObjectiveWe aim to investigate RE processes in successful project environments to discover characteristics and strategies that allow us to elaborate RE tailoring approaches in the future.MethodWe perform a field study on a set of projects at one company. First, we investigate by content analysis which RE artefacts were produced in each project and to what extent they were produced. Second, we perform qualitative analysis of semi-structured interviews to discover project parameters that relate to the produced artefacts. Third, we use cluster analysis to infer artefact patterns and probable RE execution strategies, which are\u00a0\u2026", "num_citations": "50\n", "authors": ["176"]}
{"title": "Automatically measuring the maintainability of service-and microservice-based systems: a literature review\n", "abstract": " In a time of digital transformation, the ability to quickly and efficiently adapt software systems to changed business requirements becomes more important than ever. Measuring the maintainability of software is therefore crucial for the long-term management of such products. With Service-based Systems (SBSs) being a very important form of enterprise software, we present a holistic overview of such metrics specifically designed for this type of system, since traditional metrics-eg object-oriented ones-are not fully applicable in this case. The selected metric candidates from the literature review were mapped to 4 dominant design properties: size, complexity, coupling, and cohesion. Microservice-based Systems (\u03bcSBSs) emerge as an agile and fine-grained variant of SBSs. While the majority of identified metrics are also applicable to this specialization (with some limitations), the large number of services in combination\u00a0\u2026", "num_citations": "46\n", "authors": ["176"]}
{"title": "A systematic approach based on STPA for developing a dependable architecture for fully automated driving vehicles\n", "abstract": " Fully automated driving vehicles represent a major innovation in the automotive industry which will replacedriver tasks by software functions to make traffic more comfortable. Ensuring the operational safety of the fully automated vehicles is a big challenge. The operational safety is affected by different dependability attributes such as availability, reliability, and security. However, demands on fully automated driving vehicles, like a fail operational and nominative performance,are not covered by the current automotive safety standards like ISO 26262. These standardswere notestablished for fully automated driving vehicles. STPA (Systems-Theoretic Processes Analysis) is a safety analysis approach designed for evaluating the safety of complex systems. STPA has not been used, however,to evaluate the complex architecture design of fully automated driving vehicles. For this purpose, we propose a systematic\u00a0\u2026", "num_citations": "46\n", "authors": ["176"]}
{"title": "A case study on the application of an artefact-based requirements engineering approach\n", "abstract": " [Background:] Nowadays, industries are facing the problem that the Requirements Engineering (RE) process is highly volatile, since it depends on project influences from the customer's domain or from process models used. Artefact-based approaches promise to provide guidance in the creation of consistent artefacts in volatile project environments, because these approaches concentrate on the artefacts and their dependencies, instead of prescribing processes. Yet missing, however, is empirical evidence on the advantages of applying artefact-based RE approaches in real projects. [Aim:] We developed a customisable artefact-based RE approach for the domain of business information systems. Our goal is to investigate the advantages and limitations of applying this customisable approach in an industrial context. [Method:] We conduct a case study with our artefact-based RE approach and its customisation\u00a0\u2026", "num_citations": "46\n", "authors": ["176"]}
{"title": "Towards a framework to elicit and manage security and privacy requirements from laws and regulations\n", "abstract": " [Context and motivation] The increasing demand of software systems to process and manage sensitive information has led to the need that software systems should comply with relevant laws and regulations, which enforce the privacy and other aspects of the stored information. [Question/problem] However, the task is challenging because concepts and terminology used for requirements engineering are mostly different to those used in the legal domain and there is a lack of appropriate modelling languages and techniques to support such activities. [Principal ideas/results] The legislation need to be analysed and align with the system requirements. [Contribution] This paper motivates the need to introduce a framework to assist the elicitation and management of security and privacy requirements from relevant legislation and it briefly presents the foundations of such a framework along with an example.", "num_citations": "44\n", "authors": ["176"]}
{"title": "XSTAMPP: an eXtensible STAMP platform as tool support for safety engineering\n", "abstract": " STPA (Systems-Theoretic Processes Analysis) is a new hazard analysis technique based on STAMP. STPA is already being used in different industrial domains (e.g. space, aviation, medical or automotive). To support the application of STPA and make using STPA more efficient, we developed an open tool called A-STPA. However, the current usage of ASTPA by safety analysts in different areas shows a number of shortcomings in terms of documenting unsafe control actions, drawing different levels of control structure diagrams, documenting the causal factors in STPA Step 2 and supporting the application of STPA in different areas. In this paper, we present an extensible STAMP platform called XSTAMPP as tool support designed specifically to serve the widespread adoption and use of STPA in different areas, to facilitate STPA application to different systems and to be easily extended to include different requirements and features. Moreover, XSTAMPP has the potential to be extended in the future to support the application of CAST for accident analysis. We believe that XSTAMPP is a useful first step toward establishing a base platform to support the application of STAMP methodologies in different domains.", "num_citations": "42\n", "authors": ["176"]}
{"title": "Semantics of UML 2.0 interactions with variabilities\n", "abstract": " Means for the representation of variability in UML 2.0 interactions, as presented in a previous work, are further formalised and given a mathematically formal semantics. In this way, UML 2.0 interactions can be used in the conception and development of system families within domain and application engineering tasks. Following the transition from domain to application engineering as a configuration endeavour, resolution of the variability according to a given configuration is captured by a denotational semantics for plain interactions extended to the features for the specification of variability. An example based on a previous case study explicates the semantics hereby defined.", "num_citations": "37\n", "authors": ["176"]}
{"title": "Global sensitivity analysis of predictor models in software engineering\n", "abstract": " Predictor models are an important tool in software projects for quality and cost control as well as management. There are various models available that can help the software engineer in decision-making. However, such models are often difficult to apply in practice because of the amount of data needed. Sensitivity analysis offers provides means to rank the input factors w.r.t. their importance and thereby reduce and optimise the measurement effort necessary. This paper presents an example application of global sensitivity analysis on a software reliability model used in practice. It describes the approach and the possibilities offered.", "num_citations": "36\n", "authors": ["176"]}
{"title": "Cost-Optimisation of Analytical Software Quality Assurance\n", "abstract": " Analytical quality assurance (QA) constitutes a significant part of the development costs of software. Therefore, it is important to optimise the costs and benefits in this area. The main issue is the use and combination of different QA techniques. This dissertation proposes a stochastic model of the economics of analytical QA based on expected values. On the basis of the model, the effort for the combined use of QA can be analysed and optimally planned. A literature review was done for the para...\u00bb", "num_citations": "36\n", "authors": ["176"]}
{"title": "Links between the personalities, styles and performance in computer programming\n", "abstract": " There are repetitive patterns in strategies of manipulating source code. For example, modifying source code before acquiring knowledge of how a code works is a depth-first style and reading and understanding before modifying source code is a breadth-first style. To the extent we know there is no study on the influence of personality on them. The objective of this study is to understand the influence of personality on programming styles. We did a correlational study with 65 programmers at the University of Stuttgart. Academic achievement, programming experience, attitude towards programming and five personality factors were measured via self-assessed survey. The programming styles were asked in the survey or mined from the software repositories. Performance in programming was composed of bug-proneness of programmers which was mined from software repositories, the grades they got in a software\u00a0\u2026", "num_citations": "35\n", "authors": ["176"]}
{"title": "A case study on safety cases in the automotive domain: Modules, patterns, and models\n", "abstract": " Driven by market needs and laws, automotive manufacturers develop ever more feature-rich and complex vehicles. This new functionality plays even an active role in driving, what poses many new challenges on assuring the safety of the vehicle. Safety cases constitute a proven technique to systematically use existing information about a system, its environment, and development context to show its safety. We construct the safety case for a cruise control system describe in a case study in the automotive domain with a special consideration of existing domain-specific models. In the case study, we identify generic safety case modules and several reoccurring patterns, which will simplify the development of future automotive safety cases.", "num_citations": "35\n", "authors": ["176"]}
{"title": "Using STPA in compliance with ISO 26262 for developing a safe architecture for fully automated vehicles\n", "abstract": " Safety has become of paramount importance in the development lifecycle of the modern automobile systems. However, the current automotive safety standard ISO 26262 does not specify clearly the methods for safety analysis. Different methods are recommended for this purpose. FTA (Fault Tree Analysis) and FMEA (Failure Mode and Effects Analysis) are used in the most recent ISO 26262 applications to identify component failures, errors and faults that lead to specific hazards (in the presence of faults). However, these methods are based on reliability theory, and they are not adequate to address new hazards caused by dysfunctional component interactions, software failure or human error. A holistic approach was developed called STPA (Systems-Theoretic Process Analysis) which addresses more types of hazards and treats safety as a dynamic control problem rather than an individual component failure. STPA also addresses types of hazardous causes in the absence of failure. Accordingly, there is a need for investigating hazard analysis techniques like STPA. In this paper, we present a concept on how to use STPA to extend the safety scope of ISO 26262 and support the Hazard Analysis and Risk Assessments (HARA) process. We applied the proposed concept to a current project of a fully automated vehicle at Continental. As a result, we identified 24 system- level accidents, 176 hazards, 27 unsafe control actions, and 129 unsafe scenarios. We conclude that STPA is an effective and efficient approach to derive detailed safety constraints. STPA can support the functional safety engineers to evaluate the architectural design of fully\u00a0\u2026", "num_citations": "34\n", "authors": ["176"]}
{"title": "Experiences with applying STPA to software-intensive systems in the automotive domain\n", "abstract": " Hazard analysis is one of the most important elements in developing safe-critical systems. STPA (Systems-Theoretic Process Analysis) is a modern technique based on the new accident causation model STAMP (System-Theoretic Accident Model and Process) for analyzing hazard and safety issues, which can be applied early in the design process of a system to achieve an acceptable risk level. We have applied STPA to a well-known example of safety-critical systems in the automotive industries: Adaptive Cruise Control (ACC). The results of the application of STPA to our case study and the limitations and difficulties of applying STPA are presented.", "num_citations": "32\n", "authors": ["176"]}
{"title": "Microservices migration in industry: intentions, strategies, and challenges\n", "abstract": " To remain competitive in a fast changing environment, many companies started to migrate their legacy applications towards a Microservices architecture. Such extensive migration processes require careful planning and consideration of implications and challenges likewise. In this regard, hands-on experiences from industry practice are still rare. To fill this gap in scientific literature, we contribute a qualitative study on intentions, strategies, and challenges in the context of migrations to Microservices. We investigated the migration process of 14 systems across different domains and sizes by conducting 16 in-depth interviews with software professionals from 10 companies. Along with a summary of the most important findings, we present a separate discussion of each case. As primary migration drivers, maintainability and scalability were identified. Due to the high complexity of their legacy systems, most companies\u00a0\u2026", "num_citations": "30\n", "authors": ["176"]}
{"title": "Software quality economics for defect-detection techniques using failure prediction\n", "abstract": " Defect-detection techniques, like reviews or tests, are still the prevalent method to assure the quality of software. However, the economics behind those techniques are not fully understood. It is not always obvious when and for how long to use which technique. A cost model for defect-detection techniques is proposed that uses a reliability model and expert opinion for cost estimations and predictions. It is detailed enough to allow fine-grained estimates but also can be used with different defect-detection techniques not only testing. An application of the model is shown using partly data from an industrial project.", "num_citations": "30\n", "authors": ["176"]}
{"title": "Introduction of static quality analysis in small-and medium-sized software enterprises: experiences from technology transfer\n", "abstract": " Today, small- and medium-sized enterprises (SMEs) in the software industry face major challenges. Their resource constraints require high efficiency in development. Furthermore, quality assurance (QA) measures need to be taken to mitigate the risk of additional, expensive effort for bug fixes or compensations. Automated static analysis (ASA) can reduce this risk because it promises low application effort. SMEs seem to take little advantage of this opportunity. Instead, they still mainly rely on the dynamic analysis approach of software testing. In this article, we report on our experiences from a technology transfer project. Our aim was to evaluate the results static analysis can provide for SMEs as well as the problems that occur when introducing and using static analysis in SMEs. We analysed five software projects from five collaborating SMEs using three different ASA techniques: code clone detection, bug\u00a0\u2026", "num_citations": "26\n", "authors": ["176"]}
{"title": "Towards a collaborative repository for the documentation of service-based antipatterns and bad smells\n", "abstract": " While the concepts of object-oriented antipatterns and code smells are prevalent in scientific literature and have been popularized by tools like SonarQube, the research field for service-based antipatterns and bad smells is not as cohesive and organized. The description of these antipatterns is distributed across several publications with no holistic schema or taxonomy. Furthermore, there is currently little synergy between documented antipatterns for the architectural styles SOA and Microservices, even though several antipatterns may hold value for both. We therefore conducted a Systematic Literature Review (SLR) that identified 14 primary studies. 36 service-based antipatterns were extracted from these studies and documented with a holistic data model. We also categorized the antipatterns with a taxonomy and implemented relationships between them. Lastly, we developed a web application for convenient\u00a0\u2026", "num_citations": "24\n", "authors": ["176"]}
{"title": "How a pattern-based privacy system contributes to improve context recognition\n", "abstract": " As Smart Devices have access to a lot of user-preferential data, they come in handy in any situation. Although such data-as well as the knowledge which can be derived from it-is highly beneficial as apps are able to adapt their services appropriate to the respective context, it also poses a privacy threat. Thus, a lot of research work is done regarding privacy. Yet, all approaches obfuscate certain attributes which has a negative impact on context recognition and thus service quality. Therefore, we introduce a novel access control mechanism called PATRON. The basic idea is to control access to information patterns. For instance, a person suffering from diabetes might not want to reveal his or her unhealthy eating habit, which can be derived from the pattern \u201crising blood sugar level\u201d \u201cadding bread units\u201d. Such a pattern which must not be discoverable by some parties (e. g., insurance companies) is called private pattern\u00a0\u2026", "num_citations": "24\n", "authors": ["176"]}
{"title": "How are functionally similar code clones syntactically different? An empirical study and a benchmark\n", "abstract": " Background. Today, redundancy in source code, so-called \u201cclones\u201d caused by copy&paste can be found reliably using clone detection tools. Redundancy can arise also independently, however, not caused by copy&paste. At present, it is not clear how only functionally similar clones (FSC) differ from clones created by copy&paste. Our aim is to understand and categorise the syntactical differences in FSCs that distinguish them from copy&paste clones in a way that helps clone detection research.Methods. We conducted an experiment using known functionally similar programs in Java and C from coding contests. We analysed syntactic similarity with traditional detection tools and explored whether concolic clone detection can go beyond syntax. We ran all tools on 2,800 programs and manually categorised the differences in a random sample of 70 program pairs.Results. We found no FSCs where complete files were syntactically similar. We could detect a syntactic similarity in a part of the files in< 16% of the program pairs. Concolic detection found 1 of the FSCs. The differences between program pairs were in the categories algorithm, data structure, OO design, I/O and libraries. We selected 58 pairs for an openly accessible benchmark representing these categories.Discussion. The majority of differences between functionally similar clones are beyond the capabilities of current clone detection approaches. Yet, our benchmark can help to drive further clone detection research.", "num_citations": "24\n", "authors": ["176"]}
{"title": "Reusing security requirements using an extended quality model\n", "abstract": " A reoccurring problem in software engineering constitutes ensuring sufficient completeness of requirements specifications with economically justifiable efforts. Formulating precise quality requirements and especially security requirements is elaborate as they depend on many stakeholders and technological aspects that are often unclear in early project phases. Threats that may have a severe impact on the software product are sometimes not even known. One approach to tackle this situation is reusing quality requirements, because they are to a high degree similar in different software products. The effect can be higher quality while at the same time saving time and budget.", "num_citations": "24\n", "authors": ["176"]}
{"title": "Towards software quality economics for defect-detection techniques\n", "abstract": " There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. The existing metrics for the effectiveness and efficiency of defect-detection techniques and experiences with them are combined with cost metrics to allow a more fine-grained estimation of costs and a comprehensive evaluation of defect-detection techniques. The current model is most suitable for directly comparing concrete applications of different techniques", "num_citations": "24\n", "authors": ["176"]}
{"title": "Model-based identification of fault-prone components\n", "abstract": " The validation and verification of software is typically a costly part of the development. A possibility to reduce costs is to concentrate these activities on the fault-prone components of the system. A classification approach is proposed that identifies these components based on detailed UML models. For this mainly existing code metrics are tailored to be applicable to models and are combined to a suite. Two industrial case studies confirm the ability of the approach to identify fault-prone components.", "num_citations": "24\n", "authors": ["176"]}
{"title": "Component-based development of dependable systems with UML\n", "abstract": " Dependable systems have to be developed carefully to prevent loss of life and resources due to system failures. Some of their mechanisms (for example, providing fault-tolerance) can be complicated to design and use correctly in the system context and are thus error-prone. This chapter gives an overview of reliability-related analyzes for the design of component-based software systems. This enables the identification of failure-prone components using complexity metrics and the operational profile, and the checking of reliability requirements using stereotypes. We report on the implementation of checks in a tool inside a framework for tool-supported development of reliable systems with UML and two case studies to validate the metrics and checks.", "num_citations": "24\n", "authors": ["176"]}
{"title": "Towards a practical maintainability quality model for service-and microservice-based systems\n", "abstract": " Although current literature mentions a lot of different metrics related to the maintainability of Service-based Systems (SBSs), there is no comprehensive quality model (QM) with automatic evaluation and practical focus. To fill this gap, we propose a Maintainability Model for Services (MM4S), a layered maintainability QM consisting of Service Properties (SPs) related with automatically collectable Service Metrics (SMs). This research artifact created within an ongoing Design Science Research (DSR) project is the first version ready for detailed evaluation and critical feedback. The goal of MM4S is to serve as a simple and practical tool for basic maintainability estimation and control in the context of SBSs and their specialization Microservice-based Systems (\u03bcSBSs).", "num_citations": "22\n", "authors": ["176"]}
{"title": "How to evaluate meta-models for software quality\n", "abstract": " The use of appropriate software quality models is crucial for companies to achieve the product quality required to satisfy customer needs. Most current quality models provide little operationalization and lack adaptation guidelines, which limits their usefulness in practice. It has been proposed to use meta-models to specify an explicit structure in order to ensure that quality models conforming to it can be operationalized and adapted by requiring corresponding model elements and modeling constructs. To be applicable in practice, a meta-model needs to be general enough so that existing quality models can be transferred to the new structure provided by the meta-model while preserving the knowledge they contain. This paper presents an empirical approach for evaluating generality as well as its application to a selected meta-model and six industrial quality models. The results show that (1) the proposed meta-model is general enough to model most contents of the industrial quality models,(2) the generality of a meta-model contributes to its perceived ease of use and usefulness, and (3) the evaluation approach is applicable and reflects the perception of quality model experts well.", "num_citations": "22\n", "authors": ["176"]}
{"title": "Using economics as basis for modelling and evaluating software quality\n", "abstract": " The economics and cost of software quality have been discussed in software engineering for decades now. There is clearly a relationship and a need to manage cost and quality in combination. Moreover, economics should be the basis of any quality analysis. However, this implies several issues that have not been addressed to an extent so that managing the economics of software quality is common practice. This paper discusses these issues, possible solutions, and research directions.", "num_citations": "22\n", "authors": ["176"]}
{"title": "Will my tests tell me if i break this code?\n", "abstract": " Automated tests play an important role in software evolution because they can rapidly detect faults introduced during changes. In practice, code-coverage metrics are often used as criteria to evaluate the effectiveness of test suites with focus on regression faults. However, code coverage only expresses which portion of a system has been executed by tests, but not how effective the tests actually are in detecting regression faults. Our goal was to evaluate the validity of code coverage as a measure for test effectiveness. To do so, we conducted an empirical study in which we applied an extreme mutation testing approach to analyze the tests of open-source projects written in Java. We assessed the ratio of pseudo-tested methods (those tested in a way such that faults would not be detected) to all covered methods and judged their impact on the software project. The results show that the ratio of pseudo-tested methods is\u00a0\u2026", "num_citations": "21\n", "authors": ["176"]}
{"title": "Analyzing text in software projects\n", "abstract": " Most of the data produced in software projects is of textual nature: source code, specifications, or documentation. The advances in quantitative analysis methods drove a lot of data analytics in software engineering. This has overshadowed to some degree the importance of texts and their qualitative analysis. Such analysis has, however, merits for researchers and practitioners as well.In this chapter, we describe the basics of analyzing text in software projects. We first describe how to manually analyze and code textual data. Next, we give an overview of mixed methods for automatic text analysis, including n-grams and clone detection, as well as more sophisticated natural language processing identifying syntax and contexts of words. Those methods and tools are of critical importance to aid in the challenges associated with today\u2019s huge amounts of textual data.We illustrate the methods introduced via a running\u00a0\u2026", "num_citations": "21\n", "authors": ["176"]}
{"title": "Open Tool Support for System-Theoretic Process Analysis\n", "abstract": " STPA (Systems-Theoretic Process Analysis) is a new hazard analysis technique which has been developed by Leveson based on the concept of systems and control theory. In this paper, we present a A-STPA tool as an appropriate tool which transforms STPA to executable STPA that automates the activities of STPA. The A-STPA tool is being developed to assist safety analysts in performing STPA. Moreover, the developing of such tool based on STPA will help safety analysts to have more various sights about STPA hazard analysis process. The design of the tool is discussed, and the usage of the tool is illustrated.", "num_citations": "21\n", "authors": ["176"]}
{"title": "A software safety verification method based on system-theoretic process analysis\n", "abstract": " Modern safety-critical systems are increasingly reliant on software. Software safety is an important aspect in developing safety-critical systems, and it must be considered in the context of the system level into which the software will be embedded. STPA (System-Theoretic Process Analysis) is a modern safety analysis approach which aims to identify the potential hazardous causes in complex safety-critical systems at the system level. To assure that these hazardous causes of an unsafe software\u2019s behaviour cannot happen, safety verification involves demonstrating whether the software fulfills those safety requirements and will not result in a hazardous state. We propose a method for verifying of software safety requirements which are derived at the system level to provide evidence that the hazardous causes cannot occur (or reduce the associated risk to a low acceptable level). We applied the method to a\u00a0\u2026", "num_citations": "20\n", "authors": ["176"]}
{"title": "A security requirements approach for web systems\n", "abstract": " In order to avoid the high impacts of software vulnerabilities, it is necessary to specify security requirements early in the development on a detailed level. Current approaches for security requirements engineering give insufficient support for refining high-level requirements to a concrete and assessable level. Furthermore, reuse mechanisms for these detailed requirements are missing. This paper proposes a web security model based on experiences with other quality models that is used in a security requirements engineering approach. The model provides (1) a means for refinement and (2) a requirements repository for reuse. The approach is illustrated with an example involving the Tomcat servlet container.", "num_citations": "20\n", "authors": ["176"]}
{"title": "Open science in software engineering\n", "abstract": " Open science describes the movement of making any research artifact available to the public and includes, but is not limited to, open access, open data, and open source. While open science is becoming generally accepted as a norm in other scientific disciplines, in software engineering, we are still struggling in adapting open science to the particularities of our discipline, rendering progress in our scientific community cumbersome. In this chapter, we reflect upon the essentials in open science for software engineering including what open science is, why we should engage in it, and how we should do it. We particularly draw from our experiences made as conference chairs implementing open science initiatives and as researchers actively engaging in open science to critically discuss challenges and pitfalls and to address more advanced topics such as how and under which conditions to share preprints, what\u00a0\u2026", "num_citations": "19\n", "authors": ["176"]}
{"title": "Usability and security effects of code examples on crypto apis\n", "abstract": " Context: Cryptographic APIs are said to be not usable and researchers suggest to add example code to the documentation. Aim: We wanted to create a free platform for cryptographic code examples that improves the usability and security of created applications by non security experts. Method: We created the open-source web platform CryptoExamples and conducted a controlled experiment where 58 students added symmetric encryption to a Java program. We then measured the usability and security. Results: The participants who used the platform were not only significantly more effective (+73 %) but also their code contained significantly less possible security vulnerabilities (-66 %). Conclusions: With CryptoExamples the gap between hard to change API documentation and the need for complete and secure code examples can be closed. Still, the platform needs more code examples.", "num_citations": "19\n", "authors": ["176"]}
{"title": "How Usable are Rust Cryptography APIs?\n", "abstract": " Context: Poor usability of cryptographic APIs is a severe source of vulnerabilities. Aim: We wanted to find out what kind of cryptographic libraries are present in Rust and how usable they are. Method: We explored Rust's cryptographic libraries through a systematic search, conducted an exploratory study on the major libraries and a controlled experiment on two of these libraries with 28 student participants. Results: Only half of the major libraries explicitly focus on usability and misuse resistance, which is reflected in their current APIs. We found that participants were more successful using rust-crypto which we considered less usable than ring before the experiment. Conclusion: We discuss API design insights and make recommendations for the design of crypto libraries in Rust regarding the detail and structure of the documentation, higher-level APIs as wrappers for the existing low-level libraries, and selected, good\u00a0\u2026", "num_citations": "19\n", "authors": ["176"]}
{"title": "An exploratory study on applying a scrum development process for safety-critical systems\n", "abstract": " Background: Agile techniques recently have received attention from the developers of safety-critical systems. However, a lack of empirical knowledge of performing safety assurance techniques, especially safety analysis in a real agile project hampers further steps. Aims: In this article, we aim at (1) understanding and optimizing the S-Scrum development process, a Scrum extension with the integration of a systems theory based safety analysis technique, STPA (System-Theoretic Process Analysis), for safety-critical systems; (2) validating the Optimized S-Scrum development process further. Method: We conducted a two-stage exploratory case study in a student project at the University of Stuttgart, Germany. Results: The results in stage 1 showed that S-Scrum helps to ensure safety of each release but is less agile than the normal Scrum. We explored six challenges on: priority management; communication\u00a0\u2026", "num_citations": "19\n", "authors": ["176"]}
{"title": "On Automatically Collectable Metrics for Software Maintainability Evaluation\n", "abstract": " In our work with industry partners as well as with students in seminars we noticed that many people often stick to \"aged\" metrics when they want to evaluate the maintainability of software. They consider these metrics for this purpose without second thoughts, because the metrics are so present that almost every developer has at least heard the name and has some kind of knowledge about it. This smattering leads to an unreflected usage of theses metrics today without fully understanding them. To find suitable metrics for maintainability evaluation, we did a preliminary study, which indicated that still McCabe, Halstead and some kind of lines of code measurement dominate maintainability evaluation. We will discuss examples that question the ability of those metrics to reliably and automatically evaluate maintainability of software. Instead we will present metrics we consider more suitable to make solid statements\u00a0\u2026", "num_citations": "19\n", "authors": ["176"]}
{"title": "Integrated safety analysis using systems-theoretic process analysis and software model checking\n", "abstract": " Safety-critical systems are becoming increasingly more complex and reliant on software. The increase in complexity and software renders ensuring the safety of such systems increasingly difficult. Formal verification approaches can be used to prove the correctness of software; however, even perfectly correct software could lead to an accident. The difficulty is in defining appropriate safety requirements. STPA (Systems-Theoretic Process Analysis) is a modern safety analysis approach which aims to identify the potential hazardous causes in complex systems. Model checking is an efficient technique to verify software against its requirements. In this paper, we propose an approach that integrates safety analysis and verification activities to demonstrate how a systematic combination between these approaches can help safety and software engineers to derive the software safety requirements and verify them to\u00a0\u2026", "num_citations": "19\n", "authors": ["176"]}
{"title": "Leadership gap in agile teams: how teams and scrum masters mature\n", "abstract": " Motivation: How immature teams can become agile is a question that puzzles practitioners and researchers alike. Scrum is one method that supports agile working. Empirical research on the Scrum Master role remains scarce and reveals contradicting results. While the Scrum Master role is often centred on one person in rather immature teams, the role is expected to be shared among multiple members in mature teams. Objective: Therefore, we aim to understand how the Scrum Master role changes while the team matures.Method: We applied Grounded Theory and conducted qualitative interviews with 53 practitioners of 29 software and non-software project teams from Robert Bosch GmbH.Results: We discovered that Scrum Masters initially play nine leadership roles: Method Champion, Disciplinizer on Equal Terms, Coach, Change Agent, Helicopter, Moderator, Networker, Knowledge Enabler and Protector. They transfer some of those roles to the team while it matures. The Scrum Master provides a leadership gap, which allows team members to take on a leadership role.Conclusion: The Scrum Master role changes while the team matures. Trust and freedom to take over a leadership role in teams are essential enablers. Our results support practitioners in implementing agile teams in established companies.", "num_citations": "18\n", "authors": ["176"]}
{"title": "Analyzing the relevance of SOA patterns for microservice-based systems\n", "abstract": " To bring a pattern-based perspective to the SOA vs. microservices discussion, we qualitatively analyzed a total of 118 SOA patterns from 2 popular catalogs for their (partial) applicability to microservices. Patterns had to hold up to 5 derived microservices principles to be applicable. 74 patterns (63%) were categorized as fully applicable, 30 (25%) as partially applicable, and 14 (12%) as not applicable. Most frequently violated microservices characteristics werde Decentralization and Single System. The findings suggest that microservices and SOA share a large set of architectural principles and solutions in the general space of service-based systems while only having a small set of differences in specific areas.", "num_citations": "18\n", "authors": ["176"]}
{"title": "A controlled experiment for the empirical evaluation of safety analysis techniques for safety-critical software\n", "abstract": " Context: Today's safety critical systems are increasingly reliant on software. Software becomes responsible for most of the critical functions of systems. Many different safety analysis techniques have been developed to identify hazards of systems. FTA and FMEA are most commonly used by safety analysts. Recently, STPA has been proposed with the goal to better cope with complex systems including software. Objective: This research aimed at comparing quantitatively these three safety analysis techniques with regard to their effectiveness, applicability, understandability, ease of use and efficiency in identifying software safety requirements at the system level. Method: We conducted a controlled experiment with 21 master and bachelor students applying these three techniques to three safety-critical systems: train door control, anti-lock braking and traffic collision and avoidance. Results: The results showed that there\u00a0\u2026", "num_citations": "18\n", "authors": ["176"]}
{"title": "Assuring the evolvability of microservices: insights into industry practices and challenges\n", "abstract": " While Microservices promise several beneficial characteristics for sustainable long-term software evolution, little empirical research covers what concrete activities industry applies for the evolvability assurance of Microservices and how technical debt is handled in such systems. Since insights into the current state of practice are very important for researchers, we performed a qualitative interview study to explore applied evolvability assurance processes, the usage of tools, metrics, and patterns, as well as participants' reflections on the topic. In 17 semi-structured interviews, we discussed 14 different Microservice-based systems with software professionals from 10 companies and how the sustainable evolution of these systems was ensured. Interview transcripts were analyzed with a detailed coding system and the constant comparison method. We found that especially systems for external customers relied on central\u00a0\u2026", "num_citations": "17\n", "authors": ["176"]}
{"title": "Missing no interaction\u2014Using STPA for identifying hazardous interactions of automated driving systems\n", "abstract": " The next challenge of the automotive industry is marked by automated or even selfdriving vehicles and shall enhance the safety, efficiency, and comfort of mobility. But to overcome this challenge, the systems within the vehicle need to take over tasks that were formerly under the responsibility of the driver. This leads to an increase of complexity of the automated driving systems. Especially, the interactions of an automated driving system with humans, other automated systems or other participants in the traffic. These interactions need to be well investigated. Under certain circumstances, interactions may lead to unforeseen situations in which the specified behavior of the function causes a hazard. Thus, the functional specification of the automated driving systems must avoid missing or incorrect interactions due to oversight. Analyzing the system specification for such overlooked interactions is still mostly a \u201ccreative\u201d task using eg brainstorming. Hence, new analysing approaches may be required to identify safe system engineering solutions. One of the possible analysis approaches is STPA (System-Theoretic Process Analysis). In this paper, we investigated the application of STPA for the concept of safetyin-use, which aims to identify the hazardous interactions in the absence of system malfunctions. As a result, by using STPA we could address all kinds of interactions and generate different types of requirements, including the safety-in-use requirements. We conclude that STPA is a holistic approach which can be used for addressing different kinds of interactions and generating different types of safety requirements for automated driving systems.", "num_citations": "17\n", "authors": ["176"]}
{"title": "On the relationship of inconsistent software clones and faults: An empirical study\n", "abstract": " Background: Code cloning - copying and reusing pieces of source code -- is a common phenomenon in software development in practice. There have been several empirical studies on the effects of cloning, but there are contradictory results regarding the connection of cloning and faults. Objective: Our aim is to clarify the relationship between code clones and faults. In particular, we focus on inconsistent (or type-3) clones in this work. Method: We conducted a case study with TWT GmbH where we detected the code clones in three Java systems, set them into relation to information from issue tracking and version control and interviewed three key developers. Results: Of the type-3 clones, 17 % contain faults. Developers modified most of the type-3 clones simultaneously and thereby fixed half of the faults in type-3 clones consistently. Type-2 clones with faults all evolved to fixed type-3 clones. Clone length is only\u00a0\u2026", "num_citations": "17\n", "authors": ["176"]}
{"title": "An empirical validation of cognitive complexity as a measure of source code understandability\n", "abstract": " Background: Developers spend a lot of their time on understanding source code. Static code analysis tools can draw attention to code that is difficult for developers to understand. However, most of the findings are based on non-validated metrics, which can lead to confusion and code that is hard to understand not being identified.Aims: In this work, we validate a metric called Cognitive Complexity which was explicitly designed to measure code understandability and which is already widely used due to its integration in well-known static code analysis tools.Method: We conducted a systematic literature search to obtain data sets from studies which measured code understandability. This way we obtained about 24,000 understandability evaluations of 427 code snippets. We calculated the correlations of these measurements with the corresponding metric values and statistically summarized the correlation coefficients\u00a0\u2026", "num_citations": "16\n", "authors": ["176"]}
{"title": "A theory on individual characteristics of successful coding challenge solvers\n", "abstract": " BackgroundAssessing a software engineer\u2019s ability to solve algorithmic programming tasks has been an essential part of technical interviews at some of the most successful technology companies for several years now. We do not know to what extent individual characteristics, such as personality or programming experience, predict the performance in such tasks. Decision makers\u2019 unawareness of possible predictor variables has the potential to bias hiring decisions which can result in expensive false negatives as well as in the unintended exclusion of software engineers with actually desirable characteristics.MethodsWe conducted an exploratory quantitative study with 32 software engineering students to develop an empirical theory on which individual characteristics predict the performance in solving coding challenges. We developed our theory based on an established taxonomy framework by Gregor (2006).ResultsOur findings show that the better coding challenge solvers also have better exam grades and more programming experience. Furthermore, conscientious as well as sad software engineers performed worse in our study. We make the theory available in this paper for empirical testing.DiscussionThe theory raises awareness to the influence of individual characteristics on the outcome of technical interviews. Should the theory find empirical support in future studies, hiring costs could be reduced by selecting appropriate criteria for preselecting candidates for on-site interviews and potential bias in hiring decisions could be reduced by taking suitable measures.", "num_citations": "16\n", "authors": ["176"]}
{"title": "Quality models\n", "abstract": " In this chapter, after discussing existing quality models and putting them into context, I introduce basics of software measures and details of the ISO/IEC 25010 quality model. The main part of this chapter constitutes the quality modelling approach developed in the research project Quamoco, how to maintain such quality models and three detailed examples of quality models.", "num_citations": "16\n", "authors": ["176"]}
{"title": "Combining STPA and BDD for safety analysis and verification in agile development: A controlled experiment\n", "abstract": " Context: Agile development is in widespread use, even in safety-critical domains. Motivation: However, there is a lack of an appropriate safety analysis and verification method in agile development. Objective: In this paper, we investigate the use of Behavior Driven Development (BDD) instead of standard User Acceptance Testing (UAT) for safety verification with System-Theoretic Process Analysis (STPA) for safety analysis in agile development. Method: We evaluate the effect of this combination in a controlled experiment with 44 students in terms of productivity, test thoroughness, fault detection effectiveness and communication effectiveness. Results: The results show that BDD is more effective for safety verification regarding the impact on communication effectiveness than standard UAT, whereas productivity, test thoroughness and fault detection effectiveness show no statistically significant difference in\u00a0\u2026", "num_citations": "15\n", "authors": ["176"]}
{"title": "Trace-based test selection to support continuous integration in the automotive industry\n", "abstract": " System testing in the automotive industry is a very expensive and time-consuming task of growing importance, because embedded systems in the domain are distributed over numerous controllers (ECUs). Modern software development techniques such as continuous integration require regular, repeated and fast testing. To achieve this in the automotive domain, test suites for a specific software change must be tailored.", "num_citations": "15\n", "authors": ["176"]}
{"title": "Limiting technical debt with maintainability assurance: an industry survey on used techniques and differences with service-and microservice-based systems\n", "abstract": " Maintainability assurance techniques are used to control this quality attribute and limit the accumulation of potentially unknown technical debt. Since the industry state of practice and especially the handling of Service-and Microservice-Based Systems in this regard are not well covered in scientific literature, we created a survey to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related treatment of systems based on service-orientation, and c) influences on developer satisfaction wrt maintainability. 60 software professionals responded to our online questionnaire. The results indicate that using explicit and systematic techniques has benefits for maintainability. The more sophisticated the applied methods the more satisfied participants were with the maintainability of their software while no link to a hindrance in productivity could be established. Other important findings were\u00a0\u2026", "num_citations": "14\n", "authors": ["176"]}
{"title": "The scalability-efficiency/maintainability-portability trade-off in simulation software engineering: Examples and a preliminary systematic literature review\n", "abstract": " Large-scale simulations play a central role in science and the industry. Several challenges occur when building simulation software, because simulations require complex software developed in a dynamic construction process. That is why simulation software engineering (SSE) is emerging lately as a research focus. The dichotomous trade-off between scalability and efficiency (SE) on the one hand and maintainability and portability (MP) on the other hand is one of the core challenges. We report on the SE/MP trade-off in the context of an ongoing systematic literature review (SLR). After characterizing the issue of the SE/MP trade-off using two examples from our own research, we (1) review the 33 identified articles that assess the trade-off, (2) summarize the proposed solutions for the tradeoff, and (3) discuss the findings for SSE and future work. Overall, we see evidence for the SE/MP trade-off and first solution\u00a0\u2026", "num_citations": "14\n", "authors": ["176"]}
{"title": "On the Benefit of Automated Static Analysis for Small and Medium-Sized Software Enterprises\n", "abstract": " Today\u2019s small and medium-sized enterprises (SMEs) in the software industry are faced with major challenges. While having to work efficiently using limited resources they have to perform quality assurance on their code to avoid the risk of further effort for bug fixes or compensations. Automated static analysis can reduce this risk because it promises little effort for running an analysis. We report on our experience in analysing five projects from and with SMEs by three different static analysis techniques: code clone detection, bug pattern detection and architecture conformance analysis. We found that the effort that was needed to introduce those techniques was small (mostly below one person-hour), that we can detect diverse defects in production code and that the participating companies perceived the usefulness of the presented techniques as well as our analysis results high enough to include the\u00a0\u2026", "num_citations": "14\n", "authors": ["176"]}
{"title": "A Software Reliability Model Based on a Geometric Sequence of Failure Rates\n", "abstract": " Software reliability models are an important tool in quality management and release planning. There is a large number of different models that often exhibit strengths in different areas. This paper proposes a model that is based on a geometric sequence (or progression) of the failure rates of faults. This property of the failure process was observed in practice at Siemens among others and led to the development of the proposed model. It is described in detail and evaluated using standard criteria. Most importantly, the model performs constantly well over several projects in terms of its predictive validity.", "num_citations": "14\n", "authors": ["176"]}
{"title": "Communication channels in safety analysis: An industrial exploratory case study\n", "abstract": " Context: Safety analysis is a predominant activity in developing safety-critical systems. It is a highly cooperative task among multiple functional departments due to increasingly sophisticated safety-critical systems and close-knit development processes. Communication occurs pervasively.Motivation: Effective communication channels among multiple functional departments influence safety analysis quality as well as a safe product delivery. However, the use of communication channels during safety analysis is sometimes arbitrary and poses challenges.Objective: In this article, we aim to investigate the existing communication channels, their usage frequencies, their purposes and challenges during safety analysis in industry.Method: We conducted a multiple case study by surveying 39 experts and interviewing 21 experts in safety-critical companies including software developers, quality engineers and functional\u00a0\u2026", "num_citations": "13\n", "authors": ["176"]}
{"title": "Model-Based Safety-Cases for Software-Intensive Systems\n", "abstract": " Safety cases become increasingly important for software certification. Models play a crucial role in building and combining information for the safety case. This position paper sketches an ideal model-based safety case with defect hypotheses and failure characterisations. From this, open research issues are derived.", "num_citations": "13\n", "authors": ["176"]}
{"title": "Multi-Dimensional Measures for Test Case Quality\n", "abstract": " Choosing the right test cases is an important task in software development due to high costs of software testing as well as the significance of software failures. Therefore, evaluating the quality of test techniques and test suites may help improving test results. Benchmarking has been successfully applied to various domains such as database performance. However, the difficulty in benchmarking test case quality is to find suitable measures. In this paper, a multi- dimensional measuring of test case quality is proposed. It has been shown that not only the number of detected faults but also other aspects such as development artefacts like source code or usage profiles are important. Consequences of this multi-dimensional measure on creating a test bench- mark are described.", "num_citations": "13\n", "authors": ["176"]}
{"title": "On observability and monitoring of distributed systems\u2013an industry interview study\n", "abstract": " Business success of companies heavily depends on the availability and performance of their client applications. Due to modern development paradigms such as DevOps and microservice architectural styles, applications are decoupled into services with complex interactions and dependencies. Although these paradigms enable individual development cycles with reduced delivery times, they cause several challenges to manage the services in distributed systems. One major challenge is to observe and monitor such distributed systems. This paper provides a qualitative study to understand the challenges and good practices in the field of observability and monitoring of distributed systems. In 28 semi-structured interviews with software professionals we discovered increasing complexity and dynamics in that field. Especially observability becomes an essential prerequisite to ensure stable services and\u00a0\u2026", "num_citations": "12\n", "authors": ["176"]}
{"title": "Using architectural modifiability tactics to examine evolution qualities of service-and microservice-based systems\n", "abstract": " Software evolvability is an important quality attribute, yet one difficult to grasp. A certain base level of it is allegedly provided by Service- and Microservice-Based Systems, but many software professionals lack systematic understanding of the reasons and preconditions for this. We address this issue via the proxy of architectural modifiability tactics. By qualitatively mapping principles and patterns of Service-Oriented Architecture (SOA) and Microservices onto tactics and analyzing the results, we cannot only generate insights into service-oriented evolution qualities, but can also provide a modifiability comparison of the two popular service-based architectural styles. The results suggest that both SOA and Microservices possess several inherent qualities beneficial for software evolution. While both focus strongly on loose coupling and encapsulation, there are also differences in the way they strive for\u00a0\u2026", "num_citations": "12\n", "authors": ["176"]}
{"title": "Exploratory study of the privacy extension for system theoretic process analysis (STPA-Priv) to elicit privacy risks in eHealth\n", "abstract": " Context: System Theoretic Process Analysis for Privacy (STPA-Priv) is a novel privacy risk elicitation method using a top down approach. It has not gotten very much attention but may offer a convenient structured approach and generation of additional artifacts compared to other methods. Aim: The aim of this exploratory study is to find out what benefits the privacy risk elicitation method STPA-Priv has and to explain how the method can be used. Method: Therefore we apply STPA-Priv to a real world health scenario that involves a smart glucose measurement device used by children. Different kinds of data from the smart device including location data should be shared with the parents, physicians, and urban planners. This makes it a sociotechnical system that offers adequate and complex privacy risks to be found. Results: We find out that STPA-Priv is a structured method for privacy analysis and finds complex\u00a0\u2026", "num_citations": "12\n", "authors": ["176"]}
{"title": "Towards the assessment of stress and emotional responses of a salutogenesis-enhanced software tool using psychophysiological measurements\n", "abstract": " Software development is intellectual, based on collaboration, and performed in a highly demanding economic market. As such, it is dominated by time pressure, stress, and emotional trauma. While studies of affect are emerging in software engineering research, stress has yet to find its place in the literature despite that it is highly related to affect. In this paper, we study stress coping with the affect-laden framework of Salutogenesis, which is a validated psychological framework for enhancing mental health through a feeling of coherence. We propose a controlled experiment for testing our hypotheses that a static analysis tool enhanced with the Salutogenesis model will bring 1) a higher number of fixed quality issues, 2) reduced cognitive load, 3) reduction of the overall stress, and 4) positive affect induction effects to developers. The experiment will make use of validated physiological measurements of stress as\u00a0\u2026", "num_citations": "12\n", "authors": ["176"]}
{"title": "What Is the Best Way For Developers to Learn New Software Tools? An Empirical Comparison Between a Text and a Video Tutorial\n", "abstract": " The better developers can learn software tools, the faster they can start using them and the more efficiently they can later work with them. Tutorials are supposed to help here. While in the early days of computing, mostly text tutorials were available, nowadays software developers can choose among a huge number of tutorials for almost any popular software tool. However, only little research was conducted to understand how text tutorials differ from other tutorials, which tutorial types are preferred and, especially, which tutorial types yield the best learning experience in terms of efficiency and effectiveness, especially for programmers. To evaluate these questions, we converted an existing video tutorial for a novel software tool into a content-equivalent text tutorial. We then conducted an experiment in three groups where 42 undergraduate students from a software engineering course were commissioned to operate the software tool after using a tutorial: the first group was provided only with the video tutorial, the second group only with the text tutorial and the third group with both. In this context, the differences in terms of efficiency were almost negligible: We could observe that participants using only the text tutorial completed the tutorial faster than the participants with the video tutorial. However, the participants using only the video tutorial applied the learned content faster, achieving roughly the same bottom line performance. We also found that if both tutorial types are offered, participants prefer video tutorials for learning new content but text tutorials for looking up \"missed\" information. We mainly gathered our data through questionnaires and screen\u00a0\u2026", "num_citations": "12\n", "authors": ["176"]}
{"title": "Toward Integrating a System Theoretic Safety Analysis in an Agile Development Process.\n", "abstract": " Agile development methodologies are becoming a tendency in today\u2019s changing software development. However, due to a lack of safety assurance activities, especially safety analysis, agile methods are criticized for being inadequate for the development of safe software. In this paper, we introduce an agile\u201d Safe Scrum\u201d by mapping a novel systematic safety analysis method, called STPA (System-Theoretic Process Analysis) into an existing agile development process\u201d Safe Scrum\u201d for safetycritical systems. This work is done by (1) performing safety-guided design inside each sprint, and (2) replacing the traditional RAMS (Reliability, Availability, Maintenance, and Safety) validation. We aim to extend Safe Scrum by integrating STPA, to find a balance point between Safe Scrum and basic Scrum.", "num_citations": "12\n", "authors": ["176"]}
{"title": "Efficiency analysis of defect-detection techniques\n", "abstract": " Various effectiveness and efficiency metrics have been proposed for defect-detection-techniques and quality assurance. This report aims at introducing and comparing the most common metrics that include the effort for the techniques. These metrics are based on code coverage and fault count. Furthermore two new metrics are introduced that use the failure intensity as a more reliability-oriented measure. The latter three metrics for determining efficiency are applied in a field study with the Germa...\u00bb", "num_citations": "12\n", "authors": ["176"]}
{"title": "Motivations, classification and model trial of conversational agents for insurance companies\n", "abstract": " Advances in artificial intelligence have renewed interest in conversational agents. So-called chatbots have reached maturity for industrial applications. German insurance companies are interested in improving their customer service and digitizing their business processes. In this work we investigate the potential use of conversational agents in insurance companies by determining which classes of agents are of interest to insurance companies, finding relevant use cases and requirements, and developing a prototype for an exemplary insurance scenario. Based on this approach, we derive key findings for conversational agent implementation in insurance companies.", "num_citations": "11\n", "authors": ["176"]}
{"title": "Communication in open-source projects-end of the e-mail era?\n", "abstract": " Communication is essential in software engineering. Especially in distributed open-source teams, communication needs to be supported by channels including mailing lists, forums, issue trackers, and chat systems. Yet, we do not have a clear understanding of which communication channels stakeholders in open-source projects use. In this study, we fill the knowledge gap by investigating a statistically representative sample of 400 GitHub projects. We discover the used communication channels by regular expressions on project data. We show that (1) half of the GitHub projects use observable communication channels;(2) GitHub Issues, e-mail addresses, and the modern chat system Gitter are the most common channels;(3) mailing lists are only in place five and have a lower market share than all modern chat systems combined.", "num_citations": "11\n", "authors": ["176"]}
{"title": "A study of safety documentation in a scrum development process\n", "abstract": " Context: There has been an increasing use of agile techniques for safety-critical systems. Agile techniques embrace fast changing requirements, continuously delivered products and frequent communication with lightweight documentation. Motivation: However, for safety-critical system projects, a lack of safety-related documentation influences the safety-related communication and may reduce the safety assurance's capability. Objective: In this article, we aim to improve the safety-related documentation in a Scrum development process and to support a more efficient safety-related communication. Method: We investigated three types of safety-related documentation patterns in agile development: safety epic, safety story, and agile safety plan. We further adapted and implemented them in a student project at the University of Stuttgart, Germany. We used participant observation, Scrum artifacts, documentation review\u00a0\u2026", "num_citations": "11\n", "authors": ["176"]}
{"title": "Keeping continuous deliveries safe\n", "abstract": " Allowing swift release cycles, Continuous Delivery has become popular in application software development and is starting to be applied in safety-critical domains such as the automotive industry. These domains require thorough analysis regarding safety constraints, which can be achieved by the execution of safety tests resulting from a safety analysis on the product. With continuous delivery in place, such tests need to be executed with every build to ensure the latest software still fulfills all safety requirements. Even more though, the safety analysis has to be updated with every change to ensure the safety test suite is still up-to-date. We thus propose that a safety analysis should be treated no differently from other deliverables such as source-code and dependencies, propose guidelines to adopt this and formulate implications for the development process.", "num_citations": "11\n", "authors": ["176"]}
{"title": "PATRON-Datenschutz in Datenstromverarbeitungssystemen\n", "abstract": " Aufgrund der voranschreitenden Digitalisierung gewinnt das Internet der Dinge (IoT) immer mehr an Bedeutung. Im IoT werden Ger\u00e4te mit Sensoren ausgestattet und miteinander vernetzt. Dadurch werden neuartige Anwendungen erm\u00f6glicht, in denen Sensordaten miteinander kombiniert und in h\u00f6herwertige Informationen umgewandelt werden. Diese Informationen verraten viel \u00fcber den Nutzer und m\u00fcssen daher besonders gesch\u00fctzt werden. H\u00e4ufig hat der Nutzer allerdings keine Kontrolle \u00fcber die Verarbeitung seiner Daten. Auch kann er das Ausma\u00df der daraus ableitbaren Informationen nicht ermessen. Wir stellen daher einen neuartigen Kontrollmechanismus vor, der private Informationen im IoT sch\u00fctzt. Anstelle von abstrakten Datenschutzregeln f\u00fcr einzelne Sensoren definiert derNutzer Muster, die gesch\u00fctztwerden m\u00fcssen. DerNutzer definiert die zu verheimlichenden Informationen nat\u00fcrlichsprachlich, und ein Dom\u00e4nenexperte setzt diese in formale Regeln um. Sind diese Regeln zu restriktiv, so kann die Anwendung ihre angedachte Funktionalit\u00e4t nicht erbringen. Daher muss bez\u00fcglich der Servicequalit\u00e4t ein Kompromiss zwischen gew\u00fcnschter Privatheit und ben\u00f6tigter Funktionalit\u00e4t gefunden werden.", "num_citations": "11\n", "authors": ["176"]}
{"title": "Towards continuous integration and continuous delivery in the automotive industry\n", "abstract": " Development cycles are getting shorter and Continuous Integration and Delivery are being established in the automotive industry. We give an overview of the peculiarities in an automotive deployment pipeline, introduce technologies used and analyze Tesla's deliveries as a state-of-the-art showcase.", "num_citations": "11\n", "authors": ["176"]}
{"title": "Towards applying a safety analysis and verification method based on STPA to agile software development\n", "abstract": " Agile methodologies are becoming widespread in modern software development. However, due to a lack of safety assurance activities, agile methods are criticized for being inadequate for the development of safe software. Safety analysis and safety verication are complementary methods for safety assurance. Yet, both usually rely on traditional, waterfall-like processes. Therefore, it is strongly needed to integrate an appropriate safety analysis approach into agile software development processes driving architecture design and verify the safe design at the code level. This paper presents a novel agile process model \"S-Scrum\" based on the existing development process \"Safe Scrum\"and extended by a safety analysis method and a safety verica- tion approach based on STPA (System-Theoretic Process Analysis). The proposed agile development process S-Scrum can be separated into three parts: (1) performing\u00a0\u2026", "num_citations": "11\n", "authors": ["176"]}
{"title": "Scrum for cyber-physical systems: a process proposal\n", "abstract": " Agile development processes and especially Scrum are chang-ing the state of the practice in software development. Many companies in the classical IT sector have adopted them to successfully tackle various challenges from the rapidly changing environments and increasingly complex software systems. Companies developing software for embedded or cyber-physical systems, however, are still hesitant to adopt such processes. Despite successful applications of Scrum and other agile methods for cyber-physical systems, there is still no complete process that maps their specific challenges to practices in Scrum. We propose to fill this gap by treating all design artefacts in such a development in the same way: In software development, the final design is already the product, in hardware and mechanics it is the starting point of production. We sketch the Scrum extension Scrum CPS by showing how Scrum could be\u00a0\u2026", "num_citations": "11\n", "authors": ["176"]}
{"title": "Are comprehensive quality models necessary for evaluating software quality?\n", "abstract": " The concept of software quality is very complex and has many facets. Reflecting all these facets and at the same time measuring everything related to these facets results in comprehensive but large quality models and extensive measurements. In contrast, there are also many smaller, focused quality models claiming to evaluate quality with few measures.", "num_citations": "11\n", "authors": ["176"]}
{"title": "Is the stack distance between test case and method correlated with test effectiveness?\n", "abstract": " Mutation testing is a means to assess the effectiveness of a test suite and its outcome is considered more meaningful than code coverage metrics. However, despite several optimizations, mutation testing requires a significant computational effort and has not been widely adopted in industry. Therefore, we study in this paper whether test effectiveness can be approximated using a more light-weight approach. We hypothesize that a test case is more likely to detect faults in methods that are close to the test case on the call stack than in methods that the test case accesses indirectly through many other methods. Based on this hypothesis, we propose the minimal stack distance between test case and method as a new test measure, which expresses how close any test case comes to a given method, and study its correlation with test effectiveness. We conducted an empirical study with 21 open-source projects, which\u00a0\u2026", "num_citations": "10\n", "authors": ["176"]}
{"title": "At ease with your warnings: the principles of the salutogenesis model applied to automatic static analysis\n", "abstract": " The results of an automatic static analysis run can be overwhelming, especially for beginners. The overflow of information and the resulting need for many decisions is mentally tiring and can cause stress symptoms. There are several models in health care which are designed to fight stress. One of these is the salutogenesis model created by Aaron Antonovsky. In this paper, we will present an idea on how to transfer this model into a triage and recommendation model for static analysis tools and give an example of how this can be implemented in FindBugs, a static analysis tool for Java.", "num_citations": "10\n", "authors": ["176"]}
{"title": "XSTAMPP 2.0: new improvements to XSTAMPP Including CAST accident analysis and an extended approach to STPA\n", "abstract": " XSTAMPP (eXtensible STAMP Platform ) is a software tool developed to serve the widespread adoption and use of STAMP methodologies in different domains. The first version of XSTAMPP supported only the STPA application. In this paper, we present a new version of XSTAMPP, including CAST accident analysis and extended approach to STPA. We developed two new plug-in tools called (1) A-CAST (Automated CAST) which implements the CAST activities and (2) XSTPA (Extended Approach to STPA) which supports automatically generating the context tables which will be used to refine the safety requirements and automatically transform the refined safety requirements into a formal specification in Linear Temporal Logic (LTL) to support verification activities. XSTAMPP 2.0 is available as an open source platform at our repository http://sourceforge.net/projects/stampp/files/2.0.0/.", "num_citations": "10\n", "authors": ["176"]}
{"title": "The influence of personality on computer programming: a summary of a systematic literature review\n", "abstract": " The objective of this report is to summarize the results of the systematic literature review we recently did on the influence of personality on computer programming (Karimi et al. 2014). In the SLR, we systematically searched online search resources and found 50 empirical and 4 theoretical studies with findings on the relations between personality characteristics and performance in computer programming. 28 empirical studies found an influence of personality on programming. We discussed that the other studies failed to find an influence of personality because of ceiling or bottom effects, small samples or incomprehensive personality test. We further analyzed the studies that found a relation and mapped the investigated personality characteristics of 22 empirical studies (out of 28) and 3 theoretical studies (out of 4) to the five personality factors: Openness, Conscientiousness, Extroversion, Agreeableness and Neuroticism. Due to inaccessible or invalid personality tests, we excluded several studies from this mapping. We found that either in theoretical or empirical studies all personality factors have an effect in at least one study. Except Conscientiousness which always has positive effects, other personality factors may have positive or negative effects. Moreover, all personality factors might have no effect in some cases. We concluded that there is an indication that personality affects programming but this relation is not clear and more studies are needed to clarify the influence of personality on programming.", "num_citations": "10\n", "authors": ["176"]}
{"title": "A novel approach for discovering barriers in using automatic static analysis\n", "abstract": " Context: Static analysis of source code is a promising opportunity to detect faults and badly designed areas, which will have a negative effect on the overall product quality. In addition, it is inexpensive to integrate it into the development process, because it is easy to automate. Objective: Despite these benefits, static analysis is not as commonly accepted as other quality assurance techniques. To be able to realise its full potential, we need to better understand the problems which prevent software developers from successfully using static analysis tools. Method: To gain new insights we propose to combine the observation methodologies eye tracking, think aloud and questionnaires for a comprehensive investigation on the usage barriers. Results: Our first experimental results with students show new issues: Automated static analysis tools could benefit from a more direct feedback to changes and a more clear\u00a0\u2026", "num_citations": "10\n", "authors": ["176"]}
{"title": "Integrating State Machine Analysis with System-Theoretic Process Analysis\n", "abstract": " Safety becomes a critical aspect for software-intensive systems in different applications areas. Many hazard analysis techniques are proposed and used to investigate system design models to elicit hazards and design flaws. STPA (System- Theoretic Process Analysis) is a modern hazard analysis technique, which is based on a new systems-theoretic model of accidents for large and complex systems. With STPA, the system is viewed as interacting control loops and the accidents are considered as results from inadequate enforcement of safety constraints in design, development and operation. STPA still needs appropriate diagrammatic notations to represent the relation between the process model variables, control actions and hazards. For this purpose, we propose to integrate state machine analysis with STPA to provide a suitable notation of arguments between the states of controllers, control actions and hazards.", "num_citations": "10\n", "authors": ["176"]}
{"title": "Naming the Pain in Requirements Engineering-NaPiRE-Report 2013\n", "abstract": " Context: For many years, we have observed industry struggling in defining a high quality requirements engineering (RE) and researchers trying to understand industrial expectations and problems. Although we are investigating the discipline with a plethora of empirical studies, those studies either concentrate on validating specific methods or on single companies or countries. Therefore, they allow only for limited empirical generalisations. Objective: To lay an empirical and generalisable fou...\u00bb", "num_citations": "9\n", "authors": ["176"]}
{"title": "The AVARE PATRON-A Holistic Privacy Approach for the Internet of Things\n", "abstract": " Applications for the Internet of Things are becoming increasingly popular. Due to the large amount of available context data, such applications can be used effectively in many domains. By interlinking these data and analyzing them, it is possible to gather a lot of knowledge about a user. Therefore, these applications pose a threat to privacy. In this paper, we illustrate this threat by looking at a real-world application scenario. Current state of the art focuses on privacy mechanisms either for Smart Things or for big data processing systems. However, our studies show that for a comprehensive privacy protection a holistic view on these applications is required. Therefore, we describe how to combine two promising privacy approaches from both categories, namely AVARE and PATRON. Evaluation results confirm the thereby achieved synergy effects.", "num_citations": "8\n", "authors": ["176"]}
{"title": "A systematic and semi-automatic safety-based test case generation approach based on systems-theoretic process analysis\n", "abstract": " Software safety is a crucial aspect during the development of modern safety-critical systems. Software is becoming responsible for most of the critical functions of systems. Therefore, the software components in the systems need to be tested extensively against their safety requirements to ensure a high level of system safety. However, performing testing exhaustively to test all software behaviours is impossible. Numerous testing approaches exist. However, they do not directly concern the information derived during the safety analysis. STPA (Systems-Theoretic Process Analysis) is a unique safety analysis approach based on system and control theory, and was developed to identify unsafe scenarios of a complex system including software. In this paper, we present a systematic and semi-automatic testing approach based on STPA to generate test cases from the STPA safety analysis results to help software and safety engineers to recognize and reduce the associated software risks. We also provide an open-source safety-based testing tool called STPA TCGenerator to support the proposed approach. We illustrate the proposed approach with a prototype of a software of the Adaptive Cruise Control System (ACC) with a stop-and-go function with a Lego-Mindstorms EV3 robot.", "num_citations": "8\n", "authors": ["176"]}
{"title": "Does personality influence the usage of static analysis tools? an explorative experiment\n", "abstract": " There are many techniques to improve software quality. One is using automatic static analysis tools. We have observed, however, that despite the low-cost help they offer, these tools areunderused and often discourage beginners. There is evidence that personality traits influence the perceived usability of a software. Thus, to support beginners better, we need to understand how theworkflow of people with different prevalent personality traits using these tools varies. For this purpose, we observed users' solution strategies and correlated them with their prevalent personality traits in an exploratory study with student participants within a controlled experiment. We gathered data by screen capturing and chat protocols as well as a Big Five personality traits test. We found strong correlations between particular personality traits and different strategies of removing the findings of static code analysis as well as between\u00a0\u2026", "num_citations": "8\n", "authors": ["176"]}
{"title": "Does outside-in teaching improve the learning of object-oriented programming?\n", "abstract": " Object-oriented programming (OOP) is widely used in the software industry and university introductory courses today. Following the structure of most textbooks, such courses frequently are organised starting with the concepts of imperative and structured programming and only later introducing OOP. An alternative approach is to begin directly with OOP following the Outside-In teaching method as proposed by Meyer. Empirical results for the effects of Outside-In teaching on students and lecturers are sparse, however. We describe the conceptual design and empirical evaluation of two OOP introductory courses from different universities based on Outside-In teaching. The evaluation results are compared to those from a third course serving as the control group, which was taught OOP the \"traditional\" way. We evaluate the initial motivation and knowledge of the participants and the learning outcomes. In addition, we\u00a0\u2026", "num_citations": "8\n", "authors": ["176"]}
{"title": "A Safety Argumentation for Fail-Operational Automotive Systems in Compliance with ISO 26262\n", "abstract": " For highly automated driving, fail-operational driving systems are indispensable to prevent hazardous situations in case of an E/E failure. That requires redundant system design and enhanced safety analysis for ensuring fault tolerance and further operation. Existing work addresses attributes of fail-operational systems relevant for safety, however the sufficiency of safety analysis has not been investigated. We therefore aim to identify relevant safety aspects for fail-operational systems in ISO 26262 which require analysis to ensure compliance. Further we deduce a fault model for a fail-operational driving system containing the relevant failure modes. By consolidating the fault-model and ISO 26262 into a safety argumentation using the goal structure notation we provide a safety argumentation for a fail-operational driving system sufficient according to ISO 26262. Whereas conventional fail-silent systems can be\u00a0\u2026", "num_citations": "7\n", "authors": ["176"]}
{"title": "An approach to global sensitivity analysis: FAST on COCOMO\n", "abstract": " There are various models in software engineering that are used to predict quality-related aspects of the process or artefacts. The use of these models involves elaborate data collection in order to estimate the input parameters. Hence, an interesting question is which of these input factors are most important. More specifically, which factors need to be estimated best and which might be removed from the model? This paper describes an approach based on global sensitivity analysis to answer these questions and shows its applicability in a case study on the COCOMO application at NASA.", "num_citations": "7\n", "authors": ["176"]}
{"title": "Software quality economics for combining defect-detection techniques\n", "abstract": " Software defect-detection techniques such as tests or reviews are still the prevalent means to assure the quality of a program. Although these techniques constitute a significant part of the overall development costs, their economics and interplay is still not fully understood. This paper proposes a model of software quality economics for defect-detection techniques with an emphasis on the combination of diverse techniques. This is important because in real development projects a technique is never used in isolation but in combination with others. This, however, makes the evaluation of a technique very difficult because of the influences of early used techniques on the ones that are later used. We base our combination economics on a model for software quality economics of single techniques and extend it by a model for the diversity of defect-detection techniques. This allows (1) to estimate the effects of a combination and (2) to remove such influences when evaluating a single technique.", "num_citations": "7\n", "authors": ["176"]}
{"title": "From feature models to variation representation in MSCs\n", "abstract": " This paper discusses variation representation in MSCs for their use in the conception and development of system families. For the system family to be, a feature model and a variation point modelling within use cases are assumed given. The language of MSCs is extended with two macrooperators and one operator that allow the representation of variabilities. The macrooperators are to be resolved in terms of a given configuration. Both a grammar and a diagrammatic representation of these operators are given, and their semantics is stated in informal terms.", "num_citations": "7\n", "authors": ["176"]}
{"title": "Assessing Software Quality of Agile Student Projects by Data-mining Software Repositories.\n", "abstract": " Group student software projects are important in computer science education. Students are encouraged to self-organize and learn technical skills, preparing them for real life software development. However, the projects contribute to multiple learning objectives, making coaching students a time consuming task. Thus, it is important to have a suitable best practice development process. For providing better insights for the students, the resulting software has to be of value and meet quality requirements, including maintainability, as in real life software development. Using source code quality metrics and by data mining repository data like commit history, we analyze six student projects, measuring their quality and identifying contributing factors to success or failure of a student project. Based on the findings, we formulate recommendations to improve future projects for students and researchers alike.", "num_citations": "6\n", "authors": ["176"]}
{"title": "Case studies in industry: What we have learnt\n", "abstract": " Case study research has become an important research methodology for exploring phenomena in their natural contexts. Case studies have earned a distinct role in the empirical analysis of software engineering phenomena which are difficult to capture in isolation. Such phenomena often appear in the context of methods and development processes for which it is difficult to run large, controlled experiments as they usually have to reduce the scale in several respects and, hence, are detached from the reality of industrial software development. The other side of the medal is that the realistic socio-economic environments where we conduct case studies - with real-life cases and realistic conditions - also pose a plethora of practical challenges to planning and conducting case studies. In this experience report, we discuss such practical challenges and the lessons we learnt in conducting case studies in industry. Our goal\u00a0\u2026", "num_citations": "6\n", "authors": ["176"]}
{"title": "Natural language processing is no free lunch\n", "abstract": " Today\u2019s operating systems, with their personal assistants Siri or Cortana, show the impressive progress natural language processing (NLP) has made. They make it seem like all technical and methodological challenges of NLP have been solved. As many artefacts in software engineering are full of natural language, the applications are endless. As it turns out, however, using NLP is no free lunch. We offer a brief check on how and how not to apply NLP in software analytics in this chapter.", "num_citations": "6\n", "authors": ["176"]}
{"title": "Using personality traits to understand the influence of personality on computer programming: An empirical study\n", "abstract": " Computer programming is complex and all personality factors might influence it. Personality factors are comprehensive but broad and, therefore, lower level traits may help understanding the influence of personality on computer programming. The objective of this paper is to extend the empirical knowledge in software psychology by using narrow personality traits as well as broad personality traits to explain the influence of personality. The authors surveyed 68 programming students developing software projects to investigate the influence of personality on performance in computer programming. They measured five broad personality factors, 17 personality facets, prior experience, attitude and self-assessed survey performance. They also used the grade students achieved in the software projects as an indicator of software quality. It was found that prior programming experience, attitude towards programming\u00a0\u2026", "num_citations": "6\n", "authors": ["176"]}
{"title": "A case study on artefact-based RE improvement in practice\n", "abstract": " Background: Most requirements engineering (RE) process improvement approaches are solution-driven and activity-based. They focus on the assessment of the RE of a company against an external norm of best practices. A consequence is that practitioners often have to rely on an improvement approach that skips a profound problem analysis and that results in an RE approach that might be alien to the organisational needs. Objective: In recent years, we have developed an RE improvement approach (called ArtREPI) that guides a holistic RE improvement against individual goals of a company putting primary attention to the quality of the artefacts. In this paper, we aim at exploring ArtREPI\u2019s benefits and limitations. Method: We contribute an industrial evaluation of ArtREPI by relying on a case study research. Results: Our results suggest that ArtREPI is well-suited for the establishment of an RE that\u00a0\u2026", "num_citations": "6\n", "authors": ["176"]}
{"title": "A case study on specifying quality requirements using a quality model\n", "abstract": " Quality requirements are an often neglected part of requirements engineering. If specified at all, they tend to be either too abstract or very technical and without a rationale. In this paper, we evaluate a quality requirements approach, which makes use of activity-based quality models. To this end, we conduct a comparative case study at Siemens in which we compare the requirements resulting from applying our quality model with the requirements previously used in the same environment. The results indicate an improvement of the requirements regarding, e.g., structured ness and trace ability, but also that the productivity perceived by the industry participants could not be increased. The study thus gives first insights into strengths and limitations of using a quality model in an industrial requirements engineering process.", "num_citations": "6\n", "authors": ["176"]}
{"title": "Profitability estimation of software projects: A combined framework\n", "abstract": " Decisions on carrying out software projects are a recurring problem for managers. These decisions should ideally be based on solid estimates of the profitability of the projects. However, no single solution has been established for this task. This paper combines the German WiBe framework for costs and benefits of IT projects with certain cost estimation approaches in order to ensure reliable profitability estimates. The applicability of the framework is shown in an industrial case study,", "num_citations": "6\n", "authors": ["176"]}
{"title": "Modelling the quality economics of defect-detection techniques\n", "abstract": " There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. These models allow the structuring of the costs but do not show all influencing factors and their relationships. This paper proposes an analytical model for the economics of defect-detection techniques that can be used for analysis and optimisation of the usage of such techniques. In particular we analyse the sensitivity of the model and how the model can be applied in practice.", "num_citations": "6\n", "authors": ["176"]}
{"title": "Fluid intelligence doesn't matter! effects of code examples on the usability of crypto APIs\n", "abstract": " Context: Programmers frequently look for the code of previously solved problems that they can adapt for their own problem. Despite existing example code on the web, on sites like Stack Overflow, cryptographic Application Programming Interfaces (APIs) are commonly misused. There is little known about what makes examples helpful for developers in using crypto APIs. Analogical problem solving is a psychological theory that investigates how people use known solutions to solve new problems. There is evidence that the capacity to reason and solve novel problems aka Fluid Intelligence (Gf) and structurally and procedurally similar solutions support problem solving. Aim: Our goal is to understand whether similarity and Gf also have an effect in the context of using cryptographic APIs with the help of code examples. Method: We conducted a controlled experiment with 76 student participants developing with or\u00a0\u2026", "num_citations": "5\n", "authors": ["176"]}
{"title": "Real-Life challenges in automotive release planning\n", "abstract": " Context: The use of agile software development is increasing, even in regulated domains like the automotive domain. At the same time, traditional sequential processes are still in use. Collaboration between agile and hybrid projects within these complex traditional product development processes is difficult. Especially the creation and synchronization of a qualification phase plan is challenging. Objective: The aim of this study is to provide insights into the state of the practice to understand challenges related to the combined use of agile and traditional paradigms in release planning in the automotive domain. Method: Based on semi-structured interviews, an online survey with 39 respondents was conducted at Dr. Ing. h. c. F. Porsche AG. Results: We present the challenges identified in release planning, such as lack of transparency regarding the status quo of related projects. Furthermore, we motivate how agile\u00a0\u2026", "num_citations": "5\n", "authors": ["176"]}
{"title": "Factors that influence productivity: A checklist\n", "abstract": " In all areas of professional work, there is are a lot of different factors that influence productivity. Especially in knowledge work, where we do not have easily and clearly measurable work products, it is difficult to capture these factors. Software development is a type of knowledge work that comes with even more specific difficulties, as software developers deal nowadays with incredibly large and complex systems.             We provide a list of factors that empirically have been shown to impact productivity as checklist that a developer or software manager can use to improve productivity. We will discuss technical factors related to the product, the process and the development environment and soft factors related to the corporate culture, the team culture, individual skills and experiences, the work environment and the individual project.", "num_citations": "5\n", "authors": ["176"]}
{"title": "An approach for structuring a highly automated driving multiple channel vehicle system for safety analysis\n", "abstract": " Introducing highly automated driving requires to overcome many challenges. Regarding functional safety, the shift from a fail-safe to a fail-operational vehicle control system is necessary to resume the driving task even in the case of a failure. This necessitates a completely new, multi-channel vehicle architecture including enhanced methods for safety analysis and the collating of single analyses on the system level. We introduce an approach based on a system level fault-tree analysis to ensure completeness of the safety analysis and to deduce suitable methods for detailed analysis of each type of fault, such as random hardware or systematic faults, correlating with branches of the fault-tree. Furthermore, we identify components with the necessity of absence of common cause failures.", "num_citations": "5\n", "authors": ["176"]}
{"title": "How Interesting Are Suggestions of Coupled File Changes for Software Developers?\n", "abstract": " Software repositories represent a data source from which we can extract interesting information to be presented to the developers working on their maintenance tasks. Various studies use the software repositories to extract sets of files that changed frequently in the past. However, they do not consider feedback from developers on whether they would like to use this kind of information. The aim of our research is to support developers in maintenance tasks using suggestions which other files they should also change. We investigate three software repositories to find coupled file changes to support the software developers. We also propose a set of attributes from the versioning system, the issue tracking system and the project documentation. We contrast our findings with the feedback gathered using survey and interviews with the developers. According to our results, small repositories make an insightful\u00a0\u2026", "num_citations": "5\n", "authors": ["176"]}
{"title": "Influence of Personality on Programming Styles an Empirical Study\n", "abstract": " Each programmer tend to his or her own style in programming and human factors may explain these differences in style which has considerable influence on tooling, processes and productivity. The objective of this paper is to study the influence of a previously un-investigated factor, personality, on programming styles. The authors did a survey study on 68 programmers in the University of Stuttgart. Programming experience, attitude towards programming, five personality factors, and programming styles were measured via self-assessed survey. The authors did statistical analysis to investigate links between human factors and programming styles. They found that programming experience is the most influential factor in programming styles but personality is more evident in different programming styles. They conclude that programming styles are a matter of personal preferences and help reveal the influence of\u00a0\u2026", "num_citations": "5\n", "authors": ["176"]}
{"title": "Patterns for testing in global software development\n", "abstract": " Although testing is critical in GSD, its application in this context has not been deeply investigated so far. This work investigates testing in GSD. It provides support for test managers acting in a globally distributed environment. With this it closes a gap. The leading question is\u201d What problems exist in testing in GSD and how can they be addressed in projects?\u201d. Decomposing this question we a) identify problems of testing in GSD projects and b) provide good practices to support practitioners in testing in GSD projects. The research is realized in the context of Capgemini sd&m, a German IT provider. Our contribution to solving the stated research problem is a collection of 16 patterns for testing in GSD projects. For practitioners the usage of the patterns is simplified by various views on the patterns. Herewith we stipulate research and support project managers and test managers in the realization of testing in GSD projects.", "num_citations": "5\n", "authors": ["176"]}
{"title": "Integrating a model of analytical quality assurance into the V-Modell XT\n", "abstract": " Economic models of quality assurance can be an important tool for decision-makers in software development projects. They enable to base quality assurance planning on economical factors of the product and the used defect-detection techniques. A variety of such models has been proposed but many are too abstract to be used in practice. Furthermore, even the more concrete models lack an integration with existing software development process models to increase their applicability. This paper describes an integration of a thorough stochastic model of the economics of analytical quality assurance with the systems development process model V-Modell XT. The integration is done in a modular way by providing a new process module-a concept directly available in the V-Modell XT for extension purposes-related to analytical quality assurance. In particular, we describe the work products, roles, and activities defined\u00a0\u2026", "num_citations": "5\n", "authors": ["176"]}
{"title": "The Mind Is a Powerful Place: How Showing Code Comprehensibility Metrics Influences Code Understanding\n", "abstract": " Static code analysis tools and integrated development environments present developers with quality-related software metrics, some of which describe the understandability of source code. Software metrics influence overarching strategic decisions that impact the future of companies and the prioritization of everyday software development tasks. Several software metrics, however, lack in validation: we just choose to trust that they reflect what they are supposed to measure. Some of them were even shown to not measure the quality aspects they intend to measure. Yet, they influence us through biases in our cognitive-driven actions. In particular, they might anchor us in our decisions. Whether the anchoring effect exists with software metrics has not been studied yet. We conducted a randomized and double-blind experiment to investigate the extent to which a displayed metric value for source code comprehensibility\u00a0\u2026", "num_citations": "4\n", "authors": ["176"]}
{"title": "Collecting service-based maintainability metrics from RESTful API descriptions: static analysis and threshold derivation\n", "abstract": " While many maintainability metrics have been explicitly designed for service-based systems, tool-supported approaches to automatically collect these metrics are lacking. Especially in the context of microservices, decentralization and technological heterogeneity may pose challenges for static analysis. We therefore propose the modular and extensible RAMA approach (RESTful API Metric Analyzer) to calculate such metrics from machine-readable interface descriptions of RESTful services. We also provide prototypical tool support, the RAMA CLI, which currently parses the formats OpenAPI, RAML, and WADL and calculates 10 structural service-based metrics proposed in scientific literature. To make RAMA measurement results more actionable, we additionally designed a repeatable benchmark for quartile-based threshold ranges (green, yellow, orange, red). In an exemplary run, we derived thresholds for all\u00a0\u2026", "num_citations": "4\n", "authors": ["176"]}
{"title": "Too trivial to test? An inverse view on defect prediction to identify methods with low fault risk\n", "abstract": " BackgroundTest resources are usually limited and therefore it is often not possible to completely test an application before a release. To cope with the problem of scarce resources, development teams can apply defect prediction to identify fault-prone code regions. However, defect prediction tends to low precision in cross-project prediction scenarios.AimsWe take an inverse view on defect prediction and aim to identify methods that can be deferred when testing because they contain hardly any faults due to their code being \u201ctrivial\u201d. We expect that characteristics of such methods might be project-independent, so that our approach could improve cross-project predictions.MethodWe compute code metrics and apply association rule mining to create rules for identifying methods with low fault risk (LFR). We conduct an empirical study to assess our approach with six Java open-source projects containing precise fault data at the method level.ResultsOur results show that inverse defect prediction can identify approx. 32\u201344% of the methods of a project to have a LFR; on average, they are about six times less likely to contain a fault than other methods. In cross-project predictions with larger, more diversified training sets, identified methods are even 11 times less likely to contain a fault.ConclusionsInverse defect prediction supports the efficient allocation of test resources by identifying methods that can be treated with less priority in testing activities and is well applicable in cross-project prediction scenarios.", "num_citations": "4\n", "authors": ["176"]}
{"title": "Spreadsheet guardian: An approach to protecting semantic correctness throughout the evolution of spreadsheets\n", "abstract": " Spreadsheets are powerful tools that play a business\u2010critical role in many organizations. However, many bad decisions taken due to faulty spreadsheets show that these tools need serious quality assurance. Furthermore, while collaboration on spreadsheets for maintenance tasks is common, there has been almost no support for ensuring that the spreadsheets remain correct during this process. We have developed an approach named Spreadsheet Guardian, which separates the specification of spreadsheet test rules from their execution. By automatically executing user\u2010defined test rules, our approach is able to detect semantic faults. It also protects all collaborating spreadsheet users from introducing faults during maintenance, even if only few end\u2010users specify test rules. To evaluate Spreadsheet Guardian, we implemented a representative testing technique as an add\u2010in for Microsoft Excel. We evaluated the\u00a0\u2026", "num_citations": "4\n", "authors": ["176"]}
{"title": "Simulation software engineering: experiences and challenges\n", "abstract": " Using software for large-scale simulations has become an important research method in many disciplines. With increasingly complex simulations, simulation software becomes a valuable assest. Yet, the quality of many simulation codes is worrying. In this paper, we want to collect and structure the challenges for a systematic simulation software engineering as a reference and the basis for further research. We describe our own experiences with developing simulation software and collaborating with non-computer-scientists. We complement our experienced challenges with a brief literature review. We structured the challenges for simulation software engineering into six areas: motivation and recognition; education and training; developer turnover; software length of life; verification, validation and debugging; and efficiency vs. maintainability. Overcoming these challenges needs efforts from research agencies\u00a0\u2026", "num_citations": "4\n", "authors": ["176"]}
{"title": "The use of application scanners in software product quality assessment\n", "abstract": " Software development needs continuous quality control for a timely detection and removal of quality problems. This includes frequent quality assessments, which need to be automated as far as possible to be feasible. One way of automation in assessing the security of software are application scanners that test an executing software for vulnerabilities. At present, common quality assessments do not integrate such scanners for giving an overall quality statement. This paper presents an integration of application scanners into a general quality assessment method based on explicit quality models and Bayesian nets. Its applicability and the detection capabilities of common scanners are investigated in a case study with two open-source web shops.", "num_citations": "4\n", "authors": ["176"]}
{"title": "Using the ProdFlow (TM) approach to address the myth of productivity in r&d organizations\n", "abstract": " Software productivity has been analyzed traditionally in terms of size measures such as LOC or FP. These measures have failed to provide a comprehensive basis for productivity analysis. In the research department of the Siemens AG the new approach ProdFLOW\u2122 for the analysis and management of a research & development organization's productivity is being created based on a revised understanding of the term productivity. Former studies often start with fixed, typical indicators and quantitatively analyze the relation between productivity and the indicator by regression models. ProdFLOW\u2122 departs from the fixed model approach, which might not fit to the conditions of the organization. Instead an organization-specific model based on the substantial levers of the productivity, which are both influenceable and measurable, are compiled together with the experts of the organization. The paper explains the new\u00a0\u2026", "num_citations": "4\n", "authors": ["176"]}
{"title": "A Software Reliability Model Based on a Geometric Sequence of Failure Rates\n", "abstract": " Software reliability models are an important tool in quality management and release planning. There is a large number of different models that often have strengths in different areas. This paper proposes a model that is based on a geometric sequence of the failure rates of faults. This property of the failure process was observed in practice at Siemens among others and led to the development of the Fischer-Wagner model. It is described in detail and evaluated using standard criteria. Most import...\u00bb", "num_citations": "4\n", "authors": ["176"]}
{"title": "A methodology for psycho-biological assessment of stress in software engineering\n", "abstract": " Stress pervades our everyday life to the point of being considered the scourge of the modern industrial world. The effects of stress on knowledge workers causes, in short term, performance fluctuations, decline of concentration, bad sensorimotor coordination, and an increased error rate, while long term exposure to stress leads to issues such as dissatisfaction, resignation, depression and general psychosomatic ailment and disease. Software developers are known to be stressed workers. Stress has been suggested to have detrimental effects on team morale and motivation, communication and cooperation-dependent work, software quality, maintainability, and requirements management. There is a need to effectively assess, monitor, and reduce stress for software developers. While there is substantial psycho-social and medical research on stress and its measurement, we notice that the transfer of these methods and practices to software engineering has not been fully made. For this reason, we engage in an interdisciplinary endeavor between researchers in software engineering and medical and social sciences towards a better understanding of stress effects while developing software. This article offers two main contributions. First, we provide an overview of supported theories of stress and the many ways to assess stress in individuals. Second, we propose a robust methodology to detect and measure stress in controlled experiments that is tailored to software engineering research. We also evaluate the methodology by implementing it on an experiment, which we first pilot and then replicate in its enhanced form, and report on the results with\u00a0\u2026", "num_citations": "3\n", "authors": ["176"]}
{"title": "A modular approach to calculate service-based maintainability metrics from runtime data of microservices\n", "abstract": " While several service-based maintainability metrics have been proposed in the scientific literature, reliable approaches to automatically collect these metrics are lacking. Since static analysis is complicated for decentralized and technologically diverse microservice-based systems, we propose a dynamic approach to calculate such metrics from runtime data via distributed tracing. The approach focuses on simplicity, extensibility, and broad applicability. As a first prototype, we implemented a Java application with a Zipkin integrator, 23 different metrics, and five export formats. We demonstrated the feasibility of the approach by analyzing the runtime data of an example microservice-based system. During an exploratory study with six participants, 14 of the 18 services were invoked via the system\u2019s web interface. For these services, all metrics were calculated correctly from the generated traces.", "num_citations": "3\n", "authors": ["176"]}
{"title": "Release planning in a hybrid project environment\n", "abstract": " Context: Even regulated domains like the automotive domain increasingly adopt agile software development. However, traditional sequential processes are still in use and have to coexist with the new development approaches. Collaboration between agile and hybrid projects within complex traditional product development processes is challenging, especially regarding the creation and synchronization of a qualification phase plan. Objective: The aim of this study is to motivate research related to the combined use of agile and traditional paradigms in release planning in the automotive domain and to report challenges from industry. Method: We introduce and motivate the research topic and discuss related work based on the results of a small literature study. Further, an online survey with 56 respondents from an automotive Original Equipment Manufacturer was conducted. Results: There is a clear research\u00a0\u2026", "num_citations": "3\n", "authors": ["176"]}
{"title": "The influence of culture and structure on autonomous teams in established companies\n", "abstract": " Motivation: Many companies aim to provide more autonomy to their development teams. While some teams report on successes, others still struggle with the agile adaption, eg due to the organisational environment. Objective: Our objective was to explore how organisational culture and structure influence team autonomy in bureaucratic companies. Method and Results: 30 qualitative interviews from different business divisions at a conglomerate revealed that organisational factors related to hierarchy, specialist culture and functionally departmentalised structure decreased agile team features and consequently resulted in a reduced speed of decision-making. We suggest the Agile Matching Theory which implies that prevalent organisational factors and desired agile team features need to match to allow team autonomy to occur. Conclusion: We therefore encourage managers to further work on a learning organisation and a supportive structure within which autonomous teams can grow.", "num_citations": "3\n", "authors": ["176"]}
{"title": "Evaluating Maintainability Prejudices with a Large-Scale Study of Open-Source Projects\n", "abstract": " In software engineering, relying on experience can render maintainability expertise into prejudice over time. For example, based on their own experience, some consider JavaScript as inelegant language and hence of lowest maintainability. Such prejudice should not guide decisions without prior empirical validation.               Hence, we formulated 10 hypotheses about maintainability based on prejudices and test them in a large set of open-source projects (6,897 GitHub repositories, 402 million lines, 5 programming languages). We operationalize maintainability with five static analysis metrics.               We found that JavaScript code is not worse than other code, Java code shows higher maintainability than C# code and C code has longer methods than other code. The quality of interface documentation is better in Java code than in other code. Code developed by teams is not of higher and large code bases\u00a0\u2026", "num_citations": "3\n", "authors": ["176"]}
{"title": "Interdisciplinary system courses-teaching agile systems engineering\n", "abstract": " With the advent of technologies like the Internet of Things, Industry 4.0 and Cyber-Physical Systems, many software engineering courses turn into system engineering courses. Recent advances in technologies such as 3D printing and low-cost micro controllers enable to teach agile hard- and software co-design in system engineering courses. In this paper, we describe Interdisciplinary System Courses (ISC) - a teaching approach based on interdisciplinary projects, light-weight agile techniques and solving real problems by integrating industry customers. We describe our experiences from an exploratory case study where we applied ISC in a two-week international summer school with a customer from the aerospace industry. We derive a set of hypotheses on the effects of ISC.", "num_citations": "3\n", "authors": ["176"]}
{"title": "Asheetoxy: a taxonomy for classifying negative spreadsheet-related phenomena\n", "abstract": " Spreadsheets (sometimes also called Excel programs) are powerful tools which play a business-critical role in many organizations. However, due to faulty spreadsheets many bad decisions have been taken in recent years. Since then, a number of researchers have been studying spreadsheet errors. However, one issue that hinders discussion among researchers and professionals is the lack of a commonly accepted taxonomy. Albeit a number of taxonomies for spreadsheet errors have been proposed in previous work, a major issue is that they use the term error that itself is already ambiguous. Furthermore, to apply most existing taxonomies, detailed knowledge about the underlying process and knowledge about the \"brain state\" of the acting spreadsheet users is required. Due to these limitations, known error-like phenomena in freely available spreadsheet corpora cannot be classified with these taxonomies. We propose Asheetoxy, a simple and phenomenon-oriented taxonomy that avoids the problematic term error altogether. An initial study with 7 participants indicates that even non-spreadsheet researchers similarly classify real-world spreadsheet phenomena using Asheetoxy.", "num_citations": "3\n", "authors": ["176"]}
{"title": "On groupthink in safety analysis: An industrial case study\n", "abstract": " Context: In safety-critical systems, an effective safety analysis produces high-quality safety requirements and ensures a safe product from an early stage. Motivation: In safety-critical industries, safety analysis happens mostly in groups. The occurrence of\" groupthink\", under which the group members become concurrence-seeking, potentially leads to a poor safety assurance of products and fatalities. Objective: The purpose of this study is to investigate how groupthink influences safety analysis as well as how to reduce it. Method: We conducted a multiple case study in seven companies by surveying 39 members and interviewing 17 members including software developers, software testers, quality engineers, functional safety managers, hazard/risk managers, sales, purchasing, production managers and senior managers. Results: The TOP 10 phenomena of groupthink in safety analysis are:(1) The managers are too\u00a0\u2026", "num_citations": "3\n", "authors": ["176"]}
{"title": "Poster: Combining STPA and BDD for Safety Analysis and Verification in Agile Development\n", "abstract": " Agile development is in widespread use, even in safety-critical domains. However, there is a lack of an appropriate safety analysis and verification method in agile development. In this poster, we propose the use of Behavior Driven Development for safety verification with System-Theoretic Process Analysis for safety analysis in agile development. It shows a good capability on communication effectiveness through a preliminary controlled experiment.", "num_citations": "3\n", "authors": ["176"]}
{"title": "Continuous and Focused Developer Feedback on Software Quality (CoFoDeF)\n", "abstract": " Software is intangible and, therefore, difficult to understand and control. Quality problems often creep in unnoticed because the developers do not have a full view of the history and all the consequences of their changes to the system. Therefore, we observe quality decay in many software systems over time. A countermeasure would be frequent quality analyses starting early in the development, eg in nightly builds. Frequent quality analyses reduce the problem of quality decay because quality defects are detected soon after they were introduced, but they still allow defects to be introduced. This means that builds might fail, other developers might have already worked with that code revision and changes will result in rework.", "num_citations": "3\n", "authors": ["176"]}
{"title": "Get started imminently: Using tutorials to accelerate learning in automated static analysis\n", "abstract": " Static analysis can be a valuable quality assurance technique as it can find problems by analysing the source code of a system without executing it. Getting used to a static analysis tool, however, can easily take several hours or even days. In particular, understanding the warnings issued by the tool and rooting out the false positives is time consuming. This lowers the benefits of static analysis and demotivates developers in using it. Games solve this problem by offering a tutorial. Those tutorials are integrated in the setting of the game and teach the basic mechanics of the game. Often it is possible to repeat or pick topics of interest. We transfer this pattern to static analysis lowering the initial barrier of using it as well as getting an understanding of software quality spread out to more people. In this paper we propose a research strategy starting with a piloting period in which we will gather information about the questions static analysis users have as well as hone our answers to these questions. These results will be integrated into the prototype. We will evaluate our work then by comparing the fix times of user using the original tool versus our tool.", "num_citations": "3\n", "authors": ["176"]}
{"title": "Modelling system families with message sequence charts: a case study\n", "abstract": " A production system is used as case study of MSCs enriched with a connector construct intended to improve the design. The language of MSCs is further enlarged with the notion of variation points and variants in order to capture the evolutionary aspects of system family development. The design of the basic production system is based on earlier work and provides the initial core assets of the presented system family. Variations of the system are developed and incorporated into a family model that...\u00bb", "num_citations": "3\n", "authors": ["176"]}
{"title": "Data-flow-based adaption of the System-Theoretic Process Analysis for Security (STPA-Sec)\n", "abstract": " Security analysis is an essential activity in security engineering to identify potential system vulnerabilities and specify security requirements in the early design phases. Due to the increasing complexity of modern systems, traditional approaches lack the power to identify insecure incidents caused by complex interactions among physical systems, human and social entities. By contrast, the System-Theoretic Process Analysis for Security (STPA-Sec) approach views losses as resulting from interactions, focuses on controlling system vulnerabilities instead of external threats, and is applicable for complex socio-technical systems. However, the STPA-Sec pays less attention to the non-safety but information-security issues (eg, data confidentiality) and lacks efficient guidance for identifying information security concepts. In this article, we propose a data-flow-based adaption of the STPA-Sec (named STPA-DFSec) to overcome the mentioned limitations and elicit security constraints systematically. We use the STPA-DFSec and STPA-Sec to analyze a vehicle digital key system and investigate the relationship and differences between both approaches, their applicability, and highlights. To conclude, the proposed approach can identify information-related problems more directly from the data processing aspect. As an adaption of the STPA-Sec, it can be used with other STPA-based approaches to co-design systems in multi-disciplines under the unified STPA framework.", "num_citations": "2\n", "authors": ["176"]}
{"title": "Exploring Maintainability Assurance Research for Service-and Microservice-Based Systems: Directions and Differences\n", "abstract": " To ensure sustainable software maintenance and evolution, a diverse set of activities and concepts like metrics, change impact analysis, or antipattern detection can be used. Special maintainability assurance techniques have been proposed for service-and microservice-based systems, but it is difficult to get a comprehensive overview of this publication landscape. We therefore conducted a systematic literature review (SLR) to collect and categorize maintainability assurance approaches for service-oriented architecture (SOA) and microservices. Our search strategy led to the selection of 223 primary studies from 2007 to 2018 which we categorized with a threefold taxonomy: a) architectural (SOA, microservices, both), b) methodical (method or contribution of the study), and c) thematic (maintainability assurance subfield). We discuss the distribution among these categories and present different research directions as well as exemplary studies per thematic category. The primary finding of our SLR is that, while very few approaches have been suggested for microservices so far (24 of 223,\u223c 11%), we identified several thematic categories where existing SOA techniques could be adapted for the maintainability assurance of microservices.", "num_citations": "2\n", "authors": ["176"]}
{"title": "On the impact of service-oriented patterns on software evolvability: a controlled experiment and metric-based analysis\n", "abstract": " Background Design patterns are supposed to improve various quality attributes of software systems. However, there is controversial quantitative evidence of this impact. Especially for younger paradigms such as service- and Microservice-based systems, there is a lack of empirical studies.   Objective In this study, we focused on the effect of four service-based patterns\u2014namely Process Abstraction, Service Fa\u00e7ade, Decomposed Capability, and Event-Driven Messaging\u2014on the evolvability of a system from the viewpoint of inexperienced developers.   Method We conducted a controlled experiment with Bachelor students (N = 69). Two functionally equivalent versions of a service-based web shop\u2014one with patterns (treatment group), one without (control group)\u2014had to be changed and extended in three tasks. We measured evolvability by the effectiveness and efficiency of the participants in these tasks. Additionally, we compared both system versions with nine structural maintainability metrics for size, granularity, complexity, cohesion, and coupling.   Results Both experiment groups were able to complete a similar number of tasks within the allowed 90 min. Median effectiveness was 1/3. Mean efficiency was 12% higher in the treatment group, but this difference was not statistically significant. Only for the third task, we found statistical support for accepting the alternative hypothesis that the pattern version led to higher efficiency. In the metric analysis, the pattern version had worse measurements for size and granularity while simultaneously having slightly better values for coupling metrics. Complexity and cohesion were not impacted\u00a0\u2026", "num_citations": "2\n", "authors": ["176"]}
{"title": "Using data flow-based coverage criteria for black-box integration testing of distributed software systems\n", "abstract": " Modern automotive E/E systems are implemented as distributed real-time software systems. The constantly growing complexity of safety-relevant software functions leads to an increased importance of testing during system integration of such systems. Systematic metrics are required to guide the testing process during system integration by providing coverage measures and stopping criteria but few studied approaches exist. For this purpose, we introduce a data-flow based observation scheme which captures the interplay behavior of involved ECUs during test execution and failure occurrences. In addition, we introduce a data flow-based coverage criterion designed for black box integration. By applying the observation scheme to test cases and associated faults found during execution, we first analyze similarities in data flow coverage. By further analyzing the data flow of failures, that slipped through the phase of\u00a0\u2026", "num_citations": "2\n", "authors": ["176"]}
{"title": "Identification of methods with low fault risk\n", "abstract": " Test resources are usually limited and therefore it is often not possible to completely test an application before a release. Therefore, testers need to focus their activities on the relevant code regions. In this paper, we introduce an inverse defect prediction approach to identify methods that contain hardly any faults. We applied our approach to six Java open-source projects and show that on average 31.6% of the methods of a project have a low fault risk; they contain in total, on average, only 5.8% of all faults. Furthermore, the results suggest that, unlike defect prediction, our approach can also be applied in cross-project prediction scenarios. Therefore, inverse defect prediction can help prioritize untested code areas and guide testers to increase the fault detection probability.", "num_citations": "2\n", "authors": ["176"]}
{"title": "Are suggestions from coupled file changes useful for perfective maintenance tasks?\n", "abstract": " Background                Software maintenance is an important activity in the development process where maintenance team members leave and new members join over time. The identification of files which are changed together frequently has been proposed several times. Yet, existing studies about coupled file changes ignore the feedback from developers as well as the impact of these changes on the performance of maintenance and rather these studies rely on the analysis findings and expert evaluation.                                          Methods                We investigate the usefulness of coupled file changes during perfective maintenance tasks when developers are inexperienced in programming or when they were new on the project. Using data mining on software repositories we identify files that are changed most frequently together in the past. We extract coupled file changes from the Git repository of a Java software system and join them with corresponding attributes from the versioning and issue tracking system and the project documentation. We present a controlled experiment involving 36 student participants in which we investigate if coupled file change suggestions influence the correctness of the task solutions and the required time to complete them.                                          Results                The results show that the use of coupled file change suggestions significantly increases the correctness of the solutions. However, there is only a minor effect on the time required to complete the perfective maintenance tasks. We also derived a set of the most useful attributes based on the developers\u2019 feedback\u00a0\u2026", "num_citations": "2\n", "authors": ["176"]}
{"title": "Which change sets in Git repositories are related?\n", "abstract": " Software repositories contain valuable information about the history of software changes. Using data mining, researchers have identified file changes that happened together frequently to present hints for necessary changes to developers. However, not all file change sets are related. This can affect the recommendations about coupled file changes negatively by delivering irrelevant couplings to the developers. The commit time and branching characteristics of Git have not been investigated together in previous heuristics for grouping related change sets. We exploit the mappings between commit messages and issue ids for judging the relatedness of change sets. We propose a heuristic for Git and investigate the influence of two factors, the time between the commits and their branching on the relatedness of change sets using the repositories of five open-source systems using logistic regression. According to our\u00a0\u2026", "num_citations": "2\n", "authors": ["176"]}
{"title": "Quality Planning\n", "abstract": " Quality is not a fixed property of a software system, but it depends on the needs and goals of the stakeholders. Therefore, we have to carefully plan what quality the system should have. This involves that we identify the stakeholders and understand their needs and map those needs to technical properties and, finally, quality requirements of the system that the developers will implement. In addition, we need to plan not only what quality we want to build, but also how we will build and assure it.", "num_citations": "2\n", "authors": ["176"]}
{"title": "Do We Stop Learning from Our Mistakes When Using Automatic Code Analysis Tools? An Experiment Proposal\n", "abstract": " When we learn how to program, we often do that by trial and error. We struggle with the syntax and with our own understanding of how the idea of the program should look like in the specific programming language. Today there is a huge amount of tools available, which automatically check your code and recommend alterations to the code for the sake of maintainability or correctness. The question, that has not yet been asked by science, is: Are we still learning something from these mistakes, besides the knowledge, that such mistakes will be corrected for us? In the following we will propose an experimental setup, that aims to answer this question.", "num_citations": "2\n", "authors": ["176"]}
{"title": "Modellierung von Software-Security mit aktivit\u00e4tenbasierten Qualit\u00e4tsmodellen\n", "abstract": " Security oder Informationssicherheit stellt f\u00fcr Software immer noch eine gro\u00dfe Herausforderung dar. Trotz breiter Anstrengungen Software sicher zu machen, ist die Zahl der berichteten Schwachstellen unvermindert hoch. Um dem entgegenzuwirken ist es wichtig, Security-Anforderungen klar zu formulieren und den Entwicklern und der Qualit\u00e4tssicherung detaillierte Richtlinien an die Hand zu geben. Dazu wird die Modellierung von Software-Security mit Hilfe von aktivit\u00e4tenbasierten Qualit\u00e4tsmodellen vorgestellt.", "num_citations": "2\n", "authors": ["176"]}
{"title": "Cost-Optimisation of Analytical Software Quality Assurance: Models, Data, Case Studies\n", "abstract": " Analytical software quality assurance (SQA) constitutes with about 50% a significant part of the total development costs of a software system. Hence, it is a promising areafor cost-optimisation. Various defect-detection techniques can be used to improve thequality of software. The main question is how to use those different techniques ina cost-optimal way. However, the costs and benefits of SQA and the interrelationshipsof the influencing factors are still not completely understood.This book presents an analytical and stochastic model of the economics of analytical SQA. The model can be used to analyse different techniques theoretically and to optimisethe SQA in a company using historical project data.Case studies were done in various domains and evaluating different techniques in different phases. All these case studies provided valuable feedback on the model and also contributed to the body ofknowledge on\u00a0\u2026", "num_citations": "2\n", "authors": ["176"]}
{"title": "A Data Warehouse for Cross-Species Anatomy\n", "abstract": " In modern biology genes play an important role. They are studied quite extensively in different model organisms like the mouse and the fruit fly drosophila. A new challenge is to establish connections across these organisms.XSPAN is a research project of Heriot-Watt University and the University of Edinburgh which aims to support links across organisms on an anatomical level. This MSc project plays the role of an early prototype of the XSPAN system. It consists of three main parts. A prototype of a relational database system which is the integrated data warehouse of databases of some model organisms, a prototype implementation of the warehouse loader which is based on web services and finally examples how XSPAN could be used and integrated with other biological resources.", "num_citations": "2\n", "authors": ["176"]}
{"title": "Industry practices and challenges for the evolvability assurance of microservices\n", "abstract": " Context                 Microservices as a lightweight and decentralized architectural style with fine-grained services promise several beneficial characteristics for sustainable long-term software evolution. Success stories from early adopters like Netflix, Amazon, or Spotify have demonstrated that it is possible to achieve a high degree of flexibility and evolvability with these systems. However, the described advantageous characteristics offer no concrete guidance and little is known about evolvability assurance processes for microservices in industry as well as challenges in this area. Insights into the current state of practice are a very important prerequisite for relevant research in this field.                                               Objective                 We therefore wanted to explore how practitioners structure the evolvability assurance processes for microservices, what tools, metrics, and patterns they use, and what\u00a0\u2026", "num_citations": "1\n", "authors": ["176"]}
{"title": "Experiences from Large-Scale Model Checking: Verification of a Vehicle Control System\n", "abstract": " In the age of autonomously driving vehicles, functionality and complexity of embedded systems are increasing tremendously. Safety aspects become more important and require such systems to operate with the highest possible level of fault tolerance. Simulation and systematic testing techniques have reached their limits in this regard. Here, formal verification as a long established technique can be an appropriate complement. However, the necessary preparatory work like adequately modeling a system and specifying properties in temporal logic are anything but trivial. In this paper, we report on our experiences applying model checking to verify the arbitration logic of a Vehicle Control System. We balance pros and cons of different model checking techniques and tools, and reason about our choice of the symbolic model checker NuSMV. We describe the process of modeling the architecture, resulting in ~1500 LOC, 69 state variables and 38 LTL constraints. To handle this large-scale model, we automate and optimize the model checking procedure for use on multi-core CPUs and employ Bounded Model Checking to avoid the state explosion problem. We share our lessons learned and provide valuable insights for architects, developers, and test engineers involved in this highly present topic.", "num_citations": "1\n", "authors": ["176"]}
{"title": "An STPA-based approach for systematic security analysis of in-vehicle diagnostic and software update systems\n", "abstract": " The in-vehicle diagnostic and software update system, which supports remote diagnostic and Over-The-Air (OTA) software updates, is a critical attack goal in automobiles. Adversaries can inject malicious software into vehicles or steal sensitive information through communication channels. Therefore, security analysis, which identifies potential security issues, needs to be conducted in system design. However, existing security analyses of in-vehicle systems are threat-oriented, which start with threat identification and assess risks by brainstorming. In this paper, a system-oriented approach is proposed on the basis of the System-Theoretic Process Analysis (STPA). The proposed approach extends the original STPA from the perspective of data flows and is applicable for information-flow-based systems. Besides, we propose a general model for in-vehicle diagnostic and software update systems and use it to establish a security analysis guideline. In comparison with threat-oriented approaches, the proposed approach shifts from focusing on threats to system vulnerabilities and seems to be efficient to prevent the system from known or even unknown threats. Furthermore, as an extension of the STPA, which has been proven to be applicable to high level designs, the proposed approach can be well integrated into high-level analyses and perform co-design in different disciplines within a unified STPA framework.", "num_citations": "1\n", "authors": ["176"]}
{"title": "A Quantitative Exploration of the 9-Factor Theory: Distribution of Leadership Roles Between Scrum Master and Agile Team\n", "abstract": " A number of qualitative studies find that team leadership is one essential success factor for evolving into a mature agile team. One such qualitative study suggests the 9-Factor Theory of Scrum Master roles, which claims that the Scrum Master performs a set of 9 leadership roles which are transferred to the team over time [14]. We aimed at conducting a quantitative exploration that examines the presence and change of the 9-Factor Theory in relation to team maturity. We conducted an online survey with 67 individuals at the conglomerate Robert Bosch GmbH. Descriptive statistics reveal that the Scrum Master and the agile team score differently on the 9 factors and that the Scrum Master role is most often distributed in teams that had been working between 3 and 5 months in an agile manner. Yet, we also find that the leadership roles predominantly remain with one dedicated Scrum Master. Based on our results we suggest to group the 9-Factor Theory into three clusters: the Scrum Master is rather linked to psychological team factors (1), while the team tends to be linked to rather product-related factors (2). Organizational factors (3) are less often present. Our practical implications suggest an extension of the Scrum Master description. Furthermore, our study lays groundwork for future quantitative testing of leadership in agile teams.", "num_citations": "1\n", "authors": ["176"]}
{"title": "Perception and acceptance of an autonomous refactoring bot\n", "abstract": " The use of autonomous bots for automatic support in software development tasks is increasing. In the past, however, they were not always perceived positively and sometimes experienced a negative bias compared to their human counterparts. We conducted a qualitative study in which we deployed an autonomous refactoring bot for 41 days in a student software development project. In between and at the end, we conducted semi-structured interviews to find out how developers perceive the bot and whether they are more or less critical when reviewing the contributions of a bot compared to human contributions. Our findings show that the bot was perceived as a useful and unobtrusive contributor, and developers were no more critical of it than they were about their human colleagues, but only a few team members felt responsible for the bot.", "num_citations": "1\n", "authors": ["176"]}
{"title": "Towards a central, distributed and secure default cryptography parameter set\n", "abstract": " Cryptography encryption algorithms like the Advanced Encryption Standard (AES) have to be configured with a specific key length. Usually it is required to specify a block mode (eg Galois/Counter Mode (GCM)) and a padding algorithm. All these choices influence the security fundamentally. In practice, it has been shown that developers rely on outdated documentation and tutorials to set these parameters [4, 2]. Therefore, the approach to let developers choose parameters often leads to insecure applications. A better approach would be to include a secure default parameter set with all choices made in every cryptography library. Furthermore, updating these parameters is not always as simple as for encryption of a transport channel, for instance with Transport Layer Security (TLS). With TLS several algorithms can be used, of which one is determined between the client and the server for the duration of one connection, and prioritized, again both by the client and the server. In contrast, for example encrypting files currently requires that the parameters stay the same, otherwise older data could not be decrypted or other not updated applications could not decrypt the data. This can be countered by implementing separate backwards compatible layers in the software, but that increases the maintenance costs and reduces the reliability of the software.", "num_citations": "1\n", "authors": ["176"]}
{"title": "Towards an Evolvability Assurance Method for Service-Based Systems\n", "abstract": " To enable software professionals to design and evolve long-living Service-Based Systems (SBSs) in sustainable fashion, we are developing a continuous assurance method to identify and remediate potential evolvability-related issues. With the rational of broad applicability within service-based architectural styles, we focus on the commonalities of Service-Oriented Architecture (SOA) and Microservices. The method is based on structural service-oriented metrics (e.g. coupling or cohesion), service evolution scenarios, as well as service-oriented design patterns to increase modifiability. Tool support should enable convenient usage and adoption of the method for practitioners. The final evaluation is planned as an industry case study in combination with action research.", "num_citations": "1\n", "authors": ["176"]}
{"title": "Improving Communication in Scrum Teams\n", "abstract": " Communication in teams is an important but difficult issue. In a Scrum development process, we use meetings like the Daily Scrum to inform others about important problems, news and events in the project. When persons are absent due to holiday, illness or travel, they miss relevant information because there is no guarantee that the content of these meetings is documented. We present a concept and a Twitter-like tool to improve communication in a Scrum development process. We take advantage out of the observation that many people do not like to create documentation, but they do like to share what they did. We used the tool in industrial practice and observed an improvement in communication.", "num_citations": "1\n", "authors": ["176"]}
{"title": "Assessing iterative practical software engineering courses with play money\n", "abstract": " Changing our practical software engineering course from the previous waterfall model to a more agile and iterative approach created more severe assessment challenges. To cope with them we added an assessment concept based on play money. The concept not only includes weekly expenses to simulate real running costs but also investments, which correspond to assessment results of the submissions. This concept simulates a startup-like working environment and its financing in an university course. Our early evaluation shows that the combination of the iterative approach and the play money investments is motivating for many students. At this point we think that the combined approach has advantages from both the supervising and the students point of view. We planned more evaluations to better understand all its effects.", "num_citations": "1\n", "authors": ["176"]}
{"title": "Making Software Quality Visible\n", "abstract": " Software quality is hard to comprehend and grasp. Therefore, we need to support developers by giving them quick and frequent feedback so that they can control the quality of their software systems. This talk discusses existing techniques, tools and quality models as well as directions for future research.", "num_citations": "1\n", "authors": ["176"]}
{"title": "A Quality Model for Software Quality\n", "abstract": " A large number of terms describing aspects of software quality, called quality attributes, are defined by taxonomies in standards like ISO 9126 or ISO 25010. These definitions have come under critique for being ambiguous, overlapping and incomplete. Based on our experience in quality modeling, we developed a quality model, defining a hierarchy of quality attributes. It relies on an activity-based paradigm and thus makes use of a clear decomposition criteria. We believe this clearer decompos...\u00bb", "num_citations": "1\n", "authors": ["176"]}
{"title": "8th International Workshop on Software Quality (WoSQ)\n", "abstract": " Software becomes ever more feature-rich and thereby harder to distinguish based on its functionality. Instead, quality is starting to differentiate between similar software products. Specifying, constructing, and assuring quality has been under research for several decades and continues to be a long-term research area because of its many facets and its com-plexity. Current national and international initiatives show that there is an active research community in academia and industry. This workshop builds on the rich experiences of a series of previous workshops and aims to bring this community together to discuss current issues and future developments.", "num_citations": "1\n", "authors": ["176"]}
{"title": "Using a Bayesian Network in the ProdFLOW(TM) Approach\n", "abstract": " ProdFLOW TM1 is a new approach for the productivity analysis and management of research & development organisations created by the research department of the Siemens AG. Its core are organisation-specific models based on the respective substantial levers of productivity. Levers that are both influenceable and measurable are compiled together with the experts of the organisation. This paper proposes to use Bayesian networks for building such models. It is shown how the networks are structured, how they are parameterised and used to analyse different improvement scenarios. Experiences from a case study suggest that Bayesian networks are a suitable technique for the organisation-specific models in ProdFLOW TM.", "num_citations": "1\n", "authors": ["176"]}