{"title": "Chaff: Engineering an efficient SAT solver\n", "abstract": " Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (eg GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search-especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff\u00a0\u2026", "num_citations": "4655\n", "authors": ["1780"]}
{"title": "Power analysis of embedded software: A first step towards software power minimization\n", "abstract": " Embedded computer systems are characterized by the presence of a dedicated processor and the software that runs on it. Power constraints are increasingly becoming the critical component of the design specification of these systems. At present, however, power analysis tools can only be applied at the lower levels of the design-the circuit or gate level. It is either impractical or impossible to use the lower level tools to estimate the power cost of the software component of the system. This paper describes the first systematic attempt to model this power cost. A power analysis technique is developed that has been applied to two commercial microprocessors-Intel 486DX2 and Fujitsu SPARClite 934. This technique can be employed to evaluate the power cost of embedded software. This can help in verifying if a design meets its specified power constraints. Further, it can also be used to search the design space in\u00a0\u2026", "num_citations": "1492\n", "authors": ["1780"]}
{"title": "Efficient conflict driven learning in a boolean satisfiability solver\n", "abstract": " One of the most important features of current state-of-the-art SAT solvers is the use of conflict based backtracking and learning techniques. In this paper, we generalize various conflict driven learning strategies in terms of different partitioning schemes of the implication graph. We re-examine the learning techniques used in various SAT solvers and propose an array of new learning schemes. Extensive experiments with real world examples show that the best performing new learning scheme has at least a 2/spl times/ speedup compared with learning schemes employed in state-of-the-art SAT solvers.", "num_citations": "1132\n", "authors": ["1780"]}
{"title": "Orion: A power-performance simulator for interconnection networks\n", "abstract": " We present Orion, a power-performance interconnection network simulator that is capable of providing detailed power characteristics, in addition to performance characteristics, to enable rapid power-performance tradeoffs at the architectural-level. This capability is provided within a general framework that builds a simulator starting from a microarchitectural specification of the interconnection network. A key component of this construction is the architectural-level parameterized power models that we have derived as part of this effort. Using component power models and a synthesized efficient power (and performance) simulator a microarchitect can rapidly explore the design space. As case studies, we demonstrate the use of Orion in determining optimal system parameters, in examining the effect of diverse traffic conditions, as well as evaluating new network microarchitectures. In each of the above, the ability to\u00a0\u2026", "num_citations": "984\n", "authors": ["1780"]}
{"title": "Instruction level power analysis and optimization of software\n", "abstract": " The increasing popularity of power constrained mobile computers and embedded computing applications drives the need for analyzing and optimizing power in all the components of a system. Software constitutes a major component of today\u2019s systems, and its role is projected to grow even further. Thus, an ever increasing portion of the functionality of today\u2019s systems is in the form of instructions, as opposed to gates. This motivates the need for analyzing power consumption from the point of view of instructions\u2014something that traditional circuit and gate level power analysis tools are inadequate for. This paper describes an alternative, measurement based instruction level power analysis approach that provides an accurate and practical way of quantifying the power cost of software. This technique has been applied to three commercial, architecturally different processors. The salient results of these analyses\u00a0\u2026", "num_citations": "799\n", "authors": ["1780"]}
{"title": "Performance analysis of embedded software using implicit path enumeration\n", "abstract": " Embedded computer systems are characterized by the presence of a processor running application specific dedicated software. A large number of these systems must satisfy real-time constraints. This paper examines the problem of determining the extreme (best and worst) case bounds on the running time of a given program on a given processor. This has several applications in the design of embedded systems with real-time constraints. An important aspect of this problem is determining which paths in the program are exercised in the extreme cases. The state of the art solution here relies on an explicit enumeration of program paths. This runs out of steam rather quickly since the number of feasible program paths is typically exponential in the size of the program. We present a solution for this problem that does not require an explicit enumeration of program paths, ie, the paths are considered implicitly. This\u00a0\u2026", "num_citations": "728\n", "authors": ["1780"]}
{"title": "Evaluating the security of logic encryption algorithms\n", "abstract": " Contemporary integrated circuits are designed and manufactured in a globalized environment leading to concerns of piracy, overproduction and counterfeiting. One class of techniques to combat these threats is logic encryption. Logic encryption modifies an IC design such that it operates correctly only when a set of newly introduced inputs, called key inputs, are set to the correct values. In this paper, we use algorithms based on satisfiability checking (SAT) to investigate the security of logic encryption. We present a SAT-based algorithm which allows an attacker to \u201cdecrypt\u201d an encrypted netlist using a small number of carefully-selected input patterns and their corresponding output observations. We also present a \u201cpartial-break\u201d algorithm that can reveal some of the key inputs even when the attack is not fully successful. We conduct a thorough evaluation of our attack by examining six proposals for logic encryption\u00a0\u2026", "num_citations": "517\n", "authors": ["1780"]}
{"title": "Power-driven design of router microarchitectures in on-chip networks\n", "abstract": " As demand for bandwidth increases in systems-on-a-chip and chip multiprocessors, networks are fast replacing buses and dedicated wires as the pervasive interconnect fabric for on-chip communication. The tight delay requirements faced by on-chip networks have resulted in prior microarchitectures being largely performance-driven. While performance is a critical metric, on-chip networks are also extremely power-constrained. In this paper, we investigate on-chip network microarchitectures from a power-driven perspective. We first analyze the power dissipation of existing network microarchitectures, highlighting insights that prompt us to devise several power-efficient network microarchitectures: segmented crossbar, cut-through crossbar and write-through buffer. We also study and uncover the power saving potential of existing network architecture: express cube. These techniques are evaluated with synthetic as\u00a0\u2026", "num_citations": "505\n", "authors": ["1780"]}
{"title": "The quest for efficient boolean satisfiability solvers\n", "abstract": " The classical NP-complete problem of Boolean Satisfiability (SAT) has seen much interest in not just the theoretical computer science community, but also in areas where practical solutions to this problem enable significant practical applications. Since the first development of the basic search based algorithm proposed by Davis, Putnam, Logemann and Loveland (DPLL) about forty years ago, this area has seen active research effort with many interesting contributions that have culminated in state-of-the-art SAT solvers today being able to handle problem instances with thousands, and in same cases even millions, of variables. In this paper we examine some of the main ideas along this passage that have led to our current capabilities. Given the depth of the literature in this field, it is impossible to do this in any comprehensive way; rather we focus on techniques with consistent demonstrated efficiency in\u00a0\u2026", "num_citations": "470\n", "authors": ["1780"]}
{"title": "A survey of optimization techniques targeting low power VLSI circuits\n", "abstract": " We survey state-of-the-art optimization methods that target low power dissipation in VLSI circuits. Optimizations at the circuit, logic, architectural and system levels are considered.", "num_citations": "446\n", "authors": ["1780"]}
{"title": "Conflict-driven clause learning SAT solvers\n", "abstract": " One of the most important paradigm shifts in the use of SAT solvers for solving industrial problems has been the introduction of clause learning. Clause learning entails adding a new clause for each conflict during backtrack search. This new clause prevents the same conflict from occurring again during the search process. Moreover, sophisticated techniques such as the identification of unique implication points in a graph of implications, allow creating clauses that more precisely identify the assignments responsible for conflicts. Learned clauses often have a large number of literals. As a result, another paradigm shift has been the development of new data structures, namely lazy data structures, which are particularly effective at handling large clauses. These data structures are called lazy due to being in general unable to provide the actual status of a clause. Efficiency concerns and the use of lazy data structures\u00a0\u2026", "num_citations": "435\n", "authors": ["1780"]}
{"title": "Algorithms for discrete function manipulation\n", "abstract": " An investigation was made of the analogous graph structure for representing and manipulating discrete variable problems. The authors define the multi-valued decision diagram (MDD), analyze its properties (in particular prove a strong canonical form) and provide algorithms for combining and manipulating MDDs. They give a method for mapping an MDD into an equivalent BDD (binary decision diagram) which allows them to provide a highly efficient implementation using the previously developed BDD packages. A direct implementation of the MDD structure has also been carried out, but this initial implementation has not yet been tuned to the same extent as the BDDs to allow a reasonable comparison to be made. The authors have used the mapping to BDDs to provide an initial understanding of the limits on the sizes of real problems that can be executed. The results are encouraging.<>", "num_citations": "395\n", "authors": ["1780"]}
{"title": "Validating SAT solvers using an independent resolution-based checker: Practical implementations and other applications\n", "abstract": " As the use of SAT solvers as core engines in EDA applications grows, it becomes increasingly important to validate their correctness. In this paper, we describe the implementation of an independent resolution-based checking procedure that can check the validity of unsatisfiable claims produced by the SAT solver zchaff. We examine the practical implementation issues of such a checker and describe two implementations with different pros and cons. Experimental results show low overhead for the checking process. Our checker can work with many other modern SAT solvers with minor modifications, and it can provide information for debugging when checking fails. Finally we describe additional results that can be obtained by the validation process and briefly discuss their applications.", "num_citations": "378\n", "authors": ["1780"]}
{"title": "Cache miss equations: a compiler framework for analyzing and tuning memory behavior\n", "abstract": " With the ever-widening performance gap between processors and main memory, cache memory, which is used to bridge this gap, is becoming more and more significant. Caches work well for programs that exhibit sufficient locality. Other programs, however, have reference patterns that fail to exploit the cache, thereby suffering heavily from high memory latency. In order to get high cache efficiency and achieve good program performance, efficient memory accessing behavior is necessary. In fact, for many programs, program transformations or source-code changes can radically alter memory access patterns, significantly improving cache performance. Both hand-tuning and compiler optimization techniques are often used to transform codes to improve cache utilization.  Unfortunately, cache conflicts   are difficult to predict and estimate, precluding effective transformations. Hence, effective transformations require\u00a0\u2026", "num_citations": "372\n", "authors": ["1780"]}
{"title": "Power analysis and minimization techniques for embedded DSP software\n", "abstract": " Power is becoming a critical constraint for designing embedded applications. Current power analysis techniques based on circuit-level or architectural-level simulation are either impractical or inaccurate to estimate the power cost for a given piece of application software. In this paper, an instruction-level power analysis model is developed for an embedded digital signal processor (DSP) based on physical current measurements. Significant points of difference have been observed between the software power model for this custom DSP processor and the power models that have been developed earlier for some general purpose commercial microprocessors. In particular, the effect of circuit state on the power cost of an instruction stream is more marked in the case of this DSP processor. In addition, the processor has special architectural features that allow dual memory accesses and packing of instructions into pairs\u00a0\u2026", "num_citations": "369\n", "authors": ["1780"]}
{"title": "Cache modeling for real-time software: Beyond direct mapped instruction caches\n", "abstract": " We present a method for determining a tight bound on the worst case execution time of a program when running on a given hardware system with cache memory. Caches are used to improve the average memory performance, however, their presence complicates the worst case timing analysis. Any pessimistic predictions on cache hits/misses will result in loose estimation. In our previous research in this area, we built an integer-linear-programming solution for this problem which included analysis of direct mapped instruction caches. In this paper we describe the complex extensions of this technique to deal with set associative instruction caches, data caches and unified caches. We believe that this research now provides a comprehensive solution to the problem of worst case performance analysis of software running on processors with caches. These techniques have been implemented in a design tool cinderella\u00a0\u2026", "num_citations": "328\n", "authors": ["1780"]}
{"title": "Performance estimation of embedded software with instruction cache modeling\n", "abstract": " Embedded systems generally interact in some way with the outside  world. This may involve measuring sensors and controlling actuators, communicating with other systems, or interacting with users. These functions impose real-time constraints on system design. Verification of these specifications requires computing an upper bound on the worst-case execution time (WCET) of a hardware/software system. Furthermore, it is critical to derive a tight upper bound on WCET in order to make efficient use of system resources. The problem of bounding WCET   is particularly difficult on modern processors. These processors use cache-based memory systems that vary memory access time based on the dynamic memory access pattern of the program. This must be accurately modeled in order to   tightly bound WCET. Several analysis methods have been proposed to bound WCET on processors with instruction caches\u00a0\u2026", "num_citations": "321\n", "authors": ["1780"]}