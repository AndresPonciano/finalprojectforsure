{"title": "Knowledge transfer challenges and mitigation strategies in global software development\u2014A systematic literature review and industrial validation\n", "abstract": " ContextIn this article we considered knowledge transfer (KT) in global software development (GSD) from two perspectives, state-of-the-art and state-of-the-practice, in order to identify what are the challenges that hamper the success of KT in global software teams, as well as to find out what are the mitigation strategies that can be used to overcome such challenges.ObjectivesThe overall aim of this work is to provide a body of knowledge for enabling successful KT in GSD settings. This is achieved by an in-depth understanding of KT challenges and mitigation strategies, both from the perspective of literature and industry. It also identifies the similarities and differences in challenges and strategies gathered from literature studies and industrial experts.MethodsIn order to fulfill the aim of the research, we collected data through a systematic literature review (SLR) and conducted interviews with industrial experts. Through\u00a0\u2026", "num_citations": "139\n", "authors": ["568"]}
{"title": "On the application of genetic programming for software engineering predictive modeling: A systematic review\n", "abstract": " The objective of this paper is to investigate the evidence for symbolic regression using genetic programming (GP) being an effective method for prediction and estimation in software engineering, when compared with regression/machine learning models and other comparison groups (including comparisons with different improvements over the standard GP algorithm). We performed a systematic review of literature that compared genetic programming models with comparative techniques based on different independent project variables. A total of 23 primary studies were obtained after searching different information sources in the time span 1995\u20132008. The results of the review show that symbolic regression using genetic programming has been applied in three domains within software engineering predictive modeling: (i) Software quality classification (eight primary studies). (ii) Software cost/effort/size estimation\u00a0\u2026", "num_citations": "98\n", "authors": ["568"]}
{"title": "Software test process improvement approaches: A systematic literature review and an industrial case study\n", "abstract": " Software test process improvement (STPI) approaches are frameworks that guide software development organizations to improve their software testing process. We have identified existing STPI approaches and their characteristics (such as completeness of development, availability of information and assessment instruments, and domain limitations of the approaches) using a systematic literature review (SLR). Furthermore, two selected approaches (TPI NEXT and TMMi) are evaluated with respect to their content and assessment results in industry. As a result of this study, we have identified 18 STPI approaches and their characteristics. A detailed comparison of the content of TPI NEXT and TMMi is done. We found that many of the STPI approaches do not provide sufficient information or the approaches do not include assessment instruments. This makes it difficult to apply many approaches in industry. Greater\u00a0\u2026", "num_citations": "89\n", "authors": ["568"]}
{"title": "Constrained interaction testing: A systematic literature study\n", "abstract": " Interaction testing can be used to effectively detect faults that are otherwise difficult to find by other testing techniques. However, in practice, the input configurations of software systems are subjected to constraints, especially in the case of highly configurable systems. Handling constraints effectively and efficiently in combinatorial interaction testing is a challenging problem. Nevertheless, researchers have attacked this challenge through different techniques, and much progress has been achieved in the past decade. Thus, it is useful to reflect on the current achievements and shortcomings and to identify potential areas of improvements. This paper presents the first comprehensive and systematic literature study to structure and categorize the research contributions for constrained interaction testing. Following the guidelines of conducting a literature study, the relevant data are extracted from a set of 103 research\u00a0\u2026", "num_citations": "52\n", "authors": ["568"]}
{"title": "Handling constraints in combinatorial interaction testing in the presence of multi objective particle swarm and multithreading\n", "abstract": " ContextCombinatorial testing strategies have lately received a lot of attention as a result of their diverse applications. In its simple form, a combinatorial strategy can reduce several input parameters (configurations) of a system into a small set based on their interaction (or combination). In practice, the input configurations of software systems are subjected to constraints, especially in case of highly configurable systems. To implement this feature within a strategy, many difficulties arise for construction. While there are many combinatorial interaction testing strategies nowadays, few of them support constraints.ObjectiveThis paper presents a new strategy, to construct combinatorial interaction test suites in the presence of constraints.MethodThe design and algorithms are provided in detail. To overcome the multi-judgement criteria for an optimal solution, the multi-objective particle swarm optimisation and multithreading\u00a0\u2026", "num_citations": "41\n", "authors": ["568"]}
{"title": "An experiment on the effectiveness and efficiency of exploratory testing\n", "abstract": " The exploratory testing (ET) approach is commonly applied in industry, but lacks scientific research. The scientific community needs quantitative results on the performance of ET taken from realistic experimental settings. The objective of this paper is to quantify the effectiveness and efficiency of ET vs. testing with documented test cases (test case based testing, TCT). We performed four controlled experiments where a total of 24 practitioners and 46 students performed manual functional testing using ET and TCT. We measured the number of identified defects in the 90-minute testing sessions, the detection difficulty, severity and types of the detected defects, and the number of false defect reports. The results show that ET found a significantly greater number of defects. ET also found significantly more defects of varying levels of difficulty, types and severity levels. However, the two testing approaches did not differ\u00a0\u2026", "num_citations": "41\n", "authors": ["568"]}
{"title": "A comparative evaluation of using genetic programming for predicting fault count data\n", "abstract": " There have been a number of software reliability growth models (SRGMs) proposed in literature. Due to several reasons, such as violation of models' assumptions and complexity of models, the practitioners face difficulties in knowing which models to apply in practice. This paper presents a comparative evaluation of traditional models and use of genetic programming (GP) for modeling software reliability growth based on weekly fault count data of three different industrial projects. The motivation of using a GP approach is its ability to evolve a model based entirely on prior data without the need of making underlying assumptions. The results show the strengths of using GP for predicting fault count data.", "num_citations": "34\n", "authors": ["568"]}
{"title": "Application of blockchain and Internet of Things to ensure tamper-proof data availability for food safety\n", "abstract": " Food supply chain plays a vital role in human health and food prices. Food supply chain inefficiencies in terms of unfair competition and lack of regulations directly affect the quality of human life and increase food safety risks. This work merges Hyperledger Fabric, an enterprise-ready blockchain platform with existing conventional infrastructure, to trace a food package from farm to fork using an identity unique for each food package while keeping it uncomplicated. It keeps the records of business transactions that are secured and accessible to stakeholders according to the agreed set of policies and rules without involving any centralized authority. This paper focuses on exploring and building an uncomplicated, low-cost solution to quickly link the existing food industry at different geographical locations in a chain to track and trace the food in the market.", "num_citations": "30\n", "authors": ["568"]}
{"title": "Towards earlier fault detection by value-driven prioritization of test cases using fuzzy topsis\n", "abstract": " Software testing in industrial projects typically requires large test suites. Executing them is commonly expensive in terms of effort and wall-clock time. Indiscriminately executing all available test cases leads to sub-optimal exploitation of testing resources. Selecting too few test cases for execution on the other hand might leave a large number of faults undiscovered. Limiting factors such as allocated budget and time constraints for testing further emphasizes the importance of test case prioritization in order to identify test cases that enable earlier detection of faults while respecting such constraints. This paper introduces a novel method prioritizing test cases to detect faults earlier. The method combines TOPSIS decision making with fuzzy principles. The method is based on multi-criteria like fault detection probability, execution time, or complexity. Applying the method in an industrial context for testing a train\u00a0\u2026", "num_citations": "26\n", "authors": ["568"]}
{"title": "Using faults-slip-through metric as a predictor of fault-proneness\n", "abstract": " Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Na\u00efve Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based\u00a0\u2026", "num_citations": "25\n", "authors": ["568"]}
{"title": "Dynamic integration test selection based on test case dependencies\n", "abstract": " Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unfit for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding\u00a0\u2026", "num_citations": "24\n", "authors": ["568"]}
{"title": "Testability and software robustness: A systematic literature review\n", "abstract": " The concept of software testability has been researched in several different dimensions, however the relation of this important concept with other quality attributes is a grey area where existing evidence is scattered. The objective of this study is to present a state-of-the-art with respect to issues of importance concerning software testability and an important quality attribute: software robustness. The objective is achieved by conducting a systematic literature review (SLR) on the topic. Our results show that a variety of testability issues are in focus with observability and controllability issues being most researched. Fault tolerance, exception handling and handling external influence are prominent robustness issues in focus.", "num_citations": "22\n", "authors": ["568"]}
{"title": "Search-based resource scheduling for bug fixing tasks\n", "abstract": " The software testing phase usually results in a large number of bugs to be fixed. The fixing of these bugs require executing certain activities (potentially concurrent) that demand resources having different competencies and workloads. Appropriate resource allocation to these bug-fixing activities can help a project manager to schedule capable resources to these activities, taking into account their availability and skill requirements for fixing different bugs. This paper presents a multi-objective search-based resource scheduling method for bug-fixing tasks. The inputs to our proposed method include i) a bug model, ii) a human resource model, iii) a capability matching method between bug-fixing activities and human resources and iv) objectives of bug-fixing. A genetic algorithm (GA) is used as a search algorithm and the output is a bug-fixing schedule, satisfying different constraints and value objectives. We have\u00a0\u2026", "num_citations": "22\n", "authors": ["568"]}
{"title": "Suitability of genetic programming for software reliability growth modeling\n", "abstract": " Genetic programming (GP) has been found to be effective in finding a model that fits the given data points without making any assumptions about the model structure. This makes GP a reasonable choice for software reliability growth modeling. This paper discusses the suitability of using GP for software reliability growth modeling and highlights the mechanisms that enable GP to progressively search for fitter solutions.", "num_citations": "22\n", "authors": ["568"]}
{"title": "Metrics in software test planning and test design processes\n", "abstract": " Software metrics plays an important role in measuring attributes that are critical to the success of a software project. Measurement of these attributes helps to make the characteristics and relationships between the attributes clearer. This in turn supports informed decision making.The field of software engineering is affected by infrequent, incomplete and inconsistent measurements. Software testing is an integral part of software development, providing opportunities for measurement of process attributes. The measurement of software testing process attributes enables the management to have better insight in to the software testing process.", "num_citations": "22\n", "authors": ["568"]}
{"title": "Cost-benefit analysis of using dependency knowledge at integration testing\n", "abstract": " In software system development, testing can take considerable time and resources, and there are numerous examples in the literature of how to improve the testing process. In particular, methods for selection and prioritization of test cases can play a critical role in efficient use of testing resources. This paper focuses on the problem of selection and ordering of integration-level test cases. Integration testing is performed to evaluate the correctness of several units in composition. Further, for reasons of both effectiveness and safety, many embedded systems are still tested manually. To this end, we propose a process, supported by an online decision support system, for ordering and selection of test cases based on the test result of previously executed test cases. To analyze the economic efficiency of such a system, a customized return on investment (ROI) metric tailored for system integration testing is\u00a0\u2026", "num_citations": "21\n", "authors": ["568"]}
{"title": "Incorporating metrics in an organizational test strategy\n", "abstract": " An organizational level test strategy needs to incorporate metrics to make the testing activities visible and available to process improvements. The majority of testing measurements that are done are based on faults found in the test execution phase. In contrast, this paper investigates metrics to support software test planning and test design processes. We have assembled metrics in these two process types to support management in carrying out evidence-based test process improvements and to incorporate suitable metrics as part of an organization level test strategy. The study is composed of two steps. The first step creates a relevant context by analyzing key phases in the software testing lifecycle, while the second step identifies the attributes of software test planning and test design processes along with metric(s) support for each of the identified attributes.", "num_citations": "21\n", "authors": ["568"]}
{"title": "Vertical Test Reuse for Embedded Systems: A Systematic Mapping Study\n", "abstract": " Vertical test reuse refers to the reuse of test cases or other test artifacts over different integration levels in the software or system engineering process. Vertical test reuse has previously been proposed for reducing test effort and improving test effectiveness, particularly for embedded system development. The goal of this study is to provide an overview of the state of the art in the field of vertical test reuse for embedded system development. For this purpose, a systematic mapping study has been performed, identifying 11 papers on vertical test reuse for embedded systems. The primary result from the mapping is a classification of published work on vertical test reuse in the embedded system domain, covering motivations for reuse, reuse techniques, test levels and reusable test artifacts considered, and to what extent the effects of reuse have been evaluated.", "num_citations": "19\n", "authors": ["568"]}
{"title": "Towards benchmarking feature subset selection methods for software fault prediction\n", "abstract": " Despite the general acceptance that software engineering datasets often contain noisy, irrelevant or redundant variables, very few benchmark studies of feature subset selection (FSS) methods on real-life data from software projects have been conducted. This paper provides an empirical comparison of state-of-the-art FSS methods: information gain attribute ranking (IG); Relief (RLF); principal component analysis (PCA); correlation-based feature selection (CFS); consistency-based subset evaluation (CNS); wrapper subset evaluation (WRP); and an evolutionary computation method, genetic programming (GP), on five fault prediction datasets from the PROMISE data repository. For all the datasets, the area under the receiver operating characteristic curve\u2014the AUC value averaged over 10-fold cross-validation runs\u2014was calculated for each FSS method-dataset combination before and after FSS. Two diverse\u00a0\u2026", "num_citations": "18\n", "authors": ["568"]}
{"title": "Predicting software test effort in iterative development using a dynamic Bayesian network\n", "abstract": " Projects following iterative software development methodologies must still be managed in a way as to maximize quality and minimize costs. However, there are indications that predicting test effort in iterative development is challenging and currently there seem to be no models for test effort prediction. This paper introduces and validates a dynamic Bayesian network for predicting test effort in iterative software development.", "num_citations": "16\n", "authors": ["568"]}
{"title": "Testability and software performance: A systematic mapping study\n", "abstract": " In most of the research on software testability, functional correctness of the software has been the focus while the evidence regarding testability and non-functional properties such as performance is sporadic. The objective of this study is to present the current state-of-the-art related to issues of importance, types and domains of software under test, types of research, contribution types and design evaluation methods concerning testability and software performance. We find that observability, controllability and testing effort are the main testability issues while timeliness and response time (ie, time constraints) are the main performance issues in focus. The primary studies in the area use diverse types of software under test within different domains, with realtime systems as being a dominant domain. The researchers have proposed many different methods in the area, however these methods lack implementation in practice.", "num_citations": "15\n", "authors": ["568"]}
{"title": "Similarity-based prioritization of test case automation\n", "abstract": " The importance of efficient software testing procedures is driven by an ever increasing system complexity as well as global competition. In the particular case of manual test cases at the system integration level, where thousands of test cases may be executed before release, time must be well spent in order to test the system as completely and as efficiently as possible. Automating a subset of the manual test cases, i.e, translating the manual instructions to automatically executable code, is one way of decreasing the test effort. It is further common that test cases exhibit similarities, which can be exploited through reuse when automating a test suite. In this paper, we investigate the potential for reducing test effort by ordering the test cases before such automation, given that we can reuse already automated parts of test cases. In our analysis, we investigate several approaches for prioritization in a case study at a\u00a0\u2026", "num_citations": "14\n", "authors": ["568"]}
{"title": "Functional dependency detection for integration test cases\n", "abstract": " This paper presents a natural language processing (NLP) based approach that, given software requirements specification, allows the functional dependency detection between integration test cases. We analyze a set of internal signals to the implemented modules for detecting dependencies between requirements and thereby identifying dependencies between test cases such that: module 2 depends on module 1 if an output internal signal from module 1 enters as an input internal signal to the module 2. Consequently, all requirements (and thereby test cases) for module 2 are dependent on all the designed requirements (and test cases) for module 1. The dependency information between requirements (and thus corresponding test cases) can be utilized for test case prioritization and scheduling. We have implemented our approach as a tool and the feasibility is evaluated through an industrial use case in the\u00a0\u2026", "num_citations": "14\n", "authors": ["568"]}
{"title": "Optimum Design of PI\u03bbD\u03bc Controller for an Automatic Voltage Regulator System Using Combinatorial Test Design\n", "abstract": " Combinatorial test design is a plan of test that aims to reduce the amount of test cases systematically by choosing a subset of the test cases based on the combination of input variables. The subset covers all possible combinations of a given strength and hence tries to match the effectiveness of the exhaustive set. This mechanism of reduction has been used successfully in software testing research with t-way testing (where t indicates the interaction strength of combinations). Potentially, other systems may exhibit many similarities with this approach. Hence, it could form an emerging application in different areas of research due to its usefulness. To this end, more recently it has been applied in a few research areas successfully. In this paper, we explore the applicability of combinatorial test design technique for Fractional Order (FO), Proportional-Integral-Derivative (PID) parameter design controller, named as FOPID, for an automatic voltage regulator (AVR) system. Throughout the paper, we justify this new application theoretically and practically through simulations. In addition, we report on first experiments indicating its practical use in this field. We design different algorithms and adapted other strategies to cover all the combinations with an optimum and effective test set. Our findings indicate that combinatorial test design can find the combinations that lead to optimum design. Besides this, we also found that by increasing the strength of combination, we can approach to the optimum design in a way that with only 4-way combinatorial set, we can get the effectiveness of an exhaustive test set. This significantly reduced the number of tests needed and\u00a0\u2026", "num_citations": "14\n", "authors": ["568"]}
{"title": "Opinion-Based Entity Ranking using learning to rank\n", "abstract": " As social media and e-commerce on the Internet continue to grow, opinions have become one of the most important sources of information for users to base their future decisions on. Unfortunately, the large quantities of opinions make it difficult for an individual to comprehend and evaluate them all in a reasonable amount of time. The users have to read a large number of opinions of different entities before making any decision. Recently a new retrieval task in information retrieval known as Opinion-Based Entity Ranking (OpER) has emerged. OpER directly ranks relevant entities based on how well opinions on them are matched with a user's preferences that are given in the form of queries. With such a capability, users do not need to read a large number of opinions available for the entities. Previous research on OpER does not take into account the importance and subjectivity of query keywords in individual\u00a0\u2026", "num_citations": "13\n", "authors": ["568"]}
{"title": "ESPRET: A tool for execution time estimation of manual test cases\n", "abstract": " Manual testing is still a predominant and an important approach for validation of computer systems, particularly in certain domains such as safety-critical systems. Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. In this work, we present, apply and evaluate\u00a0ESPRET (EStimation and PRediction of Execution Time) as our tool for estimating and predicting the execution time of manual test cases based on their test specifications. Our approach works by extracting timing information for various steps in manual test specification. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test\u00a0\u2026", "num_citations": "12\n", "authors": ["568"]}
{"title": "Decision making and visualizations based on test results\n", "abstract": " Background: Testing is one of the main methods for quality assurance in the development of embedded software, as well as in software engineering in general. Consequently, test results (and how they are reported and visualized) may substantially influence business decisions in software-intensive organizations. Aims: This case study examines the role of test results from automated nightly software testing and the visualizations for decision making they enable at an embedded systems company in Sweden. In particular, we want to identify the use of the visualizations for supporting decisions from three aspects: in daily work, at feature branch merge, and at release time. Method: We conducted an embedded case study with multiple units of analysis by conducting interviews, questionnaires, using archival data and participant observations. Results: Several visualizations and reports built on top of the test results\u00a0\u2026", "num_citations": "11\n", "authors": ["568"]}
{"title": "Towards execution time prediction for manual test cases from test specification\n", "abstract": " Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. This work in progress paper presents a novel approach for predicting the execution time of test cases based on test specifications and available historical data on previously executed test cases. Our approach works by extracting timing information (measured and maximum execution time)for various steps in manual test cases. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test activities is already available or not. Finally, linear regression is used to predict the actual execution time for test cases. A proof-of-concept use case\u00a0\u2026", "num_citations": "11\n", "authors": ["568"]}
{"title": "Towards a taxonomy for eliciting design-operation continuum requirements of cyber-physical systems\n", "abstract": " Software systems that are embedded in autonomous Cyber-Physical Systems (CPSs) usually have a large life-cycle, both during its development and in maintenance. This software evolves during its life-cycle in order to incorporate new requirements, bug fixes, and to deal with hardware obsolescence. The current process for developing and maintaining this software is very fragmented, which makes developing new software versions and deploying them in the CPSs extremely expensive. In other domains, such as web engineering, the phases of development and operation are tightly connected, making it possible to easily perform software updates of the system, and to obtain operational data that can be analyzed by engineers at development time. However, in spite of the rise of new communication technologies (e.g., 5G) providing an opportunity to acquire Design-Operation Continuum Engineering methods in\u00a0\u2026", "num_citations": "9\n", "authors": ["568"]}
{"title": "sortes: A supportive tool for stochastic scheduling of manual integration test cases\n", "abstract": " The main goal of software testing is to detect as many hidden bugs as possible in the final software product before release. Generally, a software product is tested by executing a set of test cases, which can be performed manually or automatically. The number of test cases which are required to test a software product depends on several parameters such as the product type, size, and complexity. Executing all test cases with no particular order can lead to waste of time and resources. Test optimization can provide a partial solution for saving time and resources which can lead to the final software product being released earlier. In this regard, test case selection, prioritization, and scheduling can be considered as possible solutions for test optimization. Most of the companies do not provide direct support for ranking test cases on their own servers. In this paper, we introduce, apply, and evaluate sOrTES as our decision\u00a0\u2026", "num_citations": "9\n", "authors": ["568"]}
{"title": "A novel methodology to classify test cases using natural language processing and imbalanced learning\n", "abstract": " Detecting the dependency between integration test cases plays a vital role in the area of software test optimization. Classifying test cases into two main classes \u2013 dependent and independent \u2013 can be employed for several test optimization purposes such as parallel test execution, test automation, test case selection and prioritization, and test suite reduction. This task can be seen as an imbalanced classification problem due to the test cases\u2019 distribution. Often the number of dependent and independent test cases is uneven, which is related to the testing level, testing environment and complexity of the system under test. In this study, we propose a novel methodology that consists of two main steps. Firstly, by using natural language processing we analyze the test cases\u2019 specifications and turn them into a numeric vector. Secondly, by using the obtained data vectors, we classify each test case into a dependent or an\u00a0\u2026", "num_citations": "8\n", "authors": ["568"]}
{"title": "An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications\n", "abstract": " Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the exponential Monte Carlo with counter (EMCQ). Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not (historically) be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to\u00a0\u2026", "num_citations": "8\n", "authors": ["568"]}
{"title": "Model-based product line engineering in an industrial automotive context: An exploratory case study\n", "abstract": " Product Line Engineering is an approach to reuse assets of complex systems by taking advantage of commonalities between product families. Reuse within complex systems usually means reuse of artifacts from different engineering domains such as mechanical, electronics and software engineering. Model-based systems engineering is becoming a standard for systems engineering and collaboration within different domains. This paper presents an exploratory case study on initial efforts of adopting Product Line Engineering practices within the model-based systems engineering process at Volvo Construction Equipment (Volvo CE), Sweden. We have used SysML to create overloaded models of the engine systems at Volvo CE. The variability within the engine systems was captured by using the Orthogonal Variability Modeling language. The case study has shown us that overloaded SysML models tend to\u00a0\u2026", "num_citations": "8\n", "authors": ["568"]}
{"title": "Exploring test overlap in system integration: An industrial case study\n", "abstract": " Tougher safety regulations, global competition and ever increasing complexity of embedded software puts extensive pressure on the effectiveness of the software testing procedures. Previous studies have found that there exist overlaps (i.e., multiple instances of highly similar test cases) and even redundancies in the software testing process. Such overlap has been found between versions, variants and integration levels, but primarily at unit test level. Given large embedded systems involving many subsystems, does overlap exist within the system integration testing as well? In this paper, we present an industrial case study, aiming to a) evaluate if there exist test overlaps within the given context, b) if so, investigate how these overlaps are distributed, and c) find ways of reducing test effort by investigating how the knowledge of overlaps and their distribution may be used for finding candidate test cases for automation\u00a0\u2026", "num_citations": "8\n", "authors": ["568"]}
{"title": "Process metrics are not bad predictors of fault proneness\n", "abstract": " The correct prediction of faulty modules or classes has a number of advantages such as improving the quality of software and assigning capable development resources to fix such faults. There have been different kinds of fault/defect prediction models proposed in literature, but a great majority of them makes use of static code metrics as independent variables for making predictions. Recently, process metrics have gained a considerable attention as alternative metrics to use for making trust-worthy predictions. The objective of this paper is to investigate different combinations of static code and process metrics for evaluating fault prediction performance. We have used publicly available data sets, along with a frequently used classifier, Naive Bayes, to run our experiments. We have, both statistically and visually, analyzed our experimental results. The statistical analysis showed evidence against any significant\u00a0\u2026", "num_citations": "7\n", "authors": ["568"]}
{"title": "An improved software reliability prediction model by using high precision error iterative analysis method\n", "abstract": " Software reliability deals with the probability that software will not cause the failure of a system in a specified time interval. Software reliability growth models (SRGMs) are used to predict future behaviour from known characteristics of software, like historical failures. With the increasing demand to deliver quality software, more accurate SRGMs are required to estimate the software release time and cost of the testing effort. Software failure predictions at early phases also provide an opportunity for investing in proper quality assurance and upfront resource planning. Up till now, many parametric software reliability growth models (PSRGMs) have been proposed. However, several limitations of them mean that their predictive capacities differ from one dataset to others. In this paper, to enhance the prediction accuracy of existing PSRGMs, a high precision error iterative analysis method (HPEIAM) has been proposed\u00a0\u2026", "num_citations": "6\n", "authors": ["568"]}
{"title": "A pragmatic perspective on regression testing challenges\n", "abstract": " Regression testing research has received significant focus during the past decades, acknowledging the benefits it can provide to organisations in terms of reduced development and maintenance costs, as well as sustained end-user satisfaction. There are several challenges left to overcome before the industry can fully take advantage of the available research results in this area. To get a better overview of how current regression testing research fits in with today's industrial practices, we read a selection of papers in the field and based on our experience, critically examined their content. As a result, we present and discuss a taxonomy of regression testing challenges, from the perspectives of both methods and organisations, that we believe will foster the industrial uptake of regression testing.", "num_citations": "6\n", "authors": ["568"]}
{"title": "Model-based testing in practice: An industrial case study using GraphWalker\n", "abstract": " Model-based testing (MBT) is a test design technique that supports the automation of software testing processes and generates test artefacts based on a system model representing behavioural aspects of the system under test (SUT). Previous research has shown some positive aspects of MBT such as low-cost test case generation and fault detection effectiveness. However, it is still a challenge for both practitioners and researchers to evaluate MBT tools and techniques in real, industrial settings. Consequently, the empirical evidence regarding the mainstream use, including the modelling and test case generation using MBT tools, is limited. In this paper, we report the results of a case study on applying GraphWalker, an open-source tool for MBT, on an industrial cyber-physical system (ie, a Train Control Management System developed by Bombardier Transportation in Sweden), from modelling of real-world\u00a0\u2026", "num_citations": "5\n", "authors": ["568"]}
{"title": "Towards a model-driven product line engineering process: An industrial case study\n", "abstract": " Many organizations developing software-intensive systems face challenges with high product complexity and large numbers of variants. In order to effectively maintain and develop these product variants, Product-Line Engineering methods are often considered, while Model-based Systems Engineering practices are commonly utilized to tackle product complexity. In this paper, we report on an industrial case study concerning the ongoing adoption of Product Line Engineering in the Model-based Systems Engineering environment at Volvo Construction Equipment (Volvo CE) in Sweden. In the study, we identify and define a Product Line Engineering process that is aligned with Model-based Systems Engineering activities at the engines control department of Volvo CE. Furthermore, we discuss the implications of the migration from the current development process to a Model-based Product Line Engineering-oriented\u00a0\u2026", "num_citations": "5\n", "authors": ["568"]}
{"title": "On the use of hackathons to enhance collaboration in large collaborative projects:-a preliminary case study of the MegaM@ Rt2 EU project\n", "abstract": " In this paper, we present the MegaM@Rt2 ECSEL project and discuss in details our approach for fostering collaboration in this project. We choose to use an internal hackathon approach that focuses on technical collaboration between case study owners and tool/method providers. The novelty of the approach is that we organize the technical workshop at our regular project progress meetings as a challenge-based contest involving all partners in the project. Case study partners submit their challenges related to the project goals and their use cases in advance. These challenges are concise enough to be experimented within approximately 4 hours. Teams are then formed to address those challenges. The teams include tool/method providers, case study owners and researchers/developers from other consortium members. On the hackathon day, partners work together to come with results addressing the challenges\u00a0\u2026", "num_citations": "5\n", "authors": ["568"]}
{"title": "Automating test data generation for testing context-aware applications\n", "abstract": " Context-aware applications are emerging applications in the modern era of computing. These applications can determine and adapt to situational context to provide better user experience. Testing these applications is not straightforward and poses several challenges such as developing context-aware test cases and generating test data etc. However, by employing model based testing technique, testing process for context-aware applications can be automated. To achieve maximum degree of automation, it is necessary to automate model transformation, test data generation and test cases execution. To execute test cases, test data is required and developing test data for context-aware applications is a challenging task. The aim of this study is to address this issue; thus, we propose automated test data generation for functional testing of context-aware applications. This automated test data generation can reduce\u00a0\u2026", "num_citations": "5\n", "authors": ["568"]}
{"title": "Search-based approaches to software fault prediction and software testing\n", "abstract": " Software verification and validation activities are essential for software quality but also constitute a large part of software development costs. Therefore efficient and costeffective software verification and validation activities are both a priority and a necessity considering the pressure to decrease time-to-market and intense competition faced by many, if not all, companies today. It is then perhaps not unexpected that decisions related to software quality, when to stop testing, testing schedule and testing resource allocation needs to be as accurate as possible. This thesis investigates the application of search-based techniques within two activities of software verification and validation: Software fault prediction and software testing for non-functional system properties. Software fault prediction modeling can provide support for making important decisions as outlined above. In this thesis we empirically evaluate symbolic regression using genetic programming (a search-based technique) as a potential method for software fault predictions. Using data sets from both industrial and open-source software, the strengths and weaknesses of applying symbolic regression in genetic programming are evaluated against competitive techniques. In addition to software fault prediction this thesis also consolidates available research into predictive modeling of other attributes by applying symbolic regression in genetic programming, thus presenting a broader perspective. As an extension to the application of search-based techniques within software verification and validation this thesis further investigates the extent of application of search-based techniques for testing\u00a0\u2026", "num_citations": "5\n", "authors": ["568"]}
{"title": "A Model-Based Test Script Generation Framework for Embedded Software\n", "abstract": " The abstract test cases generated through model-based testing (MBT) need to be concretized to make them executable on the software under test (SUT). Multiple re-searchers proposed different solutions, e.g., by utilizing adapters for concretization of abstract test cases and generation of test scripts. In this paper, we propose our Model-Based Test scrIpt GenEration fRamework (TIGER) based on GraphWalker, an open source MBT tool. The framework is capable of generating test scripts for embedded software controlling functions of a cyber physical system such as passenger trains developed at Bombardier Transportation AB. The framework follows some defined mapping rules for the concretization of abstract test cases. We have evaluated the generated test scripts using an industrial case study in terms of fault detection. We have induced faults in the model of the SUT based on three mutation operators to\u00a0\u2026", "num_citations": "4\n", "authors": ["568"]}
{"title": "MegaM@Rt2 Project: Mega-Modelling at Runtime - Intermediate Results and Research Challenges\n", "abstract": " MegaM@Rt2 Project is a major European effort towards the model-driven engineering of complex Cyber-Physical systems combined with runtime analysis. Both areas are dealt within the same methodology to enjoy the mutual benefits through sharing and tracking various engineering artifacts. The project involves 27 partners that contribute with diverse research and industrial practices addressing real-life case study challenges stemming from 9 application domains. These partners jointly progress towards a common framework to support those application domains with model-driven engineering, verification, and runtime analysis methods. In this paper, we present the motivation for the project, the current approach and the intermediate results in terms of tools, research work and practical evaluation on use cases from the project. We also discuss outstanding challenges and proposed approaches to\u00a0\u2026", "num_citations": "4\n", "authors": ["568"]}
{"title": "On measuring combinatorial coverage of manually created test cases for industrial software\n", "abstract": " Combinatorial coverage has been proposed as a way to measure the quality of test cases by using the input interaction characteristics. This paper describes the results of empirically measuring combinatorial coverage of manually created test cases by experienced industrial engineers working with embedded software development. We found that manual test cases achieve on average 78% 2-way combinatorial coverage, 57% 3-way coverage, 40% 4-way coverage, 20% 5-way combinatorial coverage and 13% for 6-way combinatorial coverage. These manual test cases can be augmented to achieve 100% combinatorial coverage for 2-way and 3-way interactions by adding eight and 66 missing test cases on average, respectively. For 4-way interactions, full combinatorial coverage can achieved by adding 658 missing test cases. For 5-way and 6-way interactions, full combinatorial coverage can be achieved by\u00a0\u2026", "num_citations": "4\n", "authors": ["568"]}
{"title": "Towards a more reliable store-and-forward protocol for mobile text messages\n", "abstract": " Businesses often use mobile text messages (SMS) as a cost effective and universal way of communicating concise information to their customers. Today, these messages are usually sent via SMS brokers, which forward them further to the next stakeholder, typically the various mobile operators, and then the messages eventually reach the intended recipients. Infoflex Connect AB delivers an SMS gateway application to the brokers with the main responsibility of reliable message delivery within set quality thresholds. However, the protocols used for SMS communication are not designed for reliability and thus messages may be lost. In this position paper we deduce requirements for a new protocol for routing messages through the SMS gateway application running at a set of broker nodes, in order to increase the reliability. The requirements cover important topics for the required communication protocol such as event\u00a0\u2026", "num_citations": "4\n", "authors": ["568"]}
{"title": "Transforming context-aware application development model into a testing model\n", "abstract": " Software testing aims at ensuring the quality of a software product. Context-aware applications are emerging applications that are capable to sense their environment and adapt to situational context to provide better user experience. Context-aware applications pose many challenges for software testing such as defining test adequacy criteria, generating test data, developing context-aware test cases etc. Test case generation process for context-aware applications can be automated using a model based testing technique. To attain this goal with maximum degree of automation, it is required to transform development model into a test model automatically. In this study, we propose a typecast of activity node of UML activity diagram, called Context-Aware Activity for modelling context-aware applications. We have also developed an approach for automatic transformation of the development model i.e., UML activity\u00a0\u2026", "num_citations": "4\n", "authors": ["568"]}
{"title": "A black-box approach to latency and throughput analysis\n", "abstract": " To enable fast and reliable delivery of mobile text messages (SMS), special bidirectional protocols are often used. Measuring the achieved throughput and involved latency is however non-trivial, due to the complexity of these protocols. Modifying an existing system would incur too much of a risk, so instead a new tool was created to analyse the log files containing information about this traffic in a black-box fashion. When the produced raw data was converted into graphs, they gave new insights into the behaviour of both the protocols and the remote systems involved.", "num_citations": "4\n", "authors": ["568"]}
{"title": "Simulation-based safety testing brake-by-wire\n", "abstract": " Mechanical systems in cars are replaced by electronic equivalents. To be authorized for the road, validation that the replacements are at least as good as the old systems is required. For electronic braking systems (brake-by-wire), this goodness translates to safety in terms of maintaining timing constraints. Yet, in the future, the safety of braking maneuvres will depend, not only, on electronic brakes, but also on cooperative driving maneuvres orchestrated among many cars. Connecting both brake-by-wire on the microscopic level with cooperative braking on the macroscopic level allows for determining safety on a broader scale, as both systems feed from the same resource: Time. This paper discusses work-in-progress, introducing and combining two threads: electronic brakes and cooperative braking. Discussing safety on two levels simultaneously motivates connecting a Simulink model of an electronic brake-by\u00a0\u2026", "num_citations": "3\n", "authors": ["568"]}
{"title": "Search-based testing for embedded telecommunication software with complex input structures: An industrial case study\n", "abstract": " In this paper, we discuss the application of search-based software testing techniques for unit level testing of a real-world telecommunication middleware at Ericsson. Input data for the system under test consists of nested data structures, and includes non-trivial variables such as uninitialized pointers. Our current implementation analyzes the existing test cases to discover how to handle pointers, set global system parameters, and any other setup code that needs to run before the actual test case. Hill climbing (HC) and (1+ 1) evolutionary algorithm (EA) metaheuristic search algorithms are used to generate input data for branch coverage. We compare HC,(1+ 1) EA, and random search as a baseline of performance with respect to effectiveness, measured as branch coverage, and efficiency, measured as number of executions needed. Difficulties arising from the specialized execution environment and the adaptations for handling these problems are also discussed.", "num_citations": "3\n", "authors": ["568"]}
{"title": "Search-based prediction of software quality: evaluations and comparisons\n", "abstract": " Software verification and validation (V&V) activities are critical for achieving software quality; however, these activities also constitute a large part of the costs when developing software. Therefore efficient and effective software V&V activities are both a priority and a necessity considering the pressure to decrease time-to-market and the intense competition faced by many, if not all, companies today. It is then perhaps not unexpected that decisions that affects software quality, eg, how to allocate testing resources, develop testing schedules and to decide when to stop testing, needs to be as stable and accurate as possible.The objective of this thesis is to investigate how search-based techniques can support decision-making and help control variation in software V&V activities, thereby indirectly improving software quality. Several themes in providing this support are investigated: predicting reliability of future software versions based on fault history; fault prediction to improve test phase efficiency; assignment of resources to fixing faults; and distinguishing fault-prone software modules from non-faulty ones. A common element in these investigations is the use of search-based techniques, often also called metaheuristic techniques, for supporting the V&V decision-making processes. Search-based techniques are promising since, as many problems in real world, software V&V can be formulated as optimization problems where near optimal solutions are often good enough. Moreover, these techniques are general optimization solutions that can potentially be applied across a larger variety of decision-making situations than other existing alternatives. Apart\u00a0\u2026", "num_citations": "3\n", "authors": ["568"]}
{"title": "Lessons from applying experimentation in software engineering prediction systems\n", "abstract": " Within software engineering prediction systems, experiments are undertaken primarily to investigate relationships and to measure/compare models\u2019 accuracy. This paper discusses our experience and presents useful lessons/guidelines in experimenting with software engineering prediction systems. For this purpose, we use a typical software engineering experimentation process as a baseline. We found that the typical experimentation process in software engineering is supportive in developing prediction systems and have highlighted issues more central to the domain of software engineering prediction systems.", "num_citations": "3\n", "authors": ["568"]}
{"title": "Fuzzy Adaptive Tuning of a Particle Swarm Optimization Algorithm for Variable-Strength Combinatorial Test Suite Generation\n", "abstract": " Combinatorial interaction testing is an important software testing technique that has seen lots of recent interest. It can reduce the number of test cases needed by considering interactions between combinations of input parameters. Empirical evidence shows that it effectively detects faults, in particular, for highly configurable software systems. In real-world software testing, the input variables may vary in how strongly they interact, variable strength combinatorial interaction testing (VS-CIT) can exploit this for higher effectiveness. The generation of variable strength test suites is a non-deterministic polynomial-time (NP) hard computational problem \\cite{BestounKamalFuzzy2017}. Research has shown that stochastic population-based algorithms such as particle swarm optimization (PSO) can be efficient compared to alternatives for VS-CIT problems. Nevertheless, they require detailed control for the exploitation and exploration trade-off to avoid premature convergence (i.e. being trapped in local optima) as well as to enhance the solution diversity. Here, we present a new variant of PSO based on Mamdani fuzzy inference system \\cite{Camastra2015,TSAKIRIDIS2017257,KHOSRAVANIAN2016280}, to permit adaptive selection of its global and local search operations. We detail the design of this combined algorithm and evaluate it through experiments on multiple synthetic and benchmark problems. We conclude that fuzzy adaptive selection of global and local search operations is, at least, feasible as it performs only second-best to a discrete variant of PSO, called DPSO. Concerning obtaining the best mean test suite size, the fuzzy adaptation even\u00a0\u2026", "num_citations": "2\n", "authors": ["568"]}
{"title": "Round-Trip time anomaly detection\n", "abstract": " Mobile text messages (SMS) are sometimes used for authentication, which requires short and reliable delivery times. The observed round-trip times when sending an SMS message provide valuable information on the quality of the connection. In this industry paper, we propose a method for detecting round-trip time anomalies, where the exact distribution is unknown, the variance is several orders of magnitude, and there are lots of shorter spikes that should be ignored. In particular, we show that using an adaption of Double Seasonal Exponential Smoothing to reduce the content dependent variations, followed by the Remedian to find short-term and long-term medians, successfully identifies larger groups of outliers. As training data for our method we use log files from a live SMS gateway. In order to verify the effectiveness of our approach, we utilize simulated data. Our contributions are a description on how to\u00a0\u2026", "num_citations": "2\n", "authors": ["568"]}
{"title": "Search-based testing for embedded telecom software with complex input structures\n", "abstract": " In this paper, we discuss the application of search-based software testing techniques for unit level testing of a real-world telecommunication middleware at Ericsson. Our current implementation analyzes the existing test cases to handle non-trivial variables such as uninitialized pointers, and to discover any setup code that needs to run before the actual test case, such as setting global system parameters. Hill climbing (HC) and (1+1) evolutionary algorithm (EA) metaheuristic search algorithms are used to generate input data for branch coverage. We compare HC, (1+1)EA, and random search with respect to effectiveness, measured as branch coverage, and efficiency, measured as number of executions needed. Difficulties arising from the specialized execution environment and the adaptations for handling these problems are also discussed.", "num_citations": "2\n", "authors": ["568"]}
{"title": "Industrial Scale Passive Testing with T-EARS\n", "abstract": " Passive testing continuously observes the system or system execution logs without any interference or instrumentation to test diverse combinations of functions, resulting in a more thorough evaluation over time. However, reaching a working solution to passive testing is not without challenges. While there have been some efforts to extract information from system requirements to create passive test cases, to our knowledge, no such efforts are mature enough to be applied in a real, industrial safety-critical context. Our passive testing approach uses the Timed Easy Approach to Requirements Syntax (T-EARS) specification language and its accompanying tool-chain. This study reports challenges and solutions to introducing system-level passive testing for a vehicular safety-critical system through industrial data analysis, including 116 safety-related requirements. Our results show that passive testing using the T-EARS\u00a0\u2026", "num_citations": "1\n", "authors": ["568"]}
{"title": "Detecting inconsistencies in annotated product line models\n", "abstract": " Model-based product line engineering applies the reuse practices from product line engineering with graphical modeling for the specification of software intensive systems. Variability is usually described in separate variability models, while the implementation of the variable systems is specified in system models that use modeling languages such as SysML. Most of the SysML modeling tools with variability support, implement the annotation-based modeling approach. Annotated product line models tend to be error-prone since the modeler implicitly describes every possible variant in a single system model. To identifying variability-related inconsistencies, in this paper, we firstly define restrictions on the use of SysML for annotative modeling in order to avoid situations where resulting instances of the annotated model may contain ambiguous model constructs. Secondly, inter-feature constraints are extracted from the\u00a0\u2026", "num_citations": "1\n", "authors": ["568"]}
{"title": "Model-based system engineering in practice: document generation-MegaM@ Rt2 project experience\n", "abstract": " [email protected] project is a collaborative initiative of the ECSEL Joint Undertaking under Horizon 2020 EU programme. The project regroups 26 partners from 6 different European countries who jointly address challenges of engineering modern cyber-physical systems by using model-based engineering methods. Since it is a model-based project, we adopted a similar approach for dealing with requirements analysis, architecture, design, roadmap planning and development status checking. In these tasks, document generation methods were particularly useful to create a set of\" live\" reference specifications and contractual reports. We believe that these methods perfectly demonstrate relevant benefits of the model-based approach and are applicable to many other contexts. Document generation has several challenges, since the produced documents should address several goals and target different audience\u00a0\u2026", "num_citations": "1\n", "authors": ["568"]}