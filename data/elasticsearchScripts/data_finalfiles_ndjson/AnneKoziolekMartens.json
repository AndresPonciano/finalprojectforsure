{"title": "Automated improvement of software architecture models for performance and other quality attributes\n", "abstract": " Quality attributes, such as performance or reliability, are crucial for the success of a software system and largely influenced by the software architecture. Their quantitative prediction supports systematic, goal-oriented software design and forms a base of an engineering approach to software design. This thesis proposes a method and tool to automatically improve component-based software architecture (CBA) models based on such quantitative quality prediction techniques.", "num_citations": "49\n", "authors": ["562"]}
{"title": "Architecture-Driven Quality Requirements Prioritization\n", "abstract": " Quality requirements are main drivers for architectural decisions of software systems. However, in practice they are often dismissed during development, because of initially unknown dependencies and consequences that complicate implementation. To decide for meaningful, feasible quality requirements and trade them off with functional requirements, tighter integration of software architecture evaluation and requirements prioritization is necessary. In this position paper, we propose a tool-supported method for architecture-driven feedback into requirements prioritization. Our method uses automated design space exploration based on quantitative quality evaluation of software architecture models. It helps requirements analysts and software architects to study the quality trade-offs of a software architecture, and use this information for requirements prioritization.", "num_citations": "21\n", "authors": ["562"]}
{"title": "Research preview: Prioritizing quality requirements based on software architecture evaluation feedback\n", "abstract": " [Context and motivation] Quality requirements are a main driver for architectural decisions of software systems. Although the need for iterative handling of requirements and architecture has been identified, current architecture design processes do not provide systematic, quantitative feedback for the prioritization and cost/benefit considerations for quality requirements. [Question/problem] Thus, in practice stakeholders still often state and prioritize quality requirements before knowing the software architecture, i.e. without knowledge about the quality dependencies, conflicts, incurred costs, and technical feasibility. However, as quality properties usually are cross-cutting architecture concerns, estimating the effects of design decisions is difficult. Thus, stakeholders cannot reliably know the appropriate required level of quality. [Principal ideas/results]  In this research proposal, we suggest an\u00a0\u2026", "num_citations": "18\n", "authors": ["562"]}
{"title": "Improved feedback for architectural performance prediction using software cartography visualizations\n", "abstract": " Software performance engineering provides techniques to analyze and predict the performance (e.g., response time or resource utilization) of software systems to avoid implementations with insufficient performance. These techniques operate on models of software, often at an architectural level, to enable early, design-time predictions for evaluating design alternatives. Current software performance engineering approaches allow the prediction of performance at design time, but often provide cryptic results (e.g., lengths of queues). These prediction results can be hardly mapped back to the software architecture by humans, making it hard to derive the right design decisions. In this paper, we integrate software cartography (a map technique) with software performance engineering to overcome the limited interpretability of raw performance prediction results. Our approach is based on model transformations\u00a0\u2026", "num_citations": "16\n", "authors": ["562"]}
{"title": "Assessing security to compare architecture alternatives of component-based systems\n", "abstract": " Modern software development is typically performed by composing a software system from building blocks. The component-based paradigm has many advantages. However, security quality attributes of the overall architecture often remain unspecified and therefore, these cannot be considered when comparing several architecture alternatives. In this paper, we propose an approach for assessing security of component-based software architectures. Our hierarchical model uses stochastic modeling techniques and includes several security related factors, such as attackers, his goals, the security attributes of a component, and the mutual security interferences between them. Applied on a component-based architecture, our approach yields its mean time to security failure, which assesses its degree of security. We extended the Palladio Component Model (PCM) by the necessary information to be able to use it as input\u00a0\u2026", "num_citations": "13\n", "authors": ["562"]}
{"title": "Towards Modeling and Analysis of Power Consumption of Self-Adaptive Software Systems in Palladio.\n", "abstract": " Architecture-level evaluations of Palladio currently lack support for the analysis of the power efficiency of software systems and the effect of power management techniques on other quality characteristics. This neglects that the power consumption of software systems constitutes a substantial proportion of their total cost of ownership. Currently, reasoning on the influence of design decisions on power consumption and making trade-off decisions with other Quality of Service (QoS) characteristics is deferred until a system is in operation. Reasoning approaches that evaluate a system\u2019s energy efficiency have not reached a suitable abstraction for architecture-level analyses. Palladio and its extension SimuLizar for self-adaptive systems lack support for specifying and reasoning on power efficiency under changing user load. In this paper, we (i) show our ideas on how power efficiency and trade-off decisions with other QoS characteristics can be evaluated for static and self-adaptive systems and (ii) propose additions to the Palladio Component Model (PCM) taking into account the power provisioning infrastructure and constraints.", "num_citations": "11\n", "authors": ["562"]}
{"title": "Continuous integration of performance model\n", "abstract": " Applying model-based performance prediction requires that an up-to-date Performance Model (PM) is available throughout the development process. Creating such a model manually is an expensive process that is unsuitable for agile software development aiming to produce rapid releases in short cycles. Existing approaches automate the extraction of a PM based on reverse engineering and/or measurements techniques. However, these approaches require to monitor and analyse the whole application. Thus, they are too costly to be applied frequently, up to after each code change. Moreover, keeping potential manual changes of the PM is another challenge as long the PM is regenerated from scratch every time. To address these problems, this paper envisions an approach for efficient continuous integration of a parametrised performance model in an agile development process. Our work will combine static code\u00a0\u2026", "num_citations": "10\n", "authors": ["562"]}
{"title": "Towards a conceptual model for unifying variability in space and time\n", "abstract": " Effectively managing variability in space and time is among the main challenges of developing and maintaining large-scale yet long-living software-intensive systems. Over the last decades, two large research fields, Software Configuration Management (SCM) and Software Product Line Engineering (SPLE), have focused on version management and the systematic handling of variability, respectively. However, neither research community has been successful in producing unified management techniques that are effective in practice, and both communities have developed largely independently of each other. As a step towards overcoming this unfortunate situation, in this paper, we report on ongoing work on conceiving a conceptual yet integrated model of SCM and SPLE concepts, originating from a recent Dagstuhl seminar on the unification of version and variant management. Our goal is to provide discussion\u00a0\u2026", "num_citations": "9\n", "authors": ["562"]}
{"title": "Considering transient effects of self-adaptations in model-driven performance analyses\n", "abstract": " Model-driven performance engineering allows software architects to reason on performance characteristics of a software system in early design phases. In recent years, model-driven analysis techniques have been developed to evaluate performance characteristics of self-adaptive software systems. These techniques aim to reason on the ability of a self-adaptive software system to fulfill performance requirements in transient phases. A transient phase is the interval in which the behavior of the system changes, e.g., due to a burst in user requests. However, the effectiveness and efficiency with which a system is able to adapt depends not only on the time when it triggers adaptation actions but also on the time at which they are completed. Executing an adaptation action can cause additional stress on the adapted system. This can further impede the performance of the system in the transient phase. Model-driven\u00a0\u2026", "num_citations": "9\n", "authors": ["562"]}
{"title": "View-based and Model-driven Outage Management for the Smart Grid\n", "abstract": " The integration of renewable energy resources is challenging the traditional electricity network. To manage this, the smart grid has been defined as a cyber-physical system consisting of a physical component, which is the electricity grid, and a computational component consisting of a communication network, metering network, and software components. Therefore, the smart grid can not just be seen as an electrical grid, but also as a system of software systems. Currently, control centers of the smart grid use an outage management software system to react to reported outages.In this paper, we present an extended outage management system that solves one main problem of the smart grid: software systems of the different domains are using different standards. Consequently, cross-domain data exchange and analysis are difficult. Therefore, we use the model-driven view-based VITRUVIUS approach to build a unified model of the smart grid. We then combine it with system stability analysis methods presented as views on the model. The result is a model-driven run-time monitoring, analysis and control framework to increase the reliability of power supply. The evaluation with statistical data of the German power grid shows that outage time can be reduced with our approach.", "num_citations": "9\n", "authors": ["562"]}
{"title": "Flexibility is key in organizing a global professional conference online: The icpe 2020 experience in the covid-19 era\n", "abstract": " Organizing professional conferences online has never been more timely. Responding to the new challenges raised by COVID-19, the organizers of the ACM/SPEC International Conference on Performance Engineering 2020 had to address the question: How should we organize these conferences online? This article summarizes their successful answer.", "num_citations": "8\n", "authors": ["562"]}
{"title": "Peropteryx: Automated improvement of software architectures\n", "abstract": " The design phase of software development processes is a decisive part of the quality characteristics of the later software system. If errors are not or only late discovered in early phases of the development, this often has strong effects on quality and costs of the project. Software architecture models systematically help to prevent errors in early phases, such as the design phase. On the basis of models, software architectures and their quality properties can be evaluated and optimized at design time. PerOpteryx supports the systematic process of evaluating and optimizing software architecture models early considering quality attributes, such as performance, reliability, costs. The approach automatically generates architecture candidates based on several degrees of freedom of component-based software architectures. PerOpteryx then automatically evaluates and optimizes these architecture candidates with regard to\u00a0\u2026", "num_citations": "8\n", "authors": ["562"]}
{"title": "Incremental calibration of architectural performance models with parametric dependencies\n", "abstract": " Architecture-based Performance Prediction (AbPP) allows evaluation of the performance of systems and to answer what-if questions without measurements for all alternatives. A difficulty when creating models is that Performance Model Parameters (PMPs, such as resource demands, loop iteration numbers and branch probabilities) depend on various influencing factors like input data, used hardware and the applied workload. To enable a broad range of what-if questions, Performance Models (PMs) need to have predictive power beyond what has been measured to calibrate the models. Thus, PMPs need to be parametrized over the influencing factors that may vary. Existing approaches allow for the estimation of the parametrized PMPs by measuring the complete system. Thus, they are too costly to be applied frequently, up to after each code change. Moreover, they do not keep manual changes to the model when\u00a0\u2026", "num_citations": "7\n", "authors": ["562"]}
{"title": "Towards consistency checking between software architecture and informal documentation\n", "abstract": " In the development process, documenting the software architecture is important to capture all reasoning and design decisions. Without a good and complete documentation, there is a lot of tacit knowledge that easily can get lost resulting in threats for success and increased costs. However, software architecture documentation is often missing or outdated. One reason for it is the tedious and costly process of creating and updating documentation in comparison to (perceived) low benefits. In this paper, we first present our long-term vision, where all information from any sources are persisted to avoid losing crucial information about a system. A base problem in this vision is keeping information from different sources consistent, with a major challenge of keeping consistency between models and informal documentation. We plan to address checking the consistency between models and textual natural language artefacts\u00a0\u2026", "num_citations": "7\n", "authors": ["562"]}
{"title": "Experience of pragmatically combining RE methods for performance requirements in industry\n", "abstract": " To meet end-user performance expectations, precise performance requirements are needed during development and testing, e.g., to conduct detailed performance and load tests. However, in practice, several factors complicate performance requirements elicitation: lacking skills in performance requirements engineering, outdated or unavailable functional specifications and architecture models, the specification of the system's context, lack of experience to collect good performance requirements in an industrial setting with very limited time, etc. From the small set of available non-functional requirements engineering methods, no method exists that alone leads to precise and complete performance requirements with feasible effort and which has been reported to work in an industrial setting. In this paper, we present our experiences in combining existing requirements engineering methods into a performance\u00a0\u2026", "num_citations": "6\n", "authors": ["562"]}
{"title": "Does BERT Understand Code?\u2013An Exploratory Study on the Detection of Architectural Tactics in Code\n", "abstract": " Quality-driven design decisions are often addressed by using architectural tactics that are re-usable solution options for certain quality concerns. Creating traceability links for these tactics is useful but costly. Automating the creation of these links can help reduce costs but is challenging as simple structural analyses only yield limited results. Transfer-learning approaches using language models like BERT are a recent trend in the field of natural language processing. These approaches yield state-of-the-art results for tasks like text classification. In this paper, we experiment with treating detection of architectural tactics in code as a text classification problem. We present an approach to detect architectural tactics in code by fine-tuning BERT. A 10-fold cross-validation shows promising results with an average -Score of 90%, which is on a par with state-of-the-art approaches. We additionally apply our\u00a0\u2026", "num_citations": "5\n", "authors": ["562"]}
{"title": "Automatic evaluation of complex design decisions in component-based software architectures\n", "abstract": " The quality of modern industrial plants depends on the quality of the hardware used, as well as software. While the impact on quality is comparably well understood by making decisions about the choice of hardware components, this is less true for the decisions on software components. The quality of the resulting software system is strongly influenced by its software architecture. Especially in early project phases a software architect has to make many design decisions. Each design decision highly influences the software architecture and thus, the resulting software quality. However, the impact on the resulting quality of architecture design decisions is hard to estimate in advance. For instance, a software architect could decide to deploy software components on a dedicated server in order to improve the system performance. However, such a decision may increase the network overhead as side-effect. Model-driven\u00a0\u2026", "num_citations": "5\n", "authors": ["562"]}
{"title": "Modelling the structure of reusable solutions for architecture-based quality evaluation\n", "abstract": " When designing cloud applications many decisions must be made like the selection of the right set of software components. Often, there are several third-party implementations on the market from which software architects have the choice between several solutions that are functionally very similar. Even though they are comparable in functionality, the solutions differ in their quality attributes, and in their software architecture. This diversity hinders automated decision support in model-driven engineering approaches, since current state-of-the-art approaches for automated quality estimation often rely on similar architectures to compare several solutions. In this paper, we address this problem by contributing with a metamodel that unifies the architecture of several functional similar solutions, and describes the different solutions' architectural degrees of freedom. Such a model can be used later to extend the process of\u00a0\u2026", "num_citations": "5\n", "authors": ["562"]}
{"title": "Considering not-quantified quality attributes in an automated design space exploration\n", "abstract": " In a software design process, the quality of the resulting software system is highly driven by the quality of its software architecture (SA). In such a process trade-off decisions must be made between multiple quality attributes (QAs), such as performance or security, that are often competing. Several approaches exist to improve SAs either quantitatively or qualitatively. The first group of approaches requires to quantify each single QA to be considered in the design process, while the latter group of approaches are often fully manual processes. However, time and cost constraints often make it impossible to either quantify all relevant QAs or manually evaluate candidate architectures. Our approach to the problem is to quantify several most important quality requirements, combine them with several not-quantified QAs and use them together in an automated design space exploration process. As our basis, we used the\u00a0\u2026", "num_citations": "5\n", "authors": ["562"]}
{"title": "Palladio optimization suite: QoS optimization for component-based cloud applications\n", "abstract": " One important issue in software engineering is to find an effective way to deal with the increasing complexity of software computing system. Modern software applications have evolved in terms of size and scope. Specific tools have been created to predict the Quality of Service (QoS) at design-time. However, the optimization of an architecture usually has to be done manually, resulting in an arduous and time-consuming process. For this reason, we present the Palladio Optimization Suite (POS), a collection of complementary plugins realized to run atop Palladio Bench with the aim of automatizing the exploration of the space of possible architectures by means of advanced search paradigms.", "num_citations": "5\n", "authors": ["562"]}
{"title": "Assessing the quality impact of features in component-based software architectures\n", "abstract": " In modern software development processes, existing software components are increasingly used to implement functionality instead of developing it from scratch. Reuse of individual components or even more complex subsystems leads to more cost-efficient development and higher quality of software. Subsystems often offer a variety of features whose use is associated with unclear effects on the quality attributes of the software architecture, such as performance. It is unclear, whether the quality requirements for the system can be met by using a certain feature of a particular subsystem. After initial selection, features must be incorporated in the target architecture. Due to a multitude of possibilities of placing the subsystem in the target system to be used, many architectural candidates may result which have to be evaluated in existing decision support solutions. The approach presented here enables software\u00a0\u2026", "num_citations": "4\n", "authors": ["562"]}
{"title": "Using informal knowledge for improving software quality trade-off decisions\n", "abstract": " To deliver high-quality software, in a software development process a variety of quality attributes must be considered such as performance, usability or security. In particular, quality attributes such as security and usability are difficult to analyze quantitatively. Knowledge about such quality attributes is often only informally available and therefore cannot be processed in structured and formalized decision-making approaches to optimize the software architecture. In this paper, we have defined a framework in order to make use of informally available knowledge in automated design decision support processes. We connect qualitative reasoning models with models for quantitative quality estimation to optimize software architectures regarding both knowledge representation models together. By our approach quality attributes for which no quantitative evaluation model is available can now be used in automated\u00a0\u2026", "num_citations": "3\n", "authors": ["562"]}
{"title": "Categories of Change Triggers in Business Processes\n", "abstract": " Business processes need to constantly adapt due to changes in their environment and requirements. Therefore, one of the main activities in business process management is the management of changes. To effectively manage changes, there is a need for categorization of change triggers in business processes. However, existing categories of change triggers are limited to information systems and neglect the change triggers of business processes. We conducted a review with a well-defined methodology to identify categories of change triggers in business processes. This paper presents a generic categorization scheme of change triggers in business processes based on the results of the review. The new categorization scheme can serve as a checklist to elicit the possible future business process changes and, thus, support the process of change and risk management.", "num_citations": "3\n", "authors": ["562"]}
{"title": "Supplementary material for the study on categories of change triggers in business processes\n", "abstract": " Nowadays, business processes in enterprises are supported by software systems and information technology (IT). Both domains face various changes during lifetime. In case of evolution and changes in the business domain, it is necessary to ensure that associated software systems/IT are still aligned in order to remain competitive [1]. Thus, the change management can be considered as one of the most important activities in the requirements engineering [3]. As a result, a business process can a ect the corresponding software system and vice versa. Thus, the change triggers in business processes need to be considered [4]. In the mid 1970s, the author Swanson de ned three dimensions in order to describe software maintenance activities. He distinguished between corrective, adaptive, and perfective maintenance based on a discussion on change triggers [6]. In the domain of information system, Swanson\u2019s categorization is used, adapted, or extended. In the domain of business processes, there are some papers examing a list of change triggers (eg,[5, 7]). However, there are not any systematic and comprehensive categorizations of change triggers in business processes. To address the limitations of existing approaches, we conducted a literature review to identify the change trigger categories in business processes [2]. In this technical report, we provide the supplementary material for our literature review [2]. We present our method for the database search. Thus, an overview of the search terms and the search engine queries for each database is given. Further, the number of document results for each search query and timespan of the analysis\u00a0\u2026", "num_citations": "3\n", "authors": ["562"]}
{"title": "Reference scenarios for self-aware computing\n", "abstract": " This chapter defines three reference scenarios to which other chapters may refer for the purpose of motivating and illustrating architectures, techniques, and methods consistently throughout the book. The reference scenarios cover a broad set of characteristics and issues that one may encounter in self-aware systems and represent a range of domains and a variety of scales and levels of complexity. The first scenario focuses on an adaptive sorting algorithm and exemplifies how a self-aware system may adapt to changes in the data on which it operates, the environment in which it executes, or the requirements or performance criteria to which it manages itself. The second focuses on self-aware multiagent applications running in a data center environment, allowing issues of collective behavior in cooperative and competitive self-aware systems to come to the fore. The third focuses on a cyber-physical system\u00a0\u2026", "num_citations": "3\n", "authors": ["562"]}
{"title": "Experience with Model-based Performance, Reliability and Adaptability Assessment of a Complex Industrial Architecture\n", "abstract": " In this paper, we report on our experience with the application of validated models to assess performance, reliability, and adaptability of a complex mission critical system that is being developed to dynamically monitor and control the position of an oil-drilling platform. We present real-time modeling results that show that all tasks are schedulable. We performed stochastic analysis of the distribution of task execution time as a function of the number of system interfaces. We report on the variability of task execution times for the expected system configurations. In addition, we have executed a system library for an important task inside the performance model simulator. We report on the measured algorithm convergence as a function of the number of vessel thrusters. We have also studied the system architecture adaptability by comparing the documented system architecture and the implemented source code. We\u00a0\u2026", "num_citations": "3\n", "authors": ["562"]}
{"title": "Enabling Consistency between Software Artefacts for Software Adaption and Evolution\n", "abstract": " Short development times of software became crucial to stay competitive. However, the quality should not suffer from the faster development processes, which is why increasingly more automation is gaining ground in this context. If models are involved in the development process and used for performance prediction, there are delays due to emerging inconsistencies between different software artifacts. The elimination of these inconsistencies is a time consuming, complex and error prone activity. Currently, there are already approaches for automated consistency preservation of software artifacts. Nevertheless, the limited scope in terms of supported change scenarios is a significant disadvantage.Therefore, we present a comprehensive approach for the maintenance of consistency between the system design and adaptive as well as evolutionary changes. In comparison to existing approaches, the consistency\u00a0\u2026", "num_citations": "2\n", "authors": ["562"]}
{"title": "Optimizing parametric dependencies for incremental performance model extraction\n", "abstract": " Model-based performance prediction in agile software development promises to evaluate design alternatives and to reduce the cost of performance tests. To minimize the differences between a real software and its performance model, parametric dependencies are introduced. They express how the performance model parameters (such as loop iteration count, branch transition probabilities, resource demands, and external service call arguments) depend on impacting factors like the input data. The approaches that perform model-based performance prediction in agile software development have two major shortcomings: they are either costly because they do not update the performance models automatically after each commit, or do not consider more complex parametric dependencies than linear. This work extends an approach for continuous integration of performance model during agile development. Our\u00a0\u2026", "num_citations": "2\n", "authors": ["562"]}
{"title": "The Effect of Temporal Aggregation on Battery Sizing for Peak Shaving\n", "abstract": " Battery systems can reduce the peak electrical consumption through proper charging and discharging strategies. To this end, consumers often rely on historic consumption data to select a cost-efficient battery system. However, historic data is an imperfect mapping of the real consumption, because of a coarse sampling rate or measurement inaccuracies. This can result in non-optimal decisions, eg, by underestimating the battery capacity required. In this article, we analyze how aggregation affects a state-of-the-art battery sizing algorithm for an industrial production site. We then use machine learning on a short period of high-resolution data to correct this error from historic data. Our experiments indicate that machine learning models can correct this error in some cases. However, adding a safety margin obtained from historic data to the battery size is a more reliable way of reducing the error.", "num_citations": "2\n", "authors": ["562"]}
{"title": "Data Stream Operations as First-Class Entities in Component-Based Performance Models\n", "abstract": " Data streaming applications are an important class of data-intensive systems. Performance is an essential quality of such systems. It is, for example, expressed by the delay of analysis results or the utilization of system resources. Architecture-level decisions such as the configuration of sources, sinks and operations, their deployment or the choice of technology impact the performance. Current component-based performance prediction approaches cannot accurately predict the performance of those systems, because they do not support the metrics that are specific to data streaming applications and only approximate the behavior of data stream operations instead of expressing it explicitly. In particular, operations that group multiple data events and thus introduce timing dependencies between different calls to the system are not represented sufficiently. In this paper, we present an approach for modeling\u00a0\u2026", "num_citations": "1\n", "authors": ["562"]}
{"title": "Flexibility Is Key in Organizing a Global Professional Conference Online: The ACM/SPEC ICPE 2020 Experience in the COVID-19 Era\n", "abstract": " Organizing professional conferences online has never been more timely. Responding to the new challenges raised by COVID-19, the organizers of the ACM/SPEC International Conference on Performance Engineering 2020 had to address the question: How should we organize these conferences online? This article summarizes their successful answer.", "num_citations": "1\n", "authors": ["562"]}
{"title": "Using BERT for the detection of architectural tactics in code\n", "abstract": " Quality-driven design decisions are often addressed by using architectural tactics that are re-usable solution options for certain quality concerns. However, it is not sufficient to only make good design decisions but also to review the realization of design decisions in code. As manual creation of traceability links for design decisions into code is costly, some approaches perform structural analyses to recover traceability links. However, architectural tactics are high-level solutions described in terms of roles and interactions and there is a wide range of possibilities to implement each. Therefore, structural analyses only yield limited results. Transfer-learning approaches using language models like BERT are a recent trend in the field of natural language processing. These approaches yield state-of-the-art results for tasks like text classification. We intent to experiment with BERT and present an approach to detect architectural tactics in code by fine-tuning BERT. A 10-fold cross-validation shows promising results with an average F1-Score of 90%, which is on a par with state-of-the-art approaches. We additionally apply our approach to a case study, where the results of our approach show promising potential but fall behind the state-of-the-art. Therefore, we discuss our approach and look at potential reasons and downsides as well as potential improvements.", "num_citations": "1\n", "authors": ["562"]}
{"title": "Towards consistency analysis between formal and informal software architecture artefacts\n", "abstract": " Documenting the architecture of a software system is important, especially to capture reasoning and design decisions. A lot of tacit knowledge can easily get lost when the documentation is incomplete, resulting in threats for the software system's success and increased costs. However, software architecture documentation is often missing or outdated. One explanation for this phenomenon is the tedious and costly process of creating documentation in comparison to (perceived) low benefits. In this paper, we first present our long-term vision, where we plan to persist information from any sources, e.g. from whiteboard discussions, to avoid losing crucial information about a system. A core problem in this vision is the possible inconsistency of information from different sources. A major challenge of ensuring consistency is the consistency between formal artefacts, i.e. models, and informal documentation. We plan to\u00a0\u2026", "num_citations": "1\n", "authors": ["562"]}
{"title": "Data stream operations as first-class entities in palladio\n", "abstract": " The Palladio Component Model (PCM) is an approach to simulate the performance of software systems using a component-based modeling language. When simulating PCM models, requests only influence each other if they compete for the same resources. However, for some applications, such as data stream processing, it is not realistic for requests to be this independent. For example, it is common to group requests in windows over time or to join data streams. Modeling the resulting behavior and resource demands in the system via stochastic approximations is possible but has drawbacks. It requires additional effort for determining the approximation and it may require spreading information across model elements that should be encapsulated in one place. In this paper, we propose a way of modeling interaction between requests that is similar to query languages for data streams. Thus, we introduce state into models without sacrificing the understandability and composability of the model.", "num_citations": "1\n", "authors": ["562"]}
{"title": "Deriving Power Models for Architecture-Level Energy Efficiency Analyses\n", "abstract": " In early design phases and during software evolution, design-time energy efficiency analyses enable software architects to reason on the effect of design decisions on energy efficiency. Energy efficiency analyses rely on accurate power models to estimate power consumption. Deriving power models that are both accurate and usable for design time predictions requires extensive measurements and manual analysis. Existing approaches that aim to automate the extraction of power models focus on the construction of models for runtime estimation of power consumption. Power models constructed by these approaches do not allow users to identify the central set of system metrics that impact energy efficiency prediction accuracy. The identification of these central metrics is important for design time analyses, as an accurate prediction of each metric incurs modeling effort. We propose a methodology for the\u00a0\u2026", "num_citations": "1\n", "authors": ["562"]}
{"title": "Session details: Special Issue on Challenges in Software Performance\n", "abstract": " Software systems have become increasingly complex. Such complexity makes it extremely challenging to deliver systems that meet their performance requirements. In recent years, there have been several high profile instances of projects that failed due to performance-related issues, e.g., the rollout of the healthcare.gov Web site. This motivates the need for new approaches for software developers to address performance problems. This special issue presents 6 papers that address this theme. A brief summary of these papers follows. Gesvindr and Buhnova study the challenges of building Platform as a Service (PaaS) cloud applications that meet their performance requirements. Using two case studies involving the Microsoft Azure cloud platform, the authors show that the use of a cloud platform in itself does not guarantee good performance and scalability. The authors then identify several best practices for\u00a0\u2026", "num_citations": "1\n", "authors": ["562"]}
{"title": "Twin Peaks goes agile\n", "abstract": " Activities related to software requirements engineering and software architecture significantly contribute to the success of software development projects. In software engineering practice, requirements and architecture affect each other and should not be treated in isolation. However, from a research and conceptual perspective, the dependencies between requirements and architecture are usually investigated by focusing on either requirements engineering or software architecture. Therefore, the Fifth International Workshop on the Twin Peaks of Requirements and Architecture explored the relationship between requirements and software architecture in the broader context of software engineering in general. Based on the outcomes of previous editions of the workshop, this edition aimed at exploring the relationship between requirements and architecture in the context of agile software development. In general, the\u00a0\u2026", "num_citations": "1\n", "authors": ["562"]}