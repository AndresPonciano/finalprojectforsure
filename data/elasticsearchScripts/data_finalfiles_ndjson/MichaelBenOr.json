{"title": "Completeness theorems for non-cryptographic fault-tolerant distributed computation\n", "abstract": " Every function of n inputs can be efficiently computed by a complete network of n processors in such a way that:", "num_citations": "3157\n", "authors": ["501"]}
{"title": "Verifiable secret sharing and multiparty protocols with honest majority\n", "abstract": " Under the assumption that each participant can broadcast a message to all other participants and that each pair of participants can communicate secretly, we present a verifiable secret sharing protocol, and show that any multiparty protocol, or game with incomplete information, can be achieved if a majority of the players are honest. The secrecy achieved is unconditional and does not rely on any assumption about computational intractability. Applications of these results to Byzantine Agreement are also presented.", "num_citations": "1232\n", "authors": ["501"]}
{"title": "Fault-tolerant quantum computation with constant error rate\n", "abstract": " This paper shows that quantum computation can be made fault-tolerant against errors and inaccuracies when , the probability for an error in a qubit or a gate, is smaller than a constant threshold . This result improves on Shor's result [Proceedings of the 37th Symposium on the Foundations of Computer Science, IEEE, Los Alamitos, CA, 1996, pp. 56\u201365], which shows how to perform fault-tolerant quantum computation when the error rate  decays polylogarithmically with the size of the computation, an assumption which is physically unreasonable. The cost of making the quantum circuit fault-tolerant in our construction is polylogarithmic in time and space. Our result holds for a very general local noise model, which includes probabilistic errors, decoherence, amplitude damping, depolarization, and systematic inaccuracies in the gates. Moreover, we allow exponentially decaying correlations between the errors both in\u00a0\u2026", "num_citations": "943\n", "authors": ["501"]}
{"title": "Another advantage of free choice (Extended Abstract) Completely asynchronous agreement protocols\n", "abstract": " Recently, Fischer, Lynch and Paterson [3] proved that no completely asynchronous consensus protocol can tolerate even a single unannounced process death. We exhibit here a probabilistic solution for this problem, which guarantees that as long as a majority of the processes continues to operate, a decision will be made (Theorem 1). Our solution is completely asynchronous and is rather strong: As in [4], it is guaranteed to work with probability 1 even against an adversary scheduler who knows all about the system.", "num_citations": "917\n", "authors": ["501"]}
{"title": "Lower bounds for algebraic computation trees\n", "abstract": " A topological method is given for obtaining lower bounds for the height of algebraic computation trees, and algebraic decision trees. Using this method we are able to generalize, and present in a uniform and easy way, almost all the known nonlinear lower bounds for algebraic computations. Applying the method to decision trees we extend all the apparently known lower bounds for linear decision trees to bounded degree algebraic decision trees, thus answering the open questions raised by Steele and Yao [20]. We also show how this new method can be used to establish lower bounds on the complexity of constructions with ruler and compass in plane Euclidean geometry.", "num_citations": "738\n", "authors": ["501"]}
{"title": "Multi-prover interactive proofs: How to remove intractability assumptions\n", "abstract": " Quite complex cryptographic machinery has been developed based on the assumption that one-way functions exist, yet we know of only a few possible such candidates. It is important at this time to find alternative foundations to the design of secure cryptography. We introduce a new model of generalized interactive proofs as a step in this direction. We prove that all NP languages have perfect zero-knowledge proof-systems in this model, without making any intractability assumptions.", "num_citations": "613\n", "authors": ["501"]}
{"title": "A deterministic algorithm for sparse multivariate polynomial interpolation\n", "abstract": " An efficient deterministic polynomial time algorithm is developed for the sparse polynomial interpolation problem. The number of evaluations needed by this algorithm is very small. The algorithm also has a simple NC implementation.", "num_citations": "397\n", "authors": ["501"]}
{"title": "The complexity of elementary algebra and geometry\n", "abstract": " In his 1948 paper\" A Decision Method for Elementary Algebra and Geometry,\" Alfred Tarski gave a decision procedure for the first-order theory of the real numbers with+,\", and=, commonly known as the theory of real closed fields. His decision procedure was nonelementary (in the complexity-theoretic sense). An elementary decision procedure was given by Leonard Monk in his Berkeley PhD thesis, and in 1974, double-exponential-time decision procedures were given independently by Collins and by Monk/Solovay. Various improvements and heuristics notwithstanding, that worst-case bound has stood since that time.", "num_citations": "393\n", "authors": ["501"]}
{"title": "Collective coin flipping, robust voting schemes and minima of Banzhaf values\n", "abstract": " The power of players in a collective decision process is a central issue in Mathematical Economics and Game Theory. Similar issues arise in Computer Science in the study of distributed, fault tolerant computations when several processes, some perhaps faulty, have to reach agreement. In the present article we study voting schemes which are relatively immune to the presence of unfair players. In particular, we discuss how to perform collective coin flipping which is only slightly biased despite the presence of unfair players. Mathematically this corresponds to problems concerning the minima of Banzhaf values in certain n -person games. These are measures of power studied in Game Theory. It is quite remarkable that while dictatorial voting games are, of course, the most sensitive to the presence of unfair players, some voting schemes that we propose here are significantly more robust than majority voting. Coin\u00a0\u2026", "num_citations": "372\n", "authors": ["501"]}
{"title": "A fair protocol for signing contracts\n", "abstract": " Two parties, A and B, want to sign a contract C over a communication network. To do so, they must simultaneously exchange their commitments to C. Since simultaneous exchange is usually impossible in practice, protocols are needed to approximate simultaneity by exchanging partial commitments in piece-by-piece manner. During such a protocol, one party or another may have a slight advantage; a fair protocol keeps this advantage within acceptable limits. A new protocol is proposed. It is fair in the sense that, at any stage in its execution, the conditional probability that one party cannot commit both parties to the contract given that the other party can, is close to zero. This is true even if A and B have vastly different computing powers and is proved under very weak cryptographic assumptions.< >", "num_citations": "356\n", "authors": ["501"]}
{"title": "Everything provable is provable in zero-knowledge\n", "abstract": " Assuming the existence of a secure probabilistic encryption scheme, we show that every language that admits an interactive proof admits a (computational) zero-knowledge interactive proof. This result extends the result of Goldreich, Micali and Wigderson, that, under the same assumption, all of NP admits zero-knowledge interactive proofs. Assuming envelopes for bit commitment, we show tht every language that admits an interactive proof admits a perfect zero-knowledge interactive proof.", "num_citations": "303\n", "authors": ["501"]}
{"title": "The universal composable security of quantum key distribution\n", "abstract": " The existing unconditional security definitions of quantum key distribution (QKD) do not apply to joint attacks over QKD and the subsequent use of the resulting key. In this paper, we close this potential security gap by using a universal composability theorem for the quantum setting. We first derive a composable security definition for QKD. We then prove that the usual security definition of QKD still implies the composable security definition. Thus, a key produced in any QKD protocol that is unconditionally secure in the usual definition can indeed be safely used, a property of QKD that is hitherto unproven. We propose two other useful sufficient conditions for composability. As a simple application of our result, we show that keys generated by repeated runs of QKD degrade slowly.", "num_citations": "223\n", "authors": ["501"]}
{"title": "Asynchronous secure computation\n", "abstract": " We initiate a study of security in asynchronous networks. We consider a completely asynchronous network where every two parties are connected via a private channel, and some of the parties may be faulty. We start by defining secure computation in this model. Our definition adapts the underlying principles of defining security(ie, comparing the computation to a computation in the presence of a trusted party) to the asynchronous model. In particular, our definition takes into account the fact that the computation must be completed even if we never hear from the faulty parties.Next, we show that whatever can be securely computed in an asynchronous network in the presence of a trusted party, can be securely computed in a network in which no such trusted party exists. We distinguish two types of faults. In case of Fail-Stop faults, our construction is valid as long as the faulty parties constitute less than a thzr-d of the\u00a0\u2026", "num_citations": "217\n", "authors": ["501"]}
{"title": "Probabilistic algorithms in finite fields\n", "abstract": " In this paper we deal with three computational problems in finite fields. The questions we deal with are the following:(i) Irreducible Polynomials-Finding an irreducible polynomial of degree n over GF (q).(ii) Root Finding-Given a polynomial of degree nf (x) E GF (q)[x], we wish to find all the roots of f (x)= 0 in GF (q).(iii) Factorization-Given a polynomial f (x) E GF (q)[x], we want to find the factorization f= fl\u2022\u2022\u2022 f r of f into its irreducible factors fi (x) E GF (q)[x].", "num_citations": "196\n", "authors": ["501"]}
{"title": "Computing algebraic formulas using a constant number of registers\n", "abstract": " It is shown that, over an arbitrary ring, the functions computed by polynomial-size algebraic formulas are also computed by polynomial-length algebraic straight-line programs that use only three registers. This was previously known for Boolean formulas [D. A. Barrington, J. Comput. System Sci., 38 (1989), pp. 150\u2013164], which are equivalent to algebraic formulas over the ring . For formulas over arbitrary rings, the result is an improvement over previous methods that require the number of registers to be logarithmic in the size of the formulas in order to obtain polynomial-length straight-line programs. Moreover, the straight-line programs that arise in these constructions have the property that they consist of statements whose actions on the registers are linear and bijective. A consequence of this is that the problem of determining the iterated product of  matrices is complete (under P-projections) for algebraic \u00a0\u2026", "num_citations": "175\n", "authors": ["501"]}
{"title": "Interactive proofs for quantum computations\n", "abstract": " The widely held belief that BQP strictly contains BPP raises fundamental questions: if we cannot efficiently compute predictions for the behavior of quantum systems, how can we test their behavior? In other words, is quantum mechanics falsifiable? In cryptographic settings, how can a customer of a future untrusted quantum computing company be convinced of the correctness of its quantum computations? To provide answers to these questions, we define Quantum Prover Interactive Proofs (QPIP). Whereas in standard interactive proofs the prover is computationally unbounded, here our prover is in BQP, representing a quantum computer. The verifier models our current computational capabilities: it is a BPP machine, with access to only a few qubits. Our main theorem states, roughly: 'Any language in BQP has a QPIP, which also hides the computation from the prover'. We provide two proofs, one based on a quantum authentication scheme (QAS) relying on random Clifford rotations and the other based on a QAS which uses polynomial codes (BOCG+ 06), combined with secure multiparty computation methods. This is the journal version of work reported in 2008 (ABOE08) and presented in ICS 2010; here we have completed the details and made the proofs rigorous. Some of the proofs required major modifications and corrections. Notably, the claim that the polynomial QPIP is fault tolerant was removed. Similar results (with different protocols) were reported independently around the same time of the original version in BFK08. The initial independent works (ABOE08, BFK08) ignited a long line of research of blind verifiable quantum computation\u00a0\u2026", "num_citations": "168\n", "authors": ["501"]}
{"title": "Asynchronous secure computations with optimal resilience\n", "abstract": " We investigate the problem of multiparty computations in a fully connected, asynchronous network of n players, in which up to t Byzantine faults may occur. It was shown in [BCG93] that secure error-less multiparty computation is possible in this setting if and only if t< n/4. We show that when exponentially small probability of error is allowed, this task can be achieved even when the number of faults is in the range n/4~ t< n/3. From the lower bounds of [BCG93] for the asynchronous fail-stop model it follows that the resilience, t< n/3, of our protocol is optimal.We describe an ([~ 1\u2013I)-resilient protocol that securely computes any function F. With overwhelming probability all the non-faulty players complete the execution of the protocol. Given that all the honest players terminate the protocol, they do so in time polynomial in n, in the boolean complexity of 3, and in Pog-$1, where c is the error probability.", "num_citations": "168\n", "authors": ["501"]}
{"title": "A theorem on probabilistic constant depth computations\n", "abstract": " I {t~ txml, ly there has been much interest ill tht, comput: Ltional power of circuits o1'bounded dt; pth. In particular Furst, Saxe and Sipser [FSS], and indcpentlently Ajtai [Aj] in a different form, have shown that no polynomial size circuits of bounded depth can compute the parity function of n boolean variables (and other related functions such as the majority function, the exact number of ones in the input, etc.). On the other hand, Stockmeyer [St] showed that probabilistic bounded depth circuits can approximate the exact number of ones in the input with very low probability of error. Tlmse results lead to the following interesting question:(*) Are probabilistic constant depth circuits more powerful than deterministic ones? While it is well known [BG] that (non uniform) deterministic polynomial size circuits are as powerful as probabilistic ones, the question is still open for bounded depth circuits. This is so because the reduction\u00a0\u2026", "num_citations": "159\n", "authors": ["501"]}
{"title": "Secure multiparty quantum computation with (only) a strict honest majority\n", "abstract": " Secret sharing and multiparty computation (also called \"secure function evaluation\") are fundamental primitives in modern cryptography, allowing a group of mutually distrustful players to perform correct, distributed computations under the sole assumption that some number of them will follow the protocol honestly. This paper investigates how much trust is necessary $that is, how many players must remain honest - in order for distributed quantum computations to be possible. We present a verifiable quantum secret sharing (VQSS) protocol, and a general secure multiparty quantum computation (MPQC) protocol, which can tolerate any cheaters among n players. Previous protocols for these tasks tolerated lfloor (n - 1)/4 rfloor and lfloor (n - 1)/6 rfloor cheaters, respectively. The threshold we achieve is tight - even in the classical case, \"fair\" multiparty computation is not possible if any set of n/2 players can cheat. Our\u00a0\u2026", "num_citations": "105\n", "authors": ["501"]}
{"title": "Multiprover interactive verification system\n", "abstract": " In a multiparty verification system, a prover and a verifier are coupled torocess respective outputs to provide a system output such as an identification verification. The prover is formed of plural units which share confidential information used to encrypt information carried by the prover. Communication between the prover units is prevented. The first prover unit encrypts the information based on additional information received from the verifier and transfers the encrypted information to the verifier. Subsequently, the verifier obtains from the second prover unit the shared confidential information required to decrypt a subset of the transmitted encrypted information.", "num_citations": "97\n", "authors": ["501"]}
{"title": "A fast parallel algorithm for determining all roots of a polynomial with real roots\n", "abstract": " Given a polynomial  of degree n with m bit integer coefficients and an integer , the problem of determining all its roots with error less than  is considered. It is shown that this problem is in the class NC if  has all real roots. Some very interesting properties of a Sturm sequence of a polynomial with distinct real roots are proved and used in the design of a fast parallel algorithm for this problem. Using Newton identities and a novel numerical integration scheme for evaluating a contour integral to high precision, this algorithm determines good approximations to the linear factors of .", "num_citations": "81\n", "authors": ["501"]}
{"title": "Fast quantum Byzantine agreement\n", "abstract": " We present a fast quantum Byzantine Agreement protocol that can reach agreement in O (1) expected communication rounds against a strong full information, dynamic adversary, tolerating up to the optimal t\u2039 n3 faulty players in the synchronous setting, and up to t\u2039 n4 faulty players for asynchronous systems. This should be contrasted with the known classical synchronous lower bound of \u03a9 (\u221a nlog n)[3] when t=(n).", "num_citations": "76\n", "authors": ["501"]}
{"title": "The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well)\n", "abstract": " We use a Bayesian approach to optimally solve problems in noisy binary search. We deal with two variants:1. Each comparison is erroneous with independent probability 1-p. 2. At each stage k comparisons can be performed in parallel and a noisy answer is returned. We present a (classical) algorithm which solves both variants optimally (with respect to p and k), up to an additive term of O(loglog n), and prove matching information-theoretic lower bounds. We use the algorithm to improve the results of Farhi et al., presenting an exact quantum search algorithm in an ordered list of expected complexity less than (log 2  n)/3.", "num_citations": "73\n", "authors": ["501"]}
{"title": "On the cryptographic security of single RSA bits\n", "abstract": " The ability to \u201chide\u201d one bit in trapdoor functions has recently gained much interest in cryptography research, and is of great importance in many transactions protocols. In this paper we study the cryptographic security of RSA bits. In particular, we show that unless the cryptanalyst can completely break the RSA encryption, any heuristic he uses to determine the least significant bit of the cleartext must have an error probability greater than 1/4\u2014\u03b5 A similar result is shown for Rabin's encryption scheme.", "num_citations": "69\n", "authors": ["501"]}
{"title": "Polynomial simulations of decohered quantum computers\n", "abstract": " Recently it has become clear, that a key issue in quantum computation is understanding how interaction with the environment, or \"decoherence\", affects the computational power of quantum computers. We adopt the standard physical method of describing systems which are interwound with their environment by \"density matrices\", and within this framework define a model of decoherence in quantum computation. Our results show that the computational power of decohered quantum computers depends strongly on the amount of parallelism in the computation. We first present a simulation of decohered sequential quantum computers, on a classical probabilistic Turing machine, and prove that the expected slowdown of this simulation is polynomial in time and space of the quantum computation, for any non zero decoherence rate. Similar results hold for quantum computers that are allowed to operate on logarithmic\u00a0\u2026", "num_citations": "62\n", "authors": ["501"]}
{"title": "Resilient-optimal interactive consistency in constant time\n", "abstract": " For a complete network of n processors within which communication lines are private, we show how to achieve concurrently many Byzantine Agreements within constant expected time both on synchronous and asynchronous networks. As an immediate consequence, this provides a solution to the Interactive Consistency problem. Our algorithms tolerate up to (n-1)/3 faulty processors in both the synchronous and asynchronous cases and are therefore resilient-optimal.                 In terms of time complexity, our results improve a time bound of  (for n concurrent agreements) which is immediately implied by the constant expected time Byzantine Agreement of Feldman and Micali (synchronous systems) and of Canetti and Rabin (asynchronous systems). In terms of resiliency, our results improve the resiliency bound of the constant time, $O(\\sqrt[4]{n})$-resilient algorithm of Ben-Or.                 An immediate\u00a0\u2026", "num_citations": "59\n", "authors": ["501"]}
{"title": "Simple algorithms for approximating all roots of a polynomial with real roots\n", "abstract": " We present a simple algorithm for approximating all roots of a polynomial p(x) when it has only real roots. The algorithm is based on some interesting properties of the polynomials appearing in the Extended Euclidean Scheme for p(x) and p\u2032(x). For example, it turns out that these polynomials are orthogonal; as a consequence, we are able to limit the precision required by our algorithm in intermediate steps. A parallel implementation of this algorithm yields a P-uniform NC2 circuit, and the bit complexity of its sequential implementation is within a polylog factor of the bit complexity of the best known algorithm for the problem.", "num_citations": "55\n", "authors": ["501"]}
{"title": "General security definition and composability for quantum & classical protocols\n", "abstract": " We generalize the universally composable definition of Canetti to the Quantum World. The basic idea is the same as in the classical world. The main contribution is that we unfold the result in a new model which is well adapted to quantum protocols. We also simplify some aspects of the classical case. In particular, the case of protocols with an arbitrary number of layers of sub-protocols is naturally covered in the proposed model.", "num_citations": "52\n", "authors": ["501"]}
{"title": "Limitations of noisy reversible computation\n", "abstract": " Noisy computation and reversible computation have been studied separately, and it is known that they are as powerful as unrestricted computation. We study the case where both noise and reversibility are combined and show that the combined model is weaker than unrestricted computation. In our noisy reversible circuits, each wire is flipped with probability p each time step, and all the inputs to the circuit are present in time 0. We prove that any noisy reversible circuit must have size exponential in its depth in order to compute a function with high probability. This is tight as we show that any circuit can be converted into a noise-resistant reversible one with a blow up in size which is exponential in the depth. This establishes that noisy reversible computation has the power of the complexity class NC^1. We extend this to quantum circuits(QC). We prove that any noisy QC which is not worthless, and for which all inputs are present at time 0, must have size exponential in its depth. (This high-lights the fact that fault tolerant QC must use a constant supply of inputs all the time.) For the lower bound, we show that quasi-polynomial noisy QC are at least powerful as logarithmic depth QC, (or QNC^1). Making these bounds tight is left open in the quantum case.", "num_citations": "52\n", "authors": ["501"]}
{"title": "A tight lower bound for randomized synchronous consensus\n", "abstract": " We prove tight upper and lower bounds of@(t/J-) on the expected number of rounds needed for randomized synchronous consensus protocols for a fail-stop, full information, dynamic adversary. In particular this proves that some restrictions are needed on the power of the adversary to allow randomized constant expected number of rounds protocols.", "num_citations": "49\n", "authors": ["501"]}
{"title": "Byzantine agreement in the full-information model in o (log n) rounds\n", "abstract": " We present a randomized Byzantine Agreement (BA) protocol with an expected running time of O (log n) rounds, in a synchronous full-information network of n players. For any constant \u03b5> 0, the constructed protocol tolerates t non-adaptive Byzantine faults, as long as n\u2265(4+ \u03b5) t. In the full-information model, no restrictions are placed on the computational power of the faulty players or the information available to them. In particular, the faulty players may be infinitely powerful, and they can observe all communication among the honest players. This constitutes significant progress over the best known randomized BA protocol in the same setting which has a round-complexity of \u0398 (t/log n) rounds [9], and answers an open problem posed by Chor and Dwork [10].", "num_citations": "48\n", "authors": ["501"]}
{"title": "Non-abelian homomorphism testing, and distributions close to their self-convolutions\n", "abstract": " In this paper, we study two questions related to the problem of testing whether a function is close to a homomorphism. For two finite groups G,H (not necessarily Abelian), an arbitrary map f:G \u2192 H, and a parameter 0 < \u03b5 <1, say that f is \u03b5-close to a homomorphism if there is some homomorphism g such that g and f differ on at most \u03b5 |G| elements of G, and say that f is \u03b5-far otherwise. For a given f and \u03b5, a homomorphism tester should distinguish whether f is a homomorphism, or if f is \u03b5-far from a homomorphism. When G is Abelian, it was known that the test which picks O(1/\u03b5) random pairs x,y and tests that f(x)+f(y)=f(x+y) gives a homomorphism tester. Our first result shows that such a test works for all groups G.               Next, we consider functions that are close to their self-convolutions. Let A = { a                                            g                  | g \u2208 G} be a distribution on G. The self-convolution of A, A\u2032 = { a\u2032                   g\u00a0\u2026", "num_citations": "48\n", "authors": ["501"]}
{"title": "A fair protocol for signing contracts\n", "abstract": " Assume that two parties, A and B, want to sign a contract over a communication network, i.e. they want to exchange their \u201ccommitments\u201c to the contract. We consider a contract signing protocol to be fair if, at any stage in its execution, the following hold: the conditional probability that party A obtains B's signature to the contract given that B has obtained A's signature to the contract, is close to 1. (Symmetrically, when switching the roles of A and B).             Contract signing protocols cannot be fair without relying on a trusted third party. We present a fair, cryptographic protocol for signing contracts that makes use of the weakest possible form of a trusted third party (judge). If both A and B are honest, the judge will never be called upon. Otherwise, the judge rules by performing a simple computation, without referring to previous verdicts. Thus, no bookkeeping is required from the judge. Our protocol is fair even if A\u00a0\u2026", "num_citations": "44\n", "authors": ["501"]}
{"title": "Simple and secure quantum key distribution with biphotons\n", "abstract": " The best qubit one-way quantum-key-distribution (QKD) protocol can tolerate up to 14.6% in the error rate. It has been shown how this rate can be increased by using larger quantum systems. The polarization state of a biphoton can encode a three-level quantum system\u2014a qutrit. The realization of a QKD system with biphotons encounters several problems in generating, manipulating, and detecting such photon states. We define those limitations and find within them a few protocols that perform almost as well as the ideal qutrit protocol. One advantage is that these protocols can be implemented with minor modifications into existing single photon systems. The security of one protocol is proved for the most general coherent attacks and the largest acceptable error rate for this protocol is found to be around 17.7%. For comparison, the security of the best possible qutrit protocol of four mutually unbiased bases was also\u00a0\u2026", "num_citations": "30\n", "authors": ["501"]}
{"title": "Fast asynchronous Byzantine agreement\n", "abstract": " Fast asynchronous Byzantine agreement (extended abstract) | Proceedings of the fourth annual ACM symposium on Principles of distributed computing ACM Digital Library Logo ACM Logo Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search podc Conference Proceedings Upcoming Events Authors Affiliations Award Winners More HomeConferencesPODCProceedingsPODC '85Fast asynchronous Byzantine agreement (extended abstract) ARTICLE Fast asynchronous Byzantine agreement (extended abstract) Share on Author: Michael Ben-Or profile image Michael Ben-Or View Profile Authors Info & Affiliations Publication: PODC '85: Proceedings of the fourth annual ACM symposium on Principles of distributed computingAugust 1985 Pages 149:'\u2026", "num_citations": "27\n", "authors": ["501"]}
{"title": "Completeness theorems for non-cryptographic fault-tolerant distributed computing\n", "abstract": " Completeness Theorems for Non-cryptographic Fault-tolerant Distributed Computing | Avi Wigderson Skip to main content Avi Wigderson Personal CV Short Bio Contact Works Book: Math and Computation Publications Talks Surveys Efficient Universe Post-Docs Students CSDM Seminars Conferences & videos Optimization, Complexity and Invariant Theory Avi60 Lens of Computation on the Sciences Pseudorandomness You are here Home \u00bb Avi Wigderson \u00bb Publications Completeness Theorems for Non-cryptographic Fault-tolerant Distributed Computing Submitted by smcneil on Mon, 2012-01-16 00:00 Author: S. Goldwasser M. Ben-Or A. Wigderson Publication: Proceedings of the 20th Symposium on the Theory of Computing (STOC), pp. 1-10, May 1988. Year: 1988 Abstract: http://www.math.ias.edu/~avi/PUBLICATIONS/ABSTRACT/gbw88.pdf Files File: Proceedings version (pdf) Proceedings version (ps) \u2039 of -\u203a \u2026", "num_citations": "26\n", "authors": ["501"]}
{"title": "Computing with faulty arrays\n", "abstract": " We present and O (1) slowdown emulation of a fault-free N x N two dimensional mesh with a slack of O (log N log log N) by a faulty mesh of the same size and slack. All components of the faulty mesh, including the memory modules, are assumed to be subject to failure. The faults may occur at any time during the emulation and the system readjusts dynamically.", "num_citations": "25\n", "authors": ["501"]}
{"title": "Efficient identification schemes using two prover interactive proofs\n", "abstract": " We present two efficient identification schemes based on the difficulty of solving the subset sum problem and the circuit satisfiability problem. Both schemes use the two prover model introduced by [BGKW], where the verifier (e.g the Bank) interacts with two untrusted provers (e.g two bank identification cards) who have jointly agreed on a strategy to convince the verifier of their identity. To believe the validity of their identity proving procedure, the verifier must make sure that the two provers can not communicate with each other dur- ing the course of the proof process. In addition to the simplicity and efficiency of the schemes, the resulting two prover interactive proofs can be shown to be perfect zero knowledge, making no intractability assumptions.", "num_citations": "22\n", "authors": ["501"]}
{"title": "The pursuit for uniqueness: extending Valiant-Vazirani theorem to the probabilistic and quantum settings\n", "abstract": " Valiant-Vazirani showed in 1985 that solving NP with the promise that \"yes\" instances have only one witness is powerful enough to solve the entire NP class (under randomized reductions). We are interested in extending this result to the quantum setting. We prove extensions to the classes Merlin-Arthur (MA) and Quantum-Classical-Merlin-Arthur (QCMA). Our results have implications on the complexity of approximating the ground state energy of a quantum local Hamiltonian with a unique ground state and an inverse polynomial spectral gap. We show that the estimation, to within polynomial accuracy, of the ground state energy of poly-gapped 1-D local Hamiltonians is QCMA-hard, under randomized reductions. This is in strong contrast to the case of constant gapped 1-D Hamiltonians, which is in NP. Moreover, it shows that unless QCMA can be reduced to NP by randomized reductions, there is no classical description of the ground state of every poly-gapped local Hamiltonian which allows the calculation of expectation values efficiently. Finally, we discuss a few obstacles towards establishing an analogous result to the class Quantum-Merlin-Arthur (QMA). In particular, we show that random projections fail to provide a polynomial gap between two witnesses.", "num_citations": "20\n", "authors": ["501"]}
{"title": "Trading Help for Interaction in Statistical Zero-Knowledge Proofs.\n", "abstract": " Abstract.  We define interactive and non-interactive statistical zero-knowledge proofs with (limited) help, as proofs that can be almost perfectly simulated, where the prover and the verifier share a reference string that is computed by a probabilistic polynomial-time trusted third party that receives as input the statement to be proven (i.e. the input to the protocol). We compare these models with the standard interactive and non-interactive SZK models, trying to understand when this form of help can replace the interaction between the prover and the verifier and vice versa. We show that every promise problem that has an SZK protocol with help also has one without help. As for the opposite, we show non-interactive SZK proofs with help for natural languages for which only interactive SZK proofs are known. In order to achieve that, we introduce a complete problem for the class of promise problems that have non\u00a0\u2026", "num_citations": "20\n", "authors": ["501"]}
{"title": "Collective coin flipping\n", "abstract": " Randomized algorithms play an important role in parallel and distributed processing. The use of such algorithms assumes that each processor is able to generate random bits during the computation. In some applications the algorithm requires that the same random bit be generated by a set of processors. This task is easy if we assume that no faults may occur. We hav e one of the processors flip a coin and announce the outcome. If, however, the processor assigned to flip the coin happens to be faulty this may ruin the probabilistic requirement for our randomized algorithm. Suppose, then, that each processor is equipped with a fair coin, how can they generate a global coin flip which is only slightly biased despite failure of some of the processors?This problem was considered before, mostly in the framework of the Byzantine Generals Problem ([ACGM, BE, BD, Br, BR, Ra, Ya]). These past solutions are all based on the assumption that information may be communicated so that only some of the parties can read it. This is achieved either by choosing an appropriate model of communication, or by resorting to cryptography. We want to avoid such assumptions. Technically this means that we deal only with games of complete information.", "num_citations": "19\n", "authors": ["501"]}
{"title": "Agreement in the presence of faults, on networks of bounded degree\n", "abstract": " We present networks of bounded degree and a fully polynomial almost everywhere agreement scheme which tolerate, with high probability, randomly located faulty processors, where processors fail independently with some constant probability.", "num_citations": "17\n", "authors": ["501"]}
{"title": "Quantum refrigerator\n", "abstract": " We consider fault-tolerant quantum computation in the context where there are no fresh ancilla qubits available during the computation, and where the noise is due to a general quantum channel. We show that there are three classes of noisy channels: In the first, typified by the depolarizing channel, computation is only possible for a logarithmic time. In the second class, of which the dephasing channel is an example, computation is possible for polynomial time. The amplitude damping channel is an example of the third class, and for this class of channels, it is possible to compute for an exponential time in the number of qubits available.", "num_citations": "16\n", "authors": ["501"]}
{"title": "Asymptotically optimal PRAM emulation on faulty hypercubes\n", "abstract": " A scheme for emulating the parallel random access machine (PRAM) on a faulty hypercube is presented. All components of the hypercube, including the memory modules, are assumed to be subject to failure. The faults may occur at any time during the emulation and the system readjusts dynamically. The scheme, which rests on LG Valiant's BSP model (1990), is the first to achieve optimal and work-preserving PRAM emulation on a dynamically faulty network.", "num_citations": "14\n", "authors": ["501"]}
{"title": "Quantum multi prover interactive proofs with communicating provers\n", "abstract": " We introduce another variant of quantum MIP, where the provers do not share entanglement, the communication between the verifier and the provers is quantum, but the provers are unlimited in the classical communication between them. At first, this model may seem very weak, as provers who exchange information seem to be equivalent in power to a simple prover. This in fact is not the case-we show that any language in NEXP can be recognized in this model efficiently, with just two provers and two rounds of communication, with a constant completeness-soundness gap. Similar ideas and techniques may help help with other models of quantum MIP, including the dual question, of non communicating provers with unlimited entanglement.", "num_citations": "13\n", "authors": ["501"]}
{"title": "Quantum search in an ordered list via adaptive learning\n", "abstract": " We use a Bayesian approach to optimally solve problems in noisy binary search. We deal with two variants: 1. Each comparison can be erroneous with some probability $1 - p$. 2. At each stage $k$ comparisons can be performed in parallel and a noisy answer is returned We present a (classic) algorithm which optimally solves both variants together, up to an additive term of O(\\log \\log(n)), and prove matching information theoretic lower bounds. We use the algorithm to improve the results of Farhi et al \\cite{FGGS99} presenting a quantum (error free) search algorithm in an ordered list of expected complexity less than (\\log_2n) / 3.", "num_citations": "12\n", "authors": ["501"]}
{"title": "On algebras related to the discrete cosine transform\n", "abstract": " An algebraic theory for the discrete cosine transform (DCT) is developed, which is analogous to the well-known theory of the discrete Fourier transform (DFT). Whereas the latter diagonalizes a convolution algebra, which is a polynomial algebra modulo a product of various cyclotomic polynomials, the former diagonalizes a polynomial algebra modulo a product of various polynomials related to the Chebyshev types. When the dimension of the algebra is a power of 2, the DCT diagonalizes a polynomial algebra modulo a product of Chebyshev polynomials of the first type. In both DFT and DCT cases, the Chinese remainder theorem plays a key role in the design of fast algorithms.", "num_citations": "11\n", "authors": ["501"]}
{"title": "Algebraic computation trees in characteristic p> 0\n", "abstract": " We provide a simple and powerful combinatorial method for proving lower bounds for algebraic computation trees over algebraically closed fields of characteristic p>0. We apply our method to prove, for example, an /spl Omega/(n log n) lower bound for the n element distinctness problem, an /spl Omega/(n log(n/k)) lower bound to the \"k-equal problem\"-that is deciding whether there are k identical elements out of n input elements, and more. The proof of the main theorem relies on the deep work of B.M. Dwork, P. Deligne, and E. Bombieri on the Weil conjectures. In particular we make use of Bombieri's bound on the degree of the Zeta function of algebraic varieties over finite fields. Our bounds provide a natural extension to the recent topological lower bounds obtained by A. Bjorner, L. Lovasz and A.C. Yao for algebraic computation trees over the real numbers. For the special cases of real subspace arrangements\u00a0\u2026", "num_citations": "11\n", "authors": ["501"]}
{"title": "Increasing the power of the dealer in non-interactive zero-knowledge proof systems\n", "abstract": " We introduce weaker models for non-interactive zero knowledge, in which the dealer is not restricted to deal a truly random string and may also have access to the input to the protocol (i.e. the statement to prove). We show in these models a non-interactive statistical zero-knowledge proof for every language that has (interactive) statistical zero-knowledge proof, and a computational zero-knowledge proof for every language in NP. We also show how to change the latter proof system to fit the model of non-interactive computational zero-knowledge with preprocessingto improve existing results in term of the number of bit commitments that are required for the protocol to work.", "num_citations": "9\n", "authors": ["501"]}
{"title": "Randomized agreement protocols\n", "abstract": " Reaching agreement in the presence of faults is one of the most important problems in fault-tolerant distributed computation, and it is also a beautiful example of the power of randomized algorithms. This problem, first introduced by Pease, Shostak and Lamport [PSL80] as the\" Byzantine Agreement Problem\", considers a situation in which each process/~, i= 1,..., n, holds an initial value Mi, and they have to agree on a common value M, such that", "num_citations": "8\n", "authors": ["501"]}
{"title": "A quasi-random approach to matrix spectral analysis\n", "abstract": " Inspired by the quantum computing algorithms for Linear Algebra problems [HHL,TaShma] we study how the simulation on a classical computer of this type of \"Phase Estimation algorithms\" performs when we apply it to solve the Eigen-Problem of Hermitian matrices. The result is a completely new, efficient and stable, parallel algorithm to compute an approximate spectral decomposition of any Hermitian matrix. The algorithm can be implemented by Boolean circuits in  parallel time with a total cost of  Boolean operations. This Boolean complexity matches the best known rigorous  parallel time algorithms, but unlike those algorithms our algorithm is (logarithmically) stable, so further improvements may lead to practical implementations. All previous efficient and rigorous approaches to solve the Eigen-Problem use randomization to avoid bad condition as we do too. Our algorithm makes further use of randomization in a completely new way, taking random powers of a unitary matrix to randomize the phases of its eigenvalues. Proving that a tiny Gaussian perturbation and a random polynomial power are sufficient to ensure almost pairwise independence of the phases  is the main technical contribution of this work. This randomization enables us, given a Hermitian matrix with well separated eigenvalues, to sample a random eigenvalue and produce an approximate eigenvector in  parallel time and  Boolean complexity. We conjecture that further improvements of our method can provide a stable solution to the full approximate spectral decomposition problem with complexity similar to the complexity (up to a\u00a0\u2026", "num_citations": "5\n", "authors": ["501"]}
{"title": "Optimal algorithms for linear algebra by quantum inspiration\n", "abstract": " Recent results by Harrow et. al.[2], and by Ta-Shma [5], suggest that quantum computers may have an exponential advantage in solving a wealth of linear algebraic problems, over classical algorithms. Building on the quantum intuition of these results, we step back into the classical domain, and explore its usefulness in designing classical algorithms. We achieve an algorithm for solving the major linear-algebraic problems in time O (n\u03c9+ \u03bd) for any \u03bd> 0, where \u03c9 is the optimal matrix-product constant. Thus our algorithm is optimal wrt matrix multiplication, and comparable to the state-of-the-art algorithm for these problems due to [1]. Being derived from quantum intuition, our proposed algorithm is completely disjoint from all previous classical algorithms, and builds on a combination of low-discrepancy sequences and perturbation analysis. As such, we hope it motivates further exploration of quantum techniques in this respect, hopefully leading to improvements in our understanding of space complexity and numerical stability of these problems.", "num_citations": "2\n", "authors": ["501"]}
{"title": "Quantum refrigerator\n", "abstract": " The threshold theorem [AB97, KLZ98, Kit97, AGP05] is the central result of the theory of fault-tolerant quantum computation. It states that, provided the error rate per gate or time step is below some constant threshold value, then arbitrarily long quantum computations are possible with only polylogarithmic overhead. The threshold theorem tells us that large quantum computers are possible in principle, provided experimentalists can achieve an error rate below the threshold value.A number of assumptions are needed to prove the threshold theorem. Some of them can be relaxed and some of them cannot be. One assumption that has been traditionally classified as necessary is the need for fresh ancilla qubits in the course of the computation. In the course of quantum error correction, ancilla qubits are introduced and used to record the error syndrome. This can be viewed as a refrigeration process, where entropy which has been introduced into the data qubits by the noise gets pumped out into the ancilla qubits, cooling down the data qubits. In order for this to work, the ancilla qubits used must be cold themselves, or they cannot absorb the extra entropy from the data. Since ancilla qubits created at the beginning of the computation are themselves subject to the noise process, we can expect them to heat up over time, eventually making them worthless for refrigeration. Based on this intuition, we expect to need a continual stream of new, freshly cooled ancilla qubits to keep the error correction running. At a rigorous mathematical level, this intuition is supported by the result of [ABIN96], which showed that for noise in the form of a depolarizing channel, it is\u00a0\u2026", "num_citations": "1\n", "authors": ["501"]}
{"title": "\u2018Efficient identification schemes using two prover interactive proofs\n", "abstract": " We present two e\ufb02icient identi\ufb01cation schemes based on the dif\ufb01culty of solving the subset sum problem and the circuit satis\ufb01ability problem. Both schemes use the two prover model introduced by [BGKW], where the veri\ufb01er (eg the Bank) interacts with two untrusted provers (eg two bank identi\ufb01cation cards) who have jointly agreed on a strategy to convince the veri\ufb01er of their identity. To believe the validity of their identity proving procedure, the veri\ufb01er must make sure that the two provers can not communicate with each other during the course of the proof process. In addition to the simplicity and e\ufb02iciency of the schemes, the resulting two prover interactive proofs can be shown to be perfect zero knowledge, making no intractability assumptions.", "num_citations": "1\n", "authors": ["501"]}