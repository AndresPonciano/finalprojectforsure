{"title": "Java quality assurance by detecting code smells\n", "abstract": " Software inspection is a known technique for improving software quality. It involves carefully examining the code, the design, and the documentation of software and checking these for aspects that are known to be potentially problematic based on past experience. Code smells are a metaphor to describe patterns that are generally associated with bad design and bad programming practices. Originally, code smells are used to find the places in software that could benefit from refactoring. In this paper we investigate how the quality of code can be automatically assessed by checking for the presence of code smells and how this approach can contribute to automatic code inspection. We present an approach for the automatic detection and visualization of code smells and discuss how this approach can be used in the design of a software inspection tool. We illustrate the feasibility of our approach with the development\u00a0\u2026", "num_citations": "523\n", "authors": ["226"]}
{"title": "Generating robust parsers using island grammars\n", "abstract": " Source model extraction, the automated extraction of information from system artifacts, is a common phase in reverse engineering tools. One of the major challenges of this phase is creating extractors that can deal with irregularities in the artifacts that are typical for the reverse engineering domain (for example, syntactic errors, incomplete source code, language dialects and embedded languages). The paper proposes a solution in the form of island grammars, a special kind of grammar that combines the detailed specification possibilities of grammars with the liberal behavior of lexical approaches. We show how island grammars can be used to generate robust parsers that combine the accuracy of syntactical analysis with the speed, flexibility and tolerance usually only found in lexical analysis. We conclude with a discussion of the development of MANGROVE, a generator for source model extractors based on\u00a0\u2026", "num_citations": "362\n", "authors": ["226"]}
{"title": "Lightweight impact analysis using island grammars\n", "abstract": " Impact analysis is needed for the planning and estimation of software maintenance projects. Traditional impact analysis techniques tend to be too expensive for this phase, so there is need for more lightweight approaches. We present a technique for the generation of lightweight impact analyzers from island grammars. We demonstrate this technique using a real-world case study in which we describe how island grammars can be used to find account numbers in the software portfolio of a large bank. We show how we have implemented this analysis and achieved lightweightness using a reusable generative framework for impact analyzers.", "num_citations": "96\n", "authors": ["226"]}
{"title": "Assessing the value of coding standards: An empirical study\n", "abstract": " In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. Not only can compliance with a set of rules having little impact on the number of faults be considered wasted effort, but it can actually result in an increase in faults, as any modification has a non-zero probability of introducing a fault or triggering a previously concealed one. Therefore, it is important to build a body of empirical knowledge, helping us understand which rules are worthwhile enforcing, and which ones should be ignored in the context of fault reduction. In this paper, we describe two approaches to quantify the relation between rule violations and actual faults, and present empirical data on this relation for the MISRA C 2004 standard on an industrial case study.", "num_citations": "87\n", "authors": ["226"]}
{"title": "Evaluating the relation between coding standard violations and faultswithin and across software versions\n", "abstract": " In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. In previous work, we performed a pilot study to assess the relation between rule violations and actual faults, using the MISRA C 2004 standard on an industrial case. In this paper, we investigate three different aspects of the relation between violations and faults on a larger case study, and compare the results across the two projects. We find that 10 rules in the standard are significant predictors of fault location.", "num_citations": "64\n", "authors": ["226"]}
{"title": "An industrial survey of safety evidence change impact analysis practice\n", "abstract": " Context. In many application domains, critical systems must comply with safety standards. This involves gathering safety evidence in the form of artefacts such as safety analyses, system specifications, and testing results. These artefacts can evolve during a system's lifecycle, creating a need for change impact analysis to guarantee that system safety and compliance are not jeopardised. Objective. We aim to provide new insights into how safety evidence change impact analysis is addressed in practice. The knowledge about this activity is limited despite the extensive research that has been conducted on change impact analysis and on safety evidence management. Method. We conducted an industrial survey on the circumstances under which safety evidence change impact analysis is addressed, the tool support used, and the challenges faced. Results. We obtained 97 valid responses representing 16 application\u00a0\u2026", "num_citations": "62\n", "authors": ["226"]}
{"title": "Prioritizing software inspection results using static profiling\n", "abstract": " Static software checking tools are useful as an additional automated software inspection step that can easily be integrated in the development cycle and assist in creating secure, reliable and high quality code. However, an often quoted disadvantage of these tools is that they generate an overly large number of warnings, including many false positives due to the approximate analysis techniques. This information overload effectively limits their usefulness. In this paper we present ELAN, a technique that helps the user prioritize the information generated by a software inspection tool, based on a demand-driven computation of the likelihood that execution reaches the locations for which warnings are reported. This analysis is orthogonal to other prioritization techniques known from literature, such as severity levels and statistical analysis to reduce false positives. We evaluate feasibility of our technique using a number\u00a0\u2026", "num_citations": "56\n", "authors": ["226"]}
{"title": "Exploring software systems\n", "abstract": " Software evolution is required to keep a software system in sync with the ever-changing needs of the system's users and environment. An unfortunate side-effect of evolution is that it often causes the knowledge about a system to degrade, which in turn impedes further evolution. In the dissertation, we investigate techniques and tools that help remedy this situation by supporting the exploration of a software system and improving its legibility (Moonen, 2002). We examine the analogy with urban exploration and present innovative techniques for the extraction, abstraction, and presentation of information needed for understanding software.", "num_citations": "53\n", "authors": ["226"]}
{"title": "On the use of data flow analysis in static profiling\n", "abstract": " Static profiling is a technique that produces estimates of execution likelihoods or frequencies based on source code analysis only. It is frequently used in determining cost/benefit ratios for certain compiler optimizations. In previous work,we introduced a simple algorithm to compute execution likelihoods,based on a control flow graph and heuristic branch prediction. In this paper we examine the benefits of using more involved analysis techniques in such a static profiler. In particular, we explore the use of value range propagation to improve the accuracy of the estimates, and we investigate the differences in estimating execution likelihoods and frequencies.", "num_citations": "48\n", "authors": ["226"]}
{"title": "Types and concept analysis for legacy systems\n", "abstract": " We combine type inference and concept analysis in order to gain insight into legacy software systems. Type inference for COBOL yields the types for variables and program parameters. These types are used to perform mathematical concept analysis on legacy systems. We have developed ConceptRefinery, a tool for interactively manipulating concepts. We show how this tool facilitates experiments with concept analysis, and lets reengineers employ their knowledge of the legacy system to refine the results of concept analysis.", "num_citations": "48\n", "authors": ["226"]}
{"title": "Generalized parsing and term rewriting: Semantics driven disambiguation\n", "abstract": " Generalized parsing technology provides the power and flexibility to attack real-world parsing applications. However, many programming languages have syntactical ambiguities that can only be solved using semantical analysis. In this paper we propose to apply the paradigm of term rewriting to filter ambiguities based on semantical information. We start with the definition of a representation of ambiguous derivations. Then we extend term rewriting with means to handle such derivations. Finally, we apply these tools to some real world examples, namely C and COBOL. The resulting architecture is simple and efficient as compared to semantic directed parsing.", "num_citations": "41\n", "authors": ["226"]}
{"title": "A generic architecture for data flow analysis to support reverse engineering\n", "abstract": " Data ow analysis is a process for collecting run-time information about data in programs without actually executing them. In this paper, we focus at the use of data ow analysis to support program understanding and reverse engineering. Data ow analysis is bene cial for these applications since the information obtained can be used to compute relationships between data objects in programs. These relations play a key role, for example, in the determination of the logical components of a system and their interaction.The general support of program understanding and reverse engineering requires the ability to analyse a variety of source languages and the ability to combine the results of analysing multiple languages. We present a exible and generic software architecture for describing and performing language-independent data ow analysis which allows such transparent multi-language analysis. All components of this architecture were formally speci ed.", "num_citations": "36\n", "authors": ["226"]}
{"title": "Visualizing similarities in execution traces\n", "abstract": " The analysis of execution traces is a common practice in the context of software understanding. A major issue during this task is scalability, as the massive amounts of data often make the comprehension process difficult. A significant portion of this data overload can be attributed to repetitions that are caused by, for example, iterations in the software\u2019s source code.In this position paper, we elaborate on a novel approach to visualize such repetitions. The idea is to compare an execution trace against itself and to visualize the matching events in a two-dimensional matrix, similar to related work in the field of code duplication detection. By revealing these similarities we hope to gain new insights into execution traces. We identify the potential purposes in facilitating the software understanding process and report on our findings so far.", "num_citations": "23\n", "authors": ["226"]}
{"title": "Implementation of a prototype for the new ASF+ SDF Meta-Environment\n", "abstract": " The ASF+ SDF Meta-Environment has become a legacy system over the last few years. This paper describes the first steps towards a new implementation of this system. This implementation is based on the latest techniques concerning the coupling of software components, construction of user interfaces and modern programming languages. Special care has been taken to ensure the flexibility and extensibility of the system, both now and in the future.", "num_citations": "23\n", "authors": ["226"]}
{"title": "Design and implementation of a new asf+ sdf meta-environment\n", "abstract": " Design and implementation of a new ASF+SDF meta-environment University of Amsterdam University of Amsterdam UvA Terms of use Contact UvA-DARE (Digital Academic Repository) Home Advanced Search Browse My selection Search UvA-DARE Author MGJ van den Brand T. Kuipers LMF Moonen PA Olivier Year 1997 Title Design and implementation of a new ASF+SDF meta-environment Book title Proceedings of the 2nd International Workshop on the Theory and Practice of Algebraic Specifications (ASF+SDF'97) Publisher Springer-Verlag Document type Chapter Faculty Faculty of Science (FNWI) Institute Informatics Institute (IVI) Language Undefined/Unknown Persistent Identifier https://hdl.handle.net/11245/1.135490 Disclaimer/Complaints regulations If you believe that digital publication of certain material infringes any of your rights or (privacy) interests, please let the Library know, stating your reasons. In /\u2026", "num_citations": "15\n", "authors": ["226"]}
{"title": "Data flow analysis for reverse engineering\n", "abstract": " This thesis is the result of my graduation research project carried out at the Programming Research Group of the Department of Computer Science, University of Amsterdam. The starting point of the project was the investigation and speci cation of techniques to support data ow analysis for reverse engineering and program understanding.I would like to thank Prof. dr. P. Klint who introduced me to the project, and gave me a lot of freedom in doing things as I thought best. The members of the Programming Research Group, especially dr. MGJ van den Brand and dr. C. Verhoef, supported me on various aspects of my work and make a nice environment to work in. Paul Klint, Chris Verhoef, Mark van den Brand and Arie van Deursen read draft versions of this document and gave many useful remarks to improve its readability.", "num_citations": "14\n", "authors": ["226"]}
{"title": "Identification of variation points using dynamic analysis\n", "abstract": " In this position paper we investigate the use of dynamic analysis to determine commonalities and variation points as a first step to the migration of similar but separate versions of a software system into an integrated product line. The approach detects forks and merges in different execution traces as an indication of variation points. It is illustrated by a simple implementation, which is applied to an academic example. Finally we formulate a number of research issues that need to be investigated further.", "num_citations": "11\n", "authors": ["226"]}
{"title": "Improving problem identification via automated log clustering using dimensionality reduction\n", "abstract": " Background: Continuous engineering practices, such as continuous integration and continuous deployment, see increased adoption in modern software development. A frequently reported challenge for adopting these practices is the need to make sense of the large amounts of data that they generate.Goal: We consider the problem of automatically grouping logs of runs that failed for the same underlying reasons, so that they can be treated more effectively, and investigate the following questions:(1) Does an approach developed to identify problems in system logs generalize to identifying problems in continuous deployment logs?(2) How does dimensionality reduction affect the quality of automated log clustering?(3) How does the criterion used for merging clusters in the clustering algorithm affect clustering quality?Method: We replicate and extend earlier work on clustering system log files to assess its\u00a0\u2026", "num_citations": "9\n", "authors": ["226"]}
{"title": "Assuring software quality by code smell detection\n", "abstract": " In this retrospective we will review the paper \u201cJava Quality Assurance by Detecting Code Smells\u201d that was published ten years ago at WCRE. The work presents an approach for the automatic detection and visualization of code smells and discusses how this approach could be used in the design of a software inspection tool. The feasibility of the proposed approach was illustrated with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in JAVA source code. It was the first tool to automatically detect code smells in source code, and we demonstrated the application of this tool in an industrial quality assessment case study. In addition to reviewing the WCRE 2002 work, we will discuss subsequent developments in this area by looking at a selection of papers that were published in its wake. In particular, we will have a look at recent related work in which we empirically investigated the relation between code smells and software maintainability in a longitudinal study where professional developers were observed while maintaining four different software systems that exhibited known code smells. We conclude with a discussion of the lessons learned and opportunities for further research.", "num_citations": "9\n", "authors": ["226"]}
{"title": "Dealing with crosscutting concerns in existing software\n", "abstract": " This paper provides a roadmap for dealing with crosscutting concerns while trying to understand, maintain, and evolve existing software systems. We describe an integrated, systematic, approach that helps a software engineer with identifying, documenting and migrating crosscutting concerns in the source code of a software system, and discuss the integration considerations. We conclude with a number of lessons learned and directions for future research.", "num_citations": "7\n", "authors": ["226"]}
{"title": "Survey on safety evidence change impact analysis in practice: Detailed description and analysis\n", "abstract": " Critical systems must comply with safety standards in many application domains. This involves gathering safety evidence in the form of artefacts such as safety analyses, system specifications, and testing results. These artefacts can evolve during a system\u2019s lifecycle, and impact analysis might be necessary to guarantee that system safety and compliance are not jeopardised. Although extensive research has been conducted on impact analysis and on safety evidence management, the knowledge about how safety evidence change impact analysis is addressed in practice is limited. This technical report presents a survey targeted at filling this gap by analysing the circumstances under which safety evidence change impact analysis is addressed, the tool support used, and the challenges faced. We obtained 97 valid responses representing 16 application domains, 28 countries, and 47 safety standards. The results suggest that most projects deal with safety evidence change impact analysis during system development and mainly from system specifications, the level of automation in the process is low, and insufficient tool support is the most frequent challenge. Other notable findings are that safety case evolution should probably be better managed, no commercial impact analysis tool has been reported as used for all artefact types, and experience and automation do not seem to greatly help in avoiding challenges.", "num_citations": "6\n", "authors": ["226"]}
{"title": "On large execution traces and trace abstraction techniques\n", "abstract": " Program comprehension is an important concern in the context of software maintenance tasks because these activities generally require a certain degree of knowledge of the system at hand. Although the use of dynamic analysis for information gathering has become increasingly popular, the literature indicates that dealing with the excessive amounts of data resulting from dynamic analysis remains a formidable challenge. Although various trace abstraction techniques have been proposed to address these scalability concerns, such techniques are typically not discussed in terms of properties such as complexity and information preservation, and lack thorough evaluation of technique-specific parameters. Moreover, the absence of a common execution trace repository makes matters even worse, as most researchers test their techniques only on their own particular traces. Consequently, it is very difficult to make a fair comparison between the abstraction techniques known from literature. In this paper, we present a characterization of large execution traces in which a set of key properties is extracted from a series of seven representative traces from six different object-oriented systems. Having highlighted the key issues in this context, we propose an assessment methodology for the quantitative evaluation and comparison of trace abstraction techniques. We apply this methodology on a selection of three light-weight abstraction methods, assessing them on the basis of metrics that are relevant in their evaluation. We discuss the results, compare the techniques, and relate the measurements to the trace characteristics found earlier.", "num_citations": "6\n", "authors": ["226"]}
{"title": "Ranking software inspection results using execution likelihood\n", "abstract": " Ranking software inspection results using execution likelihood (2006) | www.narcis.nl KNAW KNAW Narcis Back to search results CWI Publication Ranking software inspection results using execution likelihood (2006) Pagina-navigatie: Main Save publication Save as MODS Export to Mendeley Save as EndNote Export to RefWorks Title Ranking software inspection results using execution likelihood Author FC Boogerd; LMF Moonen (Leon) Supporting host Software Analysis and Transformation Date issued 2006-01-01 Access Closed Access Language English Type Conference Paper Publisher Philips Publication https://ir.cwi.nl/pub/14241 OpenURL Search this publication in (your) library Persistent Identifier urn:NBN:nl:ui:18-14241 Metadata XML Source CWI Go to Website Navigation: Home about narcis login Nederlands contact Anna van Saksenlaan 51 2593 HW Den Haag narcis@dans.knaw.nl More >>> >>> >>\u2026", "num_citations": "5\n", "authors": ["226"]}
{"title": "Towards evidence-based recommendations to guide the evolution of component-based product families\n", "abstract": " Many large-scale software-intensive systems are produced as instances of component-based product families, a well-known tactic to develop a portfolio of software products based on a collection of shared assets. However, sharing components between software products introduces dependencies that complicate maintenance and evolution: changes made in a component to address an issue in one product may have undesirable effects on other products in which the same component is used. Therefore, developers not only need to understand how a proposed change will impact the component and product at hand; they also need to understand how it affects the whole product family, including systems that are already deployed. Given that these systems contain thousands of components, it is no surprise that it is hard to reason about the impact of a change on a single product, let alone assess the effects of more\u00a0\u2026", "num_citations": "4\n", "authors": ["226"]}
{"title": "2nd international workshop on advanced software development tools and techniques (WASDeTT): Tools for software maintenance, visualization, and reverse engineering\n", "abstract": " The objective of the 2nd international workshop on advanced software development tools and techniques (WASDeTT) is to provide interested researchers with a forum to share their tool building experiences and to explore how tools can be built more effectively and efficiently. This workshop specifically focuses on tools for software maintenance and comprehension and addresses issues such as tool-building in an industrial context, component-based tool building, and tool building in teams.", "num_citations": "2\n", "authors": ["226"]}
{"title": "Spectrum-based log diagnosis\n", "abstract": " Background: Continuous Engineering practices are increasingly adopted in modern software development. However, a frequently reported need is for more effective methods to analyze the massive amounts of data resulting from the numerous build and test runs. Aims: We present and evaluate Spectrum-Based Log Diagnosis (SBLD), a method to help developers quickly diagnose problems found in complex integration and deployment runs. Inspired by Spectrum-Based Fault Localization, SBLD leverages the differences in event occurrences between logs for failing and passing runs, to highlight events that are stronger associated with failing runs.Method: Using data provided by Cisco Norway, we empirically investigate the following questions:(i) How well does SBLD reduce the effort needed to identify all failure-relevant events in the log for a failing run?(ii) How is the performance of SBLD affected by available\u00a0\u2026", "num_citations": "1\n", "authors": ["226"]}
{"title": "On the Use of Automated Log Clustering to Support Effort Reduction in Continuous Engineering\n", "abstract": " Continuous engineering (CE) practices, such as continuous integration and continuous deployment, have become key to modern software development. They are characterized by short automated build and test cycles that give developers early feedback on potential issues. CE practices help to release software more frequently, and reduces risk by increasing incrementality. However, effective use of CE practices in industrial projects requires making sense of the vast amounts of data that results from the repeated build and test cycles. The goal of this paper is to investigate to what extent these data can be treated more effectively by automatically grouping logs of runs that failed for the same underlying reasons, and what effort reduction can be achieved. To this end, we replicate and extend earlier work on system log clustering to evaluate its efficacy in the CE context, and to investigate the impact of five alternative log\u00a0\u2026", "num_citations": "1\n", "authors": ["226"]}
{"title": "Generating test-plans by mining version histories\n", "abstract": " Regression testing is an essential step in safeguarding the evolution of a system, yet there is often not enough time to exercise all available tests. Identifying the subset of tests that can reveal potential issues introduced by a change is a challenge. It requires identifying the parts of the system that are dependent on that change, a task typically done by means of static program analysis. In this paper, we investigate an alternative approach, using software repository mining. We propose a method that mines the revision-history of a system to uncover dependencies, and uses these for testselection and test-prioritization. We have implemented the approach in a prototype tool that recommends a test plan, given a set of changes. We have applied our tool on 10 years of revision-history from one of the central systems of our industrial partner, Kongsberg Maritime. Our evaluation shows that our approach accurately identifies dependencies among files, and comparing our recommendations with existing test plans shows that relevant tests are given high priority in our recommendation. By reducing the amount of test to exercise, and limiting time spend on test-plan creation, our approach helps to increase cost-effectiveness of regression testing in the company.", "num_citations": "1\n", "authors": ["226"]}
{"title": "User evaluation of a domain specific program comprehension tool\n", "abstract": " The user evaluation in this paper concerns a domain-specific tool to support the comprehension of large safety-critical component-based software systems for the maritime sector. We discuss the context and motivation of our research, and present the user-specific details of our tool, called FlowTracker. We include a walk-through of the system and present the profiles of our prospective users. Next, we discuss the design of an exploratory qualitative study that we have conducted to evaluate the usability and effectiveness of our tool. We conclude with a summary of lessons learned and challenges that we see for user evaluation of such domain-specific program comprehension tools.", "num_citations": "1\n", "authors": ["226"]}
{"title": "Using software history to guide deployment of coding standards\n", "abstract": " In spite of the widespread use of coding standards and tools enforcing their rules, there is little empirical evidence supporting the intuition that they prevent the introduction of faults in software. Therefore, we propose to use information from software and issue archives to link standard violations to known bugs. In this chapter we introduce such an approach and apply it to three industrial case studies. Furthermore, we discuss how to use the historical data to address two practical issues in using a coding standard: which rules to adhere to, and how to rank violations of those rules.", "num_citations": "1\n", "authors": ["226"]}
{"title": "Implementation of a Prototype for the New ASF+ SDF Meta-Environment\n", "abstract": " The ASF+SDF Meta-Environment has become a legacy system over the last few years. This paper describes the first steps towards a new implementation of this system. This implementation is based on the latest techniques concerning the coupling of software components, construction of user interfaces and modern programming languages. Special care has been taken to ensure the flexibility and extensibility of the system, both now and in the future. The general architecture of the new environment is discussed as well as the components which are currently implemented and operational in the environment. Each component is independent of the other components and communicates using the TOOLBUS.", "num_citations": "1\n", "authors": ["226"]}