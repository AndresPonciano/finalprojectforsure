{"title": "Requirements specification for process-control systems\n", "abstract": " The paper describes an approach to writing requirements specifications for process-control systems, a specification language that supports this approach, and an example application of the approach and the language on an industrial aircraft collision avoidance system (TCAS II). The example specification demonstrates: the practicality of writing a formal requirements specification for a complex, process-control system; and the feasibility of building a formal model of a system using a specification language that is readable and reviewable by application experts who are not computer scientists or mathematicians. Some lessons learned in the process of this work, which are applicable both to forward and reverse engineering, are also presented.< >", "num_citations": "804\n", "authors": ["1041"]}
{"title": "Completeness and consistency in hierarchical state-based requirements\n", "abstract": " This paper describes methods for automatically analyzing formal, state-based requirements specifications for some aspects of completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. State-space explosion problems are eliminated by applying the analysis at a high level of abstraction; i.e., instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collision-avoidance system required on all commercial aircraft with more than 30 passengers that fly in U.S. Airspace.", "num_citations": "443\n", "authors": ["1041"]}
{"title": "Software requirements analysis for real-time process-control systems\n", "abstract": " Software requirements errors have been found to account. for a majority of production software failures [BMU75, End75) and have been implicated in a large number of accidents [Lev86). Errors introduced during the requirements phase can cost up to 200 times more to correct than errors introduced later in the life cycle [Boe81] and can have a major impact on safety. Therefore, techniques to provide adequate requirements specifications and to find errors early are of great importance.One application area for which requirements specification is particularly critical is process-control, ie, software that controls arbitrarily large or energetic physical phenomena. Such software is usually real-time and often embedded within some larger system such as a ship, aircraft, missile, spacecraft, manufacturing or processing plant, or transportation system where the software is used to assist in the formulation and implementation of decisions made by the computer or by humans for the purpose of controlling the larger system. In such process-control systems, minor behavioral distinctions often have significant consequences. It is therefore particularly important that the requirements specifications distinguish the behavior of the desired software from that of any other, undesired program that might be designed, ie, they must be both precise (unambiguous) and correct with respect to the encompassing system requirements. The requirements analysis techniques", "num_citations": "312\n", "authors": ["1041"]}
{"title": "Coverage based test-case generation using model checkers\n", "abstract": " Presents a method for automatically generating test cases according to structural coverage criteria. We show how a model checker can be used to automatically generate complete test sequences that provide a pre-defined coverage of any software development artifact that can be represented as a finite state model. Our goal is to help reduce the high cost of developing test cases for safety-critical software applications that require a certain level of coverage for certification, e.g. safety-critical avionics systems that need to demonstrate MC/DC (modified condition and decision) coverage of the code. We define a formal framework which is suitable for modeling software artifacts like requirements models, software specifications or implementations. We then show how various structural coverage criteria can be formalized and used to make a model checker provide test sequences to achieve this coverage. To illustrate our\u00a0\u2026", "num_citations": "262\n", "authors": ["1041"]}
{"title": "Model-based safety analysis of simulink models using SCADE design verifier\n", "abstract": " Safety analysis techniques have traditionally been performed manually by the safety engineers. Since these analyses are based on an informal model of the system, it is unlikely that these analyses will be complete, consistent, and error-free. Using precise formal models of the system as the basis of the analysis may help reduce errors and provide a more thorough analysis. Further, these models allow automated analysis, which may reduce the manual effort required.             The process of creating system models suitable for safety analysis closely parallels the model-based development process that is increasingly used for critical system and software development. By leveraging the existing tools and techniques, we can create formal safety models using tools that are familiar to engineers and we can use the static analysis infrastructure available for these tools. This paper reports our initial experience in\u00a0\u2026", "num_citations": "183\n", "authors": ["1041"]}
{"title": "Coverage metrics for requirements-based testing\n", "abstract": " In black-box testing, one is interested in creating a suite of tests from requirements that adequately exercise the behavior of a software system without regard to the internal structure of the implementation. In current practice, the adequacy of black box test suites is inferred by examining coverage on an executable artifact, either source code or a software model. In this paper, we define structural coverage metrics directly on high-level formal software requirements. These metrics provide objective, implementation-independent measures of how well a black-box test suite exercises a set of requirements. We focus on structural coverage criteria on requirements formalized as LTL properties and discuss how they can be adapted to measure finite test cases. These criteria can also be used to automatically generate a requirements-based test suite. Unlike model or code-derived test cases, these tests are immediately\u00a0\u2026", "num_citations": "163\n", "authors": ["1041"]}
{"title": "Test-suite reduction for model based tests: Effects on test quality and implications for testing\n", "abstract": " Model checking techniques can be successfully employed as a test case generation technique to generate tests from formal models. The number of tests cases produced, however, is typically large for complex coverage criteria such as MCDC. Test-suite reduction can provide us with a smaller set of test cases that present the original coverage-often a dramatically smaller set. One potential drawback with test-suite reduction is that this might affect the quality of the test-suite in terms of fault finding. Previous empirical studies provide conflicting evidence on this issue. To further investigate the problem and determine its effect when testing formal models of software, we performed an experiment using a large case example of a flight guidance system, generated reduced test-suites for a variety of structural coverage criteria while presenting coverage, and recorded their fault finding effectiveness. Our results show that the\u00a0\u2026", "num_citations": "159\n", "authors": ["1041"]}
{"title": "Specification-based prototyping for embedded systems\u2019\n", "abstract": " Specification of software for safety critical, embedded computer systems has been widely addressed in literature. To achieve the high level of confidence in a specification\u2019s correctness necessary in many applications, manual inspections, formal verification, and simulation must be used in concert. Researchers have successfully addressed issues in inspection and verification; however, results in the areas of execution and simulation of specifications have not made as large an impact as desired.             In this paper we present an approach to specification-based prototyping which addresses this issue. It combines the advantages of rigorous formal specifications and rapid systems prototyping. The approach lets us refine a formal executable model of the system requirements to a detailed model of the software requirements. Throughout this refinement process, the specification is used as a prototype of the\u00a0\u2026", "num_citations": "122\n", "authors": ["1041"]}
{"title": "Programs, tests, and oracles: the foundations of testing revisited\n", "abstract": " In previous decades, researchers have explored the formal foundations of program testing. By exploring the foundations of testing largely separate from any specific method of testing, these researchers provided a general discussion of the testing process, including the goals, the underlying problems, and the limitations of testing. Unfortunately, a common, rigorous foundation has not been widely adopted in empirical software testing research, making it difficult to generalize and compare empirical research. We continue this foundational work, providing a framework intended to serve as a guide for future discussions and empirical studies concerning software testing. Specifically, we extend Gourlay's functional description of testing with the notion of a test oracle, an aspect of testing largely overlooked in previous foundational work and only lightly explored in general. We argue additional work exploring the\u00a0\u2026", "num_citations": "120\n", "authors": ["1041"]}
{"title": "A proposal for model-based safety analysis\n", "abstract": " System safety analysis techniques are well established and are used extensively during the design of safety-critical systems. Despite this, most of the techniques are highly subjective and dependent on the skill of the practitioner. Since these analyses are usually based on an informal system model, it is unlikely that they will be complete, consistent, and error free. In fact, the lack of precise models of the system architecture and its failure modes often forces the safety analysts to devote much of their effort to finding undocumented details of the system behavior and embedding this information in the safety artifacts such as the fault trees. In this paper we propose an approach, Model-Based Safety Analysis, in which the system and safety engineers use the same system models created during a model-based development process. By extending the system model with a fault model as well as relevant portions of the\u00a0\u2026", "num_citations": "115\n", "authors": ["1041"]}
{"title": "Designing specification languages for process control systems: Lessons learned and steps to the future?\n", "abstract": " Previously, we defined a blackbox formal system modeling language called RSML (Requirements State Machine Language). The language was developed over several years while specifying the system requirements for a collision avoidance system for commercial passenger aircraft. During the language development, we received continual feed- back and evaluation by FAA employees and industry representatives, which helped us to produce a specification language that is easily learned and used by application experts.             Since the completion of the RSML project, we have continued our re- search on specification languages. This research is part of a larger effort to investigate the more general problem of providing tools to assist in developing embedded systems. Our latest experimental toolset is called SpecTRM (Specification Tools and Requirements Methodology), and the formal specification\u00a0\u2026", "num_citations": "106\n", "authors": ["1041"]}
{"title": "The effect of program and model structure on MC/DC test adequacy coverage\n", "abstract": " In avionics and other critical systems domains, adequacy of test suites is currently measured using the MC/DC metric on source code (or on a model in model-based development). We believe that the rigor of the MC/DC metric is highly sensitive to the structure of the implementation and can therefore be misleading as a test adequacy criterion. We investigate this hypothesis by empirically studying the effect of program structure on MC/DC coverage.", "num_citations": "102\n", "authors": ["1041"]}
{"title": "Completeness and consistency analysis of state-based requirements\n", "abstract": " This paper describes methods for automatically analyzing formal, state-based requirements specifications for completeness and consistency. The approach uses a low-level functional formalism, simplifying the analysis process. State space exploslon problems are eliminated by applying the analysis at a high level of abstraction; i.e, instead of generating a reachability graph for analysis, the analysis is performed directly on the model. The method scales up to large systems by decomposing the specification into smaller, analyzable parts and then using functional composition rules to ensure that verified properties hold for the entire specification. The analysis algorithms and tools have been validated on TCAS II, a complex, airborne, collision-avoidance system reqmred on all commercial aircraft with more than 30 passengers that fly in U.S. airspace.", "num_citations": "99\n", "authors": ["1041"]}
{"title": "The risks of coverage-directed test case generation\n", "abstract": " A number of structural coverage criteria have been proposed to measure the adequacy of testing efforts. In the avionics and other critical systems domains, test suites satisfying structural coverage criteria are mandated by standards. With the advent of powerful automated test generation tools, it is tempting to simply generate test inputs to satisfy these structural coverage criteria. However, while techniques to produce coverage-providing tests are well established, the effectiveness of such approaches in terms of fault detection ability has not been adequately studied. In this work, we evaluate the effectiveness of test suites generated to satisfy four coverage criteria through counterexample-based test generation and a random generation approach-where tests are randomly generated until coverage is achieved-contrasted against purely random test suites of equal size. Our results yield three key conclusions. First\u00a0\u2026", "num_citations": "98\n", "authors": ["1041"]}
{"title": "Safety and software intensive systems: Challenges old and new\n", "abstract": " There is an increased use of software in safety-critical systems; a trend that is likely to continue in the future. Although traditional system safety techniques are applicable to software intensive systems, there are new challenges emerging. In this report we will address four issues we believe will pose challenges in the future. First, the nature of safety is continuing to be widely misunderstood and known system safety techniques are not applied. Second, our ability to demonstrate (certify) that safety requirements have been met is inadequate. Third, modeling and automated tools, for example, code generation and automated testing, are introduced in a hope to increase productivity; this reliance on tools rather than people, however, introduces new and poorly understood problems. Finally, safety-critical systems are increasingly relying on data (configuration data or databases), incorrect data could have catastrophic and\u00a0\u2026", "num_citations": "91\n", "authors": ["1041"]}
{"title": "Reduction and slicing of hierarchical state machines\n", "abstract": " Formal specification languages are often criticized for being difficult to understand, difficult to use, and unacceptable by software practitioners. Notations based on state machines, such as, Statecharts, Requirements State Machine Language (RSML), and SCR, are suitable for modeling of embedded systems and eliminate many of the main drawbacks of formal specification languages. Although a specification language can help eliminate accidental complexity, the inherent complexity of many of today's systems inevitably leads to large and complex specifications. Thus, there is a need for mechanisms to simplify a formal specification and present information to analysts and reviewers in digestible chunks.             In this paper, we present a two tiered approach to slicing (or simplification) of hierarchical finite state machines. We allow an analyst to simplify a specification based on a scenario. The remaining\u00a0\u2026", "num_citations": "89\n", "authors": ["1041"]}
{"title": "Specification test coverage adequacy criteria= specification test generation inadequacy criteria\n", "abstract": " The successful analysis technique model checking can be employed as a test-case generation technique to generate tests from formal models. When using a model checker for test case generation, we leverage the witness (or counter-example) generation capability of model-checkers for constructing test cases. Test criteria are expressed as temporal properties and the witness traces generated for these properties are instantiated to create complete test sequences, satisfying the criteria. In this report we describe an experiment where we investigate the fault finding capability of test suites generated to provide three specification coverage metrics proposed in the literature (state, transition, and decision coverage). Our findings indicate that although the coverage may seem reasonable to measure the adequacy of a test suite, they are unsuitable when used to generate test suites. In short, the generated test sequences\u00a0\u2026", "num_citations": "88\n", "authors": ["1041"]}
{"title": "Automated oracle creation support, or: How I learned to stop worrying about fault propagation and love mutation testing\n", "abstract": " In testing, the test oracle is the artifact that determines whether an application under test executes correctly. The choice of test oracle can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support the selection of test inputs, little work exists for supporting oracle creation. In this work, we propose a method of supporting test oracle creation. This method automatically selects the oracle data - the set of variables monitored during testing - for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experiments over four industrial examples demonstrate that our method may be a cost-effective approach for producing small, effective oracle data, with fault finding improvements over current industrial best practice of up to 145.8% observed.", "num_citations": "84\n", "authors": ["1041"]}
{"title": "Providing the shalls\n", "abstract": " Incomplete, inaccurate, ambiguous, and volatile requirements have plagued the software industry since its inception. The convergence of model-based development and formal methods offers developers of safety-critical systems a powerful new approach to the early validation of requirements. This paper describes an exercise conducted to determine if formal methods could be used to validate system requirements early in the lifecycle at reasonable cost. Several hundred functional and safety requirements for the mode logic of a typical flight guidance system were captured as natural language \u201cshall\u201d statements. A formal model of the mode logic was written in the RSML\u2212e language and translated into the NuSMV model checker and the PVS theorem prover using translators developed as part of the project. Each \u201cshall\u201d statement was manually translated into a NuSMV or PVS property and proven using\u00a0\u2026", "num_citations": "80\n", "authors": ["1041"]}
{"title": "On the danger of coverage directed test case generation\n", "abstract": " In the avionics domain, the use of structural coverage criteria is legally required in determining test suite adequacy. With the success of automated test generation tools, it is tempting to use these criteria as the basis for test generation. To more firmly establish the effectiveness of such approaches, we have generated and evaluated test suites to satisfy two coverage criteria using counterexample-based test generation and a random generation approach, contrasted against purely random test suites of equal size.               Our results yield two key conclusions. First, coverage criteria satisfaction alone is a poor indication of test suite effectiveness. Second, the use of structural coverage as a supplement\u2014not a target\u2014for test generation can have a positive impact. These observations points to the dangers inherent in the increase in test automation in critical systems and the need for more research in how\u00a0\u2026", "num_citations": "78\n", "authors": ["1041"]}
{"title": "Compositional verification of a medical device system\n", "abstract": " Complex systems are by necessity hierarchically organized. Decomposition into subsystems allows for intellectual control, as well as enabling different subsystems to be created by distinct teams. This decomposition affects both requirements and architecture. The architecture describes the structure and this affects how requirements``flow down''to each subsystem. Moreover, discoveries in the design process may affect the requirements. Demonstrating that a complex system satisfies its requirements when the subsystems are composed is a challenging problem.", "num_citations": "72\n", "authors": ["1041"]}
{"title": "Mode confusion analysis of a flight guidance system using formal methods\n", "abstract": " The paper discusses mode confusion analysis of a flight guidance system using formal methods. The paper describes the use of automated analysis tools, such as model-checkers and theorem provers, to search for potential sources of mode confusion in a representative specification of the mode logic of a Flight Guidance System.", "num_citations": "64\n", "authors": ["1041"]}
{"title": "Structuring product family requirements for n-dimensional and hierarchical product lines\n", "abstract": " The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to a safer system. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. In this paper, we draw on our experiences in applying the software product-line approach to a family of mobile robots, a family of flight guidance systems, and a family of cardiac pacemakers, as well as case studies done by others to (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains and (2) demonstrate our progress towards a solution using a set\u00a0\u2026", "num_citations": "58\n", "authors": ["1041"]}
{"title": "Behavioral fault modeling for model-based safety analysis\n", "abstract": " Recent work in the area of model-based safety analysis has demonstrated key advantages of this methodology over traditional approaches, for example, the capability of automatic generation of safety artifacts. Since safety analysis requires knowledge of the component faults and failure modes, one also needs to formalize and incorporate the system fault behavior into the nominal system model. Fault behaviors typically tend to be quite varied and complex, and incorporating them directly into the nominal system model can clutter it severely. This manual process is error-prone and also makes model evolution difficult. These issues can be resolved by separating the fault behavior from the nominal system model in the form of a \"fault model\", and providing a mechanism for automatically combining the two for analysis. Towards implementing this approach we identify key requirements for a flexible behavioral fault\u00a0\u2026", "num_citations": "57\n", "authors": ["1041"]}
{"title": "Observable modified condition/decision coverage\n", "abstract": " In many critical systems domains, test suite adequacy is currently measured using structural coverage metrics over the source code. Of particular interest is the modified condition/decision coverage (MC/DC) criterion required for, e.g., critical avionics systems. In previous investigations we have found that the efficacy of such test suites is highly dependent on the structure of the program under test and the choice of variables monitored by the oracle. MC/DC adequate tests would frequently exercise faulty code, but the effects of the faults would not propagate to the monitored oracle variables. In this report, we combine the MC/DC coverage metric with a notion of observability that helps ensure that the result of a fault encountered when covering a structural obligation propagates to a monitored variable; we term this new coverage criterion Observable MC/DC (OMC/DC). We hypothesize this path requirement will make\u00a0\u2026", "num_citations": "56\n", "authors": ["1041"]}
{"title": "Model-based safety analysis final report\n", "abstract": " System safety analysis techniques are well established and are used extensively during the design of safety-critical systems. Despite this, most of the techniques are highly subjective and dependent on the skill of the practitioner. Since these analyses are usually based on an informal system model, it is unlikely that they will be complete, consistent, and error free. In fact, the lack of precise models of the system architecture and its failure modes often forces the safety analysts to devote much of their effort to gathering architectural details about the system behavior from several sources and embedding this information in the safety artifacts such as the fault trees.This report describes Model-Based Safety Analysis, an approach in which the system and safety engineers share a common system model created using a model-based development process. By extending the system model with a fault model as well as relevant portions of the physical system to be controlled, automated support can be provided for much of the safety analysis. We believe that by using a common model for both system and safety engineering and automating parts of the safety analysis, we can both reduce the cost and improve the quality of the safety analysis. Here we present our vision of model-based safety analysis and discuss the advantages and challenges in making this approach practical.", "num_citations": "55\n", "authors": ["1041"]}
{"title": "Generating MC/DC adequate test sequences through model checking\n", "abstract": " We present a method for automatically generating test sequences to satisfy MC/DC like structural coverage criteria of software behavioral models specified in state-based formalisms. The use of temporal logic for characterizing test criteria and the application of model-checking techniques for generating test sequences to those criteria have been of interest in software verification research for some time. Nevertheless, criteria for which constraints span more than one test sequence, such as the Modified Condition/Decision Coverage (MC/DC) mandated for critical avionics software, cannot be characterized in terms of a single temporal property.  This paper discusses a method for recasting two-sequence constraints in the original model as a single sequence constraint expressed in temporal logic on a slightly modified model. The test-sequence generated by a model-checker for the modified model can be easily separated into two different test-sequences for the original model, satisfying the given test criteria. The approach has been successful in generating MC/DC test sequences from a model of the mode-logic in a flight-guidance system.", "num_citations": "55\n", "authors": ["1041"]}
{"title": "Formal verification of flight critical software\n", "abstract": " Recent advances in modeling languages have made it feasible to formally specify and analyze the behavior of large system components. Synchronous data flow languages, such as Lustre, SCR, and RSML-e are well suited to this task, and commercial versions of these tools such as SCADE and Simulink are growing rapidly in popularity among designers of safety critical systems, largely due to their ability to automatically generate code from the models. At the same time, advances in formal analysis tools have made it practical to formally verify important properties of these models to ensure that design defects are identified and corrected early in the lifecycle. This report describes how such formal verification tools have been applied to the FCS 5000, a new family of Flight Control Systems being developed by Rockwell Collins Inc.", "num_citations": "54\n", "authors": ["1041"]}
{"title": "Proving the shalls\n", "abstract": " This paper describes an experiment conducted to determine how effectively formal methods could be used to capture and validate the requirements of a typical embedded system. A model of the mode logic of a Flight Guidance System was specified in the RSML\u2009\u2212\u2009e notation and translated into the NuSMV model checker and the PVS theorem prover. These tools were then used to verify several hundred properties of the RSML\u2009\u2212\u2009e model. In the process, several errors were discovered and corrected in the original model. This demonstrates that formal requirements models can be written for real problems and that formal analysis tools have matured to the point where they can be used to find errors before implementation. It also points out a clear relationship between requirements stated informally as \"shalls\", formal properties, and requirements models.", "num_citations": "54\n", "authors": ["1041"]}
{"title": "Model-based safety analysis\n", "abstract": " System safety analysis techniques are well established and are used extensively during the design of safety-critical systems. Despite this, most of the techniques are highly subjective and dependent on the skill of the practitioner. Since these analyses are usually based on an informal system model, it is unlikely that they will be complete, consistent, and error free. In fact, the lack of precise models of the system architecture and its failure modes often forces the safety analysts to devote much of their effort to gathering architectural details about the system behavior from several sources and embedding this information in the safety artifacts such as the fault trees.This report describes Model-Based Safety Analysis, an approach in which the system and safety engineers share a common system model created using a model-based development process. By extending the system model with a fault model as well as relevant portions of the physical system to be controlled, automated support can be provided for much of the safety analysis. We believe that by using a common model for both system and safety engineering and automating parts of the safety analysis, we can both reduce the cost and improve the quality of the safety analysis. Here we present our vision of model-based safety analysis and discuss the advantages and challenges in making this approach practical.", "num_citations": "53\n", "authors": ["1041"]}
{"title": "Test-sequence generation from formal requirement models\n", "abstract": " This paper discusses a method for generating test sequences from state-based specifications. We show how a model checker can be used to automatically generate complete test sequences that will provide arbitrary structural coverage of requirements specified in a high-level language like SCR or RSML/sup -e/. We have defined a language independent formal foundation for test sequence generation using model checkers that is suitable for representing software artifacts like requirements models, software specifications, and code. This paper shows a concrete application of our formal framework for test generation in the requirements modeling domain. The framework allows one to define structural coverage criteria in terms of the formal model of a software artifact and describes how test sequences can be generated to satisfy those coverage criteria using a model-checker. The approach is illustrated using\u00a0\u2026", "num_citations": "46\n", "authors": ["1041"]}
{"title": "On the requirements of high-integrity code generation\n", "abstract": " Although formal requirements specifications can provide a complete and consistent description of a safety-critical software system, designing and developing production quality code from high-level specifications can be a time-consuming and error-prone process. Automated translation, or code generation, of the specification to production code can alleviate many of the problems associated with design and implementation. However, current approaches have been unsuitable for safety-critical environments because they employ complex and/or ad-hoc methods for translation. In this paper we discuss the issues involved in automatic code generation for high-assurance systems and define a set of requirements that code generators for this domain must satisfy. These requirements cover the formality of the translation, the quality of the code generator, and the properties of the generated code.", "num_citations": "41\n", "authors": ["1041"]}
{"title": "Model checking RSML/sup-e/requirements\n", "abstract": " Model checking is a promising technique for automated verification or refutation of software systems. Nevertheless, it has not been used widely in practice mainly due to the lack of the supporting tools that incorporate the model checking activity into the development process. As a part of our overall method supporting specification centered system development, we have implemented a translator between a formal specification language RSML/sup -e/ and a symbolic model checker NuSMV. Our translation and abstraction approach aims at usability in practice so that model checking can be used as a routine process during requirement analysis without requiring much knowledge about formal methods. Preliminary results from applying the system in a commercial setting is quite promising. We discuss our translation and abstraction approach in some depth and illustrate its feasibility with some preliminary results.", "num_citations": "40\n", "authors": ["1041"]}
{"title": "An approach to automatic code generation for safety-critical systems\n", "abstract": " Automated translation, or code generation, of a formal requirements model to production code can alleviate many of the problems associated with design and implementation. In this paper, we outline the requirements of such code generation to obtain a high level of confidence in the correctness of the translation process. We then describe a translator for a state-based modeling language called RSML (Requirements Specification Modeling Language) that largely meets these requirements.", "num_citations": "40\n", "authors": ["1041"]}
{"title": "Experiences using Statecharts for a system requirements specification\n", "abstract": " Some lessons learned and issues raised while building a system requirements specification for a real aircraft collision avoidance system using statecharts are described. Some enhancements to statecharts were necessary to model the complete system and a few notational changes were made to improve reviewability.< >", "num_citations": "36\n", "authors": ["1041"]}
{"title": "Experiences and lessons from the analysis of TCAS II\n", "abstract": " This report highlights some of the experiences gathered while analyzing the requirements specification for a commercial avionics system called TCAS II (Traffic alert and Collision Avoidance System II) for consistency and completeness. Completeness in this context is defined as a complete set of requirements, that is, there is a behavior specified for every possible input and input sequence.Under the leadership of Dr. Nancy G. Leveson, the Irvine Safety Research Group has developed a state-based requirements specification language RSML (Requirements State Machine Language) using TCAS II as a testbed [6]. The TCAS requirements specification project was very successful; RSML was well liked by all participants in the project, and the formal specification has been adopted as the official TCAS II requirements. The requirements document has been delivered to the FAA and has undergone an extensive\u00a0\u2026", "num_citations": "34\n", "authors": ["1041"]}
{"title": "Better testing through oracle selection:(NIER track)\n", "abstract": " In software testing, the test oracle determines if the application under test has performed an execution correctly. In current testing practice and research, significant effort and thought is placed on selecting test inputs, with the selection of test oracles largely neglected. Here, we argue that improvements to the testing process can be made by considering the problem of oracle selection. In particular, we argue that selecting the test oracle and test inputs together to complement one another may yield improvements testing effectiveness. We illustrate this using an example and present selected results from an ongoing study demonstrating the relationship between test suite selection, oracle selection, and fault finding.", "num_citations": "33\n", "authors": ["1041"]}
{"title": "The effect of program and model structure on the effectiveness of mc/dc test adequacy coverage\n", "abstract": " Test adequacy metrics defined over the structure of a program, such as Modified Condition and Decision Coverage (MC/DC), are used to assess testing efforts. However, MC/DC can be \u201ccheated\u201d by restructuring a program to make it easier to achieve the desired coverage. This is concerning, given the importance of MC/DC in assessing the adequacy of test suites for critical systems domains. In this work, we have explored the impact of implementation structure on the efficacy of test suites satisfying the MC/DC criterion using four real-world avionics systems. Our results demonstrate that test suites achieving MC/DC over implementations with structurally complex Boolean expressions are generally larger and more effective than test suites achieving MC/DC over functionally equivalent, but structurally simpler, implementations. Additionally, we found that test suites generated over simpler implementations achieve\u00a0\u2026", "num_citations": "32\n", "authors": ["1041"]}
{"title": "Specification and analysis of intercomponent communication\n", "abstract": " The correctness, safety and robustness of the specification of a critical system are assessed through a combination of rigorous specification capture and inspection, formal analysis of the specification, and execution and simulation of the specification. Any integrated approach to specifying critical systems should support all three activities. Embedded systems pose special challenges to the specification and analysis of intercomponent communication. The authors present a formal approach which lets the interface specifications serve as kernels that enforce safety and simple liveness constraints.", "num_citations": "32\n", "authors": ["1041"]}
{"title": "Using PVS to analyze hierarchical state-based requirements for completeness and consistency\n", "abstract": " Previously, we have defined procedures for analyzing hierarchical state based requirements specifications for two properties: (1) completeness with respect to a set of criteria related to robustness (a response is specified for every possible input and input sequence) and (2) consistency (the specification is free from conflicting requirements and undesired nondeterminism) (M.P.E. Heimdahl and N.G. Leveson, 1995; 1996). We implemented the analysis procedures in a prototype tool and evaluated their effectiveness and efficiency on a large real world requirements specification expressed in an hierarchical state based language called RSML (Requirements State Machine Language). Although our approach has been largely successful, there are some drawbacks with the current implementation that must be addressed. Our prototype implementation uses Binary Decision Diagrams (BDDs) to perform the analysis\u00a0\u2026", "num_citations": "32\n", "authors": ["1041"]}
{"title": "Partial translation verification for untrusted code-generators\n", "abstract": " Within the context of model-based development, the correctness of code generators for modeling notations such as Simulink and Stateflow is of obvious importance. If correctness of code generation can be shown, the extensive and often costly verification and validation activities conducted in the modeling domain could be effectively leveraged in the code domain. Unfortunately, most code generators in use today give no guarantees of correctness.               In this paper, we investigate a method of leveraging existing model checking tools to verify the partial correctness of code generated by code generators that offer no guarantees of correctness. We explore the feasibility of this approach through a prototype tool that allows us to verify that Linear Temporal Logic (LTL) safety properties are preserved by C code generators for Simulink models. We find that the approach scales well, allowing us to verify that 55\u00a0\u2026", "num_citations": "31\n", "authors": ["1041"]}
{"title": "SpecTRM: A CAD system for digital automation\n", "abstract": " SpecTRM (specification tools and requirements methodology) is a system engineering environment to support modeling and analysis during requirements generation, functional decomposition and tradeoff analysis, subsystem specification, implementation, verification, and system maintenance and evolution. A general goal is to build bridges among disciplines by providing integrated specifications and modeling tools that can be used by system engineers, software engineers, hardware engineers, and human factors experts. We also hope to provide seamless transitions and mappings between the various system development and maintenance stages. Because many automated real-time systems have safety-critical aspects, SpecTRM provides support for hazard analysis and building safety into the design. The safety information and activities on a project are integrated into the development and decision making\u00a0\u2026", "num_citations": "31\n", "authors": ["1041"]}
{"title": "Extending the product family approach to support n-dimensional and hierarchical product lines\n", "abstract": " The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to safer systems. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. The authors draw on their experiences in applying the software product-line approach to a family of mobile robots as well as case studies done by others to: (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains, and (2) demonstrate our initial progress towards a solution using a set-theoretic approach to reason about domains of what we call n-dimensional\u00a0\u2026", "num_citations": "30\n", "authors": ["1041"]}
{"title": "Generating code from hierarchical state-based requirements\n", "abstract": " Computer software is playing an increasingly important role in safety-critical embedded computer systems, where incorrect operation of the software could lead to loss of life, substantial material or environmental damage, or large monetary losses. Although software is a powerful and flexible tool for industry, these very advantages have contributed to a corresponding increase in system complexity. In a previous investigation, the Irvine Safety Research Group developed a requirements specification language called the Requirements State Machine Language (RSML) suitable for the specification of safety critical control embedded systems. To simplify and automate the design and implementation process, we have investigated the possibility of automatically generating code from RSML specifications.", "num_citations": "30\n", "authors": ["1041"]}
{"title": "Requirements coverage as an adequacy measure for conformance testing\n", "abstract": " Conformance testing in model-based development refers to the testing activity that verifies whether the code generated (manually or automatically) from the model is behaviorally equivalent to the model. Presently the adequacy of conformance testing is inferred by measuring structural coverage achieved over the model. We hypothesize that adequacy metrics for conformance testing should consider structural coverage over the requirements either in place of or in addition to structural coverage over the model. Measuring structural coverage over the requirements gives a notion of how well the conformance tests exercise the required behavior of the system.               We conducted an experiment to investigate the hypothesis stating structural coverage over formal requirements is more effective than structural coverage over the model as an adequacy measure for conformance testing. We found that the\u00a0\u2026", "num_citations": "26\n", "authors": ["1041"]}
{"title": "Automated oracle data selection support\n", "abstract": " The choice of test oracle-the artifact that determines whether an application under test executes correctly-can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support test input selection, little work exists for supporting oracle creation. We propose a method of supporting test oracle creation that automatically selects the oracle data-the set of variables monitored during testing-for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experimental results obtained by employing our method over six industrial systems (while varying test input types and the number of generated mutants) indicate that our method-when paired with test inputs generated either at random or to satisfy specific structural coverage criteria-may be a cost\u00a0\u2026", "num_citations": "25\n", "authors": ["1041"]}
{"title": "A methodology for the design and verification of globally asynchronous/locally synchronous architectures\n", "abstract": " Recent advanced in model-checking have made it practical to formally verify the correctness of many complex synchronous systems (ie, systems driven by a single clock). However, many computer systems are implemented by asynchronously composing several synchronous components, where each component has its own clock and these clocks are not synchronized. Formal verification of such Globally Asynchronous/Locally Synchronous (GA/LS) architectures is a much more difficult task. In this report, we describe a methodology for developing and reasoning about such systems. This approach allows a developer to start from an ideal system specification and refine it along two axes. Along one axis, the system can be refined one component at a time towards an implementation. Along the other axis, the behavior of the system can be relaxed to produce a more cost effective but still acceptable solution. We illustrate this process by applying it to the synchronization logic of a Dual Fight Guidance System, evolving the system from an ideal case in which the components do not fail and communicate synchronously to one in which the components can fail and communicate asynchronously. For each step, we show how the system requirements have to change if the system is to be implemented and prove that each implementation meets the revised system requirements through model-checking.", "num_citations": "25\n", "authors": ["1041"]}
{"title": "Interaction testing in model-based development: Effect on model-coverage\n", "abstract": " Model-based software development is gaining interest in domains such as avionics, space, and automotives. The model serves as the central artifact for the development efforts (such as, code generation), therefore, it is crucial that the model be extensively validated. Automatic generation of interaction test suites is a candidate for partial automation of this model validation task. Interaction testing is a combinatorial approach that systematically tests all t-way combinations of inputs for a system. In this paper, we report how well interaction test suites (2-way through 5-way interaction test suites) structurally cover a model of the mode- logic of a flight guidance system. We conducted experiments to (1) compare the coverage achieved with interaction test suites to that of randomly generated tests and (2) determine if interaction test suites improve the coverage of black-box test suites derived from system requirements. The\u00a0\u2026", "num_citations": "24\n", "authors": ["1041"]}
{"title": "Deviation analysis: A new use of model checking\n", "abstract": " Inaccuracies, or deviations, in the measurements of monitored variables in a control system are facts of life that control software must accommodate. Deviation analysis can be used to determine how a software specification will behave in the face of such deviations. Deviation analysis is intended to answer questions such as \u201cWhat is the effect on output O if input I is off by 0 to 100?\u201d. This property is best checked with some form of symbolic execution approach. In this report we wish to propose a new approach to deviation analysis using model checking techniques. The key observation that allows us to use model checkers is that the property can be restated as \u201cWill there be an effect on output O if input I is off by 0 to 100?\u201d\u2014this restatement of the property changes the analysis from an exploratory analysis to a verification task suitable for model checking.", "num_citations": "24\n", "authors": ["1041"]}
{"title": "On the advantages of approximate vs. complete verification: Bigger models, faster, less memory, usually accurate\n", "abstract": " We have been exploring LURCH, an approximate (not necessarily complete) alternative to traditional model checking based on a randomized search algorithm. Randomized algorithms like LURCH have been known to outperform their deterministic counterparts for search problems representing a wide range of applications. The cost of an approximate strategy is the potential for inaccuracy. If complete algorithms terminate, they find all the features they are searching for. On the other hand, by its very nature, randomized search can miss important features. Our experiments suggest that this inaccuracy problem is not too serious. In the case studies presented here and elsewhere, LURCHS random search usually found the correct results. Also, these case studies strongly suggest that LURCH can scale to much larger models than standard model checkers like NuSMV and SPIN. The two case studies presented in this\u00a0\u2026", "num_citations": "24\n", "authors": ["1041"]}
{"title": "A software certification consortium and its top 9 hurdles\n", "abstract": " In August of 2007 and December of 2007, North American academic researchers, industry representatives and regulators were invited to meetings in Washington and Minneapolis, respectively, with the goal of forming a Software Certification Consortium (SCC). At the first meeting, objectives were established for the consortium and a certification grand challenge was issued. At the second meeting, all participants were asked to complete the statement: \u201cSoftware certification is hard because \u2026\u201d. The group then synthesized the results into a \u201cTop 9\u201d list by means of discussion and voting. In this article, we describe the goals that we believe should be the goals of SCC, via details of these Top 9 hurdles that are preventing us from making software certification part of the mainstream.", "num_citations": "23\n", "authors": ["1041"]}
{"title": "On MC/DC and implementation structure: An empirical study\n", "abstract": " In civil avionics, obtaining DO-178B certification for highly critical airborne software requires that the adequacy of the code testing effort be measured using a structural coverage criterion known as Modified Condition and Decision Coverage (MC/DC). We hypothesized that the effectiveness of the MC/DC metric is highly sensitive to the structure of the implementation and can therefore be problematic as a test adequacy criterion. We tested this hypothesis by evaluating the fault-finding ability of MC/DC-adequate test suites on five industrial systems (flight guidance and display management). For each system, we created two versions of the implementations-implementations with and without expression folding (i.e., inlining).", "num_citations": "23\n", "authors": ["1041"]}
{"title": "Coverage-directed test generation with model checkers: Challenges and opportunities\n", "abstract": " When using tools to automatically generate tests-suites from a specification, the selection of coverage criterion that guides the generation process is of imperative importance. In a previous study that evaluated test generation with model checking, we observed that although a coverage criterion may seem reasonable when instrumenting a model or code to measure the adequacy of a test suite, it may be unsuitable when formalized and used to guide the model checker to generate a test suite; the generated tests technically provide adequate coverage according to the formalization, but do so in a way that exercises only small portions of the system under study and finds few faults. Based on those results, we concluded that fully automated test-suite generation techniques must be pursued with great caution and that coverage criteria specifically addressing test-suite generation from formal specifications are needed. In\u00a0\u2026", "num_citations": "23\n", "authors": ["1041"]}
{"title": "Coverage metrics for requirements-based testing: Evaluation of effectiveness\n", "abstract": " In black-box testing, the tester creates a set of tests to exercise a system under test without regard to the internal structure of the system. Generally, no objective metric is used to measure the adequacy of black-box tests. In recent work, we have proposed three requirements coverage metrics, allowing testers to objectively measure the adequacy of a black-box test suite with respect to a set of requirements formalized as Linear Temporal Logic (LTL) properties. In this report, we evaluate the effectiveness of these coverage metrics with respect to fault finding. Specifically, we conduct an empirical study to investigate two questions: (1) do test suites satisfying a requirements coverage metric provide better fault finding than randomly generated test suites of approximately the same size?, and (2) do test suites satisfying a more rigorous requirements coverage metric provide better fault finding than test suites satisfying a less rigorous requirements coverage metric?  Our results indicate (1) that test suites satisfying more rigorous coverage metrics provide better fault finding than test suites satisfying less rigorous coverage metrics and (2) only one coverage metric proposed\u00e2\u20ac\u201dUnique First Cause (UFC) coverage\u00e2\u20ac\u201dis sufficiently rigorous to ensure test suites satisfying the metric outperform randomly generated test suites of similar size.", "num_citations": "22\n", "authors": ["1041"]}
{"title": "Automatic abstraction for model checking software systems with interrelated numeric constraints\n", "abstract": " Model checking techniques have not been effective in important classes of software systems characterized by large (or infinite) input domains with interrelated linear and non-linear constraints over the input variables. Various model abstraction techniques have been proposed to address this problem. In this paper, we wish to propose domain abstraction based on data equivalence and trajectory reduction as an alternative and complement to other abstraction techniques. Our technique applies the abstraction to the input domain (environment) instead of the model and is applicable to constraint-free and deterministic constrained data transition system. Our technique is automatable with some minor restrictions.", "num_citations": "22\n", "authors": ["1041"]}
{"title": "An integrated development environment for prototyping safety critical systems\n", "abstract": " The development of software for safety critical, embedded computer systems has been widely addressed in literature. Nevertheless, there does not currently exist any single environment which provides adequate support for all of the following: static analysis, system simulation, animation and visualization, specification reuse, and refinement (from high-level requirements to implementation). In this paper we present an overview of such an environment that is currently under development at the University of Minnesota concentrating on the prototyping capabilities and refinement model.", "num_citations": "22\n", "authors": ["1041"]}
{"title": "On the effectiveness of slicing hierarchical state machines: A case study\n", "abstract": " Formal specifications can be hundreds of pages in length-a reflection of the size and complexity of the systems being specified. Lengthy documents are difficult to read understand, and use. Program slicing was developed to address these issues for programs. The authors apply similar techniques to formal specifications expressed as hierarchical state machines. They present a two tiered approach to slicing (or simplification) of hierarchical state machines. They have applied their techniques to a large case study and present empirical data highlighting the reduction and simplification capabilities of their approach to large specifications.", "num_citations": "22\n", "authors": ["1041"]}
{"title": "Checking properties of safety critical specifications using efficient decision procedures\n", "abstract": " The increasing use of software in safety critical systems entails increasing complexity, challenging the safety of these systems. Although formal specifications of real-life systems are orders of magnitude simpler than the system implementations, they are still quite complex. It is easy to overlook problems in a specification, ultimately compromising the safety of the implementation. Since it is error-prone and time consuming to check large specifications manually, mechanical support is needed. The challenge is to find the right combination of deductive power (ie, how rich a logic and what theories are decided) and efficiency to complete the verification in reasonable time. In addition, it must be possible to explain why a proof fails. As an initial approach to solving this problem, we have adapted the Stanford Validity Checker (SVC), a highly efficient, general-purpose decision procedure for quantiflerfree first-order logic with\u00a0\u2026", "num_citations": "22\n", "authors": ["1041"]}
{"title": "Model validation using automatically generated requirements-based tests\n", "abstract": " In current model-based development practice, validation that we are building a correct model is achieved by manually deriving requirements-based test cases for model testing. Model validation performed this way is time consuming and expensive, particularly in the safety critical systems domain where high confidence in the model correctness is required. In an effort to reduce the validation effort, we propose an approach that automates the generation of requirements- based tests for model validation purposes. Our approach uses requirements formalized as LTL properties as a basis for test generation. Test cases are generated to provide rigorous coverage over these formal properties. We use an abstract model in this paper-called the Requirements Model-generated from requirements and environmental constraints for automated test case generation. We illustrate and evaluate our approach using three realistic\u00a0\u2026", "num_citations": "20\n", "authors": ["1041"]}
{"title": "Flexible and extensible notations for modeling languages\n", "abstract": " In model-based development, a formal description of the software (the model) is the central artifact that drives other development activities. The availability of a modeling language well-suited for the system under development and appropriate tool support are of utmost importance to practitioners. Considering the diverse needs of different application domains, flexibility in the choice of modeling languages and tools may advance the industrial acceptance of formal methods.               We describe a flexible modeling language framework by which language and tool developers may better meet the special needs of various users groups without incurring prohibitive costs. The framework is based on a modular and extensible implementation of languages features using attribute grammars and forwarding. We show a prototype implementation of such a framework by extending the host language Mini-Lustre, an\u00a0\u2026", "num_citations": "20\n", "authors": ["1041"]}
{"title": "Deviation analysis through model checking\n", "abstract": " Inaccuracies, or deviations, in the measurements of monitored variables in a control system are facts of life that control software must accommodate $the software is expected to continue functioning correctly in the face of an expected range of deviations in the inputs. Deviation analysis can be used to determine how a software specification will behave in the face of such deviations in data from the environment. The idea is to describe the correct values of an environmental quantity; along with a range of potential deviations, and then determine the effects on the outputs of the system. The analyst can then check whether the behavior of the software is acceptable with respect to these deviations. In this report we wish to propose a new approach to deviation analysis using model checking techniques. This approach allows for more precise analysis than previous techniques, and refocuses deviation analysis from an\u00a0\u2026", "num_citations": "20\n", "authors": ["1041"]}
{"title": "Modeling and requirements on the physical side of cyber-physical systems\n", "abstract": " In a cyber-physical system (a system where the physical world interacts extensively with-often networked-software), the physical portion of the system resides in the continuous and continual domain. Thus, on the physical side of cyber-physical systems we will have to contend with not only real time requirements but also the continuous and continual nature of the system. This poses a new set of challenges for requirements engineering; we must write well defined requirements to address crucial issues not commonly addressed in the software domain. For example, the rate of change of a controlled variable, the time it takes for a controlled variable to settle sufficiently close to a set-point, and the cumulative errors built up over time may be of critical importance. In this paper we outline how early modeling in the continuous domain serves as a crucial aid in the elicitation and discovery of requirements for cyber-physical\u00a0\u2026", "num_citations": "19\n", "authors": ["1041"]}
{"title": "Structuring formal control systems specifications for reuse: Surviving hardware changes\n", "abstract": " The beginnings of our approach is a high-level re-quirements structuring technique based on the relationship between system requirements and the soft-ware specification. We developed this structuring technique to enable a software development approach we call specification-based prototyping (23] where the formal requirements model is used as a prototype (possibly controlling the actual hardware-hardware-in-the-loop-simulation) during the early stages of a project. Here we present how this structuring approach also enables reuse of the high-level requirements across members of a product family with vari-abilities in the hardware components. The approach is", "num_citations": "18\n", "authors": ["1041"]}
{"title": "Requirements Capture and Evaluation in Nimbus: The Light Control Case Study\n", "abstract": " Evaluations of methods and tools applied to a reference problem are useful when comparing various techniques. In this paper, we present a solution to the challenge of capturing the requirements for the Light Control System case study, which was proposed before the Dagstuhl Seminar on Requirements Capture, Documentation, and Validation in June of 1999.The paper focuses primarily on how the requirements were specified: what techniques were used, and what the results were. The language used to capture the requirements is RSML-e; a state-based specification language with a fully specified formal denotational semantics. In addition, the Nimbus environment-a toolset supporting RSML-e-is used to visualize and execute the high-level requirements.", "num_citations": "18\n", "authors": ["1041"]}
{"title": "Structuring simulink models for verification and reuse\n", "abstract": " Model-based development (MBD) tool suites such as Simulink and Stateflow offer powerful tools for design, development, and analysis of models. These models can be used for several purposes: for code generation, for prototyping, as descriptions of an environment (plant) that will be controlled by software, as oracles for a testing process, and many other aspects of software development. In addition, a goal of model-based development is to develop reusable models that can be easily managed in a version-controlled continuous integration process.", "num_citations": "17\n", "authors": ["1041"]}
{"title": "Complete traceability for requirements in satisfaction arguments\n", "abstract": " When establishing associations, known as tracelinks, between a requirement and the artifacts that lead to itssatisfaction, it is essential to know what the links mean. Whileresearch into this type of traceability-what we call RequirementsSatisfaction Traceability-has been an active research area forsome time, none of the literature discusses the fact that thereare often multiple ways in which a requirement can be satisfied, i.e., there are multiple satisfaction arguments. The distinctionbetween establishing a single satisfaction argument between arequirement and its implementation (tracing one way the requirement is implemented) vs. tracing all satisfaction arguments, and the possible ramifications for how the trace links can beused in analysis, has not been well studied. We examine how thisdistinction changes the way traceability is perceived, established, maintained, and used. In this RE@Next! paper, we introduce\u00a0\u2026", "num_citations": "15\n", "authors": ["1041"]}
{"title": "Experiences from specifying the TCAS II requirements using RSML\n", "abstract": " Discusses an approach to specifying system requirements for real-time, reactive systems, some criteria that should be used in designing a language for such requirements, and some lessons learned while writing a system requirements specification for an aircraft collision avoidance system. Since the completion of the project we have made advances in many areas, two of which are of particular interest to the practising engineer: (1) specification and traceability of intent and design rationale; and (2) an improved specification language and modeling methodology.", "num_citations": "15\n", "authors": ["1041"]}
{"title": "Proof-based coverage metrics for formal verification\n", "abstract": " When using formal verification on critical software, an important question involves whether we have we specified enough properties for a given implementation model. To address this question, coverage metrics for property-based formal verification have been proposed. Existing metrics are usually based on mutation, where the implementation model is repeatedly modified and re-analyzed to determine whether mutant models are \"killed\" by the property set. These metrics tend to be very expensive to compute, as they involve many additional verification problems. This paper proposes an alternate family of metrics that can be computed using the recently introduced idea of Inductive Validity Cores (IVCs). IVCs determine a minimal set of model elements necessary to establish a proof. One of the proposed metrics is both rigorous and substantially cheaper to compute than mutation-based metrics. In addition, unlike the\u00a0\u2026", "num_citations": "14\n", "authors": ["1041"]}
{"title": "Model checking software requirement specifications using domain reduction abstraction\n", "abstract": " As an automated verification and validation tool, model checking can be quite effective in practice. Nevertheless, model checking has been quite inefficient when dealing with systems with data variables over a large (or infinite) domain, which is a serious limiting factor for its applicability in practice. To address this issue, we have investigated a static abstraction technique, domain reduction abstraction, based on data equivalence and trajectory reduction, and implemented it as a prototype extension of the symbolic model checker NuSMV. Unlike on-the-fly dynamic abstraction techniques, domain reduction abstraction statically analyzes specifications and automatically produces an abstract model which can be reused over time; a feature suitable for regression verification.", "num_citations": "14\n", "authors": ["1041"]}
{"title": "NIMBUS: A tool for specification centered development\n", "abstract": " Assurance that a formal specification (system specification or software specification) possesses desired properties can be achieved through (1) manual inspections, (2) formal verification of the desired properties, or (3) simulation and testing of the specification. To achieve the high level of confidence in the correctness required in a safety-critical system, all three approaches must be used in concert. We have developed an specification language, called RSML/sup -e/, and an environment, called NIMBUS, which provides support for all these activities. The three V&V techniques fill complementary roles within the validation and verification process. Manual inspections and visualization provide the specification team, customers, and regulatory representatives the means to informally verify that the behavior described formally matches the desired \"real world\" behavior of the system. RSML/sup -e/ is a fully formal\u00a0\u2026", "num_citations": "14\n", "authors": ["1041"]}
{"title": "Modes, features, and state-based modeling for clarity and flexibility\n", "abstract": " The behavior of a complex system is frequently defined in terms of operational modes-mutually exclusive sets of the system behaviors. Within the operational modes, collections of features define the behavior of the system. Lucent and understandable modeling of operational modes and features using common state-based notations such as Statecharts or Stateflow can be challenging. In this paper we share some of our experiences from modeling modes and features in the medical device domain. We discuss the challenges and present a generic approach to structuring the modes and features of a generic Patient-Controlled Analgesia infusion pump.", "num_citations": "13\n", "authors": ["1041"]}
{"title": "Using models to address challenges in specifying requirements for medical cyber-physical systems\n", "abstract": " Gathering and analyzing Cyber-Physical System (CPS) requirements pose some challenges to the requirements engineering community warranting a fresh perspective on requirement engineering methods; a perspective that is sensitive to the interplay between the cyber and physical aspects of the system. In this paper we share our experiences and lessons learned in the process of formulating requirements for a generic version of an infusion pump, a commonly used piece of medical equipment. Specifically, determining the precise scope of the system and finding its significant attributes in the continuous physical domain in which it operates were surprisingly difficult. To address these challenges, we pursued a model-based approach, which we believe is broadly applicable to CPS requirements elicitation and specification.", "num_citations": "12\n", "authors": ["1041"]}
{"title": "On the effect of test-suite reduction on automatically generated model-based tests\n", "abstract": " Model checking techniques can be successfully employed as a test-case generation technique to generate tests from formal models. The number of tests-cases produced, however, is typically large for complex coverage criteria such as MC/DC. Test-suite reduction can provide us with a smaller set of test-cases that preserve the original coverage\u2014often a dramatically smaller set. Nevertheless, one potential drawback with test-suite reduction is that this might affect the quality of the test-suite in terms of fault finding. Previous empirical studies provide conflicting evidence on this issue. To further investigate the problem and determine its effect when testing implementations derived from formal models of software we performed an experiment using a large case example of a Flight Guidance System, generated reduced test-suites for a variety of structural coverage criteria while preserving coverage, and recorded\u00a0\u2026", "num_citations": "12\n", "authors": ["1041"]}
{"title": "Improving the accuracy of oracle verdicts through automated model steering\n", "abstract": " The oracle-a judge of the correctness of the system under test (SUT)-is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as oracles. These models, however, typically represent an idealized system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the oracle.", "num_citations": "10\n", "authors": ["1041"]}
{"title": "Moving the goalposts: coverage satisfaction is not enough\n", "abstract": " Structural coverage criteria have been proposed to measure the adequacy of testing efforts. Indeed, in some domains\u2014eg, critical systems areas\u2014structural coverage criteria must be satisfied to achieve certification. The advent of powerful search-based test generation tools has given us the ability to generate test inputs to satisfy these structural coverage criteria. While tempting, recent empirical evidence indicates these tools should be used with caution, as merely achieving high structural coverage is not necessarily indicative of high fault detection ability. In this report, we review some of these findings, and offer recommendations on how the strengths of search-based test generation methods can alleviate these issues.", "num_citations": "10\n", "authors": ["1041"]}
{"title": "Your what is my how: Why requirements and architectural design should be iterative\n", "abstract": " Systems are naturally constructed in hierarchies in which design choices made at higher levels of abstraction levy requirements on system components at lower levels of abstraction. Thus, whether an aspect of the system is a design choice or a requirement depends largely on one's location within the hierarchy of system components. In addition, it is often the case that systems are not constructed top-down, but rather middle-out; compatibility with existing systems and architectures, or availability of specific physical components may influence high-level requirements. Despite these facts, several of the reference models commonly used for requirements, including the four-variable model and world machine model, do not account for hierarchical decomposition. In this position paper, we argue that requirements and architectural design should be more closely aligned: that requirements reference models should account\u00a0\u2026", "num_citations": "10\n", "authors": ["1041"]}
{"title": "Flexibility in modeling languages and tools: A call to arms\n", "abstract": " In model-based development, the software development effort is centered around a formal description of the proposed software system; a description that can be subjected to various types of analysis and code generation. Based on years of experience with model-based development and formal modeling we believe that the following conjectures describe fundamental obstacles to wide adoption of formal modeling and the potential for automation that comes with it; (1) no single modeling notation will suit all, or even most, modeling needs, (2) no analysis tool will fit all, or even most, analysis tasks, and (3) flexible and stable tools must be made available for realistic evaluations and technology transfer. These conjectures form the basis for the call to arms outlined in this report. To make automated software engineering techniques more useful for more types of developers and allow us to move forward as a\u00a0\u2026", "num_citations": "10\n", "authors": ["1041"]}
{"title": "Automated steering of model-based test oracles to admit real program behaviors\n", "abstract": " The test oracle-a judge of the correctness of the system under test (SUT)-is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as test oracles. These models, however, typically represent an idealized system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the test oracle. We propose an automated steering framework that can adjust the behavior of the model to better match the behavior of the SUT to reduce the rate of false positives. This model steering is limited\u00a0\u2026", "num_citations": "9\n", "authors": ["1041"]}
{"title": "Using PVS to prove properties of systems modelled in a synchronous dataflow language\n", "abstract": " We report on our experience with using the PVS theorem prover as a verification tool for analyzing systems modelled in RSML\u2212\u2009e                          \u2013 a synchronous dataflow language. RSML\u2212\u2009e                          is a formal specification language particularly well-suited for specifying requirements of reactive systems. We advocate a specification-centered approach to system development, in which various development activities like prototyping, analysis, verification, testing, and code-generation are based on a formal model of the system requirements. To support the analysis and verification activities, we developed a translator from RSML\u2212\u2009e                          to PVS as part of our toolset. We used these tools to successfully verify properties of the mode logic of a flight-guidance system specified in RSML\u2212\u2009e                          by our industrial partner, Rockwell Collins Inc. The results from this exercise are encouraging\u00a0\u2026", "num_citations": "9\n", "authors": ["1041"]}
{"title": "Toward automation for model-checking requirements specifications with numeric constraints\n", "abstract": " Model-checking techniques have not been effective in important classes of software systems \u2013 systems characterised by large (or infinite) input domains with interrelated linear and non-linear constraints over the system variables. Various model abstraction techniques have been proposed to address this problem, but their effectiveness in practice is limited by two factors: first, the abstraction process is manual and requires a great deal of ingenuity; and, second, the abstraction may be coarse and introduce too many spurious behaviours to provide meaningful analysis results. In this paper, we wish to propose domain reduction abstraction based on data equivalence and trajectory reduction as an alternative and complement to other abstraction tech niques. Our technique applies the abstraction to the input domain (environment) instead of the model and is applicable to constraint free and deterministic constrained\u00a0\u2026", "num_citations": "9\n", "authors": ["1041"]}
{"title": "Static analysis of state-based requirements analysis for completeness and consistency\n", "abstract": " Software requirements errors have been found to account for a majority of production software failures and have been implicated in a large number of accidents. Errors introduced during the requirements phase are also more costly to correct than errors introduced later in the life cycle. Therefore, techniques to provide adequate requirements specifications and to find errors early are of great importance.", "num_citations": "9\n", "authors": ["1041"]}
{"title": "First steps towards exporting education: Software engineering education delivered online to professionals\n", "abstract": " Large software organizations seek internal professional staff development beyond traditional corporate training in specific technical skills (i.e., a new programming language or tool). This paper describes the results of one effort of delivery: the offering of a non-credit small, private, online course (SPOC) in software design. Participants spanned those with formal degrees in Computer Science or Software Engineering to those with no formal education in the area. After completing the course, a survey was administered. Intention to enroll in further non-credit SPOC courses was found to be more likely than intention in formal degrees in the area. Additionally, the course content was highly valued by the participants. These findings show a need for further investigation into the value and opportunity of exported education: bringing University expertise out of the traditional classroom and directly into the hands of industry\u00a0\u2026", "num_citations": "8\n", "authors": ["1041"]}
{"title": "Automated integrative analysis of state-based requirements\n", "abstract": " Statically analyzing requirements specifications to assure that they possess desirable properties is an important activity in any rigorous software development project. The analysis is performed on an abstraction of the original requirements specification. Abstractions in the model may lead to spurious errors in the analysis output. Spurious errors are conditions that are reported as errors, but information abstracted out of the model precludes the reported conditions from being satisfied. A high ratio of spurious errors to true errors in the analysis output makes it difficult, error-prone, and time consuming to find and correct the true errors. We describe an iterative and integrative approach for analyzing state-based requirements that capitalizes on the strengths of a symbolic analysis component and a reasoning component while circumventing their weaknesses. The resulting analysis method is fast enough and automated\u00a0\u2026", "num_citations": "8\n", "authors": ["1041"]}
{"title": "Efficient observability-based test generation by dynamic symbolic execution\n", "abstract": " Structural coverage metrics have been widely used to measure test suite adequacy as well as to generate test cases. In previous investigations, we have found that the fault-finding effectiveness of tests satisfying structural coverage criteria is highly dependent on program syntax - even if the faulty code is exercised, its effect may not be observable at the output. To address these problems, observability-based coverage metrics have been defined. Specifically, Observable MC/DC (OMC/DC) is a criterion that appears to be both more effective at detecting faults and more robust to program restructuring than MC/DC. Traditional counterexample-based test generation for OMC/DC, however, can be infeasible on large systems. In this study, we propose an incremental test generation approach that combines the notion of observability with dynamic symbolic execution. We evaluated the efficiency and effectiveness of our\u00a0\u2026", "num_citations": "7\n", "authors": ["1041"]}
{"title": "A reference model for simulating agile processes\n", "abstract": " Agile development processes are popular when attempting to respond to changing requirements in a controlled manner; however, selecting an ill-suited process may increase project costs and risk. Before adopting a seemingly promising agile approach, we desire to evaluate the approach's applicability in the context of the specific product, organization, and staff. Simulation provides a means to do this. However, in order to simulate agile processes we require both the ability to model individual behavior as well as the decoupling of the process and product. To our knowledge, no existing simulator nor underlying simulation model provide a means to do this. To address this gap, we introduce a process simulation reference model that provides the constructs and relationships for capturing the interactions among the individuals, product, process, and project in a holistic fashion---a necessary first step towards an agile\u00a0\u2026", "num_citations": "7\n", "authors": ["1041"]}
{"title": "Design considerations for modeling modes in cyber\u2013physical systems\n", "abstract": " Safety critical systems such as cruise control in automotive systems and variable rate bolus in medical device infusion pumps introduce complexity and reduce the flexibility of incremental code modifications. This paper proposes a generic pattern to structure the mode logic such that additions, modifications, and removal of behaviors could be done in a quick and localized fashion without losing model integrity. The authors illustrate the proposed pattern using the infusion pump as a case study and describe a design pattern for the mode logic of reactive systems that allows for flexible, understandable, and maintainable models.", "num_citations": "7\n", "authors": ["1041"]}
{"title": "Finding faults quickly in formal models using random search\n", "abstract": " As software grows more complex, automatic verification tools become increasingly important. Unfortunately many systems are large enough that complete verification requires a lot of time and memory, if it is possible at all. In our preliminary studies, random search, although not a complete technique, was able to find most faults significantly faster and with less memory than would be required for full verification. Here we present an experiment in which random search was used to find faults in fault-seeded models of a large commercial flight guidance system. To assess the performance of random search we compared it to a full verification done by the model checker NuSMV. The random search results were surprisingly complete, finding nearly 90% of the faults reported by NuSMV\u2014and these results were generated faster and using less memory. We suggest that random search be used in conjunction with verification tools, perhaps as a fast debugging tool during model development, or even as an alternative model checking strategy on models for which the time and memory requirements would make full verification impossible.", "num_citations": "7\n", "authors": ["1041"]}
{"title": "On the distribution of property violations in formal models: An initial study\n", "abstract": " Model-checking techniques are successfully used in the verification of both hardware and software systems of industrial relevance. Unfortunately, the capability of current techniques is still limited and the effort required for verification can be prohibitive (if verification is possible at all). As a complement, fast, but incomplete, search tools may provide practical benefits not attainable with full verification tools, for example, reduced need for manual abstraction and fast detection of property violations during model development. In this report we investigate the performance of a simple random search technique. We conducted an experiment on a production-sized formal model of the mode-logic of a flight guidance system. Our results indicate that random search quickly finds the vast majority of property violations in our case-example. In addition, the times to detect various property violations follow an acutely right-skewed\u00a0\u2026", "num_citations": "6\n", "authors": ["1041"]}
{"title": "Model-based testing: Challenges ahead\n", "abstract": " In model-based testing, models derived from the informal requirements (or models developed as part of the requirements process) are used to drive the testing; these models are used to generate the tests as well as serve as oracles, and the testing process can be largely automated. This move towards models, tools, and automation holds enormous promise, but it also raises new challenges that, in our experience, must be addressed before we can reap the full benefits.", "num_citations": "6\n", "authors": ["1041"]}
{"title": "Specification based prototyping of control systems\n", "abstract": " We focus on an approach to simulation and debugging of formal software specifications for control systems called specification-based prototyping. Within the context of specification execution and simulation, specification-based prototyping combines the advantages of traditional formal specifications (e.g., precision and analysis) with the advantages of rapid prototyping (e.g., risk management and early user involvement). The approach lets us refine a formal and executable model of the system requirements specification to a detailed model of the software requirements specification. Throughout this refinement process, the specification is used as an early prototype of the proposed software. By using the specification as the prototype, most of the problems that plague traditional code-based prototyping disappear. First, the formal specification will always be consistent with the behavior of the prototype (excluding real\u00a0\u2026", "num_citations": "6\n", "authors": ["1041"]}
{"title": "Specification and analysis of system level inter-component communication\n", "abstract": " In embedded systems the interfaces between software and its embedding environment are a major source of costly errors. For example, R.R. Lutz (1993) reported that 20%-35% of the safety related errors discovered during integration and system testing of two spacecraft were related to the interfaces between the software and the embedding hardware. Also, the software's operating environment is likely to change over time further complicating the issues related to system level inter component communication. We discuss a formal approach to the specification and analysis of inter component communication using a revised version of the RSML (Requirements State Machine Language) specification language. The formalism allows rigorous specification of the physical aspects of the inter component communication and enables encapsulation of communication related properties in well defined interface specifications\u00a0\u2026", "num_citations": "6\n", "authors": ["1041"]}
{"title": "Toward rigorous object-code coverage criteria\n", "abstract": " Object-branch coverage (OBC) is often used as a measure of the thoroughness of tests suites, augmenting or substituting source-code based structural criteria such as branch coverage and modified condition/decision coverage (MC/DC). In addition, with the increasing use of third-party components for which source-code access may be unavailable, robust object-code coverage criteria are essential to assess how well the components are exercised during testing. While OBC has the advantage of being programming language independent and is amenable to non-intrusive coverage measurement techniques, variations in compilers and the optimizations they perform can substantially change the structure of the generated code and the instructions used to represent branches. To address the need for a robust object coverage criterion, this paper proposes a rigorous definition of OBC such that it captures well the\u00a0\u2026", "num_citations": "5\n", "authors": ["1041"]}
{"title": "Steering model-based oracles to admit real program behaviors\n", "abstract": " The oracle\u2014an arbiter of correctness of the system under test (SUT)\u2014is a major component of the testing process. Specifying oracles is particularly challenging for real-time embedded systems, where small changes in time or sensor inputs may cause large differences in behavior. Behavioral models of such systems, often built for analysis and simulation purposes, are naturally appealing for reuse as oracles. However, these models typically provide an idealized view of the system. Even when given the same inputs, the model\u2019s behavior can frequently be at variance with some acceptable behavior of the SUT executing on a real platform. We therefore propose steering the model when used as an oracle, to admit an expanded set of behaviors when judging the SUT\u2019s adherence to its requirements. On detecting a behavioral difference, the model is backtracked and then searched for a new state that satisfies certain\u00a0\u2026", "num_citations": "5\n", "authors": ["1041"]}
{"title": "Domain reduction abstraction\n", "abstract": " We suggest \"domain reduction abstraction\" for model checking systems with numeric guarding conditions and data transition constraints.  The technique abstracts the domain of data variables using \"data equivalence\" and \"trajectory reduction\" in order to reduce the (possibly infinite) state space.  Earlier work introduced the technique for systems with no data constraints or deterministic data constraints. Here, we extend the work to \"non-deterministic constrained data transition systems\". We provide a formal definition andproof of the soundness of the technique, and illustrate the abstraction technique with a small example.", "num_citations": "5\n", "authors": ["1041"]}
{"title": "On the analysis needs when verifying state-based software requirements: an experience report\n", "abstract": " In a previous investigation we formally defined procedures for analyzing hierarchical state-based requirements specifications for two properties: (1) completeness with respect to a set of criteria related to robustness (a response is specified for every possible input and input sequence) and (2) consistency (the specification is free from conflicting requirements and undesired nondeterminism). Informally, the analysis involves determining if large Boolean expressions are tautologies. We implemented the analysis procedures in a prototype tool and evaluated their effectiveness and efficiency on a large real world requirements specification expressed in an hierarchical state-based language called Requirements State Machine Language. Although our initial approach was largely successful, there were some drawbacks with the original tools. In our initial implementation we abstracted all formulas to propositional logic\u00a0\u2026", "num_citations": "5\n", "authors": ["1041"]}
{"title": "Completeness and Consistency Analysis of State-Based Requirements\n", "abstract": " Completeness and Consistency Analysis of State-Based Requiremen Page 1 Completeness and Consistency Analysis of State-Based Requirements Supervisor: Allen Dutoit Presenter: Ming-Ju Lee Nancy G. Leveson Mats PE Heimdahl 1995 Page 2 2 Outline 1. Introduction 2. Completeness and Consistency Analysis of State-Based Requirements 3. Other analysis approaches 3.1 State-Based Model Checking of Event-Driven System Requirements 3.2 Formal Methods for Verification and Validation of partial Specifications: A Case Study 4. Conclusion & discussion 5. Bibliography Page 3 3 Introduction \u25aa Why requirements analysis? \u2022 Expensive and difficult to correct requirements errors later \u27a2 apply formal methods during requirements phase to reduce requirements errors \u25aa Formal methods \u2022 Mathematics and modeling applicable to the specification, design and verification of software \u2022 emphasize on the creation of \u2026", "num_citations": "5\n", "authors": ["1041"]}
{"title": "Analysis and testing of plexil plans\n", "abstract": " Autonomy is increasingly important in missions to remote locations (eg, space applications and deep sea exploration) since limited bandwidth and communication delays make detailed instructions from a remote base (eg, Earth or a land base) impractical. The planning systems used for autonomous operation are difficult to verify and validate because they must create plans for use in a specific environment and the correct behavior might not be easy to define.", "num_citations": "4\n", "authors": ["1041"]}
{"title": "A case for specification validation\n", "abstract": " As we are moving from a traditional software development process to a new development paradigm where the process it largely driven by tools and automation, new challenges for verification and validation (V&V) emerge. Productivity improvements will in this new paradigm be achieved through reduced emphasis on testing of implementations, increased reliance on automated analysis tools applied in the specification domain, verifiability correct generation of source-code, and verifiably correct compilation. The V&V effort will now be largely focused on assuring that the formal specifications are correct and that the tools are trustworthy so we can rely on the results of the analysis and code generation without extensive additional testing of the resulting implementation. Most effort has traditionally been devoted to the verification problem. In this position paper we point out the importance of validation and argue\u00a0\u2026", "num_citations": "4\n", "authors": ["1041"]}
{"title": "Requirements reference models revisited: Accommodating hierarchy in system design\n", "abstract": " Reference models such as Parnas' four-variable model, Jackson's and Zaves' world machine model, and Gunther et al.'s WRSPM model abstractly define and relate key artifacts in requirements engineering. Such reference models are intended to serve as a frame of reference for engineers to understand and reason about the artifacts involved in requirements engineering. However, when discussing the requirements of modern systems that are developed in a hierarchical and middle-out manner, these reference models do not provide a framework in which the relationship between requirements and architecture is explicitly discussed. Conceptual clarity about this relationship is crucial since the architecture and requirements for such systems become intrinsically intertwined as the architectural choices made during development influence the requirements and vice-versa. Hence, to precisely determine the scope of\u00a0\u2026", "num_citations": "3\n", "authors": ["1041"]}
{"title": "Inductive validity cores\n", "abstract": " Symbolic model checkers can construct proofs of properties over highly complex models. However, the results reported by the tool when a proof succeeds do not generally provide much insight to the user. It is often useful for users to have traceability information related to the proof: which portions of the model were necessary to construct it. This traceability information can be used to diagnose a variety of modeling problems such as overconstrained axioms and underconstrained properties, measure completeness of a set of requirements over a model, and assist with design optimization given a set of requirements for an existing or synthesized implementation. In this paper, we present a comprehensive treatment of a suite of algorithms to compute inductive validity cores (IVCs), minimal sets of model elements necessary to construct inductive proofs of safety properties for sequential systems. The algorithms are\u00a0\u2026", "num_citations": "3\n", "authors": ["1041"]}
{"title": "Specifying and analysing system-level inter-component interfaces\n", "abstract": " In control systems, the interfaces between software and its embedding environment are a major source of costly errors. For example, Lutz reported that 20\u201335% of the safety-related errors discovered during integration and system testing of two spacecraft were related to the interfaces between the software and the embedding hardware. Also, the software\u2019s operating environment is likely to change over time, further complicating the issues related to system-level inter-component communication. In this paper we discuss a formal approach to the specification and analysis of inter-component communication using a revised version of RSML (Requirements State Machine Language). The formalism allows rigorous specification of the physical aspects of the inter-component communication and forces encapsulation of communication-related properties in well-defined and easy-to-read interface specifications. This enables\u00a0\u2026", "num_citations": "3\n", "authors": ["1041"]}
{"title": "A general framework for interconnecting annotations of software systems\n", "abstract": " Computer-supported annotation of software systems and their documentation, including design documentation and source code, is a common and important software engineering activity. Annotated documentation is used in both formal software inspection and informal software maintenance. Viewers of annotated systems may understand the software more easily if annotations are visible not just from the annotated item itself but from other, related items. We propose a general framework for interconnecting annotatable items in software systems to achieve this visibility. We describe filtering and broadening rules that viewers can use to select the annotations they desire to see. We illustrate this framework in the context of object-oriented software system development.", "num_citations": "3\n", "authors": ["1041"]}
{"title": "Community-assisted software engineering decision making\n", "abstract": " There are a class of problems - such as deciding on a series of development practices - that challenge AI approaches for two reasons: (1) well-defined data is necessary, but it is not clear which factors are important and (2) multiple recommendations must be made, and dependence must be considered. Recommender systems offer inspiration for addressing these problems. First, they can make use of data models that broadly pair a series of recommendations with generalized information like project descriptions and design documents. More importantly, they offer feedback mechanisms to refine the calculated recommendations. Detailed feedback could be factored back into the data model to, over time, build evidence and context for recommendations.   Existing algorithms and data models would be amenable to the addition of feedback mechanisms, and the use of these dynamic models could lower start-up costs and generate more accurate results as the model grows. We believe that feedback-driven dynamic prediction models will become an exciting research topic in the field of AI-based software engineering.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Efficient test coverage measurement for mc/dc\n", "abstract": " Numerous activities require low-overhead monitoring of software applications, for example, run-time verification, test coverage measurement, and data collection. To support monitoring, current approaches usually involve extensive instrumentation of the software to be monitored, causing significant performance penalties and also requiring some means to ensure that the monitoring code will not cause incorrect behavior in the monitored application. To tackle this problem, we have explored a hardware-supported framework for monitoring and observation of software-intensive systems. In our approach, we leverage multi-core processor architectures to create a non-intrusive, predictable, fine-grained, and highly extensible general purpose monitoring framework. We have developed a novel architecture that augments each core with programmable extraction logic to observe an application executing on the core as its program state changes. Based on this architecture, we present a novel and highly efficient algorithm for tracking MC/DC coverage.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Hardware supported flexible monitoring: Early results\n", "abstract": " Monitoring of software\u2019s execution is crucial in numerous software development tasks. Current monitoring efforts generally require extensive instrumentation of the software or dedicated hardware test rig designed to provide visibility into the software. To fully understand software\u2019s behavior, the production software must be studied in its production environment. To address this fundamental software engineering challenges, we propose a compiler and hardware supported framework for monitoring and observation of software-intensive systems.             We place three fundamental requirements on our monitoring framework. The monitoring must be non-intrusive, low-overhead, and predictable so that the software is not unduly disturbed. The framework must also allow low-level monitoring and be highly flexible so we can accommodate a broad range of crucial monitoring activities.             The general idea behind\u00a0\u2026", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Testing strategies for model-based development\n", "abstract": " Model-based software development offers new opportunities and challenges for validation and verification of safety-critical software. Since models have well-defined syntax and semantics, it is possible to test models as well as source code and to define structural coverage metrics over models. Further, given a formal description of requirements, it is possible to use automated tools to check whether models satisfy requirements and to describe objective notions of requirements coverage. Recently, model-based testing tools have emerged that allow auto-generation of test-cases given a model and structural coverage metrics. Nevertheless, there is a great deal of uncertainty as to which tools and techniques are effective and how to structure a testing process using model-based development.In this report, we describe some of the issues in model-based testing, present an approach for testing artifacts generated by a model-based development process, and describe the relationship between this process and existing standards such as DO-178B. This approach divides the traditional testing process into two parts: requirements-based testing (validation testing) which determines whether the model implements the high-level requirements and model-based testing (conformance testing) which determines whether the code generated from a model is behaviorally equivalent to the model. The goals of the two processes differ significantly and this report explores suitable testing metrics and automation strategies for each. To support requirements-based testing, we define novel objective requirements coverage metrics similar to existing specification and\u00a0\u2026", "num_citations": "2\n", "authors": ["1041"]}
{"title": "FGS Partitioning Final Report\n", "abstract": " Partitioning a system consists of dividing it into components that can be physically isolated from each other while preserving the essential behavior of the system. In this report, we describe a methodology for developing and reasoning about such systems. This approach allows a developer to start from an ideal system specification and refine it along two axes. Along one axis, the system can be refined one component at a time toward an implementation. Along the other axis, the behavior of the system can be relaxed to produce a more cost effective but still acceptable solution. We illustrate this process by applying it to the synchronization logic of a Dual Fight Guidance System, evolving the system from an ideal case in which the components do not fail and communicate synchronously to one in which the components can fail and communicate asynchronously. For each step, we show how the system requirements have to change if the system is be implemented and prove that each implementation meets the revised system requirements through model-checking.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Early Validation of Requirements\n", "abstract": " This paper describes a case study conducted to determine if formal methods could be used to validate system requirements early in the lifecycle atreasonable cost. Several hundred requirements for the mode logic of a typical Flight Guidance System were captured as natural language \u201cshall\u201d statements. A formal model of the mode logic was written in the RSML                                    -e                  language and translated into the NuSMV model checker and the PVS theorem prover using translators developed as part of the project. Each \u201cshall\u201d statement was manually translated into a NuSMV or PVS property and proven using these tools.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "On the advantages of approximate vs. complete verification: Bigger models, faster, less memory, usually accurate\n", "abstract": " As software grows increasingly complex, verification becomes more and more challenging. Automatic verification by model checking has been effective in many domains including computer hardware design, networking, security and telecommunications protocols, automated control systems and others [2, 4, 6]. Many realworld software models, however, are too large for the available tools. The difficulty\u2014how to verify large systems\u2014is fundamentally a search issue: the global state space representing all possible behaviors of a complex software system is exponential in size. This state space explosion problem has yet to be solved, even after many decades of work [4].We have been exploring LURCH, an approximate (not necessarily complete) alternative to traditional model checking based on a randomized search algorithm. Randomized algorithms like LURCH have been known to outperform their deterministic counterparts for search problems representing a wide range of applications [7].", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Ideas on How Product-Line Engineering Can be Extended\n", "abstract": " Product-line engineering can result in cost savings and increases in productivity.  In addition, in safety-critical systems, the approach has the potential for reuse of analysis and testing results which can lead to a safer system.  Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family.  In this paper, we present a position on n-dimensional and hierarchical product families, which we have recently introduced. This paper focuses on our initial thoughts on how making n-dimensional and hierarchical families has the potential to affect the product-line development process as well as how using this approach might enable more organizations to use product-line approaches.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Representing the unknown in specification languages\n", "abstract": " 1. INTRODUCTION \u043c \u0439\u0438 \u0430 \u0437\u0434 \u040c \u0438 \u0433\u0432 \u0430 \u0432 \u0439 \u0437\u0418 \u0437\u0439 \u0437 \u042b\u0438 \u0438 \u0436\u0438\u0437\u2104 \u0418 \u042b \u042a \u041d\u041d\u0418 \u041d\u041c\u2104 \u0418 \u042a\u042b\u0425\u0424 \u041d\u2104 \u0418 \u0432 \u042b\u0434 \u042c\u042a\u0425\u0419\u042a\u0424 \u041d\u2104 \u0418 \u0436 \u0433\u0431\u0419 \u0432 \u0431\u0433\u0436 \u043b \u0430\u043d \u0439\u0437 \u0432 \u0438 \u0437\u0434 \u040c \u0438 \u0433\u0432 \u0433 \u0437 \u0438\u043d\u0419 \u0436 \u0438 \u0430 \u0403\u042c \u0437 \u043b\u0433\u0436 \u0437 \u0432 \u0434 \u0436\u0438 \u0430\u0430\u043d \u0437\u0439\u0434\u0434\u0433\u0436\u0438 \u043d \u0426\u042b \u0436 \u0432\u0438\u0437", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Enhancing annotation visibility for software inspection\n", "abstract": " Annotation of software artifacts is common in software development, and vital for software inspection. People viewing annotated artifacts encounter delocalization: they must understand various parts of an artifact (and their annotations) to understand the part they are viewing. We taxonomize delocalization within software systems into lateral delocalization (different items of the artifact within the same development phase), longitudinal delocalization (related items in different phases), and historical delocalization (successive versions of the same item). We report on a pilot study of code inspection with AnnoSpec, an inspection tool supporting visibility of laterally-delocalized annotations. Our results suggest that addressing delocalization may help people perform inspections more effectively.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "A CAD environment for safety-critical software\n", "abstract": " The goal of the University of Washington, University of Minnesota, and Safeware Engineering Corporation Safety-Critical Systems Projects is to develop a theoretical foundation for software safety and to build a methodology upon that foundation. This paper describes the methodology and a set of safety analysis techniques (and prototype tools) to support it. The prototype tools are being developed in order to evaluate the techniques. To ensure that the procedures scale up to realistic systems, the tools and techniques are being evaluated on real systems, including TCAS II (Traffic Alert and Collision Avoidance System), an airborne collision avoidance system required on most aircraft that fly in U.S. airspace, a NASA experimental flight management system, a NASA robot used to service tiles on the Space Shuttle, and proposed upgrades to the U.S. Air Traffic Control System.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Condition based coverage criteria: To use or not to use, that is the question\n", "abstract": " When using tools to automatically generate test-suites from a specification, the selection of coverage criterion that guides the generation process is of imperative importance. In a previous study, we observed that, although a coverage criterion may seem reasonable for measuring the adequacy of a test suite, it may be unsuitable for guiding the generation of test suites; the generated test-suite may exercise only a small portion of the system under study, and thus, finds few faults. This paper presents an approach that addresses this problem. Our approach enhances existing condition coverage criteria with additional requirements that force the test generator to produce test-suites that will exercise all portions of the system. The paper also presents an empirical study that evaluates quality of the test-suite generated with the enhanced coverage criteria. The study shows that the introduction of these additional requirements can improve the fault finding capability of the generated test-suites without significant impact on the cost for generating such test-suites.", "num_citations": "2\n", "authors": ["1041"]}
{"title": "Discovering instructions for robust binary-level coverage criteria\n", "abstract": " Object-Branch Coverage (OBC) is often used to measure effectiveness of test suites, when source code is unavailable. The traditional OBC definition can be made more resilient to variations in compilers and the structure of generated code by creating more robust definitions. However finding which instructions should be included in each new definition is laborious, error-prone, and architecture-dependent. We automate the discovery of instructions to be included for an improved OBC definition on the X86 and ARM architectures. We discover all possible valid instructions by symbolically executing instruction decoders for X86 and ARM instructions. For each discovered instruction, we translate it to Vine IR, and check if the Vine IR translation satisfies the OBC definition. We verify the correctness of our tool by comparing its output with the X86 and ARM architecture manuals. Our automated instruction classification\u00a0\u2026", "num_citations": "1\n", "authors": ["1041"]}
{"title": "ReqsCov: A tool for measuring test-adequacy over requirements\n", "abstract": " When creating test cases for software, a common approach is to create tests that exercise requirements. Determining the adequacy of test cases, however, is generally done through inspection or indirectly by measuring structural coverage of an executable artifact (such as source code or a software model). We present ReqsCov, a tool to directly measure requirements coverage provided by test cases. ReqsCov allows users to measure Linear Temporal Logic requirements coverage using three increasingly rigorous requirements coverage metrics: naive coverage, antecedent coverage, and Unique First Cause coverage. By measuring requirements coverage, users are given insight into the quality of test suites beyond what is available when solely using structural coverage metrics over an implementation.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "On MC/DC and implementation structure: An empirical study\n", "abstract": " In civil avionics, obtaining D0-178B certification for highly critical airborne software requires that the adequacy of the code testing effort be measured using a structural coverage criterion known as Modified Condition and Decision Coverage  (MC/DC). We hypothesized that the effectiveness of the MC/DC metric is highly sensitive to the structure of the implementation and can therefore be problematic as a test adequacy criterion. We tested this hypothesis by evaluating the faultfinding ability of MC/DC-adequate test suites on five industrial systems (flight guidance and display management). For each system, we created two versions of the implementations\u00e2\u20ac\u201dimplementations with and without expression folding (i.e., inlining).  We found that for all five examples, the effectiveness of the test suites was highly sensitive to the structure of the implementation they were designed to cover. MC/DC test suites adequate on an inlined implementation have greater fault finding ability than test suites generated to be MC/DC adequate on the non-inlined version of the same implementation at the 5% significance level. (The inlined test suites outperformed the non-inlined test suites in the range of 10% to 5940%.)  This observation confirms our suspicion that MC/DC used as a test adequacy metric is highly sensitive to structural changes in the implementation, and that test suite adequacy measurement using the MC/DC metric will be better served if done over the inlined implementation.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "Model-based safety analysis final report\n", "abstract": " System safety analysis techniques are well established and are used extensively during the design of safety-critical systems. Despite this, most of the techniques are highly subjective and dependent on the skill of the practitioner. Since these analyses are usually based on an informal system model, it is unlikely that they will be complete, consistent, and error free. In fact, the lack of precise models of the system architecture and its failure modes often forces the safety analysts to devote much of their effort to gathering architectural details about the system behavior from several sources and embedding this information in the safety artifacts such as the fault trees.This report describes Model-Based Safety Analysis, an approach in which the system and safety engineers share a common system model created using a model-based development process. By extending the system model with a fault model as well as relevant portions of the physical system to be controlled, automated support can be provided for much of the safety analysis. We believe that by using a common model for both system and safety engineering and automating parts of the safety analysis, we can both reduce the cost and improve the quality of the safety analysis. Here we present our vision of model-based safety analysis and discuss the advantages and challenges in making this approach practical.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "Requirements-Based Testing in a Model-Based World\n", "abstract": " Model-based software development offers new opportunities and challenges for validation and verification of safety-critical software.  In this report, we describe an approach for validating the artifacts generated in a model-based development process. Our approach divides the traditional testing process into two parts: one that validates the formal model implements the high-level requirements and another that determines whether the code generated from the model is behaviorally equivalent. The focus in this report is on validation testing; in particular, we present a framework that enables objective measures of requirements coverage and provides the ability to achieve a high degree of automation.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "Combination model checking: approach and a case study\n", "abstract": " We present combination model checking approach using a SAT-based bounded model checker together with a BDD-based symbolic model checker to provide a more efficient counter example generation process. We provide this capability without compromising the verification capability of the symbolic model checker. The basic idea is to use the symbolic model checker to determine whether or not a property holds in the model. If the property holds, we are done. If it does not, we preempt the counterexample generation and use the SAT-based model checker for this purpose. An application of the combination approach to a version of a Flight Guidance System (FGS) from Rockwell Collins, Inc. shows huge performance gain when checking a collection of several hundred properties.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "A Case for Requirements Validation\n", "abstract": " In software engineering we make a distinction between the validation and the verifi- cation of a software system under development. Verification is concerned with demonstrating that the software implements the functional and non-functional requirements. Verification answers the question \"is this implementation correct with respect to its requirements?\" Validation, on the other hand, is concerned with determining if the functional and non-functional requirements are the right requirements. Validation answers the question \"will this system, if build correctly, be safe and effective for its intended use?\" There is ample evidence that most safety problems can be traced to erroneous and inadequate requirements. Therefore, to improve the safety of software intensive systems it is critical that the requirements are properly validated. Unfortunately, current certication standards, for example, DO-178B, focus almost exclusively on various verication activities; consequently, most industry practices are geared towards verification activities such as extensive testing and code inspections. Thus, one of the most critical problems with current certification standards is a lack of robust and reliable ways of assessing whether the requirements have been adequately validated.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "Tool Intensive Software Development: New Challenges for Verification, Validation, and Certification\n", "abstract": " As we are moving from a traditional software development process to a new development paradigm where the process it largely driven by tools and automation, new challenges for verification and validation (V&V) emerge. Productivity improvements will in this new paradigm be achieved through reduced emphasis on unit testing of code, increased reliance on automated analysis tools applied in the specification domain, and trustworthy code generation. The V&V effort will now be largely focused on assuring that the emph{formal specifications are correct} and that the emph{tools are trustworthy} so we can rely on the results of the analysis and code generation without extensive additional testing of the resulting implementation.  Note here that, in our opinion, the possibility of reducing or fully automating the costly unit-testing efforts are key to the success of this new development paradigm. We have found little support for this type of development if modeling and analysis are to be performed in emph{addition} to what is currently done---these new techniques must either make current efforts more efficient or replace some currently required V&V activity. In either case, our increased reliance on tools requires that they can be trusted---this poses new challenges for V&V and certification.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "Verifying communication related safety constraints in RSML specifications\n", "abstract": " Languages based on hierarchical finite state machines, such as, Statecharts, SCR (Software Cost Reduction), and the Requirements State Machine Language (RSML), are suitable for specification of software for embedded systems. The languages are relatively easy to use, allow automated verification of properties such as completeness and consistency, and support execution and dynamic evaluation of the specifications. However, the support to rigorously specify and analyze the communication between physically distinct components in a system is currently not well-supported in any of the approaches. We know that the interfaces between the software and the embedding environment are a major source of costly errors. For example, Lutz reported that 20%-35% of the safety related errors discovered during integration and system testing of two spacecraft were related to the interfaces between the software and the embedding hardware.In this paper we introduce a formal approach to the specification of system level inter-component communication and show how this formalism can be used to prove safety constraints and a limited notion of liveness constraints. The interface definitions and the constraints are translated to PVS (Prototype Verification System) proof obligations, and the proofs of compliance with the constraints are performed in the PVS domain. To demonstrate the feasibility of the approach we have implemented a prototype tool and used the tool to prove some desirable properties of the inter-component communication of an avionics system.", "num_citations": "1\n", "authors": ["1041"]}
{"title": "Verifying communication constraints in RSML specifications\n", "abstract": " Discusses a formal approach to the specification of inter-component communication in RSML (Requirements State Machine Language) specifications. The approach is based on communicating finite state machines. The formalism allows the encapsulation of communication-related properties in well-defined interface specifications. The encapsulation enables us to use the interface specifications as simple safety kernels and to enforce certain safety and liveness constraints in these kernels, Furthermore, we describe how safety and liveness constraints related to inter-component communication can be formalized using a simple and easy-to-understand constraint language. To formally verify that the constraints are satisfied in an RSML model, we attempt to prove that the constraints are satisfied by only looking at the interface specifications. We illustrate the approach with an example from the TCAS II (Traffic Collision\u00a0\u2026", "num_citations": "1\n", "authors": ["1041"]}