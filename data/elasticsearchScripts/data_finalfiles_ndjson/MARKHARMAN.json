{"title": "Regression testing minimization, selection and prioritization: a survey\n", "abstract": " Regression testing is a testing activity that is performed to provide confidence that changes do not harm the existing behaviour of the software. Test suites tend to grow in size as software evolves, often making it too costly to execute entire test suites. A number of different approaches have been studied to maximize the value of the accrued test suite: minimization, selection and prioritization. Test suite minimization seeks to eliminate redundant test cases in order to reduce the number of tests to run. Test case selection seeks to identify the test cases that are relevant to some set of recent changes. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. This paper surveys each area of minimization, selection and prioritization technique and discusses open problems and potential directions for future research. Copyright \u252c\u2310 2010 John Wiley & Sons, Ltd.", "num_citations": "1494\n", "authors": ["23"]}
{"title": "Search-based software engineering\n", "abstract": " This paper claims that a new field of software engineering research and practice is emerging: search-based software engineering. The paper argues that software engineering is ideal for the application of metaheuristic search techniques, such as genetic algorithms, simulated annealing and tabu search. Such search-based techniques could provide solutions to the difficult problems of balancing competing (and some times inconsistent) constraints and may suggest ways of finding acceptable solutions in situations where perfect solutions are either theoretically impossible or practically infeasible.In order to develop the field of search-based software engineering, a reformulation of classic software engineering problems as search problems is required. The paper briefly sets out key ingredients for successful reformulation and evaluation criteria for search-based software engineering.", "num_citations": "1012\n", "authors": ["23"]}
{"title": "Search algorithms for regression test case prioritization\n", "abstract": " Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "865\n", "authors": ["23"]}
{"title": "The current state and future of search based software engineering\n", "abstract": " This paper describes work on the application of optimization techniques in software engineering. These optimization techniques come from the operations research and metaheuristic computation research communities. The paper briefly reviews widely used optimization techniques and the key ingredients required for their successful application to software engineering, providing an overview of existing results in eight software engineering application domains. The paper also describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of open problems, challenges and areas for future work.", "num_citations": "811\n", "authors": ["23"]}
{"title": "An orchestrated survey of methodologies for automated software test case generation\n", "abstract": " Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "721\n", "authors": ["23"]}
{"title": "The oracle problem in software testing: A survey\n", "abstract": " Testing involves examining the behaviour of a system in order to discover potential faults. Given an input for a system, the challenge of distinguishing the corresponding desired, correct behaviour from potentially incorrect behavior is called the \u0393\u00c7\u00a3test oracle problem\u0393\u00c7\u00a5. Test oracle automation is important to remove a current bottleneck that inhibits greater overall test automation. Without test oracle automation, the human has to determine whether observed behaviour is correct. The literature on test oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of test oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of test oracles, even the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "703\n", "authors": ["23"]}
{"title": "A theoretical and empirical study of search-based testing: Local, global, and hybrid search\n", "abstract": " Search-based optimization techniques have been applied to structural software test data generation since 1992, with a recent upsurge in interest and activity within this area. However, despite the large number of recent studies on the applicability of different search-based optimization approaches, there has been very little theoretical analysis of the types of testing problem for which these techniques are well suited. There are also few empirical studies that present results for larger programs. This paper presents a theoretical exploration of the most widely studied approach, the global search technique embodied by Genetic Algorithms. It also presents results from a large empirical study that compares the behavior of both global and local search-based optimization on real-world programs. The results of this study reveal that cases exist of test data generation problem that suit each algorithm, thereby suggesting that a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "443\n", "authors": ["23"]}
{"title": "Reformulating software engineering as a search problem\n", "abstract": " Metaheuristic techniques such as genetic algorithms, simulated annealing and tabu search have found wide application in most areas of engineering. These techniques have also been applied in business, financial and economic modelling. Metaheuristics have been applied to three areas of software engineering: test data generation, module clustering and cost/effort prediction, yet there remain many software engineering problems which have yet to be tackled using metaheuristics. It is surprising that metaheuristics have not been more widely applied to software engineering; many problems in software engineering are characterised by precisely the features which make metaheuristics search applicable. In the paper it is argued that the features which make metaheuristics applicable for engineering and business applications outside software engineering also suggest that there is great potential for the exploitation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "414\n", "authors": ["23"]}
{"title": "A survey of the use of crowdsourcing in software engineering\n", "abstract": " The term \u0393\u00c7\u00ffcrowdsourcing\u0393\u00c7\u00d6 was initially introduced in 2006 to describe an emerging distributed problem-solving model by online workers. Since then it has been widely studied and practiced to support software engineering. In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering, seeking to cover all literature on this topic. We first review the definitions of crowdsourcing and derive our definition of Crowdsourcing Software Engineering together with its taxonomy. Then we summarise industrial crowdsourcing practice in software engineering and corresponding case studies. We further analyse the software engineering domains, tasks and applications for crowdsourcing and the platforms and stakeholders involved in realising Crowdsourced Software Engineering solutions. We conclude by exposing trends, open issues and opportunities for future research on Crowdsourced\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "381\n", "authors": ["23"]}
{"title": "Sapienz: Multi-objective automated testing for android applications\n", "abstract": " We introduce Sapienz, an approach to Android testing that uses multi-objective search-based testing to automatically explore and optimise test sequences, minimising length, while simultaneously maximising coverage and fault revelation. Sapienz combines random fuzzing, systematic and search-based exploration, exploiting seeding and multi-level instrumentation. Sapienz significantly outperforms (with large effect size) both the state-of-the-art technique Dynodroid and the widely-used tool, Android Monkey, in 7/10 experiments for coverage, 7/10 for fault detection and 10/10 for fault-revealing sequence length. When applied to the top 1,000 Google Play apps, Sapienz found 558 unique, previously unknown crashes. So far we have managed to make contact with the developers of 27 crashing apps. Of these, 14 have confirmed that the crashes are caused by real faults. Of those 14, six already have developer\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "376\n", "authors": ["23"]}
{"title": "App store mining and analysis: MSR for app stores\n", "abstract": " This paper introduces app store mining and analysis as a form of software repository mining. Unlike other software repositories traditionally used in MSR work, app stores usually do not provide source code. However, they do provide a wealth of other information in the form of pricing and customer reviews. Therefore, we use data mining to extract feature information, which we then combine with more readily available information to analyse apps' technical, customer and business aspects. We applied our approach to the 32,108 non-zero priced apps available in the Blackberry app store in September 2011. Our results show that there is a strong correlation between customer rating and the rank of app downloads, though perhaps surprisingly, there is no correlation between price and downloads, nor between price and rating. More importantly, we show that these correlation findings carry over to (and are even\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "375\n", "authors": ["23"]}
{"title": "Software module clustering as a multi-objective search problem\n", "abstract": " Software module clustering is the problem of automatically organizing software units into modules to improve program structure. There has been a great deal of recent interest in search-based formulations of this problem in which module boundaries are identified by automated search, guided by a fitness function that captures the twin objectives of high cohesion and low coupling in a single-objective fitness function. This paper introduces two novel multi-objective formulations of the software module clustering problem, in which several different objectives (including cohesion and coupling) are represented separately. In order to evaluate the effectiveness of the multi-objective approach, a set of experiments was performed on 17 real-world module clustering problems. The results of this empirical study provide strong evidence to support the claim that the multi-objective approach produces significantly better solutions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "366\n", "authors": ["23"]}
{"title": "Search Based Software Engineering: Techniques, Taxonomy, Tutorial\n", "abstract": " The aim of Search Based Software Engineering (SBSE) research is to move software engineering problems from human-based search to machine-based search, using a variety of techniques from the metaheuristic search, operations research and evolutionary computation paradigms. The idea is to exploit humans\u0393\u00c7\u00d6 creativity and machines\u0393\u00c7\u00d6 tenacity and reliability, rather than requiring humans to perform the more tedious, error prone and thereby costly aspects of the engineering process. SBSE can also provide insights and decision support. This tutorial will present the reader with a step-by-step guide to the application of SBSE techniques to Software Engineering. It assumes neither previous knowledge nor experience with Search Based Optimisation. The intention is that the tutorial will cover sufficient material to allow the reader to become productive in successfully applying search based optimisation to a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "301\n", "authors": ["23"]}
{"title": "Using program slicing to assist in the detection of equivalent mutants\n", "abstract": " While mutation testing has proved to be an effective way of finding software faults, currently it is only applied to relatively small programs. One of the main reasons for this is the human analysis required in detecting equivalent mutants. Here program slicing is used to simplify this problem. Progam slicing is also used to reduce the number of equivalent mutants produced. Copyright \u252c\u2310 1999 John Wiley & Sons, Ltd.", "num_citations": "284\n", "authors": ["23"]}
{"title": "Pareto optimal search based refactoring at the design level\n", "abstract": " Refactoring aims to improve the quality of a software systems' structure, which tends to degrade as the system evolves. While manually determining useful refactorings can be challenging, search based techniques can automatically discover useful refactorings. Current search based refactoring approaches require metrics to be combined in a complex fashion, and producea single sequence of refactorings. In this paper we show how Pareto optimality can improve search based refactoring, making the combination of metrics easier, and aiding the presentation of multiple sequences of optimal refactorings to users.", "num_citations": "274\n", "authors": ["23"]}
{"title": "Mutation testing advances: an analysis and survey\n", "abstract": " Mutation testing realizes the idea of using artificial defects to support testing activities. Mutation is typically used as a way to evaluate the adequacy of test suites, to guide the generation of test cases, and to support experimentation. Mutation has reached a maturity phase and gradually gains popularity both in academia and in industry. This chapter presents a survey of recent advances, over the past decade, related to the fundamental problems of mutation testing and sets out the challenges and open problems for the future development of the method. It also collects advices on best practices related to the use of mutation in empirical studies of software testing. Thus, giving the reader a \u0393\u00c7\u00a3mini-handbook\u0393\u00c7\u00a5-style roadmap for the application of mutation testing as experimental methodology.", "num_citations": "227\n", "authors": ["23"]}
{"title": "Clustering test cases to achieve effective and scalable prioritisation incorporating expert knowledge\n", "abstract": " Pair-wise comparison has been successfully utilised in order to prioritise test cases by exploiting the rich, valuable and unique knowledge of the tester. However, the prohibitively large cost of the pair-wise comparison method prevents it from being applied to large test suites. In this paper, we introduce a cluster-based test case prioritisation technique. By clustering test cases, based on their dynamic runtime behaviour, we can reduce the required number of pair-wise comparisons significantly. The approach is evaluated on seven test suites ranging in size from 154 to 1,061 test cases. We present an empirical study that shows that the resulting prioritisation is more effective than existing coverage-based prioritisation techniques in terms of rate of fault detection. Perhaps surprisingly, the paper also demonstrates that clustering (even without human input) can outperform unclustered coverage-based technologies, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "225\n", "authors": ["23"]}
{"title": "An overview of program slicing\n", "abstract": " MARK HARMAN and ROBERT HIERONS review three semantic paradigms for slicing \u0393\u00c7\u00f6 static, dynamic and conditioned; and two syntactic paradigms \u0393\u00c7\u00f6 syntax\u0393\u00c7\u00c9preserving and amorphous. Slicing has been applied to many software development problems including testing, reuse, maintenance and evolution. This paper describes the main forms of program slice and some of the applications to which slicing has been put. Copyright \u252c\u2310 2001 John Wiley & Sons, Ltd.", "num_citations": "222\n", "authors": ["23"]}
{"title": "A multiple hill climbing approach to software module clustering\n", "abstract": " Automated software module clustering is important for maintenance of legacy systems written in a 'monolithic format' with inadequate module boundaries. Even where systems were originally designed with suitable module boundaries, structure tends to degrade as the system evolves, making re-modularization worthwhile. This paper focuses upon search-based approaches to the automated module clustering problem, where hitherto, the local search approach of hill climbing has been found to be most successful. In the paper we show that results from a set of multiple hill climbs can be combined to locate good 'building blocks' for subsequent searches. Building blocks are formed by identifying the common features in a selection of best hill climbs. This process reduces the search space, while simultaneously 'hard wiring' parts of the solution. The paper reports the results of an empirical study that show that the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "197\n", "authors": ["23"]}
{"title": "Amorphous program slicing\n", "abstract": " This paper introduces amorphous program slicing. Like traditional slicing, amorphous slicing simplifies a program while preserving a projection of its semantics. Unlike traditional slicing, amorphous slicing may make use of any simplifying transformation which preserves this semantic projection, thereby improving upon the simplification power of traditional slicing and consequently its applicability to program comprehension. The paper also introduces a theoretical framework of program projection. A projection is defined with respect to an equivalence relation on programs together with a simplicity measure (an ordering on programs). Having defined this framework, amorphous and traditional forms of static and conditioned slice are defined by instantiating the definition of a projection with different equivalence and ordering relations. The projection framework helps to contain the potential explosion in slicing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "197\n", "authors": ["23"]}
{"title": "A multi-objective approach to search-based test data generation\n", "abstract": " There has been a considerable body of work on search-based test data generation for branch coverage. However, hitherto, there has been no work on multi-objective branch coverage. In many scenarios a single-objective formulation is unrealistic; testers will want to find test sets that meet several objectives simultaneously in order to maximize the value obtained from the inherently expensive process of running the test cases and examining the output they produce. This paper introduces multi-objective branch coverage. The paper presents results from a case study of the twin objectives of branch coverage and dynamic memory consumption for both real and synthetic programs. Several multi-objective evolutionary algorithms are applied. The results show that multi-objective evolutionary algorithms are suitable for this problem, and illustrates the way in which a Pareto optimal search can yield insights into the trade\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "186\n", "authors": ["23"]}
{"title": "Search based approaches to component selection and prioritization for the next release problem\n", "abstract": " This paper addresses the problem of determining the next set of releases in the course of software evolution. It formulates both ranking and selection of candidate software components as a series of feature subset selection problems to which search based software engineering can be applied. The approach is automated using greedy and simulated annealing algorithms and evaluated using a set of software components from the component base of a large telecommunications organization. The results are compared to those obtained by a panel of (human) experts. The results show that the two automated approaches convincingly outperform the expert judgment approach", "num_citations": "182\n", "authors": ["23"]}
{"title": "Metrics are fitness functions too\n", "abstract": " Metrics, whether collected statically or dynamically, and whether constructed from source code, systems or processes, are largely regarded as a means of evaluating some property of interest. This viewpoint has been very successful in developing a body of knowledge, theory and experience in the application of metrics to estimation, predication, assessment, diagnosis, analysis and improvement. This paper shows that there is an alternative, complementary, view of a metric: as a fitness function, used to guide a search for optimal or near optimal individuals in a search space of possible solutions. This 'Metrics as Fitness Functions' (MAFF) approach offers a number of additional benefits to metrics research and practice because it allows metrics to be used to improve software as well as to assess it and because it provides an additional mechanism of metric analysis and validation. This paper presents a brief survey of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "179\n", "authors": ["23"]}
{"title": "The plastic surgery hypothesis\n", "abstract": " Recent work on genetic-programming-based approaches to automatic program patching have relied on the insight that the content of new code can often be assembled out of fragments of code that already exist in the code base. This insight has been dubbed the plastic surgery hypothesis; successful, well-known automatic repair tools such as GenProg rest on this hypothesis, but it has never been validated. We formalize and validate the plastic surgery hypothesis and empirically measure the extent to which raw material for changes actually already exists in projects. In this paper, we mount a large-scale study of several large Java projects, and examine a history of 15,723 commits to determine the extent to which these commits are graftable, ie, can be reconstituted from existing code, and find an encouraging degree of graftability, surprisingly independent of commit size and type of commit. For example, we find\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "177\n", "authors": ["23"]}
{"title": "Combining multi-objective search and constraint solving for configuring large software product lines\n", "abstract": " Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p <; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\u255a\u00e9 12  = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "176\n", "authors": ["23"]}
{"title": "Trivial compiler equivalence: A large scale empirical study of a simple, fast and effective equivalent mutant detection technique\n", "abstract": " Identifying equivalent mutants remains the largest impediment to the widespread uptake of mutation testing. Despite being researched for more than three decades, the problem remains. We propose Trivial Compiler Equivalence (TCE) a technique that exploits the use of readily available compiler technology to address this long-standing challenge. TCE is directly applicable to real-world programs and can imbue existing tools with the ability to detect equivalent mutants and a special form of useless mutants called duplicated mutants. We present a thorough empirical study using 6 large open source programs, several orders of magnitude larger than those used in previous work, and 18 benchmark programs with hand-analysis equivalent mutants. Our results reveal that, on large real-world programs, TCE can discard more than 7% and 21% of all the mutants as being equivalent and duplicated mutants respectively\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "176\n", "authors": ["23"]}
{"title": "A study of equivalent and stubborn mutation operators using human analysis of equivalence\n", "abstract": " Though mutation testing has been widely studied for more than thirty years, the prevalence and properties of equivalent mutants remain largely unknown. We report on the causes and prevalence of equivalent mutants and their relationship to stubborn mutants (those that remain undetected by a high quality test suite, yet are non-equivalent). Our results, based on manual analysis of 1,230 mutants from 18 programs, reveal a highly uneven distribution of equivalence and stubbornness. For example, the ABS class and half UOI class generate many equivalent and almost no stubborn mutants, while the LCR class generates many stubborn and few equivalent mutants. We conclude that previous test effectiveness studies based on fault seeding could be skewed, while developers of mutation testing tools should prioritise those operators that we found generate disproportionately many stubborn (and few equivalent\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "155\n", "authors": ["23"]}
{"title": "Efficient multi-objective higher order mutation testing with genetic programming\n", "abstract": " It is said 90% of faults that survive manufacturer\u0393\u00c7\u00d6s testing procedures are complex. That is, the corresponding bug fix contains multiple changes. Higher order mutation testing is used to study defect interactions and their impact on software testing for fault finding. We adopt a multi-objective Pareto optimal approach using Monte Carlo sampling, genetic algorithms and genetic programming to search for higher order mutants which are both hard-to-kill and realistic. The space of complex faults (higher order mutants) is much larger than that of traditional first order mutations which correspond to simple faults, nevertheless search based approaches make this scalable. The problems of non-determinism and efficiency are overcome. Easy to detect faults may become harder to detect when they interact and impossible to detect single faults may be brought to light when code contains two such faults. We use strong typing and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "155\n", "authors": ["23"]}
{"title": "Genetic improvement of software: a comprehensive survey\n", "abstract": " Genetic improvement (GI) uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of GI back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. GI has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between GI and other areas, such as program transformation, approximate computing, and\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "154\n", "authors": ["23"]}
{"title": "Using hybrid algorithm for pareto efficient multi-objective test suite minimisation\n", "abstract": " Test suite minimisation techniques seek to reduce the effort required for regression testing by selecting a subset of test suites. In previous work, the problem has been considered as a single-objective optimisation problem. However, real world regression testing can be a complex process in which multiple testing criteria and constraints are involved. This paper presents the concept of Pareto efficiency for the test suite minimisation problem. The Pareto-efficient approach is inherently capable of dealing with multiple objectives, providing the decision maker with a group of solutions that are not dominated by each other. The paper illustrates the benefits of Pareto efficient multi-objective test suite minimisation with empirical studies of two and three objective formulations, in which multiple objectives such as coverage and past fault-detection history are considered. The paper utilises a hybrid, multi-objective genetic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "154\n", "authors": ["23"]}
{"title": "Evolutionary testing in the presence of loop-assigned flags: A testability transformation approach\n", "abstract": " Evolutionary testing is an effective technique for automatically generating good quality test data. However, for structural testing, the technique degenerates to random testing in the presence of flag variables, which also present problems for other automated test data generation techniques. Previous work on the flag problem does not address flags assigned in loops.This paper introduces a testability transformation that transforms programs with loop--assigned flags so that existing genetic approaches can be successfully applied. It then presents empirical data demonstrating the effectiveness of the transformation. Untransformed, the genetic algorithm flounders and is unable to find a solution. Two transformations are considered. The first allows the search to find a solution. The second reduces the time taken by an order of magnitude and, more importantly, reduces the slope of the cost increase; thus, greatly increasing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "150\n", "authors": ["23"]}
{"title": "Using program slicing to simplify testing\n", "abstract": " Program slicing is a technique for automatically identifying the statements of a program which affect a selected subset of its variables. A large program can be divided into a number of smaller program (its slices), each constructed for different variable subsets. The slices are typically simpler than the original program, thereby simplifying the process of testing a property of the program which only concerns the corresponding subset of its variables. However, some aspects of a program's computation are not captured by a set of variables, rendering slicing inapplicable. To overcome this difficulty a program can be rewritten in a self\u0393\u00c7\u00c9checking form by the addition of assignment statements to denote these \u0393\u00c7\u00ffimplicit\u0393\u00c7\u00d6 computations. Initially this makes the program longer. However, slicing can now be applied to the introspective program, forming a slice concerned solely with the implicit computation. The simplification power of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "146\n", "authors": ["23"]}
{"title": "Comparing white-box and black-box test prioritization\n", "abstract": " Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "144\n", "authors": ["23"]}
{"title": "Testing and verification in service\u0393\u00c7\u00c9oriented architecture: a survey\n", "abstract": " Service\u0393\u00c7\u00c9oriented architecture (SOA) is gaining momentum as an emerging distributed system architecture for business\u0393\u00c7\u00c9to\u0393\u00c7\u00c9business collaborations. This momentum can be observed in both industry and academic research. SOA presents new challenges and opportunities for testing and verification, leading to an upsurge in research. This paper surveys the previous work undertaken on testing and verification of service\u0393\u00c7\u00c9centric systems, which in total are 177 papers, showing the strengths and weaknesses of current strategies and testing tools and identifying issues for future work. Copyright \u252c\u2310 2012 John Wiley & Sons, Ltd.", "num_citations": "141\n", "authors": ["23"]}
{"title": "Automated web application testing using search based software engineering\n", "abstract": " This paper introduces three related algorithms and a tool, SWAT, for automated web application testing using Search Based Software Testing (SBST). The algorithms significantly enhance the efficiency and effectiveness of traditional search based techniques exploiting both static and dynamic analysis. The combined approach yields a 54% increase in branch coverage and a 30% reduction in test effort. Each improvement is separately evaluated in an empirical study on 6 real world web applications.", "num_citations": "141\n", "authors": ["23"]}
{"title": "Test prioritization using system models\n", "abstract": " During regression testing, a modified system is retested using the existing test suite. Because the size of the test suite may be very large, testers are interested in detecting faults in the system as early as possible during the retesting process. Test prioritization tries to order test cases for execution so the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the code of the system. System modeling is a widely used technique to model state-based systems. In this paper, we present methods of test prioritization based on state-based models after changes to the model and the system. The model is executed for the test suite and information about model execution is used to prioritize tests. Execution of the model is inexpensive as compared to execution of the system; therefore the overhead associated with test prioritization is relatively small. In addition, we\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "141\n", "authors": ["23"]}
{"title": "Improving Evolutionary Testing By Flag Removal.\n", "abstract": " This paper argues that Evolutionary testing can be improved by transforming programs with flags into flag free programs. The approach is evaluated by comparing results from the application of the Daimler-Chrysler Evolutionary Testing \u251c\u00efystem to programs with flags and their transformed flagfree counterparts. The results of this empirical study are very encouraging. Programs which could not be fully covered become fully coverable and the number of generations required to achieve full coverage is greatly reduced.", "num_citations": "131\n", "authors": ["23"]}
{"title": "Multi-objective software effort estimation\n", "abstract": " We introduce a bi-objective effort estimation algorithm that combines Confidence Interval Analysis and assessment of Mean Absolute Error. We evaluate our proposed algorithm on three different alternative formulations, baseline comparators and current state-of-the-art effort estimators applied to five real-world datasets from the PROMISE repository, involving 724 different software projects in total. The results reveal that our algorithm outperforms the baseline, state-of-the-art and all three alternative formulations, statistically significantly (p <; 0.001) and with large effect size (A 12  \u0393\u00eb\u00d1 0.9) over all five datasets. We also provide evidence that our algorithm creates a new state-of-the-art, which lies within currently claimed industrial human-expert-based thresholds, thereby demonstrating that our findings have actionable conclusions for practicing software engineers.", "num_citations": "128\n", "authors": ["23"]}
{"title": "Amorphous program slicing\n", "abstract": " Traditional, syntax-preserving program slicing simplifies a program by deleting components (e.g., statements and predicates) that do not affect a computation of interest. Amorphous slicing removes the limitation to component deletion as the only means of simplification, while retaining the semantic property that a slice preserves the selected behaviour of interest from the original program. This leads to slices which are often considerably smaller than their syntax-preserving counterparts.A formal framework is introduced to define and compare amorphous and traditional program slicing. After this definition, an algorithm for computing amorphous slices, based on the system dependence graph, is presented. An implementation of this algorithm is used to demonstrate the utility of amorphous slicing with respect to code-level analysis of array access safety. The resulting empirical study indicates that programmers\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "128\n", "authors": ["23"]}
{"title": "The app sampling problem for app store mining\n", "abstract": " Many papers on App Store Mining are susceptible to the App Sampling Problem, which exists when only a subset of apps are studied, resulting in potential sampling bias. We introduce the App Sampling Problem, and study its effects on sets of user review data. We investigate the effects of sampling bias, and techniques for its amelioration in App Store Mining and Analysis, where sampling bias is often unavoidable. We mine 106,891 requests from 2,729,103 user reviews and investigate the properties of apps and reviews from 3 different partitions: the sets with fully complete review data, partially complete review data, and no review data at all. We find that app metrics such as price, rating, and download rank are significantly different between the three completeness levels. We show that correlation analysis can find trends in the data that prevail across the partitions, offering one possible approach to App Store\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "125\n", "authors": ["23"]}
{"title": "Testing web services: A survey\n", "abstract": " The Service-Oriented Computing (SOC) paradigm is allowing computer systems to interact with each other in new ways. According to the literature, SOC allows composition of distributed applications free from their platform and thus reduces the cost of such compositions and makes them easier and faster to develop. Currently web services are the most widely accepted service technology due to the level of autonomy and platform-independency they provide. However, web services also bring challenges. For example, testing web services at the client side is not as straightforward as testing traditional software due to the complex nature of web services and the absence of source code. This paper surveys the previous work undertaken on web service testing, showing the strengths and weaknesses of current web service testing strategies and identifying issues for future work.", "num_citations": "125\n", "authors": ["23"]}
{"title": "An empirical study of static program slice size\n", "abstract": " This article presents results from a study of all slices from 43 programs, ranging up to 136,000 lines of code in size. The study investigates the effect of five aspects that affect slice size. Three slicing algorithms are used to study two algorithmic aspects: calling-context treatment and slice granularity. The remaining three aspects affect the upstream dependencies considered by the slicer. These include collapsing structure fields, removal of dead code, and the influence of points-to analysis. The results show that for the most precise slicer, the average slice contains just under one-third of the program. Furthermore, ignoring calling context causes a 50% increase in slice size, and while (coarse-grained) function-level slices are 33% larger than corresponding statement-level slices, they may be useful predictors of the (finer-grained) statement-level slice size. Finally, upstream analyses have an order of magnitude less\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "123\n", "authors": ["23"]}
{"title": "Automated software transplantation\n", "abstract": " Automated transplantation would open many exciting avenues for software development: suppose we could autotransplant code from one system into another, entirely unrelated, system. This paper introduces a theory, an algorithm, and a tool that achieve this. Leveraging lightweight annotation, program analysis identifies an organ (interesting behavior to transplant); testing validates that the organ exhibits the desired behavior during its extraction and after its implantation into a host. While we do not claim automated transplantation is now a solved problem, our results are encouraging: we report that in 12 of 15 experiments, involving 5 donors and 3 hosts (all popular real-world systems), we successfully autotransplanted new functionality and passed all regression tests. Autotransplantation is also already useful: in 26 hours computation time we successfully autotransplanted the H. 264 video encoding functionality\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "121\n", "authors": ["23"]}
{"title": "Predictive mutation testing\n", "abstract": " Test suites play a key role in ensuring software quality. A good test suite may detect more faults than a poor-quality one. Mutation testing is a powerful methodology for evaluating the fault-detection ability of test suites. In mutation testing, a large number of mutants may be generated and need to be executed against the test suite under evaluation to check how many mutants the test suite is able to detect, as well as the kind of mutants that the current test suite fails to detect. Consequently, although highly effective, mutation testing is widely recognized to be also computationally expensive, inhibiting wider uptake. To alleviate this efficiency concern, we propose Predictive Mutation Testing (PMT): the first approach to predicting mutation testing results without executing mutants. In particular, PMT constructs a classification model, based on a series of features related to mutants and tests, and uses the model to predict\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "120\n", "authors": ["23"]}
{"title": "Symbolic search-based testing\n", "abstract": " We present an algorithm for constructing fitness functions that improve the efficiency of search-based testing when trying to generate branch adequate test data. The algorithm combines symbolic information with dynamic analysis and has two key advantages: It does not require any change in the underlying test data generation technique and it avoids many problems traditionally associated with symbolic execution, in particular the presence of loops. We have evaluated the algorithm on industrial closed source and open source systems using both local and global search-based testing techniques, demonstrating that both are statistically significantly more efficient using our approach. The test for significance was done using a one-sided, paired Wilcoxon signed rank test. On average, the local search requires 23.41% and the global search 7.78% fewer fitness evaluations when using a symbolic execution based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "120\n", "authors": ["23"]}
{"title": "Evolutionary testing of autonomous software agents\n", "abstract": " A system built in terms of autonomous software agents may require even greater correctness assurance than one that is merely reacting to the immediate control of its users. Agents make substantial decisions for themselves, so thorough testing is an important consideration. However, autonomy also makes testing harder; by their nature, autonomous agents may react in different ways to the same inputs over time, because, for instance they have changeable goals and knowledge. For this reason, we argue that testing of autonomous agents requires a procedure that caters for a wide range of test case contexts, and that can search for the most demanding of these test cases, even when they are not apparent to the agents\u0393\u00c7\u00d6 developers. In this paper, we address this problem, introducing and evaluating an approach to testing autonomous agents that uses evolutionary optimisation to generate demanding test\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "117\n", "authors": ["23"]}
{"title": "Flopsy-search-based floating point constraint solving for symbolic execution\n", "abstract": " Recently there has been an upsurge of interest in both, Search\u0393\u00c7\u00f4Based Software Testing (SBST), and Dynamic Symbolic Execution (DSE). Each of these two approaches has complementary strengths and weaknesses, making it a natural choice to explore the degree to which the strengths of one can be exploited to offset the weakness of the other. This paper introduces an augmented version of DSE that uses a SBST\u0393\u00c7\u00f4based approach to handling floating point computations, which are known to be problematic for vanilla DSE. The approach has been implemented as a plug in for the Microsoft Pex DSE testing tool. The paper presents results from both, standard evaluation benchmarks, and two open source programs.", "num_citations": "110\n", "authors": ["23"]}
{"title": "A study of the bi-objective next release problem\n", "abstract": " One important issue addressed by software companies is to determine which features should be included in the next release of their products, in such a way that the highest possible number of customers get satisfied while entailing the minimum cost for the company. This problem is known as the Next Release Problem (NRP). Since minimizing the total cost of including new features into a software package and maximizing the total satisfaction of customers are contradictory objectives, the problem has a multi-objective nature. In this work, we apply three state-of-the-art multi-objective metaheuristics (two genetic algorithms, NSGA-II and MOCell, and one evolutionary strategy, PAES) for solving NRP. Our goal is twofold: on the one hand, we are interested in analyzing the results obtained by these metaheuristics over a benchmark composed of six academic problems plus a real world data set provided by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "109\n", "authors": ["23"]}
{"title": "An empirical study on mutation, statement and branch coverage fault revelation that avoids the unreliable clean program assumption\n", "abstract": " Many studies suggest using coverage concepts, such as branch coverage, as the starting point of testing, while others as the most prominent test quality indicator. Yet the relationship between coverage and fault-revelation remains unknown, yielding uncertainty and controversy. Most previous studies rely on the Clean Program Assumption, that a test suite will obtain similar coverage for both faulty and fixed ('clean') program versions. This assumption may appear intuitive, especially for bugs that denote small semantic deviations. However, we present evidence that the Clean Program Assumption does not always hold, thereby raising a critical threat to the validity of previous results. We then conducted a study using a robust experimental methodology that avoids this threat to validity, from which our primary finding is that strong mutation testing has the highest fault revelation of four widely-used criteria. Our findings\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "107\n", "authors": ["23"]}
{"title": "Learning combinatorial interaction test generation strategies using hyperheuristic search\n", "abstract": " The surge of search based software engineering research has been hampered by the need to develop customized search algorithms for different classes of the same problem. For instance, two decades of bespoke Combinatorial Interaction Testing (CIT) algorithm development, our exemplar problem, has left software engineers with a bewildering choice of CIT techniques, each specialized for a particular task. This paper proposes the use of a single hyperheuristic algorithm that learns search strategies across a broad range of problem instances, providing a single generalist approach. We have developed a Hyperheuristic algorithm for CIT, and report experiments that show that our algorithm competes with known best solutions across constrained and unconstrained problems: For all 26 real-world subjects, it equals or outperforms the best result previously reported in the literature. We also present evidence that our\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "107\n", "authors": ["23"]}
{"title": "Fault localization prioritization: Comparing information-theoretic and coverage-based approaches\n", "abstract": " Test case prioritization techniques seek to maximize early fault detection. Fault localization seeks to use test cases already executed to help find the fault location. There is a natural interplay between the two techniques; once a fault is detected, we often switch focus to fault fixing, for which localization may be a first step. In this article we introduce the Fault Localization Prioritization (FLP) problem, which combines prioritization and localization. We evaluate three techniques: a novel FLP technique based on information theory, FLINT (Fault Localization using INformation Theory), that we introduce in this article, a standard Test Case Prioritization (TCP) technique, and a \u0393\u00c7\u00a3test similarity technique\u0393\u00c7\u00a5 used in previous work. Our evaluation uses five different releases of four software systems. The results indicate that FLP and TCP can statistically significantly reduce fault localization costs for 73% and 76% of cases\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "106\n", "authors": ["23"]}
{"title": "Tool-supported refactoring of existing object-oriented code into aspects\n", "abstract": " Aspect-oriented programming (AOP) provides mechanisms for the separation of crosscutting concerns - functionalities scattered through the system and tangled with the base code. Existing systems are a natural testbed for the AOP approach since they often contain several crosscutting concerns which could not be modularized using traditional programming constructs. This paper presents an automated approach to the problem of migrating systems developed according to the object-oriented programming (OOP) paradigm into aspect-oriented programming (AOP). A simple set of six refactorings has been defined to transform OOP to AOP and has been implemented in the AOP-migrator tool, an Eclipse plug-in. A set of enabling transformations from OOP to OOP complement the initial set of refactorings. The paper presents the results of four case studies, which use the approach to migrate selected crosscutting\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "106\n", "authors": ["23"]}
{"title": "Reducing energy consumption using genetic improvement\n", "abstract": " Genetic Improvement (GI) is an area of Search Based Software Engineering which seeks to improve software's non-functional properties by treating program code as if it were genetic material which is then evolved to produce more optimal solutions. Hitherto, the majority of focus has been on optimising program's execution time which, though important, is only one of many non-functional targets. The growth in mobile computing, cloud computing infrastructure, and ecological concerns are forcing developers to focus on the energy their software consumes. We report on investigations into using GI to automatically find more energy efficient versions of the MiniSAT Boolean satisfiability solver when specialising for three downstream applications. Our results find that GI can successfully be used to reduce energy consumption by up to 25%", "num_citations": "105\n", "authors": ["23"]}
{"title": "The GISMOE challenge: Constructing the pareto program surface using genetic programming to find better programs (keynote paper)\n", "abstract": " Optimising programs for non-functional properties such as speed, size, throughput, power consumption and bandwidth can be demanding; pity the poor programmer who is asked to cater for them all at once! We set out an alternate vision for a new kind of software development environment inspired by recent results from Search Based Software Engineering (SBSE). Given an input program that satisfies the functional requirements, the proposed programming environment will automatically generate a set of candidate program implementations, all of which share functionality, but each of which differ in their non-functional trade offs. The software designer navigates this diverse Pareto surface of candidate implementations, gaining insight into the trade offs and selecting solutions for different platforms and environments, thereby stretching beyond the reach of current compiler technologies. Rather than having to focus\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["23"]}
{"title": "Optimizing for the number of tests generated in search based test data generation with an application to the oracle cost problem\n", "abstract": " Previous approaches to search based test data generation tend to focus on coverage, rather than oracle cost. While there may be an aspiration that systems should have models, checkable specifications and/or contract driven development, this sadly remains an aspiration; in many real cases, system behaviour must be checked by a human. This painstaking checking process forms a significant cost, the oracle cost, which previous work on automated test data generation tends to overlook. One simple way to reduce oracle cost consists of reducing the number of tests generated. In this paper we introduce three algorithms which do this without compromising coverage achieved. We present the results of an empirical study of the effectiveness of the three algorithms on five benchmark programs containing non trivial search spaces for branch coverage. The results indicate that it is, indeed, possible to make reductions in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "105\n", "authors": ["23"]}
{"title": "Developer recommendation for crowdsourced software development tasks\n", "abstract": " Crowdsourced software development utilises an open call format to attract geographically distributed developers to accomplish various types of software development tasks. Although the open call format enables wide task accessibility, potential developers must choose from a dauntingly large set of task options (usually more than one hundred available tasks on TopCoder each day). Inappropriate developer-task matching may lower the quality of the software deliverables. In this paper, we employ content-based recommendation techniques to automatically match tasks and developers. The approach learns particular interests from registration history and mines winner history to favour appropriate developers. We measure the performance of our approach by defining accuracy and diversity metrics. We evaluate our recommendation approach by introducing 4 machine learners on 3,094 historical tasks from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "104\n", "authors": ["23"]}
{"title": "AUSTIN: An open source tool for search based software testing of C programs\n", "abstract": " ContextDespite the large number of publications on Search-Based Software Testing (SBST), there remain few publicly available tools. This paper introduces AUSTIN, a publicly available open source SBST tool for the C language.1 The paper is an extension of previous work [1]. It includes a new hill climb algorithm implemented in AUSTIN and an investigation into the effectiveness and efficiency of different pointer handling techniques implemented by AUSTIN\u0393\u00c7\u00d6s test data generation algorithms.ObjectiveTo evaluate the different search algorithms implemented within AUSTIN on open source systems with respect to effectiveness and efficiency in achieving branch coverage. Further, to compare AUSTIN against a non-publicly available, state-of-the-art Evolutionary Testing Framework (ETF).MethodFirst, we use example functions from open source benchmarks as well as common data structure implementations to check\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "103\n", "authors": ["23"]}
{"title": "An empirical study of the robustness of two module clustering fitness functions\n", "abstract": " Two of the attractions of search-based software engineering (SBSE) derive from the nature of the fitness functions used to guide the search. These have proved to be highly robust (for a variety of different search algorithms) and have yielded insight into the nature of the search space itself, shedding light upon the software engineering problem in hand. This paper aims to exploit these two benefits of SBSE in the context of search based module clustering. The paper presents empirical results which compare the robustness of two fitness functions used for software module clustering: one (MQ) used exclusively for module clustering. The other is EVM, a clustering fitness function previously applied to time series and gene expression data. The results show that both metrics are relatively robust in the presence of noise, with EVM being the more robust of the two. The results may also yield some interesting insights into the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "102\n", "authors": ["23"]}
{"title": "Threats to the validity of mutation-based test assessment\n", "abstract": " Much research on software testing and test techniques relies on experimental studies based on mutation testing. In this paper we reveal that such studies are vulnerable to a potential threat to validity, leading to possible Type I errors; incorrectly rejecting the Null Hypothesis. Our findings indicate that Type I errors occur, for arbitrary experiments that fail to take countermeasures, approximately 62% of the time. Clearly, a Type I error would potentially compromise any scientific conclusion. We show that the problem derives from such studies\u0393\u00c7\u00d6 combined use of both subsuming and subsumed mutants. We collected articles published in the last two years at three leading software engineering conferences. Of those that use mutation-based test assessment, we found that 68% are vulnerable to this threat to validity.", "num_citations": "101\n", "authors": ["23"]}
{"title": "The role of artificial intelligence in software engineering\n", "abstract": " There has been a recent surge in interest in the application of Artificial Intelligence (AI) techniques to Software Engineering (SE) problems. The work is typified by recent advances in Search Based Software Engineering, but also by long established work in Probabilistic reasoning and machine learning for Software Engineering. This paper explores some of the relationships between these strands of closely related work, arguing that they have much in common and sets out some future challenges in the area of AI for SE.", "num_citations": "99\n", "authors": ["23"]}
{"title": "Making the case for MORTO: Multi objective regression test optimization\n", "abstract": " This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.", "num_citations": "96\n", "authors": ["23"]}
{"title": "Software engineering meets evolutionary computation\n", "abstract": " This interest in SBSE in general and evolutionary computation for software engineering in particular has increased rapidly in the past 10 years. Figure 1 shows the growth in publications in SBSE and the concomitant increase in papers within the SBSE field that use evolutionary computation.SBSE is not only an academic research area\u0393\u00c7\u00f6it increasingly provides a set of methods, tools, and techniques that are finding widespread industrial application. The first (and still the most widely) studied area of research targets the application of SBSE to automated test data generation. 12 Known as search-based software testing, this widely surveyed area has its own coherent body of literature, and SBST serves as the topic area for a dedicated annual", "num_citations": "95\n", "authors": ["23"]}
{"title": "Pre/post conditioned slicing\n", "abstract": " Th paper shows how analysis of programs in terms of pre- and postconditions can be improved using a generalisation of conditioned program slicing called pre/post conditioned slicing. Such conditions play an important role in program comprehension, reuse, verification and reengineering. Fully automated analysis is impossible because of the inherent undecidability of pre- and post- conditions. The method presented reformulates the problem to circumvent this. The reformulation is constructed so that programs which respect the pre- and post-conditions applied to them have empty slices. For those which do not respect the conditions, the slice contains statements which could potentially break the conditions. This separates the automatable part of the analysis from the human analysis.", "num_citations": "93\n", "authors": ["23"]}
{"title": "Cloud engineering is search based software engineering too\n", "abstract": " Many of the problems posed by the migration of computation to cloud platforms can be formulated and solved using techniques associated with Search Based Software Engineering (SBSE). Much of cloud software engineering involves problems of optimisation: performance, allocation, assignment and the dynamic balancing of resources to achieve pragmatic trade-offs between many competing technical and business objectives. SBSE is concerned with the application of computational search and optimisation to solve precisely these kinds of software engineering challenges. Interest in both cloud computing and SBSE has grown rapidly in the past five years, yet there has been little work on SBSE as a means of addressing cloud computing challenges. Like many computationally demanding activities, SBSE has the potential to benefit from the cloud; \u0393\u00c7\u00ffSBSE in the cloud\u0393\u00c7\u00d6. However, this paper focuses, instead, of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["23"]}
{"title": "Empirical Evaluation of Search Based Requirements Interaction Management\n", "abstract": " ContextRequirements optimization has been widely studied in the Search Based Software Engineering (SBSE) literature. However, previous approaches have not handled requirement interactions, such as the dependencies that may exist between requirements, and, or, precedence, cost- and value-based constraints.ObjectiveTo introduce and evaluate a Multi-Objective Search Based Requirements Selection technique, using chromosome repair and to evaluate it on both synthetic and real world data sets, in order to assess its effectiveness and scalability. The paper extends and improves upon our previous conference paper on requirements interaction management.1MethodThe popular multi-objective evolutionary algorithm NSGA-II was used to produce baseline data for each data set in order to determine how many solutions on the Pareto front fail to meet five different requirement interaction constraints. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "89\n", "authors": ["23"]}
{"title": "Search based software engineering for program comprehension\n", "abstract": " Search based software engineering (SBSE) is an approach to software engineering in which search based optimization algorithms are used to identify optimal or near optimal solutions and to yield insight. SBSE techniques can cater for multiple, possibly competing objectives and/or constraints and applications where the potential solution space is large and complex. Such situations are common in software engineering, leading to an increasing interest in SBSE. This paper provides a brief overview of SBSE, explaining some of the ways in which it has already been applied to program-comprehension related activities. The paper also outlines some possible future applications of and challenges for the further application of SBSE to program comprehension.", "num_citations": "86\n", "authors": ["23"]}
{"title": "Causal impact analysis for app releases in google play\n", "abstract": " App developers would like to understand the impact of their own and their competitors\u0393\u00c7\u00d6 software releases. To address this we introduce Causal Impact Release Analysis for app stores, and our tool, CIRA, that implements this analysis. We mined 38,858 popular Google Play apps, over a period of 12 months. For these apps, we identified 26,339 releases for which there was adequate prior and posterior time series data to facilitate causal impact analysis. We found that 33% of these releases caused a statistically significant change in user ratings. We use our approach to reveal important characteristics that distinguish causal significance in Google Play. To explore the actionability of causal impact analysis, we elicited the opinions of app developers: 56 companies responded, 78% concurred with the causal assessment, of which 33% claimed that their company would consider changing its app release strategy as a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "85\n", "authors": ["23"]}
{"title": "ORBS: Language-independent program slicing\n", "abstract": " Current slicing techniques cannot handle systems written in multiple programming languages. Observation-Based Slicing (ORBS) is a language-independent slicing technique capable of slicing multi-language systems, including systems which contain (third party) binary components. A potential slice obtained through repeated statement deletion is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted. The resulting slice is similar to a dynamic slice. We evaluate five variants of ORBS on ten programs of different sizes and languages showing that it is less expensive than similar existing techniques. We also evaluate it on bash and four other systems to demonstrate feasible large-scale operation in which a parallelised ORBS needs up to 82% less time when using four threads. The results show that an ORBS slicer is\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "84\n", "authors": ["23"]}
{"title": "Highly scalable multi objective test suite minimisation using graphics cards\n", "abstract": " Despite claims of \u0393\u00c7\u00a3embarrassing parallelism\u0393\u00c7\u00a5 for many optimisation algorithms, there has been very little work on exploiting parallelism as a route for SBSE scalability. This is an important oversight because scalability is so often a critical success factor for Software Engineering work. This paper shows how relatively inexpensive General Purpose computing on Graphical Processing Units (GPGPU) can be used to run suitably adapted optimisation algorithms, opening up the possibility of cheap scalability. The paper develops a search based optimisation approach for multi objective regression test optimisation, evaluating it on benchmark problems as well as larger real world problems. The results indicate that speed\u0393\u00c7\u00f4ups of over 25x are possible using widely available standard GPUs. It is also encouraging that the results reveal a statistically strong correlation between larger problem instances and the degree\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "83\n", "authors": ["23"]}
{"title": "An empirical investigation into branch coverage for C programs using CUTE and AUSTIN\n", "abstract": " Automated test data generation has remained a topic of considerable interest for several decades because it lies at the heart of attempts to automate the process of Software Testing. This paper reports the results of an empirical study using the dynamic symbolic-execution tool, CUTE, and a search based tool, AUSTIN on five non-trivial open source applications. The aim is to provide practitioners with an assessment of what can be achieved by existing techniques with little or no specialist knowledge and to provide researchers with baseline data against which to measure subsequent work. To achieve this, each tool is applied \u0393\u00c7\u00ffas is\u0393\u00c7\u00d6, with neither additional tuning nor supporting harnesses and with no adjustments applied to the subject programs under test. The mere fact that these tools can be applied \u0393\u00c7\u00ffout of the box\u0393\u00c7\u00d6 in this manner reflects the growing maturity of Automated test data generation. However, as might be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "82\n", "authors": ["23"]}
{"title": "The relationship between search based software engineering and predictive modeling\n", "abstract": " Search Based Software Engineering (SBSE) is an approach to software engineering in which search based optimization algorithms are used to identify optimal or near optimal solutions and to yield insight. SBSE techniques can cater for multiple, possibly competing objectives and/or constraints and applications where the potential solution space is large and complex. This paper will provide a brief overview of SBSE, explaining some of the ways in which it has already been applied to construction of predictive models. There is a mutually beneficial relationship between predictive models and SBSE. The paper sets out eleven open problem areas for Search Based Predictive Modeling and describes how predictive models also have role to play in improving SBSE.", "num_citations": "82\n", "authors": ["23"]}
{"title": "Practical combinatorial interaction testing: Empirical findings on efficiency and early fault detection\n", "abstract": " Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "81\n", "authors": ["23"]}
{"title": "Pricing crowdsourcing-based software development tasks\n", "abstract": " Many organisations have turned to crowdsource their software development projects. This raises important pricing questions, a problem that has not previously been addressed for the emerging crowdsourcing development paradigm. We address this problem by introducing 16 cost drivers for crowdsourced development activities and evaluate 12 predictive pricing models using 4 popular performance measures. We evaluate our predictive models on TopCoder, the largest current crowdsourcing platform for software development. We analyse all 5,910 software development tasks (for which partial data is available), using these to extract our proposed cost drivers. We evaluate our predictive models using the 490 completed projects (for which full details are available). Our results provide evidence to support our primary finding that useful prediction quality is achievable (Pred(30)>0.8). We also show that simple\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "80\n", "authors": ["23"]}
{"title": "Not going to take this anymore: Multi-objective overtime planning for software engineering projects\n", "abstract": " Software Engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. In this paper, we introduce a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. We evaluate our approach on 6 real world software projects, drawn from 3 organisations using 3 standard evaluation measures and 3 different approaches to risk assessment. Our results show that our approach was significantly better (p <; 0.05) than standard multi-objective search in 76% of experiments (with high Cohen effect size in 85% of these) and was significantly better than currently used overtime planning strategies in 100% of experiments (with high effect size in all). We also show how our approach provides actionable overtime\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "80\n", "authors": ["23"]}
{"title": "Provably optimal and human-competitive results in sbse for spectrum based fault localisation\n", "abstract": " Fault localisation uses so-called risk evaluation formul\u251c\u00aa to guide the localisation process. For more than a decade, the design and improvement of these formul\u251c\u00aa has been conducted entirely manually through iterative publication in the fault localisation literature. However, recently we demonstrated that SBSE could be used to automatically design such formul\u251c\u00aa by recasting this as a problem for Genetic Programming(GP). In this paper we prove that our GP has produced four previously unknown globally optimal formul\u251c\u00aa. Though other human competitive results have previously been reported in the SBSE literature, this is the first SBSE result, in any application domain, for which human competitiveness has been formally proved. We also show that some of these formul\u251c\u00aa exhibit counter-intuitive characteristics, making them less likely to have been found solely by further human effort.", "num_citations": "79\n", "authors": ["23"]}
{"title": "Test data regeneration: generating new test data from existing test data\n", "abstract": " Existing automated test data generation techniques tend to start from scratch, implicitly assuming that no pre\u0393\u00c7\u00c9existing test data are available. However, this assumption may not always hold, and where it does not, there may be a missed opportunity; perhaps the pre\u0393\u00c7\u00c9existing test cases could be used to assist the automated generation of additional test cases. This paper introduces search\u0393\u00c7\u00c9based test data regeneration, a technique that can generate additional test data from existing test data using a meta\u0393\u00c7\u00c9heuristic search algorithm. The proposed technique is compared to a widely studied test data generation approach in terms of both efficiency and effectiveness. The empirical evaluation shows that test data regeneration can be up to 2 orders of magnitude more efficient than existing test data generation techniques, while achieving comparable effectiveness in terms of structural coverage and mutation score\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "79\n", "authors": ["23"]}
{"title": "Input domain reduction through irrelevant variable removal and its effect on local, global, and hybrid search-based structural test data generation\n", "abstract": " Search-Based Test Data Generation reformulates testing goals as fitness functions so that test input generation can be automated by some chosen search-based optimization algorithm. The optimization algorithm searches the space of potential inputs, seeking those that are \u0393\u00c7\u00a3fit for purpose,\u0393\u00c7\u00a5 guided by the fitness function. The search space of potential inputs can be very large, even for very small systems under test. Its size is, of course, a key determining factor affecting the performance of any search-based approach. However, despite the large volume of work on Search-Based Software Testing, the literature contains little that concerns the performance impact of search space reduction. This paper proposes a static dependence analysis derived from program slicing that can be used to support search space reduction. The paper presents both a theoretical and empirical analysis of the application of this approach to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "79\n", "authors": ["23"]}
{"title": "Evolving transformation sequences using genetic algorithms\n", "abstract": " Program transformation is useful in a number of applications including program comprehension, reverse engineering and compiler optimization. In all these applications, transformation algorithms are constructed by hand for each different transformation goal. Loosely speaking, a transformation algorithm defines a sequence of transformation steps to apply to a given program. It is notoriously hard to find good transformation sequences automatically, and so much (costly) human intervention is required. This work shows how search-based meta-heuristic algorithms can be used to automate, or partly automate the problem of finding good transformation sequences. In this case, the goal of transformation is to reduce program size, but the approach is sufficiently general that it can be used to optimize any source-code level metric. The search techniques used are random search (RS), hill climbing (HC) and genetic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "79\n", "authors": ["23"]}
{"title": "Why source code analysis and manipulation will always be important\n", "abstract": " This paper makes a case for Source Code Analysis and Manipulation. The paper argues that it will not only remain important, but that its importance will continue to grow. This argument is partly based on the 'law' of tendency to executability, which the paper introduces. The paper also makes a case for Source Code Analysis purely for the sake of analysis. Analysis for its own sake may not be merely indulgent introspection. The paper argues that it may ultimately prove to be hugely important as source code gradually gathers together all aspects of human socioeconomic and governmental processes and systems.", "num_citations": "78\n", "authors": ["23"]}
{"title": "Conditioned slicing supports partition testing\n", "abstract": " This paper describes the use of conditioned slicing to assist partition testing, illustrating this with a case study. The paper shows how a conditioned slicing tool can be used to provide confidence in the uniformity hypothesis for correct programs, to aid fault detection in incorrect programs and to highlight special cases. Copyright \u252c\u2310 2001 John Wiley & Sons, Ltd", "num_citations": "78\n", "authors": ["23"]}
{"title": "Automated test data generation for coverage: Haven't we solved this problem yet?\n", "abstract": " Whilst there is much evidence that both concolic and search based testing can outperform random testing, there has been little work demonstrating the effectiveness of either technique with complete real world software applications. As a consequence, many researchers have doubts not only about the scalability of both approaches but also their applicability to production code. This paper performs an empirical study applying a concolic tool, CUTE, and a search based tool, AUSTIN, to the source code of four large open source applications. Each tool is applied `out of the box'; that is without writing additional code for special handling of any of the individual subjects, or by tuning the tools' parameters. Perhaps surprisingly, the results show that both tools can only obtain at best a modest level of code coverage. Several challenges remain for improving automated test data generators in order to achieve higher levels of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "77\n", "authors": ["23"]}
{"title": "A comprehensive survey of trends in oracles for software testing\n", "abstract": " Testing involves examining the behaviour of a system in order to discover potential faults. Determining the desired correct behaviour for a given input is called the \u0393\u00c7\u00a3oracle problem\u0393\u00c7\u00a5. Oracle automation is important to remove a current bottleneck which inhibits greater overall test automation; without oracle automation, the human has to determine whether observed behaviour is correct. The literature on oracles has introduced techniques for oracle automation, including modelling, specifications, contract-driven development and metamorphic testing. When none of these is completely adequate, the final source of oracle information remains the human, who may be aware of informal specifications, expectations, norms and domain specific information that provide informal oracle guidance. All forms of oracle, even the humble human, involve challenges of reducing cost and increasing benefit. This paper provides a comprehensive survey of current approaches to the oracle problem and an analysis of trends in this important area of software testing research and practice.", "num_citations": "76\n", "authors": ["23"]}
{"title": "Code extraction algorithms which unify slicing and concept assignment\n", "abstract": " One approach to reverse engineering is to partially automate subcomponent extraction, improvement and subsequent recombination. Two previously proposed automated techniques for supporting this activity are slicing and concept assignment. However, neither is directly applicable in isolation; slicing criteria (sets of program variables) are simply too low level in many cases, while concept assignment typically fails to produce executable subcomponents. This paper introduces a unification of slicing and concept assignment which exploits their combined advantages, while overcoming their individual weaknesses. Our 'concept slices' are extracted using high level criteria, while producing executable subprograms. The paper introduces three ways of combining slicing, and concept assignment and algorithms for each. The application of the concept slicing algorithms is illustrated with a case study from a large\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "76\n", "authors": ["23"]}
{"title": "Automated session data repair for web application regression testing\n", "abstract": " This paper introduces an approach to web application regression testing, based upon repair of user session data. The approach is entirely automated. It consists of a white box examination of the structure of the changed web application to detect changes and a set of techniques to map these detected changes onto repair actions. The paper reports the results of experiments that explore both the performance and effectiveness of the approach. The effectiveness experiment uses an implementation of the repair algorithm applied to the online bookstore application over a series of 10 releases.", "num_citations": "73\n", "authors": ["23"]}
{"title": "Control dependence for extended finite state machines\n", "abstract": " Though there has been nearly three decades of work on program slicing, there has been comparatively little work on slicing for state machines. One of the primary challenges that currently presents a barrier to wider application of state machine slicing is the problem of determining control dependence. We survey existing related definitions, introducing a new definition that subsumes one and extends another. We illustrate that by using this new definition our slices respect Weiser slicing\u0393\u00c7\u00d6s termination behaviour. We prove results that clarify the relationships between our definition and older ones, following this up with examples to motivate the need for these differences.", "num_citations": "72\n", "authors": ["23"]}
{"title": "Automated test data generation using search based software engineering\n", "abstract": " Generating test data is a demanding process. Without automation, the process is slow, expensive and error-prone. However, techniques to automate test data generation must cater for a bewildering variety of functional and non-functional test adequacy criteria and must either implicitly or explicitly solve problems involving state propagation and constraint satisfaction. This talk will show how optimisation techniques associated with search based software engineering (SBSE) have been used to automate test data generation. The talk will survey the area and present the results of recent work on characterising, transforming and eliding test data search landscapes.", "num_citations": "71\n", "authors": ["23"]}
{"title": "Reducing qualitative human oracle costs associated with automatically generated test data\n", "abstract": " Due to the frequent non-existence of an automated oracle, test cases are often evaluated manually in practice. However, this fact is rarely taken into account by automatic test data generators, which seek to maximise a program's structural coverage only. The test data produced tends to be of a poor fit with the program's operational profile. As a result, each test case takes longer for a human to check, because the scenarios that arbitrary-looking data represent require time and effort to understand. This short paper proposes methods to extracting knowledge from programmers, source code and documentation and its incorporation into the automatic test data generation process so as to inject the realism required to produce test cases that are quick and easy for a human to comprehend and check. The aim is to reduce the so-called qualitative human oracle costs associated with automatic test data generation. The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["23"]}
{"title": "The relationship between program dependence and mutation analysis\n", "abstract": " This paper presents some connections between dependence analysis and mutation testing. Specifically, dependence analysis can be applied to two problems in mutation testing, captured by the questions:                                         1.                                             How do we avoid the creation of equivalent mutants?                                                                                2.                                             How do we generate test data that kills non-equivalent mutants?                                                                                       The theoretical connections described here suggest ways in which a dependence analysis tool might be used, in combination with existing tools for mutation testing, for test-data generation and equivalent-mutant detection.               In this paper the variable orientated, fine grained dependence framework of Jackson and Rollins is used to achieve these two goals. Yhis framework of dependence analysis appears to be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "68\n", "authors": ["23"]}
{"title": "Detecting trivial mutant equivalences via compiler optimisations\n", "abstract": " Mutation testing realises the idea of fault-based testing, i.e., using artificial defects to guide the testing process. It is used to evaluate the adequacy of test suites and to guide test case generation. It is a potentially powerful form of testing, but it is well-known that its effectiveness is inhibited by the presence of equivalent mutants. We recently studied Trivial Compiler Equivalence (TCE) as a simple, fast and readily applicable technique for identifying equivalent mutants for C programs. In the present work, we augment our findings with further results for the Java programming language. TCE can remove a large portion of all mutants because they are determined to be either equivalent or duplicates of other mutants. In particular, TCE equivalent mutants account for 7.4 and 5.7 percent of all C and Java mutants, while duplicated mutants account for a further 21 percent of all C mutants and 5.4 percent Java mutants, on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "67\n", "authors": ["23"]}
{"title": "An integer linear programming approach to the single and bi-objective next release problem\n", "abstract": " ContextThe Next Release Problem involves determining the set of requirements to implement in the next release of a software project. When the problem was first formulated in 2001, Integer Linear Programming, an exact method, was found to be impractical because of large execution times. Since then, the problem has mainly been addressed by employing metaheuristic techniques.ObjectiveIn this paper, we investigate if the single-objective and bi-objective Next Release Problem can be solved exactly and how to better approximate the results when exact resolution is costly.MethodsWe revisit Integer Linear Programming for the single-objective version of the problem. In addition, we integrate it within the Epsilon-constraint method to address the bi-objective problem. We also investigate how the Pareto front of the bi-objective problem can be approximated through an anytime deterministic Integer Linear\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["23"]}
{"title": "Babel pidgin: SBSE can grow and graft entirely new functionality into a real world system\n", "abstract": " Adding new functionality to an existing, large, and perhaps poorly-understood system is a challenge, even for the most competent human programmer. We introduce a \u0393\u00c7\u00ffgrow and graft\u0393\u00c7\u00d6 approach to Genetic Improvement (GI) that transplants new functionality into an existing system. We report on the trade offs between varying degrees of human guidance to the GI transplantation process. Using our approach, we successfully grew and transplanted a new \u0393\u00c7\u00ffBabel Fish\u0393\u00c7\u00d6 linguistic translation feature into the Pidgin instant messaging system, creating a genetically improved system we call \u0393\u00c7\u00ffBabel Pidgin\u0393\u00c7\u00d6. This is the first time that SBSE has been used to evolve and transplant entirely novel functionality into an existing system. Our results indicate that our grow and graft approach requires surprisingly little human guidance.", "num_citations": "66\n", "authors": ["23"]}
{"title": "An analysis of the relationship between conditional entropy and failed error propagation in software testing\n", "abstract": " Failed error propagation (FEP) is known to hamper software testing, yet it remains poorly understood. We introduce an information theoretic formulation of FEP that is based on measures of conditional entropy. This formulation considers the situation in which we are interested in the potential for an incorrect program state at statement s to fail to propagate to incorrect output. We define five metrics that differ in two ways: whether we only consider parts of the program that can be reached after executing s and whether we restrict attention to a single program path of interest. We give the results of experiments in which it was found that on average one in 10 tests suffered from FEP, earlier studies having shown that this figure can vary significantly between programs. The experiments also showed that our metrics are well-correlated with FEP. Our empirical study involved 30 programs, for which we executed a total of 7,140\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "66\n", "authors": ["23"]}
{"title": "\u0393\u00c7\u00a3Fairness analysis\u0393\u00c7\u00a5 in requirements assignments\n", "abstract": " Requirements engineering for multiple customers, each of whom have competing and often conflicting priorities, raises issues of negotiation, mediation and conflict resolution. This paper uses a multi-objective optimisation approach to support investigation of the trade-offs in various notions of fairness between multiple customers. Results are presented to validate the approach using two real-world data sets and also using data sets created specifically to stress test the approach. Simple graphical techniques are used to visualize the solution space.", "num_citations": "66\n", "authors": ["23"]}
{"title": "Computing unique input/output sequences using genetic algorithms\n", "abstract": " The problem of computing Unique Input/Ouput sequences (UIOs) is NP-hard. Genetic algorithms (GAs) have been proven to be effective in providing good solutions for some NP-hard problems. In this work, we investigated the construction of UIOs using GAs. We defined a fitness function to guide the search of potential UIOs and introduce a DO NOT CARE character to improve the GA\u0393\u00c7\u00d6s diversity. Experimental results suggest that, in a small system, the performance of the GA based approaches is no worse than that of random search while, in a more complex system, the GA based approaches outperform random search.", "num_citations": "65\n", "authors": ["23"]}
{"title": "Exact mean absolute error of baseline predictor, MARP0\n", "abstract": " Abstract Shepperd and MacDonell \u0393\u00c7\u00a3Evaluating prediction systems in software project estimation\u0393\u00c7\u00a5. Information and Software Technology 54 (8), 820\u0393\u00c7\u00f4827, 2012, proposed an improved measure of the effectiveness of predictors based on comparing them with random guessing. They suggest estimating the performance of random guessing using a Monte Carlo scheme which unfortunately excludes some correct guesses. This biases their MAR P 0 to be slightly too big, which in turn causes their standardised accuracy measure SA to over estimate slightly. In commonly used software engineering datasets it is practical to calculate an unbiased MAR P 0 exactly.", "num_citations": "64\n", "authors": ["23"]}
{"title": "Clustering mobile apps based on mined textual features\n", "abstract": " Context: Categorising software systems according to their functionality yields many benefits to both users and developers. Goal: In order to uncover the latent clustering of mobile apps in app stores, we propose a novel technique that measures app similarity based on claimed behaviour. Method: Features are extracted using information retrieval augmented with ontological analysis and used as attributes to characterise apps. These attributes are then used to cluster the apps using agglomerative hierarchical clustering. We empirically evaluate our approach on 17,877 apps mined from the BlackBerry and Google app stores in 2014. Results: The results show that our approach dramatically improves the existing categorisation quality for both Blackberry (from 0.02 to 0.41 on average) and Google (from 0.03 to 0.21 on average) stores. We also find a strong Spearman rank correlation (\u2567\u00fc= 0.96 for Google and \u2567\u00fc= 0.99 for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["23"]}
{"title": "Cooperative co-evolutionary optimization of software project staff assignments and job scheduling\n", "abstract": " This paper presents an approach to Search Based Software Project Management based on Cooperative Co-evolution. Our approach aims to optimize both developers\u0393\u00c7\u00d6 team staffing and work package scheduling through cooperative co-evolution to achieve early overall completion time. To evaluate our approach, we conducted an empirical study, using data from four real-world software projects. Results indicate that the Co-evolutionary approach significantly outperforms a single population evolutionary algorithm. Cooperative co-evolution has not previously been applied to any problem in Search Based Software Engineering (SBSE), so this paper reports the first application of cooperative co-evolution in the SBSE literature. We believe that co-evolutionary optimization may fit many applications in other SBSE problem domains, since software systems often have complex inter-related subsystems and are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "61\n", "authors": ["23"]}
{"title": "A new algorithm for slicing unstructured programs\n", "abstract": " Program slicing is an automatic program abstraction technique whose many applications include software maintenance, re\u0393\u00c7\u00c9engineering and comprehension, all of which rely crucially upon the precision of the slicing algorithm used. When slicing is applied to maintenance problems, the programs to be sliced are typically legacy systems, often written in older, \u0393\u00c7\u00ffunstructured\u0393\u00c7\u00d6 programming styles. For slicing to be a useful tool to the software maintainer it is therefore important to have precise algorithms for slicing unstructured programs. Unfortunately the standard algorithms for slicing structured programs do not extend correctly to the unstructured paradigm, and currently proposed modifications to these standard algorithms produce either unnecessarily large slices or slices which are not true subsets of the original program from which they are constructed. This paper introduces a modification of Agrawal's algorithm for\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "60\n", "authors": ["23"]}
{"title": "A parallel algorithm for static program slicing\n", "abstract": " Program Slicing is the process of deleting statements in a program that do not affect a given set of variables at a chosen point in the program. In this paper the first parallel slicing algorithm for static program slicing is introduced. It is shown how the control flow graph of the program to be sliced is converted into a network of concurrent processes, thereby producing a parallel version of Weiser's original static slicing algorithm.", "num_citations": "60\n", "authors": ["23"]}
{"title": "Program slicing\n", "abstract": " Program slicing is a decomposition technique that elides program components not relevant to a chosen computation, referred to as a slicing criterion. The remaining components form an executable program called a slice that computes a projection of the original program\u0393\u00c7\u00d6s semantics. Using examples coupled with fundamental principles, a tutorial introduction to program slicing is presented. Then applications of program slicing are surveyed, ranging from its first use as a debugging technique to current applications in property verification using finite state models. Finally, a summary of research challenges for the slicing community is discussed.", "num_citations": "59\n", "authors": ["23"]}
{"title": "App store analysis: Mining app stores for relationships between customer, business and technical characteristics\n", "abstract": " This paper argues that App Store Analysis can be used to understand the rich interplay between app customers and their developers. We use data mining to extract price and popularity information and natural language processing and data mining to elicit each app\u0393\u00c7\u00d6s claimed features from the Blackberry App Store, revealing strong correlations between customer rating and popularity (rank of app downloads). We found evidence for a mild correlation between price and the number of features claimed for an app and also found that higher priced features tended to be lower rated by their users. We also found that free apps have significantly (p-value< 0.001) higher rating than non-free apps, with a moderately high effect size", "num_citations": "58\n", "authors": ["23"]}
{"title": "A formalisation of the relationship between forms of program slicing\n", "abstract": " The widespread interest in program slicing within the source code analysis and manipulation community has led to the introduction of a large number of different forms of slicing. Each preserves some aspect of a program\u0393\u00c7\u00d6s behaviour and simplifies the program to focus exclusively upon this behaviour. In order to understand the similarities and differences between forms of slicing, a formal mechanism is required. This paper further develops a formal framework for comparing forms of slicing using a theory of program projection. This framework is used to reveal the ordering relationship between various static, dynamic, simultaneous and conditioned forms of slicing.", "num_citations": "58\n", "authors": ["23"]}
{"title": "Faster fault finding at Google using multi objective regression test optimisation\n", "abstract": " Companies such as Google tend to develop products from one continually evolving core of code. Software is neither shipped, nor released in the traditional sense. It is simply made available, with dramatically compressed release cycles regression testing. This large scale rapid release environment creates challenges for the application of regression test optimisation techniques. This paper reports initial results from a partnership between Google and the CREST centre at UCL aimed at transferring techniques from the regression test optimisation literature into industrial practice. The results illustrate the industrial potential for these techniques: regression test time can be reduced by between 33%\u0393\u00c7\u00f482%, while retaining fault detection capability. Our experience also highlights the importance of a multi objective approach: optimising for coverage and time alone is insufficient; we have, at least, to additionally prioritise historical fault revelation.", "num_citations": "57\n", "authors": ["23"]}
{"title": "An empirical investigation of the influence of a type of side effects on program comprehension\n", "abstract": " This paper reports the results of a study on the impact of a type of side effect (SE) upon program comprehension. We applied a crossover design on different tests involving fragments of C code that include increment and decrement operators. Each test had an SE version and a side-effect-free counterpart. The variables measured in the treatments were the number of correct answers and the time spent in answering. The results show that the side-effect operators considered significantly reduce performance in comprehension-related tasks, providing empirical justification for the belief that side effects are harmful.", "num_citations": "57\n", "authors": ["23"]}
{"title": "Investigating the relationship between price, rating, and popularity in the Blackberry World App Store\n", "abstract": " Context: App stores provide a software development space and a market place that are both different from those to which we have become accustomed for traditional software development: The granularity is finer and there is a far greater source of information available for research and analysis. Information is available on price, customer rating and, through the data mining approach presented in this paper, the features claimed by app developers. These attributes make app stores ideal for empirical software engineering analysis. Objective: This paper 1 exploits App Store Analysis to understand the rich interplay between app customers and their developers. Method: We use data mining to extract app descriptions, price, rating, and popularity information from the Blackberry World App Store, and natural language processing to elicit each apps\u0393\u00c7\u00d6 claimed features from its description. Results: The findings reveal that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["23"]}
{"title": "Multi objective higher order mutation testing with genetic programming\n", "abstract": " In academic empirical studies, mutation testing has been demonstrated to be a powerful technique for fault finding. However, it remains very expensive and the few valuable traditional mutants that resemble real faults are mixed in with many others that denote unrealistic faults.These twin problems of expense and realism have been a significant barrier to industrial uptake of mutation testing. Genetic programming is used to search the space of complex faults (higher order mutants). The space is much larger than the traditional first order mutation space of simple faults. However, the use of a search based approach makes this scalable, seeking only those mutants that challenge the tester, while the consideration of complex faults addresses the problem of fault realism; it is known that 90% of real faults are complex (i.e. higher order). We show that we are able to find examples that pose challenges to testing in the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "56\n", "authors": ["23"]}
{"title": "Coverage and fault detection of the output-uniqueness test selection criteria\n", "abstract": " This paper studies the whitebox coverage and fault detection achieved by Output Uniqueness, a newly proposed blackbox test criterion, using 6 web applications. We find that output uniqueness exhibits average correlation coefficients of 0.85, 0.83 and 0.97 with statement, branch and path coverage respectively. More interestingly, output uniqueness finds 92% of the real faults found by branch coverage (and a further 47% that remained undetected by such whitebox techniques). These results suggest that output uniqueness may provide a useful surrogate when whitebox techniques are inapplicable and an effective complement where they are.", "num_citations": "55\n", "authors": ["23"]}
{"title": "Search based data sensitivity analysis applied to requirement engineering\n", "abstract": " Software engineering is plagued by problems associated with unreliable cost estimates. This paper introduces an approach to sensitivity analysis for requirements engineering. It uses Search-Based Software Engineering to aid the decision maker to explore sensitivity of the cost estimates of requirements for the Next Release Problem (NRP). The paper presents both single-and multi-objective formulation of NRP with empirical sensitivity analysis on synthetic and real-world data. The results show strong correlation between the level of inaccuracy and the impact on the selection of requirements, as well as between the cost of requirements and the impact, which is as intuitively expected. However, there also exist a few sensitive exceptions to these trends; the paper uses a heat-map style visualisation to reveal these exceptions which require careful consideration. The paper also shows that such unusually sensitivity\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "54\n", "authors": ["23"]}
{"title": "Regression test suite prioritization using system models\n", "abstract": " During regression testing, a modified system is often retested using an existing test suite. Since the size of the test suite may be very large, testers are interested in detecting faults in the modified system as early as possible during this retesting process. Test prioritization attempts to order tests for execution so that the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the source code of the system under test. In this paper, we present and evaluate two model\u0393\u00c7\u00c9based selective methods and a dependence\u0393\u00c7\u00c9based method of test prioritization utilizing the state\u0393\u00c7\u00c9based model of the system under test. These methods assume that the modifications are made both on the system under test and its model. The existing test suite is executed on the system model and information about this execution is used to prioritize tests. Execution of the model is inexpensive as\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["23"]}
{"title": "Search based Software Engineering: Introduction to the special issue of the IEEE Transactions on Software Engineering\n", "abstract": " SEARCH Based Software Engineering (SBSE) consists of the application of search-based optimization to software engineering. Using SBSE, a software engineering task is formulated as a search problem by defining a suitable candidate solution representation and a fitness function to differentiate between solution candidates [11]. The candidate solution representation to the problem defines the search space in which the search takes place. In order to guide the search-based optimization process, a fitness function (or cost function) is required. This function determines the better of two candidate solutions, imbuing the search algorithm with the ability to differentiate between solutions and to measure progress. The most widely used algorithms for SBSE have been genetic algorithms, genetic programming, simulated annealing, and hill climbing (gradient descent)[12]. However, many other optimization techniques have also been applied, including greedy-based approaches and traditional operations research techniques through to more recently developed metaheuristic optimization techniques such as particle swarm optimization and ant colony optimization [12]. In this special issue, the authors have also used a variety of search-based optimization algorithms, most notably single and multi-objective genetic algorithms, genetic programming, and hill climbing. The most important attribute that these applications share is the search based formulation; the problem is thought of as a search among a large space of candidate solutions. It is from this search based approach that Search Based Software Engineering derives its name.The term SBSE was\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "53\n", "authors": ["23"]}
{"title": "Mutation-aware fault prediction\n", "abstract": " We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p\u0393\u00eb\u00f1 0.05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments.", "num_citations": "52\n", "authors": ["23"]}
{"title": "Automatically generating realistic test input from web services\n", "abstract": " Generating realistic test data is a major problem for software testers. Realistic test data generation for certain input types is hard to automate and therefore laborious. We propose a novel automated solution to test data generation that exploits existing web services as sources of realistic test data. Our approach is capable of generating realistic test data and also generating data based on tester-specified constraints. In experimental analysis, our prototype tool achieved between 93% and 100% success rates in generating realistic data using service compositions while random test data generation achieved only between 2% and 34%.", "num_citations": "52\n", "authors": ["23"]}
{"title": "Estimating the feasibility of transition paths in extended finite state machines\n", "abstract": " There has been significant interest in automating testing on the basis of an extended finite state machine (EFSM) model of the required behaviour of the implementation under test (IUT). Many test criteria require that certain parts of the EFSM are executed. For example, we may want to execute every transition of the EFSM. In order to find a test suite (set of input sequences) that achieves this we might first derive a set of paths through the EFSM that satisfy the criterion using, for example, algorithms from graph theory. We then attempt to produce input sequences that trigger these paths. Unfortunately, however, the EFSM might have infeasible paths and the problem of determining whether a path is feasible is generally undecidable. This paper describes an approach in which a fitness function is used to estimate how easy it is to find an input sequence to trigger a given path through an EFSM. Such a fitness\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "51\n", "authors": ["23"]}
{"title": "Why the virtual nature of software makes it ideal for search based optimization\n", "abstract": " This paper provides a motivation for the application of search based optimization to Software Engineering, an area that has come to be known as Search Based Software Engineering (SBSE). SBSE techniques have already been applied to many problems throughout the Software Engineering lifecycle, with new application domains emerging on a regular basis. The approach is very generic and therefore finds wide application in Software Engineering. It facilitates automated and semi-automated solutions in situations typified by large complex problem spaces with multiple competing and conflicting objectives. Previous work has already discussed, in some detail, the advantages of the SBSE approach for Software Engineering. This paper summarises previous work and goes further, by arguing that Software Engineering provides the ideal set of application problems for which optimization algorithms are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "50\n", "authors": ["23"]}
{"title": "Empirical study of optimization techniques for massive slicing\n", "abstract": " This article presents results from a study of techniques that improve the performance of graph-based interprocedural slicing of the System Dependence Graph (SDG). This is useful in \u0393\u00c7\u00a3massive slicing\u0393\u00c7\u00a5 where slices are required for many or all of the possible set of slicing criteria. Several different techniques are considered, including forming strongly connected components, topological sorting, and removing transitive edges. Data collected from a test bed of just over 1,000,000 lines of code are presented. This data illustrates the impact on computation time of the techniques. Together, the best combination produces a 71% reduction in run-time (and a 64% reduction in memory usage). The complete set of techniques also illustrates the point at which faster computation is not viable due to prohibitive preprocessing costs.", "num_citations": "50\n", "authors": ["23"]}
{"title": "Open problems in testability transformation\n", "abstract": " Testability transformation (tetra) seeks to transform a program in order to make it easier to generate test data. The test data is generated from the transformed version of the program, but it is applied to the original version for testing purposes. A transformation is a testability transformation with respect to a test adequacy criterion if all test data that is adequate for the transformed program is also adequate for the untransformed program. Testability transformation has been shown to be effective at improving coverage for search based test data generation. However, there are many interesting open problems. This paper presents some of these open problems. The aim is to show how testability transformation can be applied to a wide range of testing scenarios.", "num_citations": "48\n", "authors": ["23"]}
{"title": "Testability transformation for efficient automated test data search in the presence of nesting\n", "abstract": " The application of metaheuristic search techniques to the automatic generation of software test data has been shown to be an effective approach for a variety of testing criteria. However, for structural testing, the dependence of a target structure on nested decision statements can cause efficiency problems for the search, and failure in severe cases. This is because all information useful for guiding the search-in the form of the values of variables at branching predicates-is only gradually made available as each nested conditional is satisfied, one after the other. The provision of guidance is further restricted by the fact that the path up to that conditional must be maintained by obeying the constraints imposed by \u0393\u00c7\u00ffearlier\u0393\u00c7\u00d6conditionals. An empirical study presented in this paper shows the prevalence of types of if statement pairs in real-world code, where the second if statement in the pair is nested within the first. A testability transformation is proposed in order to circumvent the problem. The transformation allows all branch predicate information to be evaluated at the same time, regardless of whether \u0393\u00c7\u00ffearlier\u0393\u00c7\u00d6predicates in the sequence of nested conditionals have been satisfied or not. An experimental study is then presented, which shows the power of the approach, comparing evolutionary search with transformed and untransformed versions of two programs with nested target structures. In the first case, the evolutionary search finds test data in half the time for the transformed program compared to the original version. In the second case, the evolutionary search can only find test data with the transformed version of the program.", "num_citations": "48\n", "authors": ["23"]}
{"title": "Backward conditioning: a new program specialisation technique and its application to program comprehension\n", "abstract": " This paper introduces backward conditioning. Like forward conditioning (used in conditioned slicing), backward conditioning consists of specialising a program with respect to a condition inserted into the program. However, whereas forward conditioning deletes statements which are not executed when the initial state satisfies the condition, backward conditioning deletes statements which cannot cause execution to enter a state which satisfies the condition. The relationship between backward and forward conditioning is reminiscent of the relationship between backward and forward slicing. Forward conditioning addresses program comprehension questions of the form 'what happens if the program starts in a state satisfying condition c?', whereas backward conditioning addresses questions of the form 'what parts of the program could potentially lead to the program arriving in a state satisfying condition c?' The paper\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["23"]}
{"title": "An empirical study of amorphous slicing as a program comprehension support tool\n", "abstract": " Amorphous program slicing relaxes the syntactic constraint of traditional slicing and can therefore produce considerably smaller slices. This simplification power can be used to answer questions a software engineer might have about a program by first augmenting the program to make the question explicit and then slicing out an answer. One benefit of this technique is that the answer is in the form of a program and thus, in a language that the software engineer understands well. To test the usefulness of amorphous slicing in answering such questions, the question of array access safety is considered. A safety slice (an amorphous slice of an augmented program) is used to guide a software engineer to potential array bounds violations. A series of experiments was conducted to determine whether the safety slice was an effective aid to an engineer. 76 subjects participated in the controlled experiments. For\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "48\n", "authors": ["23"]}
{"title": "Deploying search based software engineering with Sapienz at Facebook\n", "abstract": " We describe the deployment of the Sapienz Search Based Software Engineering (SBSE) testing system. Sapienz has been deployed in production at Facebook since September 2017 to design test cases, localise and triage crashes to developers and to monitor their fixes. Since then, running in fully continuous integration within Facebook\u0393\u00c7\u00d6s production development process, Sapienz has been testing Facebook\u0393\u00c7\u00d6s Android app, which consists of millions of lines of code and is used daily by hundreds of millions of people around the globe.                 We continue to build on the Sapienz infrastructure, extending it to provide other software engineering services, applying it to other apps and platforms, and hope this will yield further industrial interest in and uptake of SBSE (and hybridisations of SBSE) as a result.", "num_citations": "47\n", "authors": ["23"]}
{"title": "No pot of gold at the end of program spectrum rainbow: Greatest risk evaluation formula does not exist\n", "abstract": " Spectrum Based Fault Localisation (SBFL) techniques rely on risk assessment formul\u251c\u00aa to convert program execution spectrum into risk evaluation values, which are in turn used to rank program statements according to their relative suspiciousness with respect to the observed failure. Recent work proved equivalence and hierarchy between different formul\u251c\u00aa, identifying a few groups of maximal formul\u251c\u00aa, ie, formul\u251c\u00aa that do not dominate each other. The holy grail in the field has been to come up with the greatest formula, that is, the one that dominates all known formul\u251c\u00aa. This paper proves that such a formula does not exist.", "num_citations": "47\n", "authors": ["23"]}
{"title": "Comparing the performance of metaheuristics for the analysis of multi-stakeholder tradeoffs in requirements optimisation\n", "abstract": " ContextIn requirements engineering, there will be many different stake holders. Often the requirements engineer has to find a set of requirements that reflect the needs of several different stake holders, while remaining within budget.ObjectiveThis paper introduces an optimisation-based approach to the automated analysis of requirements assignments when multiple stake holders are to be satisfied by a single choice of requirements.MethodThe paper reports on experiments using two different multi-objective evolutionary optimisation algorithms with real world data sets as well as synthetic data sets. This empirical validation includes a statistical analysis of the performance of the two algorithms.ResultsThe results reveal that the Two-Archive algorithm outperformed the others in convergence as the scale of problems increase. The paper also shows how both traditional and animated Kiviat diagrams can be used to\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["23"]}
{"title": "CONSIT: a fully automated conditioned program slicer\n", "abstract": " Conditioned slicing is a source code extraction technique. The extraction is performed with respect to a slicing criterion which contains a set of variables and conditions of interest. Conditioned slicing removes the parts of the original program which cannot affect the variables at the point of interest, when the conditions are satisfied. This produces a conditioned slice, which preserves the behaviour of the original with respect to the slicing criterion. Conditioned slicing has applications in source code comprehension, reuse, restructuring and testing. Unfortunately, implementation is not straightforward because the full exploitation of conditions requires the combination of symbolic execution, theorem proving and traditional static slicing. Hitherto, this difficultly has hindered development of fully automated conditioning slicing tools. This paper describes the first fully automated conditioned slicing system, CONSIT, detailing\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "47\n", "authors": ["23"]}
{"title": "Search-based amorphous slicing\n", "abstract": " Amorphous slicing is an automated source code extraction technique with applications in many areas of software engineering, including comprehension, reuse, testing and reverse engineering. Algorithms for syntax-preserving slicing are well established, but amorphous slicing is harder because it requires arbitrary transformation; finding good general purpose amorphous slicing algorithms therefore remains as hard as general program transformation. In this paper we show how amorphous slices can be computed using search techniques. The paper presents results from a set of experiments designed to explore the application of genetic algorithms, hill climbing, random search and systematic search to a set of six subject programs. As a benchmark, the results are compared to those from an existing analytical algorithm for amorphous slicing, which was written specifically to perform well with the sorts of program\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "46\n", "authors": ["23"]}
{"title": "Testability transformation\u0393\u00c7\u00f4program transformation to improve testability\n", "abstract": " Testability transformation is a new form of program transformation in which the goal is not to preserve the standard semantics of the program, but to preserve test sets that are adequate with respect to some chosen test adequacy criterion. The goal is to improve the testing process by transforming a program to one that is more amenable to testing while remaining within the same equivalence class of programs defined by the adequacy criterion. The approach to testing and the adequacy criterion are parameters to the overall approach. The transformations required are typically neither more abstract nor are they more concrete than standard \u0393\u00c7\u00a3meaning preserving transformations\u0393\u00c7\u00a5. This leads to interesting theoretical questions. but also has interesting practical implications. This chapter provides an introduction to testability transformation and a brief survey of existing results.", "num_citations": "44\n", "authors": ["23"]}
{"title": "Analysis and visualization of predicate dependence on formal parameters and global variables\n", "abstract": " Empirical data concerning the qualitative and quantitative nature of program dependence is presented for a set of 20 programs ranging from 600 lines of code to 167,000 lines of code. The sources of dependence considered are global variables and formal parameters and the targets considered are a program's predicate nodes. The results show that as the number of formal parameters available to a predicate increases, there is a decrease in the proportion of these formal parameters which are depended upon by the predicate. No such correlation was found for global variables. Results from theoretical and actual computation time analysis indicate that the computation of dependence information is practical, suggesting that the analysis may be beneficial to several application areas. The paper also presents results concerning correlations that provide strong evidence that the global and formal dependence sources\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "44\n", "authors": ["23"]}
{"title": "Genetically improved CUDA C++ software\n", "abstract": " Genetic Programming (GP) may dramatically increase the performance of software written by domain experts. GP and autotuning are used to optimise and refactor legacy GPGPU C\u252c\u00e1code for modern parallel graphics hardware and software. Speed ups of more than six times on recent nVidia GPU cards are reported compared to the original kernel on the same hardware.", "num_citations": "43\n", "authors": ["23"]}
{"title": "Exact scalable sensitivity analysis for the next release problem\n", "abstract": " The nature of the requirements analysis problem, based as it is on uncertain and often inaccurate estimates of costs and effort, makes sensitivity analysis important. Sensitivity analysis allows the decision maker to identify those requirements and budgets that are particularly sensitive to misestimation. However, finding scalable sensitivity analysis techniques is not easy because the underlying optimization problem is NP-hard. This article introduces an approach to sensitivity analysis based on exact optimization. We implemented this approach as a tool, OATSAC, which allowed us to experimentally evaluate the scalability and applicability of Requirements Sensitivity Analysis (RSA). Our results show that OATSAC scales sufficiently well for practical applications in Requirements Sensitivity Analysis. We also show how the sensitivity analysis can yield insights into difficult and otherwise obscure interactions between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "42\n", "authors": ["23"]}
{"title": "Transformed vargha-delaney effect size\n", "abstract": " Researchers without a technical background in statistics may be tempted to apply analytical techniques in a ritualistic manner. SBSE research is not immune to this problem. We argue that emerging rituals surrounding the use of the Vargha-Delaney effect size statistic may pose serious threats to the scientific validity of the findings. We believe investigations of effect size are important, but more care is required in the application of this statistic. In this paper, we illustrate the problems that can arise, and give guidelines for avoiding them, by applying a \u0393\u00c7\u00fftransformed\u0393\u00c7\u00d6 Vargha-Delaney effect size measurement. We argue that researchers should always consider which transformation is best suited to their problem domain before calculating the Vargha-Delaney statistic.", "num_citations": "41\n", "authors": ["23"]}
{"title": "Vada: A transformation-based system for variable dependence analysis\n", "abstract": " Variable dependence is an analysis problem in which the aim is to determine the set of input variables that can affect the values stored in a chosen set of intermediate program variables. This paper shows the relationship between the variable dependence analysis problem and slicing and describes VADA, a system that implements variable dependence analysis. In order to cover the full range of C constructs and features, a transformation to a core language is employed Thus, the full analysis is required only for the core language, which is relatively simple. This reduces the overall effort required for dependency analysis. The transformations used need preserve only the variable dependence relation, and therefore need not be meaning preserving in the traditional sense. The paper describes how this relaxed meaning further simplifies the transformation phase of the approach. Finally, the results of an empirical study\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "41\n", "authors": ["23"]}
{"title": "Robust next release problem: handling uncertainty during optimization\n", "abstract": " Uncertainty is inevitable in real world requirement engineering. It has a significant impact on the feasibility of proposed solutions and thus brings risks to the software release plan. This paper proposes a multi-objective optimization technique, augmented with Monte-Carlo Simulation, that optimizes requirement choices for the three objectives of cost, revenue, and uncertainty. The paper reports the results of an empirical study over four data sets derived from a single real world data set. The results show that the robust optimal solutions obtained by our approach are conservative compared to their corresponding optimal solutions produced by traditional Multi-Objective Next Release Problem. We obtain a robustness improvement of at least 18% at a small cost (a maximum 0.0285 shift in the 2D Pareto-front in the unit space). Surprisingly we found that, though a requirement's cost is correlated with inclusion on the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["23"]}
{"title": "Empirical study on the efficiency of search based test generation for EFSM models\n", "abstract": " Experimental work in software testing has generally focused on evaluating the effectiveness and efficiency on various source code programs. However, an important issue of testing efficiency on the model level has not been sufficiently addressed, and hitherto, no empirical studies exist. This paper presents an automated test data generation system for feasible transition paths (FTP) on Extended Finite State Machines (EFSM) models and investigates the statistical properties of testing efficiency using statistical tests for correlation and formalisation according to the test data generated by applying the system on four widely used EFSM models. An important and encouraging finding is a close positive correlation between test generation cost and the number of numerical equal operators in conditions (NNEOC) on a FTP. In addition, as the NNEOC increases, there is a raising correlation between the test generation cost\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "40\n", "authors": ["23"]}
{"title": "Are developers aware of the architectural impact of their changes?\n", "abstract": " Although considered one of the most important decisions in a software development lifecycle, empirical evidence on how developers perform and perceive architectural changes is still scarce. Given the large implications of architectural decisions, we do not know whether developers are aware of their changes' impact on the software's architecture, whether awareness leads to better changes, and whether automatically making developers aware would prevent degradation. Therefore, we use code review data of 4 open source systems to investigate the intent and awareness of developers when performing changes. We extracted 8,900 reviews for which the commits are available. 2,152 of the commits have changes in their computed architectural metrics, and 338 present significant changes to the architecture. We manually inspected all reviews for commits with significant changes and found that only in 38% of the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["23"]}
{"title": "Adaptive multi-objective evolutionary algorithms for overtime planning in software projects\n", "abstract": " Software engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. Recently, we introduced a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. This approach was empirically evaluated on six real world software projects and compared against state-of-the-art evolutionary approaches and currently used overtime strategies. The results showed that our proposal comfortably outperformed all the benchmarks considered. This paper extends our previous work by investigating adaptive multi-objective approaches to meta-heuristic operator selection, thereby extending and (as the results show) improving algorithmic performance. We also extended our empirical study to include two new real\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["23"]}
{"title": "GPGPU test suite minimisation: search based software engineering performance improvement using graphics cards\n", "abstract": " It has often been claimed that SBSE uses so-called \u0393\u00c7\u00ffembarrassingly parallel\u0393\u00c7\u00d6 algorithms that will imbue SBSE applications with easy routes to dramatic performance improvements. However, despite recent advances in multicore computation, this claim remains largely theoretical; there are few reports of performance improvements using multicore SBSE. This paper shows how inexpensive General Purpose computing on Graphical Processing Units (GPGPU) can be used to massively parallelise suitably adapted SBSE algorithms, thereby making progress towards cheap, easy and useful SBSE parallelism. The paper presents results for three different algorithms: NSGA2, SPEA2, and the Two Archive Evolutionary Algorithm, all three of which are adapted for multi-objective regression test selection and minimization. The results show that all three algorithms achieved performance improvements up to 25 times\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "39\n", "authors": ["23"]}
{"title": "Formalizing executable dynamic and forward slicing\n", "abstract": " This paper uses a projection theory of slicing to formalize the definition of executable dynamic and forward program slicing. Previous definitions, when given, have been operational, and previous descriptions have been algorithmic. The projection framework is used to provide a declarative formulation in terms of the different equivalences preserved by the different forms of slicing. The analysis of dynamic slicing reveals that the slicing criterion introduced by Korel and Laski contains three inter-woven criteria. It is shown how these three conceptually distinct criteria can be disentangled to reveal two new criteria. The analysis of dynamic slicing also reveals that the subsumes relationship between static and dynamic slicing is more intricate that previous authors have claimed. Finally, the paper uses the projection theory to investigate theoretical properties of forward slicing. This is achieved by first re-formulating forward\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["23"]}
{"title": "Slice-based measurement of coupling\n", "abstract": " Slice-Based Measurement of Coupling - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Slice-Based Measurement of Coupling Research output: Chapter in Book/Report/Conference proceeding \u0393\u00c7\u2551 Conference paper Mark Harman, Margaret Okunlawon, Bala Sivagurunathan and Overview Citation formats Original language English Title of host publication 19 th ICSE, Workshop on Process Modelling and Empirical Studies of Software Evolution Editors Rachel Harrison Published 1997 King's Authors Mark Harman (Informatics) Post to Twitter Post to FaceBook Post to Digg View graph of relations By the same authors Amorphous Slicing of Extended Finite State Machines Androutsopoulos, K., Clark, D., Harman, M., Hierons, RM, Li, Z. & Tratt.\u0393\u00c7\u00aa", "num_citations": "38\n", "authors": ["23"]}
{"title": "Data dependence based testability transformation in automated test generation\n", "abstract": " Source-code based test data generation is a process of finding program input on which a selected element, e.g., a target statement, is executed. There exist many test generation methods that automatically find a solution to the test generation problem. The existing methods work well for many programs. However, they may fail or are inefficient for programs with complex logic and intricate relationships between program elements. In this paper we present a testability transformation that transforms programs so that the chances of finding a solution are increased when the existing methods fail using only the original program. In our approach data dependence analysis is used to identify statements in the program that affect computation of the fitness function associated with the target statement. The transformed program contains only these statements, and it is used to explore different ways the fitness may be computed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "37\n", "authors": ["23"]}
{"title": "Flagremover: A testability transformation for transforming loop assigned flags\n", "abstract": " Search-Based Testing is a widely studied technique for automatically generating test inputs, with the aim of reducing the cost of software engineering activities that rely upon testing. However, search-based approaches degenerate to random testing in the presence of flag variables, because flags create spikes and plateaux in the fitness landscape. Both these features are known to denote hard optimization problems for all search-based optimization techniques. Several authors have studied flag removal transformations and fitness function refinements to address the issue of flags, but the problem of loop-assigned flags remains unsolved. This article introduces a testability transformation along with a tool that transforms programs with loop-assigned flags into flag-free equivalents, so that existing search-based test data generation approaches can successfully be applied. The article presents the results of an empirical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "36\n", "authors": ["23"]}
{"title": "Side-effect removal transformation\n", "abstract": " A side-effect is any change in program state that occurs as a by-product of the evaluation of an expression. Side-effects are often thought to impede program comprehension and to lead to complex, poorly understood and occasionally undefined semantics. Side-effect removal transformation (SERT) improves comprehension by rewriting a program p, which may contain side-effects, into a semantically equivalent program p', which is guaranteed to be side-effect free. This paper introduces the SERT approach to the side-effect problem, briefly reporting initial experience with an implementation of SERT for C programs, called Linsert.", "num_citations": "36\n", "authors": ["23"]}
{"title": "Augmenting test suites effectiveness by increasing output diversity\n", "abstract": " The uniqueness (or otherwise) of test outputs ought to have a bearing on test effectiveness, yet it has not previously been studied. In this paper we introduce a novel test suite adequacy criterion based on output uniqueness. We propose 4 definitions of output uniqueness with varying degrees of strictness. We present a preliminary evaluation for web application testing that confirms that output uniqueness enhances fault-finding effectiveness. The approach outperforms random augmentation in fault finding ability by an overall average of 280% in 5 medium sized, real world web applications.", "num_citations": "35\n", "authors": ["23"]}
{"title": "Automated patching techniques: the fix is in: technical perspective\n", "abstract": " Automated patching techniques Page 1 108 communications of the acm | may 2010 | vol. 53 | no. 5 oVer The PasT 40 years, much effort has been devoted to testing software to find bugs. Testing is difficult because the underlying problem involves undecidable questions such as statement reachability. However, testing cannot be ignored. The National Institute of Standards and Techniques estimated the cost of software failure to the US economy at $60 billion, indicating that improved software testing could reduce this by at least one-third.If finding bugs is technically demanding and yet economically vital, how much more difficult yet valuable would it be to automatically fix bugs? This question is answered precisely by the work reported in the following paper by Weimer, Forrest, Le Goues, and Nguyen. The authors use evolutionary computation to evolve patches that fix bugs. Their work is the first to show how Genetic fix \u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["23"]}
{"title": "Testing conformance to a quasi-non-deterministic stream X-machine\n", "abstract": " Stream X-machines have been used in order to specify a range of systems. One of the strengths of this approach is that, under certain well-defined conditions, it is possible to produce a finite test that is guaranteed to determine the correctness of the implementation under test (IUT). Initially only deterministic stream X-machines were considered in the literature. This is largely because the standard test algorithm relies on the stream X-machine being deterministic.               More recently the problem of testing to determine whether the IUT is equivalent to a non-deterministic stream X-machine specification has been tackled. Since non-determinism can be important for specifications, this is an extremely useful extension. In many cases, however, we wish to test for a weaker notion of correctness called conformance. This paper considers a particular form of non-determinism, within stream X-machines, that will be\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "34\n", "authors": ["23"]}
{"title": "Human competitiveness of genetic programming in spectrum-based fault localisation: Theoretical and empirical analysis\n", "abstract": " We report on the application of Genetic Programming to Software Fault Localisation, a problem in the area of Search-Based Software Engineering (SBSE). We give both empirical and theoretical evidence for the human competitiveness of the evolved fault localisation formul\u251c\u00aa under the single fault scenario, compared to those generated by human ingenuity and reported in many papers, published over more than a decade. Though there have been previous human competitive results claimed for SBSE problems, this is the first time that evolved solutions have been formally proved to be human competitive. We further prove that no future human investigation could outperform the evolved solutions. We complement these proofs with an empirical analysis of both human and evolved solutions, which indicates that the evolved solutions are not only theoretically human competitive, but also convey similar practical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "33\n", "authors": ["23"]}
{"title": "Unifying program slicing and concept assignment for higher\u0393\u00c7\u00c9level executable source code extraction\n", "abstract": " Program slicing and concept assignment have both been proposed as source code extraction techniques. Unfortunately, each has a weakness that prevents wider application. For slicing, the extraction criterion is expressed at a very low level; constructing a slicing criterion requires detailed code knowledge which is often unavailable. The concept assignment extraction criterion is expressed at the domain level. However, unlike a slice, the extracted code is not executable as a separate subprogram in its own right. This paper introduces a unification of slicing and concept assignment which exploits their combined advantages, while overcoming these two individual weaknesses. Our \u0393\u00c7\u00ffconcept slices\u0393\u00c7\u00d6 are executable programs extracted using high\u0393\u00c7\u00c9level criteria. The paper introduces four techniques that combine slicing and concept assignment and algorithms for each. These algorithms were implemented in two\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "32\n", "authors": ["23"]}
{"title": "Generating feasible input sequences for extended finite state machines (EFSMs) using genetic algorithms\n", "abstract": " Testing is an important part of the software engineering process but can be time consuming, error-prone and expensive. Test automation can help reduce these problems. Many state based systems, like protocols, have been modelled as finite state machines (FSMs) and extended finite state machines (EFSMs). They have been an effective method of modelling because a variety of techniques and automated tools exist that work with them. To ensure the reliability of these systems once implemented they must be tested for conformance to their specification. Usually the implementation of a system specified by an FSM or EFSM is tested for conformance by applying a sequence of inputs and verifying that the corresponding sequence of outputs is that which is expected. This commonly involves executing a number of transition paths, until all transitions have been tested at least once. In EFSMs the feasibility of a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "31\n", "authors": ["23"]}
{"title": "The importance of accounting for real-world labelling when predicting software vulnerabilities\n", "abstract": " Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis-lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["23"]}
{"title": "Specialising software for different downstream applications using genetic improvement and code transplantation\n", "abstract": " Genetic improvement uses automated search to find improved versions of existing software. Genetic improvement has previously been concerned with improving a system with respect to all possible usage scenarios. In this paper, we show how genetic improvement can also be used to achieve specialisation to a specific set of usage scenarios. We use genetic improvement to evolve faster versions of a C++ program, a Boolean satisfiability solver called MiniSAT, specialising it for three different applications, each with their own characteristics. Our specialised solvers achieve between 4 and 36 percent execution time improvement, which is commensurate with efficiency gains achievable using human expert optimisation for the general solver. We also use genetic improvement to evolve faster versions of an image processing tool called ImageMagick, utilising code from GraphicsMagick, another image processing tool\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["23"]}
{"title": "The value of exact analysis in requirements selection\n", "abstract": " Uncertainty is characterised by incomplete understanding. It is inevitable in the early phase of requirements engineering, and can lead to unsound requirement decisions. Inappropriate requirement choices may result in products that fail to satisfy stakeholders' needs, and might cause loss of revenue. To overcome uncertainty, requirements engineering decision support needs uncertainty management. In this research, we develop a decision support framework METRO for the Next Release Problem (NRP) to manage algorithmic uncertainty and requirements uncertainty. An exact NRP solver (NSGDP) lies at the heart of METRO. NSGDP's exactness eliminates interference caused by approximate existing NRP solvers. We apply NSGDP to three NRP instances, derived from a real world NRP instance, RALIC, and compare with NSGA-II, a widely-used approximate (inexact) technique. We find the randomness of NSGA\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["23"]}
{"title": "Search--based approaches to the component selection and prioritization problem\n", "abstract": " This poster paper addresses the problem of choosing sets of software components to combine in component--based software engineering. It formulates both ranking and selection problems as feature subset selection problems to which search based software engineering can be applied. We will consider selection and ranking of elements from a set of software components from the component base of a large telecommunications organisation.", "num_citations": "30\n", "authors": ["23"]}
{"title": "Search based software engineering\n", "abstract": " This paper was written to accompany the author\u0393\u00c7\u00d6s keynote talk for the Workshop on Computational Science in Software Engineering held in conjunction with International Conference in Computational Science 2006 in Reading, UK. The paper explains how software engineering activities can be viewed as a search for solutions that balance many competing constraints to achieve an optimal or near optimal result.               The aim of Search Based Software Engineering (SBSE) research is to move software engineering problems from human-based search to machine-based search, using a variety of techniques from the metaheuristic search, operations research and evolutionary computation paradigms. As a result, human effort moves up the abstraction chain to focus on guiding the automated search, rather than performing it. The paper briefly describes the search based approach, providing pointers to the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "30\n", "authors": ["23"]}
{"title": "An empirical study of cohesion and coupling: Balancing optimization and disruption\n", "abstract": " Search-based software engineering has been extensively applied to the problem of finding improved modular structures that maximize cohesion and minimize coupling. However, there has, hitherto, been no longitudinal study of developers' implementations, over a series of sequential releases. Moreover, results validating whether developers respect the fitness functions are scarce, and the potentially disruptive effect of search-based remodularization is usually overlooked. We present an empirical study of 233 sequential releases of ten different systems; the largest empirical study reported in the literature so far, and the first longitudinal study. Our results provide evidence that developers do, indeed, respect the fitness functions used to optimize cohesion/coupling (they are statistically significantly better than arbitrary choices with p \u0393\u00eb\u00ac 0.01), yet they also leave considerable room for further improvement (cohesion\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["23"]}
{"title": "A unifying theory of control dependence and its application to arbitrary program structures\n", "abstract": " There are several similar, but not identical, definitions of control dependence in the literature. These definitions are given in terms of control flow graphs which have had extra restrictions imposed (for example, end-reachability).We define two new generalisations of non-termination insensitive and non-termination sensitive control dependence called weak and strong control-closure. These are defined for all finite directed graphs, not just control flow graphs and are hence allow control dependence to be applied to a wider class of program structures than before.We investigate all previous forms of control dependence in the literature and prove that, for the restricted graphs for which each is defined, vertex sets are closed under each if and only if they are either weakly or strongly control-closed. Low polynomial-time algorithms for producing minimal weakly and strongly control-closed sets over generalised control flow\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["23"]}
{"title": "A trajectory-based strict semantics for program slicing\n", "abstract": " We define a program semantics that is preserved by dependence-based slicing algorithms. It is a natural extension, to non-terminating programs, of the semantics introduced by Weiser (which only considered terminating ones) and, as such, is an accurate characterisation of the semantic relationship between a program and the slice produced by these algorithms.Unlike other approaches, apart from Weiser\u0393\u00c7\u00d6s original one, it is based on strict standard semantics which models the \u0393\u00c7\u00ffnormal\u0393\u00c7\u00d6 execution of programs on a von Neumann machine and, thus, has the advantage of being intuitive. This is essential since one of the main applications of slicing is program comprehension. Although our semantics handles non-termination, it is defined wholly in terms of finite trajectories, without having to resort to complex, counter-intuitive, non-standard models of computation. As well as being simpler, unlike other approaches to this\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "29\n", "authors": ["23"]}
{"title": "Allowing overlapping boundaries in source code using a search based approach to concept binding\n", "abstract": " One approach to supporting program comprehension involves binding concepts to source code. Previously proposed approaches to concept binding have enforced non-overlapping boundaries. However, real-world programs may contain overlapping concepts. This paper presents techniques to allow boundary overlap in the binding of concepts to source code. In order to allow boundaries to overlap, the concept binding problem is reformulated as a search problem. It is shown that the search space of overlapping concept bindings is exponentially large, indicating the suitability of sampling-based search algorithms. Hill climbing and genetic algorithms are introduced for sampling the space. The paper reports on experiments that apply these algorithms to 21 COBOL II programs taken from the commercial financial services sector. The results show that the genetic algorithm produces significantly better solutions than\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "28\n", "authors": ["23"]}
{"title": "Memory mutation testing\n", "abstract": " ContextThree decades of mutation testing development have given software testers a rich set of mutation operators, yet relatively few operators can target memory faults (as we demonstrate in this paper).ObjectiveTo address this shortcoming, we introduce Memory Mutation Testing, proposing 9 Memory Mutation Operators each of which targets common forms of memory fault. We compare Memory Mutation Operators with traditional Mutation Operators, while handling equivalent and duplicate mutants.MethodWe extend our previous workshop paper, which introduced Memory Mutation Testing, with a more extensive and precise analysis of 18 open source programs, including 2 large real-world programs, all of which come with well-designed unit test suites. Specifically, our empirical study makes use of recent results on Trivial Compiler Equivalence (TCE) to identify both equivalent and duplicate mutants. Though the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["23"]}
{"title": "Refactoring as testability transformation\n", "abstract": " This paper briefly reviews the theory of Testability Transformation and outlines its implications for and relationship to refactoring for testing. The paper introduces testability refactorings, a subclass of Testability Transformations and discusses possible examples of testability refactorings. Several approaches to testability refactoring are also introduced. These include the novel concept of test-carrying code and the use of pareto optimization for balancing the competing needs of machine and human in search based testability refactoring.", "num_citations": "27\n", "authors": ["23"]}
{"title": "Analysis of dynamic memory access using amorphous slicing\n", "abstract": " Problems associated with understanding, verifying and re-engineering the way in which a system allocates and releases dynamic memory present significant challenges to the software maintainer. Because the questions underlying these problems are undecidable, no system can provide a completely fail safe certification. For example, in checking for memory leaks, a system can only warn of potential problems, but cannot guarantee that no leaks remain. We present an approach to modelling the dynamic memory access properties of a program using amorphous program slicing to create a Dynamic Memory Model (DMM). The slices are constructed from a transformed version of the original program in which heap access has been made explicit using a pseudo variable to denote the top of the heap. The DMM is a simplified version of the original program which is concerned solely with the dynamic memory access\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "27\n", "authors": ["23"]}
{"title": "Getting results from search-based approaches to software engineering\n", "abstract": " Like other engineering disciplines, software engineering is typically concerned with near optimal solutions or those which fall within a specified applicable tolerance. More recently, search-based techniques have started to find application in software engineering problem domains. This area of search-based software engineering has its origins in work on search-based testing, which began in the mid 1990s. Already, search-based solutions have been applied to software engineering problems right through the development life cycle.", "num_citations": "26\n", "authors": ["23"]}
{"title": "A denotational interprocedural program slicer\n", "abstract": " This paper extends a previously developed intraprocedural denotational program slicer to handle procedures. Using the denotational approach, slices can be defined in terms of the abstract syntax of the object language without the need of a control flow graph or similar intermediate structure. The algorithm presented here is capable of correctly handling the interplay between function and procedure calls, side-effects, and short-circuit expression evaluation. The ability to deal with these features is required in reverse engineering of legacy systems, where code often contains side-effects.", "num_citations": "26\n", "authors": ["23"]}
{"title": "< i> ConSUS</i>: a light-weight program conditioner\n", "abstract": " Program conditioning consists of identifying and removing a set of statements which cannot be executed when a condition of interest holds at some point in a program. It has been applied to problems in maintenance, testing, re-use and re-engineering. All current approaches to program conditioning rely upon both symbolic execution and reasoning about symbolic predicates. The reasoning can be performed by a \u0393\u00c7\u00ffheavy duty\u0393\u00c7\u00d6 theorem prover but this may impose unrealistic performance constraints.This paper reports on a lightweight approach to theorem proving using the FermaT Simplify decision procedure. This is used as a component to ConSUS, a program conditioning system for the Wide Spectrum Language WSL. The paper describes the symbolic execution algorithm used by ConSUS, which prunes as it conditions.The paper also provides empirical evidence that conditioning produces a significant reduction\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["23"]}
{"title": "Syntax-directed amorphous slicing\n", "abstract": " An amorphous slice of a program is constructed with respect to a set of variables. The amorphous slice is an executable program which preserves the behaviour of the original on the variables of interest. Unlike syntax-preserving slices, amorphous slices need not preserve a projection of the syntax of a program. This makes the task of amorphous slice construction harder, but it also often makes the result thinner and thereby preferable in applications where syntax preservation is unimportant.               This paper describes an approach to the construction of amorphous slices which is based on the Abstract Syntax Tree of the program to be sliced, and does not require the construction of control flow graphs nor of program dependence graphs. The approach has some strengths and weaknesses which the paper discusses.               The amorphous slicer, is part of the GUSTT slicing system, which includes syntax\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "25\n", "authors": ["23"]}
{"title": "Program comprehension assisted by slicing and transformation\n", "abstract": " Program slicing is a technique for program simpli cation based upon the deletion of statements which cannot a ect the values of a chosen set of variables. Because slicing extracts a subcomponent of the program concerned with some speci c computation on a set of variables, it can be used to assist program comprehension, allowing a programmer to remodularise a program according to arbitrarily selected slicing criteria.In this paper it is shown that the simpli cation power of slicing can be improved if the syntactic restriction to statement deletion is removed, allowing slices to be constructed using any simplifying transformation which preserves the e ect of the original program upon the set of variables of interest. It is also shown that quasi static slicing, rst proposed by Venkatesh (and de ned here in a slightly more general form), is the most suitable slicing paradigm for program comprehension. The various forms of slice are formally de ned, an algorithm, based upon transformation, symbolic execution and conventional slicing is introduced for computing syntactically unrestricted, quasi static slices. A worked example is used to show how this approach supports program comprehension by case analysis and simpli cation.", "num_citations": "25\n", "authors": ["23"]}
{"title": "An empirical comparison of combinatorial testing, random testing and adaptive random testing\n", "abstract": " We present an empirical comparison of three test generation techniques, namely, Combinatorial Testing (CT), Random Testing (RT) and Adaptive Random Testing (ART), under different test scenarios. This is the first study in the literature to account for the (more realistic) testing setting in which the tester may not have complete information about the parameters and constraints that pertain to the system, and to account for the challenge posed by faults (in terms of failure rate). Our study was conducted on nine real-world programs under a total of 1683 test scenarios (combinations of available parameter and constraint information and failure rate). The results show significant differences in the techniques' fault detection ability when faults are hard to detect (failure rates are relatively low). CT performs best overall; no worse than any other in 98 percent of scenarios studied. ART enhances RT, and is comparable to CT in\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["23"]}
{"title": "ORBS and the limits of static slicing\n", "abstract": " Observation-based slicing is a recently-introduced, language-independent slicing technique based on the dependencies observable from program behaviour. Due to the well-known limits of dynamic analysis, we may only compute an under-approximation of the true observation-based slice. However, because the observation-based slice captures all possible dependence that can be observed, even such approximations can yield insight into the limitations of static slicing. For example, a static slice, S, that is strictly smaller than the corresponding observation based slice is potentially unsafe. We present the results of three sets of experiments on 12 different programs, including benchmarks and larger programs, which investigate the relationship between static and observation-based slicing. We show that, in extreme cases, observation-based slices can find the true minimal static slice, where static techniques cannot\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["23"]}
{"title": "Coherent clusters in source code\n", "abstract": " This paper presents the results of a large scale empirical study of coherent dependence clusters. All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have \u0393\u00c7\u00ffcoherent\u0393\u00c7\u00d6 shared backward and forward dependence. We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%. Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program. Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters. A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["23"]}
{"title": "Finding the optimal balance between over and under approximation of models inferred from execution logs\n", "abstract": " Models inferred from execution traces (logs) may admit more behaviours than those possible in the real system (over-approximation) or may exclude behaviours that can indeed occur in the real system (under-approximation). Both problems negatively affect model based testing. In fact, over-approximation results in infeasible test cases, i.e., test cases that cannot be activated by any input data. Under-approximation results in missing test cases, i.e., system behaviours that are not represented in the model are also never tested. In this paper we balance over- and under-approximation of inferred models by resorting to multi-objective optimization achieved by means of two search-based algorithms: A multi-objective Genetic Algorithm (GA) and the NSGA-II. We report the results on two open-source web applications and compare the multi-objective optimization to the state-of-the-art KLFA tool. We show that it is possible\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "24\n", "authors": ["23"]}
{"title": "Cohesion metrics\n", "abstract": " We consider ways of measuring the cohesion of program fragments based upon techniques for program slicing, following the work of Ott et al [12, 2, 13, 11, 10]. The approach is based on the idea that the intersection of a program's slices represents that part of the fragment which is cohesive. We produce cohesion metrics that are structurally identical to those of Ott and Thuss [13], the difference is that we consider different ways of measuring the significance of the intersection of a program's slices. We introduce expression metrics, to calculate the significance of the code in the intersection of slices, arguing that this approach may provide better answers than theLines of Code'approach implicit in much of the literature [12, 10, 13]. The substitution of theLines of Code'metric with alternative metrics, within the metrics defined by Ott and Thuss, motivates the consideration of metrics as higher--order functions, expressed in a pure functional programming language....", "num_citations": "24\n", "authors": ["23"]}
{"title": "An empirical study on dependence clusters for effort-aware fault-proneness prediction\n", "abstract": " A dependence cluster is a set of mutually inter-dependent program elements. Prior studies have found that large dependence clusters are prevalent in software systems. It has been suggested that dependence clusters have potentially harmful effects on software quality. However, little empirical evidence has been provided to support this claim. The study presented in this paper investigates the relationship between dependence clusters and software quality at the function-level with a focus on effort-aware fault-proneness prediction. The investigation first analyzes whether or not larger dependence clusters tend to be more fault-prone. Second, it investigates whether the proportion of faulty functions inside dependence clusters is significantly different from the proportion of faulty functions outside dependence clusters. Third, it examines whether or not functions inside dependence clusters playing a more important role\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["23"]}
{"title": "Automated transplantation of call graph and layout features into Kate\n", "abstract": " We report the automated transplantation of two features currently missing from Kate: call graph generation and automatic layout for C programs, which have been requested by users on the Kate development forum. Our approach uses a lightweight annotation system with Search Based techniques augmented by static analysis for automated transplantation. The results are promising: on average, our tool requires 101\u252c\u00e1min of standard desktop machine time to transplant the call graph feature, and 31\u252c\u00e1min to transplant the layout feature. We repeated each experiment 20 times and validated the resulting transplants using unit, regression and acceptance test suites. In 34 of 40 experiments conducted our search-based autotransplantation tool,      Scalpel, was able to successfully transplant the new functionality, passing all tests.", "num_citations": "23\n", "authors": ["23"]}
{"title": "App store mining and analysis\n", "abstract": " App stores are not merely disrupting traditional software deployment practice, but also offer considerable potential benefit to scientific research. Software engineering researchers have never had available, a more rich, wide and varied source of information about software products. There is some source code availability, supporting scientific investigation as it does with more traditional open source systems. However, what is important and different about app stores, is the other data available. Researchers can access user perceptions, expressed in rating and review data. Information is also available on app popularity (typically expressed as the number or rank of downloads). For more traditional applications, this data would simply be too commercially sensitive for public release. Pricing information is also partially available, though at the time of writing, this is sadly submerging beneath a more opaque layer of in-app\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["23"]}
{"title": "State aware test case regeneration for improving web application test suite coverage and fault detection\n", "abstract": " This paper introduces two test cases regeneration approaches for web applications, one uses standard Def-Use testing but for state variables, the other uses a novel value-aware dataflow approach. Our overall approach is to combine requests from a test suite to form client-side request sequences, based on dataflow analysis of server-side session variables and database tables. We implemented our approach as a tool SART (State Aware Regeneration Tool) and used it to evaluate our proposed approaches on 4 real world web applications. Our results show that for all 4 applications, both server-side coverage and fault detection were statistically significantly improved. Even on relatively high quality test suites our algorithms improve average coverage by 14.74% and fault detection by 9.19%.", "num_citations": "23\n", "authors": ["23"]}
{"title": "Theory and algorithms for slicing unstructured programs\n", "abstract": " Program slicing identifies parts of a program that potentially affect a chosen computation. It has many applications in software engineering, including maintenance, evolution and re-engineering of legacy systems. However, these systems typically contain programs with unstructured control-flow, produced using goto statements; thus, effective slicing of unstructured programs remains an important topic of study.This paper shows that slicing unstructured programs inherently requires making trade-offs between three slice attributes: termination behaviour, size, and syntactic structure. It is shown how different applications of slicing require different tradeoffs. The three attributes are used as the basis of a three-dimensional theoretical framework, which classifies slicing algorithms for unstructured programs. The paper proves that for two combinations of these dimensions, no algorithm exists and presents algorithms for the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["23"]}
{"title": "Loop squashing transformations for amorphous slicing\n", "abstract": " Program slicing is a source code extraction technique that can be used to support reverse engineering by automatically extracting executable subprograms that preserve some aspect of the original program's semantics. Although minimal slices are not generally computable, safe approximate algorithms can be used to good effect. However, the precision of such slicing algorithms is a major factor in determining the value of slicing for reverse engineering. Amorphous slicing has been proposed as a way of reducing the size of a slice. Amorphous slices preserve the aspect of semantic interest, but not the syntax that denotes it, making them generally smaller than their syntactically restricted counterparts. Amorphous slicing is suitable for many reverse engineering applications, since reverse engineering typically abandons the existing syntax to facilitate structural improvements. Previous work on amorphous slicing has\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "23\n", "authors": ["23"]}
{"title": "Crawlability metrics for automated web testing\n", "abstract": " Web applications are exposed to frequent changes both in requirements and involved technologies. At the same time, there is a continuously growing demand for quality and trust and such a fast evolution and quality constraints claim for mechanisms and techniques for automated testing. Web application automated testing often involves random crawlers to navigate the application under test and automatically explore its structure. However, owing to the specific challenges of the modern Web systems, automatic crawlers may leave large portions of the application unexplored. In this paper, we propose the use of structural metrics to predict whether an automatic crawler with given crawling capabilities will be sufficient or not to achieve high coverage of the application under test. In this work, we define a taxonomy of such capabilities and we determine which combination of them is expected to give the highest\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "22\n", "authors": ["23"]}
{"title": "The SEMINAL workshop: reformulating software engineering as a metaheuristic search problem\n", "abstract": " This paper reports on the first international Workshop on Software Engineering using Metaheuristic INnovative ALgorithms.The aim of the workshop was to bring together researchers in search-based metaheuristic techniques with researchers and practitioners in Software Engineering. The workshop sought to support and develop the embryonic community which straddles these two communities and which is working on the application of metaheuristic search-based techniques to problems in Software Engineering.The paper outlines the nature of the nascent field of Search-Based Software Engineering, and briefly outlines the papers presented at the workshop and the discussions which took place.", "num_citations": "22\n", "authors": ["23"]}
{"title": "Approximate oracles and synergy in software energy search spaces\n", "abstract": " Reducing the energy consumption of software systems through optimisation techniques such as genetic improvement is gaining interest. However, efficient and effective improvement of software systems requires a better understanding of the code-change search space. One important choice practitioners have is whether to preserve the system's original output or permit approximation, with each scenario having its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by the modification operators is 2.69 percent (0.76 percent on average). By contrast, this figure increases dramatically to 95.60 percent (33.90 percent on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for code optimisation. We investigate synergy, a phenomenon that occurs when simultaneously\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["23"]}
{"title": "Results from a large-scale study of performance optimization techniques for source code analyses based on graph reachability algorithms\n", "abstract": " Internally, many source code analysis tools make use of graphs. For example, one of the oldest and most widely used internal graphs is the control-flow graph developed for use within a compiler. Work on compilation has also led to the development of the call graph, the procedure dependence graph (PDG), and the static-single assignment (SSA) graph. Compilers are not the only source-code analysis tools to use internal graphs. A variety of software engineering tools incorporate a variety of different graphs. A study of techniques that improve graph-based program analysis is presented. Several different techniques are considered, including forming strongly-connected components, topological sorting, and removing transitive edges. Graph reachability, a pervasive graph analysis operation, is used as a representative graph analysis operation in the study. Data collected from a test bed of just over 1000000 lines of\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "21\n", "authors": ["23"]}
{"title": "Slicing, I/O and the implicit state\n", "abstract": " Program slicing consists of deleting statements from a program, creating a reduced program, a slice, that preserves the original program's behaviour for a given set of variables at a chosen point in the program. However, some aspects of a program's semantics are not captured by a set of variables, rendering slicing inapplicable to their analysis. These aspects of the program's state shall, collectively, be termed theimplicit state'. For example, the input list supplied to a program is not denoted by a variable, rather it is part of the implicit state. It will be shown that this implicitness causes existing slicing algorithms to produce incorrect slices with respect to input. In order to solve the problem the program to be sliced will be transformed into anexplicit'version (in which all aspects of its semantics are captured by variables). The approach is also applied to a wider class of problems in which slicing is inhibited by the lack of variables upon which to form a suitable slicing criterion. Because the approach can be expressed as a source {level transformation, it has the attractive property that the slicing algorithm need not be altered.", "num_citations": "21\n", "authors": ["23"]}
{"title": "The next 700 slicing criteria\n", "abstract": " A slice is constructed by deleting statements from a program whilst preserving some projection of its semantics. Since Mark Weiser introduced program slicing in 1979, a wide variety of slicing paradigms have been proposed, each of which is based upon a new formulation of the slicing criterion, capturing the semantic projection to be preserved during the process of command deletion. This paper surveys these slicing criteria, attempting to establish a set of parameters which combine to form a slicing criterion. The effort to abstract a general set of parameters for slicing criteria highlights the existence of many new possibilities for slicing, corresponding to, as yet unpublished, criteria. Many of these novel slicing criteria may find applications in program comprehension and analysis. The paper introduces no new algorithms for constructing slices, rather it introduces new criteria with respect to which slices might usefully be constructed. The paper also goes some way towards a unification of previous, apparently different, but related, approaches to slicing in which the process of command deletion remains invariant while the semantic projections preserved during the command deletion process vary.", "num_citations": "20\n", "authors": ["23"]}
{"title": "Agent-based modelling of stock markets using existing order book data\n", "abstract": " We propose a new method for creating alternative scenarios for the evolution of a financial time series over short time periods. Using real order book data from the Chi-X exchange, along with a number of agents to interact with that data, we create a semi-synthetic time series of stock prices. We investigate the impact of using both simple, limited intelligence traders, along with a more realistic set of traders. We also test two different hypotheses about how real participants in the market would modify their orders in the alternative scenario created by the model. We run our experiments on 3 different stocks, evaluating a number of financial metrics for intra- and inter-day variability. Our results using realistic traders and relative pricing of real orders were found to outperform other approaches.", "num_citations": "19\n", "authors": ["23"]}
{"title": "An experimental search-based approach to cohesion metric evaluation\n", "abstract": " In spite of several decades of software metrics research and practice, there is little understanding of how software metrics relate to one another, nor is there any established methodology for comparing them. We propose a novel experimental technique, based on search-based refactoring, to \u0393\u00c7\u00ffanimate\u0393\u00c7\u00d6 metrics and observe their behaviour in a practical setting. Our aim is to promote metrics to the level of active, opinionated objects that can be compared experimentally to uncover where they conflict, and to understand better the underlying cause of the conflict. Our experimental approaches include semi-random refactoring, refactoring for increased metric agreement/disagreement, refactoring to increase/decrease the gap between a pair of metrics, and targeted hypothesis testing. We apply our approach to five popular cohesion metrics using ten real-world Java systems, involving 330,000 lines of code and the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["23"]}
{"title": "Grow and serve: Growing Django citation services using SBSE\n", "abstract": " We introduce a \u0393\u00c7\u00ffgrow and serve\u0393\u00c7\u00d6 approach to Genetic Improvement (GI) that grows new functionality as a web service running on the Django platform. Using our approach, we successfully grew and released a citation web service. This web service can be invoked by existing applications to introduce a new citation counting feature. We demonstrate that GI can grow genuinely useful code in this way, so we deployed the SBSE-grown web service into widely-used publications repositories, such as the GP bibliography. In the first 24 hours of deployment alone, the service was used to provide GP bibliography citation data 369 times from 29 countries.", "num_citations": "18\n", "authors": ["23"]}
{"title": "Future internet testing with fittest\n", "abstract": " The complexity of the technologies involved in the Future Internet makes testing extremely challenging and demands for novel approaches and major advancement in the field. The overall aim of the FITTEST project is to address these testing challenges, by developing an integrated environment for automated testing, which can monitor the Future Internet application under test and adapt to the dynamic changes observed. Future Internet applications do not remain fixed after their release, services and components can be dynamically added by customers. Consequently, FITTEST testing will be continuous and post-release such that maintenance and quality assurance can cope with the changes in the intended use of an application after release. The testing environment will integrate, adapt and automate various techniques for continuous Future Internet testing (dynamic model inference, model-based testing, log\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["23"]}
{"title": "Heuristics for fault diagnosis when testing from finite state machines\n", "abstract": " When testing from finite state machines, a failure observed in the implementation under test (IUT) is called a symptom. A symptom could have been caused by an earlier state transfer failure. Transitions that may be used to explain the observed symptoms are called diagnosing candidates. Finding strategies to generate an optimal set of diagnosing candidates that could effectively identify faults in the IUT is of great value in reducing the cost of system development and testing. This paper investigates fault diagnosis when testing from finite state machines and proposes heuristics for fault isolation and identification. The proposed heuristics attempt to lead to a symptom being observed in some shorter test sequences, which helps to reduce the cost of fault isolation and identification. The complexity of the proposed method is analysed. A case study is presented, which shows how the proposed approach assists in fault\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "18\n", "authors": ["23"]}
{"title": "GUSTT: An amorphous slicing system which combines slicing and transformation\n", "abstract": " The paper presents a system for amorphous program slicing which combines slicing and transformation to achieve thinner slices than are possible using conventional syntax-preserving slicing. The approach involves the validation of the transformation and slicing steps using the Coq proof assistant, thereby guaranteeing the correctness of the amorphous slices produced. The combined application of slicing and transformation is illustrated with a simple case study. Several components of the system implement transformation tactics, such as side-effect removal and dependence reduction transformations which have wider applications than the construction of amorphous slices.", "num_citations": "18\n", "authors": ["23"]}
{"title": "Slicing programs in the presence of errors\n", "abstract": " Program slicing is a technique by which statements are deleted from a program in such a way as to preserve a projection of the original program's semantics. It is shown that slicing algorithms based upon traditional defined and referenced variable sets do not preserve a projection of strict semantics with respect to computations which cause errors. Rather, these approaches preserve a projection of the program's semantics which is lazy with respect to errors. A modified version of defined and referenced variable sets is introduced, which provides the freedom to choose the form of semantics to be preserved.", "num_citations": "18\n", "authors": ["23"]}
{"title": "Automatic testing and improvement of machine translation\n", "abstract": " This paper presents TransRepair, a fully automatic approach for testing and repairing the consistency of machine translation systems. TransRepair combines mutation with metamorphic testing to detect inconsistency bugs (without access to human oracles). It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Our evaluation on two state-of-the-art translators, Google Translate and Transformer, indicates that TransRepair has a high precision (99%) on generating input pairs with consistent translations. With these tests, using automatic consistency metrics and manual assessment, we find that Google Translate and Transformer have approximately 36% and 40% inconsistency bugs. Black-box repair fixes 28% and 19% bugs on average for Google Translate and Transformer. Grey-box repair fixes 30% bugs on average\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["23"]}
{"title": "Stop-list slicing\n", "abstract": " Traditional program slicing requires two parameters: a program location and a variable, or perhaps a set of variables, of interest. Stop-list slicing adds a third parameter to the slicing criterion: those variables that are not of interest. This third parameter is called the stoplist. When a variable in the stop-list is encountered, the data-flow dependence analysis of slicing is terminated for that variable. Stop-list slicing further focuses on the computation of interest, while ignoring computations known or determined to be uninteresting. This has the potential to reduce slice size when compared to traditional forms of slicing. In order to assess the size of the reduction obtained via stop-list slicing, the paper reports the results of three empirical evaluations: a large scale empirical study into the maximum slice size reduction that can be achieved when all program variables are on the stop-list; a study on a real program, to determine\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "16\n", "authors": ["23"]}
{"title": "Genetic and Evolutionary Computation\u0393\u00c7\u00f6GECCO 2004: Genetic and Evolutionary Computation Conference, Seattle, WA, USA, June 26\u0393\u00c7\u00f430, 2004 Proceedings, Part II\n", "abstract": " MostMOEAsuseadistancemetricorothercrowdingmethodinobjectivespaceinorder to maintain diversity for the non-dominated solutions on the Pareto optimal front. By ensuring diversity among the non-dominated solutions, it is possible to choose from a variety of solutions when attempting to solve a speci? c problem at hand. Supposewehavetwoobjectivefunctionsf (x) andf (x). Inthiscasewecande? ne 1 2 thedistancemetricastheEuclideandistanceinobjectivespacebetweentwoneighboring individuals and we thus obtain a distance given by 2 2 2 d (x, x)=[f (x)? f (x)]+[f (x)? f (x)].(1) 1 2 1 1 1 2 2 1 2 2 f wherex andx are two distinct individuals that are neighboring in objective space. If 1 2 2 2 the functions are badly scaled, eg [? f (x)][? f (x)], the distance metric can be 1 2 approximated to 2 2 d (x, x)?[f (x)? f (x)].(2) 1 2 1 1 1 2 f Insomecasesthisapproximationwillresultinanacceptablespreadofsolutionsalong the Pareto front, especially for small gradual slope changes as shown in the illustrated example in Fig. 1. 1.0 0.8 0.6 0.4 0.2 0 0 20 40 60 80 100 f 1 Fig. 1. Forfrontswithsmallgradualslopechangesanacceptabledistributioncanbeobtainedeven if one of the objectives (in this casef) is neglected from the distance calculations. 2 As can be seen in the? gure, the distances marked by the arrows are not equal, but the solutions can still be seen to cover the front relatively well.", "num_citations": "16\n", "authors": ["23"]}
{"title": "Are mutants really natural? a study on how\" naturalness\" helps mutant selection\n", "abstract": " Background: Code is repetitive and predictable in a way that is similar to the natural language. This means that code is\" natural\" and this\" naturalness\" can be captured by natural language modelling techniques. Such models promise to capture the program semantics and identify source code parts thatsmell', ie, they are strange, badly written and are generally error-prone (likely to be defective). Aims: We investigate the use of natural language modelling techniques in mutation testing (a testing technique that uses artificial faults). We thus, seek to identify how well artificial faults simulate real ones and ultimately understand how natural the artificial faults can be. Our intuition is that natural mutants, ie, mutants that are predictable (follow the implicit coding norms of developers), are semantically useful and generally valuable (to testers). We also expect that mutants located on unnatural code locations (which are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["23"]}
{"title": "Homi: Searching higher order mutants for software improvement\n", "abstract": " This paper introduces HOMI, a Higher Order Mutation based approach for Genetic Improvement of software, in which the code modification granularity is finer than in previous work while scalability remains. HOMI applies the NSGAII algorithm to search for higher order mutants that improve the non-functional properties of a program while passing all its regression tests. Experimental results on four real-world C programs shows that up\u252c\u00e1to 14.7\u252c\u00e1% improvement on time and 19.7\u252c\u00e1% on memory are found using only First Order Mutants. By combining these First Order Mutants, HOMI found further improvement in Higher Order Mutants, giving an 18.2\u252c\u00e1% improvement on the time performance while keeping the memory improvement. A further manual analysis suggests that 88\u252c\u00e1% of the mutation changes cannot be generated using line based \u0393\u00c7\u00ffplastic surgery\u0393\u00c7\u00d6 Genetic Improvement approaches.", "num_citations": "15\n", "authors": ["23"]}
{"title": "Amorphous procedure extraction\n", "abstract": " The procedure extraction problem is concerned with the meaning preserving formation of a procedure from a (not necessarily contiguous) selected set of statements. Previous approaches to the problem have used dependence analysis to identify the non-selected statements which must be 'promoted' (also selected) in order to preserve semantics. All previous approaches to the problem have been syntax preserving. This work shows that by allowing transformation of the program's syntax it is possible to extract both procedures and functions in an amorphous manner. That is, although the amorphous extraction process is meaning preserving it is not necessarily syntax preserving. The amorphous approach is advantageous in a variety of situations. These include when it is desirable to avoid promotion, when a value-returning function is to be extracted from a scattered set of assignments to a variable, and when side\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "15\n", "authors": ["23"]}
{"title": "Inferring test models from kate\u0393\u00c7\u00d6s bug reports using multi-objective search\n", "abstract": " Models inferred from system execution logs can be used to test general system behaviour. In this paper, we infer test models from user bug reports that are written in the natural language. The inferred models can be used to derive new tests which further exercise the buggy features reported by users. Our search-based model inference approach considers three objectives: (1) to reduce the number of invalid user events generated (over approximation), (2) to reduce the number of unrecognised user events (under approximation), (3) to reduce the size of the model (readability). We apply our approach to 721 of Kate\u0393\u00c7\u00d6s bug reports which contain the information required to reproduce the bugs. We compare our results to start-of-the-art KLFA tool. Our results show that our inferred models require 19 tests to reveal a bug on average, which is 98 times fewer than the models inferred by KLFA.", "num_citations": "14\n", "authors": ["23"]}
{"title": "Analysis of procedure splitability\n", "abstract": " As software evolves there is a tendency for size to increase and structure to degrade, leading to problems for on going maintenance and reverse engineering. This paper introduces a greedy dependence-based procedure splitting algorithm that provides automated support for analysis and intervention where procedures show signs of poor structure and overlarge size. The paper reports on the algorithms, implementation and empirical evaluation of procedure splitability. The study reveals a surprising prevalence of splitable procedures and a strong correlation between procedure size and splitability.", "num_citations": "14\n", "authors": ["23"]}
{"title": "A non-standard semantics for program slicing and dependence analysis\n", "abstract": " We introduce a new non-strict semantics for a simple while language. We demonstrate that this semantics allows us to give a denotational definition of variable dependence and neededness, which is consistent with program slicing. Unlike other semantics used in variable dependence, our semantics is substitutive. We prove that our semantics is preserved by traditional slicing algorithms.", "num_citations": "14\n", "authors": ["23"]}
{"title": "A lazy semantics for program slicing\n", "abstract": " A lazy semantics for program slicing - Goldsmiths Research Online Research Online Research Online Logo Goldsmiths - University of London Login Menu A lazy semantics for program slicing Tools + Tools Danicic, Sebastian; Harman, Mark; Howroyd, John and Ouarbya, Lahcen. 2004. 'A lazy semantics for program slicing'. In: 1st International Workshop on Programming Language Interference and Dependence. Verona, Italy. [Conference or Workshop Item] No full text available Item Type: Conference or Workshop Item (Paper) Departments, Centres and Research Units: Computing Dates: Date Event August 2004 [\"eprint_fieldopt_dates_date_type_shown\" not defined] Event Location: Verona, Italy Item ID: 15230 Date Deposited: 02 Dec 2015 16:03 Last Modified: 20 Jun 2017 11:22 URI: http://research.gold.ac.uk/id/eprint/15230 Edit Record Edit Record (login required) Goldsmiths, University of London, New Cross, , , : \u0393\u00c7\u00aa", "num_citations": "14\n", "authors": ["23"]}
{"title": "Software engineering using metaheuristic innovative algorithms: workshop report\n", "abstract": " This paper reports on the first International Workshop on Software Engineering using Metaheuristic Innovative Algorithms, which was held in Toronto on the 14th of May 2001 as a part of the IEEE International Conference on Software Engineering.", "num_citations": "14\n", "authors": ["23"]}
{"title": "Espresso: A slicer generator\n", "abstract": " This paper introduces Espresso, a slicer generator. Espresso compiles the program, p, to be sliced and outputs a slicer. This slicer is a multi-threaded Java program tailored to produce static slices for the program p,(and no other) but with respect to arbitrary slicing criteria. The concurrent nature of the slicers produced by Espresso renders them amenable to parallel execution: Using Java's Remote Invocation Package, the programs output by Espresso can be distributed amongst many computing agents thereby speeding up the slicing protess. This slicer generator approach has a number of advantages. It facilitates portability and provides efficiency improvement opportunities (via code optimisation and speciaiisation and via automatic paxailelization). The slicers generated by Espresso also produce simultaneous slices and software surgery support information at no additional cost.", "num_citations": "14\n", "authors": ["23"]}
{"title": "WES: Agent-based user interaction simulation on real infrastructure\n", "abstract": " We introduce the Web-Enabled Simulation (WES) research agenda, and describe FACEBOOK's WW system. We describe the application of WW to reliability, integrity and privacy at FACEBOOK1, where it is used to simulate social media interactions on an infrastructure consisting of hundreds of millions of lines of code. The WES agenda draws on research from many areas of study, including Search Based Software Engineering, Machine Learning, Programming Languages, Multi Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We conclude with a set of open problems and research challenges to motivate wider investigation.", "num_citations": "13\n", "authors": ["23"]}
{"title": "Equivalence hypothesis testing in experimental software engineering\n", "abstract": " This article introduces the application of equivalence hypothesis testing (EHT) into the Empirical Software Engineering field. Equivalence (also known as bioequivalence in pharmacological studies) is a statistical approach that answers the question \"is product T equivalent to some other reference product R within some range $$\\Updelta$$?.\" The approach of \u0393\u00c7\u00a3null hypothesis significance test\u0393\u00c7\u00a5 used traditionally in Empirical Software Engineering seeks to assess evidence for differences between T and R, not equivalence. In this paper, we explain how EHT can be applied in Software Engineering, thereby extending it from its current application within pharmacological studies, to Empirical Software Engineering. We illustrate the application of EHT to Empirical Software Engineering, by re-examining the behavior of experts and novices when handling code with side effects compared to side-effect free code; a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["23"]}
{"title": "Automated generation of state abstraction functions using data invariant inference\n", "abstract": " Model based testing relies on the availability of models that can be defined manually or by means of model inference techniques. To generate models that include meaningful state abstractions, model inference requires a set of abstraction functions as input. However, their specification is difficult and involves substantial manual effort. In this paper, we investigate a technique to automatically infer both the abstraction functions necessary to perform state abstraction and the finite state models based on such abstractions. The proposed approach uses a combination of clustering, invariant inference and genetic algorithms to optimize the abstraction functions along three quality attributes that characterize the resulting models: size, determinism and infeasibility of the admitted behaviors. Preliminary results on a small e-commerce application are extremely encouraging because the automatically produced models include\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["23"]}
{"title": "Repository of publications on search-based software engineering\n", "abstract": " The test case generation is intrinsically a multi-objective problem, since the goal is covering multiple test targets (eg, branches). Existing search-based approaches either consider one target at a time or aggregate all targets into a single fitness function (whole-suite approach). Multi and many-objective optimisation algorithms (MOAs) have never been applied to this problem, because existing algorithms do not scale to the number of coverage objectives that are typically found in real-world software. In addition, the final goal for MOAs is to find alternative trade-off solutions in the objective space, while in test generation the interesting solutions are only those test cases covering one or more uncovered targets. In this paper, we present DynaMOSA (Dynamic Many-Objective Sorting Algorithm), a novel many-objective solver specifically designed to address the test case generation problem in the context of coverage testing. DynaMOSA extends our previous many-objective technique MOSA (Many-Objective Sorting Algorithm) with dynamic selection of the coverage targets based on the control dependency hierarchy. Such extension makes the approach more effective and efficient in case of limited search budget. We carried out an empirical study on 346 Java classes using three coverage criteria (ie, statement, branch, and strong mutation coverage) to assess the performance of DynaMOSA with respect to the whole-suite approach (WS), its archive-based variant (WSA) and MOSA. The results show that DynaMOSA outperforms WSA in 28% of the classes for branch coverage (+ 8% more coverage on average) and in 27% of the classes for mutation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["23"]}
{"title": "Improving test quality using robust unique input/output circuit sequences (UIOCs)\n", "abstract": " In finite state machine (FSM) based testing, the problem of fault masking in the unique input/output (UIO) sequence may degrade the test performance of the UIO based methods. This paper investigates this problem and proposes the use of a new type of unique input/output circuit (UIOC) sequence for state verification, which may help to overcome the drawbacks that exist in the UIO based techniques. When constructing a UIOC, overlap and internal state observation schema are used to increase the robustness of a test sequence. Test quality is compared by using the forward UIO method (F-method), the backward UIO method (B-method) and the UIOC method (C-method) separately. Robustness of the UIOCs constructed by the algorithm given in this paper is also compared with those constructed by the algorithm given previously. Experimental results suggest that the C-method outperforms the F- and the B-methods\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "13\n", "authors": ["23"]}
{"title": "SEMINAL: Software engineering using metaheuristic innovative algorithms\n", "abstract": " Metaheuristic search algorithms have been widely applied to almost all engineering disciplines with the exception of software engineering. It is surprising that these essentially software driven technologies have not yet fully penetrated the software engineering research community and are not widely applied when compared to the more traditional engineering disciplines.", "num_citations": "13\n", "authors": ["23"]}
{"title": "The impact of code review on architectural changes\n", "abstract": " Although considered one of the most important decisions in the software development lifecycle, empirical evidence on how developers perform and perceive architectural changes remains scarce. Architectural decisions have far-reaching consequences yet, we know relatively little about the level of developers' awareness of their changes' impact on the software's architecture. We also know little about whether architecture-related discussions between developers lead to better architectural changes. To provide a better understanding of these questions, we use the code review data from 7 open source systems to investigate developers' intent and awareness when performing changes alongside the evolution of the changes during the reviewing process. We extracted the code base of 18,400 reviews and 51,889 revisions. 4,171 of the reviews have changes in their computed architectural metrics, and 731 present\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["23"]}
{"title": "Generalized observational slicing for tree-represented modelling languages\n", "abstract": " Model-driven software engineering raises the abstraction level making complex systems easier to understand than if written in textual code. Nevertheless, large complicated software systems can have large models, motivating the need for slicing techniques that reduce the size of a model. We present a generalization of observation-based slicing that allows the criterion to be defined using a variety of kinds of observable behavior and does not require any complex dependence analysis. We apply our implementation of generalized observational slicing for tree-structured representations to Simulink models. The resulting slice might be the subset of the original model responsible for an observed failure or simply the sub-model semantically related to a classic slicing criterion. Unlike its predecessors, the algorithm is also capable of slicing embedded Stateflow state machines. A study of nine real-world models drawn\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["23"]}
{"title": "The executable experimental template pattern for the systematic comparison of metaheuristics\n", "abstract": " This is an area that is fraught with difficulty for several reasons. Firstly, for some metaheuristics there are many parameters that may require manual tuning (eg mutation or crossover rate [8]). As there is a tendency to tune these parameters to obtain the best possible performance for a new metaheuristic it is difficult to differentiate between a genuine advantage and the effect of tuning. Secondly, since metaheuristics are stochastic, it is important to distinguish genuine improvements in the performance of a metaheuristic from apparent improvements arising from chance. There are a wide variety of statistical tests for this purpose, many of which have certain preconditions regarding the nature of the data (eg assumptions of normality). It is desirable that a statistical test is chosen which is ap-", "num_citations": "12\n", "authors": ["23"]}
{"title": "Crawlability metrics for web applications\n", "abstract": " Automated web crawlers can be used to explore and exercise portions of a web application under test. However, the possibility to achieve full exploration of a web application through automated crawling is severely limited by the choice of the input values submitted with forms. Depending on the crawler's capabilities, a larger or smaller portion of web application will be automatically explored. In this paper, we introduce web crawl ability metrics to quantify properties of application pages and forms that affect crawl ability. Moreover, we show that our metrics can be used to identify the boundaries between those parts of the application that can be successfully crawled automatically and those parts that will require manual intervention or other crawl ability support. We have validated our crawl ability metrics on real web applications, for which low crawl ability was indeed associated with the existence of pages never\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "12\n", "authors": ["23"]}
{"title": "Characterising, explaining, and exploiting the approximate nature of static analysis through animation\n", "abstract": " This paper addresses the question: \"How can animated visualisation be used to express interesting properties of static analysis?\" The particular focus is upon static dependence analysis, but the approach adopted in the paper is applicable to other forms of static analysis. The challenge is twofold. First, there is the inherent difficultly of using animation, which is inherently dynamic, as a representation of static analysis, which is not. The paper shows one way in which this apparent contradiction can be overcome. Second, there is the harder challenge of ensuring that the animations so-produced correspond to features of genuine interest in the source code that are hard to visualize without animation. To address these two challenges the paper shows how properties of static dependence analysis can be formulated in a manner suitable for animated visualisation. These formulations of dependence have been\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "11\n", "authors": ["23"]}
{"title": "Aspect Oriented Software Development: Towards A Philosophical Basis\n", "abstract": " Object oriented software measurement has been given a foundational, theoretical basis by the research of Chidamber and Kemerer, partly based on Wand and Weber\u0393\u00c7\u00d6s interpretation of the ontology of Bunge. Aspect oriented software development lacks such a sound theoretical basis, with the result that there is some confusion over what constitutes an aspect and how best to measure and assess the properties of an aspect oriented program. This paper describes Dooyeweerd\u0393\u00c7\u00d6s theory of aspects which provides a theoretical basis upon which may be constructed an ontological approach to the understanding of aspect oriented programs. The paper presents an overview of Dooyeweed's theory and uses it to put forward a plan for developing an ontology for aspect oriented software development and measurement.", "num_citations": "11\n", "authors": ["23"]}
{"title": "API-constrained genetic improvement\n", "abstract": " ACGI respects the Application Programming Interface whilst using genetic programming to optimise the implementation of the API. It reduces the scope for improvement but it may smooth the path to GI acceptance because the programmer\u0393\u00c7\u00d6s code remains unaffected; only library code is modified. We applied ACGI to C++ software for the state-of-the-art OpenCV SEEDS superPixels image segmentation algorithm, obtaining a speed-up of up\u252c\u00e1to 13.2\u252c\u00e1% () to the $50\u252c\u00e1K Challenge winner announced at CVPR 2015.", "num_citations": "10\n", "authors": ["23"]}
{"title": "Guaranteed inconsistency avoidance during software evolution\n", "abstract": " The attempt to design and integrate consistent changes to an existing system is the essence of software maintenance. Software developers also confront similar problems: there are changes during testing and the release of new system builds. Whether in development or maintenance, changes to evolving systems must be made consistently; that is, without damaging correct computations. It is difficult for the programmer to ascertain the complete effect of a code change; the programmer may make a change to a program that is syntactically and semantically legal, but which has ripples into the parts of the program that were intended to remain unchanged. Using the standard denotational semantics for procedural programming languages, this paper formalizes decomposition slicing, which identifies interferences between software components and isolates the components to be changed. We enumerate the conditions\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "10\n", "authors": ["23"]}
{"title": "Evaluation of estimation models using the Minimum Interval of Equivalence\n", "abstract": " This article proposes a new measure to compare soft computing methods for software estimation. This new measure is based on the concepts of Equivalence Hypothesis Testing (EHT). Using the ideas of EHT, a dimensionless measure is defined using the Minimum Interval of Equivalence and a random estimation. The dimensionless nature of the metric allows us to compare methods independently of the data samples used.The motivation of the current proposal comes from the biases that other criteria show when applied to the comparison of software estimation methods. In this work, the level of error for comparing the equivalence of methods is set using EHT. Several soft computing methods are compared, including genetic programming, neural networks, regression and model trees, linear regression (ordinary and least mean squares) and instance-based methods. The experimental work has been performed on\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["23"]}
{"title": "Software engineering: An ideal set of challenges for evolutionary computation\n", "abstract": " Software is an engineering material to be optimised. Until comparatively recently many computer scientists doubted this; why would one want to optimise something that could be made perfect by pure logical reasoning? However, the wider community has come to realise that, while very small programs may be perfect in isolation, larger software systems may never be (because the world in which they operate is not perfect). Once we accept this, we soon arrive at evolutionary computation as a means of optimising software. However, software is not merely some other engineering material to be optimised. Software is virtual and inherently adaptive, making it better suited to evolutionary computation than any other engineering material. This is leading to breakthroughs at the interface of software engineering and evolutionary computation, though there are still many exciting open problems for evolutionary commutation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["23"]}
{"title": "Search based testing (keynote)\n", "abstract": " Search Based Testing (keynote) - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Search Based Testing (keynote) Research output: Chapter in Book/Report/Conference proceeding \u0393\u00c7\u2551 Conference paper Mark Harman Overview Citation formats Original language Undefined/Unknown Title of host publication Software & Systems Quality Conference (SQS-UK 2006, industry conference) Published 2006 King's Authors Mark Harman (Informatics) Post to Twitter Post to FaceBook Post to Digg View graph of relations By the same authors Amorphous Slicing of Extended Finite State Machines Androutsopoulos, K., Clark, D., Harman, M., Hierons, RM, Li, Z. & Tratt, L., 2013, In: IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. 39, 7, p. 892-p. \u0393\u00c7\u00aa", "num_citations": "9\n", "authors": ["23"]}
{"title": "Testing of future internet applications running in the cloud\n", "abstract": " The cloud will be populated by software applications that consist of advanced, dynamic, and largely autonomic interactions among services, end-user applications, content, and media. The complexity of the technologies involved in the cloud makes testing extremely challenging and demands novel approaches and major advancements in the field. This chapter describes the main challenges associated with the testing of applications running in the cloud. The authors present a research agenda that has been defined in order to address the testing challenges. The goal of the agenda is to investigate the technologies for the development of an automated testing environment, which can monitor the applications under test and can react dynamically to the observed changes. Realization of this environment involves substantial research in areas such as search based testing, model inference, oracle learning, and anomaly\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["23"]}
{"title": "Optimised realistic test input generation using web services\n", "abstract": " We introduce a multi-objective formulation of service-oriented testing, focusing on the balance between service price and reliability. We experimented with NSGA-II for this problem, investigating the effect on performance and quality of composition size, topology and the number of services discovered. For topologies small enough for exhaustive search we found that NSGA-II finds a pareto front very near (the fronts are a Euclidean distance of ~0.00024 price-reliability points apart) the true pareto front. Regarding performance, we find that composition size has the strongest effect, with smaller topologies consuming more machine time; a curious effect we believe is due to the influence of crowding distance. Regarding result quality, our results reveal that size and topology have more effect on the front found than the number of service choices discovered. As expected the price-reliability relationship (logarithmic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["23"]}
{"title": "Search-based software engineering for maintenance and reengineering\n", "abstract": " Summary form only given. This talk explains how software maintenance and re-engineering activities can be viewed as a search for solutions that balance many competing constraints to achieve an optimal or near optimal result. This interpretation of the problems we face leads to the inevitable conclusion that the search process, as currently followed, is a woefully labour-intensive human activity; it may not scale to meet the demands of the new and emerging software evolution scenarios. The aim of search based software engineering (SBSE) research is to move software engineering problems from human-based search to machine-based search, using a variety of techniques from the meta-heuristic search and evolutionary computation paradigms. As a result, human effort moves up the abstraction chain to focus on guiding the automated search, rather than performing it. The talk describes the search based\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["23"]}
{"title": "Evolutionary testing: Tutorial\n", "abstract": " Evolutionary Testing: Tutorial - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Evolutionary Testing: Tutorial Research output: Chapter in Book/Report/Conference proceeding \u0393\u00c7\u2551 Conference paper Mark Harman, Joachim Wegener Overview Citation formats Original language Undefined/Unknown Title of host publication Genetic and Evolutionary Computation (GECCO) Published 2003 King's Authors Mark Harman (Informatics) Post to Twitter Post to FaceBook Post to Digg View graph of relations By the same authors Amorphous Slicing of Extended Finite State Machines Androutsopoulos, K., Clark, D., Harman, M., Hierons, RM, Li, Z. & Tratt, L., 2013, In: IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. 39, 7, p. 892-909 18 p. output-\u0393\u00c7\u00aa", "num_citations": "8\n", "authors": ["23"]}
{"title": "Weakest precondition for general recursive programs formalized in Coq\n", "abstract": " This paper describes a formalization of the weakest precondition, wp, for general recursive programs using the type-theoretical proof assistant Coq. The formalization is a deep embedding using the computational power intrinsic to type theory. Since Coq accepts only structural recursive functions, the computational embedding of general recursive programs is non-trivial. To justify the embedding, an operational semantics is defined and the equivalence between wp and the operational semantics is proved. Three major healthiness conditions, namely: Strictness, Monotonicity and Conjunctivity are proved as well.", "num_citations": "8\n", "authors": ["23"]}
{"title": "Testing Web Enabled Simulation at Scale Using Metamorphic Testing\n", "abstract": " We report on Facebook\u0393\u00c7\u00d6s deployment of MIA (Meta-morphic Interaction Automaton). MIA is used to test Facebook\u0393\u00c7\u00d6s Web Enabled Simulation, built on a web infrastructure of hundreds of millions of lines of code. MIA tackles the twin problems of test flakiness and the unknowable oracle problem. It uses metamorphic testing to automate continuous integration and regression test execution. MIA also plays the role of a test bot, automatically commenting on all relevant changes submitted for code review. It currently uses a suite of over 40 metamorphic test cases. Even at this extreme scale, a non-trivial metamorphic test suite subset yields outcomes within 20 minutes (sufficient for continuous integration and review processes). Furthermore, our offline mode simulation reduces test flakiness from approximately 50% (of all online tests) to 0% (offline). Metamorphic testing has been widely-studied for 22 years. This paper is the\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["23"]}
{"title": "Comparative analysis of constraint handling techniques for constrained combinatorial testing\n", "abstract": " Constraints depict the dependency relationships between parameters in a software system under test. Because almost all systems are constrained in some way, techniques that adequately cater for constraints have become a crucial factor for adoption, deployment and exploitation of Combinatorial Testing (CT). Currently, despite a variety of different constraint handling techniques available, the relationship between these techniques and the generation algorithms that use them remains unknown, yielding an important gap and pressing concern in the literature of constrained combination testing. In this paper, we present a comparative empirical study to investigate the impact of four common constraint handling techniques on the performance of six representative (greedy and search-based) test suite generation algorithms. The results reveal that the Verify technique implemented with the Minimal Forbidden Tuple\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["23"]}
{"title": "An empirical validation of oracle improvement\n", "abstract": " We propose a human-in-the-loop approach for oracle improvement and analyse whether the proposed oracle improvement process is helping developers to create better oracles. For this, we conducted two human studies with 68 participants overall: an oracle assessment study and an oracle improvement study. Our results show that developers exhibit poor performance (29% accuracy) when manually assessing whether an assertion oracle contains a false positive, a false negative or none of the two. This shows that automated detection of these oracle deficiencies is beneficial for the users. Our tool OASIs (Oracle ASsessment and Improvement) helps developers produce assertions with higher quality. Participants who used OASIs in the improvement study were able to achieve 33% of full and 67% of partial correctness as opposed to participants without the tool who achieved only 21% of full and 43% of partial\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["23"]}
{"title": "Perturbed model validation: A new framework to validate model relevance\n", "abstract": " This paper introduces Perturbed Model Validation (PMV), a new technique to validate model relevance and detect overfitting or underfitting. PMV operates by injecting noise to the training data, re-training the model against the perturbed data, then using the training accuracy decrease rate to assess model relevance. A larger decrease rate indicates better concept-hypothesis fit. We realise PMV by perturbing labels to inject noise, and evaluate PMV on four real-world datasets (breast cancer, adult, connect-4, and MNIST) and nine synthetic datasets in the classification setting. The results reveal that PMV selects models more precisely and in a more stable way than cross-validation, and effectively detects both overfitting and underfitting.", "num_citations": "7\n", "authors": ["23"]}
{"title": "The FITTEST tool suite for testing future internet applications\n", "abstract": " Future Internet applications are expected to be much more complex and powerful, by exploiting various dynamic capabilities For testing, this is very challenging, as it means that the range of possible behavior to test is much larger, and moreover it may at the run time change quite frequently and significantly with respect to the assumed behavior tested prior to the release of such an application. The traditional way of testing will not be able to keep up with such dynamics. The Future Internet Testing (FITTEST) project (                   http://crest.cs.ucl.ac.uk/fittest/                                    ), a research project funded by the European Commission (grant agreement n. 257574) from 2010 till 2013, was set to explore new testing techniques that will improve our capacity to deal with the challenges of testing Future Internet applications. Such techniques should not be seen as replacement of the traditional testing, but rather as a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["23"]}
{"title": "Using genetic algorithms to search for key stakeholders in large-scale software projects\n", "abstract": " Large software projects have many stakeholders. In order for the resulting software system and architecture to be aligned with the enterprise and stakeholder needs, key stakeholders must be adequately consulted and involved in the project. This work proposes the use of genetic algorithms to identify key stakeholders and their actual influence in requirements elicitation, given the stakeholders\u0393\u00c7\u00d6 requirements and the actual set of requirements implemented in the project. The proposed method is applied to a large real-world software project. Results show that search is able to identify key stakeholders accurately. Results also indicate that many different good solutions exist. This implies that a stakeholder has the potential to play a key role in requirements elicitation, depending on which other stakeholders are already involved. This work demonstrates the true complexity of requirements elicitation\u0393\u00c7\u00f4all stakeholders\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "7\n", "authors": ["23"]}
{"title": "Optimised realistic test input generation\n", "abstract": " Generating realistic test data is a major problem for software testers. This problem is very severe for certain input types that are hard to generate automatically. Previously we proposed a novel automated solution to test data generation that exploits existing web services as sources of realistic test data. One of the issues for this research agenda is the ability to select/use services with high reliability and low cost. We formulate this as an optimisation problem and suggest the use of multi-objective optimisation approaches as a possible solution.", "num_citations": "7\n", "authors": ["23"]}
{"title": "of host publication\n", "abstract": " The need for effective testing techniques for architectural level descriptions is widely recognised. However, due to the variety of domain-specific architectural description languages, there remains a lack of practical techniques in many application domains. We present a simulation-based testing framework that applies optimisation-based search to achieve high-performance testing for a type of architectural model. The search based automatic test-data generation technique forms the core of the framework. Matlab/Simulink is popularly used in embedded systems engineering as an architectural-level design notation. Our prototype framework is built on Matlab for testing Simulink models. The technology involved should apply to the other architectural notations provided that the notation supports execution or simulation.", "num_citations": "7\n", "authors": ["23"]}
{"title": "Mechanized operational semantics of WSL\n", "abstract": " This paper presents an experiment on computer assisted formal verification of program transformations. The operational semantics of WSL is formalized in the type theoretical proof assistant Coq, which forms the basis on which the correctness of program transformations can be stated and proved as formulae in Coq. A group of program transformations frequently used for software maintenance have been proved correct. The existence of a machine checked formal verification increases significantly the confidence in the correctness of program transformations, which is crucial for the reliability of software maintenance systems.", "num_citations": "7\n", "authors": ["23"]}
{"title": "We need a testability transformation semantics\n", "abstract": " This paper (This paper is a brief outline of some of the content of the keynote by the author at the  International Conference on Software Engineering and Formal Methods (SEFM 2018) in Toulouse, France; 27th\u0393\u00c7\u00f429th June 2018.) briefly reviews Testability Transformation, its formal definition, and the open problem of constructing a set of formal test adequacy semantics to underpin the current practice of deploying transformations to help testing and verification activities.", "num_citations": "6\n", "authors": ["23"]}
{"title": "Prem: Prestige network enhanced developer-task matching for crowdsourced software development\n", "abstract": " Many software organizations are turning to employ crowdsourcing to augment their software production. For current practice of crowdsourcing, it is common to see a mass number of tasks posted on software crowdsourcing platforms, with little guidance for task selection. Considering that crowd developers may vary greatly in expertise, inappropriate developer-task matching will harm the quality of the deliverables. It is also not time-efficient for developers to discover their most appropriate tasks from vast open call requests. We propose an approach called PREM, aiming to appropriately match between developers and tasks. PREM automatically learns from the developers\u0393\u00c7\u00d6 historical task data. In addition to task preference, PREM considers the competition nature of crowdsourcing by constructing developers\u0393\u00c7\u00d6 prestige network. This differs our approach from previous developer recommendation methods that are based on task and/or individual features. Experiments are conducted on 3 TopCoder datasets with 9,191 tasks in total. Our experimental results show that reasonable accuracies are achievable (63%, 46%, 36% for the 3 datasets respectively, when matching 5 developers to each task) and the constructed prestige network can help improve the matching results.", "num_citations": "6\n", "authors": ["23"]}
{"title": "Regression test case prioritisation for Guava\n", "abstract": " We present a three objective formulation of regression test prioritisation. Our formulation involves the well-known, and widely-used objectives of Average Percentage of Statement Coverage (APSC) and Effective Execution Time (EET). However, we additionally include the Average Percentage of Change Coverage (APCC), which has not previously been used in search-based regression test optimisation. We apply our approach to prioritise the base and the collection package of the Guava project, which contains over 26,815 test cases. Our results demonstrate the value of search-based test case prioritisation: the sequences we find require only 0.2\u252c\u00e1% of the 26,815 test cases and only 0.45\u252c\u00e1% of their effective execution time. However, we find solutions that achieve more than 99.9\u252c\u00e1% of both regression testing objectives; covering both changed code and existing code. We also investigate the tension between\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["23"]}
{"title": "GI4GI: Improving genetic improvement fitness functions\n", "abstract": " Genetic improvement (GI) has been successfully used to optimise non-functional properties of software, such as execution time, by automatically manipulating program's source code. Measurement of non-functional properties, however, is a non-trivial task; energy consumption, for instance, is highly dependant on the hardware used. Therefore, we propose the GI4GI framework (and two illustrative applications). GI4GI first applies GI to improve the fitness function for the particular environment within which software is subsequently optimised using traditional GI.", "num_citations": "6\n", "authors": ["23"]}
{"title": "Why testing autonomous agents is hard and what can be done about it\n", "abstract": " Intuitively, autonomous agents are hard to test. Their autonomy and their flexible, context-aware behaviour implies unpredictability, and their social ability intuitively leads to unpredictable emergent behaviour. On the other hand, software agents are programs just like those designed and developed in an object-oriented, procedural fashion, or under any other approach, and just because something is intuitive does not mean it is necessarily correct.In their separate respective groups, the authors of this position paper have been examining exactly why agents may be harder to test than the kinds of software we would expect to be developed without an agentoriented approach, quantifying this difficulty, and/or proposing means to address it. This position paper summarises and expands on our groups\u0393\u00c7\u00d6 work for the purposes of discussion. While we are aware that there are other relevant approaches to tackling agent testing by other groups (including some presented at the Agent-Oriented Software Engineering workshop series), we do not attempt to give a state of the art survey here, but instead refer to a forthcoming survey of this area by Nguyen et al.[12].", "num_citations": "6\n", "authors": ["23"]}
{"title": "Applications of linear program schematology in dependence analysis\n", "abstract": " Applications of linear program schematology in dependence analysis - Goldsmiths Research Online Research Online Research Online Logo Goldsmiths - University of London Login Menu Applications of linear program schematology in dependence analysis Tools + Tools Danicic, Sebastian; Harman, Mark; Hierons, Robert; Howroyd, John; Laurence, Michael and Danicic, Sebastian. 2004. 'Applications of linear program schematology in dependence analysis'. In: 1st International Workshop on Programming Language Interference and Dependence. Verona, Italy. [Conference or Workshop Item] No full text available Item Type: Conference or Workshop Item (Paper) Departments, Centres and Research Units: Computing Dates: Date Event August 2004 [\"eprint_fieldopt_dates_date_type_shown\" not defined] Event Location: Verona, Italy Item ID: 15231 Date Deposited: 02 Dec 2015 16:05 Last Modified: 13 Jun 2016 12:: :\u0393\u00c7\u00aa", "num_citations": "6\n", "authors": ["23"]}
{"title": "Genetic and Evolutionary Computation-GECCO 2003: Genetic and Evolutionary Computation Conference, Chicago, IL, USA, July 12-16, 2003, Proceedings, Part I\n", "abstract": " The set LNCS 2723 and LNCS 2724 constitutes the refereed proceedings of the Genetic and Evolutionaty Computation Conference, GECCO 2003, held in Chicago, IL, USA in July 2003. The 193 revised full papers and 93 poster papers presented were carefully reviewed and selected from a total of 417 submissions. The papers are organized in topical sections on a-life adaptive behavior, agents, and ant colony optimization; artificial immune systems; coevolution; DNA, molecular, and quantum computing; evolvable hardware; evolutionary robotics; evolution strategies and evolutionary programming; evolutionary sheduling routing; genetic algorithms; genetic programming; learning classifier systems; real-world applications; and search based softare engineering.", "num_citations": "6\n", "authors": ["23"]}
{"title": "An interprocedural amorphous slicer for WSL\n", "abstract": " This paper presents a simple interprocedural algorithm for amorphous slicing and illustrates the way in which interprocedural amorphous slicing improves upon interprocedural syntax-preserving slicing. The paper also presents results from an empirical study of tin implementation of this algorithm for Ward's Wide Spectrum Language, WSL. The implementation uses the FermaT transformation workbench. It combines FermaT transformations with the results produced by a syntax-preserving slicer for WSL. Finally, it is shown that the combination of amorphous slicing and conditioned slicing ran be particularly attractive, by combining results from the amorphous slicer with results from a prototype conditioned slicer for WSL.", "num_citations": "6\n", "authors": ["23"]}
{"title": "Cost measures matter for mutation testing study validity\n", "abstract": " Mutation testing research has often used the number of mutants as a surrogate measure for the true execution cost of generating and executing mutants. This poses a potential threat to the validity of the scientific findings reported in the literature. Out of 75 works surveyed in this paper, we found that 54 (72%) are vulnerable to this threat. To investigate the magnitude of the threat, we conducted an empirical evaluation using 10 real-world programs. The results reveal that: i) percentages of randomly sampled mutants differ from the true execution time, on average, by 44%, varying in difference from 19% to 91%; ii) errors arising from using the surrogate correlate with program size (\u2567\u00fc= 0.74) and number of mutants (\u2567\u00fc= 0.76), making the problem more pernicious for more realistic programs; iii) scientific findings concerning sampling strategies would have approximately 37% rank disagreement, indicating potentially\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["23"]}
{"title": "Game-theoretic analysis of development practices: Challenges and opportunities\n", "abstract": " Developers continuously invent new practices, usually grounded in hard-won experience, not theory. Game theory studies cooperation and conflict; its use will speed the development of effective processes. A survey of game theory in software engineering finds highly idealised models that are rarely based on process data. This is because software processes are hard to analyse using traditional game theory since they generate huge game models. We are the first to show how to use game abstractions, developed in artificial intelligence, to produce tractable game-theoretic models of software practices. We present Game-Theoretic Process Improvement (GTPI), built on top of empirical game-theoretic analysis. Some teams fall into the habit of preferring \u0393\u00c7\u00a3quick-and-dirty\u0393\u00c7\u00a5 code to slow-to-write, careful code, incurring technical debt. We showcase GTPI\u0393\u00c7\u00d6s ability to diagnose and improve such a development process\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["23"]}
{"title": "Hierons, Search algorithms for regression test case prioritization\n", "abstract": " Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on Greedy Algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning Greedy Algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that Genetic Algorithms perform well, although Greedy approaches are surprisingly effective, given the multimodal nature of the landscape. Index Terms\u0393\u00c7\u00f6Search techniques, test case prioritization, regression testing. \u251c\u00e7 1", "num_citations": "5\n", "authors": ["23"]}
{"title": "Guest Editorial: Special Issue on Software Maintenance and Evolution\n", "abstract": " In systems developed without aspect-oriented programming, code implementing a crosscutting concern may be spread over many different parts of a system. Identifying such code automatically could be of great help during maintenance of the system. First of all, it allows a developer to more easily find the places in the code that must be changed when the concern changes and, thus, makes such changes less time consuming and less prone to errors. Second, it allows the code to be refactored to an aspect-oriented solution, thereby improving its modularity. In this paper, we evaluate the suitability of clone detection as a technique for the identification of crosscutting concerns. To that end, we manually identify five specific crosscutting concerns in an industrial C system and analyze to what extent clone detection is capable of finding them. We consider our results as a stepping stone toward an automated \"aspect miner\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["23"]}
{"title": "Slice-based dynamic memory modelling-a case study\n", "abstract": " Program slicing is a source-code extraction technique that identifies parts of a program which have no effect upon a chosen set of variables at a point of interest. Slices can be constructed statically (with respect to no input information) or conditionally (with respect to partial input information). They can also be constructed in either a purely syntax-preserving or amorphous way. Amorphous slices tend to be smaller than their syntax-preserving counterparts but they may not be syntactically related to the original. This paper presents the results of a case study which assesses the value of static and conditioned slicing (in both syntax-preserving and amorphous formulations) upon the problem of dynamic memory analysis. The results confirmed our belief that slicing is helpful for dynamic memory analysis: syntax-preserving static slicing produced an order-of-magnitude reduction in the size of the program to be analysed\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["23"]}
{"title": "A simultaneous slicing theory and derived program slicer\n", "abstract": " A Simultaneous Slicing Theory and Derived Program Slicer (invited paper) - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers A Simultaneous Slicing Theory and Derived Program Slicer (invited paper) Research output: Chapter in Book/Report/Conference proceeding \u0393\u00c7\u2551 Conference paper Sebastian Danicic, Mark Harman Overview Citation formats Original language Undefined/Unknown Title of host publication 4 th RIMS Workshop in Computing Published 1996 King's Authors Mark Harman (Informatics) Post to Twitter Post to FaceBook Post to Digg View graph of relations By the same authors Amorphous Slicing of Extended Finite State Machines Androutsopoulos, K., Clark, D., Harman, M., Hierons, RM, Li, Z. & Tratt, L., 2013, In: IEEE ON . .\u0393\u00c7\u00aa", "num_citations": "5\n", "authors": ["23"]}
{"title": "Slice-based measurement of function coupling\n", "abstract": " In this paper we outline an approach to coupling measurement based upon program slicing. We claim that our coupling metric produces more precise measurements than related information-flowbased metrics. A prototype tool has been implemented to calculate these metrics. The next step is to evaluate the coupling (and related cohesion) metrics with respect to some hypotheses relating to the predication systems in which we claim they may be embedded. 1 Background After initial interest in code-level metrics in the 70s and early 80s the focus of software measurement movedupstream'to the earlier stages of the product development life cycle. However, the definition and evaluation of code-level metrics has received renewed attention with the rise in popularity of the objectoriented paradigm [2, 3, 11]. This paper is concerned with code-level, slice-based metrics for assessing the level of information flow between functions and their coupling through this information flow. Our approach is an...", "num_citations": "5\n", "authors": ["23"]}
{"title": "Evaluating Automatic Program Repair Capabilities to Repair API Misuses\n", "abstract": " API misuses are well-known causes of software crashes and security vulnerabilities. However, their detection and repair is challenging given that the correct usages of (third-party) APIs might be obscure to the developers of client programs. This paper presents the first empirical study to assess the ability of existing automated bug repair tools to repair API misuses, which is a class of bugs previously unexplored. Our study examines and compares 14 Java test-suite-based repair tools (11 proposed before 2018, and three afterwards) on a manually curated benchmark (APIREPBENCH) consisting of 101 API misuses. We develop an extensible execution framework (APIARTY) to automatically execute multiple repair tools. Our results show that the repair tools are able to generate patches for 28% of the API misuses considered. While the 11 less recent tools are generally fast (the median execution time of the repair\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["23"]}
{"title": "Search based software testing for Android\n", "abstract": " Summary form only given. This talk will cover some of the history of search based software testing, charting some of the milestones in the intellectual development of the subject. The talk concludes with the current work by Mao, Harman and Jia on SBST for the Android platform, which saw the three of them recruited from the start-up Majicke to Facebook, where there are now working on test automation.", "num_citations": "4\n", "authors": ["23"]}
{"title": "Search based software engineering [guest editorial]\n", "abstract": " The articles in this special section focus on search-based software engineering. Search Based Software Engineering (SBSE) consists of the application of computational intelligence (CI) algorithms to hard optimization problems in software engineering (SE). It has become an important application field for CI. The term SBSE was coined by Harman and Jones in 2001, although there was work on the application of CI algorithms to SE before this date. After more than fifteen years development, CI algorithms have been used to solve SE tasks in almost all the stages of an SE lifecycle, including requirements, designing, coding, testing and maintenance. solved by three steps.", "num_citations": "4\n", "authors": ["23"]}
{"title": "Sbselector: Search based component selection for budget hardware\n", "abstract": " Determining which functional components should be integrated to a large system is a challenging task, when hardware constraints, such as available memory, are taken into account. We formulate such problem as a multi-objective component selection problem, which searches for feature subsets that balance the provision of maximal functionality at minimal memory resource cost. We developed a search-based component selection tool, and applied it to the KDE-based application, Kate, to find a set of Kate instantiations that balance functionalities and memory consumption. Our results report that, compared to the best attainment of random search, our approach can reduce at most  memory consumption with respect to the same number components. While comparing to greedy search, the memory reduction can be up\u252c\u00e1to . SBSelector finds a instantiation of Kate that provides 16 more components\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["23"]}
{"title": "Introduction to the special issue on Mutation Testing\n", "abstract": " It is our pleasure to introduce this special issue on Mutation Testing. The special issue contains nine papers, including four extended versions of papers presented at the 7th International Workshop on Mutation Analysis and five new submissions. We have divided the special issue into three broad areas based on the topics covered. The first area focuses on the techniques for making mutation testing more efficient and practical; the second area revisits some fundamental questions about mutants, whilst the third area presents some advanced applications of mutation testing for model-based testing.Mutation Testing has been proven to be an effective way to measure the quality of a test suite in terms of its ability to detect faults [1]. The history of mutation testing can be traced back to 1971 in a publication by Richard Lipton [2] as well as in publications from the late 1970s by DeMillo et al.[3] and Hamlet [4]. In Mutation\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["23"]}
{"title": "Efficient identification of linchpin vertices in dependence clusters\n", "abstract": " Several authors have found evidence of large dependence clusters in the source code of a diverse range of systems, domains, and programming languages. This raises the question of how we might efficiently locate the fragments of code that give rise to large dependence clusters. We introduce an algorithm for the identification of linchpin vertices, which hold together large dependence clusters, and prove correctness properties for the algorithm\u0393\u00c7\u00d6s primary innovations. We also report the results of an empirical study concerning the reduction in analysis time that our algorithm yields over its predecessor using a collection of 38 programs containing almost half a million lines of code. Our empirical findings indicate improvements of almost two orders of magnitude, making it possible to process larger programs for which it would have previously been impractical.", "num_citations": "4\n", "authors": ["23"]}
{"title": "Variable Dependence Analysis Technical Report: TR-10-0\n", "abstract": " Variable dependence is a source code analysis problem, related to slicing and chopping, in which the aim is to determine the set of variables that can affect the values of given program variables at specified points in the program. This paper describes an approach to solving the problem based on transforming program code into an intermediate language without preserving concrete semantics but keeping variable dependence relations intact. Interestingly the transformation phase need only preserve variable dependence, thereby admitting useful transformations that fail to preserve standard semantics (but do preserve variable dependence). The paper describes an implementation of a variable dependence analysis system called Vada, illustrating the application of non\u0393\u00c7\u00f4meaning\u0393\u00c7\u00f4preserving transformation, together with an empirical study of performance optimisation steps.", "num_citations": "4\n", "authors": ["23"]}
{"title": "Search-based software engineering\n", "abstract": " Editorial: Search-based software engineering: Computers and Operations Research: Vol 35, No 10 ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Computers and Operations Research Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsComputers and Operations ResearchVol. , No. Editorial: Search-based software engineering article Editorial: Search-based software engineering Share on Authors: Walter J Gutjahr profile image Walter J. Gutjahr University of Vienna, Austria and King's College, London University of Vienna, Austria and King's College, London View Profile , Mark Harman profile image Mark Harman University of Vienna, Austria :\u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["23"]}
{"title": "Program Slicing using Functional Networks (Concurrency Theory and Applications' 96)\n", "abstract": " Program slicing is a technique for identifying a subprogram from an original program. The subprogram, called a slice, is an executable program which maintains the effect of the original upon a chosen set of variables at some point within the Control Flow Graph (CFG) of the original. The variable set, CFG node pair is  the slicing criterion. Slices find applications in debugging, testing, parallelisation, -use, code measurement andprogram comprehension. These applications exploit the way in which slicing preserves a projectionof the original program\u0393\u00c7\u00d6s semantics, allowing, in some cases, for considerable syntactic simplification. The paper introduces an approach to the computation of a slightly more general form of a slice, called a simultaneous slice. A simultaneous slice is constructed with respect to a set of conventional slicing criteria. The approach introduced here uses a functional implementation of a network of communicating processes, each process corresponding to a CFG node and each message corresponding to either a variable name or a node identifier.", "num_citations": "4\n", "authors": ["23"]}
{"title": "Cleaving Together: Program Cohesion with Slices\n", "abstract": " Cleaving Together: Program Cohesion with Slices - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Cleaving Together: Program Cohesion with Slices Research output: Contribution to journal \u0393\u00c7\u2551 Article \u0393\u00c7\u2551 peer-review Mark Harman Overview Citation formats Original language Undefined/Unknown Pages (from-to) 35-42 Number of pages 8 Journal EXE Volume 11 Issue number 8 Published 1 Jan 1997 King's Authors Mark Harman (Informatics) Post to Twitter Post to FaceBook Post to Digg View graph of relations By the same authors Amorphous Slicing of Extended Finite State Machines Androutsopoulos, K., Clark, D., Harman, M., Hierons, RM, Li, Z. & Tratt, L., 2013, In: IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. 39, 7, p. 892-p. \u0393\u00c7\u00aa", "num_citations": "4\n", "authors": ["23"]}
{"title": "A Survey of Performance Optimization for Mobile Applications\n", "abstract": " Nowadays there is a mobile application for almost everything a user may think of, ranging from paying bills and gathering information to playing games and watching movies. In order to ensure user satisfaction and success of applications, it is important to provide high performant applications. This is particularly important for resource constraint systems such as mobile devices. Thereby, non-functional performance characteristics, such as energy and memory consumption, play an important role for user satisfaction. This paper provides a comprehensive survey of non-functional performance optimization for Android applications. We collected 155 unique publications, published between 2008 and 2020, that focus on the optimization of non-functional performance of mobile applications. We target our search at four performance characteristics, in particular: responsiveness, launch time, memory and energy\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["23"]}
{"title": "Generic software subgraph isomorphism\n", "abstract": " The omnipresence of software graphs as useful intermediate representations means that the identification of near-match subgraphs (Error-Correcting Subgraph Isomorphism) has diverse and widespread applications in software engineering, such as querying, clone detection and model checking. Each software engineering subarea has developed specific tailored approaches to subgraph isomorphism, thereby reducing comparability and generality, and potentially yielding sub-optimal results. We introduce a generic tabu-search formulation, facilitating the incorporation of software engineering domain knowledge and present initial results for our approach.", "num_citations": "3\n", "authors": ["23"]}
{"title": "Evaluating key statements analysis\n", "abstract": " Key statement analysis extracts from a program, statements that form the core of the programpsilas computation. A good set of key statements is small but has a large impact. Key statements form a useful starting point for understanding and manipulating a program. An empirical investigation of three kinds of key statements is presented. The three are based on Bieman and Ottpsilas principal variables. To be effective, the key statements must have high impact and form a small, highly cohesive unit. Using a minor improvement of metrics for measuring impact and cohesion, key statements are shown to capture about 75% of the semantic effect of the function from which they are drawn. At the same time, they have cohesion about 20 percentage points higher than the corresponding function. A statistical analysis of the differences shows that key statements have higher average impact and higher average cohesion (p<0\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["23"]}
{"title": "Slicing extended finite state machines\n", "abstract": " Slicing of Extended Finite State Machines Page 1 Slicing of Extended Finite State Machines Kelly Androutsopoulos1, David Clark1, Nicolas Gold1, Mark Harman1, Rob Hierons2, Zheng Li1 and Laurence Tratt3 Centre for Research in Evolution, Search & Testing Centre for Research in Evolution, Search & Testing 1CREST, King\u0393\u00c7\u00d6s College London 2Brunel University 3Bournemouth University Page 2 SLIcing state based Models SLIcing state based Models EPSRC Project SLIM 2008 \u0393\u00c7\u00f4 2011 CREST Dependence Analysis Data Dependence Control Dependence Nontermination Sensitive Non- termination Insensitive Order Dependence Types of Slicing Dependence Based Slicing Event Restriction Slicing Correctness Semantics Syntax Applications Comprehension Model debugging Dependence Based Slicing - Dependence Analysis - Marked Transitions - \u256c\u2561-elimination - Minimisation Event Restriction Slicing - - - R-- G-(\u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["23"]}
{"title": "Source code analysis and manipulation\n", "abstract": " The special section contains nine papers which are extended versions of papers originally published in the proceedings of the IEEE workshop on Source Code Analysis and Manipulation (SCAM 2001). The papers cover the spectrum of analysis and manipulation work from traditional transformation application areas concerned with optimisation through to more novel application areas such as transformation for evolution of websites; they cover the use of linguistic source code aspects which arise from the semantics of the language to more ephemeral, yet higher level domain-related information. The special issue brings together work on algorithms, tools, empirical results and novel approaches to source code analysis and manipulation. There are five papers on analysis and four on manipulation in this special issue. The next two sections describe the papers on analysis and manipulation respectively.", "num_citations": "3\n", "authors": ["23"]}
{"title": "Software engineering using metaheuristic innovative algorithms\n", "abstract": " Software Engineering using Metaheuristic INnovative ALgorithms - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Software Engineering using Metaheuristic INnovative ALgorithms Research output: Contribution to journal \u0393\u00c7\u2551 Article \u0393\u00c7\u2551 peer-review Mark Harman, Bryan F. Jones Overview Citation formats Original language Undefined/Unknown Pages (from-to) 905-907 Number of pages 3 Journal INFORMATION AND SOFTWARE TECHNOLOGY Volume 43 Issue number 14 Published 2001 King's Authors Mark Harman (Informatics) Post to Twitter Post to FaceBook Post to Digg View graph of relations By the same authors Amorphous Slicing of Extended Finite State Machines Androutsopoulos, K., Clark, D., Harman, M., Hierons, RM, Li, Z. & , \u0393\u00c7\u00aa", "num_citations": "3\n", "authors": ["23"]}
{"title": "Facebook\u0393\u00c7\u00d6s Cyber\u0393\u00c7\u00f4Cyber and Cyber\u0393\u00c7\u00f4Physical Digital Twins\n", "abstract": " A cyber\u0393\u00c7\u00f4cyber digital twin is a simulation of a software system. By contrast, a cyber\u0393\u00c7\u00f4physical digital twin is a simulation of a non-software (physical) system. Although cyber\u0393\u00c7\u00f4physical digital twins have received a lot of recent attention, their cyber\u0393\u00c7\u00f4cyber counterparts have been comparatively overlooked. In this paper we show how the unique properties of cyber\u0393\u00c7\u00f4cyber digital twins open up exciting opportunities for research and development. Like all digital twins, the cyber\u0393\u00c7\u00f4cyber digital twin is both informed by and informs the behaviour of the twin it simulates. It is therefore a software system that simulates another software system, making it conceptually truly a twin, blurring the distinction between the simulated and the simulator. Cyber\u0393\u00c7\u00f4cyber digital twins can be twins of other cyber\u0393\u00c7\u00f4cyber digital twins, leading to a hierarchy of twins. As we shall see, these apparently philosophical observations have practical\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "\u0393\u00c7\u00a3Ignorance and Prejudice\u0393\u00c7\u00a5 in Software Fairness\n", "abstract": " Machine learning software can be unfair when making human-related decisions, having prejudices over certain groups of people. Existing work primarily focuses on proposing fairness metrics and presenting fairness improvement approaches. It remains unclear how key aspect of any machine learning system, such as feature set and training data, affect fairness. This paper presents results from a comprehensive study that addresses this problem. We find that enlarging the feature set plays a significant role in fairness (with an average effect rate of 38%). Importantly, and contrary to widely-held beliefs that greater fairness often corresponds to lower accuracy, our findings reveal that an enlarged feature set has both higher accuracy and fairness. Perhaps also surprisingly, we find that a larger training data does not help to improve fairness. Our results suggest a larger training data set has more unfairness than a\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "Enhancing genetic improvement of software with regression test selection\n", "abstract": " Genetic improvement uses artificial intelligence to automatically improve software with respect to non-functional properties (AI for SE). In this paper, we propose the use of existing software engineering best practice to enhance Genetic Improvement (SE for AI).We conjecture that existing Regression Test Selection (RTS) techniques (which have been proven to be efficient and effective) can and should be used as a core component of the GI search process for maximising its effectiveness.To assess our idea, we have carried out a thorough empirical study assessing the use of both dynamic and static RTS techniques with GI to improve seven real-world software programs.The results of our empirical evaluation show that incorporation of RTS within GI significantly speeds up the whole GI process, making it up to 68% faster on our benchmark set, being still able to produce valid software improvements.Our findings are\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "Learning from mistakes: Machine learning enhanced human expert effort estimates\n", "abstract": " In this paper, we introduce a novel approach to predictive modeling for software engineering, named Learning From Mistakes (LFM). The core idea underlying our proposal is to automatically learn from past estimation errors made by human experts, in order to predict the characteristics of their future misestimates, therefore resulting in improved future estimates. We show the feasibility of LFM by investigating whether it is possible to predict the type, severity and magnitude of errors made by human experts when estimating the development effort of software projects, and whether it is possible to use these predictions to enhance future estimations. To this end we conduct a thorough empirical study investigating 402 maintenance and new development industrial software projects. The results of our study reveal that the type, severity and magnitude of errors are all, indeed, predictable. Moreover, we find that by\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "Ownership at Large: Open Problems and Challenges in Ownership Management\n", "abstract": " Software-intensive organizations rely on large numbers of software assets of different types, eg, source-code files, tables in the data warehouse, and software configurations. Who is the most suitable owner of a given asset changes over time, eg, due to reorganization and individual function changes. New forms of automation can help suggest more suitable owners for any given asset at a given point in time. By such efforts on ownership health, accountability of ownership is increased. The problem of finding the most suitable owners for an asset is essentially a program comprehension problem: how do we automatically determine who would be best placed to understand, maintain, evolve (and thereby assume ownership of) a given asset. This paper introduces the Facebook Ownesty system, which uses a combination of ultra large scale data mining and machine learning and has been deployed at Facebook as part\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "A Study of Bug Resolution Characteristics in Popular Programming Languages\n", "abstract": " Bug resolution is an essential part of software development. The impact of programming language on bug resolution has been a topic of much debate. Taking Python as an example, some hold the view that bugs in the language are easy to handle because its code is easy to read and understand, while others believe that the absence of static typing leads to more bug-handling effort. This paper presents the first large-scale study that investigates the connection between programming language and bug resolution characteristics. It follows the recent trend of empirical scientific reformulation of long-standing, but hitherto anecdotal, `great debates' about the influence of programming language and paradigm on software engineering concerns. We analyse bug resolution data from over 70 million SLOC drawn from 3 million commits to 600 GitHub projects in 10 languages. The results suggest that statistically significant\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "The assessor's dilemma: Improving bug repair via empirical game theory\n", "abstract": " Priority inflation occurs when a QA engineer or a project manager requesting a feature inflates the priority of their task so that developers deliver the fix or the new functionality faster. We survey developers and show that priority inflation occurs and misallocates developer time. We are the first to apply empirical game-theoretic analysis (EGTA) to a software engineering problem, specifically priority inflation. First, we extract prioritization strategies from 42,620 issues from Apache's JIRA, then use TaskAssessor, our EGTA-based modelling approach, to confirm conventional wisdom and show that the common process of a QA engineer assigning priority labels is susceptible to priority inflation. We then show that the common mitigation strategy of having a bug triage team assigning priorities does not resolve priority inflation and slows development. We then use mechanism design to devise assessor-throttling, a new\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "An empirical comparison of mutant selection assessment metrics\n", "abstract": " Mutation testing is expensive due to the large number of mutants, a problem typically tackled using selective techniques, thereby raising the fundamental question of how to evaluate the selection process. Existing mutant selection approaches rely on one of two types of metrics (or assessment criteria), one based on adequate test sets and the other based on inadequate test sets. This raises the question as to whether these two metrics are correlated, complementary or substitutable for one another. The tester's faith in mutant selection as well as the validity of previous research work using only one metric rely on the answer to this question, yet it currently remains unanswered. To answer it, we perform qualitative and quantitative comparisons with 104 different projects, consisting of over 600,000 lines of code. Our results indicate a strong connection between the two types of metrics (R 2 =0.8622 on average). The\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "Overview of TASE 2012 Talk on Search Based Software Engineering\n", "abstract": " This is an overview of the keynote presentation on SBSE at the Sixth IEEE International Symposium on Theoretical Aspects of Software Engineering (TASE 2012), held on the 4th-6th July 2012 in Beijing, China.", "num_citations": "2\n", "authors": ["23"]}
{"title": "An alternative characterization of weak order dependence\n", "abstract": " Control dependence forms the basis for many program analyses, such as program slicing. Recent work on control dependence analysis has led to new definitions of dependence that can allow for reactive programs with their necessarily non-terminating computations. One important such definition is the definition of Weak Order Dependence, which was introduced to generalize classical control dependence for a Control Flow Graph (CFG) without end nodes. In this paper we show that for a CFG where all nodes are reachable from each other, weak order dependence can be expressed in terms of traditional control dependence where one node has been converted into an end node.", "num_citations": "2\n", "authors": ["23"]}
{"title": "10111 executive summary\u0393\u00c7\u00f4practical software testing: Tool automation and human factors\n", "abstract": " The main goal of the seminar``Practical Software Testing: Tool Automation and Human Factors''was to bring together academics working on algorithms, methods, and techniques for practical software testing, with practitioners, interested in developing more soundly-based and well-understood testing processes and practices. The seminar's purpose was to make researchers aware of industry's problems, and practitioners aware of research approaches. The seminar focused in particular on testing automation and human factors. In the week of March 14-19, 2010, 40 researchers from 11 countries (Canada, France, Germany, Italy, Luxembourg, the Netherlands, Sweden, Switzerland, South Africa, United Kingdom, United States) discussed their recent work, and recent and future trends in software testing. The seminar consisted of five main types of presentations or activities: topic-oriented presentations, research-oriented presentations, short self-introduction presentations, tool demos, and working group meetings and presentations.", "num_citations": "2\n", "authors": ["23"]}
{"title": "Special Issue on Search\u0393\u00c7\u00c9Based Software Maintenance\n", "abstract": " Search-based software engineering (SBSE) consists of the application of metaheuristic search techniques\u0393\u00c7\u00f6such as genetic algorithms, simulated annealing, hill climbing, and tabu search\u0393\u00c7\u00f6to software engineering problems. This idea is based on the observation that some software engineering activities can be thought of as optimization problems, and on the fact that techniques providing exact solutions are, very often, unable to adequately scale to large, real-world software engineering problems.", "num_citations": "2\n", "authors": ["23"]}
{"title": "Finding test data on the web\n", "abstract": " This paper proposes an automated solution for generating test cases for a web service using other web services. The proposed system is based on semantic web services that use Ontology Web Language based web service ontology OWL-S. OWL-S provides semantic information for input/output data as well as information on the services behavior. The system uses this information for test case generation.", "num_citations": "2\n", "authors": ["23"]}
{"title": "Pareto Efficient Multi-Objective Test Case Selection\n", "abstract": " This project will build on the previous work by Mark Harman and Shin Yoo on Pareto Efficient Test Case Selection pertaining to two objective formulation. It will introduce the concept of Pareto Efficient Test Case selection and the benefits of this approach. This project will illustrate the benefits of Pareto efficient test case selection with empirical studies of two objective formulations.", "num_citations": "2\n", "authors": ["23"]}
{"title": "Sebastian Danicic\n", "abstract": " Several approaches to reverse and re-engineering are based upon program slicing. Unfortunately, for large systems, such as those which typically form the subject of reverse engineering activities, the space and time requirements of slicing can be a barrier to successful application. Faced with this problem, several authors have found it helpful to merge Control Flow Graph (CFG) nodes, thereby improving the space and time requirements of standard slicing algorithms. The node-merging process essentially creates a \u0393\u00c7\u00ffcoarser\u0393\u00c7\u00d6version of the original CFG. This paper introduces a theory for defining Control Flow Graph node coarsening calculi. The theory formalizes properties of interest, when coarsening is used as a precursor to program slicing. The theory is illustrated with a case study of a coarsening calculus, which is proved to have the desired properties of sharpness and consistency. 1", "num_citations": "2\n", "authors": ["23"]}
{"title": "Projecting functional models of imperative programs\n", "abstract": " Functional modelling [17,29] enables functional reasoning methods to be applied to programs written in imperative languages. It is, however, the view of many workers [4, 24] that it is not the notation in which a program is written, but the sheer size of a program which prohibits the application of reasoning and proof techniques.The beauty of Projection is that simple aspects of programs have correspondingly simple models, irrespective of the size and complexity of the overall imperative program.We describe the Projection technique, showing how it allows for the manageable application of functional language technology to 'real' imperative programs. We demonstrate how Projection may facilitate the development of a proof of a program in terms of its constituents and how Projection may allow programmers to select the safety-critical sections of programs for particular attention.We briefly discuss the connection\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "2\n", "authors": ["23"]}
{"title": "Multi-objective software performance optimisation at the architecture level using randomised search rules\n", "abstract": " Architecture-based software performance optimisation can help to find potential performance problems and mitigate their negative effects at an early stage. To automate this optimisation process, rule-based and metaheuristic-based performance optimisation methods have been proposed. However, existing rule-based methods explore a limited search space, potentially excluding optimal or near-optimal solutions. Most of current metaheuristic-based methods ignore existing practical knowledge of performance improvement, and lead to solutions that are not easily explicable to humans. To address these problems, we propose a novel approach for performance optimisation at the software architecture level named Multiobjective performance Optimisation based on Randomised search rulEs (MORE). First, we design randomised search rules (MORE-R) to provide explanation without parameters while benefiting from\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Multi-Objective Software Effort Estimation: A Replication Study\n", "abstract": " Replication studies increase our confidence in previous results when the findings are similar each time, and help mature our knowledge by addressing both internal and external validity aspects. However, these studies are still rare in certain software engineering fields. In this paper, we replicate and extend a previous study, which denotes the current state-of-the-art for multi-objective software effort estimation, namely CoGEE. We investigate the original research questions with an independent implementation and the inclusion of a more robust baseline (LP4EE), carried out by the first author, who was not involved in the original study. Through this replication, we strengthen both the internal and external validity of the original study. We also answer two new research questions investigating the effectiveness of CoGEE by using four additional evolutionary algorithms (i.e., IBEA, MOCell, NSGA-III, SPEA2) and a well\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Reducing Oracle Cost in Search Based Test Data Generation\n", "abstract": " Search Based testing has proved effective at generating test data to cover targeted branches and has consequently received a great deal of attention from the automated software testing community. However, previous approaches to search based test data generation do not take account of oracle cost. While there may be an aspiration that systems should have models, checkable specifications and/or contract driven development, this sadly remains an aspiration; in many real cases, system behaviour must be checked by a human. This painstaking checking process forms a significant cost, the oracle cost, which previous work on automated test data generation tends to overlook. In this paper we introduce three algorithms for reducing oracle cost during test data generation. Each algorithm seeks to reduce the number of test cases produced, without compromising coverage achieved. We present the results of an empirical study of the effectiveness of the three algorithms on five benchmark programs containing non trivial search spaces for branch coverage. The results indicate that it is, indeed, possible to make reductions in the number of test cases produced by search based testing, without loss of coverage.", "num_citations": "1\n", "authors": ["23"]}
{"title": "A Study of Programming Languages and Their Bug Resolution Characteristics\n", "abstract": " IEEE Bug resolution is an essential part of software development. The impact of programming language on bug resolution has been a topic of much debate. Taking Python as an example, some hold the view that bugs in the language are easy to handle because its code is easy to read and understand, while others believe that the absence of static typing leads to more bug-handling effort. This paper presents the first large-scale study that investigates the connection between programming language and bug resolution characteristics. It follows the recent trend of empirical scientific reformulation of long-standing, but hitherto anecdotal, `great debates' about the influence of programming language and paradigm on software engineering concerns. We analyse bug resolution data from over 70 million SLOC drawn from 3 million commits to 600 GitHub projects in 10 languages. The results suggest that statistically significant differences in resolution time and patch size exist between different languages and language categories. In particular, Java bug resolution consumes less elapsed time from raise to resolve, while Ruby consumes more. We also found that patches tend to touch significantly more files for strongly typed and for static languages (as one might expect given the need to maintain type annotations). However, despite this apparent extra effort, we found evidence for a significantly lower elapsed resolution time for bug resolution committed to projects constructed from statically typed languages. This finding sheds further empirical light on the debate about the importance of static typing. Indeed, more generally, we found no evidence for any\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Indexing operators to extend the reach of symbolic execution\n", "abstract": " Traditional program analysis analyses a program language, that is, all programs that can be written in the language. There is a difference, however, between all possible programs that can be written and the corpus of actual programs written in a language. We seek to exploit this difference: for a given program, we apply a bespoke program transformation Indexify to convert expressions that current SMT solvers do not, in general, handle, such as constraints on strings, into equisatisfiable expressions that they do handle. To this end, Indexify replaces operators in hard-to-handle expressions with homomorphic versions that behave the same on a finite subset of the domain of the original operator, and return bottom denoting unknown outside of that subset. By focusing on what literals and expressions are most useful for analysing a given program, Indexify constructs a small, finite theory that extends the power of a solver on the expressions a target program builds. Indexify's bespoke nature necessarily means that its evaluation must be experimental, resting on a demonstration of its effectiveness in practice. We have developed Indexif}, a tool for Indexify. We demonstrate its utility and effectiveness by applying it to two real world benchmarks --- string expressions in coreutils and floats in fdlibm53. Indexify reduces time-to-completion on coreutils from Klee's 49.5m on average to 6.0m. It increases branch coverage on coreutils from 30.10% for Klee and 14.79% for Zesti to 66.83%. When indexifying floats in fdlibm53, Indexifyl increases branch coverage from 34.45% to 71.56% over Klee. For a restricted class of inputs, Indexify permits the symbolic\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Approximate oracles and synergy in software energy search spaces\n", "abstract": " There is a growing interest in using evolutionary computation to reduce software systems\u0393\u00c7\u00d6 energy consumption by utilising techniques such as genetic improvement. However, efficient and effective evolutionary optimisation of software systems requires a better understanding of the energy search landscape. One important choice practitioners have is whether to preserve the system\u0393\u00c7\u00d6s original output or permit approximation; each of which has its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by evolutionary mutation is 2.69%(0.76% on average). By contrast, this figure increases dramatically to 95.60%(33.90% on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for effective evolutionary optimisation. We investigate synergy, a phenomenon that occurs when simultaneously applied evolutionary mutations produce a effect greater than their individual sum. Our results reveal that 12.0% of all joint code modifications produced such a synergistic effect though 38.5% produce an antagonistic interaction in which simultaneously applied mutations are less effective than when applied individually. This highlights the need for an evolutionary approach over more greedy alternatives.", "num_citations": "1\n", "authors": ["23"]}
{"title": "Introduction to the special issue on ISSTA 2013\n", "abstract": " This special issue contains five articles extended from the 2013 International Symposium on Software Testing and Analysis (ISSTA 2013), held in Lugano Switzerland, and chaired by Mauro Pezze (General Chair) and Mark Harman (Program Chair). ISSTA brings together academics, industrial researchers, and practitioners to exchange new ideas, problems, and experience on how to analyse and test software systems. Software analysis and testing is a rich and varied research area that encompasses theoretical, empirical, and practical analysis and techniques that assist software engineers with the ever-present tasks of finding and fixing faults in software systems, thereby improving confidence in the correct operation of these systems. Testing involves automated tool support to help generate test inputs, typically guided by criteria that assess test suite adequacy. Testing is also naturally targeted at critical faults that\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Search based software engineering (sbse)\n", "abstract": " Search Based Software Engineering (SBSE) | Journal of Systems and Software ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Journal of Systems and Software Periodical Home Latest Issue Archive Authors Affiliations Award Winners More HomeBrowse by TitlePeriodicalsJournal of Systems and SoftwareVol. , No. CSearch Based Software Engineering (SBSE) research-article Search Based Software Engineering (SBSE) Share on Authors: Mark Harman profile image Mark Harman View Profile , Francisco Chicano profile image Francisco Chicano View Profile Authors Info & Affiliations Publication: Journal of Systems and SoftwareMay 2015 https://doi.org/10.1016/j.jss..0citation 0 Downloads Metrics Total \u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "The GISMOE Architecture\u0393\u00a3\u00e6\n", "abstract": " The GISMOE research agenda is concerned with optimising programs for non-functional properties such as speed, size, throughput, power consumption and bandwidth can be demanding. GISMOE sets out a vision for a new kind of software development environment inspired by recent results from Search Based Software Engineering (SBSE). Details of the GISMOE research agenda are provided in the extended keynote paper for the 27th IEEE/ACM International Conference on Automated Software Engineering (ASE 2012). This talk overview is a brief introduction to the approach and a description of the talk about the GISMOE agenda at the 2nd Chinese SBSE workshop in Dalian, 8th and 9th June 2013.", "num_citations": "1\n", "authors": ["23"]}
{"title": "Cost-aware Test Suite Minimisation for Service-centric Systems\n", "abstract": " Welcome to the Fast Abstracts collection of the 4th Symposium on Search Based Software Engineering, SSBSE 2012, held in Riva del Garda, in the Province of Trento, in Italy. Riva del Garda is a small city located at the north-western corner of the Garda Lake, in the middle of the Alps, surrounded by Mediterranean vegetation with olive and lemon trees. The symposium was co-located with the 28th IEEE International Conference on Software Maintenance, the premiere international venue in software maintenance and evolution, to promote the crossfertilization between the two research communities.Fast abstracts offer an opportunity to present novel ideas that are not yet fully developed or validated to the community, and they are an established tradition of SSBSE. Fast abstract are subject to a lighter reviewing process, where each paper was reviewed by at least two reviewers. The collection of fast abstracts demonstrates that search-based software engineering increasingly influences all phases of the software engineering process from requirements analysis to software testing and maintenance.", "num_citations": "1\n", "authors": ["23"]}
{"title": "Sbse: introduction and motivation\n", "abstract": " This tutorial will provide a brief introduction to the field of Search Based Software Engineering (SBSE) to (re)establish the context in which we are working in this symposium. There is a paper in the proceedings of the symposium this year, entitled Ten Years of Search Based Software Engineering: A Bibliometric Analysis by Fabr\u251c\u00a1cio Freitas and Jerffeson Souza. This means that there will be no need for me to survey the history of the subject in this talk. Rather, I will provide a very brief introduction to the field of SBSE, summarising some of its advantages and motivations, explaining why software is the ideal engineering material for optimisation algorithms and how SBSE can serve to (re)unify apparently unconnected areas of Software Engineering.", "num_citations": "1\n", "authors": ["23"]}
{"title": "Generating Realistic Test Input Using Web Services\n", "abstract": " Generating realistic test data is a major problem for software testers. Realistic test data generation for certain input types is hard to automate and therefore laborious. We propose a novel automated solution to test data generation that exploits existing web services as sources of realistic test data. Our approach is capable of generating realistic test data and also generating data based on tester-specified constraints. In experimental analysis, our prototype tool achieved between 93% and 100% success rates in generating realistic data using service compositions while random test data generation achieved only between 2% and 34%.", "num_citations": "1\n", "authors": ["23"]}
{"title": "Environment Restriction Slicing: automated state based model simplication\n", "abstract": " Environment Restriction Slicing: automated state based model simplication - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Environment Restriction Slicing: automated state based model simplication Research output: Chapter in Book/Report/Conference proceeding \u0393\u00c7\u2551 Conference paper K Androutsopolous, D Binkley, D Clark, N Gold, M Harman, K Lano, L Zheng Overview Citation formats Original language English Title of host publication ICSE 2011 Publisher IEEE Pages 1 - 1 Number of pages 8 Published 2011 Event ICSE 2011 - Hawaii Duration: 1 Jan 2011 \u0393\u00e5\u00c6 \u0393\u00c7\u00aa Conference Conference ICSE 2011 City Hawaii Period 1/01/2011 \u0393\u00e5\u00c6 \u0393\u00c7\u00aa King's Authors K Lano (Informatics, Software Modelling and Applied Logic) Post to Twitter Post to to , \u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Regression Testing Minimisation, Selection and Prioritisation: A Survey\n", "abstract": " Regression testing is a testing activity that is performed to provide confidence that changes do not harm the existing behaviour of the software. Test suites tend to grow in size as software evolve, often making it too costly to execute entire test suites. A number of different approaches have been studied to maximise the value of the accrued test suite: minimisation, selection and prioritisation. Test suite minimisation seeks to eliminate redundant test cases in order to reduce the number of tests to run. Test case selection seeks to identify the test cases that are relevant to some set of recent changes. Test case prioritisation seeks to order test cases in such a way that early fault detection is maximised. This paper surveys each area of minimisation, selection and prioritisation technique and discusses open problems and potential directions for future research.", "num_citations": "1\n", "authors": ["23"]}
{"title": "The Importance Of Metrics In Search Based Software Engineering\n", "abstract": " This paper was written to accompany the author\u0393\u00c7\u00d6s keynote talk at the Mensura Conference 2006. The keynote will present an overview of Search Based Software Engineering (SBSE) and explain how metrics play a crucial role in SBSE.", "num_citations": "1\n", "authors": ["23"]}
{"title": "Sifting through the wreckage\n", "abstract": " Sifting Through the Wreckage - Research Portal, King's College, London King's College London King's main site Research portal Home Researchers Research Groups Research Outputs Research Funding Internal Research Outputs Theses . Journals Publishers Sifting Through the Wreckage Research output: Contribution to journal \u0393\u00c7\u2551 Article \u0393\u00c7\u2551 peer-review Mark Harman Overview Citation formats Original language English Pages (from-to) 5 Number of pages 1 Journal EXE Published 1999 Bibliographical note Editorial King's Authors Mark Harman (Informatics) Post to Twitter Post to FaceBook Post to Digg View graph of relations By the same authors Amorphous Slicing of Extended Finite State Machines Androutsopoulos, K., Clark, D., Harman, M., Hierons, RM, Li, Z. & Tratt, L., 2013, In: IEEE TRANSACTIONS ON SOFTWARE ENGINEERING. 39, 7, p. 892-909 18 p. Research output: Contribution to journal \u0393\u00c7\u2551 Article \u0393\u00c7\u2551 peer.\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Towards the measurement of objects\n", "abstract": " Towards the measurement of objects - Goldsmiths Research Online Research Online Research Online Logo Goldsmiths - University of London Login Menu Towards the measurement of objects Tools + Tools Harman, Mark and Danicic, Sebastian. 1996. Towards the measurement of objects. In: Martin Shepperd, ed. 1st Bournemouth Metrics Workshop. Bournemouth: Bournemouth University. [Book Section] No full text available Abstract or Description invited talk Item Type: Book Section Departments, Centres and Research Units: Computing Dates: Date Event April 1996 Published Item ID: 15254 Date Deposited: 07 Dec 2015 11:50 Last Modified: 13 Jun 2016 12:33 URI: http://research.gold.ac.uk/id/eprint/15254 Edit Record Edit Record (login required) Goldsmiths, University of London, New Cross, London, SE14 6NW, UK Telephone: + 44 (0) 20 7919 7166 / Email: gro@gold.ac.uk Goldsmiths Research Online OAI a ://\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}
{"title": "Programming Languages for Statistical Computation\n", "abstract": " Several workers in the field of statistical computation have pointed to the need for a statistical package which would include a programming language specifically designed for statistical applications [3].             Some interest has been shown in the possible applicability of the functional programming style. For example an implementation of a statistical language based on the functional programming language Lisp has been carried out by Tierney [8]. The general purpose language S has also been designed and written with statistical programming in mind by Chambers [1].             With the aim of providing a flexible approach to designing a language for statistical computation the authors have created a programming language prototyping system. This system allows rapid implementation of interpreters for new and mutually quite different languages, all of which are based on a core of statistical routines. Examples\u252c\u00e1\u0393\u00c7\u00aa", "num_citations": "1\n", "authors": ["23"]}