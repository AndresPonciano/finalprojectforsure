{"title": "Improving Recall of software defect prediction models using association mining\n", "abstract": " Use of software product metrics in defect prediction studies highlights the utility of these metrics. Public availability of software defect data based on the product metrics has resulted in the development of defect prediction models. These models experience a limitation in learning Defect-prone (D) modules because the available datasets are imbalanced. Most of the datasets are dominated by Not Defect-prone (ND) modules as compared to D modules. This affects the ability of classification models to learn the D modules more accurately. This paper presents an association mining based approach that allows the defect prediction models to learn D modules in imbalanced datasets. The proposed algorithm preprocesses data by setting specific metric values as missing and improves the prediction of D modules. The proposed algorithm has been evaluated using 5 public datasets. A Naive Bayes (NB) classifier has been\u00a0\u2026", "num_citations": "28\n", "authors": ["1336"]}
{"title": "Towards a generic model for software quality prediction\n", "abstract": " Various models and techniques have been proposed and applied in literature for software quality prediction. Specificity of each suggested model is one of the impediments in development of a generic model. A few models have been quality factor specific whereas others are software development paradigm specific. The models can even be company specific or domain specific. The amount of work done for software quality prediction compels the researchers to get benefit from the existing models and develop a relatively generic model. Development of a generic model will facilitate the quality managers by letting them focus on how to improve the quality instead of employing time on deciding which technique best suites their scenario. This paper suggests a generic model which takes software as input and predicts a quality factor value using existing models. This approach captures the specificity of existing models\u00a0\u2026", "num_citations": "16\n", "authors": ["1336"]}
{"title": "Ineffectiveness of use of software science metrics as predictors of defects in object oriented software\n", "abstract": " Software science metrics (SSM) have been widely used as predictors of software defects. The usage of SSM is an effect of correlation of size and complexity metrics with number of defects. The SSM have been proposed keeping in view the procedural paradigm and structural nature of the programs. There has been a shift in software development paradigm from procedural to object oriented (OO) and SSM have been used as defect predictors of OO software as well. However, the effectiveness of SSM in OO software needs to be established. This paper investigates the effectiveness of use of SSM for: (a)classification of defect prone modules in OO software (b) prediction of number of defects. Various binary and numeric classification models have been applied on dataset kc1 with class level data to study the role of SSM. The results show that the removal of SSM from the set of independent variables does not\u00a0\u2026", "num_citations": "12\n", "authors": ["1336"]}
{"title": "Evaluating performance of software defect prediction models using area under precision-Recall curve (AUC-PR)\n", "abstract": " Software defect prediction (SDP) models are used to improve effort and testing estimate of software by identifying defective modules beforehand. Precision, recall/true positive rate and false positive rate have been used to evaluate the performance of models. In literature, area under receiver operating characteristic curve (AUC-ROC) has been used to evaluate the model performance. The standard learning goal of the defect model is to optimize the (AUC-ROC). Use of this measure has also been advocated in numerous benchmarking studies. The literature has discussed the performance bar (or so-called ceiling effect) of AUC-ROC targeted models. The literature has also indicated the use of area under precision recall curve (AUC-PR) as an evaluation parameter for the models. This study investigates if AUC-PR curve gives different information regarding model performance. To this end this study ranks the existing\u00a0\u2026", "num_citations": "11\n", "authors": ["1336"]}
{"title": "Nomenclature unification of software product measures\n", "abstract": " A large number of software quality prediction models are based on software product measures (SPdMs). There are different interpretations and representations of these measures which generate inconsistencies in their naming conventions. These inconsistencies affect the efforts to develop a generic approach to predict software quality. This study identifies two types of such inconsistencies and categorises them into Type I and Type II. Type I inconsistency emerges when different labels are suggested for the same software product measure. Type II inconsistency appears when same label is used for different measures. This study suggests a unification and categorisation framework to remove Type I and Type II inconsistencies. The proposed framework categorises SPdMs with respect to three dimensions: usage frequency, software development paradigm and software lifecycle phase. The framework is applied to\u00a0\u2026", "num_citations": "8\n", "authors": ["1336"]}
{"title": "Finding focused itemsets from software defect data\n", "abstract": " Software product measures have been widely used to predict software defects. Though these measures help develop good classification models, studies propose that relationship between software measures and defects still needs to be investigated. This paper investigates the relationship between software measures and the defect prone modules by studying associations between the two. The paper identifies the critical ranges of the software measures that are strongly associated with defects across five datasets of PROMISE repository. The paper also identifies the ranges of the measures that do not necessarily contribute towards defects. These results are supported by information gain based ranking of software measures.", "num_citations": "7\n", "authors": ["1336"]}
{"title": "Agile model adaptation for e-learning students' final-year project\n", "abstract": " The final-year project (FYP) is considered as an essential part of a Computer Science degree. FYP plays a key role in preparing the students to meet the industrial expectations. In e-learning scenario lack of coordination among students and limited student-supervisor interaction often leads to poor quality projects and inadequate learning. To enhance students' skills and quality of the projects this paper suggests an Agile-Scrum model for the development phase of FYP. The effectiveness of the Agile-Scrum model on student project quality and e-learning has been compared with the in-practice Waterfall model at Virtual University of Pakistan (VU). The proposed model has increased students' involvement, coordination and student-supervisor interaction in FYP.", "num_citations": "6\n", "authors": ["1336"]}
{"title": "Impact of Using Information Gain in Software Defect Prediction Models\n", "abstract": " Presence or absence of defective modules in software is an indicator of quality of the software. Every company aspires to deliver good quality software with minimum number of defective modules. To achieve this goal, defect prediction models are used in different phases of software lifecycle. These models have to deal with a large number software metrics (as input parameters to the models). These metrics have correlation issues that affect a model\u2019s performance. Also, in some cases using all the metrics negatively impacts the models\u2019 performances. In order to reduce size of input space and resolve the possible issues of correlation in input data, models reported in literature use Principal Component Analysis (PCA) and Information Gain (IG) based dimension reduction. PCA reduces the dimensions but keeps the representation of all the input variables intact. Use of PCA is not suitable where representation\u00a0\u2026", "num_citations": "5\n", "authors": ["1336"]}
{"title": "EnerPlan: smart energy management planning for home users\n", "abstract": " The impending energy crisis has driven up the cost of electricity at an exponential rate. Managing electric consumption thus has become a very crucial task especially for home consumers. In this paper we present EnerPlan, a non-intrusive method to aid consumers to reduce their energy cost by advising them a consumption plan for their devices. Our system builds consumer classes based on regional statistical data. Using these classes a target consumer\u2019s device load and distribution is inferred. This inferred data is used to construct a device usage plan. Following this plan can reduce the electric bill of the consumer. We use expert-based and auto-generated fuzzy rules to generate the planning. Results show that in absence of experts, planning through auto-generated is also useful. The results further demonstrate that the data prepared using the proposed approach can be used to save electricity and the\u00a0\u2026", "num_citations": "5\n", "authors": ["1336"]}
{"title": "An FIS for early detection of defect prone modules\n", "abstract": " Early prediction of defect prone modules helps in better resource planning, test planning and reducing the cost of defect correction in later stages of software lifecycle. Early prediction models based on design and code metrics are difficult to develop because precise values of the model inputs are not available. Conventional prediction techniques require exact inputs, therefore such models cannot always be used for early predictions. Innovative prediction methods that use imprecise inputs, however, can be applied to overcome the requirement of exact inputs. This paper presents a fuzzy inference system (FIS) that predicts defect proneness in software using vague inputs defined as fuzzy linguistic variables. The paper outlines the methodology for developing the FIS and applies the model to a real dataset. Performance analysis in terms of recall, accuracy, misclassification rate and a few other measures has\u00a0\u2026", "num_citations": "5\n", "authors": ["1336"]}
{"title": "A comparative study of spatial complexity metrics and their impact on maintenance effort\n", "abstract": " Maintenance is an important phase of software lifecycle. A significant fraction of overall effort and cost is consumed on comprehension and maintenance. Maintenance effort is highly correlated with spatial complexity of the system. The existing metrics for measuring spatial complexity are not suitable for all types of software systems. These metrics sometimes do not depict the effect of the key factors, which contribute significantly towards maintenance effort. In addition, most of these metrics are suitable for a specific type of a system. In this paper, the existing metrics have been studied and their application areas have been identified", "num_citations": "5\n", "authors": ["1336"]}
{"title": "Using association rules to identify similarities between software datasets\n", "abstract": " A number of V&V datasets are publicly available. These datasets have software measurements and defectiveness information regarding the software modules. To facilitate V&V, numerous defect prediction studies have used these datasets and have detected defective modules effectively. Software developers and managers can benefit from the existing studies to avoid analogous defects and mistakes if they are able to find similarity between their software and the software represented by the public datasets. This paper identifies the similar datasets by comparing association patterns in the datasets. The proposed approach finds association rules from each dataset and identifies the overlapping rules from the 100 strongest rules from each of the two datasets being compared. Afterwards, average support and average confidence of the overlap is calculated to determine the strength of the similarity between the\u00a0\u2026", "num_citations": "4\n", "authors": ["1336"]}
{"title": "Refactoring of Code to Remove Technical Debt and Reduce Maintenance Effort\n", "abstract": " Software development organizations face issue like Technical Debt in their software projects. Technical Debt (TD) is incurred when a person involved in engineering of software, intentionally or unintentionally makes wrong or non-optimal design decisions. This problem occurs due to non-systematic and undefined approach to manage the high level of uncertainty in requirements. These non-optimal design decisions and the nonsystematic approach can result in introduction of code smells. Code smells are actually technical debt (also known as perceived debt), which may cause a project to fade out with time. Code of a software project with technical debt is considered as unclean code. Refactoring is an activity of modifying the code to remove technical debt from a software project and make the project code clean. Goal of this work is to study the impact of removing TD on the effort required to add features and\u00a0\u2026", "num_citations": "2\n", "authors": ["1336"]}
{"title": "Towards better knowledge management in global software engineering\n", "abstract": " Many organizations employ distributed teams for development of enterprise information systems. The trend of distributed teams or Global Software Engineering (GSE) has been on the rise for the last decade or so. Improved resource allocation, cost savings and learning of best practices are some of the benefits that cause this increase. When dealing with distributed teams these organizations face several project management challenges such as communication and coordination issues, cultural differences, knowledge management. Lack of knowledge sharing is one of the most critical issue that needs to be addressed. It is difficult to ensure availability of right knowledge at the right time to the right person. The knowledge needs to be shared across the organization but limitations of knowledge sharing tools or dispersed knowledge sharing media makes it more challenging to share knowledge. In addition, lack of tool\u00a0\u2026", "num_citations": "2\n", "authors": ["1336"]}
{"title": "Identifying association between longer itemsets and software defects\n", "abstract": " Software defects are an indicator of software quality. Software with lesser number of defective modules are desired. Prediction of software defects using software measurements facilitates early identification of defect-prone modules. Association relationship between software measures and defects improves prediction of defective modules. To find association relationship between software measures and defects, each numeric measure is divided into bins. Each bin is called 1-itemset (or an itemset of length 1). When certain itemsets and defective modules appear together in a dataset, they are considered associated with each other. Frequency of their co-occurrence depicts the strength of the association relationship. Existing studies find the relationship between 1-itemsets and defective modules. Itemsets that have high association with defects are called focused itemsets. Focused itemsets can be used to build\u00a0\u2026", "num_citations": "2\n", "authors": ["1336"]}
{"title": "Problems In Understanding Abstract Concepts Related To Regular And Context-Free Languages\n", "abstract": " Understanding abstract models in automata theory is important in the fields of computer science and digital circuit design. In automata theory, problems are categorized in different classes of computability, from simpler to harder. Problems which lie in class of regular languages are simplest kind of problems. Problems which lie in class of context-free languages are relatively harder. Students tend to confuse the relationship between these two classes of languages and between their members. Questions related to the closure properties of regular and/or context free languages pose particular problems. Students make mistakes performing operations like intersection, union and concatenation on sets of regular and context-free languages. For this research the students of automata are asked to solve some examination and assignment problems related to closure properties of the above said classes. A questionnaire, seeking descriptive answers, is used to study 1) the mental process 2) the structure of mental concept map and 3) line of reasoning followed while attempting a problem. This study reveals that the students make mistakes because their abstract concepts are not correctly structured and in particular do not support logical inferences. Students also have a tendency to apply the concepts mechanically instead of applying them meaningfully. Finally, students fail to actuate proper conceptual link at proper time.", "num_citations": "1\n", "authors": ["1336"]}