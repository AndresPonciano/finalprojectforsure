{"title": "The opensmt solver\n", "abstract": " This paper describes OpenSMT, an incremental, efficient, and open-source SMT-solver. OpenSMT has been specifically designed to be easily extended with new theory-solvers, in order to be accessible for non-experts for the development of customized algorithms. We sketch the solver\u2019s architecture and interface. We discuss its distinguishing features w.r.t. other state-of-the-art solvers.", "num_citations": "151\n", "authors": ["1643"]}
{"title": "SAFARI: SMT-based abstraction for arrays with interpolants\n", "abstract": " We present SAFARI, a model checker designed to prove (possibly universally quantified) safety properties of imperative programs with arrays of unknown length. SAFARI is based on an extension of lazy abstraction capable of handling existentially quantified formul\u00e6 for symbolically representing states. A heuristics, called term abstraction, favors the convergence of the tool by \u201ctuning\u201d interpolants and guessing additional quantified variables of invariants to prune the search space efficiently.", "num_citations": "61\n", "authors": ["1643"]}
{"title": "Interpolation-based function summaries in bounded model checking\n", "abstract": " During model checking of software against various specifications, it is often the case that the same parts of the program have to be modeled/verified multiple times. To reduce the overall verification effort, this paper proposes a new technique that extracts function summaries after the initial successful verification run, and then uses them for more efficient subsequent analysis of the other specifications. Function summaries are computed as over-approximations using Craig interpolation, a mechanism which is well-known to preserve the most relevant information, and thus tend to be a good substitute for the functions that were examined in the previous verification runs. In our summarization-based verification approach, the spurious behaviors introduced as a side effect of the over-approximation, are ruled out automatically by means of the counter-example guided refinement of the function summaries. We\u00a0\u2026", "num_citations": "59\n", "authors": ["1643"]}
{"title": "Incremental upgrade checking by means of interpolation-based function summaries\n", "abstract": " During its evolution, a typical software/hardware design undergoes a myriad of small changes. However, it is extremely costly to verify each new version from scratch. As a remedy to this problem, we propose to use function summaries to enable incremental verification of the evolving systems. During the evolution, our approach maintains function summaries derived using Craig's interpolation. For each new version, these summaries are used to perform a local incremental check. Benefit of this approach is that the cost of the check depends on the extent of the change between the two versions and can be performed cheaply for incremental changes without resorting to re-verification of the entire system. Our implementation and experimentation in the context of the bounded model checking for C confirms that incremental changes can be verified efficiently for different classes of industrial programs.", "num_citations": "53\n", "authors": ["1643"]}
{"title": "Lazy abstraction with interpolants for arrays\n", "abstract": " Lazy abstraction with interpolants has been shown to be a powerful technique for verifying imperative programs. In presence of arrays, however, the method shows an intrinsic limitation, due to the fact that successful invariants usually contain universally quantified variables, which are not present in the program specification. In this work we present an extension of the interpolation-based lazy abstraction in which arrays of unknown length can be handled in a natural manner. In particular, we exploit the Model Checking Modulo Theories framework, to derive a backward reachability version of lazy abstraction that embeds array reasoning. The approach is generic, in that it is valid for both parameterized systems and imperative programs. We show by means of experiments that our approach can synthesize and prove universally quantified properties over arrays in a completely automatic fashion.", "num_citations": "53\n", "authors": ["1643"]}
{"title": "A formal object-oriented analysis for software reliability: Design for verification\n", "abstract": " This paper presents the OOA design step in a methodology which integrates automata-based model checking into a commercially supported OO software development process.We define and illustrate a set of design rules for OOA models with executable semantics, which lead to automata models with tractable state spaces. The design rules yield OOA models with functionally structured designs similar to those of hardware systems. These structures support modelchecking through techniques known to be feasible for hardware. The formal OOA methodology, including the design rules, was applied to the design of NASA robot control software. Serious logical design errors that had eluded prior testing, were discovered in the course of model-checking.", "num_citations": "45\n", "authors": ["1643"]}
{"title": "The ComFoRT reasoning framework\n", "abstract": " Model checking is a promising technology for verifying critical behavior of software. However, software model checking is hamstrung by scalability issues and is difficult for software engineers to use directly. The second challenge arises from the gap between model checking concepts and notations, and those used by engineers to develop large-scale systems. ComFoRT [15] addresses both of these challenges. It provides a model checker, Copper, that implements a suite of complementary complexity management techniques to address state space explosion. But ComFoRT is more than a model checker. The ComFoRT                         reasoning framework includes additional support for building systems in a particular component-based idiom. This addresses transition issues.", "num_citations": "44\n", "authors": ["1643"]}
{"title": "Computing exact worst-case gas consumption for smart contracts\n", "abstract": " The Ethereum platform is a public, distributed, blockchain-based database that is maintained by independent parties. A user interacts with Ethereum by writing programs and having miners execute them for a fee charged on-the-fly based on the complexity of the execution. The exact fee, measured in gas consumption, in general depends on the unknown Ethereum state, and predicting even its worst case is in principle undecidable. Uncertainty in gas consumption may result in inefficiency, loss of money, and, in extreme cases, in funds being locked for an indeterminate duration. This feasibility study presents two methods for determining the exact worst-case gas consumption of a bounded Ethereum execution using methods influenced by symbolic model checking. We give several concrete cases where gas consumption estimation is needed, and provide two approaches for determining gas consumption\u00a0\u2026", "num_citations": "43\n", "authors": ["1643"]}
{"title": "A combined testing and verification approach for software reliability\n", "abstract": " Automatic and manual software verification is based on applying mathematical methods to a model of the software. Modeling is usually done manually, thus it is prone to modeling errors. This means that errors found in the model may not correspond to real errors in the code, and that if the model is found to satisfy the checked properties, the actual code may still have some errors. For this reason, it is desirable to be able to perform some consistency checks between the actual code and the model. Exhaustive consistency checks are usually not possible, for the same reason that modeling is necessary. We propose a methodology for improving the throughput of software verification by performing some consistency checks between the original code and the model, specifically, by applying software testing. In this paper we present such a combined testing and verification methodology and demonstrate how it is\u00a0\u2026", "num_citations": "42\n", "authors": ["1643"]}
{"title": "eVolCheck: Incremental upgrade checker for C\n", "abstract": " Software is not created at once. Rather, it grows incrementally version by version and evolves long after being first released. To be practical for software developers, the software verification tools should be able to cope with changes. In this paper, we present a tool, eVolCheck, that focuses on incremental verification of software as it evolves. During the software evolution the tool maintains abstractions of program functions, function summaries, derived using Craig interpolation. In each check, the function summaries are used to localize verification of an upgrade to analysis of the modified functions. Experimental evaluation on a range of various benchmarks shows substantial speedup of incremental upgrade checking of eVolCheck in contrast to checking each version from scratch.", "num_citations": "41\n", "authors": ["1643"]}
{"title": "An extension of lazy abstraction with interpolation for programs with arrays\n", "abstract": " Lazy abstraction with interpolation-based refinement has been shown to be a powerful technique for verifying imperative programs. In presence of arrays, however, the method suffers from an intrinsic limitation, due to the fact that invariants needed for verification usually contain universally quantified variables, which are not present in program specifications. In this work we present an extension of the interpolation-based lazy abstraction framework in which arrays of unknown length can be handled in a natural manner. In particular, we exploit the Model Checking Modulo Theories framework to derive a backward reachability version of lazy abstraction that supports reasoning about arrays. The new approach has been implemented in a tool, called safari, which has been validated on a wide range of benchmarks. We show by means of experiments that our approach can synthesize and prove universally\u00a0\u2026", "num_citations": "40\n", "authors": ["1643"]}
{"title": "Verification of evolving software\n", "abstract": " We define the substitutability problem in the context of evolving software systems as the verification of the following two criteria:(i) previously established system correctness properties must remain valid for the new version of a system, and (ii) the updated portion of the system must continue to provide all (and possibly more) services o# ered by its earlier counterpart. We present a completely automated procedure based on learning techniques for regular sets to solve the substitutability problem for component based software. We have implemented and validated our approach in the context of the ComFoRT reasoning framework and report encouraging preliminary results on an industrial benchmark.", "num_citations": "39\n", "authors": ["1643"]}
{"title": "OpenSMT2: An SMT solver for multi-core and cloud computing\n", "abstract": " This paper describes a major revision of the OpenSMT solver developed since 2008. The version 2 significantly improves its predecessor by providing a design that supports extensions, several critical bug fixes and performance improvements. The distinguishing feature of the new version is the support for a wide range of parallelization algorithms both on multi-core and cloud-computing environments. Presently the solver implements the quantifier free theories of uninterpreted functions and equalities and linear real arithmetics, and is released under the MIT license.", "num_citations": "37\n", "authors": ["1643"]}
{"title": "Decision procedures for flat array properties\n", "abstract": " We present new decidability results for quantified fragments of theories of arrays. Our decision procedures are fully declarative, parametric in the theories of indexes and elements and orthogonal with respect to known results. We also discuss applications to the analysis of programs handling arrays.", "num_citations": "33\n", "authors": ["1643"]}
{"title": "FunFrog: Bounded model checking with interpolation-based function summarization\n", "abstract": " This paper presents FunFrog, a tool that implements a function summarization approach for software bounded model checking. It uses interpolation-based function summaries as over-approximation of function calls. In every successful verification run, FunFrog generates function summaries of the analyzed program functions and reuses them to reduce the complexity of the successive verification. To prevent reporting spurious errors , the tool incorporates a counter-example-guided refinement loop. Experimental evaluation demonstrates competitiveness of FunFrog with respect to state-of-the-art software model checkers.", "num_citations": "33\n", "authors": ["1643"]}
{"title": "Leveraging interpolant strength in model checking\n", "abstract": " Craig interpolation is a well known method of abstraction successfully used in both hardware and software model checking. The logical strength of interpolants can affect the quality of approximations and consequently the performance of the model checkers. Recently, it was observed that for the same resolution proof a complete lattice of interpolants ordered by strength can be derived. Most state-of-the-art model checking techniques based on interpolation subject the interpolants to constraints that ensure efficient verification as, for example, in transition relation approximation for bounded model checking, counterexample-guided abstraction refinement and function summarization for software update checking. However, in general, these verification-specific constraints are not satisfied by all possible interpolants.               The paper analyzes the restrictions within the lattice of interpolants under which the\u00a0\u2026", "num_citations": "33\n", "authors": ["1643"]}
{"title": "Overview of ComFoRT: A model checking reasoning framework\n", "abstract": " Component technologies are gaining acceptance in the software community as effective tools to quickly assemble increasingly complex systems from components. Most of the current component technologies, however, fail to help developers predict important software qualities like performance, safety, and reliability. A prediction-enabled component technology PECT augments the capabilities of a component technology with one or more reasoning frameworks that package quality-specific analyses and the means to apply them to component-based systems. Model checking is an automated approach for exhaustively analyzing whether systems satisfy specific behavioral claims that can be used to characterize safety and reliability requirements. This technical note describes ComFoRT, a reasoning framework that packages the effectiveness of state-of-the-art model checking in a form that enables users to apply the analysis technique without being experts in its use, and its incorporation in a PECT.Descriptors:", "num_citations": "33\n", "authors": ["1643"]}
{"title": "Booster: An acceleration-based verification framework for array programs\n", "abstract": " We present Booster, a new framework developed for verifiying programs handling arrays. Booster integrates new acceleration features with standard verification techniques, like Lazy Abstraction with Interpolants (extended to arrays). The new acceleration features are the key for scaling-up in the verification of programs with arrays, allowing Booster to efficiently generate required quantified safe inductive invariants attesting the safety of the input code.", "num_citations": "31\n", "authors": ["1643"]}
{"title": "A scalable decision procedure for fixed-width bit-vectors\n", "abstract": " Efficient decision procedures for bit-vectors are essential for modern verification frameworks. This paper describes a new decision procedure for the core theory of bit-vectors that exploits a reduction to equality reasoning. The procedure is embedded in a congruence closure algorithm, whose data structures are extended in order to efficiently manage the relations between bit-vector slicings, modulo equivalence classes. The resulting procedure is incremental, backtrackable, and proof producing: it can be used as a theory-solver for a lazy SMT schema. Experiments show that our approach is comparable and often superior to bit-blasting on the core fragment, and that it also helps as a theory layer when applied over the full bit-vector theory.", "num_citations": "31\n", "authors": ["1643"]}
{"title": "PeRIPLO: A framework for producing effective interpolants in SAT-based software verification\n", "abstract": " Propositional interpolation is widely used as a means of overapproximation to achieve efficient SAT-based symbolic model checking. Different verification applications exploit interpolants for different purposes; it is unlikely that a single interpolation procedure could provide interpolants fit for all cases. This paper describes the PeRIPLO framework, an interpolating SAT-solver that implements a set of techniques to generate and manipulate interpolants for different model checking tasks. We demonstrate the flexibility of the framework in two software bounded model checking applications: verification of a given source code incrementally with respect to various properties, and verification of software upgrades with respect to a fixed set of properties. Both applications use interpolation for generating function summaries. Our systematic experimental investigation shows that size and logical strength of interpolants\u00a0\u2026", "num_citations": "30\n", "authors": ["1643"]}
{"title": "A model checking-based approach for security policy verification of mobile systems\n", "abstract": " This article describes an approach for the automated verification of mobile systems. Mobile systems are characterized by the explicit notion of location (e.g., sites where they run) and the ability to execute at different locations, yielding a number of security issues. To this aim, we formalize mobile systems as Labeled Kripke Structures, encapsulating the notion of location net that describes the hierarchical nesting of the threads constituting the system. Then, we formalize a generic security-policy specification language that includes rules for expressing and manipulating the code location. In contrast to many other approaches, our technique supports both access control and information flow specification. We developed a prototype framework for model checking of mobile systems. It works directly on the program code (in contrast to most traditional process-algebraic approaches that can model only limited details\u00a0\u2026", "num_citations": "29\n", "authors": ["1643"]}
{"title": "Verification-aided regression testing\n", "abstract": " In this paper we present Verification-Aided Regression Testing (VART), a novel extension of regression testing that uses model checking to increase the fault revealing capability of existing test suites. The key idea in VART is to extend the use of test case executions from the conventional direct fault discovery to the generation of behavioral properties specific to the upgrade, by (i) automatically producing properties that are proved to hold for the base version of a program,(ii) automatically identifying and checking on the upgraded program only the properties that, according to the developers\u2019 intention, must be preserved by the upgrade, and (iii) reporting the faults and the corresponding counter-examples that are not revealed by the regression tests. Our empirical study on both open source and industrial software systems shows that VART automatically produces properties that increase the effectiveness of testing by\u00a0\u2026", "num_citations": "27\n", "authors": ["1643"]}
{"title": "An efficient and flexible approach to resolution proof reduction\n", "abstract": " A resoution proof is a certificate of the unsatisfiability of a Boolean formula. Resolution proofs, as generated by modern SAT solvers, find application in many verification techniques. For efficiency smaller proofs are preferable over larger ones. This paper presents a new approach to proof reduction, situated among the purely post-processing methods. The main idea is to reduce the proof size by eliminating redundancies of occurrences of pivots along the proof paths. This is achieved by matching and rewriting local contexts into simpler ones. In our approach, rewriting can be easily customized in the way local contexts are matched, in the amount of transformations to be performed, or in the different application of the rewriting rules. We provide an extensive experimental evaluation of our technique on a set of benchmarks, which shows considerable reduction in the proofs size.", "num_citations": "27\n", "authors": ["1643"]}
{"title": "A framework for the verification of parameterized infinite-state systems\n", "abstract": " We present our framework for the verification of parameterized infinite-state systems. The framework has been successfully applied in the verification of heterogeneous systems, ranging from distributed fault-tolerant protocols to programs handling unbounded data-structures. In such application domains, being able to infer quantified invariants is a mandatory requirement for successful results. Our framework differentiates itself from the state-of-the-art solutions targeting the generation of quantified safe inductive invariants: instead of monolitically exploiting a single static analysis technique, it is based on the effective integration of several analysis strategies. The paper targets the description of the engineering strategies adopted for a successful implementation of such an integrated framework, and presents the extensive experimental evaluation demonstrating its effectiveness.", "num_citations": "25\n", "authors": ["1643"]}
{"title": "HiFrog: SMT-based function summarization for software verification\n", "abstract": " Function summarization can be used as a means of incremental verification based on the structure of the program. HiFrog is a fully featured function-summarization-based model checker that uses SMT as the modeling and summarization language. The tool supports three encoding precisions through SMT: uninterpreted functions, linear real arithmetics, and propositional logic. In addition the tool allows optimized traversal of reachability properties, counter-example-guided summary refinement, summary compression, and user-provided summaries. We describe the use of the tool through the description of its architecture and a rich set of features. The description is complemented by an experimental evaluation on the practical impact the different SMT precisions have on model-checking.", "num_citations": "19\n", "authors": ["1643"]}
{"title": "A proof-sensitive approach for small propositional interpolants\n", "abstract": " The labeled interpolation system (LIS) is a framework for Craig interpolation widely used in propositional-satisfiability-based model checking. Most LIS-based algorithms construct the interpolant from a proof of unsatisfiability and a fixed labeling determined by which part of the propositional formula is being over-approximated. While this results in interpolants with fixed strength, it limits the possibility of generating interpolants of small size. This is problematic since the interpolant size is a determining factor in achieving good overa performance in model checking. This paper analyses theoretically how labeling functions can be used to construct small interpolants. In addition to developing the new labeling mechanism guaranteeing small interpolants, we also present its versions managing the strength of the interpolants. We implement the labeling functions in our tool PeRIPLO and study the behavior of the\u00a0\u2026", "num_citations": "19\n", "authors": ["1643"]}
{"title": "Search-space partitioning for parallelizing SMT solvers\n", "abstract": " This paper studies how parallel computing can be used to reduce the time required to solve instances of the Satisfiability Modulo Theories problem (SMT). We address the problem in two orthogonal ways: (i) by distributing the computation using algorithm portfolios, search space partitioning techniques, and their combinations; and (ii) by studying the effect of partitioning heuristics, and in particular the lookahead heuristic, to the efficiency of the partitioning. We implemented the approaches in the OpenSMT2 solver and experimented with the QF_UF theory on a computing cloud. The results show a consistent speed-up on hard instances with up to an order of magnitude run time reduction and more instances being solved within the timeout compared to the sequential implementation.", "num_citations": "17\n", "authors": ["1643"]}
{"title": "Definability of accelerated relations in a theory of arrays and its applications\n", "abstract": " For some classes of guarded ground assignments for arrays, we show that accelerations (i.e. transitive closures) are definable in the theory of arrays via \u2203\u2009*\u2009\u2200\u2009*-first order formulae. We apply this result to model checking of unbounded array programs, where the computation of such accelerations can be used to prevent divergence of reachability analysis. To cope with nested quantifiers introduced by acceleration preprocessing, we use simple instantiation and refinement strategies during backward search analysis. Our new acceleration technique and abstraction/refinement loops are mutually beneficial: experiments conducted with the SMT-based model checker mcmt attest the effectiveness of our approach where acceleration and abstraction/refinement technologies fail if applied alone.", "num_citations": "17\n", "authors": ["1643"]}
{"title": "Flexible interpolation with local proof transformations\n", "abstract": " Model checking based on Craig's interpolants ultimately relies on efficient engines, such as SMT-Solvers, to log proofs of unsatisfiability and to derive the desired interpolant by means of a set of algorithms known in literature. These algorithms, however, are designed for proofs that do not contain mixed predicates. In this paper we present a technique for transforming the propositional proof produced by an SMT-Solver in such a way that mixed predicates are eliminated. We show a number of cases in which mixed predicates arise as a consequence of state-of-the-art solving procedures (e.g. lemma on demand, theory combination, etc.). In such cases our technique can be applied to allow the reuse of known interpolation algorithms. We demonstrate with a set of experiments that our approach is viable.", "num_citations": "17\n", "authors": ["1643"]}
{"title": "Lessons learned from model checking a NASA robot controller\n", "abstract": " This paper reports as a case study an attempt to model check the control subsystem of an operational NASA robotics system. Thirty seven properties including both safety and liveness specifications were formulated for the system. Twenty two of the thirty seven properties were successfully model checked. Several significant flaws in the original software system were identified and corrected during the model checking process. The case study presents the entire process in a semi-historical mode. The goal is to provide reusable knowledge of what worked, what did not work and why.", "num_citations": "17\n", "authors": ["1643"]}
{"title": "Resolution proof transformation for compression and interpolation\n", "abstract": " Verification methods based on SAT, SMT, and theorem proving often rely on proofs of unsatisfiability as a powerful tool to extract information in order to reduce the overall effort. For example a proof may be traversed to identify a minimal reason that led to unsatisfiability, for computing abstractions, or for deriving Craig interpolants. In this paper we focus on two important aspects that concern efficient handling of proofs of unsatisfiability: compression and manipulation. First of all, since the proof size can be very large in general (exponential in the size of the input problem), it is indeed beneficial to adopt techniques to compress it for further processing. Secondly, proofs can be manipulated as a flexible preprocessing step in preparation for interpolant computation. Both these techniques are implemented in a framework that makes use of local rewriting rules to transform the proofs. We show that a careful use of\u00a0\u2026", "num_citations": "16\n", "authors": ["1643"]}
{"title": "Modeling for verification\n", "abstract": " System modeling is the initial, and often crucial, step in verification. The right choice of model and modeling language is important for both designers and users of verification tools. This chapter aims to provide a guide to system modeling in four stages. First, it provides an overview of the main issues one must consider in modeling systems for verification. These issues involve both the selection or design of a modeling language and the steps of model creation. Next, it introduces a simple modeling language, sml, for illustrating the issues involved in selecting or designing a modeling language. sml uses an abstract state machine formalism that captures key features of widely-used languages based on transition system representations. We introduce the simple modeling language to simplify the connection between languages used by practitioners (such as Verilog, Simulink, or C) and various underlying\u00a0\u2026", "num_citations": "15\n", "authors": ["1643"]}
{"title": "Decision procedures for flat array properties\n", "abstract": " We present new decidability results for quantified fragments of theories of arrays. Our decision procedures are parametric in the theories of indexes and elements and orthogonal with respect to known results. We show that transitive closures (\u2019acceleratio\u2019) of relation expressing certain array updates produce formulas inside our fragment; this observation will be used to identify a class of programs handling arrays having decidable reachability problem.", "num_citations": "15\n", "authors": ["1643"]}
{"title": "Decomposing Farkas Interpolants.\n", "abstract": " Modern verification commonly models software with Boolean logic and a system of linear inequalities over reals and over-approximates the reachable states of the model with Craig interpolation to obtain, for example, candidates for inductive invariants. Interpolants for the linear system can be efficiently constructed from a Simplex refutation by applying the Farkas\u2019 lemma. However, this approach results in the worst case in incompleteness, since Farkas interpolants do not always suit the verification task. This work introduces the decomposed interpolants, a fundamental extension of the Farkas interpolants obtained by identifying and separating independent components from the interpolant structure using methods from linear algebra. We integrate our approach to the model checker Sally and show experimentally that a portfolio of decomposed interpolants results in immediate convergence on instances where state-of-the-art approaches diverge. Being based on the efficient Simplex method, the approach is very competitive also outside these diverging cases.", "num_citations": "13\n", "authors": ["1643"]}
{"title": "Clause sharing and partitioning for cloud-based SMT solving\n", "abstract": " Satisfiability modulo theories (SMT) allows the modeling and solving of constraint problems arising from practical domains by combining well-engineered and powerful solvers for propositional satisfiability with expressive, domain-specific background theories in a natural way. The increasing popularity of SMT as a modelling approach means that the SMT solvers need to handle increasingly complex problem instances. This paper studies how SMT solvers can use cloud computing to scale to challenging problems through sharing of learned information in the form of clauses with approaches based on both divide-and-conquer and algorithm portfolios. Our initial experiments, executed on the OpenSMT2 solver, show that parallelization with clause sharing speeds up the solving of instances, on average, by a factor of four or five depending on the problem domain.", "num_citations": "12\n", "authors": ["1643"]}
{"title": "Duality-based interpolation for quantifier-free equalities and uninterpreted functions\n", "abstract": " Interpolating, i.e., computing safe over-approximations for a system represented by a logical formula, is at the core of symbolic model-checking. One of the central tools in modeling programs is the use of the equality logic and uninterpreted functions (EUF), but certain aspects of its interpolation, such as size and the logical strength, are still relatively little studied. In this paper we present a solid framework for building compact, strength-controlled interpolants, prove its strength and size properties on EUF, implement and combine it with a propositional interpolation system and integrate the implementation into a model checker. We report encouraging results on using the interpolants both in a controlled setting and in the model checker. Based on the experimentation the presented techniques have potentially a big impact on the final interpolant size and the number of counter-example-guided refinements.", "num_citations": "10\n", "authors": ["1643"]}
{"title": "Symbolic detection of assertion dependencies for bounded model checking\n", "abstract": " Automatically generating assertions through static or runtime analysis is becoming an increasingly important initial phase in many software testing and verification tool chains. The analyses may generate thousands of redundant assertions often causing problems later in the chain, including scalability issues for automatic tools or a prohibitively large amount of information for final processing. We present an algorithm which uses a SAT solver on a bounded symbolic encoding of the program to reveal the implication relationships among spatially close assertions for use in a variety of bounded model checking applications. Our experimentation with different applications demonstrates that this technique can be used to reduce the number of assertions that need to be checked thus improving overall performance.", "num_citations": "10\n", "authors": ["1643"]}
{"title": "On interpolants and variable assignments\n", "abstract": " Craig interpolants are widely used in program verification as a means of abstraction. In this paper, we (i) introduce Partial Variable Assignment Interpolants (PVAIs) as a generalization of Craig interpolants. A variable assignment focuses computed interpolants by restricting the set of clauses taken into account during interpolation. PVAIs can be for example employed in the context of DAG interpolation, in order to prevent unwanted out-of-scope variables to appear in interpolants. Furthermore, we (ii) present a way to compute PVAIs for propositional logic based on an extension of the Labeled Interpolation Systems, and (iii) analyze the strength of computed interpolants and prove the conditions under which they have the path interpolation property.", "num_citations": "10\n", "authors": ["1643"]}
{"title": "Program compatibility approaches\n", "abstract": " This paper is a survey of several techniques that have proven useful in establishing compatibility among behaviorally similar programs (e.g., system upgrades, object sub- and supertypes, system components produced by different vendors, etc.). We give a comparative analysis of the techniques by evaluating their applicability to various aspects of the compatibility problem.", "num_citations": "10\n", "authors": ["1643"]}
{"title": "Accurate smart contract verification through direct modelling\n", "abstract": " Smart contracts challenge the existing, highly efficient techniques applied in symbolic model checking of software by their unique traits not present in standard programming models. Still, the majority of reported smart contract verification projects either reuse off-the-shelf model checking tools resulting in inefficient and even unsound models, or apply generic solutions that typically require highly-trained human intervention. In this paper, we present the solution adopted in the formal analysis engine of the official Solidity compiler. We focus on the accurate modeling of the central aspects of smart contracts. For that, we specify purpose-built rules defined in the expressive and highly automatable logic of constrained Horn clauses, which are readily supported by an effective solving infrastructure for establishing sound safety proofs or finite-length counterexamples. We evaluated our approach on an extensive set of smart\u00a0\u2026", "num_citations": "8\n", "authors": ["1643"]}
{"title": "LRA interpolants from no man\u2019s land\n", "abstract": " Interpolation is becoming a standard technique for over-approximating state spaces in software model checking with Satisfiability Modulo Theories (SMT). In particular when modelling programs with linear arithmetics, the standard state-of-the-art technique might provide either interpolants that are too specific or too generic to be useful for a given application. In this work we introduce the SI-LRA interpolation system for linear real arithmetics that allows the tuning of interpolants based on shifting between the primal and dual interpolants. We prove a strength relation between the interpolants constructed by SI-LRA, and integrate SI-LRA into a propositional interpolator in an SMT solver. Our evaluation, performed using a state-of-the-art software model checker, reveals that correct tuning with SI-LRA can reduce the number of needed refinements by up to one third and provide lower runtimes.", "num_citations": "8\n", "authors": ["1643"]}
{"title": "PVAIR: partial variable assignment InterpolatoR\n", "abstract": " Despite its recent popularity, program verification has to face practical limitations hindering its everyday use. One of these issues is scalability, both in terms of time and memory consumption. In this paper, we present Partial Variable Assignment InterpolatoR (PVAIR) \u2013 an interpolation tool exploiting partial variable assignments to significantly improve performance when computing several specialized Craig interpolants from a single proof. Subsequent interpolant processing during the verification process can thus be more efficient, improving scalability of the verification as such. We show with a wide range of experiments how our methods improve the interpolant computation in terms of their size. In particular, (i) we used benchmarks from the SAT competition and (ii) performed experiments in the domain of software upgrade checking.", "num_citations": "8\n", "authors": ["1643"]}
{"title": "Automated verification of security policies in mobile code\n", "abstract": " This paper describes an approach for the automated verification of mobile programs. Mobile systems are characterized by the explicit notion of locations (e.g., sites where they run) and the ability to execute at different locations, yielding a number of security issues. We give formal semantics to mobile systems as Labeled Kripke Structures, which encapsulate the notion of the location net. The location net summarizes the hierarchical nesting of threads constituting a mobile program and enables specifying security policies. We formalize a language for specifying security policies and show how mobile programs can be exhaustively analyzed against any given security policy by using model checking techniques.             We developed and experimented with a prototype framework for analysis of mobile code, using the SATABS model checker. Our approach relies on SATABS\u2019s support for unbounded thread\u00a0\u2026", "num_citations": "8\n", "authors": ["1643"]}
{"title": "SMTS: Distributed, Visualized Constraint Solving.\n", "abstract": " The inherent complexity of parallel computing makes development, resource monitoring, and debugging for parallel constraint-solving-based applications difficult. This paper presents SMTS, a framework for parallelizing sequential constraint solving algorithms and running them in distributed computing environments. The design (i) is based on a general parallelization technique that supports recursively combining algorithm portfolios and divide-and-conquer with the exchange of learned information,(ii) provides monitoring by visually inspecting the parallel execution steps, and (iii) supports interactive guidance of the algorithm through a web interface. We report positive experiences on instantiating the framework for one SMT solver and one IC3 solver, debugging parallel executions, and visualizing solving, structure, and learned clauses of SMT instances.", "num_citations": "7\n", "authors": ["1643"]}
{"title": "Theory refinement for program verification\n", "abstract": " Recent progress in automated formal verification is to a large degree due to the development of constraint languages that are sufficiently light-weight for reasoning but still expressive enough to prove properties of programs. Satisfiability modulo theories (SMT) solvers implement efficient decision procedures, but offer little direct support for adapting the constraint language to the task at hand. Theory refinement is a new approach that modularly adjusts the modeling precision based on the properties being verified through the use of combination of theories. We implement the approach using an augmented version of the theory of bit-vectors and uninterpreted functions capable of directly injecting non-clausal refinements to the inherent Boolean structure of SMT. In our comparison to a state-of-the-art model checker, our prototype implementation is in general competitive, being several orders of magnitudes\u00a0\u2026", "num_citations": "7\n", "authors": ["1643"]}
{"title": "Function summaries in software upgrade checking\n", "abstract": " We propose a new technique for checking of software upgrades based on an optimization of bounded model checking (BMC) with interpolation-based function summaries. In general, function summaries avoid duplicate actions during the verification process.We extract function summaries as an over-approximation of the actual function behavior after a successful model checker run and use it in the consecutive runs. It is useful in real life, when the same code gets analyzed multiple times for different properties. As a practical example of this situation, consider SLAM [1] which is used in a Static Driver Verifier to verify Windows device drivers. There the same code of the device driver is model checked repeatedly for different sets of predefined properties. In every run, function summaries could be generated and reused in the others to reduce the computational burden.", "num_citations": "7\n", "authors": ["1643"]}
{"title": "Model checking software via abstraction of loop transitions\n", "abstract": " This paper reports a data abstraction algorithm that is targeted to minimize the contribution of the loop executions to the program state space. The loop abstraction is defined as the syntactic program transformation that results in the sound representation of the concrete program. The abstraction algorithm is defined and implemented in the context of the integrated software design, testing and model checking. The loop abstraction technique was applied to verification of NASA robot control software. The abstraction enabled model checking for realistic robot configurations where all other state space reduction approaches, including BDD-based verification, predicate abstraction and partial order reduction, failed.", "num_citations": "7\n", "authors": ["1643"]}
{"title": "Flexible SAT-based framework for incremental bounded upgrade checking\n", "abstract": " Software undergoes a myriad of minor changes along its lifecycle. Each evolved transformation of a program is expected to preserve important correctness and security properties, in particular confirmed by a software model checking tool. However, it may be extremely resource- and time-consuming to repeat entire model checking for each new version of the program. As a possible solution to this problem, we propose to conduct incremental analysis of a new program version by reusing efforts of bounded model checking of the previous program version. Our approach maintains over-approximations of the bounded program behaviors by means of function summaries derived using Craig interpolation. For each new version, these summaries are used to localize the scope of model checking. A benefit of this approach is that the cost of the upgrade checking depends on the change impact between the two\u00a0\u2026", "num_citations": "6\n", "authors": ["1643"]}
{"title": "Using cross-entropy for satisfiability\n", "abstract": " This paper proposes a novel approach to SAT solving by using the cross-entropy method for optimization. It introduces an extension of the Boolean satisfiability setting to a multi-valued framework, where a probability space is induced over the set of all possible assignments. For a given formula, a cross-entropy-based algorithm (implemented in a tool named CROiSSANT) is used to find a satisfying assignment by applying an iterative procedure that optimizes an objective function correlated with the likelihood of satisfaction.", "num_citations": "6\n", "authors": ["1643"]}
{"title": "An abstraction refinement approach combining precise and approximated techniques\n", "abstract": " Predicate abstraction is a powerful technique to reduce the state space of a program to a finite and affordable number of states. It produces a conservative over-approximation where concrete states are grouped together according to a given set of predicates. A precise abstraction contains the minimal set of transitions with regard to the predicates, but as a result is computationally expensive. Most model checkers therefore approximate the abstraction to alleviate the computation of the abstract system by trading off precision with cost. However, approximation results in a higher number of refinement iterations, since it can produce more false counterexamples than its precise counterpart. The refinement loop can become prohibitively expensive for large programs. This paper proposes a new approach that employs both precise (slow) and approximated (fast) abstraction techniques within one abstraction\u00a0\u2026", "num_citations": "6\n", "authors": ["1643"]}
{"title": "A cooperative parallelization approach for property-directed k-induction\n", "abstract": " Recently presented, IC3-inspired symbolic model checking algorithms strengthen the procedure for showing inductiveness of lemmas expressing reachability of states. These approaches show an impressive performance gain in comparison to previous state-of-the-art, but also present new challenges to portfolio-based, lemma sharing parallelization as the solvers now store lemmas that serve different purposes. In this work we formalize this recent algorithm class for lemma sharing parallel portfolios using two central engines, one for checking inductiveness and the other for checking bounded reachability, and show when the respective engines can share their information. In our implementation based on the PD-KIND algorithm, the approach provides a consistent speed-up already in a multi-core environment, and surpasses in performance the winners of a recent solver competition by a comfortable margin.", "num_citations": "5\n", "authors": ["1643"]}
{"title": "Lookahead-based SMT solving\n", "abstract": " The lookahead approach for binary-tree-based search in constraint solving favors branching that provide the lowest upper bound for the remaining search space. The approach has recently been applied in instance partitioning in divide-and-conquer-based parallelization, but in general its connection to modern, clause-learning solvers is poorly understood. We show two ways of combining lookahead approach with a modern DPLL (T)-based SMT solver fully profiting from theory propagation, clause learning, and restarts. Our thoroughly tested prototype implementation is surprisingly efficient as an independent SMT solver on certain instances, in particular when applied to a non-convex theory, where the lookahead-based implementation solves 40% more unsatisfiable instances compared to the standard implementation.", "num_citations": "5\n", "authors": ["1643"]}
{"title": "A new acceleration-based combination framework for array properties\n", "abstract": " This paper presents an acceleration-based combination framework for checking the satisfiability of classes of quantified formulae of the theory of arrays. We identify sufficient conditions for which an \u2018acceleratability\u2019 result can be used as a black-box module inside such satisfiability procedures. Besides establishing new decidability results and relating them to results from recent literature, we discuss the application of our combination framework to the problem of checking the safety of imperative programs with arrays.", "num_citations": "5\n", "authors": ["1643"]}
{"title": "The synergy of precise and fast abstractions for program verification\n", "abstract": " Predicate abstraction is a powerful technique to reduce the state space of a program to a finite and affordable number of states. It produces a conservative over-approximation where concrete states are grouped together according to a given set of predicates. A precise abstraction contains the minimal set of transitions with regards to the predicates, but as a result is computationally expensive. Most model checkers therefore approximate the abstraction to alleviate the computation of the abstract system by trading off precision with cost. However, approximation results in a higher number of refinement iterations, since it can produce more false counterexamples than its precise counterpart. The refinement loop can become prohibitively expensive for large programs.", "num_citations": "5\n", "authors": ["1643"]}
{"title": "Function summarization modulo theories\n", "abstract": " SMT-based program verification can achieve high precision using bit-precise models or combinations of different theories. Often such approaches suffer from problems related to scalability due to the complexity of the underlying decision procedures. Precision is traded for performance by increasing the abstraction level of the model. As the level of abstraction increases, missing important details of the program model becomes problematic. In this paper we address this problem with an incremental verification approach that alternates precision of the program modules on demand. The idea is to model a program using the lightest possible (ie, less expensive) theories that suffice to verify the desired property. To this end, we employ safe over-approximations for the program based on both function summaries and light-weight SMT theories. If during verification it turns out that the precision is too low, our approach lazily strengthens all affected summaries or the theory through an iterative refinement procedure. The resulting summarization framework provides a natural and light-weight approach for carrying information between different theories. An experimental evaluation with a bounded model checker for C on a wide range of benchmarks demonstrates that our approach scales well, often effortlessly solving instances where the state-of-the-art model checker CBMC runs out of time or memory.", "num_citations": "4\n", "authors": ["1643"]}
{"title": "Towards completeness in bounded model checking through automatic recursion depth detection\n", "abstract": " The presence of recursive function calls is a well-known bottleneck in software model checking as they might cause infinite loops and make verification infeasible. This paper proposes a new technique for sound and complete Bounded Model Checking based on detecting depths for all recursive function calls in a program. The algorithm of detection of recursion depth uses over-approximations of function calls. It proceeds in an iterative manner by refining the function over-approximations until the recursion depth is detected or it becomes clear that the recursion depth detection is infeasible. We prove that if the algorithm terminates then it guarantees to detect a recursion depth required for complete program verification. The key advantage of the proposed algorithm is that it is suitable for generation and/or substitution of function summaries by means of Craig Interpolation helpful to speed up consequent\u00a0\u2026", "num_citations": "4\n", "authors": ["1643"]}
{"title": "Computer-Aided Verification\n", "abstract": " \u0423\u043c\u043e\u043c \u0420\u043e\u0441\u0441\u0438\u044e \u043d\u0435 \u043f\u043e\u043d\u044f\u0442\u044c, \u0410\u0440\u0448\u0438\u043d\u043e\u043c \u043e\u0431\u0449\u0438\u043c \u043d\u0435 \u0438\u0437\u043c\u0435\u0440\u0438\u0442\u044c: \u0423 \u043d\u0435\u0439 \u043e\u0441\u043e\u0431\u0435\u043d\u043d\u0430\u044f \u0441\u0442\u0430\u0442\u044c\u2014\u0412 \u0420\u043e\u0441\u0441\u0438\u044e \u043c\u043e\u0436\u043d\u043e \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u0435\u0440\u0438\u0442\u044c.", "num_citations": "3\n", "authors": ["1643"]}
{"title": "Reachability modulo theory library\n", "abstract": " Reachability analysis of infinite-state systems plays a central role in many verification tasks. In the last decade, SMT-Solvers have been exploited within many verification tools to discharge proof obligations arising from reachability analysis. Despite this, as of today there is no standard language to deal with transition systems specified in the SMT-LIB format. This paper is a first proposal for a new SMT-based verification language that is suitable for defining transition systems and safety properties.", "num_citations": "3\n", "authors": ["1643"]}
{"title": "Invariant generation by infinite-state model checking\n", "abstract": " Automatic invariant generation for programs handling unbounded data structures is a long-standing problem in software verification. To generate suitable invariants, the invariant generator needs to be able to infer quantified properties about data structures handled by the program, and at the same time, generate a formula strong enough (the invariant) to prove the correctness of the program, with respect to a given postcondition. In this paper we describe how an infinite-state model checker can be used to generate invariants for programs handling unbounded arrays.", "num_citations": "3\n", "authors": ["1643"]}
{"title": "A flexible schema for generating explanations in lazy theory propagation\n", "abstract": " Theory propagation in Satisfiability Modulo Theories is crucial for the solver's performance. It is important, however, to pay particular care to the amount of deductions to perform. The risk is in fact to clog the SAT-Solver with too many (and potentially useless clauses). In this paper we review some techniques for generating and communicating clauses to the SAT-Solver. In addition we propose a generic and flexible schema for theory propagation in which explanations for entailed facts are generated by re-using the consistency check procedure that is normally available in a theory solver. We argue that our schema can simplify the design of a theory solver, and allow a flexible form of theory propagation even for inherently hard theories (such as bit-vectors).", "num_citations": "3\n", "authors": ["1643"]}
{"title": "Dynamic component substitutability analysis\n", "abstract": " This paper presents an automated and compositional procedure to solve the substitutability problem in the context of evolving software systems. Our solution contributes two techniques for checking correctness of software upgrades: 1) a technique based on simultaneous use of over and under approximations obtained via existential and universal abstractions; 2) a dynamic assumeguarantee reasoning algorithm\u2013previously generated component assumptions are reused and altered on-the-fly to prove or disprove the global safety properties on the updated system. When upgrades are found to be non-substitutable our solution generates constructive feedback to developers showing how to improve the components. The substitutability approach has been implemented and validated in the COMFORT model checking tool set and we report encouraging results on an industrial benchmark.", "num_citations": "3\n", "authors": ["1643"]}
{"title": "Lattice-based refinement in bounded model checking\n", "abstract": " In this paper we present an algorithm for bounded model-checking with SMT solvers of programs with library functions\u2014either standard or user-defined. Typically, if the program correctness depends on the output of a library function, the model-checking process either treats this function as an uninterpreted function, or is required to use a theory under which the function in question is fully defined. The former approach leads to numerous spurious counter-examples, whereas the later faces the danger of the state-explosion problem, where the resulting formula is too large to be solved by means of modern SMT solvers.               We extend the approach of user-defined summaries and propose to represent the set of existing summaries for a given library function as a lattice of subsets of summaries, with the meet and join operations defined as intersection and union, respectively. The refinement process is then\u00a0\u2026", "num_citations": "2\n", "authors": ["1643"]}
{"title": "Visualising SMT-based parallel constraint solving\n", "abstract": " Problem instances arising from a multitude of applications can be naturally reduced to the search for a solution satisfying a set of constraints. Characteristically such applications, that include model checking and the satisfiability of propositional formulas (SAT) and SAT modulo theories, are notorious for the computational complexity of the underlying decision problem. A central approach for tackling the hardness of these problems is through the use of parallel computing, with methods such as divide-and-conquer, algorithm portfolios, and their combinations. However, such algorithms are often complicated and it is difficult to assess whether their executions have anomalies. This paper presents a user-friendly interface to analyze the partitioning tree parallelization approach that generalizes both divide-and-conquer and algorithm portfolios. We use the interface to analyze parallel executions of both the IC3/PDR algorithm and an SMT solver, demonstrating its usefulness in visualizing the otherwise complicated executions of diverse algorithms in a parallel environment. Based on the initial results we believe that the visualization of these executions will further encourage the adoption of parallel computing techniques in these domains.", "num_citations": "2\n", "authors": ["1643"]}
{"title": "Incremental upgrade checking\n", "abstract": " Software undergoes a myriad of small changes along its life-cycle. It may be extremely resource and time consuming to verify each new version from scratch. To remedy this problem, this chapter describes how to use function summaries to enable incremental verification of evolving systems. The approach maintains function summaries derived using Craig interpolation. For each new version, these summaries are used to perform a local incremental check. The cost of the check depends on the change impact between the two versions and can be performed cheaply for incremental changes without a need to re-verify the entire system. This chapter discusses the theory and implementation of the approach in the scope of the bounded model checker for C, eVolCheck. Our experimentation with eVolCheck confirms that incremental changes can be verified efficiently for different classes of industrial programs. The\u00a0\u2026", "num_citations": "2\n", "authors": ["1643"]}
{"title": "Monotonic abstraction techniques: from parametric to software model checking\n", "abstract": " Monotonic abstraction is a technique introduced in model checking parameterized distributed systems in order to cope with transitions containing global conditions within guards. The technique has been re-interpreted in a declarative setting in previous papers of ours and applied to the verification of fault tolerant systems under the so-called \"stopping failures\" model. The declarative reinterpretation consists in logical techniques (quantifier relativizations and, especially, quantifier instantiations) making sense in a broader context. In fact, we recently showed that such techniques can over-approximate array accelerations, so that they can be employed as a meaningful (and practically effective) component of CEGAR loops in software model checking too.", "num_citations": "2\n", "authors": ["1643"]}
{"title": "A parametric interpolation framework for first-order theories\n", "abstract": " Craig interpolation is successfully used in both hardware and software model checking. Generating good interpolants, and hence automatically determining the quality of interpolants is however a very hard problem, requiring non-trivial reasoning in first-order theories. An important class of state-of-the-art interpolation algorithms is based on recursive procedures that generate interpolants from refutations of unsatisfiable conjunctions of formulas. We analyze this type of algorithms and develop a theoretical framework, called a parametric interpolation framework, for arbitrary first-order theories and inference systems. As interpolation-based verification approaches depend on the quality of interpolants, our method can be used to derive interpolants of different structure and strength, with or without quantifiers, from the same proof. We show that some well-known interpolation algorithms are instantiations of our\u00a0\u2026", "num_citations": "2\n", "authors": ["1643"]}
{"title": "A uniform framework for predicate abstraction approximation\n", "abstract": " Abstraction refinement is a powerful technique that enables the verification of real systems. An initial coarse abstraction is provided and iteratively refined until either the property is proved to be true or false. Computing a precise abstraction is usually very expensive. Thus, many techniques have been conceived in order to approximate the abstract transition relation. In the framework of predicate abstraction, examples of such techniques are early quantification, Cartesian approximation, maximum cube length approximation, predicate partitioning, and interpolation-based approximation. When such approximations are employed, adding new predicates is no more sufficient to rule out all spurious counterexamples. Standard model checkers add new contraints to the transition relations in order to refine the approximation. We propose a uniform framework for describing most of known approximation techniques. The mapping among various techniques provides a conceptual basis for the development of new algorithms.", "num_citations": "2\n", "authors": ["1643"]}
{"title": "Theory-Specific Proof Steps Witnessing Correctness of SMT Executions\n", "abstract": " Ensuring hardware and software correctness increasingly relies on the use of symbolic logic solvers, in particular for satisfiability modulo theories (SMT). However, building efficient and correct SMT solvers is difficult: even state-of-the-art solvers disagree on instance satisfiability. This work presents a system for witnessing unsatisfiability of instances of NP problems, commonly appearing in verification, in a way that is natural to SMT solving. Our implementation of the system seems to often result in significantly smaller witnesses, lower solving overhead, and faster checking time in comparison to existing proof formats that can serve a similar purpose.", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Using linear algebra in decomposition of Farkas interpolants\n", "abstract": " The use of propositional logic and systems of linear inequalities over reals is a common means to model software for formal verification. Craig interpolants constitute a central building block in this setting for over-approximating reachable states, e.g. as candidates for inductive loop invariants. Interpolants for a linear system can be efficiently computed from a Simplex refutation by applying the Farkas\u2019 lemma. However, these interpolants do not always suit the verification task\u2014in the worst case, they can even prevent the verification algorithm from converging. This work introduces the decomposed interpolants, a fundamental extension of the Farkas interpolants, obtained by identifying and separating independent components from the interpolant structure, using methods from linear algebra. We also present an efficient polynomial algorithm to compute decomposed interpolants and analyse its properties. We\u00a0\u2026", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Farkas-based tree interpolation\n", "abstract": " Linear arithmetic over reals (LRA) underlies a wide range of SMT-based modeling approaches, and, strengthened with Craig interpolation using Farkas\u2019 lemma, is a central tool for efficient over-approximation. Recent advances in LRA interpolation have resulted in a range of promising interpolation algorithms with so far poorly understood properties. In this work we study the Farkas-based algorithms with respect to tree interpolation, a practically important approach where a set of interpolants is constructed following a given tree structure. We classify the algorithms based on whether they guarantee the tree interpolation property, and present how to lift a recently introduced approach producing conjunctive LRA interpolants to tree interpolation in the quantifier-free LRA fragment of first-order logic. Our experiments show that the standard interpolation and the approach using conjunctive interpolants are\u00a0\u2026", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Incremental verification by SMT-based summary repair\n", "abstract": " We present UPPROVER, a bounded model checker designed to incrementally verify software while it is being gradually developed, refactored, or optimized. In contrast to its predecessor, a SAT-based tool EVOLCHECK, our tool exploits rst-order theories available in SMT solvers, offering two more levels of encoding precision: linear arithmetic and uninterpreted functions, thus allowing a trade-off between precision and performance. Algorithmically UPPROVER is based on the reuse and repair of interpolation-based function summaries from one software version to another. UPPROVER leverages treeinterpolation systems in SMT to localize and speed up the checks of new versions. UPPROVER demonstrates an order of magnitude speedup on large-scale programs in comparison to EVOLCHECK and HIFROG, a non-incremental bounded model checker.", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Exploiting partial variable assignment in interpolation-based model checking\n", "abstract": " Craig interpolation has been successfully employed in symbolic program verification as a means of abstraction for sets of program states. In this article, we present the partial variable assignment interpolation system, an extension of the labeled interpolation system, enriched by partial variable assignments. It allows for both generation of smaller interpolants as well as for their faster computation. We present proofs of important properties of the interpolation system as well as a set of experiments proving its usefulness.", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Flexible interpolation for efficient model checking\n", "abstract": " Symbolic model checking is one of the most successful techniques for formal verification of software and hardware systems. Many model checking algorithms rely on over-approximating the reachable state space of the system. This task is critical since it not only greatly affects the efficiency of the verification but also whether the model-checking procedure terminates. This paper reports an implementation of an over-approximation tool based on first computing a propositional proof, then compressing the proof, and finally constructing the over-approximation using Craig interpolation. We give examples of how the system can be used in different domains and study the interaction between proof compression techniques and different interpolation algorithms based on a given proof. Our initial experimental results suggest that there is a non-trivial interaction between the Craig interpolation and the proof\u00a0\u2026", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Function Summarization-Based Bounded Model Checking\n", "abstract": " It is often the case that software needs to be verified against various specifications. During this process, the same parts of the program have to be modeled/verified multiple times, forcing the model checker to duplicate actions. To reduce the overall verification effort, this chapter describes a technique for extracting function summaries after a successful verification run with respect to some assertion, and then using them during subsequent verification runs of the other assertions. A well-known mechanism to compute over-approximations, called Craig interpolation, is used to create function summaries. A summary preserves the most relevant information used to prove a specific assertion, and thus tends to be a good substitute for the functions that were examined in the previous verification runs. As a side effect of the over-approximation, spurious behaviors might be introduced. In order to eliminate them\u00a0\u2026", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Optimizing Function Summaries Through Interpolation\n", "abstract": " Two of the static analysis techniques presented in this book use Craig interpolation, a means of over-approximation, to achieve efficient SAT-based symbolic model checking. Different verification applications exploit interpolants for different purposes; it is unlikely that a single interpolation procedure could provide interpolants fit for all cases. This chapter describes the PeRIPLO framework, an interpolating SAT solver that implements a set of techniques to generate and manipulate interpolants for different model-checking tasks. Even though the PeRIPLO framework can be used for many other purposes besides static analysis, we will use it to illustrate in an interesting way the benefits and challenges of Craig-interpolation-based Bounded Model Checking. We demonstrate the flexibility of the framework in two software bounded model checking applications described in Chaps.\u00a0                 5                                 and\u00a0\u2026", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Acceleration-based safety decision procedure for programs with arrays.\n", "abstract": " Acceleration-based Safety Decision Procedure for Programs with Arrays Page 1 Acceleration-based Safety Decision Procedure for Programs with Arrays F. Alberti1, S. Ghilardi2, N. Sharygina1 1University of Lugano, Switzerland 2 University of Milan, Italy LPAR-19 December 15, 2013 Talk based on results previously published at FroCoS 2013. Page 2 Context: decide the safety of programs with arrays procedure Find( a[L] , e ) { lI i = 0; lL while ( i < L \u2227 a[i] = e ) { i = i + 1; } lF assert ( \u2200x.(0 \u2264 x < i) \u2192 a[x] = e ); } F. Alberti Acceleration-based Safety Decision Procedure ... 1 / 8 Page 3 Context: decide the safety of programs with arrays procedure Find( a[L] , e ) { lI i = 0; lL while ( i < L \u2227 a[i] = e ) { i = i + 1; } lF assert ( \u2200x.(0 \u2264 x < i) \u2192 a[x] = e ); } Is this program safe? F. Alberti Acceleration-based Safety Decision Procedure ... 1 / 8 Page 4 Context: decide the safety of programs with arrays procedure Find( a[L] , e ) { lI i = 0; lL \u2026", "num_citations": "1\n", "authors": ["1643"]}
{"title": "An abstraction refinement approach combining precise and approximated techniques for efficient program verification: abstract for the invited talk\n", "abstract": " Predicate abstraction is a powerful technique to reduce the state space of a program to a finite and affordable number of states. It produces a conservative over-approximation where concrete states are grouped together according to the predicates. Given a set of predicates, a precise abstraction contains the minimal set of transitions, but as a result is computationally expensive. Most model checkers therefore approximate the abstraction to alleviate the computation of the abstract system by trading off precision with cost. However, approximation results in a higher number of refinement iterations, since it can produce more false counterexamples than its precise counterpart. The refinement loop can become prohibitively expensive for large programs.", "num_citations": "1\n", "authors": ["1643"]}
{"title": "OpenSMT 0.2 System Description\n", "abstract": " A feature of OpenSMT is to provide an automatic mechanisms to compute reasons for theory-deductions by means of the consistency check algorithm. This makes it easier the implementation of a new solver, as the solver is required just to detect deduction without having to provide an immediate explanation for them, and without storing any additional information. This mechanism enables the possibility of performing theory-propagation for inherently complex theories such as bit-vectors.", "num_citations": "1\n", "authors": ["1643"]}
{"title": "Modeling and Verification of Mobile Systems\n", "abstract": " This paper describes an approach for modeling and verification of mobile systems. Mobile systems are multi-threaded programs that are characterized by 1) the explicit notion of locations (eg, sites where they run), 2) the ability to create and execute (possibly infinite) threads at multiple locations (eg, sites), and 3) the capability to withstand network failures. We give formal semantics to mobile systems as Labeled Kripke Structures (LKSs), which encapsulate the notion of location and unbounded thread creation. This notation allows for the modeling of both data and communication structures of the multi-threaded systems and, thus, outperforms the traditional process algebra approach which captures only the communication behavior.We describe how mobile programs can be exhaustively analyzed by using model checking techniques. The LSKs are readily usable from within the SATABS toolset. SATABS implements the SAT-based counterexample-guided abstraction refinement framework (CEGAR for short) for ANSI-C programs, and supports verification of multi-threaded programs with unbounded thread creation. We are currently developing a front-end to SATABS that allows for languages with explicit location features, such as mobile agents.", "num_citations": "1\n", "authors": ["1643"]}