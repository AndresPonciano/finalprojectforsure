{"title": "Blockchain for the Internet of Things: A systematic literature review\n", "abstract": " In the Internet of Things (IoT) scenario, the block-chain and, in general, Peer-to-Peer approaches could play an important role in the development of decentralized and data-intensive applications running on billion of devices, preserving the privacy of the users. Our research goal is to understand whether the blockchain and Peer-to-Peer approaches can be employed to foster a decentralized and private-by-design IoT. As a first step in our research process, we conducted a Systematic Literature Review on the blockchain to gather knowledge on the current uses of this technology and to document its current degree of integrity, anonymity and adaptability. We found 18 use cases of blockchain in the literature. Four of these use cases are explicitly designed for IoT. We also found some use cases that are designed for a private-by-design data management. We also found several issues in the integrity, anonymity and\u00a0\u2026", "num_citations": "605\n", "authors": ["1314"]}
{"title": "Open Data Quality Measurement Framework: Definition and Application to Open Government Data\n", "abstract": " The diffusion of Open Government Data (OGD) in recent years kept a very fast pace. However, evidence from practitioners shows that disclosing data without proper quality control may jeopardize dataset reuse and negatively affect civic participation. Current approaches to the problem in literature lack a comprehensive theoretical framework. Moreover, most of the evaluations concentrate on open data platforms, rather than on datasets.In this work, we address these two limitations and set up a framework of indicators to measure the quality of Open Government Data on a series of data quality dimensions at most granular level of measurement. We validated the evaluation framework by applying it to compare two cases of Italian OGD datasets: an internationally recognized good example of OGD, with centralized disclosure and extensive data quality controls, and samples of OGD from decentralized data disclosure\u00a0\u2026", "num_citations": "274\n", "authors": ["1314"]}
{"title": "Peer to Peer for Privacy and Decentralization in the Internet of Things\n", "abstract": " In the Internet of Things (IoT) era new connected devices will spread highly sensitive personal data. Sending this type of data to centralized companies represents a serious risk for people's privacy, since economical or political interests could lead to an illegitimate use of personal information (as shown by Snowden's revelations). With the purpose of overcoming such status-quo, our research goal is to develop software systems according to the notion of decentralized private-by-design IoT. The basic idea is that data produced by personal IoT devices are safely stored in a distributed system whose design guarantees privacy, leaving to the people-the real data owners-the decision of which of them to share and with whom. To achieve this goal, a possible solution is to leverage the use of Peer-to-Peer storage networks in combination with the blockchain. However, such architecture, despite promising, embeds still\u00a0\u2026", "num_citations": "83\n", "authors": ["1314"]}
{"title": "Investigating Technical Debt Folklore-Shedding some light on technical debt opinion\n", "abstract": " We identified and organized a number of statements about technical debt (TD Folklore list) expressed by practitioners in online websites, blogs and published papers. We chose 14 statements and we evaluated them through two surveys (37 practitioners answered the questionnaires), ranking them by agreement and consensus. The statements most agreed with show that TD is an important factor in software project management and not simply another term for \u201cbad code\u201d. This study will help the research community in identifying folklore that can be translated into research questions to be investigated, thus targeting attempts to provide a scientific basis for TD management.", "num_citations": "69\n", "authors": ["1314"]}
{"title": "A Comparative Analysis of Software Reliability Growth Models using defects data of Closed and Open Source Software\n", "abstract": " The purpose of this study is to compare the fitting (goodness of fit) and prediction capability of eight Software Reliability Growth Models (SRGM) using fifty different failure Data sets. These data sets contain defect data collected from system test phase, operational phase (field defects) and Open Source Software (OSS) projects. The failure data are modelled by eight SRGM (Musa Okumoto, Inflection S-Shaped, Goel Okumoto, Delayed S-Shaped, Logistic, Gompertz, Yamada Exponential, and Generalized Goel Model). These models are chosen due to their prevalence among many software reliability models. The results can be summarized as follows: Fitting capability: Musa Okumoto fits all data sets, but all models fit all the OSS datasets. : Prediction capability: Musa Okumoto, Inflection S-Shaped and Goel Okumoto are the best predictors for industrial data sets, Gompertz and Yamada are the best predictors for OSS\u00a0\u2026", "num_citations": "54\n", "authors": ["1314"]}
{"title": "Linked Data approach for selection process automation in Systematic Reviews.\n", "abstract": " Background: a systematic review identifies, evaluates and synthesizes the available literature on a given topic using scientific and repeatable methodologies. The significant workload required and the subjectivity bias could affect results. Aim: semi-automate the selection process to reduce the amount of manual work needed and the consequent subjectivity bias. Method: extend and enrich the selection of primary studies using the existing technologies in the field of Linked Data and text mining. We define formally the selection process and we also develop a prototype that implements it. Finally, we conduct a case study that simulates the selection process of a systematic literature published in literature. Results: the process presented in this paper could reduce the work load of 20% with respect to the work load needed in the fully manually selection, with a recall of 100%. Conclusions: the extraction of knowledge from\u00a0\u2026", "num_citations": "48\n", "authors": ["1314"]}
{"title": "Definition, Implementation and Validation of Energy Code Smells: an Exploratory Study on an Embedded System\n", "abstract": " Optimizing software in terms of energy efficiency is one of the challenges that both research and industry will have to face in the next few years. We consider energy efficiency as a software product quality characteristic, to be improved through the refactoring of appropriate code pattern: the aim of this work is identifying those code patterns, hereby defined as Energy Code Smells, that might increase the impact of software over power consumption. For our purposes, we perform an experiment consisting in the execution of several code patterns on an embedded system. These code patterns are executed in two versions: the first one contains a code issue that could negatively impact power consumption, the other one is refac-tored removing the issue. We measure the power consumption of the embedded device during the execution of each code pattern. We also track the execution time to investigate whether Energy Code Smells are also Performance Smells. Our results show that some Energy Code Smells actually have an impact over power consumption in the magnitude order of micro Watts. Moreover, those Smells did not introduce a performance decrease.", "num_citations": "39\n", "authors": ["1314"]}
{"title": "An Empirical Validation of FindBugs Issues Related to Defects\n", "abstract": " Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with\u00a0\u2026", "num_citations": "38\n", "authors": ["1314"]}
{"title": "Understanding green software development: A conceptual framework\n", "abstract": " The energy efficiency of IT has become one of the hottest topics in the last few years. The problem has been typically addressed by hardware manufacturers and designers, but recently the attention of industry and academia has shifted to the role of software for IT sustainability. Writing energy-efficient software is one of the most challenging issues in this area, because it requires not only a change of mindset for software developers and designers but also models and tools to measure and reduce the effect of software on the energy consumption of the underlying hardware. In this article, the authors present a conceptual framework that provides a unifying view of the strategies, models, and tools available so far for designing and developing greener software.", "num_citations": "37\n", "authors": ["1314"]}
{"title": "On the Integration of Knowledge Graphs into Deep Learning Models for a More Comprehensible AI\u2014Three Challenges for Future Research\n", "abstract": " Deep learning models contributed to reaching unprecedented results in prediction and classification tasks of Artificial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a specific result was achieved. In contexts where the impact of AI on human life is relevant (eg, recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is-or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artificial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI\u2014while being less flexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three specific challenges for future research\u2014knowledge matching, cross-disciplinary explanations and interactive explanations. View Full-Text", "num_citations": "23\n", "authors": ["1314"]}
{"title": "Profiling power consumption on desktop computer systems\n", "abstract": " Background. Energy awareness in the ICT has become an important issue: ICT is both a key player in energy efficiency, and a power drainer. Focusing on software, recent work suggested the existence of a relationship between power consumption, software configuration and usage patterns in computer systems.                                Aim. The aim of this work was collecting and analysing power consumption data of a general-purpose computer system, simulating common usage scenarios, in order to extract a power consumption profile for each scenario.                                Methods. We selected a desktop system running Windows XP as a test machine. Meanwhile, we developed 11 usage scenarios, classified by their functionality, and automated by a GUI testing tool. Then, we conducted several test runs of the scenarios, collecting power consumption data by means of a power meter\u00a0\u2026", "num_citations": "22\n", "authors": ["1314"]}
{"title": "Medication adherence to tyrosine kinase inhibitors: 2-year analysis of medication adherence to imatinib treatment for chronic myeloid leukemia and correlation with the depth of\u00a0\u2026\n", "abstract": " Objective: Adherence to tyrosine kinase inhibitor treatment is a significant factor in the achievement of a good clinical response in chronic myeloid leukemia (CML). The aim of this retrospective study is to investigate 1-and 2-year medication adherence to imatinib treatment, linking adherence rates with the clinical outcome, in accordance with European LeukemiaNet Recommendations for the management of CML. We have tried to find a cutoff value for adherence in order to achieve a good clinical outcome. Methods: The method used to calculate medication adherence was the ratio between the received and the prescribed daily dose. Results: We observed the levels of mean adherence for each of the following response groups (in years 1 and 2, respectively): complete response (0.96, 0.95), MR4. 5 (1.00,-), MR4 (0.93, 0.91), major molecular responses (0.96, 0.97), warning (0.91, 0.89) and failure (0.79, 0.84\u00a0\u2026", "num_citations": "21\n", "authors": ["1314"]}
{"title": "Exploratory testing as a source of technical debt\n", "abstract": " Practitioners generally view exploratory testing (ET) as a cost-effective substitute for their daily testing activities. However, empirical evidence reported in the literature gives a more comprehensive picture of ET that considers its technical debt implications.", "num_citations": "21\n", "authors": ["1314"]}
{"title": "Comparing reuse practices in two large software-producing companies\n", "abstract": " ContextReuse can improve productivity and maintainability in software development. Research has proposed a wide range of methods and techniques. Are these successfully adopted in practice?ObjectiveWe propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies.MethodWe compare and interpret the study results with a focus on reuse practices, effects, and context.ResultsBoth companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. Reusable entities are retrieved from a central repository, if present. Otherwise, direct communication with trusted colleagues is crucial for access.Reuse processes remain implicit and reflect the development style. In a homogeneous infrastructure-supported context, participants strongly agreed on higher development pace and less maintenance\u00a0\u2026", "num_citations": "19\n", "authors": ["1314"]}
{"title": "An Exploratory Study on Technology Transfer in Software Engineering\n", "abstract": " Background: Technology transfer is one key to the success of research projects, especially in Software Engineering, where the (practical) impact of the outcome may depend not only on the reliability and feasibility of technologies, but also on their applicability to industrial settings. However, there is limited knowledge on the current state of practice and how to assess the success of technology transfer. Objective: We aim at elaborating a set of hypotheses on how technology transfer takes place in Software Engineering research projects. Method: We designed an exploratory survey with the participants of two large research projects in Germany, which involve both industrial and academic partners in the area of model driven development for embedded systems. Results: Base on the extracted respondents answers of this survey, we defined a resulting theory which is based on the following set of main hypothesis: Most\u00a0\u2026", "num_citations": "19\n", "authors": ["1314"]}
{"title": "Selecting the Best Reliability Model to Predict Residual Defects in Open Source Software\n", "abstract": " A proposed method evaluates eight popular software reliability growth models and selects the one that can best predict the software's remaining faults, providing practical support for project managers who are considering an open source component.", "num_citations": "19\n", "authors": ["1314"]}
{"title": "On the impact of passive voice requirements on domain modelling\n", "abstract": " Context: The requirements specification is a central artefact in the software engineering (SE) process, and its quality (might) influence downstream activities like implementation or testing. One quality defect that is often mentioned in standards is the use of passive voice. However, the consequences of this defect are still unclear. Goal: We need to understand whether the use of passive voice in requirements has an influence on other activities in SE. In this work we focus on domain modelling. Method: We designed an experiment, in which we ask students to draw a domain model from a given set of requirements written in active or passive voice. We compared the completeness of the resulting domain model by counting the number of missing actors, domain objects and their associations with respect to a specified solution. Results: While we could not see a difference in the number of missing actors and objects\u00a0\u2026", "num_citations": "19\n", "authors": ["1314"]}
{"title": "The CLoTH Simulator for HTLC Payment Networks with Introductory Lightning Network Performance Results\n", "abstract": " The Lightning Network (LN) is one of the most promising off-chain scaling solutions for Bitcoin, as it enables off-chain payments which are not subject to the well-known blockchain scalability limit. In this work, we introduce CLoTH, a simulator for HTLC payment networks (of which LN is the best working example). It simulates input-defined payments on an input-defined HTLC network and produces performance measures in terms of payment-related statistics (such as time to complete payments and probability of payment failure). CLoTH helps to predict issues and obstacles that might emerge in the development stages of an HTLC payment network and to estimate the effects of an optimisation action before deploying it. We conducted simulations on a recent snapshot of the HTLC payment network of LN. These simulations allowed us to identify network and payments configurations for which a payment is more likely to fail than to succeed. We proposed viable solutions to avoid such configurations. View Full-Text", "num_citations": "18\n", "authors": ["1314"]}
{"title": "Bridging the gap: SE technology transfer into practice: study design and preliminary results\n", "abstract": " Background: Particularly during and after research projects, technology transfer into practice plays an important role for academia to get technologies into use and for industry to improve their development. Objective: Our goal was to gain more and current knowledge about how technology transfer from software engineering (SE) research into industrial practice is accomplished best and how to measure the effectiveness of this transfer. Method: We conducted a study in the context of two German research projects, covering many different organizations from industry and academia. Results: This paper presents the design of the study and the survey performed. After introducing the concept of technology transfer we used and adapted, we present preliminary results. Conclusions: We observed that traditional means such as meetings or workshops are still the most widely used mediums for technology transfer in SE. We\u00a0\u2026", "num_citations": "16\n", "authors": ["1314"]}
{"title": "Hubs, Rebalancing and Service Providers in the Lightning Network\n", "abstract": " Payment channel networks are the most developed proposal to address the well-known issue of blockchain scalability. Currently, the Lightning Network (LN) is the mainstream and most used payment channel network. In a previous work we introduced CLoTH, a payment channel network simulator we developed to analyze capabilities and limitations of such networks. In this work, using CLoTH, we present results of three groups of simulations on a recent snapshot of the LN, aimed to shed a light on the following aspects. Firstly, we investigated how hubs influence the LN performance. Then, we analyzed the effectiveness of two different channel rebalancing approaches, an active and a passive one. Eventually, we studied performance of the LN when a few service-providers nodes receive payments from the other network nodes, which is a typical use case of payment channel networks. We found that the LN is\u00a0\u2026", "num_citations": "13\n", "authors": ["1314"]}
{"title": "Preserving the benefits of Open Government Data by measuring and improving their quality: an empirical study\n", "abstract": " Context: Open Government Data (OGD) represent an invaluable resource for enabling active citizenship. A significant example is represented by the mandatory data that Italian public administrations (PAs) are required to publish concerning their contracts. Nevertheless, a low quality of data provided by PAs could hamper the prospect of citizen involvement. Goal: Our objective is to define a set of basic metrics for public contracts OGD on the basis of the ISO SQuaRE standards family, with the goal of enabling the automated evaluation of dataset quality. Method: We started with the metrics defined in the ISO 25024 standard and adapted them to the data schema of the OGD under evaluation. We assessed the results by looking at the issues revealed by the metrics applied to the data released by a pool of PAs. Results: We were able to define a set of metrics, and apply them to to the datasets released by 12 distinct\u00a0\u2026", "num_citations": "13\n", "authors": ["1314"]}
{"title": "In Quest for proper Mediums for Technology Transfer in Software Engineering\n", "abstract": " Background: Successful transfer of the results of research projects into practice is of great interest to all project participants. It can be assumed that different transfer mediums fulfill technology transfer (TT) with different levels of success and that they are impaired by different kinds of barriers. Objective: The goal of this study is to gain a better understanding about the different mediums used for TT in software engineering, and to identify barriers weakening the success of the application of such mediums. Method: We conducted an exploratory study implemented by a survey in the context of a German research project with a broad range of used mediums. Results: The main reported barriers were low expectations of usefulness, no awareness of existence, lack of resources, or inadequateness in terms of outdated material or being in an immature state. Conclusions: We interpreted our results as symptoms of a lack of a\u00a0\u2026", "num_citations": "13\n", "authors": ["1314"]}
{"title": "Using Automatic Static Analysis to Identify Technical Debt\n", "abstract": " The technical debt (TD) metaphor describes a tradeoff between short-term and long-term goals in software development. Developers, in such situations, accept compromises in one dimension (e.g. maintainability) to meet an urgent demand in another dimension (e.g. delivering a release on time). Since TD produces interests in terms of time spent to correct the code and accomplish quality goals, accumulation of TD in software systems is dangerous because it could lead to more difficult and expensive maintenance. The research presented in this paper is focused on the usage of automatic static analysis to identify Technical Debt at code level with respect to different quality dimensions. The methodological approach is that of Empirical Software Engineering and both past and current achieved results are presented, focusing on functionality, efficiency and maintainability.", "num_citations": "13\n", "authors": ["1314"]}
{"title": "Energy Efficiency in the ICT-Profiling Power Consumption in Desktop Computer Systems\n", "abstract": " Energy efficiency is finally becoming a mainstream goal in a limited world where consumption of resources cannot grow forever. ICT is both a key player in energy efficiency, and a power drainer. The Climate Group reported that the total footprint of the ICT sector was 830 MtCO2e and that the ICT was responsible for 2% of global carbon emissions [13]. Even if energy efficient IT technologies were developed and implemented, this figure would still grow up at a rate of 6% per year until 2020. Recently, much of the attention in green IT discussions focuses on data centers. However, it is foreseen that data centers will only add up to less than 20 percent of the total emissions of ICT in 2020. The majority (57 percent) will come from PCs, peripherals, and printers, as shown in Figure 1 [13].", "num_citations": "12\n", "authors": ["1314"]}
{"title": "AI: from rational agents to socially responsible agents\n", "abstract": " PurposeThis paper aims to analyze the limitations of the mainstream definition of artificial intelligence (AI) as a rational agent, which currently drives the development of most AI systems. The authors advocate the need of a wider range of driving ethical principles for designing more socially responsible AI agents.Design/methodology/approachThe authors follow an experience-based line of reasoning by argument to identify the limitations of the mainstream definition of AI, which is based on the concept of rational agents that select, among their designed actions, those which produce the maximum expected utility in the environment in which they operate. The problem of biases in the data used by AI is taken as example, and a small proof of concept with real datasets is provided.FindingsThe authors observe that biases measurements on the datasets are sufficient to demonstrate potential risks of discriminations when\u00a0\u2026", "num_citations": "11\n", "authors": ["1314"]}
{"title": "Ethical and Socially-Aware Data Labels\n", "abstract": " Many software systems today make use of large amount of personal data to make recommendations or decisions that affect our daily lives. These software systems generally operate without guarantees of non-discriminatory practices, as instead often required to human decision-makers, and therefore are attracting increasing scrutiny. Our research is focused on the specific problem of biased software-based decisions caused from biased input data. In this regard, we propose a data labeling framework based on the identification of measurable data characteristics that could lead to downstream discriminating effects. We test the proposed framework on a real dataset, which allowed us to detect risks of discrimination for the case of population groups.", "num_citations": "11\n", "authors": ["1314"]}
{"title": "Removing barriers to transparency: A case study on the use of semantic technologies to tackle procurement data inconsistency\n", "abstract": " Public Procurement (PP) information, made available as Open Government Data (OGD), leads to tangible benefits to identify government spending for goods and services. Nevertheless, making data freely available is a necessary, but not sufficient condition for improving transparency. Fragmentation of OGD due to diverse processes adopted by different administrations and inconsistency within data affect opportunities to obtain valuable information. In this article, we propose a solution based on linked data to integrate existing datasets and to enhance information coherence. We present an application of such principles through a semantic layer built on Italian PP information available as OGD. As result, we overcame the fragmentation of datasources and increased the consistency of information, enabling new opportunities for analyzing data to fight corruption and for raising competition between companies\u00a0\u2026", "num_citations": "11\n", "authors": ["1314"]}
{"title": "On the Benefits and Barriers When Adopting Software Modelling and Model Driven Techniques-An External, Differentiated Replication\n", "abstract": " Context: Applying model driven techniques can lead to several benefits, but their adoption entails also numerous issues. Goal: We aim at understanding the benefits and barriers on the adoption of the modelling techniques for embedded systems developed in a large German research project. Method: We replicate a survey conducted in the Italian industry about relevance, benefits, and problems of software modelling and model driven techniques. Results: With respect to the original study, we could confirm design support and quality of software as achieved benefits. On the side of the barriers, too much effort required, lack of competencies and lack of supporting tools were confirmed. Other barriers were confirmed as not having an impact: refusal from management, cost of supporting tool, fear of lock-in. Conclusions: We observed that even for the not mature modelling techniques in our context of study, a few\u00a0\u2026", "num_citations": "11\n", "authors": ["1314"]}
{"title": "Fast Feedback Sycles in Empirical Software Engineering Research\n", "abstract": " Background/Context: Gathering empirical knowledge is a time consuming task and the results from empirical studies often are soon outdated by new technological solutions. As a result, the impact of empirical results on software engineering practice is often not guaranteed.Objective/Aim: In this paper, we summarise the ongoing discussion on \"Empirical Software Engineering 2.0\" as a way to improve the impact of empirical results on industrial practices. We propose a way to combine data mining and analysis with domain knowledge to enable fast feedback cycles in empirical software engineering research.Method: We identify the key concepts on gathering fast feedback in empirical software engineering by following an experience-based line of reasoning by argument. Based on the identified key concepts, we design and execute a small proof of concept with a company to demonstrate potential benefits of the\u00a0\u2026", "num_citations": "11\n", "authors": ["1314"]}
{"title": "Language Interaction and Quality Issues: An Exploratory Study\n", "abstract": " Most software systems are complex and composed of a large number of artifacts. To realize each different artifact specific techniques are used resorting to different abstractions, languages and tools. Successful composition of different elements requires coherence among them. Unfortunately constraints between artifacts written in different languages are usually not formally expressed nor checked by supporting tools; as a consequence they can be a source of problems. In this paper we explore the role of the relations between artifacts written in different languages by means of a case study on the Hadoop open source project. We present the problem introducing its terminology, we quantify the phenomenon and investigate its relation with defect proneness.", "num_citations": "11\n", "authors": ["1314"]}
{"title": "The invisible power of fairness. How machine learning shapes democracy\n", "abstract": " Many machine learning systems make extensive use of large amounts of data regarding human behaviors. Several researchers have found various discriminatory practices related to the use of human-related machine learning systems, for example in the field of criminal justice, credit scoring and advertising. Fair machine learning is therefore emerging as a new field of study to mitigate biases that are inadvertently incorporated into algorithms. Data scientists and computer engineers are making various efforts to provide definitions of fairness. In this paper, we provide an overview of the most widespread definitions of fairness in the field of machine learning, arguing that the ideas highlighting each formalization are closely related to different ideas of justice and to different interpretations of democracy embedded in our culture. This work intends to analyze the definitions of fairness that have been proposed to\u00a0\u2026", "num_citations": "9\n", "authors": ["1314"]}
{"title": "Semantic Enrichment for Recommendation of Primary Studies in a Systematic Literature Review\n", "abstract": " A Systematic Literature Review (SLR) identifies, evaluates, and synthesizes the literature available for a given topic. This generally requires a significant human workload and has subjectivity bias that could affect the results of such a review. Automated document classification can be a valuable tool for recommending the selection of studies. In this article, we propose an automated pre-selection approach based on text mining and semantic enrichment techniques. Each document is firstly processed by a named entity extractor. The DBpedia URIs coming from the entity linking process are used as external sources of information. Our system collects the bag of words of those sources and it adds them to the initial document. A Multinomial Naive Bayes classifier discriminates whether the enriched document belongs to the positive example set or not. We used an existing manually performed SLR as benchmark data\u00a0\u2026", "num_citations": "9\n", "authors": ["1314"]}
{"title": "In quest for requirements engineering oracles: dependent variables and measurements for (good) RE\n", "abstract": " Context: For many years, researchers and practitioners have been proposing various methods and approaches to Requirements Engineering (RE). Those contributions remain, however, too often on the level of apodictic discussions without having proper knowledge about the practical problems they propagate to address, or how to measure the success of the contributions when applying them in practical contexts. While the scientific impact of research might not be threatened, the practical impact of the contributions is. Aim: We aim at better understanding practically relevant variables in RE, how those variables relate to each other, and to what extent we can measure those variables. This allows for the establishment of generalisable improvement goals, and the measurement of success of solution proposals. Method: We establish a first empirical basis of dependent variables in RE and means for their measurement\u00a0\u2026", "num_citations": "9\n", "authors": ["1314"]}
{"title": "Monitoring IT Power Consumption in a Research Center: Seven Facts\n", "abstract": " We analyze the power consumption of several IT devices placed in a research center affiliated to our University. The data collection lasted about one year and the analysis let us identify: i) the average instant power consumption of each type of device ii) trends of the instant power consumption curves iii) usage profiles and their power consumption iv) energy savings obtained from a different use of resources. Our main finding is that software and usage typology could affect power consumption more than hardware.", "num_citations": "9\n", "authors": ["1314"]}
{"title": "Introducing Energy Efficiency into SQALE\n", "abstract": " Energy Efficiency is becoming a key factor in software development, given the sharp growth of IT systems and their impact on worldwide energy consumption. We do believe that a quality process infrastructure should be able to consider the Energy Efficiency of a system since its early development: for this reason we propose to introduce Energy Efficiency into the existing quality models. We selected the SQALE model and we tailored it inserting Energy Efficiency as a sub-characteristic of efficiency. We also propose a set of six source code specific requirements for the Java language starting from guidelines currently suggested in the literature. We experienced two major challenges: the identification of measurable, automatically detectable requirements, and the lack of empirical validation on the guidelines currently present in the literature and in the industrial state of the practice as well. We describe an experiment plan to validate the six requirements and evaluate the impact of their violation on Energy Efficiency, which has been partially proved by preliminary results on C code. Having Energy Efficiency in a quality model and well verified code requirements to measure it, will enable a quality process that precisely assesses and monitors the impact of software on energy consumption.", "num_citations": "6\n", "authors": ["1314"]}
{"title": "Classification of Language Interactions.\n", "abstract": " Context: the presence of several languages interacting each other within the same project is an almost universal feature in software development. Earlier work shows that this interaction might be source of problems. Objective: we aim at identifying and characterizing the cross-language interactions at semantic level. Method: we took the commits of an open source project and analyzed the cross-language pairs of files occurring in the same commit to identify possible semantic interactions. We both defined a taxonomy and applied it. Result: we identified 6 categories of semantic interactions. The most common category is the one based on shared ids, the next is when an artifact provides a description of another artifact. Conclusion: the deeper knowledge of cross-language interactions represents the basis for implementing a tool supporting the management of this kind of interactions and the detection of related\u00a0\u2026", "num_citations": "6\n", "authors": ["1314"]}
{"title": "A Recommender System for Telecom Users: Experimental Evaluation of Recommendation Algorithms\n", "abstract": " The increasing flourish of available services in telecom domain offers more choices to the end user. On the other hand, such wide offer cannot be completely evaluated by the user, and some services may pass unobserved even if useful. To face this issue, the usage of recommendation systems in telecom domain is growing, to directly notify the user about the presence of services which may meet user interests. Recommendation can be seen as an advanced form of personalization, because user preferences are used to predict the interests of users for a new service. In this paper we propose a recommender system for users of telecom services, based on different collaborative filtering algorithms applied to a complex data-set of telecom users. Experiments on the recommendation performance and accuracy are conducted to test the different effects of different algorithms on a data set coming from the telecom domain.", "num_citations": "5\n", "authors": ["1314"]}
{"title": "Detecting discriminatory risk through data annotation based on Bayesian inferences\n", "abstract": " Thanks to the increasing growth of computational power and data availability, the research in machine learning has advanced with tremendous rapidity. Nowadays, the majority of automatic decision making systems are based on data. However, it is well known that machine learning systems can present problematic results if they are built on partial or incomplete data. In fact, in recent years several studies have found a convergence of issues related to the ethics and transparency of these systems in the process of data collection and how they are recorded. Although the process of rigorous data collection and analysis is fundamental in the model design, this step is still largely overlooked by the machine learning community. For this reason, we propose a method of data annotation based on Bayesian statistical inference that aims to warn about the risk of discriminatory results of a given data set. In particular, our\u00a0\u2026", "num_citations": "4\n", "authors": ["1314"]}
{"title": "SeMi: A SEmantic Modeling machIne to build Knowledge Graphs with graph neural networks\n", "abstract": " SeMi (SEmantic Modeling machIne) is a tool to semi-automatically build large-scale Knowledge Graphs from structured sources such as CSV, JSON, and XML files. To achieve such a goal, SeMi builds the semantic models of the data sources, in terms of concepts and relations within a domain ontology. Most of the research contributions on automatic semantic modeling is focused on the detection of semantic types of source attributes. However, the inference of the correct semantic relations between these attributes is critical to reconstruct the precise meaning of the data. SeMi covers the entire process of semantic modeling: (i) it provides a semi-automatic step to detect semantic types; (ii) it exploits a novel approach to inference semantic relations, based on a graph neural network trained on background linked data. At the best of our knowledge, this is the first technique that exploits a graph neural network to support\u00a0\u2026", "num_citations": "4\n", "authors": ["1314"]}
{"title": "Combining data analytics and developers feedback for identifying reasons of inaccurate estimations in agile software development\n", "abstract": " Background: Effort estimations are critical tasks greatly influencing the accomplishment of software projects. Despite their recognized relevance, little is yet known what indicators for inaccurate estimations exist, and which are the reasons of inaccurate estimations.Aims: In this manuscript, we aim at contributing to this existing gap. To this end, we implemented a tool that combines data analytics and developers\u2019 feedback, and we employed that tool in a study. In that study, we explored the most common reasons of inaccurate user story estimations and the possible indicators of inaccurate estimations.Method: We relied on a mixed method approach used to study reasons and indicators for the identification and prediction of inaccurate estimations in practical agile software development contexts.Results: Our results add to the existing body of knowledge in multiple ways. We elaborate causes for inaccurate estimations\u00a0\u2026", "num_citations": "4\n", "authors": ["1314"]}
{"title": "OpenCoesione and Monithon-a Transparency Effort\n", "abstract": " ObjectiveOur goal is to show the utility of these portals, how this open information are helping the civil society and how the quality of the data is managed publishing these data.", "num_citations": "4\n", "authors": ["1314"]}
{"title": "Training Neural Language Models with SPARQL queries for Semi-Automatic Semantic Mapping\n", "abstract": " Knowledge graphs are labeled and directed multi-graphs that encode information in the form of entities and relationships. They are gaining attention in different areas of computer science: from the improvement of search engines to the development of virtual personal assistants. Currently, an open challenge in building large-scale knowledge graphs from structured data available on the Web (HTML tables, CSVs, JSONs) is the semantic integration of heterogeneous data sources. In fact, such diverse and scattered information rarely provide a formal description of metadata that is required to accomplish the integration task. In this paper we propose an approach based on neural networks to reconstruct the semantics of data sources to produce high quality knowledge graphs in terms of semantic accuracy. We developed a neural language model trained on a set of SPARQL queries performed on knowledge graphs\u00a0\u2026", "num_citations": "3\n", "authors": ["1314"]}
{"title": "Technology Transfer Concepts\n", "abstract": " In software engineering, transferring innovative concepts, techniques and methods into the practice of existing organizations is an expensive and complex task. This chapter gives an overview on the transfer of the SPES XT modeling framework to different organization.", "num_citations": "2\n", "authors": ["1314"]}
{"title": "Identifying Risks in Datasets for Automated Decision\u2013Making\n", "abstract": " Our daily life is profoundly affected by the adoption of automated decision making (ADM) systems due to the ongoing tendency of humans to delegate machines to take decisions. The unleashed usage of ADM systems was facilitated by the availability of large-scale data, alongside with the deployment of devices and equipment. This trend resulted in an increasing influence of ADM systems\u2019 output over several aspects of our life, with possible discriminatory consequences towards certain individuals or groups. In this context, we focus on input data by investigating measurable characteristics which can lead to discriminating automated decisions. In particular, we identified two indexes of heterogeneity and diversity, and tested them on two datasets. A limitation we found is the index sensitivity to a large number of categories, but on the whole results show that the indexes reflect well imbalances in the input data. Future\u00a0\u2026", "num_citations": "1\n", "authors": ["1314"]}