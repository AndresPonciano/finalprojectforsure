{"title": "Predicting fault incidence using software change history\n", "abstract": " This paper is an attempt to understand the processes by which software ages. We define code to be aged or decayed if its structure makes it unnecessarily difficult to understand or change and we measure the extent of decay by counting the number of faults in code in a period of time. Using change management data from a very large, long-lived software system, we explore the extent to which measurements from the change history are successful in predicting the distribution over modules of these incidences of faults. In general, process measures based on the change history are more useful in predicting fault rates than product metrics of the code: For instance, the number of times code has been changed is a better indication of how many faults it will contain than is its length. We also compare the fault rates of code of various ages, finding that if a module is, on the average, a year older than an otherwise similar\u00a0\u2026", "num_citations": "920\n", "authors": ["124"]}
{"title": "Parallel changes in large-scale software development: an observational case study\n", "abstract": " An essential characteristic of large-scale software development is parallel development by teams of developers. How this parallel development is structured and supported has a profound effect on both the quality and timeliness of the product. We conduct an observational case study in which we collect and analyze the change and configuration management history of a legacy system to delineate the boundaries of, and to understand the nature of, the problems encountered in parallel development. The results of our studies are (1) that the degree of parallelism is very highhigher than considered by tool builders; (2) there are multiple levels of parallelism, and the data for some important aspects are uniform and consistent for all levels; (3) the tails of the distributions are long,  indicating the tail, rather than the mean, must receive serious attention in providing solutions for these problems; and (4) there is a significant\u00a0\u2026", "num_citations": "281\n", "authors": ["124"]}
{"title": "An experiment to assess the cost-benefits of code inspections in large scale software development\n", "abstract": " We conducted a long term experiment to compare the costs and benefits of several different software inspection methods. These methods were applied by professional developers to a commercial software product they were creating. Because the laboratory for this experiment was a live development effort, we took special care to minimize cost and risk to the project, while maximizing our ability to gather useful data. The article has several goals: (1) to describe the experiment's design and show how we used simulation techniques to optimize it; (2) to present our results and discuss their implications for both software practitioners and researchers; and (3) to discuss several new questions raised by our findings. For each inspection, we randomly assigned three independent variables: (1) the number of reviewers on each inspection team (1, 2, or 4); (2) the number of teams inspecting the code unit (1 or 2); and (3) the\u00a0\u2026", "num_citations": "219\n", "authors": ["124"]}
{"title": "Understanding the sources of variation in software inspections\n", "abstract": " In a previous experiment, we determined how various changes in three structural elements of the software inspection process (team size and the number and sequencing of sessions) altered effectiveness and interval. Our results showed that such changes did not significantly influence the defect detection rate, but that certain combinations of changes dramatically increased the inspection interval. We also observed a large amount of unexplained variance in the data, indicating that other factors must be affecting inspection performance. The nature and extent of these other factors now have to be determined to ensure that they had not biased our earlier results. Also, identifying these other factors might suggest additional ways to improve the efficiency of inspections. Acting on the  hypothesis that the \u201cinputs\u201d into the inspection process (reviewers, authors, and code units) were significant sources of variation, we\u00a0\u2026", "num_citations": "192\n", "authors": ["124"]}
{"title": "Software product lines: a case study\n", "abstract": " A software product line is a family of products that share common features to meet the needs of a market area. Systematic processes have been developed to dramatically reduce the cost of a product line. Such product\u2010line engineering processes have proven practical and effective in industrial use, but are not widely understood. The Family\u2010Oriented Abstraction, Specification and Translation (FAST) process has been used successfully at Lucent Technologies in over 25 domains, providing productivity improvements of as much as four to one. In this paper, we show how to use FAST to document precisely the key abstractions in a domain, exploit design patterns in a generic product\u2010line architecture, generate documentation and Java code, and automate testing to reduce costs. The paper is based on a detailed case study covering all aspects from domain analysis through testing. Copyright \u00a9 2000 John Wiley &\u00a0\u2026", "num_citations": "101\n", "authors": ["124"]}
{"title": "A review of software inspections\n", "abstract": " For two decades, software inspections have proven effective for detecting defects in software. We have reviewed the different ways software inspections are done, created a taxonomy of inspection methods, and examined claims about the cost effectiveness of different methods.We detect a disturbing pattern in the evaluation of inspection methods. Although there is universal agreement on the effectiveness of software inspection, their economics are uncertain. Our examination of several empirical studies leads us to conclude that the benefits of inspections are often overstated and the costs (especially for large software development projects) are understated. Furthermore, some of the most influential studies establishing these costs and benefits are 20 years old now, which leads us to question their relevance to today's software development processes.Extensive work is needed to determine exactly how, why, and\u00a0\u2026", "num_citations": "95\n", "authors": ["124"]}
{"title": "Does the modern code inspection have value?\n", "abstract": " For years, it was believed that the value of inspections is in finding and fixing defects early in the development process. Otherwise, the cost to find and fix them later is much higher However in examining code inspection data, we are finding that inspections are beneficial for an additional reason. They make the code easier to understand and change. An analysis of data from a recent code inspection experiment shows that 60% of all issues raised in the code inspections are not problems that could have been uncovered by latter phases of testing or field usage because they have little or nothing to do with the visible execution behavior of the software. Rather they improve the maintainability of the code by making the code conform to coding standards, minimizing redundancies, improving language proficiency, improving safety and portability, and raising the quality of the documentation. We conclude that even if\u00a0\u2026", "num_citations": "87\n", "authors": ["124"]}
{"title": "An experiment to assess the cost-benefits of code inspections in large scale software development\n", "abstract": " A. Porter* CA Toman H. Siy L. G, Votta Computer Science Department Software Production Research Department University of Maryland AT&T Bell Laboratories College Park, Maryland 20742 Naperville, Illinois 60566 aporter@ cs. umd. edu cat@ intgp 1. att. com harvey@? cs. umd. edu votta@ research. att. com", "num_citations": "59\n", "authors": ["124"]}
{"title": "An experiment to assess cost-benefits of inspection meetings and their alternatives: A pilot study\n", "abstract": " We hypothesize that inspection meetings are far less effective than many people believe and that meetingless inspections are equally effective. However two of our previous industrial case studies contradict each other on this issue. Therefore, we are conducting a multi-trial, controlled experiment to assess the benefits of inspection meetings and to evaluate alternative procedures. The experiment manipulates four independent variables: the inspection method used (two methods involve meetings, one method does not); the requirements specification to be inspected (there are two); the inspection round (each team participates in two inspections); and the presentation order (either specification can be inspected first). For each experiment we measure 3 dependent variables: the individual fault detection rate; the team fault detection rate; and the percentage of faults originally discovered after the initial inspection phase\u00a0\u2026", "num_citations": "51\n", "authors": ["124"]}
{"title": "Using semantic templates to study vulnerabilities recorded in large software repositories\n", "abstract": " Software repositories are rich sources of information about vulnerabilities that occur during a product's lifecycle. Although available, such information is scattered across numerous databases. Furthermore, in large software repositories, a single vulnerability may span across multiple components and have multidimensional interactions with other vulnerabilities. Thus, identifying the patterns of vulnerability occurrence in a larger context of software development continues to be an open problem. Here we present findings from our study of vulnerable software components using an ontology-guided analysis of vulnerabilities recorded in a software project's code repository. In this approach, a semantic template for each type of vulnerability is created from information in the Common Weakness Enumeration dictionary. Next, known vulnerabilities and related concepts in the repository are tagged with concepts from the\u00a0\u2026", "num_citations": "35\n", "authors": ["124"]}
{"title": "Understanding the effects of developer activities on inspection interval\n", "abstract": " We have conducted an industrial experiment to assess the cost-benefit tradeoffs of several software inspection processes. Our results to date explain the variation in observed effectiveness very well, but are unable to satisfactorily explain variation in inspection interval.In this article we examine the effect of a new factorprocess environment-on inspection interval (calendar time needed to complete the inspection). Our analysis suggests that process environment does indeed influence inspection interval. In particular, we found that nonuniform work priorities, time-varying workloads, and deadlines have significant effects.", "num_citations": "32\n", "authors": ["124"]}
{"title": "Challenges in evolving a large scale software product\n", "abstract": " Evolving a large system presents a number of signi cant challenges. Not only is the developer concerned about how to t in a new feature to a maze of existing features, he has to make sure his changes do not con ict with those being made in parallel by his colleagues. This is a minor problem in small projects with small organizations. However, as the project size scales up, so does the organization, and management of parallel tracks of development becomes a major concern. Moreover, increasing usage by customers with diverse needs pulls the evolving software into di erent directions, necessitating the evolution of multiple customized versions and compounding the already complex problem of evolving legacy systems.We will examine one such legacy system, the Lucent Technologies 5ESSR switching system. First introduced in 1982, 5ESS was envisioned to support telecommunication needs well into the next century. Already one of the largest and most complex pieces of real time code in the world, the software to run the switch still continues to evolve with new features and in an increasing number of customized versions. In order to keep up with future evolution and maintain the growing base of customers, a combined procedural and technological solution was put in place. We will discuss this particular solution and its limitations.", "num_citations": "30\n", "authors": ["124"]}
{"title": "Measuring technology effects on software change cost\n", "abstract": " We describe a methodology for precise quantitative measurement of technology impact on software change effort. The methodology employs measures of small software changes to determine the effect of technology. We illustrate this approach in a detailed case study on the impact of using two particular technologies\u2014a version\u2010sensitive source code editor and a domain\u2010engineered application environment\u2014in a telecommunications product. In both cases, the change effort was reduced. The methodology can precisely measure cost savings in change effort and is simple and inexpensive, since it relies on information automatically collected by version control systems.", "num_citations": "29\n", "authors": ["124"]}
{"title": "Identifying important classes of large software systems through k-core decomposition\n", "abstract": " In a large software project, the number of classes, and the dependencies between them, generally increase as software evolves. The size and scale of the system often makes it difficult to easily identify the important components in a particular software product. To address this problem, we model software as a network, where the classes are the vertices in the network and the dependencies are the edges, and apply K-core decomposition to identify a core subset of vertices as potentially important classes. We study three open source Java projects over a 10-year period and demonstrate, using different metrics, that the K-core decomposition of the network can help us identify the key classes of the corresponding software. Specifically, we show that the vertices with the highest core number represent the important classes and demonstrate that the core-numbers of classes with similar functionalities evolve at similar trends.", "num_citations": "28\n", "authors": ["124"]}
{"title": "Measuring domain engineering effects on software change cost\n", "abstract": " Domain engineering (DE) is an increasingly popular process for efficiently producing software. DE uses detailed knowledge of a particular application domain to define rigorously a family of software products within that domain. We describe a methodology for precise quantitative measurement of DE impact on software change efforts. The methodology employs measures of small software changes to determine the effect of DE. We illustrate this approach in a detailed case study of DE in a telecommunications product. In the particular case the change effort was dramatically reduced. The methodology can precisely measure cost savings in change effort and is simple and inexpensive since it relies on information automatically collected by version control systems.", "num_citations": "25\n", "authors": ["124"]}
{"title": "Making the software factory work: Lessons from a decade of experience\n", "abstract": " At the heart of proposals to use process-oriented techniques for creating organizations that are capable of creating high-quality software at low cost is a focus on software process maturity, organizational learning to foster continuous improvement, and contractual arrangements that support an exclusive focus on software construction activities, as opposed to a broader focus on end-to-end development of an entire product. We study an organization that was to provide fast, low-cost, high-quality software development services to product teams within Lucent Technologies. The vision called for an organization with a culture that is distinct and isolated from the rest of Lucent, characterized by a commitment to a well-defined software development process, use of state-of-the-art technology that fits into the process, and use of various forms of feedback to recognize and take advantage of opportunities for process\u00a0\u2026", "num_citations": "23\n", "authors": ["124"]}
{"title": "The chicken and the pig: User involvement in developing usability heuristics\n", "abstract": " Traditionally, users have not been involved in certain usability engineering methods, although they arguably are the most important stakeholders. This paper explores the possibility of involving users in developing a set of usability heuristics for a specific type of application, an activity they are not usually involved in.Using a qualitative approach based on interviews, a focus group, and an online survey, usability experts and software users evaluated existing sets of heuristics in terms of their applicability to a specific type of application and developed new heuristics to supplement them. The results indicate that the users provide a valuable contribution to the adaptation of existing heuristics to a specific type of application. Users add a new perspective and can address problem areas that usability experts, especially those with little or no experience with the specific application area, would not have identified.", "num_citations": "20\n", "authors": ["124"]}
{"title": "Method and apparatus for measuring the quality of speech transmissions that use speech compression\n", "abstract": " A method and apparatus are provided for determining the quality of a speech transmission, including temporal clipping, delay and jitter, using a carefully constructed test signal (300) and digital signal processing techniques. The test signal that is to be transmitted through a speech transmission system (100) is created (700). Then the test signal is transmitted through the speech transmission system such that the speech transmission system creates an output signal that corresponds to the input signal, as modified by the speech transmission system (702). The test signal includes multiple segments (500) of speech signals interleaved with periods of silence. The periods of silence vary in duration according to a predefined pattern. Each segment of speech signals includes multiple predefined speech samples or symbols (400, 402, 404, 406, 408, 410, 412, 414) interleaved with a plurality of silence gaps. The speech\u00a0\u2026", "num_citations": "20\n", "authors": ["124"]}
{"title": "Identifying the mechanisms driving code inspection costs and benefits\n", "abstract": " Software inspections have long been considered to be an effective way to detect and remove defects from software. However, there are costs associated with carrying out inspections and these costs may outweigh the expected benefits.", "num_citations": "19\n", "authors": ["124"]}
{"title": "Adapting to a new environment: How a legacy software organization copes with volatility and change\n", "abstract": " Many of the key players in the telecommunications industry produce legacy products, software systems that were designed ten or even twenty years ago but also serve as the underlying basis for new products. A key challenge facing these firms is how to remain innovative and adaptive despite their ties with the past. This paper describes how one successful US company coped with such a problem. Based on interview data, we identify three key transitions this organization underwent during the product\u2019s twenty year lifetime and test their effects using data from internal company databases. We found that environmental pressures to radically restructure the software system were inhibited due to certain inertial legacy factors. Instead, the organization coped by making more incremental adjustments to the product, organizational structure, coordination and control systems, and processes. A key proposition emerging from the study is that displacing change in this way increases the degree of technical and organizational interdependencies the firm is required to manage.", "num_citations": "18\n", "authors": ["124"]}
{"title": "Empirical results on the study of software vulnerabilities (NIER track)\n", "abstract": " While the software development community has put a significant effort to capture the artifacts related to a discovered vulnerability in organized repositories, much of this information is not amenable to meaningful analysis and requires a deep and manual inspection. In the software assurance community a body of knowledge that provides an enumeration of common weaknesses has been developed, but it is not readily usable for the study of vulnerabilities in specific projects and user environments. We propose organizing the information in project repositories around semantic templates. In this paper, we present preliminary results of an experiment conducted to evaluate the effectiveness of using semantic templates as an aid to studying software vulnerabilities.", "num_citations": "17\n", "authors": ["124"]}
{"title": "Summarizing developer work history using time series segmentation: challenge report\n", "abstract": " Temporal segmentation partitions time series data with the intent of producing more homogeneous segments. It is a technique used to preprocess data so that subsequent time series analysis on individual segments can detect trends that may not be evident when performing time series analysis on the entire dataset.", "num_citations": "16\n", "authors": ["124"]}
{"title": "Aspectual support for specifying requirements in software product lines\n", "abstract": " We present an aspect-oriented requirements specification system for software product lines. We encapsulate nonfunctional concerns as a set of advices for transforming parameterized requirements to product-specific requirements. We apply our system to the Health Watcher case study to demonstrate our approach. We sort out system requirements, exception handling requirements (alternate flows) and non-functional requirements and represent them as aspects in our framework. We have implemented a prototype transformation tool which takes these aspects along with the basic functional requirements as input and produces a requirements document with all applicable aspects woven in.", "num_citations": "16\n", "authors": ["124"]}
{"title": "Discovering dynamic developer relationships from software version histories by time series segmentation\n", "abstract": " Time series analysis is a promising approach to discover temporal patterns from time stamped, numeric data. A novel approach to apply time series analysis to discern temporal information from software version repositories is proposed. Version logs containing numeric as well as non-numeric data are represented as an item-set time series. A dynamic programming based algorithm to optimally segment an item-set time series is presented. The algorithm automatically produces a compacted item-set time series that can be analyzed to discern temporal patterns. The effectiveness of the approach is illustrated by applying to the Mozilla data set to study the change frequency and developer activity profiles. The experimental results show that the segmentation algorithm produces segments that capture meaningful information and is superior to the information content obtaining by arbitrarily segmenting time period into\u00a0\u2026", "num_citations": "15\n", "authors": ["124"]}
{"title": "Architecture-based reliability analysis of web services in multilayer environment\n", "abstract": " The reliability analysis of web services is often focused on the web service components, ignoring the impact of the middleware located beneath the web services. A service-based software system is a multilayered system that includes the web service (WS), shared resources, and the hosting application server (AS). It is conjectured that the reliability prediction of the web services is improved if the reliability model accounts for such underlying layers. The initial experiment illustrates that the AS and shared resources can impact the overall reliability of web services greatly. This observation is demonstrated by simulating the interaction between a web service and the AS.", "num_citations": "12\n", "authors": ["124"]}
{"title": "Risk prioritization and management\n", "abstract": " Methods for managing and prioritizing risk include receiving a data set and analyzing the data set for duplicates, false positives, false negatives, and tool errors. Said duplicates, false positives, false negatives and results of tool errors are removed from the data set, creating an input file. The input file is compared against compliance standards to identify any weaknesses, defects, bugs, flaws, vulnerabilities, and/or failures in the input file. The compared input file is mapped to Common Weakness Enumeration standards. A risk prioritization can be generated based on the mapped results. At least one report can be generated based on the risk prioritization.", "num_citations": "11\n", "authors": ["124"]}
{"title": "An ontology to support empirical studies in software engineering\n", "abstract": " Ontologies are semantically organized collections of information pieces. Ontologies provide a way of organizing and encoding the collected knowledge for a given domain. Formalizing the accumulated knowledge in such a framework enables all sorts of automated analysis. We present an ontology for analyzing empirical studies of software engineering, in particular the design of software engineering experiments. The design of such experiments consists of assigning human subjects to apply treatments, such as techniques or tools, to artifacts such as code or specifications. The particular design and available treatments depend on the goals of the experiment. Provisions for addressing various threats to validity constrain the available design space. Furthermore, the assignments have to be consistent with the available resources. By encapsulating the existing knowledge on designing experiments, we posit that it is\u00a0\u2026", "num_citations": "11\n", "authors": ["124"]}
{"title": "Architectural reliability analysis of framework-intensive applications: A web service case study\n", "abstract": " A novel methodology for modeling the reliability and performance of web services (WSs) is presented. To present the methodology, an experimental environment is developed in house, where WSs are treated as atomic entities but the underlying middleware is partitioned into layers. WSs are deployed in JBoss AS. Web service requests are generated to a remote middleware on which JBoss runs, and important performance parameters under various configurations are collected. In addition, a modularized simulation model in Petri net is developed from the architecture of the middleware and run-time behavior of the WSs. The results show that (1) the simulation model provides for measuring the performance and reliability of WSs under different loads and conditions that may be of great interest to WS designers and the professionals involved; (2) configuration parameters have substantial impact on the overall\u00a0\u2026", "num_citations": "10\n", "authors": ["124"]}
{"title": "Studying software vulnerabilities\n", "abstract": " There have been several research efforts to enumerate and categorize software weaknesses that lead to vulnerabilities. To consolidate these efforts, the Common Weakness Enumeration (CWE) is a community-developed dictionary of software weakness types and their relationships. Yet, using the CWE to study and prevent vulnerabilities in specific software projects is difficult. This article presents a novel approach for using the CWE to organize and integrate the vulnerability information recorded in large project repositories.", "num_citations": "10\n", "authors": ["124"]}
{"title": "ERTSAL: a prototype of a domain-specific aspect language for analysis of embedded real-time systems\n", "abstract": " A primary characteristic of Embedded Real-Time Systems (ERTS) is the fact that they are resource constrained. Such constraints present unique challenges to the embedded systems programmer who must develop software satisfying a given set of functional requirements while simultaneously addressing the limitations of available resources and dependability concerns.", "num_citations": "10\n", "authors": ["124"]}
{"title": "The difficulties of type resolution algorithms\n", "abstract": " One method of confirming a newly developed tool\u2019s resolution algorithm is to compare its results with extensively used tools that also make use of resolution algorithms, such as Netbeans and Eclipse. Since these tools are used extensively, one might assume that they can correctly determine resolution within a given set of java files. However, we have found that correctly resolving all corner cases of java resolution is so difficult that even the popular tools have bugs.", "num_citations": "8\n", "authors": ["124"]}
{"title": "FireBugs: finding and repairing bugs with security patterns\n", "abstract": " Security is often a critical problem in software systems. The consequences of the failure lead to substantial economic loss or extensive environmental damage. Developing secure software is challenging, and retrofitting existing systems to introduce security is even harder. In this paper, we propose an automated approach for Finding and Repairing Bugs based on security patterns (FireBugs), to repair defects causing security vulnerabilities. To locate and fix security bugs, we apply security patterns that are reusable solutions comprising large amounts of software design experience in many different situations. In the evaluation, we investigated 2,800 Android app repositories to apply our approach to 200 subject projects that use javax.crypto APIs. The vision of our automated approach is to reduce software maintenance burdens where the number of outstanding software defects exceeds available resources. Our\u00a0\u2026", "num_citations": "7\n", "authors": ["124"]}
{"title": "Incorporating standard Java libraries into the design of embedded systems\n", "abstract": " When used in embedded systems, JVMs are typically restricted in a variety of ways. The practical consequence to the Java developer is that not all Java programs can be executed on a restricted JVM. A notable example is that standard class libraries available to the \u201cJava desktop programmer\u201d may not be available or may only be partially available to the \u201cJava embedded systems programmer\u201d. This limitation can have far reaching consequences and can ultimately impact the overall design of an embedded system. This chapter describes an ongoing effort to develop a highly-automated approach for migrating Java source-code, such as standard class libraries, to restricted JVMs. We are developing a transformation-based tool called Monarch 2 capable of migrating Java source-code to restricted JVMs.", "num_citations": "7\n", "authors": ["124"]}
{"title": "A segmentation\u2010based approach for temporal analysis of software version repositories\n", "abstract": " Time series segmentation is a promising approach to discover temporal patterns from time\u2010stamped numeric data. A novel approach to apply time series segmentation to discern temporal information from software version repositories is proposed. Data from such repositories, both numeric and non\u2010numeric, are represented as item\u2010set time series data. A dynamic programming algorithm for optimal segmentation is presented. The algorithm automatically produces a compacted item\u2010set time series that can be analyzed to identify temporal patterns. The effectiveness of the approach is illustrated by analyzing version control repositories of several open\u2010source projects to identify time\u2010varying patterns of developer activity. The experimental results show that the segmentation algorithm produces segments that capture meaningful information and is superior to the information content obtained by arbitrarily segmenting\u00a0\u2026", "num_citations": "7\n", "authors": ["124"]}
{"title": "Construction of ontology-based software repositories by text mining\n", "abstract": " Software document repositories store artifacts produced in the course of developing software products. But most repositories are simply archives of documents. It is not unusual to find projects where different software artifacts are scattered in unrelated repositories with varying levels of granularity and without a centralized management system. This makes the information available in existing repositories difficult to reuse. In this paper, a methodology for constructing an ontology-based repository of reusable knowledge is presented. The information in the repository is extracted from specification documents using text mining. Ontologies are used to guide the extraction process and organize the extracted information. The methodology is being used to develop a repository of recurring and crosscutting aspects in software specification documents.", "num_citations": "7\n", "authors": ["124"]}
{"title": "Lack of software engineering practices in the development of bioinformatics software\n", "abstract": " Bioinformatics is a growing field in the software industry. However there is very little evidence that sound software engineering practices are being applied to bioinformatics software development. As bioinformatics is a merging of the disciplines of biology and computer science, it would appear very odd that this, particularly important aspect of the computer science field would be absent, however that is the case. This paper will attempt to go into the reasons for this, as well as propositions that others have put forward to remedy this issue. We finally propose an approach towards resolving the software/requirement engineering challenges by comparing four methodologies Agile, SSADM, UP and Domain Engineering, and select the best approach that can help resolve the software/requirement engineering issues while developing bioinformatics software.", "num_citations": "6\n", "authors": ["124"]}
{"title": "Ontology-based product line modeling and generation\n", "abstract": " Software product line engineering defines a family of related software products. Every software product line engineering method has two essential elements, a set of models representing the product family, and a process for instantiating product members from those models. In this paper, we investigate the use of ontologies to model product lines. We also show how product members can be instantiated from an ontology-based model. We discuss our early experiences using ontologies to specify a family of workflows for a large insurance company.", "num_citations": "6\n", "authors": ["124"]}
{"title": "Assessing the impact of refactoring activities on the JHotDraw project\n", "abstract": " Refactoring is a well-known technique for improving the maintainability of software products. However, it is not easy to justify the time and effort needed to refactor code as the benefits are difficult to quantify, especially the perception of improved maintainability. In this paper, we highlight some results of a retrospective case study undertaken to shed light on how refactoring affects maintainability of a software product. There are several findings. First of all, refactoring affects the amount of subsequent changes. Furthermore, refactoring has a positive impact on the coupling relationships with dependent software applications.", "num_citations": "6\n", "authors": ["124"]}
{"title": "Identifying the mechanisms to improve code inspection costs and benefits\n", "abstract": " Software inspections have long been considered to be an effective way to detect and remove defects from software. However, there are costs associated with carrying out inspections and these costs may outweigh the expected benefits.", "num_citations": "6\n", "authors": ["124"]}
{"title": "Measuring disruption from software evolution activities using graph-based metrics\n", "abstract": " In this paper, we investigate how class relationships are disrupted after large scale changes. We use graphs to represent different software versions and study changes to graph properties. We explore different combinatorial metrics to measure the extent of disruption after perfective maintenance activities. Our early results, on JHotDraw, demonstrate that combinatorial metrics can provide a good indicator to the degree to which relationships are disrupted or preserved across different versions.", "num_citations": "5\n", "authors": ["124"]}
{"title": "Empirical study of software evolution using community detection\n", "abstract": " In this paper, we propose an approach for un-derstanding how class relationships evolve after large scale changes in object-oriented software. We apply a community detection algorithm to the network of class relationships and examine changes in communities of interacting classes. We apply techniques from dynamic network analysis to examine how these communities evolve. We demonstrate the feasibility of this approach through a case study on the JHotDraw project.", "num_citations": "5\n", "authors": ["124"]}
{"title": "The tyranny of the vital few: The Pareto principle in language design\n", "abstract": " Modern high-level programming languages often contain constructs whose semantics are non-trivial. In practice how-ever, software developers generally restrict the use of such constructs to settings in which their semantics is simple (programmers use language constructs in ways they understand and can reason about). As a result, when developing tools for analyzing and manipulating software, a disproportionate amount of effort ends up being spent developing capabilities needed to analyze constructs in settings that are infrequently used. This paper takes the position that such distinctions between theory and practice are an important measure of the analyzability of a language.", "num_citations": "5\n", "authors": ["124"]}
{"title": "Architecture-based reliability modeling of web services using petri nets\n", "abstract": " Reliability of web applications depends on reliability of the application itself as well as the underlying application server, external and internal services. Many research works have been conducted on reliability modeling of service-based software, which concentrate on internal services located in the organization. The purpose of this in-progress study is to analyze the reliability of service-based software using internal and external services. Petri net modeling is employed in evaluating the reliability of the composite software. The main challenge is obtaining and aggregating the reliability of all components forming the system architecture. It is conjectured that differentiating between internal and external reliability models can lead to more accurate reliability prediction.", "num_citations": "5\n", "authors": ["124"]}
{"title": "Consistently incorporating changes to evolve transition-based systems\n", "abstract": " Evolving software-intensive systems from one consistent state to another is a challenging activity due to the intricate inter-dependencies among the components. In this paper, we propose a novel, semantic approach to incorporate software changes while automatically preserving system consistency. Systems are modeled as a network of reactive components whose behaviors are specified by communicating finite state machines extended with finite domain variables. Changes perform addition/ deletion/ replacement of one or more transitions in one or more components. Consistency of a system is modeled in terms of application-independent reachability properties over system global states. The proposed approach takes a change and a consistent system as inputs and automatically synthesizes a set of changes that can be consistently incorporated into the system. Each synthesized change represents a different\u00a0\u2026", "num_citations": "5\n", "authors": ["124"]}
{"title": "Stepwise refinement in block-based programming\n", "abstract": " With the popularity of block-based programming in CS0 courses, a growing number of students will learn a block language in their first exposure to programming. Studies indicate that blocks make it easier for novices to complete simple tasks and help them maintain or increase interest in computer science. But as they attempt more complicated programs, the mere ease of dragging and dropping blocks will not be sufficient to help them think through the process of composing blocks into a working program. We propose to incorporate stepwise refinement into block programming environments as an approach for novices to work out the computational thinking processes needed to write more complex programs. We present a prototype, developed by modifying the Google Blockly platform, to facilitate stepwise refinement. We conduct a small exploratory study with high school AP CS0 teachers to determine the feasibility of this approach and report on the teachers\u2019 feedback.", "num_citations": "4\n", "authors": ["124"]}
{"title": "SPARCS: A personalized problem-based learning approach for developing successful computer science learning experiences in middle school\n", "abstract": " To keep pace with today's computer and technology-driven workforce, strategies that spark and foster student interest in computer science during the K-12 years are sorely needed. Thus it is essential that teachers at these formative years are able to provide successful learning experiences in computing. We provide an overview of the Strategic Problem-based Approach to Rouse Computer Science (SPARCS) project being conducted at University of Nebraska at Omaha. SPARCS aims to help middle school teachers incorporate computer science lessons into the classes they teach, and inspire their students to consider careers in computing-related industries. We present our experiences and findings from the first year of a planned three-year study on SPARCS.", "num_citations": "4\n", "authors": ["124"]}
{"title": "Identification of the emergent leaders within a cse professional development program\n", "abstract": " The need for high quality, sustainable Computer Science Education (CSE) professional development (PD) at the grades K-12 level is essential to the success of the global CSE initiatives. This study investigates the use of Social Network Analysis (SNA) to identify emergent teacher leaders within a high quality CSE PD program. The CSE PD program was designed and implemented through collaboration between the computer science and teacher education units at a Midwestern metropolitan university in North America. A unique feature of this specific program is in the intentional development of a social network. This study discusses the importance of social networks, the development of social capital, and its impact on the sustainability of the goals of the CSE PD program. The role of emergent teacher leaders in the development of the social capital of the CSE PD cohort is investigated using SNA techniques. The\u00a0\u2026", "num_citations": "4\n", "authors": ["124"]}
{"title": "Semantic web representations for reasoning about applicability and satisfiability of federal regulations for information security\n", "abstract": " In this paper, the Nomos 2 framework for modeling law-compliant solutions in software system design is applied in the context of the Federal Information Security Modernization Act (FISMA) of 2014. Information security regulatory statements with a high variability space are examined to explore the utility and limits of the Nomos 2 framework for information security regulations. Additionally, Nomos 2 concepts are modeled in a semantic web representation for reasoning about the applicability and satisfiablity of FISMA regulations for information systems. The use of freely available semantic web toolsets for knowledge modeling and reasoning are demonstrated in an example scenario requiring the determination of FISMA related authorities and functions.", "num_citations": "4\n", "authors": ["124"]}
{"title": "Gauging the impact of FISMA on software security\n", "abstract": " A newly developed instrument provides sophisticated content analysis to help determine the relevance for software security of the National Institute of Standards and Technology's FISMA-mandated security controls.", "num_citations": "4\n", "authors": ["124"]}
{"title": "Locating fault-inducing patterns from structural inputs\n", "abstract": " In this paper, we propose a new fault localization technique for testing software which requires structured input data. We adopt a symbolic grammar to represent structured data input, and use an automatic grammar-based test generator to produce a set of well-distributed test cases, each of which is equipped with a set of structural features. We show that structural features can be effectively used as test coverage criteria for test suite reduction. By learning structural features associated with failed test cases, we present an automatic fault localization approach to find out software defects which result in the testing failures. Preliminary experiments justify that our fault localization approach is able to accurately locate fault-inducing patterns.", "num_citations": "3\n", "authors": ["124"]}
{"title": "Test-driven learning in high school computer science\n", "abstract": " Test-driven development (TDD) is an accepted practice in the software development industry. Although computer science teaching programs have been slower to adopt test-driven practices, test-driven learning has been used in a number of universities with generally positive results. The use of test-driven learning at the high school level is less studied. We introduce and assess the benefits of using test-driven learning in a high school Advanced Placement (AP) computer science course. This course is a strong candidate for the introduction of TDD. The Java language used in AP computer science is well-supported by TDD tools, and the concepts of TDD show promise in helping students develop the ability to analyze problem statements and develop programs. Preliminary results indicate that students respond well to the use of TDD tools to complement other teaching techniques in AP CS.", "num_citations": "3\n", "authors": ["124"]}
{"title": "A survey on model driven software development\n", "abstract": " A survey on Model Driven Software Development (MDSD) is presented in this paper. This survey provides a static overview of essential elements and relationships between them in MDSD. Major elements in MDSD consist of modeling languages (GPLs and DSLs), domain knowledge, metamodels, formal methods, model transformations, standards, and tools are emphasized in this research work. Presenting a comprehensive survey to better understanding of MDSD is the main goal of this paper. In the end we will present some characteristics of six well-known tools which support MDSD paradigm.", "num_citations": "3\n", "authors": ["124"]}
{"title": "Making aspect-orientation accessible through syntax-based language composition\n", "abstract": " A generic syntax-based approach is presented by which a fixed set of aspect-oriented features belonging to an aspect language family  L   A  can be applied to a domain-specific language (DSL). The approach centres on the construction of a grammar in which a predefined and fixed set of abstract join points and join point environments are linked with their concrete counterparts within the DSL. This connection enables the behaviour of static weaving to be expressed in a generic manner. The resulting framework is one in which aspect orientation is accessible to non-experts across a wide spectrum of abstractions.", "num_citations": "3\n", "authors": ["124"]}
{"title": "Discovering Meaningful Clusters from Mining the Software Engineering Literature.\n", "abstract": " Document clustering is becoming an increasingly popular technique for identifying relationships in unstructured text. In this paper, we attempt to make sense of the output of a clustering algorithm applied to software engineering research papers. We introduce a notion of cluster \u201cstability\u201d as a measure of the meaningfulness of a cluster. We assess its usefulness and limitations in identifying meaningful clusters. In the process, we track how important research topics may have changed from year to year.", "num_citations": "3\n", "authors": ["124"]}
{"title": "Aspect traceability through invertible weaving\n", "abstract": " The dichotomy resulting from tangled and untangled representations may prove beneficial with respect to the manipulation and analysis of system requirements. This paper describes an invertible approach to weaving requirements documents that is based on \u03b2-conversion as defined in the \u03bb-calculus.", "num_citations": "3\n", "authors": ["124"]}
{"title": "The Role of Aspects in Domain Engineering\n", "abstract": " Aspect-oriented domain engineering is a promising extension to the domain engineering process. We present several lines of inquiry that demonstrate how domain engineering benefits from adopting aspectoriented software development concepts.", "num_citations": "3\n", "authors": ["124"]}
{"title": "Driving secure software initiatives using FISMA: Issues and Opportunities\n", "abstract": " Federal agencies install many security controls for Federal Information Security Management Act (FISMA) implementation. National Institute of Standards and Technology (NIST) Special Publication (SP) 800-53 revision 4 (rev4) standardizes these security and privacy controls. This article presents a study of NIST SP 800-53 security controls. The purpose is to classify the security controls from dimensions relevant to software security. This classification highlights issues and motivates opportunities to drive software security initiatives using FISMA.", "num_citations": "2\n", "authors": ["124"]}
{"title": "Specifying performance requirements for product lines\n", "abstract": " We investigate the product line approach to specifying performance requirements. Within a given application domain such as telecommunications, performance requirements tend to have similarities. However, differences in external needs, such as environmental conditions and user needs, can dictate varying performance requirements across similar products. We examine how we can use these external needs to drive the generation of specific requirements for product line members.We put forward a framework to support the specification of performance requirements in product lines, including a proposed set of domain-specific languages (DSLs) to realize this framework. We draw on our previous industrial experiences with voice-over-IP (VoIP) and voice-over-ATM (VoATM) voice quality to illustrate this approach.", "num_citations": "2\n", "authors": ["124"]}
{"title": "An Interdisciplinary Approach for Teaching Artificial Intelligence to Computer Science Students\n", "abstract": " Artificial intelligence (AI) is a demanding and important course for computer science education in the universities and open online courses (MOOCs). It includes various introductory and specialized courses for artificial intelligence like knowledge representation, machine learning, reasoning under uncertainty, natural language processing, robotics, and perception of computer vision, etc. We observed that mostly AI courses focus on the Computer Science (CS)-centric approach and lacks core explanation from their roots including philosophy, neuroscience, psychology, cognitive science, linguistics, economics, social science, etc. In this paper, we propose to engage the interdisciplinary approach along with CS-centric approach for teaching AI that includes the disciplines that have been established to tackle the age-old problem of understanding the science of thinking.", "num_citations": "1\n", "authors": ["124"]}
{"title": "CryptoTutor: Teaching Secure Coding Practices through Misuse Pattern Detection\n", "abstract": " Insecure program practices seriously threaten software security. Misusing security primitives in application-level code is not unusual. For example, in mobile banking apps, developers might store customers' privacy information in plaintext, leading to sensitive information leakage. To leverage cryptographic primitives, developers need to correctly select the cryptographic algorithm, appropriate parameters, and sometimes its post-process. While recent research discusses pitfalls in cryptography-related implementations, few academic programs integrate these concepts in their educational programs. One big challenge is the lack of automated guidance on how to utilize existing libraries for secure coding. In this paper, we discuss the prevalence of the problem, especially with respect to implementing programs that utilize cryptography, to motivate the need for better tool support for guidance in writing secure code. We\u00a0\u2026", "num_citations": "1\n", "authors": ["124"]}
{"title": "Modular norm models: practical representation and analysis of contractual rights and obligations\n", "abstract": " Compliance analysis requires legal counsel but is generally unavailable in many software projects. Analysis of legal text using logic-based models can help developers understand requirements for the development and use of software-intensive systems throughout its lifecycle. We outline a practical modeling process for norms in legally binding agreements that include contractual rights and obligations. A computational norm model analyzes available rights and required duties based on the satisfiability of situations, a state of affairs, in a given scenario. Our method enables modular norm model extraction, representation, and reasoning. For norm extraction, using the theory of frame semantics, we construct two foundational norm templates for linguistic guidance. These templates correspond to Hohfeld\u2019s concepts of claim-right and its jural correlative, duty. Each template instantiation results in a norm model\u00a0\u2026", "num_citations": "1\n", "authors": ["124"]}
{"title": "Semantics-based automated web testing\n", "abstract": " We present TAO, a software testing tool performing automated test and oracle generation based on a semantic approach. TAO entangles grammar-based test generation with automated semantics evaluation using a denotational semantics framework. We show how TAO can be incorporated with the Selenium automation tool for automated web testing, and how TAO can be further extended to support automated delta debugging, where a failing web test script can be systematically reduced based on grammar-directed strategies. A real-life parking website is adopted throughout the paper to demonstrate the effectivity of our semantics-based web testing approach.", "num_citations": "1\n", "authors": ["124"]}
{"title": "Enhancing cs education in high school stem curricula\n", "abstract": " This paper describes a project engaging high school teachers to the development of Problem-Based Learning (PBL) modules and course materials to infuse computer science principles to the existing STEM curricula. The project involves training teachers in a research environment to design personalized implementation plan (PIP) of lesson modules to translate their research experience to their classrooms. The course materials and lesson modules are aimed at exciting and stimulating the interest and knowledge of students in computer science education and career pathway.", "num_citations": "1\n", "authors": ["124"]}
{"title": "Semi-Automatic Annotation of Natural Language Vulnerability Reports\n", "abstract": " Those who do not learn from past vulnerabilities are bound to repeat it. Consequently, there have been several research efforts to enumerate and categorize software weaknesses that lead to vulnerabilities. The Common Weakness Enumeration (CWE) is a community developed dictionary of software weakness types and their relationships, designed to consolidate these efforts. Yet, aggregating and classifying natural language vulnerability reports with respect to weakness standards is currently a painstaking manual effort. In this paper, the authors present a semi-automated process for annotating vulnerability information with semantic concepts that are traceable to CWE identifiers. The authors present an information-processing pipeline to parse natural language vulnerability reports. The resulting terms are used for learning the syntactic cues in these reports that are indicators for corresponding standard weakness\u00a0\u2026", "num_citations": "1\n", "authors": ["124"]}
{"title": "Lightweight formal models of software weaknesses\n", "abstract": " Many vulnerabilities in today's software products are rehashes of past vulnerabilities. Such rehashes could be a result of software complexity that masks inadvertent loopholes in design and implementation, developer ignorance/disregard for security issues, or use of software in contexts not anticipated for the original specification. While weaknesses and exposures in code are vendor, language, or environment specific, to understand them we need better descriptions that identify their precise characteristics in an unambiguous representation. In this paper, we present a methodology to develop precise and accurate descriptions of common software weaknesses through lightweight formal modeling using Alloy. Natural language descriptions of software weaknesses used for formalization are based on the community developed Common Weakness Enumerations (CWE).", "num_citations": "1\n", "authors": ["124"]}
{"title": "Concept to commit: A pattern designed to trace code changes from user requests to change implementation by analyzing mailing lists and code repositories\n", "abstract": " The concept to commit pattern is used for tracing code changes from user requests (analyzing the mailing list) to change implementation (analyzing the code repository). The analysis is done via text mining of both emails and commits descriptions in 4 stages. The first stage is identifying a search time window for the mailing list by evaluating a targeted commit time stamp. Once a window is established, the body of the mailing list is reduced to match the search window. The next stage involves basic text mining processing (tokenization, stemming, and document matrix creation). The final step is to perform frequency analysis (word cloud, heat map, or dendrogram).", "num_citations": "1\n", "authors": ["124"]}
{"title": "Applying Environmental Factors to Trust Algorithms in Competitive Multi-Agent Systems.\n", "abstract": " In any multi-agent system (MAS) where agents must rely on each other in order to achieve individual or shared goals, the issue of trust is important. How does one agent decide whether or not to rely on another, particular agent to perform a task? This and other related questions are at the forefront of recent research into trust and reputation in MAS. One area not deeply explored is the effect of the MAS environment itself on trust decisions. For example, if an agent is operating in a MAS where it is expensive (in whatever manner \u201cexpensive\u201d makes sense in the MAS) to initiate a transaction with another agent, should that relatively high cost affect the agent\u2019s trust decisions and if so, how? What about the level of competitiveness in the MAS? Are the agents working towards a set of common goals, or is it \u201cevery agent for itself\u201d? How should each type of environment\u2013or even an environment where the level of competitiveness changes over time\u2013affect a participating agent\u2019s trust decisions? This work explores methods for considering those types of environmental factors in an agent\u2019s trust algorithm. The theory is that an agent capable of a) detecting and b) reacting to certain environmental factors will be more effective in accomplishing its goals, whether those goals are shared with other agents or not. Using the current state-ofthe-art research testbed, an \u201cenvironmentally-aware\u201d trust algorithm will be designed and implemented in a software agent. This agent will then be pitted against a \u201cstock\u201d(unmodified) agent in a simulated competitive MAS to see if the modified agent outperforms its peers.", "num_citations": "1\n", "authors": ["124"]}
{"title": "A prototype of a generic weaver\n", "abstract": " A generic weaver is presented capable of realizing the weaving function over a large class of languages. There are several reasons why such a weaver is interesting. First, properties that can be proven about the weaver hold for all languages that fall within the domain of the weaver. Second, the problem of constructing a weaver for a particular language is reduced to constructing a language falling within the domain of the weaver.", "num_citations": "1\n", "authors": ["124"]}
{"title": "A segmentation-based approach for temporal analysis of software version repositories\n", "abstract": " Time series segmentation is a promising approach to discover temporal patterns from time stamped, numeric data. A novel approach to apply time series segmentation to discern temporal information from software version repositories is proposed. Data from such repositories, both numeric and non-numeric, are represented as item-set time series data. A dynamic programming algorithm for optimal segmentation is presented. The algorithm automatically produces a compacted item-set time series that can be analyzed to identify temporal patterns. The effectiveness of the approach is illustrated by analyzing version control repositories of several open source projects to identify time-varying patterns of developer activity. The experimental results show that the segmentation algorithm produces segments that capture meaningful information and is superior to the information content obtained by arbitrarily segmenting software history into regular time intervals.", "num_citations": "1\n", "authors": ["124"]}
{"title": "Quantifying the value of new technologies for software development\n", "abstract": " Introducing relevant software technologies may provide significant advantages to a software organization. Unfortunately, the value the technology may provide is almost never quantified. We describe a methodology for precise quantitative measurement of the value a software technology may add to the project in terms of the impact on quality and lead time. The methodology employs measures derived from version control and problem tracking repositories to determine the value of technology. We illustrate this approach in a detailed case study on the impact of using two particular technologies \u2014 a version-sensitive source code editor and a domain engineered application environment \u2014 in a telecommunications product. In both cases use of technology had a strong positive impact on the considered quality measures. The methodology relies on information commonly available in project version control and\u00a0\u2026", "num_citations": "1\n", "authors": ["124"]}
{"title": "Analyzing Source Code in Source Control Repositories\n", "abstract": " #% $6 iq p r D6Us PR06t9u hv 0@ 9I6U9V\u00a2 3 w'xhy 06\" fg#% $ ap#% $ & p U 0219C\" rQP2 E8 195\" H\" 86Ua D6ra I6U\u00a2 V65\" H 8S1\u00a2 C BA 0\" B 979 S1X\u00a9 Wv I6U9V\u00a2 3\u00a2 fxh#% $6 iq r D6Us PR06t9u hv 0@ 9I6U9V\u00a2 3Xhv 06\" fg#% $ a#% $ & r D6Us PR06t\u00a9'xhv 0@ 9I6U9V\u00a2 3Xhv 06\" fg#% $ a#% $ ap U 0S19C\" r P2 SI6U9VA tT\u00a2 B S1X Wy I6U9V bc 06d fxh U 86C 59U 1 P u fx h", "num_citations": "1\n", "authors": ["124"]}
{"title": "Systematic text-mining approach for deriving aspects and patterns from domain knowledge\n", "abstract": " As the theoretical underpinnings of aspect-orientation mature, its application across the software lifecycle has expanded. An active area of research focuses on the application of aspect oriented techniques to unstructured or semi-structured requirements documents. In this context, primary issues involve the identification of early aspects and various forms of aspectual manipulation (eg, weaving, traceability) supporting all phases of the software lifecycle.This paper proposes novel use of text mining as a technology to assist in the discovery and verification of early aspects in requirements documents. A key result expected from text mining is a document summary whose content forms the foundation of domain-specific pattern identification as well as ontology construction.", "num_citations": "1\n", "authors": ["124"]}