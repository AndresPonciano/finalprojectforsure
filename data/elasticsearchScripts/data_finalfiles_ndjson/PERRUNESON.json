{"title": "Experimentation in software engineering\n", "abstract": " Like other sciences and engineering disciplines, software engineering requires a cycle of model building, experimentation, and learning. Experiments are valuable tools for all software engineers who are involved in evaluating and choosing between different methods, techniques, languages and tools. The purpose of Experimentation in Software Engineering is to introduce students, teachers, researchers, and practitioners to empirical studies in software engineering, using controlled experiments. The introduction to experimentation is provided through a process perspective, and the focus is on the steps that we have to go through to perform an experiment. The book is divided into three parts. The first part provides a background of theories and methods used in experimentation. Part II then devotes one chapter to each of the five experiment steps: scoping, planning, execution, analysis, and result presentation. Part III completes the presentation with two examples. Assignments and statistical material are provided in appendixes. Overall the book provides indispensable information regarding empirical studies in particular for experiments, but also for case studies, systematic literature reviews, and surveys. It is a revision of the authors\u2019 book, which was published in 2000. In addition, substantial new material, eg concerning systematic literature reviews and case study research, is introduced. The book is self-contained and it is suitable as a course book in undergraduate or graduate studies where the need for empirical studies in software engineering is stressed. Exercises and assignments are included to combine the more theoretical material with\u00a0\u2026", "num_citations": "8167\n", "authors": ["142"]}
{"title": "Guidelines for conducting and reporting case study research in software engineering\n", "abstract": " Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors\u2019 own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case\u00a0\u2026", "num_citations": "4242\n", "authors": ["142"]}
{"title": "Detection of duplicate defect reports using natural language processing\n", "abstract": " Defect reports are generated from various testing and development activities in software engineering. Sometimes two reports are submitted that describe the same problem, leading to duplicate reports. These reports are mostly written in structured natural language, and as such, it is hard to compare two reports for similarity with formal methods. In order to identify duplicates, we investigate using natural language processing (NLP) techniques to support the identification. A prototype tool is developed and evaluated in a case study analyzing defect reports at Sony Ericsson mobile communications. The evaluation shows that about 2/3 of the duplicates can possibly be found using the NLP techniques. Different variants of the techniques provide only minor result differences, indicating a robust technology. User testing shows that the overall attitude towards the technique is positive and that it has a growth potential.", "num_citations": "652\n", "authors": ["142"]}
{"title": "Software product line testing\u2013a systematic mapping study\n", "abstract": " ContextSoftware product lines (SPL) are used in industry to achieve more efficient software development. However, the testing side of SPL is underdeveloped.ObjectiveThis study aims at surveying existing research on SPL testing in order to identify useful approaches and needs for future research.MethodA systematic mapping study is launched to find as much literature as possible, and the 64 papers found are classified with respect to focus, research type and contribution type.ResultsA majority of the papers are of proposal research types (64%). System testing is the largest group with respect to research focus (40%), followed by management (23%). Method contributions are in majority.ConclusionsMore validation and evaluation research is needed to provide a better foundation for SPL testing.", "num_citations": "388\n", "authors": ["142"]}
{"title": "A systematic review on regression test selection techniques\n", "abstract": " Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations.", "num_citations": "338\n", "authors": ["142"]}
{"title": "A survey of unit testing practices\n", "abstract": " Unit testing is testing of individual units or groups of related units. What are a company's typical strengths and weaknesses when applying unit testing? Per Beremark and the author surveyed unit testing practices on the basis of focus group discussions in software process improvement network (SPIN) and launched a questionnaire to validate the results. This survey is an indication of unit testing in several companies. You can use the questionnaire at your own company to clarify what you mean by unit testing, to identify the strengths and weaknesses of your unit testing practices, and to compare with other organizations to improve those practices", "num_citations": "291\n", "authors": ["142"]}
{"title": "Combining agile methods with stage-gate project management\n", "abstract": " We encountered cultural changes and different management and engineering viewpoints in a study of three large software development organizations that started using agile methods. Case studies at three large software product companies show the benefits and pitfalls of integrating agile methods with stage-gate management models.", "num_citations": "262\n", "authors": ["142"]}
{"title": "Using students as experiment subjects\u2013an analysis on graduate and freshmen student data\n", "abstract": " The question whether students can be used as subjects in software engineering experiments is debated. In order to investigate the feasibility of using students as subjects, a study is conducted in the context of the Personal Software Process (PSP) in which the performance of freshmen students and graduate students are compared and also related to another study in an industrial setting. The hypothesis is that graduate students perform similarly to industry personnel, while freshmen student\u2019s performance differ. A quantitative analysis compares the freshmen\u2019and graduate students. The improvement trends are also compared to industry data, although limited data access does not allow a full comparison. It can be concluded that very much the same improvement trends can be identified for the three groups. However, the dispersion is larger in the freshmen group. The absolute levels of the measured characteristics are significantly different between the student groups primarily with respect to time, ie graduate students do the tasks in shorter time. The data does not give a sufficient answer to the hypothesis, but is a basis for further studies on the issue.", "num_citations": "260\n", "authors": ["142"]}
{"title": "Integrating agile software development into stage-gate managed product development\n", "abstract": " Agile methods have evolved as a bottom-up approach to software development. However, as the software in embedded products is only one part of development projects, agile methods must coexist with project management models typically of the stage-gate type. This paper presents a qualitative case study of two large independent software system projects that have used eXtreme Programming (XP) for software development within contexts of stage-gate project management models. The study is comprised of open ended interviews with managers as well as practitioners, followed by a structured, fully traceable, qualitative analysis. We conclude that it is possible to integrate XP in a gate model context. Key issues for success are the interfaces towards the agile subproject and management attitudes towards the agile approach.", "num_citations": "207\n", "authors": ["142"]}
{"title": "An experimental comparison of usage-based and checklist-based reading\n", "abstract": " Software quality can be defined as the customers' perception of how a system works. Inspection is a method to monitor and control the quality throughout the development cycle. Reading techniques applied to inspections help reviewers to stay focused on the important parts of an artifact when inspecting. However, many reading techniques focus on finding as many faults as possible, regardless of their importance. Usage-based reading helps reviewers to focus on the most important parts of a software artifact from a user's point of view. We present an experiment, which compares usage-based and checklist-based reading. The results show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. Usage-based reading may be preferable for software organizations that utilize or start utilizing\u00a0\u2026", "num_citations": "199\n", "authors": ["142"]}
{"title": "A replicated quantitative analysis of fault distributions in complex software systems\n", "abstract": " To contribute to the body of empirical research on fault distributions during development of complex software systems, a replication of a study of Fenton and Ohlsson is conducted. The hypotheses from the original study are investigated using data taken from an environment that differs in terms of system size, project duration, and programming language. We have investigated four sets of hypotheses on data from three successive telecommunications projects: 1) the Pareto principle, that is, a small number of modules contain a majority of the faults (in the replication, the Pareto principle is confirmed), 2) fault persistence between test phases (a high fault incidence in function testing is shown to imply the same in system testing, as well as prerelease versus postrelease fault incidence), 3) the relation between number of faults and lines of code (the size relation from the original study could be neither confirmed nor\u00a0\u2026", "num_citations": "191\n", "authors": ["142"]}
{"title": "Recovering from a decade: a systematic mapping of information retrieval approaches to software traceability\n", "abstract": " Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude\u00a0\u2026", "num_citations": "171\n", "authors": ["142"]}
{"title": "A qualitative survey of regression testing practices\n", "abstract": " Aim: Regression testing practices in industry have to be better understood, both for the industry itself and for the research community. Method: We conducted a qualitative industry survey by i) running a focus group meeting with 15 industry participants and ii) validating the outcome in an on line questionnaire with 32 respondents. Results: Regression testing needs and practices vary greatly between and within organizations and at different stages of a project. The importance and challenges of automation is clear from the survey. Conclusions: Most of the findings are general testing issues and are not specific to regression testing. Challenges and good practices relate to test automation and testability issues.", "num_citations": "154\n", "authors": ["142"]}
{"title": "What do we know about defect detection methods?[software testing]\n", "abstract": " A survey of defect detection studies comparing inspection and testing techniques yields practical recommendations: use inspections for requirements and design defects, and use testing for code. Evidence-based software engineering can help software practitioners decide which methods to use and for what purpose. EBSE involves defining relevant questions, surveying and appraising avail able empirical evidence, and integrating and evaluating new practices in the target environment. This article helps define questions regarding defect detection techniques and presents a survey of empirical studies on testing and inspection techniques. We then interpret the findings in terms of practical use. The term defect always relates to one or more underlying faults in an artifact such as code. In the context of this article, defects map to single faults", "num_citations": "144\n", "authors": ["142"]}
{"title": "Empirical evaluations of regression test selection techniques: a systematic review\n", "abstract": " Regression testing is the verification that previously functioning software remains after a change. In this paper we report on a systematic review of empirical evaluations of regression test selection techniques, published in major software engineering journals and conferences. Out of 2,923 papers analyzed in this systematic review, we identified 28 papers reporting on empirical comparative evaluations of regression test selection techniques. They report on 38 unique studies (23 experiments and 15 case studies), and in total 32 different techniques for regression test selection are evaluated. Our study concludes that no clear picture of the evaluated techniques can be provided based on existing empirical evidence, except for a small group of related techniques. Instead, we identified a need for more and better empirical studies were concepts are evaluated rather than small variations. It is also necessary to carefully\u00a0\u2026", "num_citations": "133\n", "authors": ["142"]}
{"title": "Certification of software components\n", "abstract": " Reuse is becoming one of the key areas in dealing with the cost and quality of software systems. An important issue is the reliability of the components, hence making certification of software components a critical area. The objective of this article is to try to describe methods that can be used to certify and measure the ability of software components to fulfil the reliability requirements placed on them. A usage modelling technique is presented, which can be used to formulate usage models for components. This technique will make it possible not only to certify the components, but also to certify the system containing the components. The usage model describes the usage from a structural point of view, which is complemented with a profile describing the expected usage in figures. The failure statistics from the usage test form the input of a hypothesis certification model, which makes it possible to certify a specific\u00a0\u2026", "num_citations": "133\n", "authors": ["142"]}
{"title": "Checklists for software engineering case study research\n", "abstract": " Case study is an important research methodology for software engineering. We have identified the need for checklists supporting researchers and reviewers in conducting and reviewing case studies. We derived checklists for researchers and reviewers respectively, using systematic qualitative procedures. Based on nine sources on case studies, checklists are derived and validated, and hereby presented for further use and improvement.", "num_citations": "119\n", "authors": ["142"]}
{"title": "Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts\n", "abstract": " Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial\u00a0\u2026", "num_citations": "110\n", "authors": ["142"]}
{"title": "Capture\u2013recapture in software inspections after 10 years research\u2013\u2013theory, evaluation and application\n", "abstract": " Software inspection is a method to detect faults in the early phases of the software life cycle. In order to estimate the number of faults not found, capture\u2013recapture was introduced for software inspections in 1992 to estimate remaining faults after an inspection. Since then, several papers have been written in the area, concerning the basic theory, evaluation of models and application of the method. This paper summarizes the work made in capture\u2013recapture for software inspections during these years. Furthermore, and more importantly, the contribution of the papers are classified as theory, evaluation or application, in order to analyse the performed research as well as to highlight the areas of research that need further work. It is concluded that (1) most of the basic theory is investigated within biostatistics, (2) most software engineering research is performed on evaluation, a majority ending up in recommendation of\u00a0\u2026", "num_citations": "102\n", "authors": ["142"]}
{"title": "Open innovation in software engineering: a systematic mapping study\n", "abstract": " Open innovation (OI) means that innovation is fostered by using both external and internal influences in the innovation process. In software engineering (SE), OI has existed for decades, while we currently see a faster and broader move towards OI in SE. We therefore survey research on how OI takes place and contributes to innovation in SE. This study aims to synthesize the research knowledge on OI in the SE domain. We launched a systematic mapping study and conducted a thematic analysis of the results. Moreover, we analyzed the strength of the evidence in the light of a rigor and relevance assessment of the research. We identified 33 publications, divided into 9 themes related to OI. 17/33 studies fall in the high\u2013rigor/high\u2013relevance category, suggesting the results are highly industry relevant. The research indicates that start-ups have higher tendency to opt for OI compared to incumbents. The\u00a0\u2026", "num_citations": "90\n", "authors": ["142"]}
{"title": "Verification and validation in industry-a qualitative survey on the state of practice\n", "abstract": " Verification and validation activities take a substantial share of project budgets and need improvements. This is an accepted truth, but the current practices are seldom assessed and analyzed. In this paper we present a qualitative survey of the verification and validation processes at 11 Swedish companies. The purpose was to exchange experience between the companies and to lay out a foundation for further research on the topic. The survey is conducted through workshop and interview sessions, loosely guided by a questionnaire scheme. It is concluded from the survey that there are substantial differences between small and large companies. In large companies, the documented process is emphasized while in small companies, single key, persons have a dominating impact on the procedures. Large companies use commercial tools while small companies make in-house tools or use shareware. Common to all\u00a0\u2026", "num_citations": "87\n", "authors": ["142"]}
{"title": "Reference-based search strategies in systematic reviews\n", "abstract": " In systematic reviews, the number of articles found by search strings tend to be very large. In order to limit the number of articles to handle manually, we investigate a search strategy based on references between papers. We first identify a \u201ctake-off paper\u201d which is the starting point for the search and then we follow the references from that paper. We also investigate \u201ccardinal papers\u201d, ie papers that are referenced by many authors, and let the references to those papers guide the selection in the systematic review. We evaluate the search strategies on three published systematic reviews. The results vary greatly between the three studied systematic reviews, from 88% reduction to 92% extension of the original paper set.", "num_citations": "74\n", "authors": ["142"]}
{"title": "An experimental evaluation of capture\u2010recapture in software inspections\n", "abstract": " The use of capture\u2010recapture to estimate the residual faults in a software artifact has evolved as a promising method. However, the assumptions needed to make the estimates are not completely fulfilled in software development, leading to an underestimation of the residual fault content. Therefore, a method employing a filtering technique with an experience factor to improve the estimate of the residual faults is proposed in this paper. An experimental study of the capture\u2010recapture method with this correction method has been conducted. It is concluded that the correction method improves the capture\u2010recapture estimate of the number of residual defects in the inspected document.", "num_citations": "71\n", "authors": ["142"]}
{"title": "Improving Regression Testing Transparency and Efficiency with History-Based Prioritization--An Industrial Case Study\n", "abstract": " Background: History based regression testing was proposed as a basis for automating regression test selection, for the purpose of improving transparency and test efficiency, at the function test level in a large scale software development organization. Aim: The study aims at investigating the current manual regression testing process as well as adopting, implementing and evaluating the effect of the proposed method. Method: A case study was launched including: identification of important factors for prioritization and selection of test cases, implementation of the method, and a quantitative and qualitative evaluation. Results: 10 different factors, of which two are history-based, are identified as important for selection. Most of the information needed is available in the test management and error reporting systems while some is embedded in the process. Transparency is increased through a semi-automated method. Our\u00a0\u2026", "num_citations": "65\n", "authors": ["142"]}
{"title": "Test processes in software product evolution\u2014a qualitative survey on the state of practice\n", "abstract": " In order to understand the state of test process practices in the software industry, we have conducted a qualitative survey, covering software development departments at 11 companies in Sweden of different sizes and application domains. The companies develop products in an evolutionary manner, which means either new versions are released regularly, or new product variants under new names are released. The survey was conducted through workshop and interview sessions, loosely guided by a questionnaire scheme. The main conclusions of the survey are that the documented development process is emphasized by larger organizations as a key asset, while smaller organizations tend to lean more on experienced people. Further, product volution is performed primarily as new product variants for embedded systems, and as new versions for packaged software. The development is structured using\u00a0\u2026", "num_citations": "63\n", "authors": ["142"]}
{"title": "Efficient evaluation of multifactor dependent system performance using fractional factorial design\n", "abstract": " Performance of computer-based systems may depend on many different factors, internal and external. In order to design a system to have the desired performance or to validate that the system has the required performance, the effect of the influencing factors must be known. Common methods give no or little guidance on how to vary the factors during prototyping or validation. Varying the factors in all possible combinations would be too expensive and too time-consuming. This paper introduces a systematic approach to the prototyping and the validation of a system's performance, by treating the prototyping or validation as an experiment, in which the fractional factorial design methodology is commonly used. To show that this is possible, a case study evaluating the influencing factors of the false and real target rate of a radar system is described. Our findings show that prototyping and validation of system\u00a0\u2026", "num_citations": "59\n", "authors": ["142"]}
{"title": "A second replicated quantitative analysis of fault distributions in complex software systems\n", "abstract": " Background: Software engineering is searching for general principles that apply across contexts, for example, to help guide software quality assurance. Fenton and Ohlsson presented such observations on fault distributions, which have been replicated once. Objectives: We aimed to replicate their study again to assess the robustness of the findings in a new environment, five years later. Method: We conducted a literal replication, collecting defect data from five consecutive releases of a large software system in the telecommunications domain, and conducted the same analysis as in the original study. Results: The replication confirms results on unevenly distributed faults over modules, and that fault proneness distributions persist over test phases. Size measures are not useful as predictors of fault proneness, while fault densities are of the same order of magnitude across releases and contexts. Conclusions: This\u00a0\u2026", "num_citations": "51\n", "authors": ["142"]}
{"title": "A replicated experiment of usage-based and checklist-based reading\n", "abstract": " Software inspection is an effective method to detect faults in software artefacts. Several empirical studies have been performed on reading techniques, which are used in the individual preparation phase of software inspections. Besides new experiments, replications are needed to increase the body of knowledge in software inspections. We present a replication of an experiment, which compares usage-based and checklist-based reading. The results of the original experiment show that reviewers applying usage-based reading are more efficient and effective in detecting the most critical faults from a user's point of view than reviewers using checklist-based reading. We present the data of the replication together with the original experiment and compares the experiments. The main result of the replication is that it confirms the result of the original experiment. This replication strengthens the evidence that usage-based\u00a0\u2026", "num_citations": "51\n", "authors": ["142"]}
{"title": "A case study of the class firewall regression test selection technique on a large scale distributed software system\n", "abstract": " Regression testing is expensive and may consume much, of organizations' software development budgets. Thus, it is of interest to reduce the total time devoted to test execution by using test selection techniques. Many techniques have been proposed but few have been evaluated on real-world, large scale systems. In this paper we report on an empirical evaluation of using the class firewall regression test selection technique, in combination with scenario testing, on a large scale industrial software system using the Java byte code in the analysis. The study was performed, on a large complex distributed software system in one of Sweden's largest banks. Effects of using scenario testing together with regression test selection are reported. The results are that not all test cases were selected by the class firewall selection technique. Using scenario testing, where test cases are dependent, affects the number of test cases\u00a0\u2026", "num_citations": "50\n", "authors": ["142"]}
{"title": "An empirical evaluation of regression testing based on fix-cache recommendations\n", "abstract": " Background: The fix-cache approach to regression test selection was proposed to identify the most fault-prone files and corresponding test cases through analysis of fixed defect reports. Aim: The study aims at evaluating the efficiency of this approach, compared to the previous regression test selection strategy in a major corporation, developing embedded systems. Method: We launched a post-hoc case study applying the fix-cache selection method during six iterations of development of a multi-million LOC product. The test case execution was monitored through the test management and defect reporting systems of the company. Results: From the observations, we conclude that the fix-cache method is more efficient in four iterations. The difference is statistically significant at \u03b1 = 0.05. Conclusions: The new method is significantly more efficient in our case study. The study will be replicated in an environment with\u00a0\u2026", "num_citations": "49\n", "authors": ["142"]}
{"title": "A spiral process model for case studies on software quality monitoring\u2014method and metrics\n", "abstract": " This article presents a spiral process model for an iterative case study on quality monitoring, conducted in an industrial environment. In a highly iterative project, everything seems to happen at the same time: analysis, design and testing. We propose a spiral process model for case studies, and present a study conducted according to the proposed process. In the study, metrics collected from three software development projects are analysed to investigate which characteristics are stable across projects and feature groups of the product. The contribution of the article is multi\u2010fold, detailing the case study methodology used with its sub\u2010goals and procedures. Furthermore, the article presents the metrics collected and the results as such from the case study, which gives insights into a complex development environment and the trends of the retrieved data. The analysed data serve as feedback to the project staff to\u00a0\u2026", "num_citations": "49\n", "authors": ["142"]}
{"title": "A theory of distances in software engineering\n", "abstract": " ContextCoordinating a software project across distances is challenging. Even without geographical and time zone distances, other distances within a project can cause communication gaps. For example, organisational and cognitive distances between product owners and development-near roles such as developers and testers can lead to differences in understanding and interpretation of the business requirements. Applying good software development practices, known to enhance alignment and coordination within development projects, can alleviate these challenges.ObjectiveThe aim of our research is to identify and describe underlying factors which can explain why certain practices support aligning and coordinating software development projects.MethodWe have inductively generated a theory analysing empirical data consisting of 15 interviews from 5 different companies. The systematic and iterative\u00a0\u2026", "num_citations": "47\n", "authors": ["142"]}
{"title": "A minimal test practice framework for emerging software organizations\n", "abstract": " Testing takes a large share of software development efforts, and hence is of interest when seeking improvements. Several test process improvement frameworks exist, but they are extensive and much too large to be effective for smaller organizations. This paper presents a minimal test practice framework (MTPF) that allows the incremental introduction of appropriate practices at the appropriate time in rapidly expanding organizations. The process for introducing the practice framework tries to minimize resistance to change by maximizing the involvement of the entire organization in the improvement effort and ensuring that changes are made in small steps with a low threshold for each step. The practice framework created and its method of introduction have been evaluated at one company by applying the framework for a one\u2010year period. Twelve local software development companies have also evaluated the\u00a0\u2026", "num_citations": "47\n", "authors": ["142"]}
{"title": "Evaluation of usage-based reading\u2014conclusions after three experiments\n", "abstract": " Software inspections have been introduced in software engineering in order to detect faults before testing is performed. Reading techniques provide reviewers in software inspections with guidelines on how they should check the documents under inspection. Several reading techniques with different purposes have been introduced and empirically evaluated. In this paper, we describe a reading technique with the special aim to detect faults that are severe from a user\u2019s point of view. The reading technique is named usage-based reading (UBR) and it can be used to inspect all software artefacts. In the series of experiments, a high-level design document is used. The main focus of the paper is on the third experiment, which investigates the information needed for UBR in the individual preparation and the meeting of software inspections. Hence, the paper discusses (1) the series of three experiments of UBR\u00a0\u2026", "num_citations": "47\n", "authors": ["142"]}
{"title": "A case study on regression test suite maintenance in system evolution\n", "abstract": " When a system is maintained, its automated test suites must also be maintained to keep the tests up to date. Even though practice indicates that test suite maintenance can be very costly we have seen few studies considering the actual efforts for maintenance of test-ware. We conducted a case study on an evolving system with three updated versions, changed with three different change strategies. Test suites for automated unit and functional tests were used for regression testing the extended applications. With one change strategy more changes were made in the tests code than in the system that was tested, and with another strategy no changes were needed for the unit tests to work.", "num_citations": "44\n", "authors": ["142"]}
{"title": "Experiences from teaching PSP for freshmen\n", "abstract": " The Personal Software Process (PSP) is launched as a means for improving software development capabilities for the individual engineer. It is proposed that it should be used in software engineering curricula; some authors propose it to be used already during the first student year. The PSP course is successfully given for graduate students at Lund University since 1996. During the spring semester of 1999, it was given to undergraduate students at the software engineering program in their first year of university studies, directly after their first programming course. This paper reports results and experiences from the course given to these freshmen students. A quantitative analysis is conducted to compare the freshmen student data to data from graduate student courses, and a qualitative evaluation is conducted concerning the differences between the courses. It can be concluded that mostly the same improvement\u00a0\u2026", "num_citations": "44\n", "authors": ["142"]}
{"title": "A survey of lead-time challenges in the development and evolution of distributed real-time systems\n", "abstract": " This paper presents a survey that identifies lead-time consumption in the development and evolution of distributed real-time systems DRTSs. Data has been collected through questionnaires, focused interviews and non-directive interviews with senior designers. Quantitative data has been analyzed using the Analytic Hierarchical Process (AHP). A trend in the 11 organizations is that there is a statistically significant shift of the main lead-time burden from programming to integration and testing, when distributing systems. From this, it is concluded that processes, tools and technologies that either reduce the need for or the time for testing have an impact on the development and evolution of lead-time of DRTSs.", "num_citations": "41\n", "authors": ["142"]}
{"title": "Experience from replicating empirical studies on prediction models\n", "abstract": " When conducting empirical studies, replications are important contributors to investigating the generality of the studies. By replicating a study in another context, we investigate what impact the specific environment has, related to the effect of the studied object. In this paper, we define different levels of replication to characterise the similarities and differences between an original study and a replication, with particular focus on prediction models for the identification of fault-prone software components. Further, we derive a set of issues and concerns which are important in order to enable replication of an empirical study and to enable practitioners to use the results. To illustrate the importance of the issues raised, a replication case study is presented in the domain of prediction models for fault-prone software components. It is concluded that the results are very divergent, depending on how different parameters are\u00a0\u2026", "num_citations": "37\n", "authors": ["142"]}
{"title": "Bridges and barriers to hardware-dependent software ecosystem participation\u2013a case study\n", "abstract": " BackgroundSoftware ecosystems emerged as means for several actors to jointly provide more value to the market than any of them can do on its own. Recently, software ecosystems are more often used to support the development of hardware-dependent solutions.ObjectivesThis work aims at studying barriers and bridges to participation in an ecosystem with substantial hardware dependencies.MethodWe conducted an interview-based case study of an ecosystem around Axis\u2019 network video surveillance systems, interviewing 10 internal experts and 8 external representatives of 6 companies, complemented by document studies at Axis.ResultsMajor bridges to the ecosystem include end customer demands, open and transparent communication and relationship, as well as internal and external standardizations. Barriers include the two-tier business model, entry barriers and execution performance issues\u00a0\u2026", "num_citations": "36\n", "authors": ["142"]}
{"title": "An experimental evaluation of inspection and testing for detection of design faults\n", "abstract": " The two most common strategies for verification and validation, inspection and testing, are in a controlled experiment evaluated in terms of their fault detection capabilities. These two techniques are in the previous work compared applied to code. In order to compare the efficiency and effectiveness of these techniques on a higher abstraction level than code, this experiment investigates inspection of design documents and testing of the corresponding program, to detect faults originating from the design document. Usage-based reading (UBR) and usage-based testing (UBT) were chosen for inspections and testing, respectively. These techniques provide similar aid to the reviewers as to the testers. The purpose of both fault detection techniques is to focus the inspection and testing from a user's viewpoint. The experiment was conducted with 51 Master's students in a two-factor blocked design; each student applied\u00a0\u2026", "num_citations": "36\n", "authors": ["142"]}
{"title": "Detection or isolation of defects? An experimental comparison of unit testing and code inspection\n", "abstract": " Code inspections and white-box testing have both been used for unit testing. One is a static analysis technique, the other, a dynamic one, since it is based on executing test cases. Naturally, the question arises whether one is superior to the other, or, whether either technique is better suited to detect or isolate certain types of defects. We investigated this question with an experiment with a focus on detection of the defects (failures) and isolation of the underlying sources of the defects (faults). The results indicate that there exist significant differences for some of the effects of using code inspection versus testing. White-box testing is more effective, i.e. detects significantly more defects while inspection isolates the underlying source of a larger share of the defects detected. Testers spend significantly more time, hence the difference in efficiency is smaller, and is not statistically significant. The two techniques are also\u00a0\u2026", "num_citations": "35\n", "authors": ["142"]}
{"title": "Challenges in flexible safety-critical software development\u2013an industrial qualitative survey\n", "abstract": " Context. Development of safety-critical systems is mostly governed by process-heavy paradigms, while increasing demands on flexibility and agility also reach this domain. Objectives. We wanted to explore in more detail the industrial needs and challenges when facing this trend. Method. We launched a qualitative survey, interviewing engineers from four companies in four different industry domains. Results. The survey identifies human factors (skills, experience, and attitudes) being key in safety-critical systems development, as well as good documentation. Certification cost is related to change frequency, which is limiting flexibility. Component reuse and iterative processes were found to increase adaptability to changing customer needs. Conclusions. We conclude that agile development and flexibility may co-exist with safety-critical software development, although there are specific challenges to\u00a0\u2026", "num_citations": "34\n", "authors": ["142"]}
{"title": "V-GQM: A feed-back approach to validation of a GQM study\n", "abstract": " The GQM (Goals/Questions/Metrics) paradigm is used for empirical studies on software projects. Support is given on how to define and execute a study. However, the support for running several subsequent studies is poor. V-GQM (Validating GQM) introduces a life-cycle perspective, creating a process spanning several GQM studies. After the GQM study has been completed, an analysis step of the plan is initiated. The metrics are analysed to investigate if they comply with the plan or if they have extended it, and also to investigate if the metrics collected answer more questions than are posed in the plan. The questions derived from the metrics are then used to form the goal for the next GQM study, effectively introducing a feedback loop. By introducing a bottom-up approach, a structured analysis of each GQM study is possible when constructing several consecutive GQM studies. A case study using V-GQM has been\u00a0\u2026", "num_citations": "30\n", "authors": ["142"]}
{"title": "Aggregating viewpoints for strategic software process improvement-a method and a case study\n", "abstract": " Decisions regarding strategic software process improvement (SPI) are generally based on the management's viewpoint of the situation, and in some cases also the viewpoints of some kind of an SPI group. This may result in strategies which are not accepted throughout the organisation, as the views of how the process is functioning are different throughout the company. A method for identifying the major factors affecting a process-improvement goal and how the perception of the importance of the factors varies throughout the organisation are described The method lets individuals from the whole development organisation rate the expected effect of these factors from their own viewpoint. In this way the strategic SPI decision can be taken using input from the entire organisation, and any discrepancies in the ratings can also give important SPI-decision information. The method is applied to a case study performed at\u00a0\u2026", "num_citations": "29\n", "authors": ["142"]}
{"title": "Capture-recapture estimations for perspective-based reading-a simulated experiment\n", "abstract": " Inspections are established as important contributors to software quality. In order to improve the quality control and the efficiency of inspections, new methods and models are presented. Capture-recapture (CRC) models estimate the number of remaining faults after an inspection. Perspective-based reading (PBR) focuses different inspectors on different areas, thus reducing overlap in terms of detection effort and faults found. The combination of PBR and CRC is, however, assumed to give problems, since the prerequisites of the two techniques are partly contradictory. In order to investigate whether this is a practical problem, a simulated experiment is designed. The experiment results indicate that the CRC estimators are rather robust to simulated PBR data. For three inspectors, Mh-JK and DPM are superior, while for six inspectors, five out of six investigated estimators show acceptable results. The DPM estimator gives acceptable estimates for three inspectors, but not for the six-inspector case. The experiment gives a basic understanding on the relation between PBR and CRC.", "num_citations": "29\n", "authors": ["142"]}
{"title": "A theory of openness for software engineering tools in software organizations\n", "abstract": " ContextThe increased use of Open Source Software (OSS) affects how software-intensive product development organizations (SIPDO) innovate and compete, moving them towards Open Innovation (OI). Specifically, software engineering tools have the potential for OI, but require better understanding regarding what to develop internally and what to acquire from outside the organization, and how to cooperate with potential competitors.AimThis paper aims at synthesizing a theory of openness for software engineering tools in SIPDOs, that can be utilized by managers in defining more efficient strategies towards OSS communities.MethodWe synthesize empirical evidence from a systematic mapping study, a case study, and a survey, using a narrative method. The synthesis method entails four steps: (1) Developing a preliminary synthesis, (2) Exploring the relationship between studies, (3) Assessing the validity of the\u00a0\u2026", "num_citations": "28\n", "authors": ["142"]}
{"title": "Evaluation of a perspective based review method applied in an industrial setting\n", "abstract": " Inspections are an important means for developing high quality software. The effects of using a perspective based review method with a testing perspective (PBRM(T)) on requirements specifications have been evaluated in an industrial project. PBRM(T) was employed and compared to the normal checklist-based review method (CBRM) which was used earlier in the project. The effects studied were the effectiveness, the efficiency and the number of system faults related to requirements specifications knowledge. Inspection data from CBRM and PBRM(T) were analysed and compared. Problem report data were also analysed and a small survey was used for evaluation of the two methods. PBRM(T) showed an effectiveness of 0.3 faults found per requirement compared with 0.05 with CBRM, or 2.4 faults found per page compared with 0.5 with CBRM in the case study. The efficiency with PBRM(T) was 1.9 faults found\u00a0\u2026", "num_citations": "28\n", "authors": ["142"]}
{"title": "Continuous experimentation and a/b testing: A mapping study\n", "abstract": " Background. Continuous experimentation (CE) has recently emerged as an established industry practice and as a research subject. Our aim is to study the application of CE and A/B testing in various industrial contexts. Objective. We wanted to investigate whether CE is used in different sectors of industry, by how it is reported in academic studies. We also wanted to explore the main topics researched to give an overview of the subject and discuss future research directions. Method. We performed a systematic mapping study of the published literature and included 62 papers, using a combination of database search and snowballing. Results. Most reported software experiments are done online and with software delivered as a service, although varied exemptions exist for e.g., financial software and games. The most frequently researched topics are challenges to conduct experiments and statistical methods for\u00a0\u2026", "num_citations": "27\n", "authors": ["142"]}
{"title": "Using a visual abstract as a lens for communicating and promoting design science research in software engineering\n", "abstract": " Empirical software engineering research aims to generate prescriptive knowledge that can help software engineers improve their work and overcome their challenges, but deriving these insights from real-world problems can be challenging. In this paper, we promote design science as an effective way to produce and communicate prescriptive knowledge. We propose using a visual abstract template to communicate design science contributions and highlight the main problem/solution constructs of this area of research, as well as to present the validity aspects of design knowledge. Our conceptualization of design science is derived from existing literature and we illustrate its use by applying the visual abstract to an example use case. This is work in progress and further evaluation by practitioners and researchers will be forthcoming.", "num_citations": "27\n", "authors": ["142"]}
{"title": "Technical requirements for the implementation of an experience base\n", "abstract": " Reuse of different types of experience is a key issue in successful improvement in software engineering. The approach taken in the Quality Improvement Paradigm (QIP) and its supporting organisation, the Experience Factory (EF), is to define an Experience Base (EB), where all types of reusable objects are organised and stored. The objects may be of any type related to software development, for example, code, methods, quality models and specifications. Furthermore, not only objects identical to the target object needed can be reused, but also similar ones. This paper defines important concepts in a reuse context, elaborates a reuse scenario from which the requirements for the implementation of an Experience Base are derived. A list of 22 technical requirements is defined which cover the functional aspects related to the use of an Experience Base. The evaluation of three alternative implementation\u00a0\u2026", "num_citations": "27\n", "authors": ["142"]}
{"title": "How much information is needed for usage-based reading? A series of experiments\n", "abstract": " Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts of a software document by using prioritized use cases. This paper presents a series of three UBR experiments on design specifications, with focus on the third. The first experiment evaluates the prioritization of UBR and the second compares UBR against checklist-based reading. The third experiment investigates the amount of information needed in the use cases and whether a more active approach helps the reviewers to detect more faults. The third study was conducted at two different\u00a0\u2026", "num_citations": "26\n", "authors": ["142"]}
{"title": "Confidence intervals for capture\u2013recapture estimations in software inspections\n", "abstract": " Software inspections are an efficient method to detect faults in software artefacts. In order to estimate the fault content remaining after inspections, a method called capture\u2013recapture has been introduced. Most research published in fault content estimations for software inspections has focused upon point estimations. However, confidence intervals provide more information of the estimation results and are thus preferable. This paper replicates a capture\u2013recapture study and investigates confidence intervals for capture\u2013recapture estimators using data sets from two recently conducted software inspection experiments. Furthermore, a discussion of practical application of capture\u2013recapture with confidence intervals is provided. In capture\u2013recapture, used for software inspection, most research papers have reported Mh-JK to be the best estimator, but only one study has investigated its subestimators. In addition\u00a0\u2026", "num_citations": "25\n", "authors": ["142"]}
{"title": "Test overlay in an emerging software product line\u2013an industrial case study\n", "abstract": " ContextIn large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests.AimsThis study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy.MethodAn in-depth case study was launched including both qualitative and quantitative observations.ResultsA high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test\u00a0\u2026", "num_citations": "24\n", "authors": ["142"]}
{"title": "Applying sampling to improve software inspections\n", "abstract": " The main objective of software inspections is to find faults in software documents. The benefits of inspections are reported from researchers as well as software organizations. However, inspections are time consuming and the resources may not be sufficient to inspect all documents. Sampling of documents in inspections provides a systematic solution to select what to be inspected in the case resources are not sufficient to inspect everything. The method presented in this paper uses sampling, inspection and resource scheduling to increase the efficiency of an inspection session. A pre-inspection phase is used in order to determine which documents need most inspection time, i.e. which documents contain most faults. Then, the main inspection is focused on these documents. We describe the sampling method and provide empirical evidence, which indicates that the method is appropriate to use. A Monte Carlo\u00a0\u2026", "num_citations": "24\n", "authors": ["142"]}
{"title": "A taxonomy of orthogonal properties of software architecture\n", "abstract": " There are several suggestions regarding the definition of the term software architecture. Yet discussions concerning the subject are ambiguous. This leads to unclear discussions and slow scientific progress. This paper suggests a coherent set of properties-a Taxonomy of Orthogonal Properties of Software Architecture (TOPSA)-that enables more precise communication within the software architecture domain. The TOPSA has been empirically validated by using it during an industrial development project. The paper shows some uses of the TOPSA in education and system development. It is concluded that the TOPSA is applicable for a wide range of uses. The TOPSA sharpens the conceptual tools available in the domain, resulting in more precise preservation, retrieval and use of knowledge within the software architecture domain.", "num_citations": "24\n", "authors": ["142"]}
{"title": "How software engineering research aligns with design science: a review\n", "abstract": " Background Assessing and communicating software engineering research can be challenging. Design science is recognized as an appropriate research paradigm for applied research, but is rarely explicitly used as a way to present planned or achieved research contributions in software engineering. Applying the design science lens to software engineering research may improve the assessment and communication of research contributions.   Aim The aim of this study is 1) to understand whether the design science lens helps summarize and assess software engineering research contributions, and 2) to characterize different types of design science contributions in the software engineering literature.   Method In previous research, we developed a visual abstract template, summarizing the core constructs of the design science paradigm. In this study, we use this template in a review of a set of 38 award winning\u00a0\u2026", "num_citations": "23\n", "authors": ["142"]}
{"title": "A machine learning approach for semi-automated search and selection in literature studies\n", "abstract": " Background. Search and selection of primary studies in Systematic Literature Reviews (SLR) is labour intensive, and hard to replicate and update. Aims. We explore a machine learning approach to support semi-automated search and selection in SLRs to address these weaknesses. Method. We 1) train a classifier on an initial set of papers, 2) extend this set of papers by automated search and snowballing, 3) have the researcher validate the top paper, selected by the classifier, and 4) update the set of papers and iterate the process until a stopping criterion is met. Results. We demonstrate with a proof-of-concept tool that the proposed automated search and selection approach generates valid search strings and that the performance for subsets of primary studies can reduce the manual work by half. Conclusions. The approach is promising and the demonstrated advantages include cost savings and replicability. The\u00a0\u2026", "num_citations": "23\n", "authors": ["142"]}
{"title": "A classification scheme for studies on fault-prone components\n", "abstract": " Various approaches are presented in the literature to identify faultprone components. The approaches represent a wide range of characteristics and capabilities, but they are not comparable, since different aspects are compared and different data sets are used. In order to enable a consistent and fair comparison, we propose a classification scheme, with two parts, 1) a characterisation scheme which captures information on input, output and model characteristics, and 2) an evaluation scheme which is designed for comparing different models\u2019 capabilities. The schemes and the rationale for the elements of the schemes are presented in the paper. Important capabilities to evaluate when comparing different models are rate of misclassification, classification efficiency and total classification cost. Further, the schemes are applied in an example study to illustrate the use of the schemes. It is expected that applying\u00a0\u2026", "num_citations": "23\n", "authors": ["142"]}
{"title": "A replicated study on duplicate detection: Using Apache Lucene to search among Android defects\n", "abstract": " Context: Duplicate detection is a fundamental part of issue management. Systems able to predict whether a new defect report will be closed as a duplicate, may decrease costs by limiting rework and collecting related pieces of information. Goal: Our work explores using Apache Lucene for large-scale duplicate detection based on textual content. Also, we evaluate the previous claim that results are improved if the title is weighted as more important than the description. Method: We conduct a conceptual replication of a well-cited study conducted at Sony Ericsson, using Lucene for searching in the public Android defect repository. In line with the original study, we explore how varying the weighting of the title and the description affects the accuracy. Results: We show that Lucene obtains the best results when the defect report title is weighted three times higher than the description, a bigger difference than has been\u00a0\u2026", "num_citations": "22\n", "authors": ["142"]}
{"title": "Engineering open innovation\u2013towards a framework for fostering open innovation\n", "abstract": " Open innovation is an emerging innovation paradigm that can greatly accelerate technical knowledge innovation in software companies. The increasing importance and density of software in today\u2019s products and services puts extensive pressure on excelling the discovery, description and execution of innovation. Despite that, software engineering literature lacks methods, tools and frameworks for full exploitation of technological advantages that open innovation can bring. This paper proposes a software engineering framework, designed to foster open innovation by designing and tailoring appropriate software engineering methods and tools. Furthermore, this paper discusses the methodological and process dimensions and outlines challenge areas that should be reviewed when transitioning to software engineering driven open innovation.", "num_citations": "22\n", "authors": ["142"]}
{"title": "Improving class firewall regression test selection by removing the class firewall\n", "abstract": " One regression test selection technique proposed for object-oriented programs is the Class firewall regression test selection technique. The selection technique selects test cases for regression test, which test changed classes and classes depending on changed classes. However, in empirical studies of the application of the technique, we observed that another technique found the same defects, selected fewer tests and required a simpler, less costly, analysis. The technique, which we refer to as the Change-based regression test selection technique, is basically the Class firewall technique, but with the class firewall removed. In this paper we formulate a hypothesis stating that these empirical observations are not incidental, but an inherent property of the Class firewall technique. We prove that the hypothesis holds for Java in a stable testing environment, and conclude that the effectiveness of the Class firewall\u00a0\u2026", "num_citations": "22\n", "authors": ["142"]}
{"title": "Variation factors in the design and analysis of replicated controlled experiments\n", "abstract": " In formal experiments on software engineering, the number of factors that may impact an outcome is very high. Some factors are controlled and change by design, while others are are either unforeseen or due to chance. This paper aims to explore how context factors change in a series of formal experiments and to identify implications for experimentation and replication practices to enable learning from experimentation. We analyze three experiments on code inspections and structural unit testing. The first two experiments use the same experimental design and instrumentation (replication), while the third, conducted by different researchers, replaces the programs and adapts defect detection methods accordingly (reproduction). Experimental procedures and location also differ between the experiments. Contrary to expectations, there are significant differences between the original experiment and the\u00a0\u2026", "num_citations": "21\n", "authors": ["142"]}
{"title": "The 4+ 1 view model of industry--academia collaboration\n", "abstract": " Industry--academia projects exist in complex contexts of various stakeholders, time perspectives, and goals. In order to analyze projects and communicate about them, we have defined an\" architectural\" model for industry--academia collaboration, inspired by Kruchten's software architecture model. The model has four views of i) time, ii) space, iii) activity and iv) domain, corresponding to the questions: when, where, how and what. The+ 1 view is the scenario, binding the other four together. We illustrate the model by applying it to the Industrial Excellence Center EASE and the Sigrun Software Innovation and Engineering Institute. The model helps analyzing industry--academia collaboration projects, to find gaps and reduce redundant work.", "num_citations": "21\n", "authors": ["142"]}
{"title": "Changes, evolution, and bugs\n", "abstract": " Changes in evolving software systems are often managed using an issue repository. This repository may contribute to information overload in an organization, but it may also help in navigating the software system. Software developers spend much effort on issue triage, a task in which the mere number of issue reports becomes a significant challenge. One specific difficulty is to determine whether a newly submitted issue report is a duplicate of an issue previously reported, if it contains complementary information related to a known issue, or if the issue report addresses something that has not been observed before. However, the large number of issue reports may also be used to help a developer to navigate the software development project to find related software artifacts, required both to understand the issue itself, and to analyze the impact of a possible issue resolution. This chapter presents\u00a0\u2026", "num_citations": "21\n", "authors": ["142"]}
{"title": "It Takes Two to Tango--An Experience Report on Industry--Academia Collaboration\n", "abstract": " Industry - academia collaboration is critical for empirical research to exist. However, there are many obstacles in the collaboration process. This paper reports on the experiences gained by the author, in a 2-year collaboration project on software testing which involved on-site work by the researcher in the industry premises. Based on notes, minutes of meetings, and progress reports, the project history is outlined. The project is analyzed, using collaboration models as a frame of reference. We conclude that there must be a balance between company 'pull' and academia 'push' in the collaboration Management support is inevitably a key factor to success, while other factors like cross-cultural skills and interfaces towards key resources also contribute.", "num_citations": "21\n", "authors": ["142"]}
{"title": "Software Product Line Testing--A 3D Regression Testing Problem\n", "abstract": " In software product line engineering, testing for regression concerns not only versions, as in one-off product development, but also regression across variants. We propose a 3D process model, with the dimensions of level, version and variant, to help analyze, plan and manage software product line testing. We derive the model from empirical observations of regression testing practice and software product line testing theory and practice, and look forward to see the model evaluated in practitioner-oriented research.", "num_citations": "21\n", "authors": ["142"]}
{"title": "Software quality assurance-concepts and misconceptions\n", "abstract": " Software quality engineering is concerned with building software products with required quality and assessing the level of quality. Software processes are important assets in achieving and assessing the software quality. Furthermore the adherence to defined processes is a key issue to having the software engineering under control and to enable process improvement. The software quality assurance (SQA) key process area in the capability maturity model (CMM) is elaborated, i.e. activities for monitoring adherence to the processes. The term SQA creates some misconceptions and confusions with the general quality management concept. The SQA concept is compared to quality management and the differences and similarities are elaborated in order to eliminate the misconceptions. Furthermore, some experiences from supporting the implementation of the SQA concepts in two different software companies are\u00a0\u2026", "num_citations": "21\n", "authors": ["142"]}
{"title": "A method proposal for early software reliability estimation.\n", "abstract": " This paper presents a method proposal for estimation of software reliability before the implementation phase. The method is based upon that a formal description technique is used and that it is possible to develop a tool performing dynamic analysis, ie locating semantic faults in the design. The analysis is performed with both applying a usage profile as input as well as doing a full analysis, ie locate all faults that the tool can find. The tool must provide failure data in terms of time since the last failure was detected. The mapping of the dynamic failures to the failures encountered during statistical usage testing and operation is discussed. The method can be applied either on the software specification or as a step in the development process by applying it on the design descriptions. The proposed method will allow for software reliability estimations that can be used both as a quality indicator, but also for planning and\u00a0\u2026", "num_citations": "20\n", "authors": ["142"]}
{"title": "Alignment practices affect distances in software development: a theory and a model\n", "abstract": " Coordinating a software project across distances is challenging. Even without geographical and time zone distances, other distances within a project can cause communication gaps. For example, organisational and cognitive distances between product owners and development-near roles such as developers and testers can lead to weak alignment of the software and the business requirements. Applying good software development practices, known to enhance alignment, can alleviate these challenges. We present a theoretical model called the Gap Model of how alignment practices affect different types of distances. This model has been inductively generated from empirical data. We also present an initial version of a theory based on this model that explains, at a general level, how practices affect communication within a project by impacting distances between people, activities and artefacts. The presented results\u00a0\u2026", "num_citations": "19\n", "authors": ["142"]}
{"title": "Robust estimations of fault content with capture\u2013recapture and detection profile estimators\n", "abstract": " Inspections are widely used in the software engineering community as efficient contributors to reduced fault content and improved product understanding. In order to measure and control the effect and use of inspections, the fault content after an inspection must be estimated. The capture\u2013recapture method, with its origin in biological sciences, is a promising approach for estimation of the remaining fault content in software artefacts. However, a number of empirical studies show that the estimates are neither accurate nor robust. In order to find robust estimates, i.e., estimates with small bias and variations, the adherence to the prerequisites for different estimation models is investigated. The basic hypothesis is that a model should provide better estimates the closer the actual sample distribution is to the model's theoretical distribution. Firstly, a distance measure is evaluated and secondly a \u03c72-based procedure is\u00a0\u2026", "num_citations": "19\n", "authors": ["142"]}
{"title": "IR in software traceability: From a bird's eye view\n", "abstract": " Background. Several researchers have proposed creating after-the-fact structure among software artifacts using trace recovery based on Information Retrieval (IR). Due to significant variation points in previous studies, results are not easily aggregated. Aim. We aim at an overview picture of the outcome of previous evaluations. Method. Based on a systematic mapping study, we perform a synthesis of published research. Results. Our synthesis shows that there are no empirical evidence that any IR model outperforms another model consistently. We also display a strong dependency between the Precision and Recall (P-R) values and the input datasets. Finally, our mapping of P-R values on the possible output space highlights the difficulty of recovering accurate trace links using na\u00efve cut-off strategies. Conclusion. Based on our findings, we stress the need for empirical evaluations beyond the basic P-R 'race'.", "num_citations": "17\n", "authors": ["142"]}
{"title": "A self-assessment framework for finding improvement objectives with ISO/IEC 29119 test standard\n", "abstract": " One of the latest additions in defining the test process is the upcoming ISO/IEC 29119 standard, which aims to define a universally applicable generic test process model. However, currently the standard does not offer any support for the adoption process of the model. In this paper, we present our framework, which aims to combine a maturity level-based approach with the standard process. Our objective was to create an easy-to-use framework for organizations to assess how closely their existing test process follows the standard, and give feedback on improvement objectives. Our results indicate that combining maturity levels with the standard is a viable approach to assess the implementation of the standard in practice.", "num_citations": "16\n", "authors": ["142"]}
{"title": "Open collaborative data-using OSS principles to share data in SW engineering\n", "abstract": " Reliance on data for software systems engineering is increasing, e.g., to train machine learning applications. We foresee increasing costs for data collection and maintenance, leading to the risk of development budgets eaten up by commodity features, thus leaving little resources for differentiation and innovation. We therefore propose Open Collaborative Data (OCD) - a concept analogous to Open Source Software (OSS) - as a means to share data. In contrast to Open Data (OD), which e.g., governmental agencies provide to catalyze innovation, OCD is shared in open collaboration between commercial organizations, similar to OSS. To achieve this, there is a need for technical infrastructure (e.g., tools for version and access control), licence models, and governance models, all of which have to be tailored for data. However, as data may be sensitive for privacy, anonymization and obfuscation of data is also a\u00a0\u2026", "num_citations": "14\n", "authors": ["142"]}
{"title": "Get the cogs in synch: time horizon aspects of industry--academia collaboration\n", "abstract": " In industry--academia collaboration projects, there are many issues related to different time horizons in industry and academia. If not adressed upfront, they may hinder collaboration in such projects. We analyze our experiences from a 10 year industry--academia collaboration program, the EASE Industrial Excellence Center in Sweden, and identify issues and feasible practices to overcome the hurdles of different time horizons. Specifically, we identify issues related to contracts, goals, results, organization (in) stability, and work practices. We identify several areas where the time horizon is different, and conclude that mutual awareness of these differences and management commitment to the collaboration are the key means to overcome the differences. The launch of a mediating institute may also be part of the solution.", "num_citations": "14\n", "authors": ["142"]}
{"title": "Extreme programming and rational unified process\u2013contrasts or synonyms?\n", "abstract": " The agile movement has received much attention in software engineering recently. Established methodologies try to surf on the wave and present their methodologies as being agile, among those Rational Unified Process (RUP). In order to evaluate the statements we evaluate the RUP against eXtreme Programming (XP) to find out to what extent they are similar and where they are different. We use a qualitative approach, utilizing a framework for comparison. We conclude from the analysis that the business concepts of the two\u2013commercial for RUP and freeware for XP\u2013is a main source of the differences. RUP is a top-down solution and XP is a bottom-up approach. Which of the two is really best in different situations has to be investigated in new empirical studies.", "num_citations": "14\n", "authors": ["142"]}
{"title": "The design science paradigm as a frame for empirical software engineering\n", "abstract": " Software engineering research aims to help improve real-world practice. With the adoption of empirical software engineering research methods, the understanding of real-world needs and validation of solution proposals have evolved. However, the philosophical perspective on what constitutes theoretical knowledge and research contributions in software engineering is less discussed in the community. In this chapter, we use the design science paradigm as a frame for articulating and communicating prescriptive software engineering research contributions. Design science embraces problem conceptualization, solution (or artifact) design, and validation of solution proposals, with recommendations for practice phrased as technological rules. Design science is used in related research areas, particularly information systems and management theory. We elaborate the constructs of design science for software\u00a0\u2026", "num_citations": "13\n", "authors": ["142"]}
{"title": "A quantitative analysis of the unit verification perspective on fault distributions in complex software systems: an operational replication\n", "abstract": " Unit verification, including software inspections and unit tests, is usually the first code verification phase in the software development process. However, principles of unit verification are weakly explored, mostly due to the lack of data, since unit verification data are rarely systematically collected and only a few studies have been published with such data from industry. Therefore, we explore the theory of fault distributions, originating in the quantitative analysis by Fenton and Ohlsson, in the weakly explored context of unit verification in large-scale software development. We conduct a quantitative case study on a sequence of four development projects on consecutive releases of the same complex software product line system for telecommunication exchanges. We replicate the operationalization from earlier studies, analyzed hypotheses related to the Pareto principle of fault distribution, persistence of faults\u00a0\u2026", "num_citations": "13\n", "authors": ["142"]}
{"title": "Decision support for test management and scope selection in a software product line context\n", "abstract": " In large software organizations with a product line development approach, system test planning and scope selection is a complex tasks for which tool support is needed. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of double testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. This paper discusses the need and challenges of providing decision support for test planning and test selection in a product line context, and highlights possible paths towards a pragmatic implementation of context-specific decision support of various levels of automation. With existing regression testing approaches it is possible to provide automated decision support in a few specific cases, while test management in general may be supported through visualization of test execution coverage, the testing space\u00a0\u2026", "num_citations": "13\n", "authors": ["142"]}
{"title": "A new software engineering programme-structure and initial experiences\n", "abstract": " Software plays an increasingly important role in new products of different kinds. Therefore, the need for engineers developing software is continuously increasing. However computer science education programmes are not enough to fulfil the industrial needs. Software engineering programmes are required with a holistic approach to the software life-cycle and its economics, as well as education in monitoring and managing the software process. At Lund University, Sweden, a new Bachelors software engineering programme was launched 1998. In this paper, the education programme's principles and structures are presented, as well as experiences from the first year. Software processes and methods play an important part in the education programme. It is concluded that the basic principles do function as expected, although the programme must be changed in the direction of requiring more programming skills\u00a0\u2026", "num_citations": "13\n", "authors": ["142"]}
{"title": "Sensitivity of software system reliability to usage profile changes\n", "abstract": " Usage profiles are an important factor in software system reliability estimation. To assess the sensitivity of a system's reliability to changes in the usage profile, a Markov based system model is used. With the help of this model, the statistical sensitivity to many independent changes can be estimated. The theory supports both absolute and relative changes and can be used for systems with or without a terminal state. With this approach it is possible to very quickly estimate the uncertainty on the predicted reliability calculated from a Markov model based upon the uncertainty on the usage profile. Finally the theory is applied to an example to illustrate its use and to show its validity.", "num_citations": "12\n", "authors": ["142"]}
{"title": "Prospects and Limitations for Cross-Study Analyses\u2013A Study on an Experiment Series\n", "abstract": " In software engineering research, experiments are conducted to evaluate new methods or techniques. The experimentation as such is beginning to mature, but little effort is spent on learning across different studies, except for a few meta-analyses. Meta-analysis can be applied to a set of experiments with the same design. This paper discusses learning across a set of experimental studies on fault detection techniques, conducted in very similar environments, although with different hypotheses. Four experiments have been conducted applying Usage-Based Reading (UBR), hence establishing a point of reference for other techniques. In the different experiments, UBR is compared to Checklist-Based Reading (CBR), two variants of UBR and Usage-Based Testing (UBT). We present an approach to analysis across different experimental studies, and identify a set of issues for discussion on whether the approach is feasible for further use in empirical software engineering.", "num_citations": "12\n", "authors": ["142"]}
{"title": "Application of factorial design to validation of system performance\n", "abstract": " System verification and validation are performed to secure that a system fulfils the different quality aspects, such as reliability, functionality, maintainability, performance and user friendliness. Since the verification and validation activities take a large share of the total project budget, efficiency and effectiveness are key issues. In this paper a method is presented for application of the well-known technique for experimental planning, factorial design, to validation of system performance. The proposed method is applied in this case study to the validation of the effectiveness of a radar system. It is concluded that the factorial design is a considerable support for the validation planning, enabling efficient system performance validation. The cost reduction was in this case 40%, compared with the previously used method. At the same time more information of the effects of factors was gained.", "num_citations": "12\n", "authors": ["142"]}
{"title": "Architecture design recovery of a family of embedded software systems\n", "abstract": " Understandability of the current system is a key issue in most reengineering processes. An architecture description of the system may increase its understandability. This paper presents experiences from architectural design recovery in a product family of large distributed, embedded systems. Automated recovery tools were hard to apply due to the nature of the source code. A qualitative evaluation procedure was applied on the performance of the recovery process. The results suggest that producing the necessary architectural documentation during the recovery project costs eight to twelve times as much as producing the same set of documentation during the original development project. By applying a common architectural style for all members of the product family, the component reuse made possible decreased source code volume by 65%.", "num_citations": "12\n", "authors": ["142"]}
{"title": "DigiTrust: Tillit i det digitala. Tv\u00e4rvetenskapliga perspektiv fr\u00e5n ett forskningsprojekt\n", "abstract": " Fr\u00e5gor kring tillit i det digitala, tycks det, st\u00e4lls oftare och oftare: Hur ska vi hantera integritets-och \u00f6vervakningsfr\u00e5gor? Hur ska vi hantera den m\u00e4tbarhet som f\u00f6ljer av v\u00e5r digitaliserade tillvaro? Vad \u00e4r det som avg\u00f6r vilka tj\u00e4nster vi litar p\u00e5 och vilka vi inte litar p\u00e5, i vilken m\u00e5n spelar den tekniska s\u00e4kerheten en roll? Liknande fr\u00e5gor kan st\u00e4llas om vilka kunskapsinstitutioner vi litar p\u00e5, eller borde lita p\u00e5\u2013hur \u00e4r det med forskningens egen granskning, som ocks\u00e5 f\u00f6r\u00e4ndras och utmanas i en digital tid? Paradoxalt nog tycks bilden och den fysiska gestaltningen bli allt viktigare i den digitala v\u00e4rlden. D\u00e4rf\u00f6r samlar vi resultat fr\u00e5n forskningsprojektet DigiTrust i en bok.", "num_citations": "11\n", "authors": ["142"]}
{"title": "Regression testing in software product line engineering\n", "abstract": " Software product line engineering is an approach to cost-efficiently derive tailored products to markets and customers, utilizing common components and services in a planned manner. Product lines have been applied to other engineering fields for decades, while being quite recently introduced in software engineering. For software product lines, productivity gains are mostly related to the development process. Especially, software product line testing faces challenges in the vast number of versions and variants of software products to be tested, originating from a software product line, and consequently the risk for redundant testing. The testing challenges resemble those of regression testing in one-off software development, although adding the complexity of parallel variants. Ongoing research provide some support for software product line test selection, although they are too small-scale and require more formalism\u00a0\u2026", "num_citations": "11\n", "authors": ["142"]}
{"title": "Test Benchmarks--what is the question?\n", "abstract": " In the automotive press, there are lots of benchmarks. Acceleration from 0 to 100 km/h or 0 to 60 mph is a frequently used benchmark. But how often do you accelerate as fast as possible from 0 to 100 km/h? Similarly is the power and the torque of the engine benchmarked, but rarely it is noticed whether the power is delivered at revs which are useful in my daily driving or at top revs. And I rarely use more than some 25 kW to run my car, although I have access to hundreds. Furthermore, the EuroNCAP1 and NT SB2 do benchmarks on crash resistance and rate car models according to their resistance to the benchmark tests.", "num_citations": "11\n", "authors": ["142"]}
{"title": "Defect content estimation for two reviewers\n", "abstract": " Estimation of the defect content is important to enable quality control throughout the software development process. Capture-recapture methods and curve fitting methods have been suggested as tools to estimate the defect content after a review. The methods are highly reliant on the quality of the data. If the number of reviewers is fairly small, it becomes difficult or even impossible to get reliable estimates. This paper presents a comprehensive study of estimates based on two reviewers, using real data from reviews. Three experience-based defect content estimation methods are evaluated vs. methods that use data only from the current review. Some models are possible to distinguish from each other in terms of statistical significance. In order to gain an even better understanding, the best models are compared subjectively. It is concluded that the experience-based methods provide some good opportunities to\u00a0\u2026", "num_citations": "11\n", "authors": ["142"]}
{"title": "Supporting regression test scoping with visual analytics\n", "abstract": " Background: Test managers have to repeatedly select test cases for test activities during evolution of large software systems. Researchers have widely studied automated test scoping, but have not fully investigated decision support with human interaction. We previously proposed the introduction of visual analytics for this purpose. Aim: In this empirical study we investigate how to design such decision support. Method: We explored the use of visual analytics using heat maps of historical test data for test scoping support by letting test managers evaluate prototype visualizations in three focus groups with in total nine industrial test experts. Results: All test managers in the study found the visual analytics useful for supporting test planning. However, our results show that different tasks and contexts require different types of visualizations. Conclusion: Important properties for test planning support are: ability to overview\u00a0\u2026", "num_citations": "10\n", "authors": ["142"]}
{"title": "Empirical Strategies\n", "abstract": " There are two types of research paradigms that have different approaches to empirical studies. Exploratory research is concerned with studying objects in their natural setting and letting the findings emerge from the observations. This implies that a flexible research design [1] is needed to adapt to changes in the observed phenomenon. Flexible design research is also referred to as qualitative research, as it primarily is informed by qualitative data. Inductive research attempts to interpret a phenomenon based on explanations that people bring forward. It is concerned with discovering causes noticed by the subjects in the study, and understanding their view of the problem at hand. The subject is the person, which is taking part in an empirical study in order to evaluate an object.", "num_citations": "10\n", "authors": ["142"]}
{"title": "Usage of open source in commercial software product development\u2013findings from a focus group meeting\n", "abstract": " Open source components can be used as one type of software component in development of commercial software. In development using this type of component, potential open source components must first be identified, then specific components must be selected, and after that selected components should maybe be adapted before they are included in the developed product. A company using open source components must also decide how they should participate in open source project from which they use software. These steps have been investigated in a focus group meeting with representatives from industry. Findings, in the form of recommendations to engineers in the field are summarized for all the mentioned phases. The findings have been compared to published literature, and no major differences or conflicting facts have been found.", "num_citations": "10\n", "authors": ["142"]}
{"title": "Tutorial: Case studies in software engineering\n", "abstract": " This document presents a tutorial on case study research methodology in software engineering, held at the 10th International Conference on Product Focused Software Development and Process Improvement (Profes).", "num_citations": "10\n", "authors": ["142"]}
{"title": "Decision support for extreme programming introduction and practice selection\n", "abstract": " This paper presents an investigation concerning the introduction of Extreme Programming (XP) in software development organisations. More specifically the concept of using a decision support method known as the Analytical Hierarchy Process (AHP) is evaluated by a group of students and a group of developers and the outcome is compared to experiences from an XP case study. The results provide an indication that different practices are thought to be easier and more effective to implement in the two groups. A company considering implementing only a few practices can use this as help for deciding which practices to implement. Companies introducing all practices can use the results of this kind of method to see where more attention might be needed after or during the introduction of XP.", "num_citations": "10\n", "authors": ["142"]}
{"title": "Statistical usage testing using SDL\n", "abstract": " host publication SDL'95 with MSC in CASE: proceedings of the seventh SDL forum, Oslo, Norway, 26-29 September 1995-proceedings of the seventh SDL forum, Oslo, Norway, 26-29 September 1995", "num_citations": "10\n", "authors": ["142"]}
{"title": "Software testing in open innovation: An exploratory case study of the acceptance test harness for Jenkins\n", "abstract": " Open Innovation (OI) has gained significant attention since the term was introduced in 2003. However, little is known whether general software testing processes are well suited for OI. An exploratory case study on the Acceptance Test Harness (ATH) is conducted to investigate OI testing activities of Jenkins. As far as the research methodology is concerned, we extracted the change log data of ATH followed by five interviews with key contributors in the development of ATH. The findings of the study are threefold. First, it highlights the key stakeholders involved in the development of ATH. Second, the study compares the ATH testing activities with ISO/IEC/IEEE testing process and presents a tailored process for software testing in OI. Finally, the study underlines some key challenges that software intensive organizations face while working with the testing in OI.", "num_citations": "9\n", "authors": ["142"]}
{"title": "Evaluation of traceability recovery in context: a taxonomy for information retrieval tools\n", "abstract": " Background: Development of complex, software intensive systems generates large amounts of information. Several researchers have developed tools implementing information retrieval (IR) approaches to suggest traceability links among artifacts. Aim: We explore the consequences of the fact that a majority of the evaluations of such tools have been focused on benchmarking of mere tool output. Method: To illustrate this issue, we have adapted a framework of general IR evaluations to a context taxonomy specifically for IR-based traceability recovery. Furthermore, we evaluate a previously proposed experimental framework by conducting a study using two publicly available tools on two datasets originating from development of embedded software systems. Results: Our study shows that even though both datasets contain software artifacts from embedded development, the characteristics of the two datasets differ\u00a0\u2026", "num_citations": "9\n", "authors": ["142"]}
{"title": "A comparative analysis of three replicated experiments comparing inspection and unit testing\n", "abstract": " Code inspections and structural unit testing are two common defect detection methods in software development. This paper analyzes three replications of an experiment aimed at comparing which method finds more defects. The first two experiments use the same experimental design and materials, including the programs to be inspected or tested, while the third experiment replaces the programs with code for embedded systems. Participants are a mixture of graduate and undergraduate students. Our analysis shows that both the differences in the instrumentation and the between-experiment participants themselves were larger than the differences between inspection versus unit testing, especially in terms of the time to complete the tasks.", "num_citations": "9\n", "authors": ["142"]}
{"title": "System level mutation analysis applied to a state-based language\n", "abstract": " One of the great challenges within software testing is to know if a test suite covers a program sufficiently. Mutation analysis is presented as an approach to address that challenge. Faulty versions, or mutants, are created, and whether the test suite is able to discover the defects is investigated. The technique is mostly applied to the unit level testing of software programs. Mutation analysis is applied to integration and system level testing in addition to unit level testing. A state based specification and description language, SDL, is used. The specific structural information is used to employ mutation analysis at different abstraction levels.", "num_citations": "9\n", "authors": ["142"]}
{"title": "Cognitive load drivers in large scale software development\n", "abstract": " Software engineers handle a lot of information in their daily work. We explore how software engineers interact with information management systems/tools, and to what extent these systems expose users to increased cognitive load. We reviewed the literature of cognitive aspects, relevant for software engineering, and performed an exploratory case study on how software engineers perceive information systems. Data was collected through five semistructured interviews. We present empirical evidence of the presence of cognitive load drivers, as a consequence of tool use in large scale software engineering.", "num_citations": "8\n", "authors": ["142"]}
{"title": "Navigating information overload caused by automated testing-A clustering approach in multi-branch development\n", "abstract": " Background. Test automation is a widely used technique to increase the efficiency of software testing. However, executing more test cases increases the effort required to analyze test results. At Qlik, automated tests run nightly for up to 20 development branches, each containing thousands of test cases, resulting in information overload. Aim. We therefore develop a tool that supports the analysis of test results. Method. We create NIOCAT, a tool that clusters similar test case failures, to help the analyst identify underlying causes. To evaluate the tool, experiments on manually created subsets of failed test cases representing different use cases are conducted, and a focus group meeting is held with test analysts at Qlik. Results. The case study shows that NIOCAT creates accurate clusters, in line with analyses performed by human analysts. Further, the potential time-savings of our approach is confirmed by the\u00a0\u2026", "num_citations": "8\n", "authors": ["142"]}
{"title": "A model-based framework for flexible safety-critical software development: a design study\n", "abstract": " This paper presents the findings from a design study of a model-based framework for safety-critical software development, called SimPal. The objective of the study was to better understand the necessary properties of such a framework and to learn more about the challenges of realizing it. Our research approach can be labeled as design research, which means that we try to answer our research questions by developing an artifact, in our case SimPal, and analyzing our experiences from the design of the artifact. In the paper we present what we identify as the necessary quality characteristics, using the ISO25010 quality in use quality model, of a framework like SimPal. These characteristics are then used to evaluate the SimPal framework in combination with a simple design case where we design a soft safety controller. We show that our approach has potential considering safety-critical software development\u00a0\u2026", "num_citations": "8\n", "authors": ["142"]}
{"title": "An evaluation of functional size methods and a bespoke estimation method for real-time systems\n", "abstract": " Functional size, or function points, is a language-independent measure of software size. It is used as an estimator of development effort and software code size, in particular in the domain of information systems, while in real-time systems it is not so widely spread. In this paper, functional size measurements are empirically evaluated by applying them to a recently completed project. Two established methods, Mark II Function Points (MKII) and Full Function Points (FFP) are used. In addition, a bespoke method that focuses on the information supplied by the requirements specifications at the studied company is developed and evaluated. The method is designed to make effort estimates based on states and transitions in a state machine representation of a requirements specification. The results indicate fairly weak relationships between functional size and LOC for both of the established methods. The bespoke\u00a0\u2026", "num_citations": "8\n", "authors": ["142"]}
{"title": "A review of software engineering research from a design science perspective\n", "abstract": " Background: Communicating software engineering research to industry practitioners and to other researchers can be challenging due to its context dependent nature. Design science is recognized as a pragmatic research paradigm, addressing this and other characteristics of applied and prescriptive research. Applying the design science lens to software engineering research may improve the communication of research contributions. Aim: The aim of this study is to 1) evaluate how well the design science lens helps frame software engineering research contributions, and 2) identify and characterize different types of design science contributions in the software engineering literature. Method: In previous research we developed a visual abstract template, summarizing the core constructs of the design science paradigm. In this study, we use this template in a review of a selected set of 38 top software engineering publications to extract and analyze their design science contributions. Results: We identified five clusters of papers, classified based on their alignment to the design science paradigm. Conclusions: The design science lens helps to pinpoint the theoretical contribution of a research output, which in turn is the core for assessing the practical relevance and novelty of the prescribed rule as well as the rigor of applied empirical methods in support of the rule.", "num_citations": "7\n", "authors": ["142"]}
{"title": "Software Engineers' Information Seeking Behavior in Change Impact Analysis-An Interview Study\n", "abstract": " Software engineers working in large projects must navigate complex information landscapes. Change Impact Analysis (CIA) is a task that relies on engineers' successful information seeking in databases storing, e.g., source code, requirements, design descriptions, and test case specifications. Several previous approaches to support information seeking are task-specific, thus understanding engineers' seeking behavior in specific tasks is fundamental. We present an industrial case study on how engineers seek information in CIA, with a particular focus on traceability and development artifacts that are not source code. We show that engineers have different information seeking behavior, and that some do not consider traceability particularly useful when conducting CIA. Furthermore, we observe a tendency for engineers to prefer less rigid types of support rather than formal approaches, i.e., engineers value support\u00a0\u2026", "num_citations": "7\n", "authors": ["142"]}
{"title": "Experiment Process Illustration\n", "abstract": " The primary objective of the presentation of this experiment is to illustrate experimentation and the steps in the experiment process introduced in the previous chapters. The presentation of the experiment in this chapter is focused on the experiment process rather than following the proposed report structure in Chap. 11.", "num_citations": "6\n", "authors": ["142"]}
{"title": "Investigating test teams' defect detection in function test\n", "abstract": " In a case study, the defect detection for functional test teams is investigated. In the study it is shown that the test teams not only discover defects in the features under test that they are responsible for, but also defects in interacting components, belonging to other test teams' features. The paper presents the metrics collected and the results as such from the study, which gives insights into a complex development environment and highlights the need for coordination between test teams in function test.", "num_citations": "5\n", "authors": ["142"]}
{"title": "Are found defects an indicator of software correctness? an investigation in a controlled case study\n", "abstract": " In quality assurance programs, we want indicators of software quality, especially software correctness. The number of found defects during inspection and testing are often used as the basis for indicators of software correctness. However, there is a paradox in this approach, since the remaining defects is what impacts negatively on software correctness, not the found ones. In order to investigate the validity of using found defects or other product or process metrics as indicators of software correctness, a controlled case study is launched. 57 sets of 10 different programs from the PSP course are assessed using acceptance test suites for each program. In the analysis, the number of defects found during the acceptance test are compared to the number of defects found during development, code size, share of development time spent on testing etc. It is concluded from a correlation analysis that 1) fewer defects remain in\u00a0\u2026", "num_citations": "5\n", "authors": ["142"]}
{"title": "Evaluation of user assistance in graphical user interface software\n", "abstract": " When using different software applications, the user will end up in situations where some form of help is a necessity in order to continue-that is, User Assistance. There exist at least ten common User Assistance methods, such as: Contextual Help, Procedural Help, Tutorial etc. The Help features facilitates the comprehension and the use of a program and contributes of making it more user friendly.With this Master Thesis we have investigated which Help feature that is to prefer in different situations-through the eyes of our testers. Also, we face the problem from the developer\u2019s point of view, concerning the maintenance cost of each help feature: Are there help features associated with some sort of load that makes it less attractive compared to others?", "num_citations": "5\n", "authors": ["142"]}
{"title": "Document use in software development: a qualitative survey\n", "abstract": " Software engineering consists to a large extent of management of information. A qualitative survey on information management in software development companies has been conducted. The survey studies how the companies work with different kinds of documents, which notations are used, quality of the documents etc. The focus is on high-level documents, such as requirements specifications and system test specifications. 10 participants from 4 different companies all situated in Sweden participated in the survey. The result indicates that natural language is by far the most common notation used and that traditional documents are predominant. Further, high-level documents are generally considered to be of lower quality than other documents. However, the high-level documents are usually considered to be more useful, which is conflicting the reports of lower quality.", "num_citations": "5\n", "authors": ["142"]}
{"title": "Challenges and Opportunities in Open Data Collaboration\u2013a focus group study\n", "abstract": " Data-driven software is becoming prevalent, especially with the advent of machine learning and artificial intelligence. With data-driven systems come both challenges \u2013 to keep collecting and maintaining high quality data \u2013 and opportunities \u2013 open innovation by sharing data with others. We propose Open Data Collaboration (ODC) to describe pecuniary and non-pecuniary sharing of open data, similar to Open Source Software (OSS) and in contrast to Open Government Data (OGD), where public authorities share data. To understand challenges and opportunities with ODC, we organized five focus groups with in total 27 practitioners from 22 companies, public organizations, and research institutes. In the discussions, we observed a general interest in the subject, both from private companies and public authorities. We also noticed similarities in attitudes to open innovation practices, i.e. initial resistance which\u00a0\u2026", "num_citations": "4\n", "authors": ["142"]}
{"title": "Palcom mist: A metaprotocol for internet systems of things\n", "abstract": " Future applications of internet of things (IoT) will go far beyond connecting sensors to the internet. The real challenge lies in how to easily build well working systems of things: well integrated distributed applications that support users, yet built from a variety of devices running on a variety of networks and protocols, and including services that were not designed to interoperate. Based on our experiences from more than a decade of experimentation with heterogeneous, distributed systems, we identify a number of key requirements for building such systems of IoT: transparent communication and routing over heterogeneous networks, interoperability across standards, scalable evolvable architecture, and high-level development. We present a scenario from healthcare, instantiating these general requirements, and we explain how they are supported by our middleware PalCom which is an implementation of the MIST\u00a0\u2026", "num_citations": "4\n", "authors": ["142"]}
{"title": "Automated controlled experimentation on software by evolutionary bandit optimization\n", "abstract": " Controlled experiments, also called A/B tests or split tests, are used in software engineering to improve products by evaluating variants with user data. By parameterizing software systems, multivariate experiments can be performed automatically and in large scale, in this way, controlled experimentation is formulated as an optimization problem. Using genetic algorithms for automated experimentation requires repetitions to evaluate a variant, since the fitness function is noisy. We propose to combine genetic algorithms with bandit optimization to optimize where repetitions are evaluated, instead of uniform sampling. We setup a simulation environment that allows us to evaluate the solution, and see that it leads to increased fitness, population diversity, and rewards, compared to only genetic algorithms.", "num_citations": "4\n", "authors": ["142"]}
{"title": "It is more blessed to give than to receive\u2013open software tools enable open innovation\n", "abstract": " Open Innovation (OI) has attracted scholarly interest from a wide range of disciplines since introduced by Chesbrough, ie\u201d a paradigm that assumes that firms can and should use external ideas as well as internal ideas, and internal and external paths to market, as they look to advance their technology\u201d. However, OI remains unexplored for software engineering (SE), although widespread in practice through Open Source Software (OSS). We studied the relation between SE and OI and in particular how OSS tools impact on software-intensive organization\u2019s innovation capability. We surveyed the literature on SE and OI and found that studies conclude that start-ups have higher tendency to opt for OI compared to established companies. The literature also suggests that firms assimilating external knowledge into their internal R&D activities, have higher likelihood of gaining financial advantages. In a case study, we observed how OSS tools Jenkins and Gerrit enabled open innovation. We mined software commits to identify major contributors, found them be affiliated to Sony Mobile, contacted five of them for interviews about their and their employer\u2019s principles and practices with respect to OI and tools, which they gave a consistent view of. Our findings indicate that the company\u2019s transition to OI was part of a major paradigm shift towards OSS, while the adoption of open tools was driven bottom up by engineers with support from management. By adopting OI, Sony Mobile achieved freed-up developers\u2019 time, better quality assurance, inner source initiatives, flexible development environment, faster releases and upgrades. Particularly, the introduction of a\u00a0\u2026", "num_citations": "4\n", "authors": ["142"]}
{"title": "SimPal: a design study on a framework for flexible safety-critical software development\n", "abstract": " This paper presents the findings from a design study on a framework for flexible safety-critical software development, called SimPal. It is an extended version of a paper that was published in SAC'13 Proceedings of the 2013 ACM Symposium on Applied Computing, in which additional details about SimPal as well as a more extensive evaluation of the framework is presented. The objective is to identify necessary quality properties and to learn more about the challenges of realizing frameworks such as SimPal. We approach our research questions by developing a framework and by analysing our experiences from the design and evaluation process. Some necessary quality characteristics has been identified by discussing the ISO25010 quality in use quality model in relation to the problem domain, which were then used to design and evaluate the developed framework. The evaluation was conducted as a design\u00a0\u2026", "num_citations": "4\n", "authors": ["142"]}
{"title": "A case study on testware maintenance and change strategies in system evolution\n", "abstract": " Relationships between classes and objects in object-oriented software are necessary in order for the parts of the systems to provide dynamic behavior. These inherent relationships also create dependencies which can give rise to problems for software evolution of object-oriented software systems. Dependencies in software make systems difficult to understand, reuse, change and verify.This thesis presents analytical and empirical investigations of dependency-related problems in software evolution of object-oriented software and on how such problems can be handled with dependency focused techniques, methods and processes.", "num_citations": "4\n", "authors": ["142"]}
{"title": "Scaling extreme programming in a market driven development context\n", "abstract": " This paper briefly summarizes a research project aiming at analyzing the scaling up of Extreme Programming (XP) in a market driven context for embedded software and the integration of XP towards management processes in this context. Both the scaling of XP and market driven software development issues are not addressed by the original XP methodology as it relies on developing software by simplifying the development situation as far as possible in order to do the simplest thing that could possibly work. The research project is composed of a number of steps aimed at gathering information from actual development organizations, analyzing, synthesizing, and incorporating the conclusions into the models created.", "num_citations": "4\n", "authors": ["142"]}
{"title": "An experiment on lead-time impact in testing of distributed real-time systems\n", "abstract": " In the search for an efficient test method for distributed real-time systems, an experiment is conducted that investigates the impact on the lead time required to identify a defect, depending on what information is available. One group of experiment participants were given a system monitoring trace, while a control group were given node monitoring traces. The information made available in the two sets of traces was identical except for which monitoring approach was being used. The system under study is the control software for a private branch exchange (PBX) with three nodes. The experiment, conducted with 23 participants, suggests with statistically significant confidence that it takes a shorter time to isolate a defect using system monitoring traces than using node monitoring traces. Further, the identification of the defect source is more correct with system monitoring. These findings help in motivating investments in\u00a0\u2026", "num_citations": "4\n", "authors": ["142"]}
{"title": "Collaboration in Open Government Data Ecosystems: Open Cross-sector Sharing and Co-development of Data and Software\n", "abstract": " Background: Open innovation highlights the potential benefits of external collaboration and knowledge-sharing, often exemplified through Open Source Software (OSS). The public sector has thus far mainly focused on the sharing of Open Government Data (OGD), often with a supply-driven approach with limited feedback-loops. We hypothesize that public sector organizations can extend the open innovation benefits by also creating platforms, where OGD, related OSS, and open standards are collaboratively developed and shared. Objective: The objective of this study is to explore how public sector organizations in the role of platform providers facilitate such collaboration in the form of OGD ecosystems and how the ecosystem\u2019s governance may be structured to support the collaboration. Method: We conduct an exploratory multiple-case study of two such ecosystems, focused on OGD related to the Swedish labor\u00a0\u2026", "num_citations": "3\n", "authors": ["142"]}
{"title": "Plug-in software engineering case studies\n", "abstract": " Empirical software engineering is a growing research area. Industrial experience gathered by systematic empirical case studies is extremely important for further evolution of the software engineering discipline. Scientific theory cannot provide effective means for software industry without fundamental understanding of the evolutionary development of complex software systems. However, there are certain limitations in performing observational quantitative case studies in real software engineering environments, and to enable their replication. In this paper, we propose a framework that would allow plug-in case studies for industries, aiming to overcome obstacles of engagement and wide replications of industrial empirical studies.", "num_citations": "3\n", "authors": ["142"]}
{"title": "Automated platform testing using input generation and code coverage\n", "abstract": " When using a Java platform it is important to test all aspects of it thoroughly. In this thesis an attempt to test the stability and overall function is presented.The method is to test the platform by running a large number of Java applications on the platform and see if anything goes wrong. To stress the platform as much as possible it is important to not only start the application but to attempt to explore it as well. This makes sure that as much of the platform as possible is tested.", "num_citations": "3\n", "authors": ["142"]}
{"title": "A case study using sampling to improve software inspection effectiveness\n", "abstract": " Software inspections have shown to contribute improved software quality. However, as they are time consuming; sometimes the resources available are not sufficient to inspect all documents. Instead of selecting documents ad hoc, sample-driven inspections (SDI) is proposed as a systematic approach to select which subset of documents to inspect. The selection is based on a pre-inspection, where samples of documents are inspected, and based on the sample, it is estimated which documents need the quality improvement the most. Two important questions are how select the pre-inspection sample, and how large segments the documents should be divided into. In this paper, we apply the SDI approach to industrial data from inspections of requirements specifications. Different sampling strategies are applied and the documents are split into segments of different size. It is concluded that the SDI approach is more\u00a0\u2026", "num_citations": "3\n", "authors": ["142"]}
{"title": "Software reliability estimations through usage analysis of specifications and designs\n", "abstract": " This paper presents a method proposal for estimation of software reliability before the implementation phase. The method is based upon that a formal specification technique is used and that it is possible to develop a tool performing dynamic analysis, i.e., locating semantic faults in the design. The analysis is performed with both applying a usage profile as input as well as doing a full analysis, i.e., locate all faults that the tool can find. The tool must provide failure data in terms of time since the last failure was detected. The mapping of the dynamic failures to the failures encountered during statistical usage testing and operation is discussed. The method can be applied either on the software specification or as a step in the development process by applying it on the software design. The proposed method allows for software reliability estimations that can be used both as a quality indicator, and for planning and\u00a0\u2026", "num_citations": "3\n", "authors": ["142"]}
{"title": "Governance and Management of Green IT: A Multi-Case Study\n", "abstract": " ContextThe changes that are taking place with respect to environmental sensitivity are forcing organizations to adopt a new approach to this problem. Implementing sustainability initiatives has become a priority for the social and environmental awareness of organizations that want to stay ahead of the curve. One of the business areas that has, more than others, proven to be a vital asset and a potential ally of the environment, is the area of Information Technology (IT). Through this area, Green IT practices advocate sustainability in and by IT. However, organizations have a significant handicap in this regard, due to the lack of specific Green IT standards and frameworks that help them carry out this type of sustainability practices.ObjectiveWe have developed the \u201cGovernance and Management Framework for Green IT\u201d (GMGIT), which establishes the necessary characteristics to implement Green IT in organizations\u00a0\u2026", "num_citations": "2\n", "authors": ["142"]}
{"title": "Getting Started with Chaos Engineering-design of an implementation framework in practice\n", "abstract": " Background. Chaos Engineering is proposed as a practice to verify a system's resilience under real, operational conditions. It employs fault injection, is originally developed at Netflix, and supported by several tools from there and other sources. Aims. We aim to introduce Chaos Engineering at ICA Gruppen AB, a group of companies whose core business is grocery retail, to improve their systems' resilience, and to capture our knowledge gained from literature and interviews in a process framework for the introduction of Chaos Engineering. Method. The research is conducted under the design science paradigm, where the problem is conceptualized through a literature study of Chaos Engineering and exploratory interviews in the company. The solution framework is designed based on the literature and a tool survey, and validated by letting software engineers at ICA apply parts of it to the software systems of ica. se\u00a0\u2026", "num_citations": "2\n", "authors": ["142"]}
{"title": "Public Sector Platforms going Open: Creating and Growing an Ecosystem with Open Collaborative Development\n", "abstract": " Background: By creating ecosystems around platforms of Open Source Software (OSS) and Open Data (OD), and adopting open collaborative development practices, platform providers may exploit open innovation benefits. However, adopting such practices in a traditionally closed organization is a maturity process that we hypothesize cannot be undergone without friction.Objective: This study aims to investigate what challenges may occur for a newly-turned platform provider in the public sector, aiming to adopt open collaborative practices to create an ecosystem around the development of the underpinning platform.Method: An exploratory case-study is conducted at a Swedish public sector platform provider, which is creating an ecosystem around OSS and OD, related to the labor market. Data is collected through interviews, document studies, and prolonged engagement.Results: Findings highlight a fear among\u00a0\u2026", "num_citations": "2\n", "authors": ["142"]}
{"title": "Navigating information overload caused by automated testing\n", "abstract": " The growing demand of high quality software creates an increasing pressure on software development organizations to increase test coverage in order to meet the quality requirements. At the software company Qlik, automated testing helps improve test coverage and allows development of multiple features in parallel while maintaining a stable code base. In order to benefit from the automated testing, the results must be processed and analysed. This is currently done by an analyst reading log files, interpreting error messages and looking at screenshot images. Automated tests run every night for up to twenty development branches, each containing thousands of test cases-resulting in information overload. It is extremely difficult and time consuming for a human to process the test results and get an overview of the state of the development. Qlik is in desperate need of an automated analysis approach. In this thesis we create NIOCAT, a tool that automatically analyses test results. The output is an overview of all failed test cases, where similar failures have been grouped together. To evaluate NIOCAT, experiments on manually created subsets representing different use cases are conducted. To further enhance the evaluation methodology a focus group meeting is held with test result analyst experts at Qlik. The experiments conducted in this case study show that NIOCAT can create accurate clusters, in line with analyses performed by humans. Further, the need and potential time-savings of our approach is confirmed by the participants in the focus group. NIOCAT thus provides a feasible complement to current automated testing practices at Qlik\u00a0\u2026", "num_citations": "2\n", "authors": ["142"]}
{"title": "Det Digitala Samh\u00e4llet\n", "abstract": " Under h\u00f6sten 2012 och vintern 2013 ansvarade Stefan Larsson och Per Runeson f\u00f6r en tv\u00e4rvetenskaplig studiegrupp om Det digitala samh\u00e4llet p\u00e5 Pufendorfinstitutet vid Lunds universitet. Vid sex seminarietillf\u00e4llen under h\u00f6sten 2012 och b\u00f6rjan av 2013 tr\u00e4ffades 17 forskare fr\u00e5n sex fakulteter, en eftermiddag, var tredje vecka f\u00f6r att f\u00e5 olika perspektiv att m\u00f6tas och konfronteras kring aspekter p\u00e5 det digitala samh\u00e4llet som sp\u00e4nner fr\u00e5n konst och musik, via litteratur och bibliotek, lagar och normer, arbetsmilj\u00f6 och yrkesroller, till algoritmer, mjukvara och kryptering. Den blogg som f\u00f6rdes under studiegruppens tr\u00e4ffar har nu omvandlats till denna bok, skriven av Stefan Larsson och Per Runeson.", "num_citations": "2\n", "authors": ["142"]}
{"title": "Operation\n", "abstract": " When an experiment has been designed and planned it must be carried out in order to collect the data that should be analyzed. This is what we mean with the operation of an experiment. In the operational phase of an experiment, the treatments are applied to the subjects. This means that this part of the experiment is the part where the experimenter actually meets the subjects. In most experiments in software engineering there are only a few other times when the subjects actually are involved. These occasions can, for example, be in a briefing before subjects commit to participate in the experiment and after the experiment when the results of the experiment are presented to the subjects. Since experiments in software engineering in most cases deal with humans, although it is possible to run technology-oriented experiments to as discussed in Sect. 2.4. This chapter deals to some extent with how to motivate\u00a0\u2026", "num_citations": "2\n", "authors": ["142"]}
{"title": "Simulation of Experiments for Data Collection\u2013a replicated study\n", "abstract": " Simulations can be used as a means for extension of data collection from empirical studies. A simulation model is developed, based on the data from experiments, and new data is generated from the simulation model. This paper replicates an initial investigation by M\u00fcnch and Armbrust, with the purpose of evaluating the generality of their approach. We replicate their study using data from two inspection experiments. We conclude that the replicated study corroborates the original one. The deviation between the detection rate of the underlying experiment and the simulation models was 2% for the original study and is 4% in the replicated study. Both figures are acceptable for using the approach further. Still the model is based on some adjustment variables that are not directly possible to interpret in terms of the original experiment, and hence the model is subject to improvement.", "num_citations": "2\n", "authors": ["142"]}
{"title": "Baselining Software Processes as a Starting Point for Research and Improvement\n", "abstract": " Establishing a baseline of the current situation is an important starting point for software process improvement as well as for empirical research in software engineering. In practice, however, practical problems hinder the baselining process. This paper presents a case study to illustrate such problems. The case study aims at investigating the current status of the verification and validation process of a larger software development company. It is concluded that the quantitative data available is not sufficient for quantitative research, but qualitative issues are better covered through interviews. Finally, proposals are raised which could improve the situation on a longer term.", "num_citations": "2\n", "authors": ["142"]}
{"title": "How to Enable Collaboration in Open Government Data Ecosystems: A Public Platform Provider\u2019s Perspective\n", "abstract": " Open Government Data (OGD) is an important driver for open innovation among public entities. However, extant research highlights a need for improved feedback loops, collaboration, and a more demand-driven publication of OGD. In this study, we explore how public platform providers can address this issue by enabling collaboration within OGD ecosystems, both in terms of the OGD, and any related Open Source Software (OSS) and standards. We conducted an exploratory multiple-case study of four OGD ecosystems with diverse characteristics, using a qualitative research approach. Based on the cases, we present a conceptual model that highlights different attributes of OGD ecosystems that may help public entities in designing and orchestrating new or existing OGD ecosystems. We conclude that enabling collaboration in an OGD ecosystem is a complex exercise yet believe that it offers ways for public entities in how they can leverage open innovation to address their goals and directives.", "num_citations": "1\n", "authors": ["142"]}
{"title": "A Grounded Theory of Cognitive Load Drivers in Novice Agile Software Development Teams\n", "abstract": " Objective: The purpose of this paper is to identify the largest cognitive challenges faced by novices developing software in teams. Method: Using grounded theory, we conducted an ethnographic study for two months following four ten person novice teams, consisting of computer science students, developing software systems. Result: This paper identifies version control and merge operations as the largest challenge faced by the novices. The literature studies reveal that little research appears to have been carried out in the area of version control from a user perspective. Limitations: A qualitative study on students is not applicable in all contexts, but the result is credible and grounded in data and substantiated by extant literature. Conclusion: We conclude that our findings motivate further research on cognitive perspectives to guide improvement of software engineering and its tools.", "num_citations": "1\n", "authors": ["142"]}
{"title": "How Companies Use OSS Tools Ecosystems for Open Innovation\n", "abstract": " Moving toward the open innovation (OI) model requires multifaceted transformations within companies. It often involves giving away the tools for product development or sharing future product directions with open tools ecosystems. Moving from the traditional closed innovation model toward an OI model for software development tools shows the potential to increase software development competence and efficiency of organizations. We report a case study in software-intensive company developing embedded devices (e.g., smartphones) followed by a survey in OSS communities such as Gerrit, Git, and Jenkins. The studied branch focuses on developing Android phones. This paper presents contribution strategies and triggers for openness. These strategies include avoid forking OSS tools, empower developers to participate in the ecosystem, steer ecosystems through contributions, create business through\u00a0\u2026", "num_citations": "1\n", "authors": ["142"]}
{"title": "Open data collaborations: a snapshot of an emerging practice\n", "abstract": " Data defined software is becoming more and more prevalent, especially with the advent of machine learning and artificial intelligence. With data defined systems come both challenges-to continue to collect and maintain quality data-and opportunities-open innovation by sharing with others. We propose Open Data Collaboration (ODC) to describe pecuniary and non-pecuniary sharing of open data, similar to Open Source Software. To understand challenges and opportunities with ODC, we ran focus groups with 22 companies and organizations. We observed an interest in the subject, but we conclude that the overall maturity is low and ODC is rare.", "num_citations": "1\n", "authors": ["142"]}
{"title": "Open collaborative data: a pre-study on an emerging practice\n", "abstract": " Data intense defined software is becoming more and more prevalent, especially with the advent of machine learning and artificial intelligence. With data intense systems comes both challenges\u2013to continue to collect and maintain quality\u2013and opportunities\u2013open innovation by sharing with others.", "num_citations": "1\n", "authors": ["142"]}
{"title": "Open tools for software engineering: Validation of a theory of openness in the automotive industry\n", "abstract": " Context: Open tools (eg, Jenkins, Gerrit and Git) offer a lucrative alternative to commercial tools. Many companies and developers from OSS communities make a collaborative effort to improve the tools. Prior to this study, we developed an empirically based theory for companies' strategic choices on the development of these tools, based on empirical observations in the telecom domain. Aim: The aim of this study is to validate the theory of openness for tools in software engineering, in another domain, automotive. Specifically, we validated the theory propositions and mapped the case companies onto the model of openness. Method: We run focus groups in two automotive companies, collecting data in a survey and follow-up discussions. We used the repertory grid technique to analyze the survey responses, in combination with qualitative data from the focus group, to validate the propositions. Results: Openness of\u00a0\u2026", "num_citations": "1\n", "authors": ["142"]}
{"title": "Unit verification effects on reused components in sequential project releases\n", "abstract": " Background. The effects of different practices on fault distributions in evolving complex software systems is not fully understood. Software reuse and unit verification are practices used to improve system reliability by minimising the number of late faults. Reused software benefits from already being verified while unit verification aims to find faults early. Aims. We want to study effects of software reuse and unit verification on future modifications, fault densities of software units, and fault distributions. Method. We applied statistical analysis to a sample of 520 units that were reused and modified within four sequential projects from one product line in the telecommunication domain. Results. In reused units, the results of unit verification are correlated to a smaller degree of modifications and decreased fault densities. Conclusion. Unit verification in complex systems may improve system evolution in terms of smaller\u00a0\u2026", "num_citations": "1\n", "authors": ["142"]}
{"title": "Mere numbers aren't enough: A plea for visualization\n", "abstract": " Quantitative data comes with enormous possibilities for presenting key characteristics of the data in a very compressed form. Basic descriptive statistics, like mean and standard deviation, comprise thousands or millions of data points into single numbers. In contrast, qualitative data, with its focus on descriptions, words, and phrases does not come with such tools, leading to wordy descriptions of the analysis. However, mean and standard deviation do not bring the full understanding of the underlying. Thus, we need visualization, or visual analytics, which combine the exactness and compactness of quantitative data with the richness and communication of qualitative communication.", "num_citations": "1\n", "authors": ["142"]}
{"title": "Quantitative analysis of unit verification as predictor in large scale software enginering\n", "abstract": " Unit verification, including software inspections and unit tests, is usually the first code verification phase in the software development process. However, principles of unit verification are weakly explored, mostly due to the lack of data, since unit verification data are rarely systematically collected and only a few studies have been published with such data from industry. Therefore, we explore the theory of fault distributions, originating in the quantitative analysis by Fenton and Ohlsson, in the weakly explored context of unit verification in large-scale software development. We conduct a quantitative case study on a sequence of four development projects on consecutive releases of the same complex software product line system for telecommunication exchanges. We replicate the operationalization from earlier studies, analyzed hypotheses related to the Pareto principle of fault distribution, persistence of faults, effects of module size, and quality in terms of fault densities, however, now from the perspective of unit verification. The patterns in unit verification results resemble those of later verification phases, e.g., regarding the Pareto principle, and may thus be used for prediction and planning purposes. Using unit verification results as predictors may improve the quality and efficiency of software verification.", "num_citations": "1\n", "authors": ["142"]}
{"title": "\u00d6vervakning t\u00e4r p\u00e5 medborgarnas tillit\n", "abstract": " Artikeln \u00e4r en debattartikel p\u00e5 Svenska Dagbladets Br\u00e4nnpunkt. Medborgarnas acceptans f\u00f6r s\u00e4kerhetsrelaterad \u00f6vervakning \u00e4r begr\u00e4nsad. Exempelvis tycker 55 procent av svenskarna inte att det \u00e4r acceptabelt att FRA samlar in och bearbetar data om internetvanor, skriver forskare fr\u00e5n DigiTrust-projektet vid Lunds universitets Pufendorfinstitut.", "num_citations": "1\n", "authors": ["142"]}
{"title": "Sampling in software development\n", "abstract": " In software development, sampling inspection is applied during the development process, as the production process for software is trivial. In software inspection, which is a manual scrutiny of software code or specifications, sampling is applied to estimate the number of remaining faults and to guide which parts of the artifact to inspect. Capture\u2013recapture techniques are used in the fault content estimation. In software testing, which is the dynamic execution of code, sampling is implicitly or explicitly applied to select test cases. Since the number of states is extremely high in software products, a complete coverage of all cases is practically impossible. In addition to scoping the tests to a reasonable magnitude of size, sampling is used to estimate the number of remaining faults and to predict the operational failure behavior of the software, e.g. through software reliability growth models. The theory is quite well established\u00a0\u2026", "num_citations": "1\n", "authors": ["142"]}
{"title": "Analysis and Interpretation\n", "abstract": " The experiment data from the operation is input to the analysis and interpretation. After collecting experimental data in the operation phase, we want to be able to draw conclusions based on this data. To be able to draw valid conclusions, we must interpret the experiment data.", "num_citations": "1\n", "authors": ["142"]}
{"title": "A Factorial Experimental Evaluation of Automated Test Input Generation\n", "abstract": " Background. When delivering an embedded product, such as a mobile phone, third party products, like games, are often bundled with it in the form of Java MIDlets. To verify the compatibility between the runtime platform and the MIDlet is a labour-intensive task, if input data should be manually generated for thousands of MIDlets. Aim. In order to make the verification more efficient, we investigate four different automated input generation methods which do not require extensive modeling; random, feedback based, with and without a constant startup sequence. Method. We evaluate the methods in a factorial design experiment with manual input generation as a reference. One original experiment is run, and a partial replication. Result. The results show that the startup sequence gives good code coverage values for the selected MIDlets. The feedback method gives somewhat better code coverage\u00a0\u2026", "num_citations": "1\n", "authors": ["142"]}
{"title": "Software Testing\n", "abstract": " Software Testing Page 1 Lund University / Faculty of Engineering/ Department of Computer Science / Software Engineering Research Group Software Testing ETSN20 http://cs.lth.se/etsn20 Chapter 4, 5, 3.5 Prof. Per Runeson Page 2 Lund University / Faculty of Engineering/ Department of Computer Science / Software Engineering Research Group Lecture 1 wish list \u2022 exploratory testing \u2022 automated tests (cypress, selenium) \u2022 testing of large systems \u2022 ui testing \u2022 startup testing requirements \u2022 debugging \u2022 agile perspective \u2022 how good is your test \u2022 continuous integration development Page 3 Lund University / Faculty of Engineering/ Department of Computer Science / Software Engineering Research Group Lecture \u2022 White-box testing techniques (Lab 1) \u2013 Control flow (Chapter 4) \u2013 Data flow (Chapter 5) \u2022 Mutation testing (Section 3.5) Page 4 Lund University / Faculty of Engineering/ Department of Computer Science / ? \u2022 (\u2019\u2026", "num_citations": "1\n", "authors": ["142"]}
{"title": "Introduction to Case Study Research\n", "abstract": " \u2022 D. Karlstr\u00f6m and P. Runeson. Integrating agile software development into stage-gate managed product development. Empirical Software Engineering, 11 (2): 203\u2013225, 2006.\u2022 AW Rainer. The longitudinal, chronological case study research strategy: A definition and an example from IBM Hursley Park. Info. and Software Technology, 53 (7): 730-746, 2011.\u2022 C. Andersson and P. Runeson. A spiral process model for case studies on software quality monitoring\u2014method and metrics. Software Process: Improvement and Practice, 12 (2): 125\u2013140, 2007.", "num_citations": "1\n", "authors": ["142"]}
{"title": "Evaluating the Impact of Software Process Simulations\u2013A Case Study\n", "abstract": " The software process simulation field has been growing during the last decade and has become increasingly popular in the software engineering community. The field has focused on the scope of the simulation models and the development approaches of these, resulting in a growing body of knowledge in simulation methods. Experience with the simulation technologies has given rise to enhancements and extensions of existing systems. Lessons have been learned on the use of simulation and modeling tools and techniques. While attention has been given to that the simulation and modeling increase the understanding of the development process, discussed for example by Kellner et al.[Kellner99], little effort have been given to evaluate whether the impact of the model really is measurable in terms of increased understanding of process behaviour and to what extent simulation can be used for increased understanding. To our knowledge, only a few published studies take this viewpoint. Experiences on using software process simulation as a tool for project management education and training have been described in [Drappa00],[Madachy00] and [Pfahl04].In order to investigate the impact of the use of process simulation models, a qualitative study was conducted during the development of a simulation model of an industrial software development process to evaluate the impact the model had on project participants, see Figure 1. Using questionnaires, the staff\u2019s opinions before the model development were compared to the opinions after the introduction of the model.", "num_citations": "1\n", "authors": ["142"]}
{"title": "Addressing Attitudes Explicitly in Engineering Education-An Exercise to Stimulate Reflection through Pictures\n", "abstract": " In the process of establishing goals for education programmes and courses, we often work with goals regarding knowledge, skills and attitudes [1]. When it comes to planning the content of the education programme or the course, the goal of knowledge and skills are much more tangible than the attitude goals. Hence, attitudes tend to be addressed implicitly rather than explicitly, if addressed at all. In software engineering education, much of the learning is about attitudes. The student\u2019s attitudes before entering the programme tend to be that software engineering is mostly about programming and technical issues, while research shows that organizational and managerial issues are far more important contributors to success or failure in software engineering. Furthermore, software development is often considered art or craft, while we want to change it into and engineering discipline, which means fostering an attitude of teamwork, structured approaches and learning from own and others\u2019s experiences. In our software engineering education, we want to address and affect the student\u2019s attitudes regarding these issues.In this paper, we present an exercise in an introductory software engineering course, which explicitly addresses attitudes towards the course topic and the students\u2019 future work roles, using pictures to stimulate discussion and analysis. We present the exercise and analyze briefly the outcome of the two occasions when it was given.", "num_citations": "1\n", "authors": ["142"]}
{"title": "Models for estimation of Software Faults and Failures in Inspection and Test\n", "abstract": " In software engineering, the quality of software and in particular the software reliability has to be controlled and improved. Hence there is a need for measuring and controlling the number and appearance of software faults and failures. This thesis presents models for estimation of residual faults after inspections and a model for usage specifications which enables estimation of operational failure behaviour and software reliability already in test. The fault estimation models are based on the capture-recapture technique. A filter approach and an experience-based approach are developed and evaluated in two experiments. The experimental results show that the new estimation models give less absolute error and more stable estimates than established models. However, replication of the experiments is needed to get sufficient confidence in the results. The failure estimation, or software reliability estimation models require test cases to be representative of the future usage of the system in order to make estimates in test. The future usage is specified in a usage specification. The state hierarchy usage specification model is developed to be suitable for modelling of the usage of real-time multi-user systems. The model is applied in early phases in the life cycle to dynamic analysis, as well as to test of components. Furthermore the implementation of the usage specification in an executable language for automatic generation of test cases is studied. The model is evaluated in minor case studies in different application domains, which have provided promising results. The models contribute to estimation of software faults and failures, hence providing means\u00a0\u2026", "num_citations": "1\n", "authors": ["142"]}