{"title": "Off-line variable substitution for scaling points-to analysis\n", "abstract": " Most compiler optimizations and software productivity tools rely on information about the effects of pointer dereferences in a program. The purpose of points-to analysis is to compute this information safely, and as accurately as is practical. Unfortunately, accurate points-to information is difficult to obtain for large programs, because the time and space requirements of the analysis become prohibitive. We consider the problem of scaling flow- and context-insensitive points-to analysis to large programs, perhaps containing hundreds of thousands of lines of code. Our approach is based on a variable substitution transformation, which is performed off-line, i.e., before a standard points-to analysis is performed. The general idea of variable substitution is that a   set  of variables in a program can be replaced by a single representative variable, thereby reducing the input size of the problem. Our main contribution is a linear\u00a0\u2026", "num_citations": "142\n", "authors": ["295"]}
{"title": "Packet types: abstract specification of network protocol messages\n", "abstract": " In writing networking code, one is often faced with the task of interpreting a raw buffer according to a standardized packet format. This is needed, for example, when monitoring network traffic for specific kinds of packets, or when unmarshaling an incoming packet for protocol processing. In such cases, a programmer typically writes C code that understands the grammar of a packet and that also performs any necessary byte-order and alignment adjustments. Because of the complexity of certain protocol formats, and because of the low-level of programming involved, writing such code is usually a cumbersome and error-prone process. Furthermore, code written in this style loses the domain-specific information, viz. the packet format, in its details, making it difficult to maintain.", "num_citations": "134\n", "authors": ["295"]}
{"title": "Teapot: Language support for writing memory coherence protocols\n", "abstract": " Recent shared-memory parallel computer systems offer the exciting possibility of customizing memory coherence protocols to fit an application's semantics and sharing patterns. Custom protocols have been used to achieve message-passing performance---while retaining the convenient programming model of a global address space---and to implement high-level language constructs. Unfortunately, coherence protocols written in a conventional language such as C are difficult to write, debug, understand, or modify. This paper describes Teapot, a small, domain-specific language for writing coherence protocols. Teapot uses continuations to help reduce the complexity of writing protocols. Simple static analysis in the Teapot compiler eliminates much of the overhead of continuations and results in protocols that run nearly as fast as hand-written C code. A Teapot specification can be compiled both to an executable\u00a0\u2026", "num_citations": "83\n", "authors": ["295"]}
{"title": "Effective interprocedural resource leak detection\n", "abstract": " Garbage collection relieves programmers from the burden of explicit memory management. However, explicit management is still required for finite system resources, such as I/O streams, fonts, and database connections. Failure to release unneeded system resources results in resource leaks, which can lead to performance degradation and system crashes.", "num_citations": "68\n", "authors": ["295"]}
{"title": "Mechanisms for cooperative shared memory\n", "abstract": " This paper explores the complexity of implementing directory protocols by examining their mechanisms primitive operations on directories, caches, and network interfaces. We compare the following protocols: Dir1B, Dir4B, Dir4NB, DirnNB[2], Dir1SW[9] and an improved version of Dir1SW (Dir1SW+). The comparison shows that the mechanisms and mechanism sequencing of Dir1SW and Dir1SW+ are simpler than those for other protocols.   We also compare protocol performance by running eight benchmarks on 32 processor systems. Simulations show that Dir1SW+s performance is comparable to more complex directory protocols. The significant disparity in hardware complexity and the small difference in performance argue that Dir1SW+ may be a more effective use of resources. The small performance difference is attributable to two factors: the low degree of sharing in the benchmarks and Check- In/Check-Out\u00a0\u2026", "num_citations": "67\n", "authors": ["295"]}
{"title": "Efficient use of memory bandwidth to improve network processor throughput\n", "abstract": " We consider the efficiency of packet buffers used in packet switches built using network processors (NPs). Packet buffers are typically implemented using DRAM, which provides plentiful buffering at a reasonable cost. The problem we address is that a typical NP workload may be unable to utilize the peak DRAM bandwidth. Since the bandwidth of the packet buffer is often the bottleneck in the performance of a shared-memory packet switch, inefficient use of available DRAM bandwidth further reduces the packet throughput. Specialized hardware-based schemes that alleviate the DRAM bandwith problem in high-end routers may be less applicable to NP-based systems, in which cost is an important consideration.In this paper, we propose cost-effective ways to enhance average-case DRAM bandwidth. In modern DRAMs, successive accesses falling within the same DRAM row are significantly faster than those falling\u00a0\u2026", "num_citations": "63\n", "authors": ["295"]}
{"title": "Teapot: A domain-specific language for writing cache coherence protocols\n", "abstract": " In this paper, we describe Teapot, a domain-specific language for writing cache coherence protocols. Cache coherence is of concern when parallel and distributed systems make local replicas of shared data to improve scalability and performance. In both distributed shared memory systems and distributed file systems, a coherence protocol maintains agreement among the replicated copies as the underlying data are modified by programs running on the system. Cache coherence protocols are notoriously difficult to implement, debug, and maintain. Moreover, protocols are not off-the-shelf, reusable components, because their details depend on the requirements of the system under consideration. The complexity of engineering coherence protocols can discourage users from experimenting with new, potentially more efficient protocols. We have designed and implemented Teapot, a domain-specific language that\u00a0\u2026", "num_citations": "53\n", "authors": ["295"]}
{"title": "Optimizing communication in HPF programs on fine-grain distributed shared memory\n", "abstract": " Unlike compiler-generated message-passing code, the coherence mechanisms in shared-memory systems work equally well for regular and irregular programs. In many programs, however compile-time information about data accesses would permit data to be transferred more efficiently---if the underlying shared-memory system offered suitable primitives. This paper demonstrates that cooperation between a compiler and a memory coherence protocol can improve the performance of High Performance Fortran (HPF) programs running on fine-grain distributed shared memory system up to a factor of 2, while retaining the versatility and portability of shared memory. As a consequence, shared memory's performance becomes competitive with message passing for regular applications, while not affecting (or in some cases, even improving) its large advantage for irregular codes. This paper describes the design of our\u00a0\u2026", "num_citations": "50\n", "authors": ["295"]}
{"title": "Deriving object typestates in the presence of inter-object references\n", "abstract": " We are interested in static analysis of Java classes with the goal of discovering the preconditions under which a certain program point within a method may be reached, taking into account the effects of previous method calls on an object of that class. The information pertinent to this computation is represented as the object's typestate, which is a finite set of relevant predicates that abstract the object's actual state. The execution of a method depends on an object's current typestate as well as other input parameters; the object may transition to a different typestate during the method's execution. It is common for objects to contain references to other ob-jects. In such cases, an object's behavior may depend on, in addition to its own state, the state of objects it has a refer-ence to. The main contribution of this paper is to discover relevant object typestates, as well as transitions between typestates, in the presence of inter\u00a0\u2026", "num_citations": "49\n", "authors": ["295"]}
{"title": "Experience with a language for writing coherence protocols\n", "abstract": " In this paper we describe our experience with Teapot [7], a domain-specific language for writing cache coherence protocols. Cache coherence is of concern when parallel and distributed computing systems make local replicas of shared data to improve scalability and performance. In both distributed shared memory systems and distributed file systems, a coherence protocol maintains agreement among the replicated copies as the underlying data are modified by programs running on the system.Cache coherence protocols are notoriously difficult to implement, debug, and maintain. Unfortunately, protocols are not off-the-shelf items, as their details depend on the requirements of the system under consideration. This paper presents case studies detailing the successes and shortcomings of using Teapot for writing coherence protocols in two systems. The first system, loosely coherent memory (LCM)[16], implements a particular type of distributed shared memory suitable for dataparallel programming. The second system, the xFS distributed file system [9], implements a highperformance, serverless file system.", "num_citations": "39\n", "authors": ["295"]}
{"title": "Predictive test selection\n", "abstract": " Change-based testing is a key component of continuous integration at Facebook. However, a large number of tests coupled with a high rate of changes committed to our monolithic repository make it infeasible to run all potentially-impacted tests on each change. We propose a new predictive test selection strategy which selects a subset of tests to exercise for each change submitted to the continuous integration system. The strategy is learned from a large dataset of historical test outcomes using basic machine learning techniques. Deployed in production, the strategy reduces the total infrastructure cost of testing code changes by a factor of two, while guaranteeing that over 95% of individual test failures and over 99.9% of faulty changes are still reported back to developers. The method we present here also accounts for the non-determinism of test outcomes, also known as test flakiness.", "num_citations": "38\n", "authors": ["295"]}
{"title": "Code prediction by feeding trees to transformers\n", "abstract": " Code prediction, more specifically autocomplete, has become an essential feature in modern IDEs. Autocomplete is more effective when the desired next token is at (or close to) the top of the list of potential completions offered by the IDE at cursor position. This is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play. We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. Our work uses Transformers as the base neural architecture. We show that by making the Transformer architecture aware of the syntactic structure of code, we increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of several state-of-the-art next token prediction systems by margins ranging from 14% to 18%. We present in the paper\u00a0\u2026", "num_citations": "34\n", "authors": ["295"]}
{"title": "Mimic: computing models for opaque code\n", "abstract": " Opaque code, which is executable but whose source is unavailable or hard to process, can be problematic in a number of scenarios, such as program analysis. Manual construction of models is often used to handle opaque code, but this process is tedious and error-prone.(In this paper, we use model to mean a representation of a piece of code suitable for program analysis.) We present a novel technique for automatic generation of models for opaque code, based on program synthesis. The technique intercepts memory accesses from the opaque code to client objects, and uses this information to construct partial execution traces. Then, it performs a heuristic search inspired by Markov Chain Monte Carlo techniques to discover an executable code model whose behavior matches the opaque code. Native execution, parallelization, and a carefully-designed fitness function are leveraged to increase the effectiveness\u00a0\u2026", "num_citations": "33\n", "authors": ["295"]}
{"title": "Dependent types for program understanding\n", "abstract": " Weakly-typed languages such as Cobol often force programmers to represent distinct data abstractions using the same low-level physical type. In this paper, we describe a technique to recover implicitly-defined data abstractions from programs using type inference. We present a novel system of dependent types which we call guarded types, a path-sensitive algorithm for inferring guarded types for Cobol programs, and a semantic characterization of correct guarded typings. The results of our inference technique can be used to enhance program understanding for legacy applications, and to enable a number of type-based program transformations.", "num_citations": "32\n", "authors": ["295"]}
{"title": "Using tracing and dynamic slicing to tune compilers\n", "abstract": " Performance tuning improves a compiler's performance by detecting errors and missed opportunities in its analysis, optimization, and code generation stages. Normally, a compiler's author tunes it by examining the generated code to find suboptimal code sequences. This paper describes a collection of tools, called compiler auditors, that assist a compiler writer by partially mechanizing the process of finding suboptimal code sequences. Although these code sequences do not always exhibit compiler bugs, they frequently illustrate problems in a compiler. Experiments show that auditors effectively find suboptimal code, even in a high-quality, commercial compiler.After writing a high-quality compiler, its authors improve it with the time-consuming and tedious process of examining generated assembly code to find inefficient code sequences that could run faster or consume less space. These sequences direct a compiler\u00a0\u2026", "num_citations": "30\n", "authors": ["295"]}
{"title": "Alternate and learn: Finding witnesses without looking all over\n", "abstract": " Most symbolic bug detection techniques perform search over the program control flow graph based on either forward symbolic execution or backward weakest preconditions computation. The complexity of determining inter-procedural all-path feasibility makes it difficult for such analysis to judge up-front whether the behavior of a particular caller or callee procedure is relevant to a given property violation. Consequently, these methods analyze several program fragments irrelevant to the property, often repeatedly, before arriving at a goal location or an entrypoint, thus wasting resources and diminishing their scalability.               This paper presents a systematic and scalable technique for focused bug detection which, starting from the goal function, employs alternating backward and forward exploration on the program call graph to lazily infer a small scope of program fragments, sufficient to detect the bug or\u00a0\u2026", "num_citations": "25\n", "authors": ["295"]}
{"title": "Systems and methods for resource leak detection\n", "abstract": " Systems and methods for detecting resource leaks in a program using static analysis are disclosed. Dynamically adjustable sets of must-access paths can be employed for aliasing purposes to track resources intra-and inter-procedurally through a program. Actionable reports are also disclosed, in which resource leaks are prioritized, filtered and clustered to improve utility.", "num_citations": "23\n", "authors": ["295"]}
{"title": "Packet types\n", "abstract": " One of the reasons that networking code is hard to construct is that the physical on-the-wire representation of a network packet is not the same as the logical structure of the protocol data the packet carries. A programmer must, therefore, write interface code, usually as low-level C, that understands the grammar of a packet and also performs necessary byte-order and alignment adjustments. Moreover, domain-speci c information about the packet structure must often be coded up at a number of places in a system. The implementation of interface code, therefore, becomes a cumbersome and error-prone process.We propose to use the idea of types to simplify writing interface code. Unfortunately| as we explain in the paper| types in traditional programming languages are not well-suited for this purpose. Therefore, we have designed a small packet speci cation language that supplies an external type system for packet formats. A compiler for this language can generate e cient code for type checking a packet, ie, matching a packet against a type. With our system, layering of protocols is expressed naturally as re nement on types, and packet classi cation (ltering) as type-based dispatch. We show that by using our language, clean and e cient interface code can be obtained, sparing low-level programming tasks.", "num_citations": "18\n", "authors": ["295"]}
{"title": "Neural query expansion for code search\n", "abstract": " Searching repositories of existing source code for code snippets is a key task in software engineering. Over the years, many approaches to this problem have been proposed. One recent tool called NCS, takes in a natural language query and outputs relevant code snippets, often being able to correctly answer Stack Overflow questions. But what happens when the developer doesn\u2019t provide a query with a clear intent? What if shorter queries are used to demonstrate a more vague intent?", "num_citations": "15\n", "authors": ["295"]}
{"title": "What gives? A hybrid algorithm for error trace explanation\n", "abstract": " When a program fails, the cause of the failure is often buried in a long, hard-to-understand error trace. We present a new technique for automatic error localization, which formally unifies prior approaches based on computing interpolants and minimal unsatisfiable cores of failing executions. Our technique works by automatically reducing an error trace to its essential components\u2014a minimal set of statements that are responsible for the error, together with key predicates that explain how these statements lead to the failure. We prove that our approach is sound, and we show that it is useful for debugging real programs.", "num_citations": "13\n", "authors": ["295"]}
{"title": "Precise fault localization\n", "abstract": " Systems and methods for identifying expressions that are potential causes of program bugs are disclosed. A program and at least one input resulting in at least one passing test of the program can be received. Further, at least one plausible repair candidate expression in the program can be identified. In addition, the methods and systems can determine whether replacement of the at least one identified expression with at least one value, which is different from a value provided by the at least one identified expression, maintains the passage of the at least one passing test. Moreover, the at least one identified expression can be output when the replacement maintains the passage of the at least one passing test to enable a determination of a modification of the program that repairs a bug in the program.", "num_citations": "13\n", "authors": ["295"]}
{"title": "Neural Code Search Evaluation Dataset\n", "abstract": " There has been an increase of interest in code search using natural language. Assessing the performance of such code search models can be difficult without a readily available evaluation suite. In this paper, we present an evaluation dataset consisting of natural language query and code snippet pairs, with the hope that future work in this area can use this dataset as a common benchmark. We also provide the results of two code search models ([1] and [6]) from recent work. The evaluation dataset is available at https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset", "num_citations": "10\n", "authors": ["295"]}
{"title": "Technical forum: Using logical data models for understanding and transforming legacy business applications\n", "abstract": " Many legacy applications perform essential business functions; yet, due to a number of factors, modifying such applications in order to accommodate new business requirements can be troublesome. Such factors include: the volume of code in a typical application, logical code structure that has deteriorated as updates have accumulated over time, functional redundancy, code structure that reflects the dated technology on which it was built, and scarce technical skills. We have argued that the consequent difficulty of understanding and modifying legacy code can be ameliorated through the use of logical data models. In the Mastery project, we are developing both algorithms for extracting logical data models from legacy COBOL applications and software tools that use the generated models to query and transform the code from which the models are derived.", "num_citations": "10\n", "authors": ["295"]}
{"title": "Scaffle: bug localization on millions of files\n", "abstract": " Despite all efforts to avoid bugs, software sometimes crashes in the field, leaving crash traces as the only information to localize the problem. Prior approaches on localizing where to fix the root cause of a crash do not scale well to ultra-large scale, heterogeneous code bases that contain millions of code files written in multiple programming languages. This paper presents Scaffle, the first scalable bug localization technique, which is based on the key insight to divide the problem into two easier sub-problems. First, a trained machine learning model predicts which lines of a raw crash trace are most informative for localizing the bug. Then, these lines are fed to an information retrieval-based search engine to retrieve file paths in the code base, predicting which file to change to address the crash. The approach does not make any assumptions about the format of a crash trace or the language that produces it. We\u00a0\u2026", "num_citations": "9\n", "authors": ["295"]}
{"title": "Preparation for network interface recognition of network packet portion with declarative notation for field thereof and constraint therefor\n", "abstract": " A system includes a compiler component that employs a declarative notation, for a description, that describes one or more fields of a network packet. The compiler component employs a declarative notation, for the description, that describes one or more constraints for at least one field of the one or more fields. The description is of a portion of the network packet. A representation based on the description is employable for recognition of the portion of the network packet at a network interface.", "num_citations": "9\n", "authors": ["295"]}
{"title": "Neural Software Analysis\n", "abstract": " Many software development problems can be addressed by program analysis tools, which traditionally are based on precise, logical reasoning and heuristics to ensure that the tools are practical. Recent work has shown tremendous success through an alternative way of creating developer tools, which we call neural software analysis. The key idea is to train a neural machine learning model on numerous code examples, which, once trained, makes predictions about previously unseen code. In contrast to traditional program analysis, neural software analysis naturally handles fuzzy information, such as coding conventions and natural language embedded in code, without relying on manually encoded heuristics. This article gives an overview of neural software analysis, discusses when to (not) use it, and presents three example analyses. The analyses address challenging software development problems: bug detection, type prediction, and code completion. The resulting tools complement and outperform traditional program analyses, and are used in industrial practice.", "num_citations": "4\n", "authors": ["295"]}
{"title": "Industry-scale IR-based Bug Localization: A Perspective from Facebook\n", "abstract": " We explore the application of Information Retrieval (IR) based bug localization methods at a large industrial setting, Facebook. Facebook\u2019s code base evolves rapidly, with thousands of code changes being committed to a monolithic repository every day. When a bug is detected, it is often time-sensitive and imperative to identify the commit causing the bug in order to either revert it or fix it. This is complicated by the fact that bugs often manifest with complex and unwieldy features, such as stack traces and other metadata. Code commits also have various features associated with them, ranging from developer comments to test results. This poses unique challenges to bug localization methods, making it a highly non-trivial operation.In this paper we lay out several practical concerns for industry-level IR-based bug localization, and propose Bug2Commit, a tool that is designed to address these concerns. We also assess\u00a0\u2026", "num_citations": "3\n", "authors": ["295"]}
{"title": "Finding Fix Locations for CFL-Reachability Analyses via Minimum Cuts\n", "abstract": " Static analysis tools are increasingly important for ensuring code quality. Ideally, all warnings from a static analysis would be addressed, but the volume of warnings and false positives usually makes this effort prohibitive. We present techniques for finding fix locations, a small set of program locations where fixes can be applied to address all static analysis warnings. We focus on analyses expressible as context-free-language reachability, where a set of fix locations is naturally expressed as a min-cut of the CFL graph. We show, surprisingly, that computing such a CFL min-cut is NP-hard. We then phrase the problem of finding CFL min-cuts as an optimization problem which allows us to trade-off the size of the cut vs. the preservation of computed information. We then show how to solve the optimization problem via a MaxSAT encoding.                 Our evaluation shows that we compute fix location sets that are\u00a0\u2026", "num_citations": "3\n", "authors": ["295"]}
{"title": "Explaining mispredictions of machine learning models using rule induction\n", "abstract": " While machine learning (ML) models play an increasingly prevalent role in many software engineering tasks, their prediction accuracy is often problematic. When these models do mispredict, it can be very difficult to isolate the cause. In this paper, we propose a technique that aims to facilitate the debugging process of trained statistical models. Given an ML model and a labeled data set, our method produces an interpretable characterization of the data on which the model performs particularly poorly. The output of our technique can be useful for understanding limitations of the training data or the model itself; it can also be useful for ensembling if there are multiple models with different strengths. We evaluate our approach through case studies and illustrate how it can be used to improve the accuracy of predictive models used for software engineering tasks within Facebook.", "num_citations": "1\n", "authors": ["295"]}
{"title": "Scalable Statistical Root Cause Analysis on App Telemetry\n", "abstract": " Despite engineering workflows that aim to prevent buggy code from being deployed, bugs still make their way into the Facebook app. When symptoms of these bugs, such as user submitted reports and automatically captured crashes, are reported, finding their root causes is an important step in resolving them. However, at Facebook\u2019s scale of billions of users, a single bug can manifest as several different symptoms according to the various user and execution environments in which the software is deployed. Root cause analysis (RCA) therefore requires tedious manual investigation and domain expertise to extract out common patterns that are observed in groups of reports and use them for debugging. We propose Minesweeper, a technique for RCA that moves towards automatically identifying the root cause of bugs from their symptoms. The method is based on two key aspects: (i) a scalable algorithm to efficiently\u00a0\u2026", "num_citations": "1\n", "authors": ["295"]}
{"title": "AI in Software Engineering at Facebook\n", "abstract": " We address the question: How can AI help software engineers better do their jobs and advance the state of the practice? Our perspective comes from building and integrating AI-based techniques in Facebook\u2019s developer infrastructure over the past two years. In this article, we describe three productivity tools that we have built that learn patterns from software artifacts: code search using natural language, code recommendation, and automatic bug fixing. We also present a broader picture of how machine learning can bring insights to virtually all stages of the software lifecycle.", "num_citations": "1\n", "authors": ["295"]}
{"title": "Packet classification using abstract types\n", "abstract": " One of the reasons that networking code is hard to construct is that the physical onthe-wire representation of a network packet is not the same as the logical structure of the protocol data the packet carries. A programmer must, therefore, write interface code, usually as low-level C, that understands the grammar of a packet and also performs necessary byte-order and alignment adjustments. Moreover, domain-speci c information about the packet structure must often be coded up at a number of places in a system. The implementation of interface code, therefore, becomes a cumbersome and error-prone process. We propose to use the idea of types to simplify writing interface code. Unfortunately| as we explain in the paper| types in traditional programming languages are not well-suited for this purpose. Therefore, we have designed a small packet specication language that supplies an external type system for packet\u00a0\u2026", "num_citations": "1\n", "authors": ["295"]}
{"title": "Software techniques for customizable distributed shared memory\n", "abstract": " Customizable distributed shared memory is a new interface to distributed memory machines that promises to combine the performance advantage of message passing and the better programmability of shared memory. This interface allows a programmer to supply a cache coherence protocol--written entirely in software--that is customized for the application at hand. An application-specific coherence protocol can be more efficient than a general-purpose coherence protocol, because it can exploit knowledge about memory accesses in the various phases of the application. However, achieving good performance on this interface is still a difficult programming task. This thesis investigates software techniques that make it easier to exploit the power of customizable distributed shared memory systems.", "num_citations": "1\n", "authors": ["295"]}