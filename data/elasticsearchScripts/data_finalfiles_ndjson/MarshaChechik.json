{"title": "Merging partial behavioural models\n", "abstract": " Constructing comprehensive operational models of intended system behaviour is a complex and costly task. Consequently, practitioners have adopted techniques that support incremental elaboration of partial behaviour descriptions. A noteworthy example is the wide adoption of scenario-based notations such as message sequence charts. Scenario-based specifications are partial descriptions that can be incrementally elaborated to cover the system behaviour that is of interest. However, how should partial behavioural models described by different stakeholders with different viewpoints covering different aspects of behaviour be composed? How should partial models of component instances of the same type be put together. In this paper, we propose model merging as a general solution to these questions. We formally define model merging based on observational refinement and show that merging consistent\u00a0\u2026", "num_citations": "193\n", "authors": ["664"]}
{"title": "Partial models: Towards modeling and reasoning with uncertainty\n", "abstract": " Models are good at expressing information about software but not as good at expressing modelers' uncertainty about it. The highly incremental and iterative nature of software development nonetheless requires the ability to express uncertainty and reason with models containing it. In this paper, we build on our earlier work on expressing uncertainty using partial models, by elaborating an approach to reasoning with such models. We evaluate our approach by experimentally comparing it to traditional strategies for dealing with uncertainty as well as by conducting a case study using open source software. We conclude that we are able to reap the benefits of well-managed uncertainty while incurring minimal additional cost.", "num_citations": "157\n", "authors": ["664"]}
{"title": "A survey of feature location techniques\n", "abstract": " Feature location techniques aim at locating software artifacts that implement a specific program functionality, a.k.a. a feature. These techniques support developers during various activities such as software maintenance, aspect- or feature-oriented refactoring, and others. For example, detecting artifacts that correspond to product line features can assist the transition from unstructured to systematic reuse approaches promoted by software product line engineering (SPLE). Managing features, as well as the traceability between these features and the artifacts that implement them, is an essential task of the SPLE domain engineering phase, during which the product line resources are specified, designed, and implemented. In this chapter, we provide an overview of existing feature location techniques. We describe their implementation strategies and exemplify the techniques on a realistic use-case. We also\u00a0\u2026", "num_citations": "151\n", "authors": ["664"]}
{"title": "Synthesis of partial behavior models from properties and scenarios\n", "abstract": " Synthesis of behavior models from software development artifacts such as scenario-based descriptions or requirements specifications helps reduce the effort of model construction. However, the models favored by existing synthesis approaches are not sufficiently expressive to describe both universal constraints provided by requirements and existential statements provided by scenarios. In this paper, we propose a novel synthesis technique that constructs behavior models in the form of modal transition systems (MTS) from a combination of safety properties and scenarios. MTSs distinguish required, possible, and proscribed behavior, and their elaboration not only guarantees the preservation of the properties and scenarios used for synthesis but also supports further elicitation of new requirements.", "num_citations": "139\n", "authors": ["664"]}
{"title": "Combining related products into product lines\n", "abstract": " We address the problem of refactoring existing, closely related products into product line representations. Our approach is based on comparing and matching artifacts of these existing products and merging those deemed similar while explicating those that vary. Our work focuses on formal specification of a product line refactoring operator called merge-in that puts individual products together into product lines. We state sufficient conditions of model compare, match and merge operators that allow application of merge-in. Based on these, we formally prove correctness of the merge-in operator. We also demonstrate its operation on a small but realistic example.", "num_citations": "113\n", "authors": ["664"]}
{"title": "A buffer overflow benchmark for software model checkers\n", "abstract": " Software model checking based on abstraction-refinement has recently achieved widespread success in verifying API conformance in device drivers, and we believe this success can be replicated for the problem of buffer overflow detection. This paper presents a publicly-available benchmark suite to help guide and evaluate this research. The benchmark consists of 298 code fragments of varying complexity capturing 22 buffer overflow vulnerabilities in 12 open source applications. We give a preliminary evaluation of the benchmark using the SatAbs model checker", "num_citations": "102\n", "authors": ["664"]}
{"title": "Behaviour model synthesis from properties and scenarios\n", "abstract": " Synthesis of behaviour models from software development artifacts such as scenario-based descriptions or requirements specifications not only helps significantly reduce the effort of model construction, but also provides a bridge between approaches geared toward requirements analysis and those geared towards reasoning about system design at the architectural level. However, the models favoured by existing synthesis approaches are not sufficiently expressive to describe both universal constraints provided by requirements and existential statements provided by scenarios. In this paper, we propose a novel synthesis technique that constructs behaviour models in the form of modal transition systems (MTS) from a combination of safety properties and scenarios. MTSs distinguish required, possible and proscribed behaviour, and their elaboration not only guarantees the preservation of the properties and\u00a0\u2026", "num_citations": "96\n", "authors": ["664"]}
{"title": "MTSA: The modal transition system analyser\n", "abstract": " Modal transition systems (MTS) are operational models that distinguish between required and proscribed behaviour of the system to be and behaviour which it is not yet known whether the system should exhibit. MTS, in contrast with traditional behaviour models, support reasoning about the intended system behaviour in the presence of incomplete knowledge. In this paper, we present MTSA a tool that supports the construction, analysis and elaboration of Modal Transition Systems (MTS).", "num_citations": "95\n", "authors": ["664"]}
{"title": "Managing requirements uncertainty with partial models\n", "abstract": " Models are good at expressing information that is known but do not typically have support for representing what information a modeler does not know at a particular phase in the software development process. Partial models address this by being able to precisely represent uncertainty about model content. In previous work, we developed a general approach for defining partial models and using them to reason about design models containing uncertainty. In this paper, we show how to apply our approach to managing uncertainty in requirements by providing support for uncertainty capture, elaboration, and change. In particular, we address the problem of specifying uncertainty within a requirements model, refining a model as uncertainty reduces, providing meaning to traceability relations between models containing uncertainty, and propagating uncertainty-reducing changes between related models. We\u00a0\u2026", "num_citations": "90\n", "authors": ["664"]}
{"title": "N-way model merging\n", "abstract": " Model merging is widely recognized as an essential step in a variety of software development activities. During the process of combining a set of related products into a product line or consolidating model views of multiple stakeholders, we need to merge multiple input models into one; yet, most of the existing approaches are applicable to merging only two models. In this paper, we define the n-way merge problem. We show that it can be reduced to the known and widely studied NP-hard problem of weighted set packing. Yet, the approximation solutions for that problem do not scale for real-sized software models. We thus evaluate alternative approaches of merging models that incrementally process input models in small subsets and propose our own algorithm that considerably improves precision over such approaches without sacrificing performance.", "num_citations": "84\n", "authors": ["664"]}
{"title": "Language independent refinement using partial modeling\n", "abstract": " Models express not only information about their intended domain but also about the way in which the model is incomplete, or \u201cpartial\u201d. This partiality supports the modeling process because it permits the expression of what is known without premature decisions about what is still unknown, until later refinements can fill in this information. A key observation of this paper is that a number of partiality types can be defined in a modeling language-independent way, and we propose a formal framework for doing so. In particular, we identify four types of partiality and show how to extend a modeling language to support their expression and refinement. This systematic approach provides a basis for reasoning as well as a framework for generic tooling support. We illustrate the framework by enhancing the UML class diagram and sequence diagram languages with partiality support and using Alloy to automate\u00a0\u2026", "num_citations": "74\n", "authors": ["664"]}
{"title": "Lifting model transformations to product lines\n", "abstract": " Software product lines and model transformations are two techniques used in industry for managing the development of highly complex software. Product line approaches simplify the handling of software variants while model transformations automate software manipulations such as refactoring, optimization, code generation, etc. While these techniques are well understood independently, combining them to get the benefit of both poses a challenge because most model transformations apply to individual models while model-level product lines represent sets of models. In this paper, we address this challenge by providing an approach for automatically``lifting''model transformations so that they can be applied to product lines. We illustrate our approach using a case study and evaluate it through a set of experiments.", "num_citations": "67\n", "authors": ["664"]}
{"title": "Automatic analysis of consistency between requirements and designs\n", "abstract": " Writing requirements in a formal notation permits automatic assessment of such properties as ambiguity, consistency, and completeness. However, verifying that the properties expressed in requirements are preserved in other software life cycle artifacts remains difficult. The existing techniques either require substantial manual effort and skill or suffer from exponential explosion of the number of states in the generated state spaces. \"Light-weight\" formal methods is an approach to achieve scalability in fully automatic verification by checking an abstraction of the system for only certain properties. We describe light-weight techniques for automatic analysis of consistency between software requirements (expressed in SCR) and detailed designs in low-degree-polynomial time, achieved at the expense of using imprecise data-flow analysis techniques. A specification language SCR describes the systems as state machines\u00a0\u2026", "num_citations": "66\n", "authors": ["664"]}
{"title": "A framework for managing cloned product variants\n", "abstract": " We focus on the problem of managing a collection of related software products realized via cloning. We contribute a framework that explicates operators required for developing and maintaining such products, and demonstrate their usage on two concrete scenarios observed in industrial settings: sharing of features between cloned variants and re-engineering the variants into \u201csingle-copy\u201d representations advocated by software product line engineering approaches. We discuss possible implementations of the operators, including synergies with existing work developed in seemingly unrelated contexts, with the goal of helping understand and structure existing work and identify opportunities for future research.", "num_citations": "64\n", "authors": ["664"]}
{"title": "Runtime monitoring of web service conversations\n", "abstract": " For a system of distributed processes, correctness can be ensured by (statically) checking whether their composition satisfies properties of interest. In contrast, Web services are being designed so that each partner discovers properties of others dynamically, through a published interface. Since the overall system may not be available statically and since each business process is supposed to be relatively simple, we propose to use runtime monitoring of conversations between partners as a means of checking behavioural correctness of the entire web service system. Specifically, we identify a subset of UML 2.0 Sequence Diagrams as a property specification language and show that it is sufficiently expressive for capturing safety and liveness properties. By transforming these diagrams to automata, we enable conformance checking of finite execution traces against the specification. We describe an implementation of\u00a0\u2026", "num_citations": "58\n", "authors": ["664"]}
{"title": "Transformation of models containing uncertainty\n", "abstract": " Model transformation techniques typically operate under the assumption that models do not contain uncertainty. In the presence of uncertainty, this forces modelers to either postpone working or to artificially remove it, with negative impacts on software cost and quality. Instead, we propose a technique to adapt existing model transformations so that they can be applied to models even if they contain uncertainty, thus enabling the use of transformations earlier. Building on earlier work, we show how to adapt graph rewrite-based model transformations to correctly operate on May uncertainty, a technique that allows explicit uncertainty to be expressed in any modeling language. We evaluate our approach on the classic Object-Relational Mapping use case, experimenting with models of varying levels of uncertainty.", "num_citations": "52\n", "authors": ["664"]}
{"title": "A relationship-based approach to model integration\n", "abstract": " A key problem in model-based development is integrating a collection of models into a single, larger, specification as a way to construct a functional system, to develop a unified understanding, or to enable automated reasoning about properties of the resulting system. In this article, we suggest that the choice of a particular model integration operator depends on the inter-model relationships that hold between individual models. Based on this observation, we distinguish three key integration operators studied in the literature\u2014merge, composition and weaving\u2014and describe each operator along with the notion of relationship that underlies it. We then focus on the merge activity and provide a detailed look at the factors that one must consider in defining a merge operator, particularly the way in which the relationships should be captured during merge. We illustrate these factors using two merge operators that\u00a0\u2026", "num_citations": "50\n", "authors": ["664"]}
{"title": "Semantic slicing of software version histories\n", "abstract": " Software developers often need to transfer functionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level, semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a segment of the change history, \u201cinheriting\u201d additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and\u00a0\u2026", "num_citations": "45\n", "authors": ["664"]}
{"title": "Locating distinguishing features using diff sets\n", "abstract": " In this paper, we focus on the problem of feature location for families of related software products realized via code cloning. Locating code that corresponds to features in such families is an important task in many software development activities, such as support for sharing features between different products of the family or refactoring the code into product line representations that eliminate duplications and facilitate reuse. We suggest two heuristics for improving the accuracy of existing feature location techniques when locating distinguishing features - those that are present in one product variant while absent in another. Our heuristics are based on identifying code regions that have a high potential to implement a feature of interest. We refer to these regions as diff sets and compute them by comparing product variants to each other. We exemplify our approach on a small but realistic example and describe initial\u00a0\u2026", "num_citations": "41\n", "authors": ["664"]}
{"title": "A relationship-driven framework for model merging\n", "abstract": " A key problem in model-based development is merging a set of distributed models into a single seamless model. To merge a set of models, we need to know how they are related. In this position paper, we discuss the methodological aspects of describing the relationships between models. We argue that relationships between models should be treated as first-class artifacts in the merge problem and propose a general framework for model merging based on this argument. We illustrate the usefulness of our framework by instantiating it to the state-machine modelling domain and developing a flexible tool for merging state-machines.", "num_citations": "39\n", "authors": ["664"]}
{"title": "Events in property patterns\n", "abstract": " A pattern-based approach to the presentation, codification and reuse of property specifications for finite-state verification was proposed by Dwyer and his colleagues in [4,3]. The patterns enable non-experts to read and write formal specifications for realistic systems and facilitate easy conversion of specifications between formalisms, such as LTL, CTL, QRE. In this paper we extend the pattern system with events \u2014 changes of values of variables in the context of LTL.", "num_citations": "39\n", "authors": ["664"]}
{"title": "Weak alphabet merging of partial behavior models\n", "abstract": " Constructing comprehensive operational models of intended system behavior is a complex and costly task, which can be mitigated by the construction of partial behavior models, providing early feedback and subsequently elaborating them iteratively. However, how should partial behavior models with different viewpoints covering different aspects of behavior be composed? How should partial models of component instances of the same type be put together? In this article, we propose model merging of modal transition systems (MTSs) as a solution to these questions. MTS models are a natural extension of labelled transition systems that support explicit modeling of what is currently unknown about system behavior. We formally define model merging based on weak alphabet refinement, which guarantees property preservation, and show that merging consistent models is a process that should result in a minimal\u00a0\u2026", "num_citations": "37\n", "authors": ["664"]}
{"title": "Events in linear-time properties\n", "abstract": " For over a decade, researchers in formal methods tried to create formalisms that permit natural specification of systems and allow mathematical reasoning about their correctness. The availability of fully-automated reasoning tools enables more non-specialists to use formal methods effectively-their responsibility reduces to just specifying the model and expressing the desired properties. Thus, it is essential that these properties be represented in a language that is easy to use and sufficiently expressive. Linear-time temporal logic is a formalism that has been extensively used by researchers for specifying properties of systems. When such properties are closed under stuttering, i.e. their interpretation is not modified by transitions that leave the system in the same state, verification tools can utilize a partial-order reduction technique to reduce the size of the model and thus analyze larger systems. If LTL formulas do not\u00a0\u2026", "num_citations": "34\n", "authors": ["664"]}
{"title": "RuleMerger: automatic construction of variability-based model transformation rules\n", "abstract": " Unifying similar model transformation rules into variability-based ones can improve both the maintainability and the performance of a model transformation system. Yet, manual identification and unification of such similar rules is a tedious and error-prone task. In this paper, we propose a novel merge-refactoring approach for automating this task. The approach employs clone detection for identifying overlapping rule portions and clustering for selecting groups of rules to be unified. Our instantiation of the approach harnesses state-of-the-art clone detection and clustering techniques and includes a specialized merge construction algorithm. We formally prove correctness of the approach and demonstrate its ability to produce high-quality outcomes in two\u00a0real-life\u00a0case-studies.", "num_citations": "33\n", "authors": ["664"]}
{"title": "Splitting models using information retrieval and model crawling techniques\n", "abstract": " In team environments, models are often shared and edited by multiple developers. To allow modularity and facilitate developer independence, we consider the problem of splitting a large monolithic model into sub-models. We propose an approach that assists users in incrementally discovering the set of desired sub-models. Our approach is supported by an automated tool that performs model splitting using information retrieval and model crawling techniques. We demonstrate the effectiveness of our approach on a set of real-life case studies, involving UML class models and EMF meta-models.", "num_citations": "33\n", "authors": ["664"]}
{"title": "Quality of merge-refactorings for product lines\n", "abstract": " In this paper, we consider the problem of refactoring related software products specified in UML into annotative product line representations. Our approach relies on identifying commonalities and variabilities in existing products and further merging those into product line representations which reduce duplications and facilitate reuse. Varying merge strategies can lead to producing several semantically correct, yet syntactically different refactoring results. Depending on the goal of the refactoring, one result can be preferred to another. We thus propose to capture the goal using a syntactic quality function and use that function to guide the merge strategy. We define and implement a quality-based merge-refactoring framework for UML models containing class and statechart diagrams and report on our experience applying it on three case-studies.", "num_citations": "31\n", "authors": ["664"]}
{"title": "Looking into the crystal ball: requirements evolution over time\n", "abstract": " Goal modeling has long been used in the literature to model and reason about system requirements, constraints within the domain and environment, and stakeholders' goals. Goal model analysis helps stakeholders answer 'what if' questions enabling them to make tradeoff decisions about their project requirements. However, questions concerning the evolution over time of stakeholder requirements or changes in actor intentionality are not explicitly addressed by current approaches. In this paper, we tackle this problem by presenting a method for specifying changes in intentions over time, and a technique that uses simulation for asking a variety of 'what if' questions about such models. Using the development of a web-based modeling tool as an example, we demonstrate that this technique is effective for debugging goal models and answering stakeholder questions.", "num_citations": "27\n", "authors": ["664"]}
{"title": "A Methodology for Verifying Refinements of Partial Models.\n", "abstract": " Abstract Models are typically used for expressing information that is known at a particular stage in the software development process. Yet, it is also important to express what information a modeler is still uncertain about and to ensure that model refinements actually reduce this uncertainty. Furthermore, when a refining transformation is applied to a model containing uncertainty, it is natural to consider the effect that the transformation has on the level of uncertainty, eg, whether it always reduces it. In our previous work, we have presented a general approach for precisely expressing uncertainty within models. In this paper, we use these foundations and define formal conditions for uncertainty reducing refinement between individual models and within model transformations. We describe tooling for automating the verification of these conditions within transformations and describe its application to example transformations.", "num_citations": "27\n", "authors": ["664"]}
{"title": "Automatic verification of requirements implementation\n", "abstract": " Requirements of event-based systems can be automatically analyzed to determine if certain safety properties hold. However, we lack comparable methods to verify that implementations maintain the properties guaranteed by the requirements. We have built a tool that compares implementations written in C with their requirements. Requirements describe events which cause state transitions. Implementations are annotated to describe changes in the values of their requirement's variables, and dataflow analysis techniques are used to determine the set of events which cause particular state changes. To show that an implementation is consistent with its requirements, we show that each event causing a change of state in the implementation appears in the requirements, and that all the events specified to cause state changes in the requirements appear in the implementation. The annotation language encourages\u00a0\u2026", "num_citations": "27\n", "authors": ["664"]}
{"title": "Managing design-time uncertainty\n", "abstract": " Managing design-time uncertainty, i.e., uncertainty that developers have about making design decisions, requires creation of \u201cuncertainty-aware\u201d software engineering methodologies. In this paper, we propose a methodological approach for managing uncertainty using partial models. To this end, we identify the stages in the lifecycle of uncertainty-related design decisions and characterize the tasks needed to manage it. We encode this information in the Design-Time Uncertainty Management (DeTUM) model. We then use the DeTUM model to create a coherent, tool-supported methodology centred around partial model management. We demonstrate the effectiveness and feasibility of our methodology through case studies.", "num_citations": "25\n", "authors": ["664"]}
{"title": "Variability-based model transformation: formal foundation and application\n", "abstract": " Model transformation systems often contain transformation rules that are substantially similar to each other, causing maintenance issues and performance bottlenecks. To address these issues, we introduce variability-based model transformation. The key idea is to encode a set of similar rules into a compact representation, called variability-based rule. We provide an algorithm for applying such rules in an efficient manner. In addition, we introduce rule merging, a three-component mechanism for enabling the automatic creation of variability-based rules. Our rule application and merging mechanisms are supported by a novel formal framework, using category theory to provide precise definitions and to prove correctness. In two realistic application scenarios, the created variability-based rules enabled considerable speedups, while also allowing the overall specifications to become more compact.", "num_citations": "25\n", "authors": ["664"]}
{"title": "A variability-based approach to reusable and efficient model transformations\n", "abstract": " Large model transformation systems often contain transformation rules that are substantially similar to each other, causing performance bottlenecks for systems in which rules are applied nondeterministically, as long as one of them is applicable. We tackle this problem by introducing variability-based graph transformations. We formally define variability-based rules and contribute a novel match-finding algorithm for applying them. We prove correctness of our approach by showing its equivalence to the classic one of applying the rules individually, and demonstrate the achieved performance speed-up on a realistic transformation scenario.", "num_citations": "24\n", "authors": ["664"]}
{"title": "Properties of behavioural model merging\n", "abstract": " Constructing comprehensive operational models of intended system behaviour is a complex and costly task. Consequently, practitioners adopt techniques that support partial behaviour decription such as scenario-based specifications, and focus on elaborating these descriptions iteratively. In previous work, we show how this process can be formally supported by Modal Transition Systems (MTSs), observational refinement, and model merging. In this paper, we study a number of properties of merging MTSs and give insights on the implications these results have on engineering and reasoning about behaviour models. We illustrate the utility of our results on a case study.", "num_citations": "24\n", "authors": ["664"]}
{"title": "Transformations of software product lines: A generalizing framework based on category theory\n", "abstract": " Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while\u00a0\u2026", "num_citations": "23\n", "authors": ["664"]}
{"title": "Partial models: A position paper\n", "abstract": " Model-based software development inevitably involves dealing with incomplete information. Yet, MDE methodologies rarely, if ever, address uncertainty in a systematic way. Drawing inspiration from the field of behavioural modeling [6, 4], we propose to use partial models as first-class development artifacts to abstract, reason with, visualize and manipulate sets of possible alternative models. Aiming to set up a research agenda for a systematic and robust treatment of uncertainty in MDE, we discuss how uncertainty can be captured by partial models, and how these models can be validated and correctly refined.", "num_citations": "23\n", "authors": ["664"]}
{"title": "On closure under stuttering\n", "abstract": " Abstract.                              For over a decade, researchers in formal methods have tried to create formalisms that permit natural specification of systems and allow mathematical reasoning about their correctness. The availability of fully automated reasoning tools enables non-experts to use formal methods effectively\u2014their responsibility reduces to specifying the model and expressing the desired properties. Thus, it is essential that these properties be represented in a language that is easy to use, sufficiently expressive and succinct. Linear-time temporal logic (LTL) is a formalism that has been used extensively by researchers for program specification and verification. One of the desired properties of LTL formulas is closure under stuttering. That is, we do not want the interpretation of formulas to change over traces where some states are repeated. This property is important from both practical and theoretical\u00a0\u2026", "num_citations": "21\n", "authors": ["664"]}
{"title": "Precise semantic history slicing through dynamic delta refinement\n", "abstract": " Semantic history slicing solves the problem of extracting changes related to a particular high-level functionality from software version histories. State-of-the-art techniques combine static program analysis and dynamic execution tracing to infer an over-approximated set of changes that can preserve the functional behaviors captured by a test suite. However, due to the conservative nature of such techniques, the sliced histories may contain irrelevant changes. In this paper, we propose a divide-and-conquer-style partitioning approach enhanced by dynamic delta refinement to produce much smaller semantic history slices. We utilize deltas in dynamic invariants generated from successive test executions to learn significance of changes with respect to the target functionality. Additionally, we introduce a file-level commit splitting technique for untangling unrelated changes introduced in a single commit. Empirical results\u00a0\u2026", "num_citations": "20\n", "authors": ["664"]}
{"title": "A generalized formal framework for partial modeling\n", "abstract": " Uncertainty is pervasive within software engineering, negatively affecting software quality as well as development time. In previous work, we have developed a language-independent partial modeling technique called MAVO that allows a software modeler to explicitly express and reason about model uncertainty. The cost of such a broadly applicable technique was to focus exclusively on the syntactic aspects of models. In addition, we have found that while MAVO expresses uncertainty at the model level, it is often more natural to do so for the entire submodels.                 In this paper, we introduce a new language-independent formal framework for partial modeling called GMAVO that generalizes MAVO by providing the means for addressing model semantics and allowing uncertainty to be specified at the granularity of a submodel. We then show that GMAVO is sufficiently general to express Modal\u00a0\u2026", "num_citations": "20\n", "authors": ["664"]}
{"title": "Change propagation due to uncertainty change\n", "abstract": " Uncertainty is ubiquitous in software engineering; however, it has been typically handled in adhoc and informal ways within software models. Automated change propagation is recognized as a key tool for managing the accidental complexity that comes with multiple interrelated models. In this paper, we address change propagation in the context of model uncertainty and consider the case where changes in the level of uncertainty in a model can be propagated to related models. We define such uncertainty change propagation using our earlier formalization and develop automated propagation algorithms using an SMT solver. A preliminary evaluation shows that the approach is feasible.", "num_citations": "20\n", "authors": ["664"]}
{"title": "Towards a methodology for verifying partial model refinements\n", "abstract": " Models are good at expressing information that is known but do not typically have support for representing what information a modeler does not know or does not care about at a particular stage in the software development process. Partial models address this by being able to precisely represent uncertainty about model content. In previous work, we have defined a general approach for defining partial model semantics using a first order logic encoding. In this paper, we use this FO encoding to formally define the conditions for partial model refinement in the manner of the refinement of algebraic specifications. We use this approach to verify both manual refinements and automated transformation-based refinements. We illustrate our approach using example models and transformations.", "num_citations": "20\n", "authors": ["664"]}
{"title": "Migrating automotive product lines: a case study\n", "abstract": " Software Product Lines (SPL) are widely used to manage variability in the automotive industry. In a rapidly changing industrial environment, model transformations are necessary to aid in automating the evolution of SPLs. However, existing transformation technologies are not well-suited to handling industrial-grade variability in software artifacts. We present a case study where we \u201clift\u201d a previously developed migration transformation so that it becomes applicable to realistic industrial product lines. Our experience indicates that it is both feasible and scalable to lift transformations for industrial SPLs.", "num_citations": "19\n", "authors": ["664"]}
{"title": "Supporting early decision-making in the presence of uncertainty\n", "abstract": " Requirements Engineering (RE) involves eliciting, understanding, and capturing system requirements, which naturally involves much uncertainty. During RE, analysts choose among alternative requirements, gradually narrowing down the system scope, and it is unlikely that all requirements uncertainties can be resolved before such decisions are made. There is a need for methods to support early requirements decision-making in the presence of uncertainty. We address this need by describing a novel technique for early decision-making and tradeoff analysis using goal models with uncertainty. The technique analyzes goal satisfaction over sets of models that can result from resolving uncertainty. Users make choices over possible analysis results, allowing our tool to find critical uncertainty reductions which must be resolved. An iterative methodology guides the resolution of uncertainties necessary to achieve\u00a0\u2026", "num_citations": "19\n", "authors": ["664"]}
{"title": "From Products to Product Lines Using Model Matching and Refactoring.\n", "abstract": " In this paper, we suggest a method for refactoring UML structural and behavioral models of closely related individual products into product lines. We propose to analyze duplications in the models of individual products using a heterogeneous match algorithm which takes into account structural and behavioral information to identify identical and similar model elements. Identical elements (exact matches) are refactored to common parts of the product line, similar elements are refactored to variable alternative parts, and unmatched elements are refactored to variable optional parts. We further propose to adjust the quality of the match by analyzing quality of the resulting refactoring. We evaluate UML comprehensibility before and after the change using prediction models that are based on static metrics, and use the results to set the optimal thresholds for identity and similarity between model elements. We illustrate our proposed approach on an example.", "num_citations": "19\n", "authors": ["664"]}
{"title": "Let's agree to disagree\n", "abstract": " Almost every kind of software development periodically needs to merge models. Perhaps they come from different stakeholders during the requirements analysis phase, or perhaps they are modifications of the same model done independently by several groups of people. Sometimes these models are consistent and can be merged. Sometimes they are not, and negotiation between the stakeholders is needed in order to resolve inconsistencies. While various methods support merging, we need formal approaches that help stakeholders negotiate. We present a formal framework for merging and conflict resolution. It facilitates automatic merging of consistent models, enables users to visualize and explore potential disagreements and identify their priorities, and suggests ways to resolve the priority items.", "num_citations": "19\n", "authors": ["664"]}
{"title": "Client-specific equivalence checking\n", "abstract": " Software is often built by integrating components created by different teams or even different organizations. With little understanding of changes in dependent components, it is challenging to maintain correctness and robustness of the entire system. In this paper, we investigate the effect of component changes on the behavior of their clients. We observe that changes in a component are often irrelevant to a particular client and thus can be adopted without any delays or negative effects. Following this observation, we formulate the notion of client-specific equivalence checking (CSE) and develop an automated technique optimized for checking such equivalence. We evaluate our technique on a set of benchmarks, including those from the existing literature on equivalence checking, and show its applicability and effectiveness.", "num_citations": "18\n", "authors": ["664"]}
{"title": "Perspectives of model transformation reuse\n", "abstract": " Model Transformations have been called the \u201cheart and soul\u201d of Model-Driven software development. However, they take a lot of effort to build, verify, analyze, and debug. It is thus imperative to develop good reuse strategies that address issues specific to model transformations. Some of the effective reuse strategies are adopted from other domains, specifically, programming languages. Others are custom developed for models. In this paper, we survey techiques from both categories.                 Specifically, we present two techniques adoped from the PL world: subtyping and mapping, and then two techniques, lifting and aggregating, that are novel in the modeling world. Subtyping is a way to reuse a transformation for different - but similar - input modelling languages. Mapping a transformation designed for single models reuses it for model collections, such as megamodels. Lifting a transformation reuses it\u00a0\u2026", "num_citations": "18\n", "authors": ["664"]}
{"title": "Automatic analysis of consistency between implementations and requirements: A case study\n", "abstract": " Formal methods like model checking can be used to demonstrate that safety properties of event-based systems are enforced by the system's requirements. Unfortunately, proving these properties provides no guarantee that they will be preserved in an implementation of the system. This paper describes a tool, called Analyzer, which discovers instances of inconsistency and incompleteness in implementations, and a case study of its use. Analyzer uses requirements information to check: that all state transitions implemented in the code are exactly those specified in the requirements. If programmers annotate their implementations with comments describing changes of values of requirements variables, our tool checks it against the requirements. Analyzer can also verify that the implementation is a model of user-specified safety properties. The tool can perform interprocedural analysis and verify properties which\u00a0\u2026", "num_citations": "18\n", "authors": ["664"]}
{"title": "The semantics of partial model transformations\n", "abstract": " Model transformations are traditionally designed to operate on models that do not contain uncertainty. In previous work, we have developed partial models, i.e., models that explicitly capture uncertainty. In this paper, we study the transformation of partial models. We define the notion of correct lifting of transformations so that they can be applied to partial models. For this, we encode transformations as transfer predicates and describe the mechanics of applying transformations using logic. We demonstrate the approach using two example transformations (addition and deletion) and outline a method for testing the application of transformations using a SAT solver. Reflecting on these preliminary attempts, we discuss the main limitations and challenges and outline future steps for our research on partial model transformation.", "num_citations": "17\n", "authors": ["664"]}
{"title": "Supporting incremental behaviour model elaboration\n", "abstract": " Behaviour model construction remains a difficult and labour intensive task which hinders the adoption of model-based methods by practitioners. We believe one reason for this is the mismatch between traditional approaches and current software development process best practices which include iterative development, adoption of use-case and scenario-based techniques and viewpoint- or stakeholder-based analysis; practices which require modelling and analysis in the presence of partial information about system behaviour.               Our objective is to address the limitations of behaviour modelling and analysis by shifting the focus from traditional behaviour models and verification techniques that require full behaviour information to partial behaviour models and analysis techniques, that drive model elaboration rather than asserting adequacy. We aim to develop sound theory, techniques and tools that\u00a0\u2026", "num_citations": "16\n", "authors": ["664"]}
{"title": "Merging partial behaviour models with different vocabularies\n", "abstract": " Modal transition systems (MTSs) and their variants such as Disjunctive MTSs (DMTSs) have been extensively studied as a formalism for partial behaviour model specification. Their semantics is in terms of implementations, which are fully specified behaviour models in the form of Labelled Transition Systems. A natural operation for these models is that of merge, which should yield a partial model which characterizes all common implementations. Merging has been studied for models with the same vocabularies; however, to enable composition of specifications from different viewpoints, merging of models with different vocabularies must be supported as well. In this paper, we first prove that DMTSs are not closed under merge for models with different vocabularies. We then define an extension to DMTS called rDMTS, for which we describe a first exact algorithm for merging partial models, provided they satisfy\u00a0\u2026", "num_citations": "16\n", "authors": ["664"]}
{"title": "On interpreting results of model-checking with abstraction\n", "abstract": " Model-checking offers a potential for push-button verification. Abstraction is often used to combat the state-space explosion problem and focus the analysis on relevant properties. However, in many such cases, it is difficult to interpret the results of verification on an abstract system with respect to a concrete one. In this paper we present an abstract model-checking approach that guarantees that the True and False answers are sound with respect to the original system.", "num_citations": "16\n", "authors": ["664"]}
{"title": "Automatic analysis of consistency between requirements and designs\n", "abstract": " Writing requirements in a formal notation permits automatic assessment of such properties as ambiguity, consistency, and completeness. However, verifying that the properties expressed in requirements are preserved in other software life cycle artifacts remains difficult.", "num_citations": "16\n", "authors": ["664"]}
{"title": "GrowingLeaf: Supporting Requirements Evolution over Time.\n", "abstract": " Goal modeling and analysis techniques help stakeholders consider possible tradeoff alternatives in their requirements and answer \u201cwhat if\u201d questions about those alternatives. Software projects today exist in an ephemeral state, and current goal modeling approaches do not answer questions about model evolution and changes in actors\u2019 intentionality. We have developed a technique [8] that allows stakeholders to explicate the dynamic nature of their intentional elements, and ask questions about how this dynamicity affects project outcomes. In this paper, we present GrowingLeaf, a new web-based goal modeling and analysis tool that implements this technique for iStar models. We discuss its support for modeling, simulation, and static analysis and illustrate it for a small example of making lunch.", "num_citations": "15\n", "authors": ["664"]}
{"title": "Exploring inconsistencies between modal transition systems\n", "abstract": " It is commonplace to have multiple behaviour models that describe the same system but have been produced by different stakeholders or synthesized from different sources. Although in practice, such models frequently exhibit inconsistencies, there is a lack of tool support for analyzing them. There are two key difficulties in explaining why two behavioural models are inconsistent: (1) explanations often require branching structures rather than linear traces, or scenarios; and (2) there can be multiple sources of inconsistency and many different ways of explaining each one. In this paper, we present an approach that supports exploration of inconsistencies between modal transition systems, an extension to labelled transition systems. We show how to produce sound graphical explanations for inconsistencies, how to compactly represent all possible explanations in a composition of the models being compared\u00a0\u2026", "num_citations": "15\n", "authors": ["664"]}
{"title": "Lightweight reasoning about program correctness\n", "abstract": " Automated verification tools vary widely in the types of properties they are able to analyze, the complexity of their algorithms, and the amount of necessary user involvement. In this paper we propose a framework for step-wise automatic verification and describe a lightweight scalable program analysis tool that combines abstraction and model checking. The tool guarantees that its True and False answers are sound with respect to the original system. We also check the effectiveness of the tool on an implementation of the Safety-Injection System.", "num_citations": "15\n", "authors": ["664"]}
{"title": "Supporting Verification-Driven Incremental Distributed Design of Components.\n", "abstract": " Software systems are usually formed by multiple components which interact with one another. In large systems, components themselves can be complex systems that need to be decomposed into multiple sub-components. Hence, system design must follow a systematic approach, based on a recursive decomposition strategy. This paper proposes a comprehensive verification-driven framework which provides support for designers during development. The framework supports hierarchical decomposition of components into sub-components through formal specification in terms of pre-and post-conditions as well as independent development, reuse and verification of sub-components.", "num_citations": "14\n", "authors": ["664"]}
{"title": "A dataset for dynamic discovery of semantic changes in version controlled software histories\n", "abstract": " Over the last few years, researchers proposed several semantic history slicing approaches that identify the set of semantically-related commits implementing a particular software functionality. However, there is no comprehensive benchmark for evaluating these approaches, making it difficult to assess their capabilities. This paper presents a dataset of 81 semantic change data collected from 8 real-world projects. The dataset is created for benchmarking semantic history slicing techniques. We provide details on the data collection process and the storage format. We also discuss usage and possible extensions of the dataset.", "num_citations": "14\n", "authors": ["664"]}
{"title": "MU-MMINT: an IDE for model uncertainty\n", "abstract": " Developers have to work with ever-present design-time uncertainty, i.e., Uncertainty about selecting among alternative design decisions. However, existing tools do not support working in the presence of uncertainty, forcing developers to either make provisional, premature decisions, or to avoid using the tools altogether until uncertainty is resolved. In this paper, we present a tool, called MU-MMINT, that allows developers to express their uncertainty within software artifacts and perform a variety of model management tasks such as reasoning, transformation and refinement in an interactive environment. In turn, this allows developers to defer the resolution of uncertainty, thus avoiding having to undo provisional decisions. See the companion video: http://youtu.be/kAWUm-iFatM.", "num_citations": "14\n", "authors": ["664"]}
{"title": "Towards compositional synthesis of evolving systems\n", "abstract": " Synthesis of system configurations from a given set of features is an important and very challenging problem. This paper makes a step towards this goal by describing an efficient technique for synthesizing pipeline configurations of feature-based systems. We identify and formalize a design pattern that is commonly used in featurebased development. We show that this pattern enables compositional synthesis of feature arrangements. In particular, the pattern allows us to add or remove features from an existing system without having to reconfigure the system from scratch. We describe an implementation of our technique and evaluate its applicability and effectiveness using a set of telecommunication features from AT&T, arranged within the DFC architecture.", "num_citations": "14\n", "authors": ["664"]}
{"title": "SCR3: towards usability of formal methods\n", "abstract": " This paper gives an overview of SCR3 -- a toolset designed to increase the usability of formal methods for software development. Formal requirements are specified in SCR3 in an easy to use and review format, and then used in checking requirements for correctness and in verifying consistency between annotated code and requirements. In this paper we discuss motivations behind this work, describe several tools which are part of SCR3, and illustrate their operation on an example of a Cruise Control system.", "num_citations": "13\n", "authors": ["664"]}
{"title": "Model transformation product lines\n", "abstract": " Model transformations enable automation in Model-Driven Engineering (MDE) and are key to its success. The emphasis of MDE on using domain-specific languages has caused a proliferation of meta-models, many of them capturing variants of base languages. In this scenario, developing a transformation for a new meta-model is usually performed manually with no reuse, even if comparable transformations for similar meta-models exist. This is a suboptimal process that precludes a wider adoption of MDE in industry.", "num_citations": "11\n", "authors": ["664"]}
{"title": "Property-based methods for collaborative model development\n", "abstract": " Industrial applications of mo del-driven engineering to de- velop large and complex systems resulted in an increasing demand for collab oration features. However, use cases such as mo del di erencing and merging have turned out to b e a di cult challenge, due to (i) the graph- like nature of mo dels, and (ii) the complexity of certain op erations (e.g. hierarchy refactoring) that are common to day. In the pap er, we present a novel search-based automated mo del merge approach where rule-based design space exploration is used to search the space of solution candi- dates that represent con ict-free merged mo dels. Our metho d also allows engineers to easily incorp orate domain-sp eci c knowledge into the merge pro cess to provide b etter solutions. The merge pro cess automatically cal- culates multiple merge candidates to b e presented to domain exp erts for  nal selection. Furthermore, we prop ose to adopt a generic synthetic b enchmark to carry out an initial scalability assessment for mo del merge with large mo dels and large change sets.", "num_citations": "11\n", "authors": ["664"]}
{"title": "Management of time requirements in component-based systems\n", "abstract": " In component-based systems, a number of existing software components are combined in order to achieve business goals. Some of such goals may include system-level (global) timing requirements (GTR). It is essential to refine GTR into a set of component-level (local) timing requirements (LTRs) so that if a set of candidate components collectively meets them, then the corresponding GTR is also satisfied. Existing techniques for computing LTRs produce monolithic representations, that have dependencies over multiple components. Such representations do not allow for effective component selection and repair. In this paper, we propose an approach for building under-approximated LTRs (uLTR) consisting of independent constraints over components. We then show how uLTR can be used to improve the design, monitoring and repair of component-based systems under time requirements. We also report on\u00a0\u2026", "num_citations": "11\n", "authors": ["664"]}
{"title": "FHistorian: Locating features in version histories\n", "abstract": " Feature location techniques aim to locate software artifacts that implement a specific program functionality, aka a feature. In this paper, we build upon the previous work of semantic history slicing to locate features in software version histories. We leverage the information embedded in version histories for identifying changes implementing features and discovering relationships between features.", "num_citations": "10\n", "authors": ["664"]}
{"title": "Modeling and reasoning with changing intentions: an experiment\n", "abstract": " Existing modeling approaches in requirements engineering assume that stakeholder goals are static: once set, they remain the same throughout the lifecycle of the project. Of course, such goals, like anything else, may change over time. In earlier work, we introduced Evolving Intentions: an approach that allows stakeholders to specify how evaluations of goal model elements change over time. Simulation over Evolving Intentions enables stakeholders to ask a variety of 'what if' questions, and evaluate possible evolutions of a goal model. GrowingLeaf is a web-based tool that implements both the modeling and analysis components of this approach. In this paper, we investigate the effectiveness and usability of Evolving Intentions, Simulation over Evolving Intentions, and GrowingLeaf. We report on a between-subjects experiment we conducted with fifteen graduate students familiar with requirements engineering\u00a0\u2026", "num_citations": "10\n", "authors": ["664"]}
{"title": "Supporting agility in MDE through modeling language relaxation\n", "abstract": " Agility requires expressive freedom for the modeler; however, automated MDE processes such as transformations require models to conform to strict constraints (eg well-formed rules). One way out of this apparent conflict is to allow a \u201crelaxed\u201d version of a modeling language to be used by modelers and then use tool support to \u201ctighten\u201d such models so that they are conformant to the original constraints. In this paper, we explore the issues of relaxation and tightening of modeling languages and discuss the possibilities for tool support.", "num_citations": "10\n", "authors": ["664"]}
{"title": "Comparing the effectiveness of reasoning formalisms for partial models\n", "abstract": " Uncertainty is pervasive in Model-based Software Engineering. In previous work, we have proposed partial models as a way to explicate uncertainty during modeling. Using partial models, modelers can perform certain forms of reasoning, like checking properties, without the having to prematurely resolve uncertainty. In this paper, we present a strategy for encoding partial models into different reasoning formalisms and conduct an empirical study aimed to compare the effectiveness of these formalisms for checking properties of partial models.", "num_citations": "10\n", "authors": ["664"]}
{"title": "Formal modeling in a commercial setting: A case study\n", "abstract": " Formal modeling, particularly of software requirements, has long been advocated by the software engineering research community. Usually, such modeling is linked with formal verification and is confined to safety-critical projects where software correctness is the pivotal goal. In contrast, the software industry seeks practical techniques that can be seamlessly integrated into the existing processes and improve productivity; very high quality is often a desirable but not crucial objective. A number of modeling languages have been designed to address this issue, and some have industrial-strength tool support that can be used effectively in commercial settings. This paper describes a case study conducted in collaboration with Nortel Networks to demonstrate the economic feasibility and effectiveness of applying formal modeling techniques to telecommunication systems. A formal specification and description language\u00a0\u2026", "num_citations": "10\n", "authors": ["664"]}
{"title": "Test generation using model checking\n", "abstract": " Testing in the software industry is, in general an ad hoc task. There are guidelines to follow but in most cases do not cover sufficient portions of the software product. Recent work has been done to automatically generate test cases such that designers are no longer responsible for designing the test cases but ensuring that the specification of the software is valid. These formal specifications are then used as inputs into the automatic test generation tools, the results of the tool would be a set of test cases that do a better job at covering the range of tests than current non-automated methodologies. In this paper, we surveay a few such techniques that use model checking technologyas the test generation engine. There are two areas of interest we intend to cover. First is the actual test generation. We discuss a couple techniques that alter the specification to force the model checker to output counter-examples that are then used as test cases for the software application.Second we examine a few methods that attack automated generation from a state space perspective. That is the specifications that are designed for practical industrial products typically contain far too many states for the model checker to verify to completion. These methods attempt to reduce the state space while maintaining sufficient details that test generation is still possible.", "num_citations": "10\n", "authors": ["664"]}
{"title": "Lifting datalog-based analyses to software product lines\n", "abstract": " Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to\u201d lift\u201d particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (eg, pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\u00e9 Datalog engine. We evaluate our implementation on a set of benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.", "num_citations": "9\n", "authors": ["664"]}
{"title": "Tools and Algorithms for the Construction and Analysis of Systems: 22nd International Conference, TACAS 2016, Held as Part of the European Joint Conferences on Theory and\u00a0\u2026\n", "abstract": " This book constitutes the proceedings of the 22nd International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS 2016, which took place in Eindhoven, The Netherlands, in April 2016, held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2016. The 44 full papers presented in this volume were carefully reviewed and selected from 175 submissions. They were organized in topical sections named: abstraction and verification; probabilistic and stochastic systems; synthesis; tool papers; concurrency; tool demos; languages and automata; security; optimization; and competition on software verification\u2013SV-COMP.", "num_citations": "9\n", "authors": ["664"]}
{"title": "Using developer conversations to resolve uncertainty in software development: a position paper\n", "abstract": " Software development is a social process: tasks such as implementing a requirement or fixing a bug typically spark conversations between the stakeholders of a software project, where they identify points of uncertainty in the solution space and explore proposals to resolve them. Due to the fluid nature of these interactions, it is hard for project managers to maintain an overall understanding of the state of the discussion and to know when and how to intervene. We propose an approach for extracting the uncertainty information from developer conversations in order to provide managers with analytics. Using these allows us to recommend specific actions that managers can take to better facilitate the resolution of uncertainty.", "num_citations": "9\n", "authors": ["664"]}
{"title": "Formal modeling in a commercial setting: A case study\n", "abstract": " This paper describes a case study conducted in collaboration with Nortel to demonstrate the feasibility of applying formal modeling techniques to telecommunication systems. A formal description language, SDL, was chosen by our qualitative CASE tool evaluation to model a multimedia-messaging system described by an 80-page natural language specification. Our model was used to identify errors in the software requirements document and to derive test suites, shadowing the existing development process and keeping track of a variety of productivity data.", "num_citations": "9\n", "authors": ["664"]}
{"title": "Bloomingleaf: a formal tool for requirements evolution over time\n", "abstract": " Our previous work presented the Evolving Intentions framework, which specified how evolving qualitative goal models can be modeled and analyzed. Recent improvements to the framework allow for precise semantics of goal relationships with propagation of both evidence for and evidence against a goal's satisfaction (as in Tropos), and enables evaluation of evolution with absolute time (in addition to relative time). The reasoning is expressed as a constraint satisfaction problem. In this paper, we present BloomingLeaf, a new web-based tool that implements the new semantics. We showcase how the implementation and architecture of BloomingLeaf can be used to answer time-based questions.", "num_citations": "8\n", "authors": ["664"]}
{"title": "Correct reuse of transformations is hard to guarantee\n", "abstract": " As model transformations become more complex and more central to software development, reuse mechanisms become more important to enable effective and efficient development of high-quality transformations. A number of transformation-reuse mechanisms have been proposed, but so far there have been no effective attempts at evaluating the quality of reuse that can be achieved by these approaches. In this paper, we build on our earlier work on transformation intents and propose a systematic approach for analyzing the soundness and completeness of a given transformation reuse mechanism with respect to the preservation of transformation intent. We apply this approach to analyze transformation-reuse mechanisms currently proposed in the literature and show that these mechanisms are not sound or complete. We show why providing sound transformation reuse mechanisms is a hard problem\u00a0\u2026", "num_citations": "8\n", "authors": ["664"]}
{"title": "Thorough checking revisited\n", "abstract": " Previous years have seen a proliferation of 3-valued models for capturing abstractions of systems, since these enable verifying both universal and existential properties. Reasoning about such systems is either inexpensive and imprecise (compositional checking), or expensive and precise (thorough checking). In this paper, we prove that thorough and compositional checks for temporal formulas in their disjunctive forms coincide, which leads to an effective procedure for thorough checking of a variety of abstract models and the entire mu-calculus", "num_citations": "8\n", "authors": ["664"]}
{"title": "Towards requirements specification for machine-learned perception based on human performance\n", "abstract": " The application of machine learning (ML) based perception algorithms in safety-critical systems such as autonomous vehicles have raised major safety concerns due to the apparent risks to human lives. Yet assuring the safety of such systems is a challenging task, in a large part because ML components (MLCs) rarely have clearly specified requirements. Instead, they learn their intended tasks from the training data. One of the most well-studied properties that ensure the safety of MLCs is the robustness against small changes in images. But the range of changes considered small has not been systematically defined. In this paper, we propose an approach for specifying and testing requirements for robustness based on human perception. With this approach, the MLCs are required to be robust to changes that fall within the range defined based on human perception performance studies. We demonstrate the\u00a0\u2026", "num_citations": "7\n", "authors": ["664"]}
{"title": "A verification-driven framework for iterative design of controllers\n", "abstract": " Controllers often are large and complex reactive software systems and thus they typically cannot be developed as monolithic products. Instead, they are usually comprised of multiple components that interact to provide the desired functionality. Components themselves can be complex and in turn be decomposed into multiple sub-components. Designing such systems is complicated and must follow systematic approaches, based on recursive decomposition strategies that yield a modular structure. This paper proposes FIDDle\u2013a comprehensive verification-driven framework which provides support for designers during development. FIDDle supports hierarchical decomposition of components into sub-components through formal specification in terms of pre- and post-conditions as well as independent development, reuse and verification of sub-components. The framework allows the development of an initial\u00a0\u2026", "num_citations": "7\n", "authors": ["664"]}
{"title": "Analysing meta-model product lines\n", "abstract": " Model-driven engineering advocates the use of models to describe and automate many software development tasks. The syntax of modelling languages is defined by meta-models, making them essential artefacts. A combination of product line engineering methods and meta-models has been proposed to enable specification of modelling language variants, eg, to describe a range of systems. However, there is a lack of techniques for ensuring syntactic correctness of all meta-models within a family (including their OCL constraints), and semantic correctness related to properties of individual instances of the different variants. The absence of verification methods at the product-line level can cause synthesis of ill-formed meta-models and problematic feature combinations whose effect at the instance level may go unnoticed.", "num_citations": "7\n", "authors": ["664"]}
{"title": "FPH: efficient non-commutativity analysis of feature-based systems\n", "abstract": " Feature-oriented software development (FOSD) is a promising approach for developing a collection of similar software products from a shared set of software assets. A well-recognized issue in FOSD is the analysis of feature interactions: cases where the integration of multiple features would alter the behavior of one or several of them. Existing approaches to feature interaction detection require a fixed order in which the features are to be composed but do not provide guidance as to how to define this order or how to determine a relative order of a newlydeveloped feature wrt existing ones. In this paper, we argue that classic feature non-commutativity analysis, ie, determining when an order of composition of features affects properties of interest, can be used to complement feature interaction detection to help build orders between features and determine many interactions. To this end, we develop and evaluate Mr. Feature Potato Head (FPH)\u2013a modular approach to noncommutativity analysis that does not rely on temporal properties and applies to systems expressed in Java. Our experiments running FPH on 29 examples show its efficiency and effectiveness.", "num_citations": "7\n", "authors": ["664"]}
{"title": "Partial behavioural models for requirements and early design\n", "abstract": " The talk will discuss the problem of creation, management, and specifically merging of partial behavioural models, expressed as model transition systems. We argue why this formalism is essential in the early stages of the software cycle and then discuss why and how to merge information coming from different sources using this formalism. The talk is based on papers presented in FSE'04 and FME'06 and will also include emerging results on synthesizing partial behavioural models from temporal properties and scenarios.", "num_citations": "7\n", "authors": ["664"]}
{"title": "CTL model-checking over logics with non-classical negations\n", "abstract": " In earlier work [9], we defined CTL model-checking over finite-valued logics with De Morgan negation. In this paper, we extend this work to logics with intuitionistic, Galois and minimal negations, calling the resulting language /spl chi/CTL. We define /spl chi/CTL operators and show that they can be computed using fixpoints. We further discuss how to extend our existing multi-valued model-checker /spl chi/Chek [8] to reasoning over these logics.", "num_citations": "7\n", "authors": ["664"]}
{"title": "Property satisfiability analysis for product lines of modelling languages\n", "abstract": " Software engineering uses models throughout most phases of the development process. Models are defined using modelling languages. To make these languages applicable to a wider set of scenarios and customizable to specific needs, researchers have proposed using product lines to specify modelling language variants. However, there is currently a lack of efficient techniques for ensuring correctness with respect to properties of the models accepted by a set of language variants. This may prevent detecting problematic combinations of language variants that produce undesired effects at the model level. To attack this problem, we first present a classification of instantiability properties for language product lines. Then, we propose a novel approach to lifting the satisfiability checking of model properties of individual language variants, to the product line level. Finally, we report on an implementation of our proposal\u00a0\u2026", "num_citations": "6\n", "authors": ["664"]}
{"title": "Serious games for NP-hard problems: challenges and insights\n", "abstract": " This paper describes the on-going work of developing a serious game (aka\" game with a purpose\") to solve the NP-hard problem of n-way merging. We outline the challenges that were encountered while designing the game, steps that we took to overcome these challenges and results of the preliminary evaluation of our current game design. We hope our experience will be useful for those developing serious games to solve other computationally expensive problems.", "num_citations": "6\n", "authors": ["664"]}
{"title": "Integrating crowd intelligence into software\n", "abstract": " The knowledge resources available on the Internet are increasingly being used to support software both at development time and at execution time. These take the form of conventional services as well as human knowledge work both through crowd sourcing and information stored directly on the World Wide Web. But while these resources are vast and rich, they are also unreliable. In this paper, we propose a novel software development pattern called Contributional Implementation (CI) inspired by the way humans mitigate this unreliability: sources are treated as opinion providers with varying amounts of trust and aggregating multiple opinions from different sources helps improve the quality of the answers. We sketch some detailed examples of how a CI could be coded, discuss issues related to the realization of CI's in practice and outline plans for an evaluation of the approach.", "num_citations": "6\n", "authors": ["664"]}
{"title": "Using Model Checking to Analyze Requirements and Designs\n", "abstract": " Precise notations have been developed to specify unambiguous requirements and ensure that all cases of appropriate system behavior are considered and documented. Using one such notation, we have developed techniques to analyze software artifacts automatically at early stages of the software development life cycle. We use model checking as our verification technique because it can be fully automated and can check properties of large systems. This paper describes model checking and summarizes our efforts to use it to analyze software requirements and designs. We prove that requirements model system safety properties and that designs model consistency properties derived from requirements by creating abstractions of these software artifacts and using model checking to determine if the abstractions are models of the properties. We present results from a case study in which we analyzed the\u00a0\u2026", "num_citations": "6\n", "authors": ["664"]}
{"title": "Automatic and efficient variability-aware lifting of functional programs\n", "abstract": " A software analysis is a computer program that takes some representation of a software product as input and produces some useful information about that product as output. A software product line encompasses many software product variants, and thus existing analyses can be applied to each of the product variations individually, but not to the entire product line as a whole. Enumerating all product variants and analyzing them one by one is usually intractable due to the combinatorial explosion of the number of product variants with respect to product line features. Several software analyses (e.g., type checkers, model checkers, data flow analyses) have been redesigned/re-implemented to support variability. This usually requires a lot of time and effort, and the variability-aware version of the analysis might have new errors/bugs that do not exist in the original one.  Given an analysis program written in a functional\u00a0\u2026", "num_citations": "5\n", "authors": ["664"]}
{"title": "Just enough formality in assurance argument structures\n", "abstract": " Safety assurance cases (ACs) are structured arguments that assert the safety of cyber-physical systems. ACs use reasoning steps, or strategies, to show how a safety claim is decomposed into subclaims which are then supported by evidence. In practice, ACs are informal, and thus it is difficult to check whether these decompositions are valid and no subclaims are missed. This may lead to the approval of fallacious safety arguments and thus the deployment of unsafe systems. Fully formalizing ACs to facilitate rigorous evaluation is not realistic due to the complexity of creating and comprehending such ACs. We take an intermediate approach by formalizing several types of decomposition strategies, proving the conditions under which they are deductive, and applying them as templates that guard against common errors in ACs. We demonstrate our approach on two scenarios: creation of ACs with deductive reasoning\u00a0\u2026", "num_citations": "5\n", "authors": ["664"]}
{"title": "Transformation Reuse: What is the Intent?\n", "abstract": " The ability to reuse transformations across a range of related metamodels is highly desired for many model-driven approaches. For example, it would be useful to be able to reuse standard transformations like state-machine flattening across models instantiating different metamodels of hierarchical state-machines, instead of having to reimplement the same fundamental algorithm just because of small syntactic (or semantic) variations. Typing typically captures the question of identifying models for which a given transformation can be successfully applied. Type compatibility between meta-models (or the related notion of sub-typing), is intended to ensure that a transformation defined over one type can be successfully executed over any model instantiating a compatible meta-model. However, compatibility mechanisms have not explicitly addressed the question of what it makes for a transformation to be successfully applied. In this paper, we argue that answering this question is central to a meaningful notion of transformation reuse and must take into account the intent of a transformation and seek to preserve it in reuse. We describe how current definitions of type compatibility fail to satisfy this criterion and propose a research agenda for addressing it.", "num_citations": "5\n", "authors": ["664"]}
{"title": "Towards a catalog of non-functional requirements for model transformations\n", "abstract": " Model transformations play an increasingly important role in Model-Driven Engineering (MDE), and thus understanding desired non-functional requirements of model transformations and being able to determine how existing transformation languages stack up wrt these is also of interest. This paper is a first step towards producing a catalog that systematically captures the transformation community\u2019s experience in developing transformations wrt non-functional requirements. We survey the literature to provide a list of non-functional requirements of model transformations and find comparisons between three popular model transformation languages (ATL, QVT-Relations and AGG) according to these requirements.", "num_citations": "5\n", "authors": ["664"]}
{"title": "Production cell revisited\n", "abstract": " This paper presents an analysis of the Production Cell system. We were able to model the system and verify most of its properties in Promela/SPIN. Our model is very close to the implementation level, and deriving code from it is trivial. In order to verify properties with SPIN's partial order reduction algorithms, we needed to ensure that all of our properties are closed under stuttering. We introduce the notion of logic edges and use them to show that properties of interest to us are closed under stuttering. 1 Introduction In recent years model-checking has become a verification tool of choice for many projects. However, limitations of model-checking are well known-systems have to be finite, and abstractions have to be used to combat the state-space explosion. Although abstractions are essential in reasoning about complicated algorithms [6], they are not always natural or even feasible. Consider the following scenario: a customer specifies the environment at a certain level...", "num_citations": "5\n", "authors": ["664"]}
{"title": "Verification of consistency between concurrent program designs and their requirements\n", "abstract": " Writing requirements in a formal notation allows the automatic assessment of such properties as ambiguity, consistency and completeness. However, verifying that the properties expressed in the requirements are preserved in an implementation remains difficult. In our earlier work (1995), we described a technique for analyzing the consistency of detailed program designs with their requirements. To ensure that our methods scale up to realistic systems, we need to develop compositional approaches. This paper describes a first step in this direction: a technique for analyzing concurrent program designs. We present a language for specifying detailed designs of concurrent programs and an analysis tool, called Analyzer, which uses this language to build a finite-state abstraction of the design. This abstraction is compared with properties derived from the set of requirements to determine if the former is consistent with\u00a0\u2026", "num_citations": "5\n", "authors": ["664"]}
{"title": "Variability-aware datalog\n", "abstract": " Variability-aware computing is the efficient application of programs to different sets of inputs that exhibit some variability. One example is program analyses applied to Software Product Lines (SPLs). In this paper we present the design and development of a variability-aware version of the Souffl\u00e9 Datalog engine. The engine can take facts annotated with Presence Conditions (PCs) as input, and compute the PCs of its inferred facts, eliminating facts that do not exist in any valid configuration. We evaluate our variability-aware Souffl\u00e9 implementation on several fact sets annotated with PCs to measure the associated overhead in terms of processing time and database size.", "num_citations": "4\n", "authors": ["664"]}
{"title": "Transformation of software product lines\n", "abstract": " We present our paper from the proceedings of the 2017 edition of the MODELS conference. Software product lines are used to manage the development of highly complex software with many variants. In the literature, various forms of rule-based product line modifications have been considered. However, when considered in isolation, their expressiveness for specifying combined modifications of feature models and domain models is limited. In this paper, we present a formal framework for product line transformations that is able to combine several kinds of product line modifications presented in the literature. Moreover, it defines new forms of product line modifications supporting various forms of product lines and transformation rules. Our formalization of product line transformations is based on category theory, and concentrates on properties of product line relations instead of their single elements. Our framework provides improved expressiveness and flexibility of software product line transformations while abstracting from the considered type of model.", "num_citations": "4\n", "authors": ["664"]}
{"title": "Using the crowds to satisfy unbounded requirements\n", "abstract": " The Internet is a social space that is shaped by humans through the development of websites, the release of web services, the collaborative creation of encyclopedias and forums, the exchange of information through social networks, the provision of work through crowdsourcing platforms, etc. This landscape offers novel possibilities for software systems to satisfy their requirements, e.g., by retrieving and aggregating the information from Internet websites as well as by crowdsourcing the execution of certain functions. In this paper, we present a special type of functional requirements (called unbounded) that is not fully satisfiable and whose satisfaction is increased by gathering evidence from multiple sources. In addition to charac- terizing unbounded requirements, we explain how to maximize their satisfaction by asking and by combining opinions of mul- tiple sources: people, services, information, and algorithms. We\u00a0\u2026", "num_citations": "4\n", "authors": ["664"]}
{"title": "Edge-Shifted Decision Diagrams for Multiple-Valued Logic\n", "abstract": " Symbolic data structures for multi-valued logics are useful in a number of applications, from model-checking to circuit design and switch-level circuit verification. Such data structures are referred to as decision diagrams, and are typically considered effective if they are small, ie, common co-factors of a function are shared, and canonical, ie, given a variable ordering, there is a unique decision diagram representing this function. In this paper we propose and evaluate a decision diagram variety, called Edge-Shifted Decision Diagrams (ESDDs). These diagrams are multi-valued, support structure sharing, and enjoy canonicity. We further list sufficient conditions to ensure canonicity of decision diagrams with unary edge transformers.", "num_citations": "4\n", "authors": ["664"]}
{"title": "Bisimulation analysis of SDL-expressed protocols: a case study.\n", "abstract": " Faster, better networks algorithms are often being discovered, and it is desirable to be able to replace an old algorithm by a new in a manner that is completely transparent to the application using it. This paper investigates the technique for ensuring such transparency for protocols expressed in SDL, via bisimulation checking. We discuss the main issues involved in translating SDL into Concurrency Workbench, a tool for performing bisimulation checking, and illustrate the feasibility of the technique by comparing the SDL specification of the Go-Back protocol with the family of new protocols, called Asynchronous Retransmission Go-Back-", "num_citations": "4\n", "authors": ["664"]}
{"title": "MMINT-A 2.0: tool support for the lifecycle of model-based safety artifacts\n", "abstract": " In recent years, the complexity of safety-critical systems such as automotive systems has been rapidly increasing. The need to address safety concerns in such systems led to the development of industry-specific safety standards. The standards mandate activities that generate model-based safety artifacts (eg, safety cases and fault trees). Given the importance of these safety models, tool support is needed to facilitate manipulating them throughout their lifecycle while maintaining their connection to system models.", "num_citations": "3\n", "authors": ["664"]}
{"title": "Uncertain requirements, assurance and machine learning\n", "abstract": " From financial services platforms to social networks to vehicle control, software has come to mediate many activities of daily life. Governing bodies and standards organizations have responded to this trend by creating regulations and standards to address issues such as safety, security and privacy. In this environment, the compliance of software development to standards and regulations has emerged as a key requirement. Compliance claims and arguments are often captured in assurance cases, with linked evidence of compliance. Evidence can come from testcases, verification proofs, human judgement, or a combination of these. That is, we try to build (safety-critical) systems carefully according to well justified methods and articulate these justifications in an assurance case that is ultimately judged by a human. Yet software is deeply rooted in uncertainty making pragmatic assurance more inductive than\u00a0\u2026", "num_citations": "3\n", "authors": ["664"]}
{"title": "Debugging Models in the Context of Automotive Software Development.\n", "abstract": " Different models are involved in the automotive development process. In the integration phase, AUTOSAR is often the only model description available for a controller. The models that were used to generate the behavioral code (eg, SIMULINK\u00ae) and sometimes even the source code itself are often not available due to IP protection. The controller software is verified using simulation-based testing, which may involve different kinds of environment models and legacy components. When a test fails, developers need to find the cause of the error. Even if the source code is available, source code debugging can be difficult, because the code has often been generated and thus may be confusing. Developers then use signal plotting for known variables, but plots can be ambiguous and lead to false conclusions. Therefore, exploiting the structural and semantic information of the involved models for debugging can be a valuable addition. However, the methods and tooling available are rather limited. Most of the existing approaches only consider live debugging. The intended PhD thesis aims at developing a methodology and tooling for debugging that makes better use of the models and the simulation data. It includes the application of techniques like slicing and the use of model hierarchies. A case study is planned in an industry context.", "num_citations": "3\n", "authors": ["664"]}
{"title": "Trace reduction and pattern analysis to assist debugging in model-based testing\n", "abstract": " Model-based testing (MBT) is a technique for generating test cases from test models. One of the benefits of MBT is the ability to have a computer generate and execute extensive test sets from the test models, achieving high coverage. However, when such large test sets are automatically generated and executed, the resulting failure traces can be very large and difficult to debug for root cause analysis. In this paper, we present a technique for minimizing the length of a failure trace, creating variants of it, and for pattern mining the trace variants to assist in root cause analysis. We demonstrate the technique on a model of a GSM SIM card.", "num_citations": "3\n", "authors": ["664"]}
{"title": "A comparison of three black-box optimization approaches for model-based testing\n", "abstract": " Model-based testing is a technique for generating test cases from a test model. Various notations and techniques have been used to express the test model and generate test cases from those models. Many use customized modelling languages and in-depth white-box static analysis for test generation. This allows for optimizing generated tests to specific paths in the model. Others use general-purpose programming languages and light-weight black-box dynamic analysis. While this light-weight approach allows for quick prototyping and easier integration with existing tools and user skills, optimizing the resulting test suite becomes more challenging since less information about the possible paths is available. In this paper, we present and compare three approaches to such black-box optimization.", "num_citations": "3\n", "authors": ["664"]}
{"title": "Fundamental Approaches to Software Engineering\n", "abstract": " Fundamental Approaches to Software Engineering - NASA/ADS Now on home page ads icon ads Enable full ADS view NASA/ADS Fundamental Approaches to Software Engineering Chechik, Marsha ; Wirsing, Martin Abstract Publication: Lecture Notes in Computer Science Pub Date: 2009 DOI: 10.1007/978-3-642-00593-0 Bibcode: 2009LNCS......C Keywords: Computer Science; Software Engineering; Logics and Meanings of Programs; Programming Languages; Compilers; Interpreters; Programming Techniques; Computation by Abstract Devices full text sources Publisher | \u00a9 The SAO/NASA Astrophysics Data System adshelp[at]cfa.harvard.edu The ADS is operated by the Smithsonian Astrophysical Observatory under NASA Cooperative Agreement NNX16AC86A NASA logo Smithsonian logo Resources About ADS ADS Help What's New Careers@ADS Social @adsabs ADS Blog Project Switch to full ADS Is \u2026", "num_citations": "3\n", "authors": ["664"]}
{"title": "Behavioural model fusion: an overview of challenges\n", "abstract": " In large-scale model-based development, developers periodically need to combine collections of interrelated models. These models may capture different features of a system, describe alternative perspectives on a single feature, or express ways in which different features may alter one another's structure or behaviour. We refer to the process of combining a set of interrelated models as model fusion. In this position paper, we provide an overview of our work on two key fusion activities, merging and composition, for behavioural models. The practical basis of our work comes from two case studies that we conducted using models from the telecommunications domain. We illustrate our work using these case studies, summarize the results our research has led to so far, and describe the future research challenges.", "num_citations": "3\n", "authors": ["664"]}
{"title": "Behavioural Model Fusion: Experiences from Two Telecommunication Case Studies\n", "abstract": " In large-scale model-based development, developers periodically need to combine collections of interrelated models. These models may capture different features of a system, describe alternative perspectives on a single feature, or express ways in which different features may alter one another\u2019s structure or behaviour. We refer to the process of combining a set of interrelated models as model fusion. In this position paper, we provide an overview of our work on two key fusion activities, merging and composition, for behavioural models. The practical basis of our work comes from two case studies that we conducted using models from the telecommunications domain. We illustrate our work using these case studies, summarize the results our research has led to so far, and describe the future research challenges.", "num_citations": "3\n", "authors": ["664"]}
{"title": "Extending extended vacuity\n", "abstract": " There has been a growing interest in detecting whether a logic specification holds in the system vacuously. For example, a specification\" every request is eventually followed by an acknowledgment\" holds vacuously on those systems that never generate requests. In a recent paper, Armoni et al. have argued against previous definitions of vacuity, defined as sensitivity with respect to syntactic perturbation. They suggested that vacuity should be robust, ie, insensitive to trivial changes in the logic and in the model, and is better described as sensitivity with respect to semantic perturbation, represented by universal propositional quantification. In this paper, we extend the above results by giving a formal definition of robust vacuity that allows us to define and detect vacuous satisfaction and vacuous failure for arbitrary CTL* properties, even with respect to multiple occurrences of subformulas. We discuss complexity of our\u00a0\u2026", "num_citations": "3\n", "authors": ["664"]}
{"title": "A framework for multi-valued reasoning over inconsistent viewpoints\n", "abstract": " In requirements elicitation, different stakeholders often hold different views of how a proposed system should behave, resulting in inconsistencies between their descriptions. Consensus may not be needed for every detail, but it can be hard to determine whether a particular disagreement affects the critical properties of the system. In this paper, we describe the Xbel framework for merging and reasoning about multiple, inconsistent state machine models. Xbel permits the analyst to choose how to combine information from the multiple viewpoints, where each viewpoint is described using an underlying multi-valued logic. \u0418\u0433\u0435 different values of our logics typically represent different levels of agreement. Our multi-valued model checker, Xchek, allows us to check the merged model against properties expressed in a temporal logic. The resulting framework can be used as an exploration tool to support requirements\u00a0\u2026", "num_citations": "3\n", "authors": ["664"]}
{"title": "Formal Methods When Money is Tight\n", "abstract": " Formal methods have been shown to improve the quality of software, but they are seldom if ever used outside the safety-critical system domain. Typically, the initial cost of creating formal requirements speci cations is perceived to be prohibitively large while not necessarily guaranteeing future bene ts. In this paper we discuss ways of tailoring formal methods to suit current economic realities.", "num_citations": "3\n", "authors": ["664"]}
{"title": "Automatic Analysis of Consistency between Implementations and Requirements\n", "abstract": " Formal methods like model checking can be used to demonstrate that safety properties of embedded systems are enforced by the system's requirements.  Unfortunately, proving these properties provides no guarantee that they will be preserved in an  implementation of the system.  We have developed a tool, called Analyzer, which helps discover instances of inconsistency and incompleteness in implementations with respect to requirements.    Analyzer uses requirements information to automatically generate properties which ensure that required state transitions appear in  a model of an implementation.  A model is created through abstract  interpretation of an implementation annotated with assertions about values of state variables which appear in requirements.  Analyzer  determines if the model satisfies both automatically-generated and  user-specified safety properties.    This paper presents a description of our implementation of Analyzer  and our experience in applying it to a small but realistic problem. (Also cross-referenced as UMIACS-TR-94-137)", "num_citations": "3\n", "authors": ["664"]}
{"title": "Scaling client-specific equivalence checking via impact boundary search\n", "abstract": " Client-specific equivalence checking (CSEC) is a technique proposed previously to perform impact analysis of changes to downstream components (libraries) from the perspective of an unchanged system (client). Existing analysis techniques, whether general (re-gression verification, equivalence checking) or special-purpose, when applied to CSEC, either require users to provide specifications, or do not scale. We propose a novel solution to the CSEC problem, called 2clever, that is based on searching the control-flow of a program for impact boundaries. We evaluate a prototype implementation of 2clever on a comprehensive set of benchmarks and conclude that our prototype performs well compared to the state-of-the-art.", "num_citations": "2\n", "authors": ["664"]}
{"title": "GenSlice: Generalized Semantic History Slicing\n", "abstract": " Semantic history slicing addresses the problem of identifying changes related to a particular high-level functionality from the software change histories. Existing solutions are either imprecise, resulting in larger-than-necessary history slices, or inefficient, taking a long time to execute. In this paper, we develop a generalized history slicing framework, named GenSlice, which overcomes the aforementioned limitations. GenSlice abstracts existing history slicing techniques and change history management operations (such as splitting commits into fine-grained changes) as history transformation operators, making it possible to apply them sequentially in various orders. We study and prove properties of various orders of operators and devise a systematic approach for efficiently producing history slices that are optimal for practical purposes. We report on an empirical evaluation of our framework, demonstrating its\u00a0\u2026", "num_citations": "2\n", "authors": ["664"]}
{"title": "Reconstructing the past: the case of the Spadina Expressway\n", "abstract": " In order to build resilient systems that can be operational for a long time, it is important that analysts are able to model the evolution of the requirements of that system. The Evolving Intentions framework models how stakeholders\u2019 goals change over time. In this work, our aim is to validate applicability and effectiveness of this technique on a substantial case. In the absence of ground truth about future evolutions, we used historical data and rational reconstruction to understand how a project evolved in the past. Seeking a well-documented project with varying stakeholder intentions over a substantial period of time, we selected requirements of the Toronto Spadina Expressway. In this paper, we report on the experience and the results of modeling this project over different time periods, which enabled us to assess the modeling and reasoning capabilities of the approach, its support for asking and answering \u2018what if\u00a0\u2026", "num_citations": "2\n", "authors": ["664"]}
{"title": "CSlicerCloud: a web-based semantic history slicing framework\n", "abstract": " Traditional commit-based sequential organization of software version histories is insufficient for many development tasks which require high-level, semantic understanding of program functionality, such as porting features or cutting new releases. Semantic history slicing is a technique which uses well-organized unit tests as identifiers for corresponding software functionalities and extracts a set of commits that correspond to a specific high-level functionality. In this paper, we present CSlicerCloud, a Web-based semantic history slicing service tailored for Java projects hosted on GitHub. It is accessible through Web browsers and powered in the backend by a collection of history slicing techniques underneath. We evaluated CSlicerCloud on a dataset containing developer-annotated change histories collected from 10 open source software projects. A video demonstration which showcases the main features of\u00a0\u2026", "num_citations": "2\n", "authors": ["664"]}
{"title": "The impact of visual load on performance in a human-computation game\n", "abstract": " It is well-known that tasks imposing high cognitive load, ie, the mental effort required to carry out a task, place a strain on people's ability to perform. In light of this, the present study investigates whether poor performance also occurs in human-computation games. That is, do players perform better in game designs that increase the visual information presented? These designs have the advantage of exposing players to more of the solution space, but may come with the caveat of imposing a higher cognitive load. We present a case study by considering alternative layouts differing in the amount of visual information given to players in a human-computation game. The findings of the study seem to support the idea that presenting more information is beneficial to players. This is surprising result that challenges prevailing beliefs about cognitive load, and invites more detailed, future investigation.", "num_citations": "2\n", "authors": ["664"]}
{"title": "Security Benchmarking using Partial Verification.\n", "abstract": " Implementation-level vulnerabilities are a persistent threat to the security of computing systems. We propose using the results of partially-successful verification attempts to place a numerical upper bound on the insecurity of systems, in order to motivate improvement.", "num_citations": "2\n", "authors": ["664"]}
{"title": "Applying formal methods to a telecommunications system in a commercial setting\n", "abstract": " Formal methods have long been advocated by the software engineering research community; however, most successful applications of formal methods are con ned to safety-critical projects where software correctness is the pivotal goal. In contrast, the software industry seeks practical techniques that can be seamlessly integrated into their existing processes and improve productivity; absolute quality is often a desirable but not crucial objective. A number of formal methods have been designed to address this issue, and some have industrial-strength implementations that can be used e ectively in commercial settings.This paper describes a case study conducted in collaboration with Nortel to demonstrate the feasibility of applying formal methods to telecommunications systems. A lightweight formal method, SDL, was chosen by our qualitative CASE tool evaluation to model a multimedia-messaging system described by an 80-page natural language speci cation. Our model was used to identify errors in the software requirements document and to derive test suites, shadowing the existing development process and keeping track of a variety of productivity data. Creating a model which was easy to review by Nortel engineers, allowed us to locate speci cation errors that were missed by several manual inspections. In addition, the model was used to derive a variety of test cases that doubled the number of errors discovered by Nortel testers. The success of our case study was in nding a suitable project where we were able to integrate a well-chosen formal method into the existing development process.", "num_citations": "2\n", "authors": ["664"]}
{"title": "A lean approach to building valid model-based safety arguments\n", "abstract": " In recent decades, cyber-physical systems developed using Model-Driven Engineering (MDE) techniques have become ubiquitous in safety-critical domains. Safety assurance cases (ACs) are structured arguments designed to comprehensively show that such systems are safe; however, the reasoning steps, or strategies, used in AC arguments are often informal and difficult to rigorously evaluate. Consequently, AC arguments are prone to fallacies, and unsafe systems have been deployed as a result of fallacious ACs. To mitigate this problem, prior work [32] created a set of provably valid AC strategy templates to guide developers in building rigorous ACs. Yet instantiations of these templates remain error-prone and still need to be reviewed manually. In this paper, we report on using the interactive theorem prover Lean to bridge the gap between safety arguments and rigorous model-based reasoning. We generate\u00a0\u2026", "num_citations": "1\n", "authors": ["664"]}
{"title": "Applying Declarative Analysis to Software Product Line Models: An Industrial Study\n", "abstract": " Software Product Lines (SPLs) are families of related software products developed from a common set of artifacts. Most existing analysis tools can be applied to a single product at a time, but not to an entire SPL. Some tools have been redesigned/re-implemented to support the kind of variability exhibited in SPLs, but this usually takes a lot of effort, and is error-prone. Declarative analyses written in languages like Datalog have been collectively lifted to SPLs in prior work [1], which makes the process of applying an existing declarative analysis to a product line more straightforward. In this paper, we take an existing declarative analysis (behaviour alteration) and apply it to a set of automotive software product lines from General Motors. We discuss the design of the analysis pipeline used in this process, present its scalability results, and provide a means to visualize the analysis results for a subset of products filtered by\u00a0\u2026", "num_citations": "1\n", "authors": ["664"]}
{"title": "Software Product Line Analysis Using Variability-aware Datalog\n", "abstract": " Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersectionof Product Line Engineering and software analysis. Different attempts have been made to \"lift\" particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\u2000 Datalog engine. We evaluate our implementation on a set of Java and C-language benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.", "num_citations": "1\n", "authors": ["664"]}
{"title": "Exploring Next States and Alternative Paths in Goal Model Analysis\n", "abstract": " GORE is concerned with the use of goals in various requirement engineering activities such as eliciting, elaborating, structuring, specifying, analyzing, negotiating, documenting, and modifying requirements and it has been advocated to help stakeholders make trade-off decisions in the early stages of project development by analyzing models that contain intentions (goals), system requirements and constraints in projects [1, 2, 3]. BloomingLeaf is a web-based goal modeling tool with automated formal analysis [4]. BloomingLeaf allows stakeholders to model changes in projects by assigning evolving functions to intentions in the model. When stakeholders specify questions about the project\u2019s evolution over time, the current analysis of the tool generates a single evolution path: a collection of simulated status of a predefined number of time points of the model [5]. However, one random possible evolution path might not be sufficient to understand the domain, and stakeholders may want to guide the generation of the path when one state is more favoured than another state. To better represent the users\u2019 problems and let them explore more possible solutions, we extend our analysis to allow users to customize their solution path by adding the functionality that supports stepping into any time point and exploring the solution space. This new functionality allows them to generate new solutions that better fit their projects.", "num_citations": "1\n", "authors": ["664"]}
{"title": "FPH: efficient detection of feature interactions through non-commutativity\n", "abstract": " Summary form only given. Feature-oriented software development (FOSD) has recently emerged as a promising approach for developing a collection of similar software products from a shared set of software assets. A well-recognized issue in FOSD is the analysis of feature interactions: cases where the integration of multiple features would alter the behavior of one or several of them. Existing approaches to detecting feature interactions require specification of correctness of individual features and operate on the entire family of software products. In this poster, we develop and evaluate a highly scalable and modular approach, called Mr. Feature Potato Head (FPH), to detect interactions stemming from non-commutativity of features, i.e., cases where behavior of products changes depending on the order in which features have been composed. We instantiate FPH for systems expressed in Java and evaluate its\u00a0\u2026", "num_citations": "1\n", "authors": ["664"]}
{"title": "Observational refinement and merge for disjunctive MTSs\n", "abstract": " Modal Transition System (MTS) is a well studied formalism for partial model specification. It allows a modeller to distinguish between required, prohibited and possible transitions. Disjunctive MTS (DMTS) is an extension of MTS that has been getting attention in recent years. A key concept for (D)MTS is refinement, supporting a development process where abstract specifications are gradually refined into more concrete ones. Refinement comes in different flavours: strong, observational (where -labelled transitions are taken into account), and alphabet (allowing the comparison of models defined on different alphabets). Another important operation on (D)MTS is that of merge: given two models M and N, their merge is a model P which refines both M and N, and which is the least refined one.                 In this paper, we fill several missing parts in the theory of DMTS refinement and merge. First and foremost, we\u00a0\u2026", "num_citations": "1\n", "authors": ["664"]}
{"title": "Modeling and reasoning about software systems containing uncertainty and variability\n", "abstract": " Summary form only given. When building large software-intensive systems, engineers need to express and reason about at least two different types of choices. One type concerns uncertainty - choosing between different design alternatives, resolving inconsistencies, or resolving conflicting stakeholder requirements. Another type deals with variability - supporting different variants of software that serve multiple customers or market segments. Partial modeling has been proposed as a technique for managing uncertainty within a software model. A partial model explicates points of uncertainty and represents the set of possible models that could be obtained by making decisions and resolving the uncertainty. Methods for reasoning about the entire set of possibilities, transforming the entire set and uncertainty-reducing refinements have recently been developed. Software product line engineering approaches propose\u00a0\u2026", "num_citations": "1\n", "authors": ["664"]}
{"title": "A process for model transformation testing\n", "abstract": " Background\u25aa A process initially defined while I was visiting at UofT, using their Class diagram to ER diagram as a case study in Eclipse ATL transformation environment\u25aa After returning to Finland, extended with a more realistic case study of EAST-ADL model transformations in MetaEdit+", "num_citations": "1\n", "authors": ["664"]}
{"title": "A Variability-Based Approach to Reusable and Efficient Model Transformation-Technical Report\n", "abstract": " Large model transformation systems often contain transformation rules that are substantially similar to each other, causing performance bottlenecks for systems in which rules are applied nondeterministically, as long as one of them is applicable. We tackle this problem by introducing variability-based graph transformations. We formally define variability-based rules and contribute a novel match-finding algorithm for applying them. We prove correctness of our approach by showing its equivalence to the classic one of applying the rules individually, and demonstrate the achieved performance speed-up on a realistic transformation scenario.", "num_citations": "1\n", "authors": ["664"]}
{"title": "Verification of Uncertainty Reducing Model Transformations\n", "abstract": " Models are typically used for expressing information that is known at a particular stage in the software development process. Yet, it is also important to express what information a modeler is still uncertain about. Furthermore, when a transformation is applied to a model containing uncertainty, it is natural to consider the effect that the transformation has on the level of uncertainty, eg, whether it always reduces it. In previous work, we have presented a general approach for precisely expressing uncertainty within models and defined formal conditions for uncertainty reduction between models. In this paper, we use these foundations to develop an automated method for verifying that a partial model transformation is uncertainty reducing.", "num_citations": "1\n", "authors": ["664"]}
{"title": "A relationship-based approach to model management\n", "abstract": " There is a rapidly growing interest in model-based development as a way to increase the level of abstraction and automation in software engineering. The ultimate goal of model-based development is to improve the software process by promoting the use of models as the primary artifacts of development, and to provide computer-supported technologies to transform models into running systems.", "num_citations": "1\n", "authors": ["664"]}
{"title": "Automated support for building behavioral models of event-driven systems\n", "abstract": " Programmers understand a piece of software by building simplified mental models of it. Aspects of these models lend themselves naturally to formalization \u2013 e.g., structural relationships can be partly captured by module dependency graphs. Automated support for generating and analyzing such structural models has proven useful. For event-driven systems, behavioral models, which capture temporal and causal relationships between events, are important and deserve similar methodological and tool support. In this paper, we describe such a technique. Our method supports building and elaboration of behavioral models, as well as maintaining such models as systems evolve. The method is based on model-checking and witness generation, using strategies to create goal-driven simulation traces. We illustrate it on a two-lift/three-floor elevator system, and describe our tool, Sawblade, which provides\u00a0\u2026", "num_citations": "1\n", "authors": ["664"]}
{"title": "AE introduction\n", "abstract": " Editorial: AE introduction Page 1 Thomas Ball receiving the PhD degree in computer science in 1993 from the University of Wisconsin\u2014 Madison. He is a senior researcher at Microsoft Research (MSR) where he leads the Testing, Verification, and Measurement group. His research interests are in how combinations of static and dynamic program analysis, model checking, and theorem proving techniques can help improve the correctness and reliability of programs. For the last four years, he has been working on the SLAM project with Sriram Rajamani, the main product of which is an analysis engine for checking temporal safety properties of C programs. This engine forms the core of a new tool called Static Driver Verifier, in development in the Windows division, for checking that Windows device drivers are good clients of the Windows kernel API. Previous to working at MSR, he was a researcher at Bell Labs (1993-\u2026", "num_citations": "1\n", "authors": ["664"]}
{"title": "Using SCR Requirements\n", "abstract": " Software Cost Reduction project originated over 20 years ago as an attempt to give a complete methodology for mathematically-precise development of software. Most of the successes of this project came from development and analysis of speci cations. The SCR community was able to develop an easy to write and review requirements notation and a variety of techniques facilitating reasoning about the requirements and showed their e ectiveness on a number of case studies. Thus, I nd it important to document the following developments in this area: informal description and formal semantics of SCR; techniques to reason about correctness of SCR speci cations; tool support for specifying and reasoning about systems in the SCR notation; applications of methods and tools to real-life projects (both successful and unsuccessful).Developments in the SCR are centered around a group of researchers at NRL lead by Connie Heitmeyer. This group is doing a great job conducting research in the area of requirements analysis, and publicizing the SCR method among academic and industrial software engineers. The requirements notation is easy to read and review, has simple semantics, and has in general been liked by engineers who got to use it. There are industrial-quality automated tools, like SCR* 2], and a number of researchers outside NRL have used the method successfully on their projects. However, despite a growing number of successful case studies, it seems that the industry is slow adopting and embracing SCR as their development methodology. What is wrong? The following is a (partial) list of possible reasons:", "num_citations": "1\n", "authors": ["664"]}