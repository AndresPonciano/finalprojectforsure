{"title": "An experiment in partial evaluation: The generation of a compiler generator\n", "abstract": " It has been known for several years that in theory the program transformation principle called IA~ rtl'a] evaluation or mixsd compu~ a~ don can be used for compiling and compiler generation (given an interpreter for the language to be implemented), and even for the generation of a compiler generator. The present paper describes an experimental partial evaluator able to generate stand-alone compilers and compiler generators. As far as we know, such generations had not been done in practice prior to summer 1984. Partial evaluation of a subIect program with respect to some of its input parameters results in a resfdualprogram. By definition, running the residual program on any remaining input yields the same result as running the original subject program on all of its input. Thus a residual program can be considered a s;~ cialdzat2\" on of the subject program to known, fixed values of some of its parameters. A] l~ r\u00a0\u2026", "num_citations": "350\n", "authors": ["674"]}
{"title": "Mix: A self-applicable partial evaluator for experiments in compiler generation\n", "abstract": " The program transformation principle called partial evaluation has interesting applications in compilation and compiler generation. Self-applicable partial evaluators may be used for transforming interpreters into corresponding compilers and even for the generation of compiler generators. This is useful because interpreters are significantly easier to write than compilers, but run much slower than compiled code. A major difficulty in writing compilers (and compiler generators) is the thinking in terms of distinct binding times: run time and compile time (and compiler generation time). The paper gives an introduction to partial evaluation and describes a fully automatic though experimental partial evaluator, called mix, able to generate stand-alone compilers as well as a compiler generator. Mix partially evaluates programs written in Mixwell, essentially a first-order subset of statically scoped pure Lisp. For compiler\u00a0\u2026", "num_citations": "322\n", "authors": ["674"]}
{"title": "A semantics-based framework for the abstract interpretation of Prolog\n", "abstract": " CiNii \u8ad6\u6587 - A semantics-based framework for the abstract interpretation of prolog CiNii \u56fd\u7acb \u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853\u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb \u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005 \u540d \u8457\u8005ID \u8457\u8005\u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3\u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 A semantics-based framework for the abstract interpretation of prolog JONES ND \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6 \u8457\u8005 JONES ND \u53ce\u9332\u520a\u884c\u7269 Abstract interpretation of declarative languages Abstract interpretation of declarative languages, 123-142, 1987 Ellis Horwood Limited \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u62bd\u8c61\u5b9f\u884c \u305d\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3068\u5b9f\u4f8b(\u305d\u306e 1) \u5c0f\u91ce \u8aed , \u5c0f\u5ddd \u745e\u53f2 , Satoshi Ono , Mizuhito Ogawa , NTT\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u7814\u7a76\u6240 , NTT\u57fa\u790e\u7814\u7a76 \u6240 , NTT Software Laboratories. , NTT Basic Research Laboratories. \u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2 \u2026", "num_citations": "168\n", "authors": ["674"]}
{"title": "An application of abstract interpretation of logic programs: Occur check reduction\n", "abstract": " The occur check in Robinson unification is superfluous in most unifications that take place in practice. The present paper is concerned with the problem of determining circumstances under which the occur check may be safely dispensed with. The method given draws on one outlined by David Plaisted. The framework, however, differs in that we systematically apply the abstract interpretation principle to logic programs. The aim is to give a clear presentation and to facilitate justification of the soundness of the method.", "num_citations": "157\n", "authors": ["674"]}
{"title": "Precise and efficient groundness analysis for logic programs\n", "abstract": " We show how precise groundness information can be extracted from logic programs. The idea is to use abstract interpretation with Boolean functions as \u201capproximations\u201d to groundness dependencies between variables. This idea is not new, and different classes of Boolean functions have been used. We argue, however, that one class, the positive functions, is more suitable than others. Positive Boolean functions have a certain property which we (inspired by A. Langen) call \u201ccondensation.\u201d This property allows for rapid computation of groundness information.", "num_citations": "155\n", "authors": ["674"]}
{"title": "Two classes of Boolean functions for dependency analysis\n", "abstract": " Many static analyses for declarative programming/database languages use Boolean functions to express dependencies among variables or argument positions. Examples include groundness analysis, arguably the most important analysis for logic programs, finiteness analysis and functional dependency analysis for databases. We identify two classes of Boolean functions that have been used: positive and definite functions, and we systematically investigate these classes and their efficient implementation for dependency analyses. On the theoretical side, we provide syntactic characterizations and study the expressiveness and algebraic properties of the classes. In particular, we show that both are closed under existential quantification. On the practical side, we investigate various representations for the classes based on reduced ordered binary decision diagrams (ROBDDs), disjunctive normal form, conjunctive\u00a0\u2026", "num_citations": "125\n", "authors": ["674"]}
{"title": "Referential transparency, definiteness and unfoldability\n", "abstract": " The term \u201creferential transparency\u201d is frequently used to indicate that a programming language has certain useful substitution properties. We observe, however, that the formal and informal definitions given in the literature are not equivalent and we investigate their relationships. To this end, we study the definitions in the context of a simple expression language and show that in the presence of non-determinism, the differences between the definitions are manifest. We propose a definition of \u201creferential transparency\u201d, based on Quine's, as well as of the related notions: definiteness and unfoldability. We demonstrate that these notions are useful to characterize programming languages.", "num_citations": "122\n", "authors": ["674"]}
{"title": "Non-determinism in functional languages\n", "abstract": " The introduction of a non-deterministic operator in even a very simple functional programming language gives rise to a plethora of semantic questions. These questions are not only concerned with the choice operator itself. A surprisingly large number of different parameter passing mechanisms are made possible by the introduction of bounded non-determinism. The diversity of semantic possibilities is examined systematically using denotational definitions based on mathematical structures called power domains. This results in an improved understanding of the different kinds of non-determinism and the properties of different kinds of non-deterministic languages.", "num_citations": "120\n", "authors": ["674"]}
{"title": "Denotational abstract interpretation of logic programs\n", "abstract": " Logic-programming languages are based on a principle of separation \u201clogic\u201d and \u201ccontrol.\u201d. This means that they can be given simple model-theoretic semantics without regard to any particular execution mechanism (or proof procedure, viewing execution as theorem proving). Although the separation is desirable from a semantical point of view, it makes sound, efficient implementation of logic-programming languages difficult. The lack of \u201ccontrol information\u201d in programs calls for complex data-flow analysis techniques to guide execution. Since data-flow analysis furthermore finds extensive use in error-finding and transformation tools, there is a need for a simple and powerful theory of data-flow analysis of logic programs. This paper offers such a theory, based on F. Nielson's extension of P. Cousot and R. Cousot's abstract interpretation. We present a denotational definition of the semantics of definite logic programs\u00a0\u2026", "num_citations": "118\n", "authors": ["674"]}
{"title": "Boolean functions for dependency analysis: Algebraic properties and efficient representation\n", "abstract": " Many analyses for logic programming languages use Boolean functions to express dependencies between variables or argument positions. Examples include groundness analysis, arguably the most important analysis for logic programs, finiteness analysis and functional dependency analysis. We identify two classes of Boolean functions that have been used: positive and definite functions, and we systematically investigate these classes and their efficient implementation for dependency analyses. We provide syntactic characterizations and study their algebraic properties. In particular, we show that both classes are closed under existential quantification. We investigate representations for these classes based on: reduced ordered binary decision diagrams (ROBDDs), disjunctive normal form, conjunctive normal form, Blake canonical form, dual Blake canonical form, and a form specific to definite functions\u00a0\u2026", "num_citations": "96\n", "authors": ["674"]}
{"title": "Bottom-up abstract interpretation of logic programs\n", "abstract": " CiNii \u8ad6\u6587 - Bottom-up Abstract Interpretation of Logic Programs CiNii \u56fd\u7acb\u60c5\u5831\u5b66\u7814\u7a76\u6240 \u5b66\u8853 \u60c5\u5831\u30ca\u30d3\u30b2\u30fc\u30bf[\u30b5\u30a4\u30cb\u30a3] \u65e5\u672c\u306e\u8ad6\u6587\u3092\u3055\u304c\u3059 \u5927\u5b66\u56f3\u66f8\u9928\u306e\u672c\u3092\u3055\u304c\u3059 \u65e5\u672c\u306e\u535a\u58eb\u8ad6\u6587\u3092\u3055\u304c\u3059 \u65b0\u898f\u767b\u9332 \u30ed\u30b0\u30a4\u30f3 English \u691c\u7d22 \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u3059\u3079\u3066 \u672c\u6587\u3042\u308a \u9589\u3058\u308b \u30bf\u30a4\u30c8\u30eb \u8457\u8005\u540d \u8457\u8005ID \u8457\u8005 \u6240\u5c5e \u520a\u884c\u7269\u540d ISSN \u5dfb\u53f7\u30da\u30fc\u30b8 \u51fa\u7248\u8005 \u53c2\u8003\u6587\u732e \u51fa\u7248\u5e74 \u5e74\u304b\u3089 \u5e74\u307e\u3067 \u691c\u7d22 \u691c\u7d22 \u691c\u7d22 CiNii\u7a93\u53e3 \u696d\u52d9\u306e\u518d\u958b\u306b\u3064\u3044\u3066 Bottom-up Abstract Interpretation of Logic Programs MARRIOTT K. \u88ab\u5f15\u7528 \u6587\u732e: 1\u4ef6 \u8457\u8005 MARRIOTT K. \u53ce\u9332\u520a\u884c\u7269 Proceedings of the 5th International Conference and Symposium on Logic Programming Proceedings of the 5th International Conference and Symposium on Logic Programming, 733-748, 1988 MIT Press \u88ab\u5f15\u7528\u6587\u732e: 1\u4ef6\u4e2d 1-1\u4ef6\u3092 \u8868\u793a 1 \u8ad6\u7406\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u62bd\u8c61\u89e3\u91c8\u3092\u7528\u3044\u305f\u89e3\u6790 \u5800\u5185 \u8b19\u4e8c , Kenji Horiuchi , (\u8ca1)\u65b0\u4e16\u4ee3\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf \u6280\u8853\u958b\u767a\u6a5f\u69cb:(\u73fe)\u4e09\u83f1\u96fb\u6a5f\u7523\u696d\u30b7\u30b9\u30c6\u30e0\u7814\u7a76\u6240 , Institute for New Generation Computer \u2026", "num_citations": "89\n", "authors": ["674"]}
{"title": "Collaborative learning through formative peer review: Pedagogy, programs and potential\n", "abstract": " We examine student peer review, with an emphasis on formative practice and collaborative learning, rather than peer grading. Opportunities to engage students in such formative peer assessment are growing, as a range of online tools become available to manage and simplify the process of administering student peer review. We consider whether pedagogical requirements for student peer review are likely to be discipline-specific, taking computer science and software engineering as an example. We then summarise what attributes are important for a modern generic peer review tool, and classify tools according to four prevalent emphases, using currently available, mature tools to illustrate each. We conclude by identifying some gaps in current understanding of formative peer review, and discuss how online tools for student peer review can help create opportunities to answer some of these questions.", "num_citations": "81\n", "authors": ["674"]}
{"title": "Semantics-based dataflow analysis of logic programs\n", "abstract": " The increased acceptance of Prolog has motivated widespread interest in the semanticsbased dataflow analysis of logic programs and a number of different approaches have been suggested. However, the relationships between these approaches are not clear. The present paper provides a unifying introduction to the approaches by giving novel denotational semantic definitions which capture their essence. In addition, the wide range of analysis tools supported by semantics-based dataflow analysis are discussed.", "num_citations": "79\n", "authors": ["674"]}
{"title": "What drives curriculum change?\n", "abstract": " While promotional literature about computer science programs may claim that curricula are determined by the needs of the students and by international best practice, the reality is often different. In this paper we reflect on the factors underlying curriculum change in computer science departments and schools, from institutional requirements and financial pressures to purely academic considerations. We have used these reflections as the basis of an investigation of curriculum management practices at institutions in Australasia, via a survey instrument sent to a range of colleagues. Our findings from the survey are consistent with our own experiences, namely, that curriculum change is driven or inhibited by factors such as vocal individuals and practical constraints rather than higher academic motives.", "num_citations": "78\n", "authors": ["674"]}
{"title": "Analysis of constraint logic programs\n", "abstract": " Analysis of constraint logic programs | Proceedings of the 1990 North American conference on Logic programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the 1990 North American conference on Logic programmingAnalysis of constraint logic programs ARTICLE Analysis of constraint logic programs Share on Authors: Kim G Marriott profile image Kim Marriott View Profile , Harald S\u00f8ndergaard profile image Harald S\u00f8ndergaard View Profile Authors Info & Affiliations Publication: Proceedings of the 1990 North American conference on Logic programmingJanuary 1990 Pages 531\u2013547 9citation 0 \u2026", "num_citations": "77\n", "authors": ["674"]}
{"title": "State joining and splitting for the symbolic execution of binaries\n", "abstract": " Symbolic execution can be used to explore the possible run-time states of a program. It makes use of a concept of \u201cstate\u201d where a variable\u2019s value has been replaced by an expression that gives the value as a function of program input. Additionally, a state can be equipped with a summary of control-flow history: a \u201cpath constraint\u201d keeps track of the class of inputs that would have caused the same flow of control. But even simple programs can have trillions of paths, so a path-by-path analysis is impractical. We investigate a \u201cstate joining\u201d approach to making symbolic execution more practical and describe the challenges of applying state joining to the analysis of unmodified Linux x86 executables. The results so far are mixed, with good results for some code. On other examples, state joining produces cumbersome constraints that are more expensive to solve than those generated by normal symbolic execution.", "num_citations": "68\n", "authors": ["674"]}
{"title": "Termination analysis for Mercury\n", "abstract": " Since the late eighties, much progress has been made in the theory of termination analysis for logic programs. However, the practical significance of much of this work is hard to judge, since experimental evaluations rarely get published. Here we describe and evaluate a termination analyzer for Mercury, a strongly typed and moded logic-functional programming language. Mercury's high degree of referential transparency and the guaranteed availability of reliable mode information simplify termination analysis. Our analyzer uses a variant of a method developed by Pl\u00fcmer. It deals with full Mercury, including modules and I/O. In spite of these obstacles, it produces state-of-the-art termination information, while having a negligible impact on the running time of the compiler of which it is part, even for large programs.", "num_citations": "65\n", "authors": ["674"]}
{"title": "Sharing and groundness dependencies in logic programs\n", "abstract": " We investigate Jacobs and Langen's Sharing domain, introduced for the analysis of variable sharing in logic programs, and show that it is isomorphic to Marriott and S\u00f8ndergaard's Pos domain, introduced for the analysis of groundness dependencies. Our key idea is to view the sets of variables in a Sharing domain element as the models of a corresponding Boolean function. This leads to a recasting of sharing analysis in terms of the property of \u201cnot being affected by the binding of a single variable.\u201d  Such an \u201cunaffectedness dependency\u201d analysis has close connections with groundness dependency analysis using positive Boolean functions. This new view improves our understanding of sharing analysis, and leads to an elegant expression of its combination with  groundness dependency analysis based on the reduced product of Sharing and Pos. It also opens up new avenues for the efficient implementation of\u00a0\u2026", "num_citations": "55\n", "authors": ["674"]}
{"title": "Notes for a tutorial on abstract interpretation of logic programs\n", "abstract": " The present notes are concerned with semantics-based dataflow analysis of definite clause logic programs. They have been produced for a tutorial given by the authors to the North American Conference on Logic Programming in Cleveland, Ohio, 16 October 1989. Thenotes are a condensed version of two forthcoming papers [33, 36]. Proofs omitted here appear in these papers. In Section 1 we give a brief introduction and historical background to the subject. In Section 2 we introduce some preliminary notation. In Section 3 we give a general theory for dataflow analysis which is basically that of abstract interpretation as introduced by P. and R. Cousot. We develop a simple abstract interpretation based on the well-known TP semantics of definite clause programs. In Section 4 we consider the abstract interpretation of definite clause logic programs and detail its uses. We discuss the limitations of dataflow analyses which are based on either the TP or SLD semantics of logic programs and develop a denotational semantics which may be used as a basis for most existing dataflow analyses. In Section 5 a non-trivial dataflow analysis for groundness propagation is developed from the denotational definitions given in Section 4. 1", "num_citations": "52\n", "authors": ["674"]}
{"title": "Automatic abstraction for congruences\n", "abstract": " One approach to verifying bit-twiddling algorithms is to derive invariants between the bits that constitute the variables of a program. Such invariants can often be described with systems of congruences where in each equation , m is a power of two, c is a vector of integer coefficients, and x is a vector of propositional variables (bits). Because of the low-level nature of these invariants and the large number of bits that are involved, it is important that the transfer functions can be derived automatically. We address this problem, showing how an analysis for bit-level congruence relationships can be decoupled into two parts: (1) a SAT-based abstraction (compilation) step which can be automated, and (2) an interpretation step that requires no SAT-solving. We exploit triangular matrix forms to derive transfer functions efficiently, even in the presence of large numbers of bits. Finally we propose program\u00a0\u2026", "num_citations": "51\n", "authors": ["674"]}
{"title": "Learning from and with peers: The different roles of student peer reviewing\n", "abstract": " There are many different approaches to student peer assessment. In this paper I lay out the pedagogical philosophy behind my own use of student peer reviews. These should not only be seen as adding to the amount of formative feedback in a class, nor are they only about the development of certain higher-order cognitive skills. Properly aligned with an overall assessment strategy, peer reviewing can help build a stronger learning community. I describe such a strategy and my experience using PRAZE, an online tool for student peer reviewing, as well as students' response to the tool and its use.", "num_citations": "49\n", "authors": ["674"]}
{"title": "Bottom-up dataflow analysis of normal logic programs\n", "abstract": " A theory of semantics-based dataflow analysis using a notion of \u201cinsertion\u201d is presented. This notion relaxes the Galois connections used in Cousot and Cousot's theory of abstract interpretation. The aim is to obtain a firm basis for the development of dataflow analyses of normal logic programs. A dataflow analysis is viewed as a nonstandard semantics that approximates the standard semantics by manipulating descriptions of data objects rather than the objects themselves. A Kleene logic-based semantics for normal logic programs is defined, similar to Fitting's \u0444P semantics. This provides the needed semantic base for \u201cbottom-up\u201d dataflow analyses. Such analyses give information about the success and failure sets of a program. A major application of bottom-up analysis is therefore type inference. We detail a dataflow analysis using descriptions similar to Sato and Tamaki's depth-k abstractions and another using\u00a0\u2026", "num_citations": "47\n", "authors": ["674"]}
{"title": "Abstract interpretation of active rules and its use in termination analysis\n", "abstract": " The behaviour of rules in an active database system can be difficult to predict, and much work has been devoted to the development of automatic support for reasoning about properties such as confluence and termination. We show how abstract interpretation can provide a generic framework for analysis of active rules. Abstract interpretation is a well-understood, semantics-based method for static analysis. Its advantage, apart from generality, lies in the separation of concerns: Once the underlying semantics has been captured formally, a variety of analyses can be derived, almost for free, as approximations to the semantics. Powerful general theorems enable simple proofs of global correctness and uniform termination of specific analyses. We outline these ideas and show, as an example application, a new method for termination analysis. In terms of precision, the method compares favourably with previous\u00a0\u2026", "num_citations": "41\n", "authors": ["674"]}
{"title": "Combining string abstract domains for JavaScript analysis: An evaluation\n", "abstract": " Strings play a central role in JavaScript and similar scripting languages. Owing to dynamic features such as the eval function and dynamic property access, precise string analysis is a prerequisite for automated reasoning about practically any kind of runtime property. Although the literature presents a considerable number of abstract domains for capturing and representing specific aspects of strings, we are not aware of tools that allow flexible combination of string abstract domains. Indeed, support for string analysis is often confined to a single, dedicated string domain. In this paper we describe a framework that allows us to combine multiple string abstract domains for the analysis of JavaScript programs. It is implemented as an extension of SAFE, an open-source static analysis tool. We investigate different combinations of abstract domains that capture various aspects of strings. Our evaluation suggests\u00a0\u2026", "num_citations": "33\n", "authors": ["674"]}
{"title": "Synthesizing optimal switching lattices\n", "abstract": " The use of nanoscale technologies to create electronic devices has revived interest in the use of regular structures for defining complex logic functions. One such structure is the switching lattice, a two-dimensional lattice of four-terminal switches. We show how to directly construct switching lattices of polynomial size from arbitrary logic functions; we also show how to synthesize minimal-sized lattices by translating the problem to the satisfiability problem for a restricted class of quantified Boolean formulas. The synthesis method is an anytime algorithm that uses modern SAT solving technology and dichotomic search. It improves considerably on an earlier proposal for creating switching lattices for arbitrary logic functions.", "num_citations": "30\n", "authors": ["674"]}
{"title": "Signedness-agnostic program analysis: Precise integer bounds for low-level code\n", "abstract": " Many compilers target common back-ends, thereby avoiding the need to implement the same analyses for many different source languages. This has led to interest in static analysis of LLVM code. In LLVM (and similar languages) most signedness information associated with variables has been compiled away. Current analyses of LLVM code tend to assume that either all values are signed or all are unsigned (except where the code specifies the signedness). We show how program analysis can simultaneously consider each bit-string to be both signed and unsigned, thus improving precision, and we implement the idea for the specific case of integer bounds analysis. Experimental evaluation shows that this provides higher precision at little extra cost. Our approach turns out to be beneficial even when all signedness information is available, such as when analysing C or Java code.", "num_citations": "30\n", "authors": ["674"]}
{"title": "Abstract interpretation over non-lattice abstract domains\n", "abstract": " The classical theoretical framework for static analysis of programs is abstract interpretation. Much of the power and elegance of that framework rests on the assumption that an abstract domain is a lattice. Nonetheless, and for good reason, the literature on program analysis provides many examples of non-lattice domains, including non-convex numeric domains. The lack of domain structure, however, has negative consequences, both for the precision of program analysis and for the termination of standard Kleene iteration. In this paper we explore these consequences and present general remedies.", "num_citations": "29\n", "authors": ["674"]}
{"title": "An abstract domain of uninterpreted functions\n", "abstract": " We revisit relational static analysis of numeric variables. Such analyses face two difficulties. First, even inexpensive relational domains scale too poorly to be practical for large code-bases. Second, to remain tractable they have extremely coarse handling of non-linear relations. In this paper, we introduce the subterm domain, a weakly relational abstract domain for inferring equivalences amongst sub-expressions, based on the theory of uninterpreted functions. This provides an extremely cheap approach for enriching non-relational domains with relational information, and enhances precision of both relational and non-relational domains in the presence of non-linear operations. We evaluate the idea in the context of the software verification tool SeaHorn.", "num_citations": "28\n", "authors": ["674"]}
{"title": "Definiteness analysis for CLP (R)\n", "abstract": " Constraint logic programming (CLP) languages generalise logic programming languages, amalgamating logic programming and constraint programming. Combining the best of two worlds, they provide powerful tools for wide classes of problems. As with logic programming languages, code optimization by compilers is an important issue in the implementation of CLP languages. A compiler needs sophisticated global information, collected by dataflow analyses, to generate competitive code. One kind of useful dataflow information concerns the point at which variables become definite, that is, constrained to take a unique value. In this paper we present a very precise dataflow analysis to determine definiteness, and we discuss its applications. By separating the two concerns: correctness and implementation techniques, abstract interpretation enables us to develop a sophisticated dataflow analysis in a straightforward manner, in fact in a framework where the correctness of the analysis is easily establis...", "num_citations": "27\n", "authors": ["674"]}
{"title": "An optimizing compiler for CLP (\u211b)\n", "abstract": " The considerable expressive power and flexibility gained by combining constraint programming with logic programming is not without cost. Implementations of constraint logic programming (CLP) languages must include expensive constraint solving algorithms tailored to specific domains, such as trees, Booleans, or real numbers. The performance of many current CLP compilers and interpreters does not encourage the widespread use of CLP. We outline an optimizing compiler for CLP(\u211b), a CLP language which extends Prolog by allowing linear arithmetic constraints. The compiler uses sophisticated global analyses to determine the applicability of different program transformations. Two important components of the compiler, the analyzer and the optimizer, work in continual interaction in order to apply semantics-preserving transformations to the source program. A large suite of transformations are planned\u00a0\u2026", "num_citations": "25\n", "authors": ["674"]}
{"title": "A characterization of non-floundering logic programs\n", "abstract": " A characterization of non-floundering logic programs | Proceedings of the 1990 North American conference on Logic programming ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Browse Browse Digital Library Collections More HomeBrowse by TitleProceedingsProceedings of the 1990 North American conference on Logic programmingA characterization of non-floundering logic programs Article A characterization of non-floundering logic programs Share on Authors: Kim G Marriott profile image Kim Marriott View Profile , Harald S\u00f8ndergaard profile image Harald S\u00f8ndergaard View Profile , Philip W Dart profile image Philip Dart View Profile Authors Info & Affiliations Publication: Proceedings of the 1990 \u2026", "num_citations": "25\n", "authors": ["674"]}
{"title": "Exploiting sparsity in difference-bound matrices\n", "abstract": " Relational numeric abstract domains are very important in program analysis. Common domains, such as Zones and Octagons, are usually conceptualised with weighted digraphs and implemented using difference-bound matrices (DBMs). Unfortunately, though conceptually simple, direct implementations of graph-based domains tend to perform poorly in practice, and are impractical for analyzing large code-bases. We propose new DBM algorithms that exploit sparsity and closed operands. In particular, a new representation which we call split normal form reduces graph density on typical abstract states. We compare the resulting implementation with several existing DBM-based abstract domains, and show that we can substantially reduce the time to perform full DBM analysis, without sacrificing precision.", "num_citations": "23\n", "authors": ["674"]}
{"title": "Unbounded model-checking with interpolation for regular language constraints\n", "abstract": " We present a decision procedure for the problem of, given a set of regular expressions R                 1, \u2026, R                                    n                 , determining whether R\u2009=\u2009R                 1\u2009\u2229\u2009\u22ef\u2009\u2229\u2009R                                    n                  is empty. Our solver, revenant, finitely unrolls automata for R                 1, \u2026, R                                    n                 , encoding each as a set of propositional constraints. If a SAT solver determines satisfiability then R is non-empty. Otherwise our solver uses unbounded model checking techniques to extract an interpolant from the bounded proof. This interpolant serves as an overapproximation of R. If the solver reaches a fixed-point with the constraints remaining unsatisfiable, it has proven R to be empty. Otherwise, it increases the unrolling depth and repeats. We compare revenant with other state-of-the-art string solvers. Evaluation suggests that it behaves better for constraints that express the\u00a0\u2026", "num_citations": "22\n", "authors": ["674"]}
{"title": "Inferring congruence equations using SAT\n", "abstract": " This paper proposes a new approach for deriving invariants that are systems of congruence equations where the modulo is a power of 2. The technique is an amalgam of SAT-solving, where a propositional formula is used to encode the semantics of a basic block, and abstraction, where the solutions to the formula are systematically combined and summarised as a system of congruence equations. The resulting technique is more precise than existing congruence analyses since a single optimal transfer function is derived for a basic block as a whole.", "num_citations": "22\n", "authors": ["674"]}
{"title": "Horn clauses as an intermediate representation for program analysis and transformation\n", "abstract": " Many recent analyses for conventional imperative programs begin by transforming programs into logic programs, capitalising on existing LP analyses and simple LP semantics. We propose using logic programs as an intermediate program representation throughout the compilation process. With restrictions ensuring determinism and single-modedness, a logic program can easily be transformed to machine language or other low-level language, while maintaining the simple semantics that makes it suitable as a language for program analysis and transformation. We present a simple LP language that enforces determinism and single-modedness, and show that it makes a convenient program representation for analysis and transformation.", "num_citations": "21\n", "authors": ["674"]}
{"title": "Failure tabled constraint logic programming by interpolation\n", "abstract": " We present a new execution strategy for constraint logic programs called Failure Tabled CLP. Similarly to Tabled CLP our strategy records certain derivations in order to prune further derivations. However, our method only learns from failed derivations. This allows us to compute interpolants rather than constraint projection for generation of reuse conditions. As a result, our technique can be used where projection is too expensive or does not exist. Our experiments indicate that Failure Tabling can speed up the execution of programs with many redundant failed derivations as well as achieve termination in the presence of infinite executions.", "num_citations": "21\n", "authors": ["674"]}
{"title": "Interval analysis and machine arithmetic: Why signedness ignorance is bliss\n", "abstract": " The most commonly used integer types have fixed bit-width, making it possible for computations to \u201cwrap around,\u201d and many programs depend on this behaviour. Yet much work to date on program analysis and verification of integer computations treats integers as having infinite precision, and most analyses that do respect fixed width lose precision when overflow is possible. We present a novel integer interval abstract domain that correctly handles wrap-around. The analysis is signedness agnostic. By treating integers as strings of bits, only considering signedness for operations that treat them differently, we produce precise, correct results at a modest cost in execution time.", "num_citations": "20\n", "authors": ["674"]}
{"title": "A comparison of three occur-check analysers\n", "abstract": " A well known problem with many Prolog interpreters and compilers is the lack of occur-check in the implementation of the unification algorithm. This means that such systems are unsound with respect to first-order predicate logic. Static analysis offers an appealing approach to the problem of occur-check reduction, that is, the safe omission of occur-checks in unification. We compare, for the first time, three static methods that have been suggested for occur-check reduction, two based on assigning \u201cmodes\u221d to programs and one which uses abstract interpretation. In each case, the analysis or some essential part of it had not been implemented so far. Of the mode-based methods, one is due to Chadha and Plaisted and the other is due to Apt and Pellegrini. The method using abstract interpretation is based on earlier work by Plaisted, S\u00d8ndergaard and others who have developed groundness and sharing\u00a0\u2026", "num_citations": "20\n", "authors": ["674"]}
{"title": "Effective feedback to small and large classes\n", "abstract": " Educational experts appear to be in broad agreement when it comes to the importance of feedback for effective learning. Students benefit from plenty of opportunity and encouragement to express their understanding, and from informed, supportive, possibly challenging, feedback. At the same time, we observe that many students at our university do not find that they receive helpful feedback. One in three engineering students disagree or strongly disagree with the quality of teaching questionnaire's \"I received helpful feedback on how I was going\" in the individual course, and most other disciplines find themselves in a similar situation. For the university as a whole, student responses to this question are clearly less positive than to other questions on quality of teaching, intellectual stimulation, staff interest, workload, and so on, and this state of affairs seems quite common in the Australian context. We discuss best\u00a0\u2026", "num_citations": "18\n", "authors": ["674"]}
{"title": "Meta-circular abstract interpretation in Prolog\n", "abstract": " We give an introduction to the meta-circular approach to the abstract interpretation of logic programs. This approach is particularly useful for prototyping and for introductory classes on abstract interpretation. Using interpreters, students can immediately write, adapt, and experiment with interpreters and working dataflow analysers. We use a simple meta-circular interpreter, based on a \u201cnon-ground T                         P\u201d semantics, as a generic analysis engine. Instantiating the engine is a matter of providing an appropriate domain of approximations, together with definitions of \u201cabstract\u201d unification and disjunction. Small changes of the interpreter let us vary both what can be \u201cobserved\u201d by an analyser, and how fixed point computation is done. Amongst the dataflow analyses used to exemplify this approach are a parity analysis, groundness dependency analysis, call patterns, depth-k analysis, and a \u201cpattern\u00a0\u2026", "num_citations": "18\n", "authors": ["674"]}
{"title": "An iterative approach to precondition inference using constrained Horn clauses\n", "abstract": " We present a method for automatic inference of conditions on the initial states of a program that guarantee that the safety assertions in the program are not violated. Constrained Horn clauses (CHCs) are used to model the program and assertions in a uniform way, and we use standard abstract interpretations to derive an over-approximation of the set of unsafe initial states. The precondition then is the constraint corresponding to the complement of that set, under-approximating the set of safe initial states. This idea of complementation is not new, but previous attempts to exploit it have suffered from the loss of precision. Here we develop an iterative specialisation algorithm to give more precise, and in some cases optimal safety conditions. The algorithm combines existing transformations, namely constraint specialisation, partial evaluation and a trace elimination transformation. The last two of these transformations\u00a0\u2026", "num_citations": "17\n", "authors": ["674"]}
{"title": "Boolean constraints for binding-time analysis\n", "abstract": " To achieve acceptable accuracy, many program analyses for functional programs are \u201cproperty polymorphic\u201d. That is, they can infer different input-output relations for a function at separate applications of the function, in a manner similar to type inference for a polymorphic language. We extend a property polymorphic (or \u201cpolyvariant\u201d) method for binding-time analysis, due to Dussart, Henglein, and Mossin, so that it applies to languages with ML-style type polymorphism. The extension is non-trivial and we have implemented it for Haskell. While we follow others in specifying the analysis as a non-standard type inference, we argue that it should be realised through a translation into the well-understood domain of Boolean constraints. The expressiveness offered by Boolean constraints opens the way for smooth extensions to sophisticated language features and it allows for more accurate analysis.", "num_citations": "16\n", "authors": ["674"]}
{"title": "Differential methods in logic program analysis\n", "abstract": " Program analysis based on abstract interpretation has proven very useful in compilation of constraint and logic programming languages. Unfortunately, the traditional goal-dependent framework is inherently imprecise. This is because it handles call and return in such a way that dataflow information may be re-asserted unnecessarily, leading to a loss of precision for many description domains. For a few specific domains, the literature contains proposals to overcome the problem, and some implementations use various unpublished tricks that sometimes avoid the precision loss. The purpose of this paper is to map the landscape of goal-dependent, goal-independent, and combined approaches to generic analysis of logic programs. This includes formalising existing methods and tricks in a way that is independent of specific description domains. Moreover, we suggest new methods for overcoming the loss of precision\u00a0\u2026", "num_citations": "16\n", "authors": ["674"]}
{"title": "Exception analysis for non-strict languages\n", "abstract": " In this paper we present the first exception analysis for a non-strict language. We augment a simply-typed functional language with exceptions, and show that we can define a type-based inference system to detect uncaught exceptions. We have implemented this exception analysis in the GHC compiler for Haskell, which has been recently extended with exceptions. We give empirical evidence that the analysis is practical.", "num_citations": "15\n", "authors": ["674"]}
{"title": "Semantics-based analysis and transformation of logic programs\n", "abstract": " Dataflow analysis is an essential component of many programming tools. One use of dataflow information is to identify errors in a program, as done by program \u201cdebuggers\u201d and type checkers. Another is in compilers and other program transformers, where the analysis may guide various optimisations.The correctness of a programming tool\u2019s dataflow analysis component is usually of paramount importance. The theory of abstract interpretation, originally developed by P. and R. Cousot aims at providing a framework for the development of correct analysis tools. In this theory, dataflow analysis is viewed as \u201cnon-standard\u201d semantics, and abstract interpretation prescribes certain relations between standard and non-standard semantics, in order to guarantee the correctness of the nonstandard semantics with respect to the standard semantics.", "num_citations": "13\n", "authors": ["674"]}
{"title": "A bit-vector solver with word-level propagation\n", "abstract": " Reasoning with bit-vectors arises in a variety of applications in verification and cryptography. Michel and Van Hentenryck have proposed an interesting approach to bit-vector constraint propagation on the word level. Each of the operations except comparison are constant time, assuming the bit-vector fits in a machine word. In contrast, bit-vector SMT solvers usually solve bit-vector problems by bit-blasting, that is, mapping the resulting operations to conjunctive normal form clauses, and using SAT technology to solve them. This also means generating intermediate variables which can be an advantage, as these can be searched on and learnt about. Since each approach has advantages it is important to see whether we can benefit from these advantages by using a word-level propagation approach with learning. In this paper we describe an approach to bit-vector solving using word-level propagation with\u00a0\u2026", "num_citations": "12\n", "authors": ["674"]}
{"title": "Places for learning engineering: A preliminary report on informal learning spaces\n", "abstract": " This is a report on an investigation of undergraduate engineering learning spaces that were newly introduced in 2008. The new spaces include formal learning spaces designed to enable student-centred, small group learning, and informal caf\u00e9-style spaces. The project investigates the research question: Does an availability of spaces for informal learning lead to an increase in behaviours that one might expect to be conducive to increased informal or collaborative learning? The initial findings from the survey data suggest that there may be some increased use of informal learning spaces if they are made available to students.", "num_citations": "12\n", "authors": ["674"]}
{"title": "Closure operators for ROBDDs\n", "abstract": " Program analysis commonly makes use of Boolean functions to express information about run-time states. Many important classes of Boolean functions used this way, such as the monotone functions and the Boolean Horn functions, have simple semantic characterisations. They also have well-known syntactic characterisations in terms of Boolean formulae, say, in conjunctive normal form. Here we are concerned with characterisations using binary decision diagrams. Over the last decade, ROBDDs have become popular as representations of Boolean functions, mainly for their algorithmic properties. Assuming ROBDDs as representation, we address the following problems: Given a function \u03c8 and a class of functions \u0394, how to find the strongest \u03d5\u03b5\u0394 entailed by \u03c8 (when such a \u03d5 is known to exist)? How to find the weakest \u03d5\u03b5\u0394 that entails \u03c8? How to determine that a function \u03c8 belongs to a class \u0394? Answers\u00a0\u2026", "num_citations": "12\n", "authors": ["674"]}
{"title": "Difference-list transformation for Prolog\n", "abstract": " Difference-lists are terms that represent lists. The use of difference-lists can speed up most list-processing programs considerably. Prolog programmers routinely use \u201cdifference-list versions\u201d of programs, but very little investigation has taken place into difference-list transformation. Thus, to most programmers it is either unknown that the use of difference-lists is far from safe in all contexts, or else this fact is known but attributed to Prolog\u2019s infamous \u201coccur check problem.\u201d In this paper we study the transformation of list-processing programs into programs that use differencelists. In particular we are concerned with finding circumstances under which the transformation is safe. We show that dataflow analysis can be used to determine whether the transformation is applicable to a given program, thereby allowing for automatic transformation. We prove that our transformation preserves strong operational equivalence.", "num_citations": "12\n", "authors": ["674"]}
{"title": "Constraint programming for dynamic symbolic execution of JavaScript\n", "abstract": " Dynamic Symbolic Execution (DSE) combines concrete and symbolic execution, usually for the purpose of generating good test suites automatically. It relies on constraint solvers to solve path conditions and to generate new inputs to explore. DSE tools usually make use of SMT solvers for constraint solving. In this paper, we show that constraint programming (CP) is a powerful alternative or complementary technique for DSE. Specifically, we apply CP techniques for DSE of JavaScript, the de facto standard for web programming. We capture the JavaScript semantics with MiniZinc and integrate this approach into a tool we call Aratha. We use G-Strings, a CP solver equipped with string variables, for solving path conditions, and we compare the performance of this approach against state-of-the-art SMT solvers. Experimental results, in terms of both speed and coverage, show the benefits of our approach, thus\u00a0\u2026", "num_citations": "11\n", "authors": ["674"]}
{"title": "The Boolean logic of set sharing analysis\n", "abstract": " We show that Jacobs and Langen's domain for set-sharing analysis is isomorphic to the domain of positive Boolean functions, introduced by Marriott and S\u00f8ndergaard for groundness dependency analysis. Viewing a set-sharing description as a minterm representation of a Boolean function leads to re-casting sharing analysis as an instantiation dependency analysis. The key idea is to view the sets of variables in a sharing domain element as the models of a Boolean function. In this way, sharing sets are precisely dual negated positive Boolean functions. This new view improves our understanding of sharing analysis considerably and opens up new avenues for the efficient implementation of this kind of analysis, for example using ROBDDs. To this end we express Jacobs and Langen's abstract operations for set sharing in logical form.", "num_citations": "11\n", "authors": ["674"]}
{"title": "On Prolog and the occur check problem\n", "abstract": " It is well-known that omission of the occur check in unification leads to unsound Prolog systems. Nevertheless, most Prolog systems omit the occur check because this makes unification much faster and unsoundness allegedly seldom manifests itself. We revisit the occur check problem and point to two aspects that have previously received no attention. Firstly, \"unification without the occur check\" is ambiguous, and in practice, Prolog systems vary markedly in their reaction to programs having occur check problems. Secondly, even very simple program transformations are unsafe for pure Prolog when the occur check is omitted. We conclude that the occur check problem is important, and in particular, that the current efforts to standardize Prolog should address it.", "num_citations": "11\n", "authors": ["674"]}
{"title": "Reference abstract domains and applications to string analysis\n", "abstract": " Abstract interpretation is a well established theory that supports reasoning about the run-time behaviour of programs. It achieves tractable reasoning by considering abstractions of run-time states, rather than the states themselves. The chosen set of abstractions is referred to as the abstract domain. We develop a novel framework for combining (a possibly large number of) abstract domains. It achieves the effect of the so-called reduced product without requiring a quadratic number of functions to translate information among abstract domains. A central notion is a reference domain, a medium for information exchange. Our approach suggests a novel and simpler way to manage the integration of large numbers of abstract domains. We instantiate our framework in the context of string analysis. Browser-embedded dynamic programming languages such as JavaScript and PHP encourage the use of strings as a universal\u00a0\u2026", "num_citations": "10\n", "authors": ["674"]}
{"title": "Optimizing compilation for CLP (R)\n", "abstract": " Constraint Logic Programming (CLP) is a recent innovation in programming language design. CLP languages extend logic programming by allowing constraints from different domains such as real numbers or Boolean functions. This gives considerable expressive power and flexibility and CLP programs have proven to be a high-level programming paradigm for applications based on interactive mathematical modeling. These advantages, however, are not without cost. Implementations of CLP languages must include expensive constraint solving algorithms tailored to the specific domains. Indeed, performance of the current generation of CLP compilers and interpreters is one of the main obstacles to the widespread use of CLP. Here we outline the design of a highly optimizing compiler for CLP (R), a CLP language which extends Prolog by allowing linear arithmetic constraints. This compiler is intended to overcome the efficiency problems of the current implementation technology. The main innovation in the comp...", "num_citations": "10\n", "authors": ["674"]}
{"title": "A generic object-oriented incremental analyser for constraint logic programs\n", "abstract": " CiteSeerX \u2014 A Generic Object-Oriented Incremental Analyser for Constraint Logic Programs Documents Authors Tables Log in Sign up MetaCart DMCA Donate CiteSeerX logo Documents: Advanced Search Include Citations Authors: Advanced Search Include Citations Tables: DMCA A Generic Object-Oriented Incremental Analyser for Constraint Logic Programs (1996) Cached Download as a PDF Download Links [www.csse.monash.edu.au] Save to List Add to Collection Correct Errors Monitor Changes by Andrew D. Kelly , Kim Marriott , Harald S\u00f8ndergaard , Peter J. Stuckey Summary Citations Active Bibliography Co-citation Clustered Documents Version History Share Facebook Twitter Reddit Bibsonomy OpenURL Abstract Keyphrases generic object-oriented incremental analyser constraint logic program Powered by: Apache Solr About CiteSeerX Submit and Index Documents Privacy Policy Help Data Source \u2026", "num_citations": "9\n", "authors": ["674"]}
{"title": "Leveraging abstract interpretation for efficient dynamic symbolic execution\n", "abstract": " Dynamic Symbolic Execution (DSE) is a technique to automatically generate test inputs by executing a program with concrete and symbolic values simultaneously. A key challenge in DSE is scalability; executing all feasible program paths is not possible, owing to the potentially exponential or infinite number of paths. Loops are a main source of path explosion, in particular where the number of iterations depends on a program's input. Problems arise because DSE maintains symbolic values that capture only the dependencies on symbolic inputs. This ignores control dependencies, including loop dependencies that depend indirectly on the inputs. We propose a method to increase the coverage achieved by DSE in the presence of input-data dependent loops and loop dependent branches. We combine DSE with abstract interpretation to find indirect control dependencies, including loop and branch indirect\u00a0\u2026", "num_citations": "8\n", "authors": ["674"]}
{"title": "Solving difference constraints over modular arithmetic\n", "abstract": " Difference logic is commonly used in program verification and analysis. In the context of fixed-precision integers, as used in assembly languages for example, the use of classical difference logic is unsound. We study the problem of deciding difference constraints in the context of modular arithmetic and show that it is strongly NP-complete. We discuss the applicability of the Bellman-Ford algorithm and related shortest-distance algorithms to the context of modular arithmetic. We explore two approaches, namely a complete method implemented using SMT technology and an incomplete fixpoint-based method, and the two are experimentally evaluated. The incomplete method performs considerably faster while maintaining acceptable accuracy on a range of instances.", "num_citations": "8\n", "authors": ["674"]}
{"title": "Immediate fixpoints and their use in groundness analysis\n", "abstract": " A theorem by Schr\u00f6der says that for a certain natural class of functions F: B \u2192 B defined on a Boolean lattice B, F(x)=F(F(F(x))) for all x \u2203 B. An immediate corollary is that if such a function is monotonic then it is also idempotent, that is, F(x)=F(F(x)). We show how this corollary can be extended to recognize cases where recursive definitions can immediately be replaced by an equivalent closed form, that is, they can be solved without Kleene iteration. Our result applies more generally to distributive lattices. It has applications for example in the abstract interpretation of declarative programs and deductive databases. We exemplify this by showing how to accelerate simple cases of strictness analysis for first-order functional programs and, perhaps more successfully, groundness analysis for logic programs.", "num_citations": "8\n", "authors": ["674"]}
{"title": "Compositional symbolic execution using fine-grained summaries\n", "abstract": " Compositional symbolic execution has been proposed as a way to increase the efficiency of symbolic execution. Essentially, when a function is symbolically executed, a summary of the path that was executed is stored. This summary records the precondition and post condition of the path, and on subsequent calls that satisfy that precondition, the corresponding post condition can be returned instead of executing the function again. However, using functions as the unit of summarisation leaves the symbolic execution tool at the mercy of a program designer, essentially resulting in an arbitrary summarisation strategy. In this paper, we explore the use of fine-grained summaries, in which blocks within functions are summarised. We propose three types of summarisation and demonstrate how to generate these. At such a fine-grained level, symbolic execution of a path effectively becomes the concatenation of the\u00a0\u2026", "num_citations": "7\n", "authors": ["674"]}
{"title": "Higher-precision groundness analysis\n", "abstract": " Groundness analysis of logic programs using Pos-based abstract interpretation is one of the clear success stories of the last decade in the area of logic program analysis. In this work we identify two problems with the Pos domain, the multiplicity and sign problems, that arise independently in groundness and uniqueness analysis. We describe how these problems can be solved using an analysis based on a domain Size for inferring term size relations. However this solution has its own shortcomings because it involves a widening operator which leads to a loss of Pos information. Inspired by Pos, Size and the LSign domain for abstract linear arithmetic constraints we introduce a new domain LPos, and show how it can be used for groundness and uniqueness analysis. The idea is to use the sign information of LSign to improve the widening of Size so that it does not lose Pos information. We prove that the\u00a0\u2026", "num_citations": "7\n", "authors": ["674"]}
{"title": "Transforming communication skills instruction: The conference approach\n", "abstract": " From the social constructivist perspective of education, learning is best achieved when students face complex, real world problems in which there are no clear answers. Faced with a sizeable common goal, students work collaboratively towards outcomes and maintain ownership over key decisions. The role of staff is that of facilitators whose role is to challenge learners to explore multiple aspects of the problem as they go about reaching viable solutions. Such a role contrasts, for example, to an approach which sets out to lead students to a presumed correct solution that is already possessed by the instructor. Based on these principles we designed and implemented a course on communication skills in Computer Science. Here, we describe our experiences using a student-run conference as a means to teach communication skills. In this approach, students were charged with the task of planning and organising a\u00a0\u2026", "num_citations": "7\n", "authors": ["674"]}
{"title": "A practical object\u2010oriented analysis engine for CLP\n", "abstract": " The incorporation of global program analysis into recent compilers for Constraint Logic Programming (CLP) languages has greatly improved the efficiency of compiled programs. We present a global analyser based on abstract interpretation. Unlike traditional optimizers, whose designs tend to be ad hoc, the analyser has been designed with flexibility in mind. The analyser is incremental, allowing substantial program transformations by a compiler without requiring redundant re\u2010computation of analysis data. The analyser is also generic in that it can perform a large number of different program analyses. Furthermore, the analyser has an object\u2010oriented design, enabling it to be adapted to different applications easily and allowing it to be used with various CLP languages with simple modifications. As an example of this generality, we sketch the use of the analyser in two different applications involving two distinct CLP\u00a0\u2026", "num_citations": "7\n", "authors": ["674"]}
{"title": "A general theory of abstraction\n", "abstract": " Abstraction is the process of mapping one representation of a problem into another representation so as to simplify reasoning while preserving the essence of the problem. It has been investigated in many different areas of computer science. In artificial intelligence, abstraction is used in automated theorem proving, problem solving, planning, common sense reasoning and qualitative reasoning. In dataflow analysis of programs, it has been formalized in the theory of abstract interpretation. However, the possible connections between research on abstraction in artificial intelligence and dataflow analysis have gone unnoticed until now. This paper investigates these connections and provides a general theory of abstraction for artificial intelligence. In particular, we present a method for building an \u201coptimal\u201d abstraction and give general sufficient conditions for the abstraction to be \u201csafe.\u201d The usefulness and generality of our theory is illustrated with a number of example applications and by comparisons with related work.", "num_citations": "7\n", "authors": ["674"]}
{"title": "Using metamorphic testing to improve dynamic symbolic execution\n", "abstract": " Dynamic symbolic execution (DSE) is an approach for automatically generating test inputs from source code using constraint information. It is used in fuzzing: the execution of tests while monitoring for generic properties such as buffer overflows and other security violations. Limitations of DSE for fuzzing are two-fold: (1) only generic properties are checked: many deviations from specified behaviour are not found, and (2) many programs are not entirely amenable to DSE because they give rise to hard constraints, so that some parts of a program remain uncovered. In this paper, we discuss how to mitigate these problems using metamorphic testing (MT). Metamorphic testing uses domain-specific properties about program behaviour, relating pairs of inputs to pairs of outputs. From a given test suite, follow-up tests inputs are generated, and their outputs are compared to outputs from the original tests, using metamorphic\u00a0\u2026", "num_citations": "6\n", "authors": ["674"]}
{"title": "A tool for intersecting context-free grammars and its applications\n", "abstract": " This paper describes a tool for intersecting context-free grammars. Since this problem is undecidable the tool follows a refinement-based approach and implements a novel refinement which is complete for regularly separable grammars. We show its effectiveness for safety verification of recursive multi-threaded programs.", "num_citations": "6\n", "authors": ["674"]}
{"title": "Fragment-based planning using column generation\n", "abstract": " We introduce a novel algorithm for temporal planning in Golog using shared resources, and describe the Bulk Freight Rail Scheduling Problem, a motivating example of such a temporal domain. We use the framework of column generation to tackle complex resource constrained temporal planning problems that are beyond the scope of current planning technology by combining: the global view of a linear programming relaxation of the problem; the strength of search infinding action sequences; and the domain knowledge that can be encoded in a Golog program. We show that our approach significantly outperforms state-of-the-art temporal planning and constraint programming approaches in this domain, in addition to existing temporal Golog implementations. We also apply our algorithm to a temporal variant of blocks-world where our decomposition speeds proof of optimality significantly compared to other anytime algorithms. We discuss the potential of the underlying algorithm being applicable to STRIPS planning, with further work.", "num_citations": "6\n", "authors": ["674"]}
{"title": "Boolean approximation revisited\n", "abstract": " Most work to date on Boolean approximation assumes that Boolean functions are represented by formulas in conjunctive normal form. That assumption is appropriate for the classical applications of Boolean approximation but potentially limits wider use. We revisit, in a lattice-theoretic setting, so-called envelopes and cores in propositional logic, identifying them with upper and lower closure operators, respectively. This leads to recursive representation-independent characterisations of Boolean approximation for a large class of classes. We show that Boolean development can be applied in a representation-independent setting to develop approximation algorithms for a broad range of Boolean classes, including Horn and Krom functions.", "num_citations": "6\n", "authors": ["674"]}
{"title": "On propagation-based analysis of logic programs\n", "abstract": " Notions such as\\reexecution\" and\\propagation\" have recently attracted attention in data ow analysis of logic programs. Both techniques promise more accurate data ow analysis without requiring more complex description domains. Propagation, however, has never been given a formal de nition. It has therefore been di cult to discuss properties such as correctness, precision, and termination of propagation.We suggest a de nition of propagation. Comparing propagation-based analysis with the more conventional approach based on abstract interpretation, we nd that propagation involves a certain inherent loss of precision when data ow analyses are based on description domains which are not\\downwards closed\"(including mode analysis). In the archetypical downwards closed case, groundness analysis, we contrast approaches using Boolean functions as descriptions with those using propagation or reexecution.", "num_citations": "6\n", "authors": ["674"]}
{"title": "Low-contact learning in a first year programming course\n", "abstract": " Low-contact learning in a first year programming course | Proceedings of the 1st Australasian conference on Computer science education ACM Digital Library home ACM home Google, Inc. (search) Advanced Search Browse About Sign in Register Advanced Search Journals Magazines Proceedings Books SIGs Conferences People More Search ACM Digital Library SearchSearch Advanced Search Proceedings ACM Proceedings Conferences when & where ICPS Proceedings More HomeICPS ProceedingsACSE '96Low-contact learning in a first year programming course Article Low-contact learning in a first year programming course Share on Authors: Roy Johnston View Profile , Alistair Moffat View Profile , Harald S\u00f8ndergaard View Profile , Peter Stuckey View Profile Authors Info & Affiliations ACSE '96: Proceedings of the 1st Australasian conference on Computer science educationJuly 1996 Pages 19\u201326https://\u2026", "num_citations": "5\n", "authors": ["674"]}
{"title": "Wombit: A portfolio bit-vector solver using word-level propagation\n", "abstract": " We develop an idea originally proposed by Michel and Van Hentenryck of how to perform bit-vector constraint propagation on the word level. Most operations are propagated in constant time, assuming the bit-vector fits in a machine word. In contrast, bit-vector SMT solvers usually solve bit-vector problems by (ultimately) bit-blasting, that is, mapping the resulting operations to conjunctive normal form clauses, and using SAT technology to solve them. Bit-blasting generates intermediate variables which can be an advantage, as these can be searched on and learnt about. As each approach has advantages, it makes sense to try to combine them. In this paper, we describe an approach to bit-vector solving using word-level propagation with learning. We have designed alternative word-level propagators to Michel and Van Hentenryck\u2019s, and evaluated different variants of the approach. We have also\u00a0\u2026", "num_citations": "4\n", "authors": ["674"]}
{"title": "Analyzing array manipulating programs by program transformation\n", "abstract": " We explore a transformational approach to the problem of verifying simple array-manipulating programs. Traditionally, verification of such programs requires intricate analysis machinery to reason with universally quantified statements about symbolic array segments, such as \u201cevery data item stored in the segment A[i] to A[j] is equal to the corresponding item stored in the segment B[i] to B[j].\u201d We define a simple abstract machine which allows for set-valued variables and we show how to translate programs with array operations to array-free code for this machine. For the purpose of program analysis, the translated program remains faithful to the semantics of array manipulation. Based on our implementation in LLVM, we evaluate the approach with respect to its ability to extract useful invariants and the cost in terms of code size.", "num_citations": "4\n", "authors": ["674"]}
{"title": "Improved analysis of logic programs using a differential approach\n", "abstract": " Abstract interpretation based program analysis has proven very useful in compilation of constraint and logic programming languages. Unfortunately, existing theoretical frameworks are inherently imprecise. This is because of the way the frameworks handle call and return from an atom evaluation--in effect the same information may be added twice, leading to a loss of precision in many description domains. For this reason some implementations use seemingly ad hoc tricks. Here we formalize these tricks and suggest three methods for overcoming this loss of precision. Experimental and theoretical results indicate that use of these methods leads to more accurate and faster analyses for little extra implementation effort.", "num_citations": "4\n", "authors": ["674"]}
{"title": "A Benders decomposition approach to deciding modular linear integer arithmetic\n", "abstract": " Verification tasks frequently require deciding systems of linear constraints over modular (machine) arithmetic. Existing approaches for reasoning over modular arithmetic use bit-vector solvers, or else approximate machine integers with mathematical integers and use arithmetic solvers. Neither is ideal; the first is sound but inefficient, and the second is efficient but unsound. We describe a linear encoding which correctly describes modular arithmetic semantics, yielding an optimistic but sound approach. Our method abstracts the problem with linear arithmetic, but progressively refines the abstraction when modular semantics is violated. This preserves soundness while exploiting the mostly integer nature of the constraint problem. We present a prototype implementation, which gives encouraging experimental results.", "num_citations": "3\n", "authors": ["674"]}
{"title": "A partial-order approach to array content analysis\n", "abstract": " We present a parametric abstract domain for array content analysis. The method maintains invariants for contiguous regions of the array, similar to the methods of Gopan, Reps and Sagiv, and of Halbwachs and Peron. However, it introduces a novel concept of an array content graph, avoiding the need for an up-front factorial partitioning step. The resulting analysis can be used with arbitrary numeric relational abstract domains; we evaluate the domain on a range of array manipulating program fragments.", "num_citations": "3\n", "authors": ["674"]}
{"title": "Making connections: First year transition for computer science and software engineering students\n", "abstract": " During the last decade, an increasing emphasis has been placed on the need for carefully planned transition programs to help first-year students integrate into university. In this paper we critically examine our experiences in designing and running successive transition programs for Computer Science and Software Engineering students. Over the last three years we have trialled several models. At present, our program requires all entering students to be enrolled in a transition subject,\u201cMaking Connections\u201d, which runs for half a semester. The subject, led by designated academic staff, serves as a forum for students to learn about each other, the department and the university. The program includes a computer-based language and study skills assessment component, including self-assessment tasks. Students can extend the subject by taking academic skills workshops run by the university\u2019s student support services\u00a0\u2026", "num_citations": "3\n", "authors": ["674"]}
{"title": "Abstract Interpretation, Symbolic Execution and Constraints\n", "abstract": " Abstract interpretation is a static analysis framework for sound over-approximation of all possible runtime states of a program. Symbolic execution is a framework for reachability analysis which tries to explore all possible execution paths of a program. A shared feature between abstract interpretation and symbolic execution is that each-implicitly or explicitly-maintains constraints during execution, in the form of invariants or path conditions. We investigate the relations between the worlds of abstract interpretation, symbolic execution and constraint solving, to expose potential synergies.", "num_citations": "2\n", "authors": ["674"]}
{"title": "Symbolic execution with invariant inlay: Evaluating the potential\n", "abstract": " Dynamic symbolic execution (DSE) is a non-standard execution mechanism which, loosely, executes a program symbolically and, simultaneously, on concrete input. DSE is attractive because of several uses in software engineering, including the generation of test data suites with large coverage relative to test suite size. However, DSE struggles in the face of execution path explosion, and is often unable to cover certain kinds of difficult-to-reach program points. Invariant inlay is a technique that aims to improve a DSE tool by interspersing code with invariants, generated automatically using off-the-shelf tools for static program analysis using abstract interpretation. To capitalise fully on a static analyzer, invariant inlay applies certain instrumentations and testability transformations to the program source. In this paper we outline the invariant inlay approach, and how we have evaluated the idea, in order to determine its\u00a0\u2026", "num_citations": "2\n", "authors": ["674"]}
{"title": "Compositional Symbolic Execution: Incremental Solving Revisited\n", "abstract": " Symbolic execution can automatically explore different execution paths in a system under test and generate tests to precisely cover them. It has two main advantages-being automatic and thorough within a theory-and has many successful applications. The bottleneck of symbolic execution currently is the computation consumption for complex systems. Compositional Symbolic Execution (CSE) introduces a summarisation module to eliminate the redundancy in the exploration of repeatedly encountered code. In our previous work, we generalised the summarisation for any code fragments instead of functions. In this paper, we transplant this idea onto LLVM with many additional features, one of them being the use of incremental solving. We show that the combination of CSE and incremental solving is mutually beneficial. The obvious weakness of CSE is the lack of context during summarisation. We discuss the use of\u00a0\u2026", "num_citations": "2\n", "authors": ["674"]}
{"title": "Generating source inputs for metamorphic testing using dynamic symbolic execution\n", "abstract": " Metamorphic testing uses domain-specific properties about a program's intended behaviour to alleviate the oracle problem. From a given set of source test inputs, a set of follow-up test inputs are generated which have some relation to the source inputs, and their outputs are compared to outputs from the source tests, using metamorphic relations. We evaluate the use of an automated test input generation technique called dynamic symbolic execution (DSE) to generate the source test inputs for metamorphic testing. We investigate whether DSE increases source-code coverage and fault finding effectiveness of metamorphic testing compared to the use of random testing, and whether the use of metamorphic relations as a supportive technique improves the test inputs generated by DSE. Our results show that DSE improves the coverage and fault detection rate of metamorphic testing compared to random testing using\u00a0\u2026", "num_citations": "2\n", "authors": ["674"]}
{"title": "A low overhead method for recovering unused memory inside regions\n", "abstract": " Automating memory management improves both resource safety and programmer productivity. One approach, region-based memory management [9](RBMM), applies compile-time reasoning to identify points in a program at which memory can be safely reclaimed. The main advantage of RBMM over traditional garbage collection (GC) is the avoidance of expensive runtime analysis, which makes reclaiming memory much faster. On the other hand, GC requires no static analysis, and, operating at runtime, can have significantly more accurate information about object lifetimes. In this paper we propose a hybrid system that seeks to combine the advantages of both methods while avoiding the overheads that previous hybrid systems incurred. Our system can also reclaim array segments whose elements are no longer reachable.", "num_citations": "2\n", "authors": ["674"]}
{"title": "Towards region-based memory management for Go\n", "abstract": " Region-based memory management aims to lower the cost of deallocation through bulk processing: instead of recovering the memory of each object separately, it recovers the memory of a region containing many objects. It relies on static analysis to determine the set of memory regions needed by a program, the program points at which each region should be created and removed, and, for each memory allocation, the region that should supply the memory. The concurrent language Go has features that pose interesting challenges for this analysis. We present a novel design for region-based memory management for Go, combining static analysis, to guide region creation, and lightweight runtime bookkeeping, to help control reclamation. The main advantage of our approach is that it greatly limits the amount of re-work that must be done after each change to the program source code, making our approach more\u00a0\u2026", "num_citations": "2\n", "authors": ["674"]}
{"title": "Information loss in knowledge compilation: A comparison of Boolean envelopes\n", "abstract": " Since Selman and Kautz's seminal work on the use of Horn approximation to speed up the querying of knowledge bases, there has been great interest in Boolean approximation for AI applications. There are several Boolean classes with desirable computational properties similar to those of the Horn class. The class of affine Boolean functions, for example, has been proposed as an interesting alternative to Horn for knowledge compilation. To investigate the trade-offs between precision and efficiency in knowledge compilation, we compare, analytically and empirically, four well-known Boolean classes, and their combinations, for ability to preserve information. We note that traditional evaluation which explores unit-clause consequences of random hard 3-CNF formulas does not tell the full story, and we complement that evaluation with experiments based on a variety of assumptions about queries and the underlying\u00a0\u2026", "num_citations": "2\n", "authors": ["674"]}
{"title": "An algorithm for affine approximation of binary decision diagrams\n", "abstract": " This paper is concerned with the problem of Boolean approximation in the following sense: given a Boolean function class and an arbitrary Boolean function, what is the function\u2019s best proxy in the class? Specifically, what is its strongest logical consequence (or envelope) in the class of affine Boolean functions. We prove various properties of affine Boolean functions and their representation as ROBDDs. Using these properties, we develop an ROBDD algorithm to find the affine envelope of a Boolean function.", "num_citations": "2\n", "authors": ["674"]}
{"title": "Boolean Affine Approximation with Binary Decision Diagrams.\n", "abstract": " Selman and Kautz\u2019s work on knowledge compilation has established how approximation (strengthening and/or weakening) of a propositional knowledgebase can be used to speed up query processing, at the expense of completeness. In the classical approach, the knowledge-base is assumed to be presented as a propositional formula in conjunctive normal form (CNF), and Horn functions are used to overand under-approximate it (in the hope that many queries can be answered efficiently using the approximations only). However, other representations are possible, and functions other than Horn can be used for approximations, as long as they have deductioncomputational properties similar to those of the Horn functions. Zanuttini has suggested that the class of affine Boolean functions would be especially useful in knowledge compilation and has presented various affine approximation algorithms. Since CNF is awkward for presenting affine functions, Zanuttini considers both a sets-of-models representation and the use of modulo 2 congruence equations. Here we consider the use of reduced ordered binary decision diagrams (ROBDDs), a representation which is more compact than the sets of models and which (unlike modulo 2 congruences) can express any source knowledge-base. We present an ROBDD algorithm to find strongest affine upper-approximations of a Boolean function and we argue its correctness.", "num_citations": "2\n", "authors": ["674"]}
{"title": "Un-Kleene Boolean equation solving\n", "abstract": " We present a new method for finding closed forms of recursive Boolean function definitions. Traditionally, these closed forms are found by Kleene iteration: iterative approximation until a fixed point is reached. Conceptually, our new method replaces each k-ary function by 2k Boolean constants defined by mutual recursion. The introduction of an exponential number of constants is mitigated by the simplicity of their definitions and by the use of a novel variant of ROBDDs to avoid repeated computation. Experiments suggest that this approach is significantly faster than Kleene iteration for examples that require many Kleene iteration steps.", "num_citations": "2\n", "authors": ["674"]}
{"title": "A Fresh Look at Zones and Octagons\n", "abstract": " Zones and Octagons are popular abstract domains for static program analysis. They enable the automated discovery of simple numerical relations that hold between pairs of program variables. Both domains are well understood mathematically but the detailed implementation of static analyses based on these domains poses many interesting algorithmic challenges. In this article, we study the two abstract domains, their implementation and use. Utilizing improved data structures and algorithms for the manipulation of graphs that represent difference-bound constraints, we present fast implementations of both abstract domains, built around a common infrastructure. We compare the performance of these implementations against alternative approaches offering the same precision. We quantify the differences in performance by measuring their speed and precision on standard benchmarks. We also assess, in the\u00a0\u2026", "num_citations": "1\n", "authors": ["674"]}
{"title": "Transformation-Enabled Precondition Inference\n", "abstract": " Precondition inference is a non-trivial problem with important applications in program analysis and verification. We present a novel iterative method for automatically deriving preconditions for the safety and unsafety of programs. Each iteration maintains over-approximations of the set of safe and unsafe initial states, which are used to partition the program\u2019s initial states into those known to be safe, known to be unsafe and unknown. We then construct revised programs with those unknown initial states and iterate the procedure until the approximations are disjoint or some termination criteria are met. An experimental evaluation of the method on a set of software verification benchmarks shows that it can infer precise preconditions (sometimes optimal) that are not possible using previous methods.", "num_citations": "1\n", "authors": ["674"]}
{"title": "String constraint solving: past, present and future\n", "abstract": " String constraint solving is an important emerging field, given the ubiquity of strings over different fields such as formal analysis, automated testing, database query processing, and cybersecurity. This paper highlights the current state-of-the-art for string constraint solving, and identifies future challenges in this field.", "num_citations": "1\n", "authors": ["674"]}
{"title": "Dissecting Widening: Separating Termination from Information\n", "abstract": " Widening ensures or accelerates convergence of a program analysis, and sometimes contributes a guarantee of soundness that would otherwise be absent. In this paper we propose a generalised view of widening, in which widening operates on values that are not necessarily elements of the given abstract domain, although they must be in a correspondence, the details of which we spell out. We show that the new view generalizes the traditional view, and that at least three distinct advantages flow from the generalization. First, it gives a handle on \u201ccompositional safety\u201d, the problem of creating widening operators for product domains. Second, it adds a degree of flexibility, allowing us to define variants of widening, such as delayed widening, without resorting to intrusive surgery on an underlying fixpoint engine. Third, it adds a degree of robustness, by making it difficult for an analysis implementor to make certain\u00a0\u2026", "num_citations": "1\n", "authors": ["674"]}
{"title": "Optimal bounds for floating-point addition in constant time\n", "abstract": " Reasoning about floating-point numbers is notoriously difficult, owing to the lack of convenient algebraic properties such as associativity. This poses a substantial challenge for program analysis and verification tools which rely on precise floating-point constraint solving. Currently, interval methods in this domain often exhibit slow convergence even on simple examples. We present a new theorem supporting efficient computation of exact bounds of the intersection of a rectangle with the preimage of an interval under floating-point addition, in any radix or rounding mode. We thus give an efficient method of deducing optimal bounds on the components of an addition, solving the convergence problem.", "num_citations": "1\n", "authors": ["674"]}
{"title": "A complete refinement procedure for regular separability of context-free languages\n", "abstract": " Often, when analyzing the behaviour of systems modelled as context-free languages, we wish to know if two languages overlap. To this end, we present a class of semi-decision procedures for regular separability of context-free languages, based on counter-example guided abstraction refinement. We propose two effective instances of this approach, one that is complete but relatively expensive, and one that is inexpensive and sound, but for which we do not have a completeness proof. The complete method will prove disjointness whenever the input languages are regularly separable. Both methods will terminate whenever the input languages overlap. We provide an experimental evaluation of these procedures, and demonstrate their practicality on a range of verification and language-theoretic instances.", "num_citations": "1\n", "authors": ["674"]}
{"title": "Optimisation and Relaxation for Multiagent Planning in the Situation Calculus\n", "abstract": " The situation calculus can express rich agent behaviours and goals and facilitates the reduction of complex planning problems to theorem proving. However, in many planning problems, solution quality is critically important, and the achievable quality is not necessarily known in advance. Existing Golog implementations merely search for a Legal plan, typically relying on depth-first search to find an execution. We illustrate where existing strategies will not terminate when quality is considered, and to overcome this limitation we formally introduce the notion of cost to simplify the search for a solution. The main contribution is a new class of relaxations of the planning problem, termed precondition relaxations, based on Lagrangian relaxation. We show how this facilitates optimisation of a restricted class of Golog programs for which plan existence (under a cost budget) is decidable. It allows for tractably computing relaxations to the planning problem and leads to a general, blackbox, approach to optimally solving multi-agent planning problems without explicit reference to the semantics of interleaved concurrency.", "num_citations": "1\n", "authors": ["674"]}
{"title": "Boolean equation solving as graph traversal\n", "abstract": " We present a new method for finding closed forms of recursive Boolean function definitions. Traditionally, these closed forms are found by iteratively approximating until a fixed point is reached. Conceptually, our new method replaces each k-ary function by 2k Boolean variables defined by mutual recursion. The introduction of an exponential number of variables is mitigated by the simplicity of their definitions and by the use of a novel variant of ROBDDs to avoid repeated computation. Experimental evaluation suggests that this approach is significantly faster than Kleene iteration for examples that would require many Kleene iteration steps.", "num_citations": "1\n", "authors": ["674"]}
{"title": "Strictness analysis as finite-domain constraint solving\n", "abstract": " It has become popular to express dataflow analyses in logical form. In this paper we investigate a new approach to the analysis of functional programs, based on synthesis of constraint logic programs. We sketch how the language Toupie, originally designed with logic program analysis as one objective, lends itself also to sophisticated strictness analysis. Strictness analysis is straightforward in the simplest case, that of analysing a first-order functional language using just two strictness values, namely divergence and \u201cdon\u2019t know\u201d. Mycroft\u2019s classical translation immediately yields perfectly valid Boolean constraint logic programs, which, when run, provide the desired strictness information. However, more sophisticated analysis requires more complex domains of strictness values. We recast Wadler\u2019s classical analysis over a 2n-point domain as finite-domain constraint solving. This approach has several\u00a0\u2026", "num_citations": "1\n", "authors": ["674"]}
{"title": "Two applications of an incremental analysis engine for (constraint) logic programs\n", "abstract": " High-level programming languages based on logic programming have developed considerably over recent years. Many real world problems have solutions naturally expressed in logic, and the combination of constraint programming and logic programming has made available new powerful programming languages that allow solutions to he expressed neatly and with minimal effort. An unfortunate but expected initial disadvantage of these new programming languages is their lack of efficiency compared to imperative languages. However, sophisticated optimization techniques based on abstract interpretation are now being included into the next generation of compilers for these languages, often causing dramatic efficiency improvements.", "num_citations": "1\n", "authors": ["674"]}